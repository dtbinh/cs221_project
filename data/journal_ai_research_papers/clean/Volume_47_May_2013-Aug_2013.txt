Journal of Artificial Intelligence Research 47 (2013) 521-573

Submitted 01/13; published 07/13

Topic Segmentation and Labeling
in Asynchronous Conversations
Shafiq Joty

sjoty@qf.org.qa

Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar

Giuseppe Carenini
Raymond T. Ng

carenini@cs.ubc.ca
rng@cs.ubc.ca

University of British Columbia
Vancouver, BC, Canada, V6T 1Z4

Abstract
Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing
(NLP) applications. We present two new corpora of email and blog conversations annotated
with topics, and evaluate annotator reliability for the segmentation and labeling tasks in
these asynchronous conversations. We propose a complete computational framework for
topic segmentation and labeling in asynchronous conversations. Our approach extends
state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods
for NLP. For topic segmentation, we propose two novel unsupervised models that exploit
the fine-grained conversational structure, and a novel graph-theoretic supervised model that
combines lexical, conversational and topic features. For topic labeling, we propose two novel
(unsupervised) random walk models that respectively capture conversation specific clues
from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our
best models beat the state-of-the-art, and are highly correlated with human annotations.

1. Introduction
With the ever increasing popularity of Internet technologies, it is very common nowadays
for people to discuss events, issues, tasks and personal experiences in social media (e.g.,
Facebook, Twitter, blogs, fora) and email (Verna, 2010; Baron, 2008). These are examples of asynchronous conversations where participants communicate with each other at
different times. The huge amount of textual data generated everyday in these conversations calls for automated methods of conversational text analysis. Effective processing of
these conversational texts can be of great strategic value for both organizations and individuals (Carenini, Murray, & Ng, 2011). For instance, managers can find the information
exchanged in email conversations within a company to be extremely valuable for decision
auditing. If a decision turns out to be ill-advised, mining the relevant conversations may
help in determining responsibility and accountability. Similarly, conversations that led to
favorable decisions could be mined to identify effective communication patterns and sources
within the company. In public blogging services (e.g., Twitter, Slashdot), conversations ofc
2013
AI Access Foundation. All rights reserved.

fiJoty, Carenini, & Ng

ten get very large involving hundreds of bloggers making potentially thousands of comments.
During a major event such as a political uprising in Egypt, relevant messages are posted
by the thousands or millions. It is simply not feasible to read all messages relevant to such
an event, and so mining and summarization technologies can help providing an overview of
what people are saying and what positive or negative opinions are being expressed. Mining
and summarizing of conversations also aid improved indexing and searching. On a more
personal level, an informative summary of a conversation could greatly support a new participant to get up to speed and join an already existing conversation. It could also help
someone to quickly prepare for a follow-up discussion of a conversation she was already part
of, but which occurred too long ago for her to remember the details.
Topic segmentation and labeling is often considered a prerequisite for higher-level
conversation analysis (Bangalore, Di Fabbrizio, & Stent, 2006) and has been shown to be
useful in many Natural Language Processing (NLP) applications including automatic summarization (Harabagiu & Lacatusu, 2005; Kleinbauer, Becker, & Becker, 2007; Dias, Alves,
& Lopes, 2007), text generation (Barzilay & Lee, 2004), information extraction (Allan,
2002), and conversation visualization (Liu, Zhou, Pan, Song, Qian, Cai, & Lian, 2012).
Adapting the standard definition of topic (Galley, McKeown, Fosler-Lussier, & Jing,
2003) to asynchronous conversations, we consider a topic to be something about which
the participants discuss or argue or express their opinions. Multiple topics seem to occur
naturally in social interactions, whether synchronous (e.g., meetings, chats) or asynchronous
(e.g., emails, blogs). In the naturally occurring ICSI multi-party meetings (Janin et al.
2003), Galley et al. (2003) report an average of 7.5 topical segments per conversation. In
multi-party chat, Elsner and Charniak (2010) report an average of 2.75 discussions active
at a time. In the email and blog corpora, that we present in this article, annotators found
an average of 2.5 and 10.77 topics per email and blog conversation, respectively.
Topic segmentation refers to the task of grouping the sentences of an asynchronous
conversation into a set of coherent topical clusters (or segments)1 , and topic labeling is the
task of assigning a short description to each of the topical clusters to facilitate interpretations of the topics (Purver, 2011). For example, in the sample truncated email conversation
from our corpora shown in Figure 1, the majority of our three annotators found three different topics (or clusters). Likewise, in the truncated blog conversation shown in Figure 2,
our annotators found six different topics. The right most column in each figure specifies
a particular segmentation by assigning the same topic ID (or cluster ID) to sentences belonging to the same topic. The topics in each figure are also differentiated using different
colors. The topic labels assigned by the annotators are listed below each conversation (e.g.,
Telecon cancellation, Tag document, Responding to I18N in Figure 1).
While extensive research has been conducted in topic segmentation for monolog (e.g.,
news articles) and synchronous dialog (e.g., meetings), none has studied the problem of
segmenting and labeling asynchronous conversations (e.g., email, blog). Therefore, there is
no reliable annotation scheme, no standard corpus, and no agreed-upon metrics available.
Also, it is our key observation that, because of its asynchronous nature, and the use of
quotation (Crystal, 2001), topics in these conversations are often interleaved and do not
change in a sequential way. That is, if we look at the temporal order of the sentences in
1. In this article, we use the terms topical cluster and topical segment interchangeably.

522

fiTopic Segmentation and Labeling in Asynchronous Conversations

From: Brian To: rdf core Subject: 20030220 telecon Date: Tue Feb 17 13:52:15
I propose to cancel this weeks telecon and schedule another for 12 Mar 2004, if needed.
I would like to get moving on comments on the TAG architecture document.
Jan  are you still up for reviewing?
Can we aim to get other comments in by the end of this week and agreement by email next week?

Topic
[1]
[2]
[2]
[2]

From: Jeremy To: Brian Subject: Re: 20030220 telecon Date: Wed Feb 18 05:18:10
> I propose to cancel this weeks telecon and schedule another for 12 Mar 2004, if needed.
> .. agreement by email next week?
I think that means we will not formally respond to I18N on the charmod comments, shall I tell them that we do
not intend to, but that the e-mail discussion has not shown any disagreement.
e.g. I have informed the RDF Core WG of your decisions, and no one has indicated unhappiness - however we
have not formally discussed these issues; and are not likely to.

[3]
[3]

From: Brian To: Jeremy Subject: Re: 20030220 telecon Date: Wed Feb 18 13:16:21
> I think that means we will not formally respond to I18N on the charmod comments, shall
> I tell them that we do not intend to, but that the e-mail discussion has not shown any disagreement.
Ah, Is this a problem. Have I understood correctly they are going through last call again anyway.
> e.g. I have informed the RDF Core WG of your decisions, and no one has indicated unhappiness
> - however we have not formally discussed these issues; and are not likely to.
When is the deadline?
I'm prepared to decide by email so we can formally respond by email.

[3]

[3]
[3]

From: Pat To: Brian Subject: Re: 20030220 telecon Date: Wed Feb 18 16:56:26
> I propose to cancel this weeks telecon and schedule another for 12 Mar 2004, if needed.
Im assuming that they are all cancelled unless I hear otherwise.
Maybe that should be our default?
> I would like to get moving on comments on the TAG architecture document.
I still plan to write a rather long diatribe on this if I can find the time.
I doubt if the rest of the WG will endorse all of it but I will send it along asap, hopefully some time next week.

[1]
[1]
[2]
[2]

From: Jeremy To: Brian Subject: Re: 20030220 telecon Date: Thu Feb 19 05:42:21
> Ah. Is this a problem. Have I understood correctly they are going through last call again anyway.
Yes  I could change my draft informal response to indicate that if we have any other formal response it will be
included in our LC review comments on their new documents.
> When is the deadline?
> I'm prepared to decide by email so we can formally respond by email.
Two weeks from when I received the message ....i.e. during Cannes
-I suspect that is also the real deadline, in that I imagine they want to make their final decisions at Cannes.
I am happy to draft a formal response that is pretty vacuous, for e-mail vote. is pretty vacuous, for e-mail vote.

[3]

[3]
[3]
[3]

Topic Labels
Topic 1 (green): Telecon cancellation, Topic 2 (magenta): TAG document, Topic 3 (blue): Responding to I18N.

Figure 1: Sample truncated email conversation from our email corpus. Each color indicates a
different topic. The right most column specifies the topic assignments for the sentences.

523

fiJoty, Carenini, & Ng

Author: Soulskill Title: Bethesda Releases Daggerfall For Free Type: Article
On Thursday, Bethesda announced that for the 15th anniversary of the Elder Scrolls series, they were releasing
The Elder Scrolls II: Daggerfall for free.
They aren't providing support for the game anymore, but they posted a detailed description of how to get the
game running in DOSBox.
Fans of the series can now easily relive the experience of getting completely lost in those enormous dungeons.
Save often.

Fragment
(a)

Topic
[1]
[1]

(b)

[2]
[2]

(c)

[2]
[2]
[2]
[2]

Author: Datamonstar Title: Nice nice nice nice... Comment id: 1 Parent id: None Type: Comment
>Fans of the series can now easily relive the experience of getting completely lost in those enormous dungeons.
>Save often.
... well not really, since this game is soooo old, but still its a huge HUGE gameworld.
Really, It's big.
Can't wait to play it.
It makes Oblivion look like Sesame Street.
Author: Freetardo Title: Re: Nice nice nice nice... Comment id: 2 Parent id: 1 Type: Comment
Yes it is big, but most of it is just the same thing over and over again.
It was quite monotonous at times, really.

[3]
(d)

[3]

(e)

[4]

Author: gbarules2999 Title: Re: Nice nice nice nice... Comment id: 3 Parent id: 1 Type: Comment
Randomly generated HUGE isn't nearly as good as designed small.
Back to Morrowind, folks.

(f)

[5]

Author: drinkypoo Title: Re: Nice nice nice nice... Comment id: 4 Parent id: 3 Type: Comment
>Randomly generated HUGE isn't nearly as good as designed small.
The solution is obviously to combine both approaches.
That way a single game will satisfy both types of players.

(g)

[4]
[4]

(h)

[1]

(i)

[1]

(j)

[3]
[3]

Author: ElrondHubbard Title: Rest well this night -- Comment id: 5 Parent id: None Type: Comment
-- for tomorrow, you sail for the kingdom... of Daggerfall.
Many, many enjoyable hours I spent playing this game when I could (should) have been working on my thesis.
Chief complaint: The repetitive dungeons, stitched together seemingly near-randomly from prefabbed bits and
pieces that were repeated endlessly.
Still, a great game.

[1]

Author: Anonymous Title: Re:Rest well this night -- Comment id: 6 Parent id: 5 Type: Comment
>Many, many enjoyable hours I spent playing this game when I could (should) have been working on my thesis
So, how did your thesis go?
>Chief complaint: The repetitive dungeons, stitched together seemingly near-randomly a great game
I also think this is a great game.

(k)

[0]

(l)

[1]

Topic Labels
Topic 1 (green): Free release of Daggerfall and reaction, Topic 2 (purple): Game contents or size, Topic 3 (orange): Bugs
or faults, Topic 4 (magenta): Game design, Topic 5 (blue): Other gaming options, Topic 0 (red): `OFF-TOPIC'.

Figure 2: Sample truncated blog conversation from our blog corpus. Each color indicates a different
topic. The right most column (i.e., Topic) specifies the topic assignments for the sentences. The
Fragment column specifies the fragments in the FQG (see Section 3.1.3).

524

fiTopic Segmentation and Labeling in Asynchronous Conversations

a conversation, the discussion of a topic may appear to intersect with the discussion of
others. As can be seen in Figure 1, after a discussion of topic 3 in the second and third
email, topics 1 and 2 are revisited in the fourth email, then topic 3 is again brought back
in the fifth email. Therefore, the sequentiality constraint of topic segmentation in monolog
and synchronous dialog does not hold in asynchronous conversation. As a result, we do
not expect models which have proved successful in monolog or synchronous dialog to be as
effective, when directly applied to asynchronous conversation.
Our contributions in this article aim to remedy these problems. First, we present two
new corpora of email and blog conversations annotated with topics, and evaluate annotator
reliability for the topic segmentation and labeling tasks using a new set of metrics, which
are also used to evaluate the computational models. To our knowledge, these are the
first such corpora that will be made publicly available. Second, we present a complete
topic segmentation and labeling framework for asynchronous conversations. Our approach
extends state-of-the-art methods (for monologs and synchronous dialogs) by considering
a fine-grained structure of the asynchronous conversation along with other conversational
features. In doing so, we apply recent graph-based methods for NLP (Mihalcea & Radev,
2011) such as min-cut and random walk on paragraph, sentence or word graphs.
For topic segmentation, we propose two novel unsupervised models that exploit, in
a principled way, the fine-grained conversational structure beyond the lexical information.
We also propose a novel graph-theoretic supervised topic segmentation model that combines
lexical, conversational, and topic features. For topic labeling, we propose to generate labels
using an unsupervised extractive approach that identifies the most representative phrases in
the text. Specifically, we propose two novel random walk models that respectively captures
two forms of conversation specific information: (i) the fact that the leading sentences in a
topical cluster often carry the most informative clues, and (ii) the fine-grained conversational
structure. To the best of our knowledge, this is also the first comprehensive study to address
the problem of topic segmentation and labeling in asynchronous conversation.
Our framework was tested in a series of experiments. Experimental results in the topic
segmentation task show that the unsupervised segmentation models benefit when they consider the finer conversational structure of asynchronous conversations. A comparison of the
supervised segmentation model with the unsupervised models reveals that the supervised
method, by optimizing the relative weights of the features, outperforms the unsupervised
ones even using only a few labeled conversations. Remarkably, the segmentation decisions
of the best unsupervised and the supervised models are also highly correlated with human
annotations. As for the experiments on the topic labeling task, they show that the random
walk model performs better when it exploits the conversation specific clues from the leading
sentences and the conversational structure. The evaluation of the end-to-end system also
shows promising results in both corpora, when compared with human annotations.
In the rest of this article, after discussing related work in Section 2, we present our
segmentation and labeling models in Section 3. We then describe our corpora and evaluation metrics in Section 4. The experiments and analysis are presented in Section 5. We
summarize our contributions and consider directions for future work in Section 6.
525

fiJoty, Carenini, & Ng

2. Related Work
Three research areas are directly related to our study: topic segmentation, topic labeling,
and extracting the conversation structure of asynchronous conversations.
2.1 Topic Segmentation
Topic segmentation has been extensively studied both for monologs and synchronous dialogs
where the task is to divide the discourse into topically coherent sequential segments (for a
detailed overview see Purver, 2011). The unsupervised models rely on the discourse cohesion
phenomenon, where the intuition is that sentences in a segment are lexically similar to each
other but not to sentences in the preceding or the following segment. These approaches
mainly differ in how they measure the lexical similarity between sentences.
One such early approach is TextTiling (Hearst, 1997), which still forms the baseline
for many recent advancements. It operates in three steps: tokenization, lexical score determination, and depth score computation. In the tokenization step, it forms the fixed
length pseudo-sentences, each containing n stemmed words. Then it considers blocks of k
pseudo-sentences, and for each gap between two consecutive pseudo-sentences it measures
the cosine-based lexical similarity between the adjacent blocks by representing them as vectors of term frequencies. Finally, it measures the depth of the similarity valley for each gap,
and assigns the topic boundaries at the appropriate sentence gaps based on a threshold.
When similarity is computed only on the basis of raw term frequency (TF) vectors, it can
cause problems because of sparseness, and because it treats the terms independently. Choi,
Hastings, and Moore (2001) use Latent Semantic Analysis (LSA) to measure the similarity
and show that LSA-based similarity performs better than the raw TF-based similarity.
Unlike TextTiling, which uses a threshold to decide on topic boundaries, Choi et al. use
divisive clustering to find the topical segments. We use similarity measures based on both
TF and LSA as features in our supervised segmentation model.
Another variation of the cohesion-based approach is LCSeg (Galley et al., 2003), which
uses lexical chains (Morris & Hirst, 1991). LCSeg first finds the chains based on term
repetitions, and weights those based on term frequency and chain length. The cosine similarity between two adjacent blocks lexical chain vectors is then used as a measure of lexical
cohesion in a TextTiling-like algorithm to find the segments. LCSeg achieves results comparable to the previous approaches (e.g., Choi et al., 2001) in both monolog (i.e., newspaper)
and synchronous dialog (i.e., meeting). Galley et al. also propose a supervised model for
segmenting meeting transcripts. They use a C4.5 probabilistic classifier with lexical and
conversational features and show that it outperforms the unsupervised method (LCSeg).
Hsueh, Moore, and Renals (2006) apply the models of Galley et al. (2003) to both the
manual transcripts and the ASR (automatic speech recognizer) output of meetings. They
perform segmentation at both coarse (topic) and fine (subtopic) levels. At the topic level,
they get similar results as Galley et al.  the supervised model outperforming LCSeg. However, at the subtopic level, LCSeg surprisingly outperforms the supervised model indicating
that finer topic shifts are better characterized by lexical similarity alone.
In our work, we initially show how LCSeg performs poorly, when applied to the temporal
ordering of the asynchronous conversation. This is because, as we mentioned earlier, topics
in asynchronous conversations are interleaved and do not change sequentially following the
526

fiTopic Segmentation and Labeling in Asynchronous Conversations

temporal order of the sentences. To address this, we propose a novel extension of LCSeg that
leverages the fine conversational structure of asynchronous conversations. We also propose
a novel supervised segmentation model for asynchronous conversation that achieves even
higher segmentation accuracy by combining lexical, conversational, and topic features.
Malioutov and Barzilay (2006) use a minimum cut clustering model to segment spoken
lectures (i.e., spoken monolog). They form a weighted undirected graph where the nodes
represent the sentences and the weighted edges represent the TF.IDF-based cosine similarity
between the sentences. Then the segmentation can be solved as a graph partitioning problem
with the assumption that the sentences in a segment should be similar, while sentences in
different segments should be dissimilar. They optimize the normalized cut criterion (Shi &
Malik, 2000) to extract the segments. In general, the minimization of the normalized cut is
NP-complete. However, the sequentiality constraint of topic segmentation in monolog allows
them to find an exact solution in polynomial time. Their approach performs better than
the approach of Choi et al. (2001) in the corpus of spoken lectures. Since the sequentiality
constraint does not hold in asynchronous conversation, we implement this model without
this constraint by approximating the solution, and compare it with our models.
Probabilistic generative models, such as variants of Latent Dirichlet Allocation (LDA)
(Blei, Ng, & Jordan, 2003) have also proven to be successful for topic segmentation in
monolog and synchronous dialog. Blei and Moreno (2001) propose an aspect Hidden Markov
Model (AHMM) to perform topic segmentation in written and spoken (i.e., transcribed)
monologs, and show that the AHMM model outperforms the HMM for this task. Purver
et al. (2006) propose a variant of LDA for segmenting meeting transcripts, and use the top
words in the topic-word distributions as topic labels. However, their approach does not outperform LCSeg. Eisenstein and Barzilay (2008) propose another variant by incorporating
cue words into the (sequential) segmentation model. In a follow-up work, Eisenstein (2009)
proposes a constrained LDA model that uses multi-scale lexical cohesion to perform hierarchical topic segmentation. Nguyen, Boyd-Graber, and Resnik (2012) successfully incorporate speaker identity into a hierarchical nonparametric model for segmenting synchronous
conversations (e.g., meeting, debate). In our work, we demonstrate how the general LDA
model performs for topic segmentation in asynchronous conversation and propose a novel
extension of LDA that exploits the fine conversational structure.
2.2 Topic Labeling
In the first comprehensive approach to topic labeling, Mei, Shen, and Zhai (2007) propose
methods to label multinomial topic models (e.g., the topic-word distributions returned by
LDA). Crucial to their approach is how they measure the semantic similarity between a
topic-word distribution and a candidate topic label extracted from the same corpus. They
perform such task by assuming another word distribution for the label and deriving the
Kullback-Leibler divergence between the two distributions. It turns out that this measure
is equivalent to the weighted point-wise mutual information (PMI) of the topic-words with
the candidate label, where the weights are actually the probabilities in the topic-word
distribution. They use Maximum Marginal Relevance (MMR) (Carbonell & Goldstein,
1998) to select the labels which are relevant, but not redundant. When labeling multiple
topic-word distributions, to find discriminative labels, they adjust the semantic similarity
527

fiJoty, Carenini, & Ng

scoring function such that a candidate label which is also similar to other topics gets lower
score. In our work, we also use MMR to promote diversity in the labels for a topic. However,
to get distinguishable labels for different topical segments in a conversation, we rank the
words so that a high scoring word in one topic should not have high scores in other topics.
Recently, Lau, Grieser, Newman, and Baldwin (2011) propose methods to learn topic
labels from Wikipedia titles. They use the top-10 words in each topic-word distribution
to extract the candidate labels from Wikipedia. Then they extract a number of features
to represent each candidate label. The features are actually different metrics used in the
literature to measure the association between the topic words and the candidate label (e.g.,
PMI, t-test, chi-square test). They use Amazon Mechanical Turk to get humans to rank
the top-10 candidate labels and use the average scores to learn a regression model.
Zhao, Jiang, He, Song, Achananuparp, Lim, and Li (2011a) addresses the problem of
topical keyphrase extraction from Twitter. Initially they use a modified Twitter-LDA model
(Zhao, Jiang, Weng, He, Lim, Yan, & Li, 2011b), which assumes a single topic assignment
for a tweet, to discover the topics in the corpus. Then, they use a PageRank (Page, Brin,
Motwani, & Winograd, 1999) to rank the words in each topic-word distribution. Finally,
they perform a bi-gram test to generate keyphrases from the top ranked words in each topic.
While all the above studies try to mine topics from the whole corpus, our problem
is to find the topical segments and label those for a given conversation, where topics are
closely related and distributional variations are subtle (e.g., Game contents or size, Game
design in Figure 2). Therefore, statistical association metrics like PMI, t-test, chi-square
test may not be reliable in our case because of data scarcity. Also at the conversation-level,
the topics are too specific to a particular discussion (e.g., Telecon cancellation, TAG
document, Responding to I18N in Figure 1) that exploiting external knowledge bases
like Wikipedia as a source of candidate labels is not a reasonable option for us. In fact,
none of the human-authored labels in our developement set appears in Wikipedia as a title.
Therefore, we propose to generate topic labels using a keyphrase extraction method that
finds the most representative phrase(s) in the given text.
Several supervised and unsupervised methods have been proposed for keyphrase extraction (for a comprehensive overview see Medelyan, 2009). The supervised models (e.g.,
Hulth, 2003; Medelyan, Frnak, & Witten, 2009) follow the same two-stage framework. First,
candidate keyphrases are extracted using n-gram sequences or a shallow parser (chunker).
Second, a classifier filters the candidates. This strategy has been quite successful, but it
is domain specific and labor intensive. Every new domain may require new annotations,
which at times becomes too expensive and unrealistic. In contrast, our approach is to adopt
an unsupervised paradigm, which is more robust across new domains, but still capable of
achieving comparable performance to the supervised methods.
Mihalcea and Tarau (2004) use a graph-based (unsupervised) random walk model to
extract keyphrases from journal abstracts and achieve the state-of-the-art performance
(Mihalcea & Radev, 2011).2 However, this model is generic and not designed to exploit
properties of asynchronous conversations. We propose two novel random walk models to
incorporate conversation specific information. Specifically, our models exploit information
2. The original work was published before by Mihalcea and Tarau (2004).

528

fiTopic Segmentation and Labeling in Asynchronous Conversations

from two different sources: (i) from the leading sentences of the topical segments, and (ii)
from the fine conversational structure of the conversation.
2.3 Conversational Structure Extraction
Several approaches have been proposed to capture the underlying conversational structure
of a conversation. Recent work on synchronous conversations has been focusing on disentangling multi-party chats, which have a linear structure. For example, several studies propose
models to disentangle multi-party chat (Elsner & Charniak, 2010, 2011; Wang & Oard,
2009; Mayfield, Adamson, & Rose, 2012). On the other hand, asynchronous conversations
like email and social media services (e.g., Gmail, Twitter) generally organize comments into
tree-structured threads using headers. Automatic methods to uncover such more complex
structures have also been proposed (e.g., Wang, Wang, Zhai, & Han, 2011; Aumayr, Chan,
& Hayes, 2011). However, the use of quotation in asynchronous conversations can express
a conversational structure that is finer grained and can be more informative than the one
revealed by reply-to relations between comments (Carenini et al., 2011). For example, in
Figures 1 and 2, the proximity between a quoted paragraph and an unquoted one can represent an informative conversational link between the two (i.e., they talk about the same
topic) that would not appear by only looking at the reply-to relations.
We previously presented a novel method to capture an email conversation at this finer
level by analyzing the embedded quotations in emails (Carenini, Ng, & Zhou, 2007). A
Fragment Quotation Graph (FQG) was formed, which was shown to be beneficial for email
summarization (Carenini, Ng, & Zhou, 2008) and dialog act modeling (Joty, Carenini,
& Lin, 2011). In this work, we generalize the FQG to any asynchronous conversation and
demonstrate that topic segmentation and labeling models can also benefit significantly from
this fine conversational structure of asynchronous conversation.

3. Topic Models for Asynchronous Conversations
Developing topic segmentation and labeling models for asynchronous conversations is challenging partly because of the specific characteristics of these media. As mentioned earlier,
unlike monolog (e.g., a news article) and synchronous dialog (e.g., a meeting), topics in
asynchronous conversations may not change in a sequential way, with topics being interleaved. Furthermore, as can be noticed in Figures 1 and 2, writing style varies among
participants, and many people tend to use informal, short and ungrammatical sentences,
thus making the discourse much less structured. One aspect of asynchronous conversation
that at first glance may appear to help topic modeling is that each message comes with
a header. However, often headers do not convey much topical information and sometimes
they can even be misleading. For example, in the blog conversation (Figure 2), participants
keep talking about different topics using the same title (i.e., nice nice nice), which does
not convey any topic information. Arguably, all these unique properties of asynchronous
conversations limit the application of state-of-the-art techniques that have been successful
in monolog and synchronous dialog. Below, we first describe these techniques and then we
present how we have extended them to effectively deal with asynchronous conversations.
529

fiJoty, Carenini, & Ng

3.1 Topic Segmentation Models
We are the first to study the problem of topic segmentation in asynchronous conversation. Therefore, we first show how the existing models, which are originally developed for
monolog and synchronous dialog, can be naively applied to asynchronous conversations.
Then, by pointing out their limitations, we propose our novel topic segmentation models
for asynchronous conversations.
3.1.1 Existing Models
LCSeg (Galley et al., 2003) and LDA (Blei et al., 2003) are the two state-of-the-art unsupervised models for topic segmentation in monolog and synchronous dialog (Purver, 2011).
In the following, we briefly describe these models and how they can be directly applied to
asynchronous conversations.
Lexical Cohesion-based Segmenter (LCSeg)
LCSeg is a sequential segmentation model originally developed for segmenting meeting transcripts. It exploits the linguistic property called lexical cohesion, and assumes that topic
changes are likely to occur where strong word repetitions start and end. It first computes
lexical chains (Morris & Hirst, 1991) for each non-stop word based on word repetitions.3
Then the chains are weighted according to their term frequency and the chain length. The
more populated and compact chains get higher scores. The algorithm then works with two
adjacent analysis windows, each of a fixed size k, which is empirically determined. At each
sentence boundary, it computes the cosine similarity (or lexical cohesion function) between
the two windows by representing each window as a vector of chain-scores of its words.
Specifically, the lexical cohesion between windows (X and Y ) is computed with:
PN

wi,X .wi,Y
LexCoh(X, Y ) = cos sim(X, Y ) = qP i=1
PN
N
2
2
i=1 wi,Y
i=1 wi,X .

(1)

where N is the number of chains and

rank(Ci ) if chain Ci overlaps   {X, Y }
wi, =
0
otherwise
A sharp change at local minima in the resulting similarity curve signals a high probability
of a topic boundary. The curve is smoothed, and for each local minimum it computes a
segmentation probability based on its relative depth below its nearest peaks on either side.
Points with the highest segmentation probability are then selected as hypothesized topic
boundaries. This method is similar to TextTiling (Hearst, 1997) except that the similarity
is computed based on the scores of the chains instead of term frequencies.
LCSeg can be directly applied to an asynchronous conversation by arranging its comments based on their arrival time (i.e., temporal order) and running the algorithm to get
the topic boundaries.
3. One can also consider other lexical semantic relations (e.g., synonym, hypernym) in lexical chaining but
the best results account for only repetition.

530

fiTopic Segmentation and Labeling in Asynchronous Conversations

Latent Dirichlet Allocation (LDA)
LDA is a generative model that relies on the fundamental idea that documents are admixtures of topics, and a topic is a multinomial distribution over words. It specifies the
following distribution over words within a document:
P (xij ) =

K
X

P (xij |zij = k, bk )P (zij = k|i )

(2)

k=1

where K is the number of topics, P (xij |zij = k, bk ) is the probability of word xij in document
i for topic k, and P (zij = k|i ) is the probability that k th topic was sampled for the word
token xij . We refer the multinomial distributions bk and i as topic-word and documenttopic distributions, respectively. Figure 3 shows the resultant graphical model in plate
notation for N documents, K topics and Mi tokens in each document i. Note that,  and 
are the standard Dirichlet priors on i and bk , respectively. Variational EM can be used to
estimate  and b (Blei et al., 2003). One can also use Gibbs sampling to directly estimate
the posterior distribution over z, i.e., P (zij = k|xij ); namely, the topic assignments for word
tokens (Steyvers & Griffiths, 2007).
i




zi,j
bk
xi,j

K
Mi
N

Figure 3: Graphical model for LDA in plate notation.
This framework can be directly applied to an asynchronous conversation by considering
each comment as a document. By assuming the words in a sentence occur independently
we can estimate the topic assignments for each sentence s as follows:
P (zm = k|s) =

Y

P (zm = k|xm )

(3)

xm s

Finally, the topic for s can be assigned by:
k  = argmaxk P (zm = k|s)

(4)

3.1.2 Limitations of Existing Models
The main limitation of the two models discussed above is that they make the bag-of-words
(BOW) assumption ignoring facts that are specific to a multi-party, asynchronous conversation. LCSeg considers only term frequency and how closely these terms occur in the
531

fiJoty, Carenini, & Ng

temporal order of the sentences. If topics are interleaved and do not change sequentially in
the temporal order, as is often the case in an asynchronous conversation, then LCSeg would
fail to find the topical segments correctly.
On the other hand, the only information relevant to LDA is term frequency. Several extensions of LDA over the BOW approach have been proposed. For example, Wallach (2006)
extends the model beyond BOW by considering n-gram sequences. Griffiths, Steyvers, Blei,
and Tenenbaum (2005) present an extension that is sensitive to word-order and automatically learns the syntactic as well as semantic factors that guide word choice. Boyd-Graber
and Blei (2008) describe another extension to consider the syntax of the sentences.
We argue that these models are still inadequate for finding topical segments correctly
in asynchronous conversations especially when topics are closely related and their distributional variations are subtle (e.g., Game contents or size and Game design). To better
identify the topics one needs to consider the features specific to asynchronous conversations
(e.g., conversation structure, speaker, recipient). In the following, we propose our novel
unsupervised and supervised topic segmentation models that incorporate these features.
3.1.3 Proposed Unsupervised Models
One of the most important indicators for topic segmentation in asynchronous conversation
is its conversation structure. As can be seen in the examples (Figures 1 and 2), participants
often reply to a post and/or use quotations to talk about the same topic. Notice also
that the use of quotations can express a conversational structure that is at a finer level of
granularity than the one revealed by reply-to relations. In our corpora, we found an average
quotation usage of 9.85 per blog conversation and 6.44 per email conversation. Therefore,
we need to leverage this key information to get the best out of our models. Specifically, we
need to capture the conversation structure at the quotation (i.e., text fragment) level, and
to incorporate this structure into our segmentation models in a principled way.
In the following, we first describe how we can capture the conversation structure at the
fragment level. Then we show how the unsupervised models LCSeg and LDA can be extended to take this conversation structure into account, generating two novel unsupervised
models for topic segmentation in asynchronous conversation.
Extracting Finer-level Conversation Structure
Since consecutive turns in asynchronous conversations can be far apart in time, when participants reply to a post or comment, a quoted version of the original message is often
included (specially in email) by default in the draft reply in order to preserve context.
Furthermore, people tend to break down the quoted message so that different questions,
requests or claims can be dealt with separately. As a result, each message, unless it is at the
beginning, will contain a mix of quoted and novel paragraphs (or fragments) that may well
reflect a reply-to relationship between paragraphs that is at a finer level of granularity than
the one explicitly recorded between comments. We proposed a novel approach to capture
this finer level conversation structure in the form of a graph called Fragment Quotation
Graph (FQG) (Carenini et al., 2007). In the following, we demonstrate how to build
a FQG for the sample blog conversation shown in Figure 2. Figure 4(a) shows the same
blog conversation, but for the sake of illustration, instead of showing the real content, we
532

fiTopic Segmentation and Labeling in Asynchronous Conversations

abbreviate it as a sequence of labels (e.g., a, b), each label corresponding to a text fragment
(see the Fragment column in Figure 2). Building a FQG is a two-step process.

Figure 4: (a) The main Article and the C omments with the fragments for the example in
Figure 2. Arrows indicate reply-to relations. (b) The corresponding FQG.
 Node creation: Initially, by processing the whole conversation, we identify the new
and quoted fragments of different depth levels. The depth level of a quoted fragment
is determined by the number of quotation marks (e.g., >, >>, >>>). For instance,
comment C1 contains a new fragment c and a quoted fragment b of depth level 1. C6
contains two new fragments k and l, and two quoted fragments i and j of depth level
1, and so on. Then in the second step, we compare the fragments with each other and
based on their lexical overlap we find the distinct fragments. If necessary, we split
the fragments in this step. For example, ef in C3 is divided into e and f distinct
fragments when compared with the fragments of C4 . This process gives 12 distinct
fragments which constitute the nodes of the FQG shown in Figure 4(b).
 Edge creation: We create edges to represent likely replying relationship between fragments assuming that any new fragment is a potential reply to its neighboring quotations of depth level 1. For example, for the fragments of C6 in Figure 4(a), we create
two edges from k (i.e., (k,i),(k,j)) and one edge from l (i.e., (l,j)) in Figure 4(b). If
a comment does not contain any quotation, then its fragments are linked to the new
fragments of the comment to which it replies, capturing the original reply-to relation.
Note that the FQG is only an approximation of the reply relations between fragments.
In some cases, proximity may not indicate any connection and in other cases a connection can exist between fragments that are never adjacent in any comment. Furthermore,
this process could lead to less accurate conversational structure when quotation marks (or
cues) are not present. Nonetheless, we previously showed that considering the FQG can
be beneficial in dialog act modeling (Joty et al., 2011) and email summarization (Carenini
et al., 2008). In this study, we show that topic segmentation (this Section) and labeling
533

fiJoty, Carenini, & Ng

(Section 3.2) models can also benefit significantly from this fine conversational structure of
asynchronous conversation. Minimizing the noise in FQGs is left as future work.
LCSeg with FQG (LCSeg+FQG)
If we examine the FQG carefully, the paths (considering the fragments of the first comment
as root nodes) can be interpreted as subconversations, and topic shifts are likely to occur
along the pathway as we walk down a path. We incorporate FQG into LCSeg in three steps.
 Path extraction: First, we extract all the paths of a FQG. For example, for the FQG
in Figure 4(b), we extract the paths < a, j, l >, < b, c, e, g >, < b, c, d >, and so on.
 LCSeg application: We then run LCSeg algorithm on each of the extracted paths
separately and collect the segmentations. For example, when applied LCSeg to <
b, c, e, g > and < b, c, d > paths separately, we may get the following segmentations< b, c | e, g > and < b, c | d >, where | denotes the segment boundary.4 Notice that a
fragment can be in multiple paths (e.g., b, c) which will eventually cause its sentences
to be in multiple segments. So, in the final step, we need a consolidation method.
 Consolidation: Our intuition is that sentences in a consolidated segment should appear
together in a segment more often when LCSeg is applied in step 2, and if they do not
appear together in any segment, they should at least be similar. To achieve this,
we construct a weighted undirected graph G(V, E), where the nodes V represent the
sentences and the edge weights w(x, y) represent the number of segments in which
sentences x and y appear together; if x and y do not appear together in any segment,
then their cosine similarity is used as edge weights. More formally,

(
n,
w(x, y) =
cos sim(x, y),

if x and y appear together in n segments and n > 0
if n = 0

We measure the cosine similarity between sentences x and y as follows:
P

cos sim(x, y) = qP

tfw,x .tfw,y
qP
2 .
2
tf
x
,x
xi x
yi y tfyi ,y
i
wx,y

(5)

where tfa,s denotes the term frequency of term a in sentence s. The cosine similarity
(0  cos sim(x, y)  1) provides informative edge weights for the sentence pairs that
are not directly connected by LCSeg segmentation decisions.5 Now, the consolidation
problem can be formulated as a k-way-mincut graph partitioning problem with the
normalized cut (Ncut) criterion (Shi & Malik, 2000):
4. For convenience, we are showing the segmentations at the fragment level, but the segmentations are
actually at the sentence level.
5. In our earlier work (Joty, Carenini, Murray, & Ng, 2010), we did not consider the cosine similarity when
two sentences do not appear together in any of the segments. However, later we found out that including
the cosine similarity offers more than 2% absolute gain in segmentation performance.

534

fiTopic Segmentation and Labeling in Asynchronous Conversations

N cutk (V ) =

cut(A1 , V  A1 ) cut(A2 , V  A2 )
cut(Ak , V  Ak )
+
+  +
assoc(A1 , V )
assoc(A2 , V )
assoc(Ak , V )

(6)

where A1 , A2    Ak form a partition (i.e., disjoint sets of nodes) of the graph, and
V  Ak is the set difference between V (i.e., set of all nodes) and Ak . The cut(A, B)
measures the total edge weight from the nodes in set A to the nodes in set B, and
assoc(A, V ) measures the total edge weight from the nodes in set A to all nodes in
the graph. More formally:

cut(A, B) =

X

w(u, v)

(7)

w(u, t)

(8)

uA,vB

assoc(A, V ) =

X
uA,tV

Note that the partitioning problem can be solved using any correlation clustering
method (e.g., Bansal, Blum, & Chawla, 2002). Previous work on graph-based topic
segmentation (Malioutov & Barzilay, 2006) has shown that the Ncut criterion is more
appropriate than just the cut criterion, which accounts only for total edge weight
connecting A and B, and therefore, favors cutting small sets of isolated nodes in the
graph. However, solving Ncut is NP-complete. Hence, we approximate the solution
following the method proposed by Shi and Malik, (2000), which is time efficient and
has been successfully applied to image segmentation in computer vision.
Notice that this approach makes a difference only if the FQG of the conversation contains more than one path. In fact, in our corpora we found an average number of paths of
7.12 and 16.43 per email and blog conversations, respectively.
LDA with FQG (LDA+FQG)
A key advantage of probabilistic Bayesian models, such as LDA, is that they allow us
to incorporate multiple knowledge sources in a coherent way in the form of priors (or
regularizer). To incorporate FQG into LDA, we propose to regularize LDA so that two
sentences in the same or adjacent fragments are likely to appear in the same topical cluster.
The first step towards this aim is to regularize the topic-word distributions (i.e., b in Figure
3) with a word network such that two connected words get similar topic distributions.
For now, assume that we are given a word network as an undirected graph G(V, E), with
nodes V representing the words and the edges (u, v)E representing the links between words
u and v. We want to regularize the topic-word distributions of LDA such that two connected
words u and v in the word network have similar topic distributions (i.e., bk (u)  bk (v) for
k = 1 . . . K). The standard conjugate Dirichlet prior Dir(bk |), however does not allow us
to do that, because here all words share a common variance parameter, and are mutually
independent except normalization constraint (Minka, 1999). Recently, Andrzejewski, Zhu,
and Craven (2009) describe a method to encode must-links and cannot-links between
535

fiJoty, Carenini, & Ng

words using a Dirichlet Forest prior. Our goal is just to encode the must-links. Therefore,
we reimplemented their model with its capability of encoding just the (must-)links.
Must-links between words such as (a, b), (b, c), or (x, y) in Figure 5(a) can be encoded
into LDA using a Dirichlet Tree (DT) prior. Like the traditional Dirichlet, DT prior
is also a conjugate to the multinomial, but under a different parameterization. Instead of
representing a multinomial sample as the outcome of a K-sided die, in the tree representation
(e.g., Figure 5(b)), a sample (i.e., a leaf in the tree) is represented as the outcome of a finite
stochastic process. The probability of a leaf (i.e., a word in our case) is the product of branch
probabilities leading to that leaf. A DT prior is the distribution over leaf probabilities.
Let  n be the edge weight leading into node n, C(n) be the children of node n, L be the
leaves of the tree, I be the internal nodes, and L(n) be the leaves in the subtree under node
n. We generate a sample bk from DT() by drawing a multinomial at each internal node
i  I from Dir( C(i) ) (i.e., the edge weights from node i to its children). The probability
density function of DT(bk |) is given by:
 
(i) 
!
Y l 1 Y X

DT (bk |) 
blk
bkj 
(9)
 

iI

lL

jL(i)

P
where (i) =  i  jC(i)  j , the difference between the in-degree and out-degree of an
internal node i. Notice when (i) = 0 for all i I, the DT reduces to the standard Dirichlet.
Suppose we are given the word network as shown in Figure 5(a). The network can
be decomposed into a collection of chains (e.g., (a, b, c), (p), and (x, y)). For each chain
containing multiple elements (e.g., (a, b, c), (x, y)), there is a subtree in the DT (Figure 5(b)),
with one internal node (blank in Figure) and the words of the chain as its leaves. The weight
from the internal node to each of its leaves is , where  is the regularization strength
and  is the parameter of the standard symmetric Dirichlet prior on bk . The root node
of the DT then connects to the internal nodes with |L(i)| weight. The leaves (words) for
the single element chains (e.g, (p)) are then connected to the root of the DT directly with
weight . Notice that when  = 1, (i) = 0, and it reduces to the standard LDA (i.e., no
regularization). By tuning  we control the strength of the regularization.
a
3

x
b



p

p




y
c

2

a

b



c





x

y

(b)

(a)

Figure 5: (a) Sample word network, (b) A Dirichlet Tree (DT) built from such word network.
At this point what is left to be explained is how we construct the word network. To
regularize LDA with a FQG, we construct the word network where a word is linked to the
words in the same or adjacent fragments in the FQG. Specifically, if word wi f ragx and
536

fiTopic Segmentation and Labeling in Asynchronous Conversations

word wj f ragy and wi 6=wj , we create a link (wi , wj ) if x = y or (x, y)Ef qg , where Ef qg is
the set of edges in the FQG. This implicitly compels two sentences in the same or adjacent
fragments to have similar topic distributions, and appear in the same topical segment.
3.1.4 Proposed Supervised Model
Although the unsupervised models discussed in the previous section have the key advantage
of not requiring any labeled data, they can be limited in their ability to learn domain-specific
knowledge from a possible large and diverse set of features (Eisenstein & Barzilay, 2008).
Beside discourse cohesion, which captures the changes in content, there are other important domain-specific distinctive features which signal topic change. For example, discourse
markers (or cue phrases) (e.g., okay, anyway, now, so) and prosodic cues (e.g., longer pause)
directly provide clues about topic change, and have been shown to be useful features for
topic segmentation in monolog and synchronous dialog (Passonneau & Litman, 1997; Galley
et al., 2003). We hypothesize that asynchronous conversations can also feature their own
distinctive characteristics for topic shifts. For example, features like sender and recipient
are arguably useful for segmenting asynchronous conversations, as different participants can
be more or less active during the discussion of different topics. Therefore, as a next step to
build an even more accurate topic segmentation model for asynchronous conversations, we
propose to combine different sources of possibly useful information in a principled way.
The supervised framework serves as a viable option to combine a large number of features and optimize their relative weights for decision making, but relies on labeled data
for training. The amount of labeled data required to achieve an acceptable performance is
always an important factor to consider for choosing supervised vs. unsupervised. In this
work, we propose a supervised segmentation model that outperforms all the unsupervised
models, even when it is trained on a small number of labelled conversations.
Our supervised model is built on the graph-theoretic framework which has been used
in many NLP tasks, including coreference resolution (Soon, Ng, & Lim, 2001) and chat
disentanglement (Elsner & Charniak, 2010). This method works in two steps.
 Classification: A binary classifier which is trained on a labeled dataset, marks each
pair of sentences of a given conversation as same or different topics.
 Graph partitioning: A weighted undirected graph G = (V, E) is formed, where the
nodes V represent the sentences in the conversation and the edge-weights w(x, y)
denote the probability (given by the classifier) of the two sentences x and y to appear
in the same topic. Then an optimal partition is extracted.
Sentence pair classification
The classifiers accuracy in deciding whether a pair of sentences x and y is in the same
or different topics is crucial for the models performance. Note that since each sentence
pair of a conversation defines a data point, a conversation containing n sentences gives
1+2+. . .+(n-1)= n(n1)
= O(n2 ) training examples. Therefore, a training dataset containing
2
Pm ni (ni 1)
m conversations gives i=1
training examples, where ni is the number of sentences
2
th
in the i conversation. This quadratic expansion of training examples enables the classifier
to achieve its best classification accuracy with only a few labeled conversations.
537

fiJoty, Carenini, & Ng

By pairing up the sentences of each email conversation in our email corpus, we got a
total of 14, 528 data points of which 58.8% are in the same class (i.e., same is the most
likely in email), and by pairing up the sentences of each blog conversation in our blog
corpus, we got a total of 572, 772 data points of which 86.3% are in the different class (i.e.,
different is the most likely in blog).6 To select the best classifier, we experimented with
a variety of classifiers with the full feature set (Table 2). Table 1 shows the performance
of the classifiers averaged over a leave-one-out procedure, i.e., for a corpus containing m
conversations, train on m  1 conversations and test on the rest.
Classifier
KNN
LR
LR
RMLR (rbf)
SVM (lin)
SVM (rbf)
Majority class

Type

Regularizer

non-parametric
parametric
parametric
non-parametric
parametric
non-parametric
-

l2
l1
l2
-

Accuracy (Blog)
Train
Test
62.7%
61.4 %
90.8% 91.9%
86.8%
87.6%
91.7%
82.0%
76.6%
78.7 %
80.5%
77.9%
86.3% (different)

Accuracy (Email)
Train
Test
54.6%
55.2%
71.7%
72.5%
69.9%
67.7%
91.1%
62.1%
68.3%
69.6%
75.9%
67.7%
58.8% (same)

Table 1: Performance of the classifiers using the full feature set (Table 2). For each training
set, regularizer strength  (or C in SVMs) was learned by 10-fold cross validation.

K-Nearest Neighbor (KNN) performs very poorly. Logistic Regression (LR) with l2
regularizer delivers the highest accuracy on both datasets. Support Vector Machines (SVMs)
(Cortes & Vapnik, 1995) with linear and rbf kernels perform reasonably well, but not as
well as LR. The Ridged Multinomial Logistic Regression (RMLR) (Krishnapuram et al.
2005), a kernelized LR, extremely overfits the data. We opted for the LR with l2 regularizer
because it not only delivers the best performance in term of accuracy, but it is also very
efficient. The limited memory BFGS (L-BFGS) fitting algorithm used in LR is efficient in
terms of both time (quadratic convergence rate; fastest among the listed models) and space
(O(mD), where m is the memory parameter of L-BFGS and D is the number of features).
Table 2 summarizes the full feature set and the mean test set accuracy (using leave-oneout procedure) achieved with different types of features in our LR classifier.
Lexical features encode similarity between two sentences x and y based on their raw
content. Term frequency-based similarity is a widely used feature in previous work, e.g.,
TextTiling (Hearst, 1997). We compute this feature by considering two analysis windows,
each of fixed size k. Let X be the window including sentence x and the preceding k  1
sentences, and Y be the window including sentence y and the following k  1 sentences. We
measure the cosine similarity between the two windows by representing them as vectors of
TF.IDF (Salton & McGill, 1986) values of the words. Another important domain specific
feature that proved to be useful in previous research (e.g., Galley et al., 2003) is cue words
(or discourse markers) that sign the presence of a topic boundary (e.g., coming up, joining
us in news). Since our work concerns conversations (not monologs), we adopt the cue word
6. See Section 4 for a detailed description of our corpora. The class labels are produced by taking the
maximum vote of the three annotators.

538

fiTopic Segmentation and Labeling in Asynchronous Conversations

Lexical

Accuracy: 86.8 Precision: 62.4 Recall: 4.6 (Blog)
Accuracy: 59.6 Precision: 59.7 Recall: 99.8 (Email)

T F IDF1
T F IDF2
Cue Words
QA

TF.IDF-based similarity between x and y with window size k=1.
TF.IDF-based similarity between x and y with window size k=2.
Either x or y contains a cue word.
x asks a question explicitly using ? and y answers it using any
of (yes, yeah, okay, ok, no, nope).
Either x or y has a greeting word (hi, hello, thanks, thx, tnx, thank.)

Greet
Conversation

Accuracy: 88.2 Precision: 81.6 Recall: 20.5 (Blog)
Accuracy: 65.3 Precision: 66.7 Recall: 85.1 (Email)

Gap
Speaker
F QG1

The gap between y and x in number of sentence(s).
x and y have the same sender (yes or no).
Distance between x and y in FQG in terms of fragment id.
(i.e., |f rag id(y)  f rag id(x)|).
Distance between x and y in FQG in terms of number of edges.
Distance between x and y in FQG in number of edges but this
time considering it as an undirected graph.
whether x and y are in the same comment or one is a reply to
the other.
x mentions ys speaker or vice versa.

F QG2
F QG3
Same/Reply
Name
Topic

Accuracy: 89.3 Precision: 86.4 Recall: 17.3 (Blog)
Accuracy: 67.5 Precision: 68.9 Recall: 76.8 (Email)

LSA1
LSA2
LDA
LDA+FQG
LCSeg
LCSeg+FQG
LexCoh

LSA-based similarity between x and y with window size k=1.
LSA-based similarity between x and y with window size k=2.
LDA segmentation decision on x and y (same or different).
LDA+FQG segmentation decision on x and y (same or different).
LCSeg segmentation decision on x and y (same or different).
LCSeg+FQG segmentation decision on x and y (same or different).
Lexical cohesion between x and y.

Combined

Accuracy: 91.9 Precision: 78.8 Recall: 25.8 (Blog)
Accuracy: 72.5 Precision: 70.4 Recall: 81.5 (Email)

Table 2: Features with average performance on testsets (using leave-one-out).

539

fiJoty, Carenini, & Ng

list derived automatically from a meeting corpus by Galley et al. (2003). If y answers or
greets x then it is likely that they are in the same topic. Therefore, we use the Question
Answer (QA) pairs and greeting words as two other lexical features.
Conversational features capture conversational properties of an asynchronous conversation. Time gap and speaker are commonly used features for segmenting synchronous
conversations (e.g., Galley et al. 2003). We encode similar information in asynchronous media by counting the number of sentences between x and y (in their temporal order) as the
gap, and their senders as the speakers. The strongest baseline Speaker (see Section 5.1)
also proves its effectiveness in asynchronous domains. The results in Section 5.1 also suggest
that fine conversational structure in the form of FQG can be beneficial when it is incorporated into the unsupervised segmentation models. We encode this valuable information
into our supervised segmentation model by computing three distance features on the FQG:
F QG1 , F QG2 , F QG3 . State-of-the-art email and blog systems use reply-to relation to
group comments into threads. If ys comment is same as or reply to xs comment, then
it is likely that the two sentences talk about the same topic. Participants sometimes mention each others name in multi-party conversations to make disentanglement easier (Elsner
& Charniak, 2010). We also use this as a feature in our supervised segmentation model.
Topic features are complex and encode topic information from existing segmentation
models. Choi et al. (2001) used Latent Semantic Analysis (LSA) to measure the similarity
between two sentences and showed that the LSA-based similarity yields better results than
the direct TF.IDF-based similarity since it surmounts the problems of synonymy (e.g., car,
auto) and polysemy (e.g., money bank, river bank). To compute LSA, we first construct
a word-document matrix W for a conversation, where Wi,j = the frequency of word i in
comment j  the IDF score of word i. We perform truncated Singular Value Decomposition
(SVD) of W : W  Uk k VkT , and represent each word i as a k dimensional7 vector ki . Each
sentence is then represented by the weighted
sum of its word vectors. Formally, the LSA
P
k
s
s
representation for sentence s is s =
tf
is i .i , where tfi = the term frequency of
word i in sentence s. Then just like the TF.IDF-based similarity, we compute the LSAbased similarity between sentences x and y, but this time by representing the corresponding
windows (i.e., X and Y ) as LSA vectors.
The segmentation decisions of LDA, LDA+FQG, LCSeg and LCSeg+FQG models
described in the previous section are also encoded as topic features.8 As described in Section 3.1.1, LCSeg computes a lexical cohesion (LexCoh) function between two consecutive
windows based on the scores of the lexical chains. Galley et al. (2003) shows a significant
improvement when this function is used as a feature in the supervised (sequential) topic
segmentation model for meetings. However, since our problem of topic segmentation is not
sequential, we want to compute this function for any two given windows X and Y (not
necessary consecutive). To do that, we first extract the lexical chains with their scores and
spans (i.e., beginning and end sentence numbers) for the conversation. The lexical cohesion
function is then computed with the method described in Equation 1.
7. In our study, k was empirically set to 14 number of comments based on a held-out development set.
8. Our earlier work (Joty, Carenini, Murray, & Ng, 2011) did not include the segmentation decisions of
LDA+FQG and LCSeg+FQG models as features. However, including these features improves both
classification accuracy and segmentation accuracy.

540

fiTopic Segmentation and Labeling in Asynchronous Conversations

0.35
0.3
0.25
0.2
0.15
0.1

Email
Blog

LCSeg

LCSeg+FQG

LexCoh

LDA+FQG

LDA

LSA2

LSA1

Name

Same/Reply

F QG2

F QG3

F QG1

Gap

Speaker

QA

Greet

Cue

T F.IDF2

T F.IDF1

5  102
0

Figure 6: Relative importance of the features averaged over leave-one-out.

We describe our classifiers performance in terms of raw accuracy (correct decisions/total),
precision and recall of the same class for different types of features averaged over a leaveone-out procedure (Table 2). Among the feature types, topic features yield the highest
accuracy and same-class precision in both corpora (p < 0.01).9 Conversational features also
have proved to be important and achieve higher accuracy than lexical features (p < 0.01).
Lexical features have poor accuracy, only slightly higher than the majority baseline that
always picks the most likely class. However, when we combine all the features, we get the
best performance (p < 0.005). These results demonstrate the importance of topical and
conversational features beyond the lexical features used only by the existing segmentation
models. When we compare the performance on the two corpora, we notice that while in
blog the accuracy and the same-class precision are higher than in email, the same-class
recall is much lower. Although this is reasonable given the class distributions in the two
corpora (i.e., 13.7% and 58.8% examples are in the same-class in blog and email, respectively), surprisingly, when we tried to deal with this problem by applying the bagging
technique (Breiman, 1996), the performance does not improve significantly. Note that some
of the classification errors occurred in the sentence-pair classification phase are recovered
in the graph partitioning step (see below). The reason is that the incorrect decisions will
be outvoted by the nearby sentences that are clustered correctly.
We further analyze the contribution of individual features. Figure 6 shows the relative
importance of the features based on the absolute values of their coefficients in our LR
classifier. The segmentation decision of LCSeg+FQG is the most important feature in both
domains. The Same/Reply is also an effective feature, especially in blog. In blog, the
Speaker feature also plays an important role. The F QG2 (distance in number of edges in
the directed FQG) is also effective in both domains, especially in email. The other two
features on FQG (F QG1 , F QG3 ) are also very relevant in email.
Finally, in order to determine how many annotated conversations we need to achieve
the best segmentation performance, Figure 7 shows the classification error rate (incorrect
decisions/total), tested on 5 randomly selected conversations and trained on an increasing
9. All tests of statistical significance were performed using paired t-test.

541

fiJoty, Carenini, & Ng

Classification error rate

number of randomly added conversations. Our classifier appears to achieve its best performance with a small number of labeled conversations. For blog, the error rate flattens with
only 8 conversations, while for email, this happens with about 15. This is not surprising
since blog conversations are much longer (an average of 220.55 sentences) than email conversations (an average of 26.3 sentences), generating a similar number of training examples
with only a few conversations (recall, for n sentences we get O(n2 ) training examples).

50
45
40
35
30
25
20
15
10
5

Email
Blog

2

4

6

8

10

12

14

16

18

20

22

24

26

28

30

32

34

Number of training conversations

Figure 7: Error rate vs. number of training conversations.
Graph partitioning
Given a weighted undirected graph G = (V, E), where the nodes V represent the sentences
and the edge-weights w(x, y) denote the probability (given by our classifier) of the two
sentences x and y to appear in the same topic, we again formulate the segmentation task as
a k-way-mincut graph partitioning problem with the intuition that sentences in a segment
should discuss the same topic, while sentences in different segments should discuss different
topics. We optimize the normalized cut criterion (i.e., equation 6) to extract an optimal
partition as was done before for consolidating various segments in LCSeg+FQG.
3.2 Topic Labeling Models
Now that we have methods to automatically identify the topical segments in an asynchronous conversation, the next step in the pipeline is to generate one or more informative
descriptions or topic labels for each segment to facilitate interpretations of the topics. We
are the first to address this problem in asynchronous conversation.
Ideally, a topic label should be meaningful, semantically similar to the underlying topic,
general and discriminative (when there are multiple topics) (Mei et al., 2007). Traditionally,
the top k words in a multinomial topic model like LDA are used to describe a topic. However,
as pointed out by Mei et al., at the word-level, topic labels may become too generic and
impose cognitive difficulties on a user to interpret the meaning of the topic by associating
the words together. For example, in Figure 2, without reading the text, from the words
{release, free, reaction, Daggerfall}, it may be very difficult for a user to understand that
the topic is about Daggerfalls free release and peoples reaction to it. On the other hand,
if the labels are expressed at the sentence-level, they may become too specific to cover the
542

fiTopic Segmentation and Labeling in Asynchronous Conversations

whole theme of the topic (Mei et al., 2007). Based on these observations, recent studies
(e.g., Mei et al., 2007; Lau et al., 2011) advocate for phrase-level topic labels, which
are also consistent with the monolog corpora built as a part of the Topic Detection and
Tracking (TDT) project10 . Note that we also observe a preference for phrase-level labels
within our own asynchronous conversational corpora in which human annotators without
specific instructions spontaneously generated topic labels at the phrase-level. Considering
all this, we treat phrase-level as our target level of granularity for a topic label.
Our problem is no different from the problem of keyphrase indexing (Medelyan,
2009) where the task is to find a set of keyphrases either from the given text or from a
controlled vocabulary (i.e., domain-specific terminologies) to describe the topics covered
in the text. In our setting, we do not have such a controlled vocabulary. Furthermore,
exploiting generic knowledge bases like Wikipedia as a source of devising such a controlled
vocabulary (Medelyan, 2009) is not a viable option in our case since the topics are very specific to a particular discussion (e.g., Free release of Daggerfall and reaction, Game contents
or size in Figure 2). In fact, none of the human-authored labels in our developement set
appears verbatim in Wikipedia. We propose to generate topic labels using a keyphrase
extraction approach that identifies the most representative phrase(s) in the given text.
We adapt a graph-based unsupervised ranking framework, which is domain independent,
and without relying on any labeled data achieves state-of-the-art performance on keyphrase
extraction (Mihalcea & Radev, 2011). Figure 8 shows our topic labeling framework. Given
a (topically) segmented conversation, our system generates k keyphrases to describe each
topic in the conversation. Below we discuss the different components of the system.
Word Ranking

words

Segmented
input conversation

Segment-level
Ranking

top M words
Phrase
Generation

Preprocessor
Conversation-level
Ranking

top M words
conversationlevel phrases

Segment-level
phrases

k output phrases

Redundancy
Checking

relevant
phrases

Conversation-level
Phrase Reranking

Figure 8: Topic labeling framework for asynchronous conversation.

3.2.1 Preprocessing
In the preprocessing step, we tokenize the text and apply a syntactic filter to select the
words of a certain part-of-speech (POS). We use a state-of-the-art tagger11 to tokenize the
10. http://projects.ldc.upenn.edu/TDT/
11. Available at http://cogcomp.cs.illinois.edu/page/software

543

fiJoty, Carenini, & Ng

text and annotate the tokens with their POS tags. We experimented with five different
syntactic filters. They select (i) nouns, (ii) nouns and adjectives, (iii) nouns, adjectives
and verbs, (iv) nouns, adjectives, verbs and adverbs, and (v) all words, respectively. The
filters also exclude the stopwords. The second filter, that selects only nouns and adjectives,
achieves the best performance on our development set, which is also consistent with the
finding of Mihalcea and Tarau (2004). Therefore, this syntactic filter is used in our system.
3.2.2 Word Ranking
The words selected in the preprocessing step correspond to the nodes in the graph. A direct
application of the ranking method described by Mihalcea and Tarau (2004) would define the
edges based on the co-occurrence relation between the respective words, and then apply
the PageRank (Page et al., 1999) algorithm to rank the nodes. We argue that co-occurrence
relations may be insufficient for finding topic labels in an asynchronous conversation. To
better identify the labels one needs to consider aspects that are specific to asynchronous
conversation. In particular, we propose to exploit two different forms of conversation specific
information into our graph-based ranking model: (1) informative clues from the leading
sentences of a topical segment, and (2) the fine-grained conversational structure (i.e., the
Fragment Quotation Graph (FQG)) of an asynchronous conversation. In the following,
we describe these two novel extensions in turn.
Incorporating Information from the Leading Sentences
In general, the leading sentences of a topic segment carry informative clues for the topic
labels, since this is where the speakers will most likely try to signal a topic shift and
introduce the new topic. Our key observation is that this is especially true for asynchronous
conversations, in which topics are interleaved and less structured. For example, in Figure 2,
notice that in almost every case, the leading sentences of the topical segments covers the
information conveyed by the labels. This property is further confirmed in Figure 9, which
shows the percentage of non-stopwords in the human-authored labels that appear in leading
sentences of the segments in our development set. The first sentence covers about 29% and
38% of the words in the gold labels in the blog and email corpora, respectively. The first
two sentences cover around 35% and 45% of the words in the gold labels in blog and email,
respectively. When we consider the first three sentences, the coverage raises up to 39% and
49% for blog and email, respectively. The increment is less as we add more sentences.
To leverage this useful information in our ranking model, we propose the following
biased random walk model, where P (w|Uk ), the score of a word w given a set of leading
sentences Uk in topic segment k, is expressed as a convex combination of its relevance to the
leading sentences Uk (i.e., (w|Uk )) and its relatedness with other words in the segment:
P (w|Uk ) =  P

X
(w|Uk )
e(y, w)
P
+ (1  )
P (y|Uk )
zCk (z|Uk )
zCk e(y, z)

(10)

yCk

where the value of  (0    1), which we call the bias, is a trade-off between the two
components and should be set empirically. For higher values of , we give more weight to
the words relevance to the leading sentences compared to its relatedness with other words
544

fiTopic Segmentation and Labeling in Asynchronous Conversations

55
50

Email
Blog

44.86

45
40

25

42.01

52.94
44.57

38.98

38.08
34.73

35
30

50.65

48.95

28.74
One

Two

Four

Three

Five

Figure 9: Percentage of words in the human-authored labels appearing in leading sentences
of the topical segments.
in the segment. Here, Ck is the set of words in segment k, which represents the nodes in the
graph. The denominators in both components are for normalization. We define (w|Uk ) as:
(w|Uk ) = log(tfwUk + 1).log(tfwk + 1)

(11)

where tfwUk and tfwk are the number of times word w appears in Uk and segment k, respectively. A similar model has proven to be successful in measuring the relevance of a sentence
to a query in query-based sentence retrieval (Allan, Wade, & Bolivar, 2003).
Recall that when there are multiple topics in a conversation, a requirement for the topic
labels is that labels of different topics should be discriminative (or distinguishable) (Mei
et al., 2007). This implicitly indicates that a high scoring word in one segment should not
have high scores in other segments of the conversation. Keeping this criterion in mind, we
define the (undirected) edge weights e(y, w) in equation 10 as follows:
k
e(y, w) = tfw,y
 log

K
k0
0.5 + tfw,y

(12)

k
where K denotes the number of topics (or topic segments) in the conversation, and tfw,y
0
k
and tfw,y are the number of times words w and y co-occur in a window of size s in segment
k and in segments except k in the conversation, respectively. Notice that this measure is
similar in spirit to the TF.IDF metric (Salton & McGill, 1986), but it is at the co-occurrence
level. The co-occurrence relationship between words captures syntactic dependencies and
lexical cohesion in a text, and is also used by Mihalcea and Tarau (2004).12
Equation 10 above can be written in matrix notation as:

 = [Q + (1  )R]T  = AT ,
where Q and R are square matrices such that Qi,j =

(13)

P (j|Uk )
zC (z|Uk )

for all i, and Ri,j =

k

P e(i,j)
,
jC e(i,j)

respectively. Notice that A is a stochastic matrix (i.e., all rows add up to 1),

k

therefore, it can be treated as the transition matrix of a Markov chain. If we assume each
12. Mihalcea and Tarau (2004) use an unweighted graph for key phrase extraction. However, in our experiments, we get better results with a weighted graph.

545

fiJoty, Carenini, & Ng

word is a state in a Markov chain, then Ai,j specifies the transition probability from state
i to state j in the corresponding Markov chain. Another interpretation of A can be given
by a biased random walk on the graph. Imagine performing a random walk on the graph,
where at every time step, with probability , a transition is made to the words that are
relevant to the leading sentences and with probability 1  , a transition is made to the
related words in the segment. Every transition is weighted according to the corresponding
elements of Q and R. The vector  we are looking for is the stationary distribution of this
Markov chain and is also the (normalized) eigenvector of A for the eigenvalue 1. A Markov
chain will have a unique stationary distribution if it is ergodic (Seneta, 1981). We can
ensure the Markov chain to have this property by reserving a small probability for jumping
to any other state from the current state (Page et al., 1999).13 For larger matrices,  can
be efficiently computed by an iterative method called power method.
Incorporating Conversational Structure
In Section 3.1, we described how the fine conversation structure in the form of a Fragment
Quotation Graph (FQG) can be effectively exploited in our topic segmentation models. We
hypothesize that our topic labeling model can also benefit from the FQG. In our previous
work on email summarization (Carenini et al., 2008), we applied PageRank to the FQG to
measure the importance of a sentence and demonstrated the benefits of using a FQG. This
finding implies that an important node in the FQG is likely to cover an important aspect
of the topics discussed in the conversation. Our intuition is that, to be in the topic label,
a keyword should not only co-occur with other keywords, but it should also come from an
important fragment in the FQG. We believe there is a mutually reinforcing relationship
between the FQG and the Word Co-occurrence Graph (WCG) that should be reflected in
the rankings. Our proposal is to implement this idea as a process of co-ranking (Zhou et
al., 2007) in a heterogeneous graph, where three random walks are combined together.
Let G = (V, E) = (VF  VW , EF  EW  EF W ) be the heterogeneous graph of fragments
and words. As shown in Figure 10, it contains three sub-graphs. First, GF = (VF , EF ) is the
unweighted directed FQG, with VF denoting the set of fragments and EF denoting the set
of directed links between fragments. Second, GW = (VW , EW ) is the weighted undirected
WCG, where VW is the set of words in the segment and EW is the set of edge-weights as
defined in equation 12. Third, GF W = (VF W , EF W ) is the weighted bipartite graph that
ties GF and GW together representing the occurrence relations between the words and the
fragments. Here, VF W = VF  VW , and weighted undirected edges in EF W connect each
fragment vf  VF to each word vw  VW , with the weight representing the number of times
word vw occurs in fragment vf .
The co-ranking framework combines three random walks, one on GF , one on GW and
one on GF W . Let F and W denote the transition matrices for the (intra-class) random walks
in GF and GW , respectively, and f and w denote their respective stationary distributions.
Since, GF W is a bipartite graph, the (inter-class) random walk on GF W can be described
by two transition matrices, F W|VF ||VW | and W F|VW ||VF | . One intra-class step changes the
probability distribution from (f, 0) to (F T f, 0) or from (0, w) to (0, W T w), while one inter13. For simplicity, we do not make this random jump component explicit in our equations. But, readers
should keep in mind that all the transition matrices described in this article contain this component.

546

fiTopic Segmentation and Labeling in Asynchronous Conversations

GW
GF

Figure 10: Three sub-graphs used for co-ranking: the fragment quotation graph GF , the
word co-occurrence graph GW , and the bipartite graph GF W that ties the two together.
Blue nodes represent fragments, red nodes represent words.
class step changes the distribution from (f, w) to (W F T w, F W T f ) (for details see Zhou,
Orshanskiy, Zha, & Giles, 2007). The coupling is regulated by a parameter  (0    1)
that determines the extent to which the ranking of words and fragments depend on each
other. Specifically, the two update steps in the power method are:

f t+1 = (1  ) (F T f t ) +  W F T (F W T W F T )wt
w

t+1

T

t

T

T

T

= (1  ) (W w ) +  F W (W F F W )f

(14)
t

(15)

We described the co-ranking framework above assuming that we have a WCG and its
corresponding FQG. However, recall that while the WCG is built for a topic segment, the
FQG described so far (Figure 4) is based on the whole conversation. In order to construct a
FQG for a topic segment in the conversation, we take only those fragments (and the edges)
from the conversation-level FQG that include only the sentences of that segment. This
operation has two consequences. One, some conversation-level fragments may be pruned.
Two, some sentences in a conversation-level fragment may be discarded. For example, the
FQG for topic (segment) ID 1 in Figure 2 includes only the fragments a, h, i, j, and l, and the
edges between them. Fragment j, which contains three sentences in the conversation-level
FQG, contains only one sentence in the FQG for topic ID 1.
3.2.3 Phrase Generation
Once we have a ranked list of words for describing a topical segment, we select the top M
keywords for constructing the keyphrases (labels) from these keywords. We take a similar
approach to Mihalcea and Tarau (2004). Specifically, we mark the M selected keywords in
the text, and collapse the sequences of adjacent keywords into keyphrases. For example,
consider the first sentence, .. 15th anniversary of the Elder Scrolls series .. in Figure 2. If
Elder, Scrolls and series are selected as keywords, since they appear adjacent in the text,
547

fiJoty, Carenini, & Ng

they are collapsed into one single keyphrase Elder Scrolls series. The score of a keyphrase
is then determined by taking the maximum score of its constituents (i.e., keywords).
Rather than constructing the keyphrases in the post-processing phase, as we do, an
alternative approach is to first extract the candidate phrases using either n-gram sequences
or a chunker in the preprocessing, and then rank those candidates (e.g., Medelyan, 2009;
Hulth, 2003). However, determining the optimal value of n in the n-gram sequence is
an issue, and including all possible n-gram sequences for ranking excessively increases the
problem size. Mei et al. (2007) also show that using a chunker leads to poor results due to
the inaccuracies in the chunker, especially when it is applied to a new domain like ours.
3.2.4 Conversation-Level Phrase Re-ranking
So far, we have extracted phrases only from the topic segment ignoring the rest of the
conversation. This method fails to find a label if some of its constituents appear outside
the segment. For example, in our Blog corpus, the phrase server security in the humanauthored label server security and firewall does not appear in its topical segment, but
appears in the whole conversation. In fact, in our development set, about 14% and 8%
words in the blog and email labels, respectively, come from the part of the conversation
that is outside the topic segment. Thus, we propose to extract informative phrases from
the whole conversation, re-rank those with respect to the individual topics (or segments)
and combine only the relevant conversation-level phrases with the segment-level ones.
We rank the words of the whole conversation by applying the ranking models described
in Section 3.2.2 and extract phrases using the same method described in Section 3.2.3. Note
that when we apply our biased random walk model to the whole conversation, there is no
concept of leading sentences and no distinction between the topics. Therefore, to apply to
the whole conversation, we adjust our biased random walk model (Equation 10) as follows:
P (w) =

X

e(y, w)
P (y)
zCk e(y, z)

P
yCk

(16)

where e(y, w) = tfw,y , is the number of times words w and y co-occur in a window of size s
in the conversation. On the other hand, the co-ranking framework, when applied to whole
conversation, combines two conversation-level graphs: the FQG, and the WCG.
To re-rank the phrases extracted from the whole conversation with respect to a particular
topic in the conversation, we reuse the score of the words in that topic segment (given by
the ranking models in Section 3.2.2). As before, the score of a (conversation-level) phrase
is determined by taking the maximum (segment-level) score of its constituents (words). If
a word does not occur in the topic segment, its score is assumed to be 0.
3.2.5 Redundancy Checking
Once we have the ranked list of labels (keyphrases), the last step is to produce the final
k labels as output. When selecting multiple labels for a topic, we expect the new labels
to be diverse without redundant information to achieve broad coverage of the topic. We
use the Maximum Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) criterion to
select the labels that are relevant, but not redundant. Specifically, we select the labels one
by one, by maximizing the following MMR criterion each time:
548

fiTopic Segmentation and Labeling in Asynchronous Conversations

l = argmaxlW S [ Score(l)  (1  ) max

lS Sim(l, l)]

(17)

where W is the set of all labels and S is the set of labels already selected as output. We
define the similarity between two labels l and l as: Sim(l, l) = no /nl , where no is the
number of overlapping (modulo stemming) words between l and l, and nl is the number of
words in l. The parameter  (0    1) quantifies the amount of redundancy allowed.

4. Corpora and Metrics
Due to the lack of publicly available corpora of asynchronous conversations annotated with
topics, we have developed the first corpora annotated with topic information.
4.1 Data Collection
For email, we selected our publicly available BC3 email corpus (Ulrich, Murray, & Carenini,
2008) which contains 40 email conversations from the World Wide Web Consortium (W3C)
mailing list14 . The BC3 corpus, previously annotated with sentence-level speech acts, subjectivity, extractive and abstractive summaries, is one of a growing number of corpora being
used for email research (Carenini et al., 2011). This has an average of 5 emails per conversation and a total of 1024 sentences after excluding the quoted sentences. Each conversation
also provides the thread structure based on reply-to relations between emails.
For blog, we manually selected 20 conversations of various lengths, all short enough to
still be feasible for humans to annotate, from the popular technology-related news website
Slashdot15 . Slashdot was selected because it provides reply-to links between comments,
allowing accurate thread reconstruction, and since the comments are moderated by the users
of the site, they are expected to have a decent standard. A conversation in Slashdot begins
with an article (i.e., a short synopsis paragraph possibly with a link to the original story),
and is followed by a lengthy discussion section containing multiple threads of comments
and single comments. This is unlike an email conversation which contains a single thread
of emails. The main article is assumed to be the root in the conversation tree (based on
reply-to), while the threads and the single comments form the sub-trees in the tree. In
our blog corpus, we have a total of 4, 411 sentences. The total number of comments per
blog conversation varies from 30 to 101 with an average of 60.3, the number of threads per
conversation varies from 3 to 16 with an average of 8.35 and the number of single comments
varies from 5 to 50 with an average of 20.25.
4.2 Topic Annotation
Topic segmentation and labeling in general is a nontrivial and subjective task even for
humans, particularly when the text is unedited and less organized (Purver, 2011). The
conversation phenomenon called Schism makes it even more challenging for conversations.
During a schism, a new conversation takes birth from an existing one, not necessarily because
of a topic shift but because some participants refocus their attention onto each other, and
14. http://research.microsoft.com/en-us/um/people/nickcr/w3c-summary.html
15. http://slashdot.org/

549

fiJoty, Carenini, & Ng

away from whoever held the floor in the parent conversation (Sacks, Schegloff, & Jefferson,
1974). In the example email conversation shown in Figure 1, a schism takes place when the
participants discuss the topic responding to I18N. Not all our annotators agree on the fact
that the topic responding to I18N swerves from the topic TAG document.
To properly design an effective annotation manual and procedure, we performed a twophase pilot study before carrying out the actual annotation. Our initial annotation manual
was inspired by the AMI annotation manual used for topic segmentation of ICSI meeting
transcripts16 . For the pilot study, we selected two blog conversations from Slashdot and five
email conversations from the W3C corpus. Note that these conversations were not picked
from our corpora. Later in our experiments we use these conversations as our development
set for tuning different parameters of the computational models. In the first phase of
the pilot study five computer science graduate students volunteered to do the annotation,
generating five different annotations for each conversation. We then revised our annotation
manual based on their feedback and a detailed analysis of possible sources of disagreement.
In the second phase, we tested our procedure with a university postdoc doing the annotation.
We prepared two different annotation manuals  one for email and one for blog. We chose
to do so for mainly two reasons. (i) As discussed earlier, our email and blog conversations
are structurally different and have their own specific characteristics. (ii) The email corpus
already had some annotations (e.g., abstract summaries) that we could reuse for topic
annotation, whereas our blog corpus is brand new without any existing annotation.
For the actual annotation we recruited and paid three cognitive science fourth year
under-graduates, who are native speakers of English and also Slashdot bloggers. On average,
they took about 7 and 28.5 hours to annotate the 40 email and 20 blog conversations,
respectively. In all, we have three different annotations for each conversation in our corpora.
For blog conversations, the task of finding topics was carried out in four steps:
1. The annotators read the conversation (i.e., article, threads of comments and single
comments) and wrote a short summary ( 3 sentences) only for the threads.
2. They provided short high-level descriptions for the topics discussed in the conversation
(e.g., Game contents or size, Bugs or faults). These descriptions serve as reference
topic labels in our work. The target number of topics and their labels were not given
in advance and they were instructed to find as many or as few topics as needed to
convey the overall content of the conversation.
3. They assigned the most appropriate topic to each sentence. However, if a sentence
covered more than one topic, they labeled it with all the relevant topics according to
their order of relevance. They used the predefined topic OFF-TOPIC if the sentence
did not fit into any topic. Wherever appropriate they also used two other predefined
topics: INTRO (e.g., hi X) and END (e.g., Best, X).
4. The annotators authored a single high-level 250 words summary of the whole conversation. This step was intended to help them remember anything they may have
forgotten and to revise the annotations in the previous three steps.
16. http://mmm.idiap.ch/private/ami/annotation/TopicSegmentationGuidelinesNonScenario.pdf

550

fiTopic Segmentation and Labeling in Asynchronous Conversations

For each email conversation in BC3, we already had three human-authored summaries.
So, along with the actual conversations, we provided the annotators with such summaries to
give them a brief overview of the discussion. After reading a conversation and the associated
summaries, they performed tasks 2 and 3 as in the procedure they follow for annotating
blogs. The annotators carried out the tasks on paper. We created the hierarchical thread
view of the conversation based on the reply-to relations between the comments (or emails)
using indentations and printed each participants information in a different color as in Gmail.
In the email corpus, the three annotators found 100, 77 and 92 topics respectively (269
in total), and in the blog corpus, they found 251, 119 and 192 topics respectively (562
in total). Table 3 shows some basic statistics computed on the three annotations of the
conversations.17 On average, we have 26.3 sentences and 2.5 topics per email conversation,
and 220.55 sentences and 10.77 topics per blog conversation. On average, a topic in email
conversations contains 12.6 sentences, and a topic in blog conversations contains 27.16
sentences. The average number of topics active at a time are 1.4 and 5.81 for email and
blog conversations, respectively. The average entropy which corresponds to the granularity
of an annotation (as described in the next Section) is 0.94 for email conversations and 2.62
for blog conversations. These statistics (i.e., the number of topics and the topic density)
indicate that there is a substantial amount of segmentation (and labeling) to do.

Number
Number
Average
Average
Entropy

of sentences
of topics
topic length
topic density

Mean
Email Blog
26.3
220.55
2.5
10.77
12.6
27.16
1.4
5.81
0.94
2.62

Max
Email Blog
55
430
7
23
35
61.17
3.1
10.12
2.7
3.42

Min
Email Blog
13
105
1
5
3
11.67
1
2.75
0
1.58

Table 3: Statistics on three human annotations per conversation.

4.3 Evaluation (and Agreement) Metrics
In this section we describe the metrics used to compare different annotations. These metrics
measure both how much our annotators agree with each other, and how well our models
and various baselines perform. For a given conversation, different annotations can have
different numbers of topics, different topic assignments of the sentences (i.e., the clustering)
and different topic labels. Below we describe the metrics used to measure the segmentation
performance followed by the metrics used to measure the labeling performance.
4.3.1 Metrics for Topic Segmentation
As different annotations can group sentences in different numbers of clusters, agreement
metrics widely used in supervised classification, such as the  statistic and F1 score, are
not applicable. Again, our problem of topic segmentation in asynchronous conversation is
17. We got 100% agreement on the two predefined topics INTRO and END. Therefore, in all our computations we excluded the sentences marked as either INTRO or END.

551

fiJoty, Carenini, & Ng

not sequential in nature. Therefore, the standard metrics widely used in sequential topic
segmentation in monolog and synchronous dialog, such as the Pk (Beeferman, Berger, &
Lafferty, 1999) and W indowDif f (W D) (Pevzner & Hearst, 2002), are also not applicable.
Rather, the one-to-one and local agreement metrics described by Elsner and Charniak
(2010) are more appropriate for our segmentation task.
The one-to-one metric measures global agreement between two annotations by pairing
up topical segments from the two annotations in a way (i.e., by computing the optimal
max-weight bipartite matching) that maximizes the total overlap, and then reports the
percentage of overlap. The local agreement metric lock measures agreement within a context
of k sentences. To compute the loc3 score for the m-th sentence in the two annotations, we
consider the previous 3 sentences: m-1, m-2 and m-3, and mark them as either same or
different depending on their topic assignment. The loc3 score between two annotations is
the mean agreement on these same or different judgments, averaged over all sentences.
See Appendix A for a detailed description of these metrics with concrete examples.
We report the annotators agreement found in one-to-one and loc3 metrics in Table 4. For
each human annotation, we measure its agreement with the two other human annotations
separately, and report the mean agreements. For email, we get high agreement in both
metrics, though the local agreement (average of 83%) is a little higher than the global one
(average of 80%). For blog, the annotators have high agreement in loc3 (average of 80%),
but they disagree more in one-to-one (average of 54%). A low one-to-one agreement in
blog is quite acceptable since blog conversations are much longer and less focused than
email conversations (see Table 3). By analyzing the two corpora we also noticed that in
blogs, people are more informal and often make implicit jokes (see Figure 2). As a result,
the segmentation task in blogs is more challenging for humans as well as for our models.
Note that in a similar annotation task for chat disentanglement, Elsner and Charniak (2010)
report an average one-to-one score of 53%. Since the one-to-one score for naive baselines (see
Section 5.1) is much lower than the human agreement, this metric differentiates human-like
performance from baseline. Therefore, computing one-to-one correlation with the human
annotations is a legitimate evaluation for our models.

one-to-one
loc3

Mean
Email Blog
80.4
54.2
83.2
80.1

Max
Email Blog
100.0
84.1
100.0
94.0

Min
Email Blog
31.3
25.3
43.7
63.3

Table 4: Annotator agreement in one-to-one and loc3 on the two corpora.

When we analyze the source of disagreement in the annotation, we find that by far the
most frequent reason is the same as the one observed by Elsner and Charniak (2010) for the
chat disentanglement task; namely, some annotators are more specific (i.e., fine) than others
(i.e., coarse). To determine the level of specificity in an annotation, similarly to Elsner and
Charniak, we use the information-theoretic concept of entropy. If we consider the topic
of a randomly picked sentence in a conversation as a random variable X, its entropy H(X)
measures the level of details in an annotation. For topics k each having length nk in a
conversation of length N , we compute H(X) as follows:
552

fiTopic Segmentation and Labeling in Asynchronous Conversations

H(X) = 

K
X
nk
k=1

N

log2

nk
N

(18)

where K is the total number of topics (or topical segments) in the conversation. The
entropy gets higher as the number of topics increases and the topics are evenly distributed
in a conversation. In our corpora, it varies from 0 to 2.7 in email conversations and from
1.58 to 3.42 in blog conversations (Table 3). These variations demonstrate the differences
in specificity for different annotators, but do not determine their agreement on the general
structure. To quantify this, we use the many-to-one metric proposed by Elsner and
Charniak (2010). It maps each of the source clusters to the single target cluster with
which it gets the highest overlap, then computes the total percentage of overlap. This
metric is asymmetrical, and not to be used for performance evaluation.18 However, it
provides some insights about the annotation specificity. For example, if one splits a cluster
of another annotator into multiple sub-clusters then, the many-to-one score from fine to
coarse annotation is 100%. In our corpora, by mapping from fine (high-entropy) to coarse
(low-entropy) annotation we get high many-to-one score, with an average of 95% in email
conversations and an average of 72% in blog conversations (Table 5). This suggests that
the finer annotations have mostly the same scopic boundaries as the coarser ones.

many-to-one

Mean
Email Blog
94.9
72.3

Max
Email Blog
100
98.2

Min
Email Blog
61.1
51.4

Table 5: Annotator agreement in many-to-one on the two corpora.

4.3.2 Metrics for Topic Labeling
Recall that we extract keyphrases from the text as topic labels. Traditionally keyphrase extraction is evaluated using precision, recall and F-measure based on exact matches between
the extracted keyphrases and the human-assigned keyphrases (e.g., Mihalcea and Tarau,
2004; Medelyan et al., 2009). However, it has been noted that this approach based on exact
matches underestimates the performance (Turney, 2000). For example, when compared with
the reference keyphrase Game contents or size, a credible candidate keyphrase Game contents gets evaluated as wrong in this metric. Therefore, recent studies (Zesch & Gurevych,
2009; Kim, Baldwin, & Kan, 2010a) suggest to use the n-gram-based metrics that account
for near-misses, similar to the ones used in text summarization, e.g., ROUGE (Lin, 2004),
and machine translation, e.g., BLEU (Papineni, Roukos, Ward, & Zhu, 2002).
Kim et al. (2010a) evaluated the utility of different n-gram-based metrics for keyphrase
extraction and showed that the metric which we call mutual-overlap (m-o), correlates
most with human judgments.19 Therefore, one of the metrics we use for evaluating our topic
18. One can easily optimize it by assigning a different topic to each of the source sentences.
19. Kim et al. (2010a) call this metric R-precision (R-p), which is different from the actual definition
of R-p for keyphrase evaluation given by Zesch and Gurevych (2009). Originally, R-p is the precision
measured when the number of candidate keyphrases equals the number of gold keyphrases.

553

fiJoty, Carenini, & Ng

labeling models is m-o. Given a reference keyphrase pr of length (in words) nr , a candidate
keyphrase pc of length nc , and no being the number of overlapping (modulo stemming)
words between pr and pc , mutual-overlap is formally defined as:
mutualoverlap(pr , pc ) =

no
max(nr , nc )

(19)

This metric gives full credit to exact matches and morphological variants, and partial credit to two cases of overlapping phrases: (i) when the candidate keyphrase includes
the reference keyphrase, and (ii) when the candidate keyphrase is a part of the reference
keyphrase. Notice that m-o as defined above evaluates a single candidate keyphrase against
a reference keyphrase. In our setting, we have a single reference keyphrase (i.e., topic label)
for each topical cluster, but as mentioned before, we may want our models to extract the
top k keyphrases. Therefore, we modify m-o to evaluate a set of k candidate keyphrases Pc
against a reference keyphrase pr as follows, calling it weighted-mutual-overlap (w-m-o):
weightedmutualoverlap(pr , Pc ) =

k
X
i=1

no
S(pic )
max(nr , nic )

(20)

P
where S(pic ) is the normalized score (i.e., S(pic ) satisfies 0  S(pic )  1 and ki=1 S(pic ) = 1)
of the i-th candidate phrase pic  Pc . For k = 1, this metric is equivalent to m-o, and for
higher values of k, it takes the sum of k m-o scores, each weighted by its normalized score.
The w-m-o metric described above only considers word overlap and ignores other semantic relations (e.g., synonymy, hypernymy) between words. However, annotators when
writing the topic descriptions, may use words that are not directly from the conversation,
but are semantically related. For example, given a reference keyphrase meeting agenda,
its lexical semantic variants like meeting schedule or meeting plan should be treated as
correct. Therefore, we also consider a generalization of w-m-o that incorporates lexical
semantics. We define weighted-semantic-mutual-overlap (w-s-m-o) as follows:

weightedsemanticmutualoverlap(pr , Pc ) =

k
X
i=1

P

tr pr

P

tc pic

(tr , tc )

max(nr , nic )

S(pic )

(21)

where (tr , tc ) is the semantic similarity between the nouns tr and tc . The value of (tr , tc ) is
between 0 and 1, where 1 denotes notably high similarity and 0 denotes little-to-none. Notice
that, since this metric considers semantic similarity between all possible pairs of nouns, the
value of this measure can be greater than 100% (when presented in percentage). We use
the metrics (e.g., lin similarity, wup similarity) provided in the WordNet::Similarity package
(Pedersen, Patwardhan, & Michelizzi, 2004) for computing WordNet-based similarity, and
always choose the most frequent sense for a noun. The results we get are similar across the
similarity metrics. For brevity, we just mention the lin similarity in this article.
4.3.3 Metrics for End-to-End Evaluation
Just like the human annotators, our end-to-end system takes an asynchronous conversation
as input, finds the topical segments in the conversation, and then assigns short descriptions
554

fiTopic Segmentation and Labeling in Asynchronous Conversations

(topic labels) to each of the topical segments. It would be fairly easy to compute agreement
on topic labels based on mutual overlaps, if the number of topics and topical segments were
fixed across the annotations of a given conversation. However, since different annotators
(system or human) can identify a different number of topics and different clustering of
sentences, measuring annotator (model or human) agreement on the topic labels is not a
trivial task. To solve this, we first map the clusters of one annotation (say A1 ) to the
clusters of another (say A2 ) by the optimal one-to-one mapping described in the previous
section. After that, we compute the w-m-o and w-s-m-o scores on the labels of the mapped
(or paired) clusters. Formally, if li1 is the label of cluster c1i in A1 that is mapped to the
cluster c2j with label lj2 in A2 , we compute w-m-o(li1 , lj2 ) and w-s-m-o(li1 , lj2 ).
Table 6 reports the human agreement for w-m-o and w-s-m-o on the two corpora. Similar
to segmentation, we get higher agreement on labeling for both metrics on email. Plausibly,
the reasons remain the same; the length and the characteristics (e.g., informal, less focused)
of blog conversations make the annotators disagree more. However, note that these measures
are computed based on one-to-one mappings of the clusters and may not reflect the same
agreement one would get if the annotators were asked to label the same segments.

w-m-o
w-s-m-o

Mean
Email Blog
36.8
19.9
42.5
28.2

Max
Email Blog
100.0
54.2
107.3
60.8

Min
Email Blog
0.0
0.0
0.0
5.2

Table 6: Annotator agreement in w-m-o and w-s-m-o on the two corpora.

5. Experiments
In this section we present our experimental results. First, we show the performance of the
segmentation models. Then we show the performance of the topic labeling models based
on manual segmentation. Finally, we present the performance of the end-to-end system.
5.1 Topic Segmentation Evaluation
In this section we present the experimental setup and results for the segmentation task.
5.1.1 Experimental Setup for Segmentation
We ran six different topic segmentation models on our corpora presented in Section 4. Our
first model is the graph-based unsupervised segmentation model presented by Malioutov
and Barzilay (2006). Since the sequentiality constraint of topic segmentation in monolog
and synchronous dialog does not hold in asynchronous conversation, we implement this
model without this constraint. Specifically, this model (call it M&B) constructs a weighted
undirected graph G(V, E), where the nodes V represent the sentences and the edge weights
w(x, y) represent the cosine similarity (Equation 5) between sentences x and y. It then
finds the topical segments by optimizing the normalized cut criterion (Equation 6). Thus,
M&B considers the conversation globally, but models only lexical similarity.
555

fiJoty, Carenini, & Ng

The other five models are LDA, LDA+FQG, LCSeg, LCSeg+FQG and the Supervised
model (SUP) as described in Section 3. The tunable parameters of the different models
were set based on their performance on our developement set. The hyperparameters  and
 in LDA were set to their default values (=50/K, =0.01) as suggested by Steyvers
and Griffiths (2007).20 The regularization strength  in LDA+FQG was set to 20. The
parameters of LCSeg were set to their default values since this setting delivers the best
performance on the development set. For a fair comparison, we set the same number of
topics per conversation in all of the models. If at least two of the three annotators agree on
the topic number, we set that number, otherwise we set the floor value of the average topic
number. The mean statistics of the six model annotations are shown in Table 7. Comparing
with the statistics of the human annotations in Table 3, we can notice that these numbers
are within the bounds of the human annotations.21

Email

Blog

Topic number
Topic length
Topic density
Entropy
Topic number
Topic length
Topic density
Entropy

M&B
2.41
12.41
1.90
0.99
10.65
20.32
7.38
2.54

LDA
2.10
13.3
1.83
0.98
10.65
20.32
9.39
3.33

LDA+FQG
1.90
15.50
1.60
0.75
10.65
20.32
8.32
2.37

LCSeg
2.41
12.41
1.01
0.81
10.65
20.32
1.00
2.85

LCSeg+FQG
2.41
12.41
1.39
0.93
10.65
20.32
5.21
2.81

SUP
2.41
12.41
1.42
0.98
10.65
20.32
5.30
2.85

Table 7: Mean statistics of different models annotation

We also evaluate the following baselines, which any useful model should outperform.
 All different Each sentence in the conversation constitutes a separate topic.
 All same The whole conversation constitutes a single topic.
 Speaker The sentences from each participant constitute a separate topic.
 Blocks of k (= 5, 10, 15, 20, 25, 30): Each consecutive group of k sentences in
the temporal order of the conversation constitutes a separate topic.
5.1.2 Results for Segmentation
Table 8 presents the human agreement and the agreement of the models with the human
annotators on our corpora. For each model annotation, we measure its agreement with the
three human annotations separately using the metrics described in Section 4.3.1, and report
the mean agreements. In the table, we also show the performance of the two best baselines
the Speaker and the Blocks of k.
20. The performance of LDA does not seem to be sensitive to the values of  and .
21. Although the topic numbers per conversation are fixed for different models, LDA and LDA+FQG may
find less number of topics (see Equation 3 and 4).

556

fiTopic Segmentation and Labeling in Asynchronous Conversations

Email

Blog

Mean 1-to-1
Max 1-to-1
Min 1-to-1
Mean loc3
Max loc3
Min loc3
Mean 1-to-1
Max 1-to-1
Min 1-to-1
Mean loc3
Max loc3
Min loc3

Baselines
Speaker Blocks M&B LDA
of k
51.8
38.3
62.8
57.3
94.3
77.1 100.0 100.0
23.4
14.6
36.3
24.3
64.1
57.4
62.4
54.1
97.0
73.1 100.0 100.0
27.4
42.6
36.3
38.1
33.5
32.0
30.0
25.2
61.1
46.0
45.3
42.1
13.0
15.6
18.2
15.3
67.0
52.8
54.1
53.0
87.1
68.4
64.3
65.6
53.4
42.3
45.1
38.6

Models
LDA+ LCSeg LCSeg SUP
FQG
+FQG
61.5
62.2
69.3
72.3
100.0 100.0 100.0 100.0
24.0
33.1
38.0
42.4
60.6
72.0
72.7
75.8
100.0 100.0 100.0 100.0
38.4
40.7
40.6
40.4
28.0
36.6
46.7
48.5
56.3
53.6
67.4
66.1
16.1
23.7
26.6
28.4
55.4
56.5
75.1
77.2
67.1
76.0
89.0
96.4
46.3
43.1
56.7
63.2

Human

80.4
100.0
31.3
83.2
100.0
43.7
54.2
84.1
25.3
80.1
94.0
63.3

Table 8: Topic segmentation performance of the two best Baselines, Human and Models.
In the Blocks of k column, k = 5 for email and k = 20 for blog.

Most of the baselines perform rather poorly. All different is the worst baseline of all with
mean one-to-one scores of only 0.05 and 0.10, and mean loc3 scores of only 0.47 and 0.25
in the blog and email corpus, respectively. Blocks of 5 is one of the best baselines in email,
but it performs poorly in blog with mean one-to-one of 0.19 and mean loc3 of 0.54. On the
contrary, Blocks of 20 is one of the best baselines in blog, but performs poorly in email.
This is intuitive since the average number of topics and topic length in blog conversations
(10.77 and 27.16) are much higher than those of email (2.5 and 12.6). All same is optimal
for conversations containing only one topic, but its performance rapidly degrades as the
number of topics increases. It has mean one-to-one scores of 0.29 and 0.28 and mean loc3
scores of 0.53 and 0.54 in the blog and email corpora, respectively. Speaker is the strongest
baseline in both domains.22 In several cases it beats some of the under-performing models.
In the email corpus, in one-to-one, generally the models agree with the annotators more
than the baselines do, but less than the annotators agree with each other. We observe a
similar trend in the local metric loc3 , however on this metric, some models fail to beat the
best baselines. Notice that human agreement for some of the annotations is quite low (see
the Min scores), even lower than the mean agreement of the baselines. As explained before,
this is due to the fact that some human annotations are much more fine-grained than others.
In the blog corpus, the agreement on the global metric (one-to-one) is much lower than
that on the email corpus. The reasons were already explained in Section 4.3.1. We notice
a similar trend in both metrics some under-performing models fail to beat the baselines,
while others perform better than the baselines, but worse than the human annotators.
The comparison among the models reveals a general pattern. The probabilistic generative models LDA and LDA+FQG perform disappointingly on both corpora. A likely
explanation is that the independence assumption made by these models when computing
the distribution over topics for a sentence from the distributions of its words causes nearby
22. There are many anonymous authors in our blog corpus. We treated each of them as a separate author.

557

fiJoty, Carenini, & Ng

sentences (i.e., local context) to be excessively distributed over topics. Another reason
could be the limited amount of data available for training. In our corpora, the average
number of sentences per blog conversation is 220.55 and per email conversation is 26.3,
which might not be sufficient for the LDA models (Murphy, 2012). If we compare the performance of LDA+FQG with the performance of LDA, we get a significant improvement
with LDA+FQG in both metrics on both corpora (p<0.01). The regularization with the
FQG prevents the local context from being excessively distributed over topics.
The unsupervised graph-based model M&B performs better than the LDA models in
most cases (i.e., except loc3 in blog) (p < 0.001). However, its performance is still far below
the performance of the top performing models like LCSeg+FQG and the supervised model.
The reason is that even though, by constructing a complete graph, this method considers the
conversation globally, it only models the lexical similarity and disregards other important
features of asynchronous conversation like the fine conversation structure and the speaker.
Comparison of LCSeg with LDAs and M&B reveals that LCSeg in general is a better
model. LCSeg outperforms LDA by a wide margin in one-to-one on two datasets and in
loc3 on email (p < 0.001). The difference between LCSeg and LDA in loc3 on blog is also
significant with p < 0.01. LCSeg also outperforms M&B in most cases (p < 0.01) except in
one-to-one on email. Since LCSeg is a sequential model it extracts the topics keeping the
context intact. This helps it to achieve high loc3 agreement for shorter conversations like
email conversations. But, for longer conversations like blog conversations, it overdoes this
(i.e., extracts larger chunks of sentences as a topic segment) and gets low loc3 agreement.
This is unsurprising if we look at its topic density in Table 7 on the two datasets the density
is very low in the blog corpus compared to annotators and other well performing models.
Another reason of its superior performance over LDAs and M&B could be its term weighting
scheme. Unlike LDAs and M&B, which consider only repetition, LCSeg also considers how
tightly the repetition happens. However, there is still a large gap in performance between
LCSeg and other top performing models (LCSeg+FQG, the supervised). As explained
earlier, topics in an asynchronous conversation may not change sequentially in the temporal
order of the sentences. If topics are interleaved then LCSeg fails to identify them correctly.
Furthermore, LCSeg does not consider other important features beyond the lexical cohesion.
When we incorporate FQG into LCSeg, we get a significant improvement in one-to-one
on both corpora and in loc3 on blog (p<0.0001). Even though the improvement in loc3
on email is not significant, the agreement is quite high compared to other unsupervised
models. Overall, LCSeg+FQG is the best unsupervised model. This supports our claim
that sentences connected by reply-to relations in the FQG usually refer to the same topic.
Finally, when we combine all the features into our graph-based supervised model (SUP
in Table 8), we get a significant improvement over LCSeg+FQG in both metrics across both
domains (p<0.01). The agreements achieved by the supervised model are also much closer
to that of human annotators. Beside the features, this improvement might also be due to
the fact that, by constructing a complete graph, this model considers relations between all
possible sentence pairs in a conversation, which we believe is a key requirement for topic
segmentation in asynchronous conversations.
558

fiTopic Segmentation and Labeling in Asynchronous Conversations

5.2 Topic Labeling Evaluation
In this section we present the experimental evaluation of the topic labeling models when
the models are provided with manual (or gold) segmentation. This allows us to judge their
performance independently of the topic segmentation task.
5.2.1 Experimental Setup for Topic Labeling
As mentioned in Section 4, in the email corpus, the three annotators found 100, 77 and
92 topics (or topical segments) respectively (269 in total), and in the blog corpus, they
found 251, 119 and 192 topics respectively (562 in total). The annotators wrote a short
high-level description for each topic. These descriptions serve as reference topic labels
in our evaluation.23 The goal of the topic labeling models is to automatically generate
such informative descriptions for each topical segment. We compare our approach with
two baselines. The first baseline FreqBL ranks the words according to their frequencies.
The second baseline LeadBL, expressed by equation 11, ranks the words based on their
relevance only to the leading sentences in a topical segment.
We also compare our model with two state-of-the-art keyphrase extraction methods.
The first one is the unsupervised general TextRank model proposed by Mihalcea and Tarau
(2004) (call it M&T) that does not incorporate any conversation specific information. The
second one is the supervised model Maui proposed by Medelyan et al. (2009). Briefly, Maui
first extracts all n-grams up to a maximum length of 3 as candidate keyphrases. Then a
bagged decision tree classifier filters the candidates using nine different features. Due to the
lack of labeled training data in asynchronous conversations, we train Maui on the humanannotated dataset released as part of the SemEval-2010 task 5 on automatic keyphrase
extraction from scientific articles (Kim, Medelyan, Kan, & Baldwin, 2010b). This dataset
contains 244 scientific papers from the ACM digital library, each comes with a set of authorassigned and reader-assigned keyphrases. The total number of keyphrases assigned to the
244 articles by both the authors and the readers is 3705.
We experimented with two different versions of our biased random walk model that
incorporates informative clues from the leading sentences. One, BiasRW, does not include
any conversation-level phrase (Section 3.2.4), and the other one BiasRW+, does. The
parameter Uk , the set of leading sentences, was empirically set to the first two sentences
and the bias parameter  was set to 0.85 based on our development set.
We experimented with four different versions of the co-ranking framework depending
on what type of random walk is performed on the word co-occurrence graph (WCG)
and whether the model includes any conversation-level phrases. Let CorGen denote the
co-ranking model with a general random walk on WCG, and CorBias denote the coranking model with a biased random walk on WCG. These two models do not include any
conversation-level phrase while CorGen+ and CorBias+ do. The coupling strength 
and the co-occurrence window size s were empirically set to 0.4 and 2, respectively, based
on the development set. The dumping factor was set to its default value 0.85.
Note that all the models (except Maui) and the baselines follow the same preprocessing
and post-processing (i.e., phrase generation and redundancy checking) steps. The value of
23. Notice that in our setting, for each topic segment we have only one reference label to compare with.
Therefore, we do not show the human agreement on the topic labeling task in Table 9 and 10.

559

fiJoty, Carenini, & Ng

M in phrase generation was set to 25% of the total number of words in the cluster, and 
in redundancy checking was set to 0.35 based on the development set.
5.2.2 Results for Topic Labeling
We evaluate the performance of different models using the metrics described in Section
4.3.2. Table 9 and 10, respectively, show the mean weighted-mutual-overlap (w-m-o) and
weighted-semantic-mutual-overlap (w-s-m-o) scores in percentage of different models for
different values of k (i.e., number of output labels) on the two corpora.
Both the baselines have proved to be strong, beating the existing models in almost every
case. This tells us that the frequency of the words in the topic segment and their occurrence
in the leading sentences carry important information for topic labeling. Generally speaking,
LeadBL is a better baseline for email, while for blog FreqBL is better than LeadBL.
The supervised model Maui is the worst performer in both metrics on the two corpora.
Its performance is also consistently low across the corpora for any particular value of k.
A possible explanation is that Maui was trained on a domain (scientific articles), which is
rather different from asynchronous conversations. Another reason may be that Maui does
not consider any conversational features.
The general random walk model M&T also delivers poor performance on our corpora,
failing to beat the baselines in both measures. This indicates that the random walk model
based on only co-occurrence relations between the words is not sufficient for finding topic
labels in asynchronous conversations. It needs to consider conversation specific information.
By incorporating the clues from the leading sentences, our biased random walk model
BiasRW improves the performance significantly over the baselines in both metrics for all the
values of k on the two corpora (p<0.05). This demonstrates the usefulness of considering the
leading sentences as an information source for topic labeling in asynchronous conversation.

Baselines

Models

FreqBL
LeadBL
M&T
Maui
BiasRW
BiasRW+
CorGen
CorGen+
CorBias
CorBias+

k=1
Email Blog
22.86 19.05
22.41 18.17
15.87 18.23
10.48 10.03
24.77 20.83
24.91 23.65
17.60 20.76
18.32 22.44
24.84 20.96
25.13 23.83

k=2
Email Blog
17.47 16.17
18.94 15.95
12.68 14.31
9.86
9.56
19.78 17.28
20.36 19.69
15.32 17.64
15.86 19.65
19.88 17.73
20.20 19.97

k=3
Email Blog
14.96 13.83
15.92 13.75
10.33 12.15
9.03
9.23
17.38 15.06
18.09 17.76
15.14 15.78
15.46 18.01
17.61 16.22
18.21 18.33

k=4
Email Blog
13.17 13.45
14.36 12.61
9.63
11.38
8.71
8.90
16.24 14.53
16.20 16.78
14.23 15.03
14.89 16.90
16.99 15.64
17.15 17.28

k=5
Email Blog
12.06 12.59
13.76 11.93
9.07
11.03
8.50
8.53
15.80 14.26
15.78 15.86
14.08 14.75
14.45 16.13
16.81 15.38
16.90 16.55

Table 9: Mean weighted-mutual-overlap (w-m-o) scores for different values of k on two corpora.

The general co-ranking model CorGen, by incorporating the conversation structure,
outperforms the baselines in both metrics for all k on blog (p<0.05), but fails to do so in
many cases on email. On blog, there is also no significant difference between BiasRW and
CorGen in w-m-o for all k (Table 9), but CorGen outperforms BiasRW in w-s-m-o (Table
10) for higher values of k (2,3,4,5) (p<0.05). On the other hand, on email, BiasRW always
outperforms CorGen in both metrics for all k (p<0.05). So we can conclude that, on blog,
560

fiTopic Segmentation and Labeling in Asynchronous Conversations

exploiting the conversation structure seems to be more beneficial than the leading sentences,
whereas on email, we observe the opposite. The reason could be that the topic segments
in blog are much longer than those of email (average length 27.16 vs. 12.6). Therefore,
the FQGs of blog segments are generally larger and capture more information than the
FQGs of email segments. Besides, email discussions are more focused than blog discussions.
The leading sentences in email segments carry more informative clues than that of blog
segments. This is also confirmed in Figure 9, where the leading sentences in email cover
more of the human-authored words than they do in blog.

Baselines

Models

FreqBL
Lead-BL
M&T
Maui
BiasRW
BiasRW+
CorGen
CorGen+
CorBias
CorBias+

k=1
Email Blog
23.36 23.52
24.99 21.19
18.71 22.08
14.79 14.14
28.87 24.63
27.96 24.51
23.66 24.69
23.50 24.30
28.44 25.66
27.97 25.26

k=2
Email Blog
20.50 21.03
21.69 20.61
16.25 19.59
13.76 13.67
24.76 22.51
24.71 23.05
21.97 23.83
22.09 24.35
26.39 24.15
26.34 24.19

k=3
Email Blog
19.82 20.18
20.40 19.49
14.62 17.91
13.03 12.87
22.48 21.36
22.56 22.88
21.51 22.86
21.96 23.89
24.47 23.18
24.69 23.60

k=4
Email Blog
18.47 19.58
19.57 18.98
14.29 17.27
12.69 12.10
21.67 20.95
21.19 22.08
20.98 22.37
21.36 23.42
23.70 22.76
23.65 23.44

k=5
Email Blog
17.81 19.27
19.17 18.71
14.06 16.92
11.73 11.52
21.28 20.78
20.82 21.73
20.44 22.22
20.90 23.00
23.56 22.67
23.23 23.20

Table 10: Mean weighted-semantic-mutual-overlap scores for different values of k on two corpora.

By combining the two forms of conversation specific information into a single model,
CorBias delivers improved performance over CorGen and BiasRW in both metrics. On
email, CorBias is significantly better than CorGen for all k in both metrics (p<0.01). On
blog, CorBias gets significant improvement over BiasRW for higher values of k (3, 4, 5) in
both metrics (p<0.05). The two sources of information are complementary and help each
other to overcome the domain-specific limitations of the respective models. Therefore, one
should exploit both information sources to build a generic domain-independent system.
When we include the conversation-level phrases (+ versions), we get a significant improvement in w-m-o on blog (p<0.01), but not on email. This may be because blog conversations have many more topical segments than email conversations (average topic number
10.77 vs. 2.5). Thus, there is little information for the label of a topical segment outside that segment in email conversations. However, note that including conversation-level
phrases does not hurt the performance significantly in any case.
To further analyze the performance, Table 11 shows the mean w-m-o scores when only
the best of k output labels is considered. This allows us to judge the models ability to
generate the best label in the top k list. The results are much clearer here. Generally
speaking, among the models that do not include conversation-level phrases, CorBias is the
best model, while including conversation-level phrases improves the performance further.
Table 12 shows some of the examples from our test set where the system-generated (i.e.,
CorBias+) labels are very similar to the human-authored ones. There are also many cases
like the ones in Table 13, where the system-generated labels are reasonable, although they
get low w-m-o and w-s-m-o scores when compared with the human-authored labels.
561

fiJoty, Carenini, & Ng

Baselines

Models

FreqBL
LeadBL
M&T
Maui
BiasRW
BiasRW+
CorGen
CorGen+
CorBias
CorBias+

k=2
Email Blog
27.02 23.69
28.72 21.69
21.45 21.70
14.00 14.85
29.34 24.92
29.47 25.88
23.45 25.05
24.56 25.87
28.98 25.27
29.76 25.96

k=3
Email Blog
29.79 24.29
30.86 23.14
23.12 23.18
15.57 17.33
31.42 25.18
31.43 27.38
28.44 25.72
28.46 26.61
30.90 26.41
31.04 27.65

k=4
Email Blog
31.12 24.88
31.99 24.19
25.23 23.82
17.15 19.23
32.58 25.89
32.96 28.47
30.10 26.40
31.14 27.63
32.24 27.14
33.61 28.63

k=5
Email Blog
31.25 25.58
31.99 25.33
25.45 24.07
18.40 20.03
32.97 26.64
33.87 29.17
30.33 27.10
32.91 28.50
33.25 27.65
35.35 29.58

Table 11: Mean weighted-mutual-overlap (w-m-o) scores when the best of k labels is considered.

Email

Blog

Human-authored
Details of Bristol meeting
Nashville conference
Meeting agenda
Design guidelines
Contact with Steven
faster than light (FTL) travel
Dr. Paul Laviolette
Vietnam and Iraq warfare
Pulsars
Linux distributions

System-generated (top 5)
Bristol, face2face meeting, England, October
Nashville conference, Courseware developers, mid October, event
detailed agenda, main point, meetings, revision, wcag meetings
general rule, design guidelines, accessible design, absolutes, forbid
Steven Pemberton, contact, charter, status, schedule w3c
FTL travel, need FTL, limited FTL, FTL drives, long FTL
Dr. Paul Laviolette, bang theory, systems theory, extraterrestial beacons, laugh
Vietnam war, incapable guerrilla war, war information, war ii, vietnamese war
mean pulsars, pulsars slow time, long pulsars, relative pulsars, set pulsars
linux distro, linux support, major linux, viable linux

Table 12: Examples of Human-authored labels and System-generated labels.

Human-authored
Meeting time and place
Archaeology
Bio of Al
Budget Constraints
Food choice

System-generated
October, mid October, timing, w3c timing issues, Ottawa
religious site, burial site, ritual site, barrows tomb
Al Gilman, standards reformer, repair interest group, ER IG, ER teams
budget, notice, costs, smaller companies, travel
roast turkey breast, default choices, small number, vegetable rataouille, lunch

Table 13: Examples of System-generated labels that are reasonable but get low scores.

562

fiTopic Segmentation and Labeling in Asynchronous Conversations

This is because most of the human-authored labels in our corpora are abstractive in
nature. Annotators often write their own labels rather than simply copying keyphrases
from the text. In doing so, they rely on their expertise and general world knowledge that
may go beyond the contents of the conversation. In fact, although annotators reuse many
words from the conversation, only 9.81% of the human-authored labels in blog and 12.74%
of the human-authored labels in email appear verbatim in their respective conversations.
Generating human-like labels will require a deeper understanding of the text and robust
textual inference, for which our extractive approach can provide some useful input.
5.3 Full System Evaluation
In this section we present the performance of our end-to-end system. We first segment a
given asynchronous conversation using our best topic segmenter (the supervised model),
and then feed its output to our best topic labeler (the CorBias+ model). Table 14 presents
the human agreement and the agreement of our system with the human annotators based
on the best of k outputs. For each system annotation we measure its agreement in w-m-o
and w-s-m-o with the three human annotations using the method described in Section 4.3.3.

Email

Blog

Mean w-m-o
Max w-m-o
Mean w-s-m-o
Max w-s-m-o
Mean w-m-o
Max w-m-o
Mean w-s-m-o
Max w-s-m-o

k=1
19.19
100.0
24.98
108.43
9.71
26.67
15.46
47.10

k=2
23.62
100.0
32.08
108.43
11.71
26.67
19.77
47.28

System
k=3
k=4
26.19
27.06
100.0
100.0
34.63
36.92
108.43 108.43
14.55
15.83
35.00
35.00
23.35
25.57
47.28
48.54

Human
k=5
28.06
100.0
38.95
108.43
16.72
35.00
26.23
48.54

36.84
100.0
42.54
107.31
19.97
54.17
28.22
60.76

Table 14: Performance of the end-to-end system and human agreement.

Notice that in email, our system gets 100% agreement in w-m-o metric for some conversations. However, there is a substantial gap between the mean and the max w-m-o scores.
Similarly, in w-s-m-o, our system achieves a maximum of 108% agreement, but the mean
varies from 25% to 39% depending on different values of k. In blog, the w-m-o and w-s-m-o
scores are much lower. The maximum scores achieved in w-m-o and w-s-m-o metrics in blog
are only 35% and 49% (for k = 5), respectively. The mean w-m-o score varies from 10% to
17%, and the mean w-s-m-o score varies from 15% to 28% for different values of k. This
demonstrates the difficulties of topic segmentation and labeling tasks in blog conversations.
Comparing with Table 11, we can notice that inaccuracies in the topic segmenter affects
the overall performance. However, our results are encouraging. Even though for lower
values of k there is a substantial gap between our results and the human agreement, as the
value of k increases, our results get closer to the human agreement, especially in w-s-m-o.
563

fiJoty, Carenini, & Ng

6. Conclusion and Future Direction
This work presents two new corpora of email and blog conversations annotated with topics,
which, along with the proposed metrics, will allow researchers to evaluate their work quantitatively. We also present a complete computational framework for topic segmentation and
labeling in asynchronous conversation.24 Our approach extends state-of-the-art methods by
considering the fine-grained structure of the asynchronous conversation, along with other
conversational features. We do this by applying recent graph-based methods for NLP such
as min-cut and random walk on paragraph, sentence or word graphs.
For topic segmentation, we extend the LDA and LCSeg unsupervised models to incorporate the fine-grained conversational structure (the Fragment Quotation Graph (FQG)),
generating two novel unsupervised models LDA+FQG and LCSeg+FQG. In addition to
that, we also present a novel graph-theoretic supervised segmentation model that combines
lexical, conversational and topic features. For topic labeling, we propose two novel random
walk models that extract the most representative keyphrases from the text, by respectively
capturing conversation specific clues from two different sources: the leading sentences and
the fine conversational structure (i.e., the FQG).
Experimental results in the topic segmentation task demonstrate that both LDA and
LCSeg benefit significantly when they are extended to consider the FQG, with LCSeg+FQG
being the best unsupervised model. The comparison of the supervised segmentation model
with the unsupervised models shows that the supervised method outperforms the unsupervised ones even using only a few labeled conversations, being the best segmentation model
overall. The outputs of LCSeg+FQG and the supervised model are also highly correlated
with human annotations in both local and global metrics. The experiment on the topic
labeling task reveals that the random walk models perform better when they exploit the
conversation specific clues and the best results are achieved when all the sources of clues are
exploited. The evaluation of the complete end-to-end system also shows promising results
when compared with human performance.
This work can be extended in many ways. Given that most of the human-authored
labels are abstractive in nature, we plan to extend our labeling framework to generate
more abstract human-like labels that could better synthesize the information expressed in
a topic segment. A promising approach would be to rely on more sophisticated methods for
information extraction, combined with more semantics (e.g., phrase entailment) and datato-text generation techniques. Another interesting venue for future work is to perform a
more extrinsic evaluation of our methods. Instead of testing them with respect to a human
gold standard, it would be extremely interesting to see how effective they are when used to
support other NLP tasks, such as summarization and conversation visualization. We are
also interested in the future to transfer our approach to other similar domains by domain
adaptation methods. We plan to work on both synchronous and asynchronous domains.

24. Our annotated corpora, annotation manual and source code will be made publicly available from
www.cs.ubc.ca/labs/lci/bc3.html

564

fiTopic Segmentation and Labeling in Asynchronous Conversations

Bibliographic Note
Portions of this work were previously published in two conference proceedings (Joty et al.,
2010, 2011). This article significantly extends our previous work in several ways, most
notably: (i) we complete the topic modeling pipeline by presenting a novel topic labeling
framework (Section 3.2), (ii) we propose a new set of metrics for the topic labeling task
(Section 5.2), (iii) we present a new annotated corpus of blog conversations, and show how
the topic segmentation and labeling models perform on this new dataset (Section 4 and 5),
and (iv) we demonstrate the performance of the end-to-end system (Section 5.3).

Acknowledgments
This work was conducted at the University of British Columbia. We acknowledge the
funding support of NSERC Canada Graduate Scholarship (CGS-D), NSERC BIN Strategic
Network and NSERC discovery grant. We are grateful to the annotators for their great
effort. Many thanks to Gabriel Murray, Jackie Cheung, Yashar Mehdad, Shima Gerani,
Kelsey Allen and the anonymous reviewers for their thoughtful suggestions and comments.

Appendix A. Metrics for Topic Segmentation
A.1 One-to-One Metric
Consider the two different annotations of the same conversation having 10 sentences (denoted by colored boxes) in Figure 11(a). In each annotation, the topics are distinguished
by different colors. For example, the model output has four topics, whereas the human annotation has three topics. To compute one-to-one accuracy, we take the model output and
map its segments optimally (by computing the optimal max-weight bipartite matching) to
the segments of the gold-standard human annotation. For example, the red segment in the
model output is mapped to the green segment in the human annotation. We transform the
model output based on this mapping and compute the percentage of overlap as the one-toone accuracy. In our example, seven out of ten sentences overlap, therefore, the one-to-one
accuracy is 70%.
A.2 Lock Metric
Consider the model output (at the left most column) and the human annotation (at the
right most column) of the same conversation having 5 sentences (denoted by colored boxes)
in Figure 12. Similar to Figure 11, the topics in an annotation are distinguished using
different colors. Suppose we want to measure the loc3 score for the fifth sentence (marked
with yellow arrows at the bottom of the two annotations). In each annotation, we look at
the previous 3 sentences and transform them based on whether they have same or different
topics. For example, in the model output one of the previous three sentences is same (red),
and in the human annotation two of the previous three sentences are same (green), when
compared with the sentence under consideration. In the transformed annotations, same
topics are denoted by gray boxes and different topics are denoted by black boxes. We
565

fiJoty, Carenini, & Ng

One-to-One
accuracy

Model Human
output annotation

Transform model
output according to
optimal mapping

70%

Transformed Human
model output annotation

Model
output
(b)

(a)

Figure 11: Computing one-to-one accuracy.

566

fiTopic Segmentation and Labeling in Asynchronous Conversations

compute loc3 by measuring the overlap of the same or different judgments in the 3-sentence
window. In our example, two of three overlap, therefore, the loc3 agreement is 66.6%.

Loc3 accuracy

66.6%
Same or
different?

Model
Output

Same or
different?

Transformed
human annotation

Transformed
model output

Human
annotation

Figure 12: Computing loc3 accuracy.

References
Allan, J. (2002). Topic Detection and Tracking: Event-based Information Organization, pp.
116. Kluwer Academic Publishers, Norwell, MA, USA.
Allan, J., Wade, C., & Bolivar, A. (2003). Retrieval and Novelty Detection at the Sentence
Level. In Proceedings of the 26th annual international ACM SIGIR conference on
Research and development in informaion retrieval, SIGIR 03, pp. 314321, Toronto,
Canada. ACM.
Andrzejewski, D., Zhu, X., & Craven, M. (2009). Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceedings of the 26th Annual International
567

fiJoty, Carenini, & Ng

Conference on Machine Learning, ICML 09, pp. 2532, Montreal, Quebec, Canada.
ACM.
Aumayr, E., Chan, J., & Hayes, C. (2011). Reconstruction of threaded conversations in
online discussion forums. In Proceedings of the Fifth International AAAI Conference
on Weblogs and Social Media (ICWSM-11), pp. 2633.
Bangalore, S., Di Fabbrizio, G., & Stent, A. (2006). Learning the Structure of Task-Driven
Human-Human Dialogs. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pp. 201208. ACL.
Bansal, N., Blum, A., & Chawla, S. (2002). Correlation clustering. In Proceedings of the 43rd
Symposium on Foundations of Computer Science, FOCS 02, pp. 238, Washington,
DC, USA. IEEE Computer Society.
Baron, N. S. (2008). Always on: Language in an online and mobile world. Oxford ; New
York : Oxford University Press.
Barzilay, R., & Lee, L. (2004). Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL.
Beeferman, D., Berger, A., & Lafferty, J. (1999). Statistical models for text segmentation.
In Machine Learning, Vol. 34, pp. 177210, Hingham, MA, USA. Kluwer Academic
Publishers.
Blei, D., Ng, A., & Jordan, M. (2003). Latent Dirichlet Allocation. The Journal of Machine
Learning Research, 3, 9931022.
Blei, D. M., & Moreno, P. J. (2001). Topic segmentation with an aspect hidden markov
model. In Proceedings of the 24th annual international ACM SIGIR conference on
Research and development in information retrieval, SIGIR 01, pp. 343348, New York,
NY, USA. ACM.
Boyd-Graber, J., & Blei, D. M. (2008). Syntactic topic models. In Neural Information
Processing Systems.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24 (2), 123140.
Carbonell, J., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for
reordering documents and producing summaries. In Proceedings of the 21st annual
international ACM SIGIR conference on Research and development in information
retrieval, pp. 335336, Melbourne, Australia. ACM.
Carenini, G., Murray, G., & Ng, R. (2011). Methods for mining and summarizing text
conversations, Vol. 3. Morgan Claypool.
Carenini, G., Ng, R. T., & Zhou, X. (2007). Summarizing Email Conversations with Clue
Words. In Proceedings of the 16th international conference on World Wide Web, pp.
91100, Banff, Canada. ACM.
Carenini, G., Ng, R. T., & Zhou, X. (2008). Summarizing Emails with Conversational
Cohesion and Subjectivity. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 353361,
OH. ACL.
568

fiTopic Segmentation and Labeling in Asynchronous Conversations

Choi, F. Y. Y., Hastings, P. W., & Moore, J. (2001). Latent semantic analysis for text
segmentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural
Language Processing, EMNLP01, pp. 109117, Pittsburgh, USA. ACL.
Cortes, C., & Vapnik, V. N. (1995). Support Vector Networks. Machine Learning, 20,
273297.
Crystal, D. (2001). Language and the Internet. Cambridge University Press.
Dias, G., Alves, E., & Lopes, J. G. P. (2007). Topic Segmentation Algorithms for Text
Summarization and Passage Retrieval: an Exhaustive Evaluation. In Proceedings of
the 22nd national conference on Artificial intelligence - Volume 2, pp. 13341339,
Vancouver, BC, Canada. AAAI.
Eisenstein, J. (2009). Hierarchical text segmentation from multi-scale lexical cohesion. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, NAACL
09, pp. 353361, Stroudsburg, PA, USA. Association for Computational Linguistics.
Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP
08, pp. 334343, Honolulu, Hawaii. Association for Computational Linguistics.
Elsner, M., & Charniak, E. (2010). Disentangling chat. Computational Linguistics, 36,
389409.
Elsner, M., & Charniak, E. (2011). Disentangling chat with local coherence models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT 11, pp. 11791189, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Galley, M., McKeown, K., Fosler-Lussier, E., & Jing, H. (2003). Discourse segmentation of
multi-party conversation. In Proceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL 03, pp. 562569, Sapporo, Japan.
ACL.
Griffiths, T. L., Steyvers, M., Blei, D. M., & Tenenbaum, J. B. (2005). Integrating topics
and syntax. In Advances in Neural Information Processing Systems, pp. 537544. MIT
Press.
Harabagiu, S., & Lacatusu, F. (2005). Topic Themes for Multi-document Summarization.
In Proceedings of the 28th annual international ACM SIGIR conference on Research
and development in information retrieval, pp. 202209, Salvador, Brazil. ACM.
Hearst, M. A. (1997). TextTiling: segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23 (1), 3364.
Hsueh, P., Moore, J. D., & Renals, S. (2006). Automatic segmentation of multiparty dialogue. In the Proceedings of the 11th Conference of the European Chapter of the
Association for Computational Linguistics, EACL06, Trento, Italy. ACL.
Hulth, A. (2003). Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 conference on Empirical methods in natural language
processing, EMNLP 03, pp. 216223. Association for Computational Linguistics.
569

fiJoty, Carenini, & Ng

Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T.,
Shriberg, E., Stolcke, A., & Wooters, C. (2003). The ICSI Meeting Corpus. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSP-03), pp. 364367.
Joty, S., Carenini, G., & Lin, C. (2011). Unsupervised modeling of dialog acts in asynchronous conversations. In Proceedings of the twenty second International Joint Conference on Artificial Intelligence (IJCAI), Barcelona.
Joty, S., Carenini, G., Murray, G., & Ng, R. (2010). Exploiting conversation structure
in unsupervised topic segmentation for Emails. In Proceedings of the conference on
Empirical Methods in Natural Language Processing, EMNLP10, pp. 388398, Massachusetts, USA. ACL.
Joty, S., Carenini, G., Murray, G., & Ng, R. (2011). Supervised topic segmentation of Email
conversations. In Proceedings of the Fifth International AAAI Conference on Weblogs
and Social Media, ICWSM11, pp. 530533, Barcelona, Spain. AAAI.
Kim, S., Baldwin, T., & Kan, M. (2010a). Evaluating n-gram based evaluation metrics for
automatic keyphrase extraction. In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING10, pp. 572580, Beijing, China. ACL.
Kim, S. N., Medelyan, O., Kan, M.-Y., & Baldwin, T. (2010b). Semeval-2010 task 5 :
Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th
International Workshop on Semantic Evaluation, pp. 2126, Uppsala, Sweden. Association for Computational Linguistics.
Kleinbauer, T., Becker, S., & Becker, T. (2007). Combining Multiple Information Layers
for the Automatic Generation of Indicative Meeting Abstracts. In Proceedings of the
Eleventh European Workshop on Natural Language Generation, ENLG07, pp. 151
154, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lau, J., Grieser, K., Newman, D., & Baldwin, T. (2011). Automatic Labelling of Topic
Models. In Proceedings of the 49th annual meeting on Association for Computational
Linguistics, pp. 15361545, Portland, USA. ACL.
Lin, C.-Y. (2004). ROUGE: a package for automatic evaluation of summaries. In Proceedings
of Workshop on Text Summarization Branches Out, pp. 7481, Barcelona.
Liu, S., Zhou, M. X., Pan, S., Song, Y., Qian, W., Cai, W., & Lian, X. (2012). TIARA:
interactive, topic-based visual text summarization and analysis. ACM Trans. Intell.
Syst. Technol., 3 (2), 25:125:28.
Malioutov, I., & Barzilay, R. (2006). Minimum cut model for spoken lecture segmentation.
In Proceedings of the 21st International Conference on Computational Linguistics and
the 44th annual meeting of the Association for Computational Linguistics, ACL-44,
pp. 2532, Sydney, Australia. Association for Computational Linguistics.
Mayfield, E., Adamson, D., & Rose, C. P. (2012). Hierarchical conversation structure prediction in multi-party chat. In Proceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, SIGDIAL 12, pp. 6069, Stroudsburg, PA,
USA. Association for Computational Linguistics.
570

fiTopic Segmentation and Labeling in Asynchronous Conversations

Medelyan, O. (2009). Human-competitive automatic topic indexing. Ph.D. thesis, The
University of Waikato, Hamilton, New Zealand.
Medelyan, O., Frank, E., & Witten, I. H. (2009). Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP09, pp. 13181327, Singapore. Association for Computational Linguistics.
Mei, Q., Shen, X., & Zhai, C. (2007). Automatic labeling of Multinomial topic models.
In Proceedings of the 13th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 490499, California, USA. ACM.
Mihalcea, R., & Radev, D. (2011). Graph-based natural language processing and information
retrieval. Cambridge University Press.
Mihalcea, R., & Tarau, P. (2004). Textrank: Bringing order into text. In Proceedings of the
2004 Conference on Empirical Methods in Natural Language Processing, EMNLP04,
pp. 404411, Barcelona, Spain.
Minka, T. (1999). The dirichlet-tree distribution. Tech. rep., Justsystem Pittsburgh Research Center.
Morris, J., & Hirst, G. (1991). Lexical cohesion computed by thesaural relations as an
indicator of structure of text. Computational Linguistics, 17 (1), 2148.
Murphy, K. (2012). Machine Learning A Probabilistic Perspective. The MIT Press.
Nguyen, V.-A., Boyd-Graber, J., & Resnik, P. (2012). Sits: a hierarchical nonparametric
model using speaker identity for topic segmentation in multiparty conversations. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL 12, pp. 7887, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). The pagerank citation ranking:
Bringing order to the web.. Technical report 1999-66.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL02, pp. 311318, Philadelphia, Pennsylvania. Association for Computational Linguistics.
Passonneau, R. J., & Litman, D. J. (1997). Discourse segmentation by human and automated means. Computational Linguistics, 23 (1), 103139.
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - Measuring
the Relatedness of Concepts. In Proceedings of Fifth Annual Meeting of the North
American Chapter of the Association for Computational Linguistics (NAACL-04),
pp. 3841, Boston, MA.
Pevzner, L., & Hearst, M. A. (2002). A critique and improvement of an evaluation metric
for text segmentation. Computational Linguistics, 28 (1), 1936.
Purver, M. (2011). Topic segmentation. In Tur, G., & de Mori, R. (Eds.), Spoken Language
Understanding: Systems for Extracting Semantic Information from Speech, pp. 291
317. Wiley.
571

fiJoty, Carenini, & Ng

Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised
topic modelling for multi-party spoken discourse. In Proceedings of the ACL06, pp.
1724, Sydney, Australia. ACL.
Sacks, H., Schegloff, A., & Jefferson, G. (1974). A simplest systematics for the organization
of turn-taking for conversation. Language, 50, 696735.
Salton, G., & McGill, M. J. (1986). Introduction to Modern Information Retrieval. McGrawHill, Inc., New York, NY, USA.
Seneta, E. (1981). Non-negative Matrices and Markov Chains. Springer-Verlag.
Shi, J., & Malik, J. (2000). Normalized cuts and image segmentation. IEEE Trans. Pattern
Anal. Mach. Intell., 22 (8), 888905.
Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics, 27 (4), 521544.
Steyvers, M., & Griffiths, T. (2007). Latent Semantic Analysis: A Road to Meaning, chap.
Probabilistic topic models. Laurence Erlbaum.
Turney, P. D. (2000). Learning algorithms for keyphrase extraction. Information Retrieval,
2 (4), 303336.
Ulrich, J., Murray, G., & Carenini, G. (2008). A publicly available annotated corpus for
supervised email summarization. In EMAIL-2008 Workshop, pp. 428435. AAAI.
Verna, P. (2010). The blogosphere: Colliding with social and mainstream media. eMarketer.
Wallach, H. M. (2006). Topic modeling: beyond bag-of-words. In Proceedings of the 23rd
international conference on Machine learning, ICML 06, pp. 977984, Pittsburgh,
Pennsylvania. ACM.
Wang, H., Wang, C., Zhai, C., & Han, J. (2011). Learning online discussion structures
by conditional random fields. In Proceedings of the 34th international ACM SIGIR
conference on Research and development in Information Retrieval, SIGIR 11, pp.
435444, Beijing, China. ACM.
Wang, L., & Oard, D. W. (2009). Context-based message expansion for disentanglement of
interleaved text conversations. In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL 09, pp. 200208, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Zesch, T., & Gurevych, I. (2009). Approximate matching for evaluating keyphrase extraction. In Proceedings of the 7th International Conference on Recent Advances in
Natural Language Processing, RANLP09, pp. 484489, Borovets, Bulgaria.
Zhao, W. X., Jiang, J., He, J., Song, Y., Achananuparp, P., Lim, E.-P., & Li, X. (2011a).
Topical keyphrase extraction from twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies
- Volume 1, HLT 11, pp. 379388, Stroudsburg, PA, USA. Association for Computational Linguistics.
572

fiTopic Segmentation and Labeling in Asynchronous Conversations

Zhao, W. X., Jiang, J., Weng, J., He, J., Lim, E.-P., Yan, H., & Li, X. (2011b). Comparing
twitter and traditional media using topic models. In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR11, pp. 338349, Berlin,
Heidelberg. Springer-Verlag.
Zhou, D., Orshanskiy, S. A., Zha, H., & Giles, C. L. (2007). Co-ranking authors and
documents in a heterogeneous network. In Proceedings of the 2007 Seventh IEEE
International Conference on Data Mining, ICDM 07, pp. 739744, Washington, DC,
USA. IEEE Computer Society.

573

fiJournal of Artificial Intelligence Research 47 (2013) 649-695

Submitted 02/13; published 08/13

Protecting Privacy through Distributed Computation
in Multi-agent Decision Making
Thomas Leaute
Boi Faltings

thomas.leaute@a3.epfl.ch
boi.faltings@epfl.ch

Ecole Polytechnique Federale de Lausanne (EPFL)
Artificial Intelligence Laboratory (LIA)
Station 14
CH-1015 Lausanne, Switzerland

Abstract
As large-scale theft of data from corporate servers is becoming increasingly common, it
becomes interesting to examine alternatives to the paradigm of centralizing sensitive data
into large databases. Instead, one could use cryptography and distributed computation so
that sensitive data can be supplied and processed in encrypted form, and only the final
result is made known. In this paper, we examine how such a paradigm can be used to
implement constraint satisfaction, a technique that can solve a broad class of AI problems
such as resource allocation, planning, scheduling, and diagnosis. Most previous work on
privacy in constraint satisfaction only attempted to protect specific types of information,
in particular the feasibility of particular combinations of decisions. We formalize and
extend these restricted notions of privacy by introducing four types of private information,
including the feasibility of decisions and the final decisions made, but also the identities of
the participants and the topology of the problem. We present distributed algorithms that
allow computing solutions to constraint satisfaction problems while maintaining these four
types of privacy. We formally prove the privacy properties of these algorithms, and show
experiments that compare their respective performance on benchmark problems.

1. Introduction
Protecting the privacy of information is becoming a crucial concern to many users of the
increasingly ubiquitous Information and Communication Technologies. Companies invest a
lot of effort into keeping secret their internal costs and their future development strategies
from other actors on the market, most importantly from their competitors. Individuals also
have a need for privacy of their personal information: for instance, carelessly disclosing ones
activity schedule or location might reveal to burglars opportunities to break into ones home.
On the other hand, accessing and using such private information is often necessary to solve
problems that depend on these data. In the context of supply chain management, companies
need to exchange information with their contractors and subcontractors about the quantities
of goods that must be produced, and at what price. When scheduling meetings or various
events with friends or co-workers, individuals are confronted with the challenge of taking
coordinated scheduling decisions, while protecting their respective availability schedules.
Artificial Intelligence can be a crucial tool to help people make better decisions under
privacy concerns, by delegating part or all of the decision problem to personal intelligent
agents executing carefully chosen algorithms that are far too complex to be performed
2013 AI Access Foundation. All rights reserved.

fiLeaute & Faltings

by the human alone. In particular, the framework of Constraint Satisfaction Problems
(CSPs) is a core AI technology that has been successfully applied to many decision-making
problems, from configuration to scheduling, to solving strategic games. Here we show how
distributed AI algorithms can be used to solve such CSPs, while providing strong guarantees
on the privacy of the problem knowledge, through the use of techniques borrowed from
cryptography. This makes it possible to solve coordination problems that depend on secret
data, without having to reveal these data to other parties. On the other hand, distributed,
encrypted computation involving message exchange has a cost in terms of performance,
such that a suitable tradeoff between privacy and scalability must be found.
1.1 Motivating Examples
In this paper, we present a set of novel, privacy-protecting algorithms for Distributed Constraint Satisfaction Problems (DisCSPs), a wide class of multi-agent decision-making problems with applications to many problems such as configuration, scheduling, planning, design
and diagnosis. We consider three examples to illustrate the privacy requirements that might
arise: meeting scheduling, airport slot allocation, and computing game equilibria.
In a meeting scheduling problem (Maheswaran, Tambe, Bowring, Pearce, & Varakantham, 2004), a number of meetings need to be scheduled, involving possibly overlapping sets
of participants. Taking into account their respective availability constraints, all participants
to any given meeting must agree on a time for the meeting. One given participant can be
involved in multiple meetings, which creates constraints between meetings. In this problem class, participants usually want to protect the privacy of their respective availability
schedules, as well as the lists of meetings they are involved in.
Another problem class is airport slot allocation (Rassenti, Smith, & Bulfin, 1982), where
airlines express interests in combinations of takeoff and landing time slots at airports, corresponding to possible travel routes for their aircraft. While the end goal for the airports is
to efficiently allocate their slots to airlines, from the point of view of the airlines it is crucial
that the combinations of slots they are interested in remain private, because they indicate
the routes they intend to fly, which is sensitive strategic information that they want to hide
from their competitors.
Finally, consider the general class of one-shot strategic games, such as the party game
(Singh, Soni, & Wellman, 2004): the players are invited to a party, and must decide whether
to attend, based on their respective intrinsic costs of attendance, and on whether the people
they like or dislike also choose to attend. Players would best play strategies that form a
Nash equilibrium, where no single player can be better off by deviating from its chosen
strategy. The problem of computing such an equilibrium is a typical example of a multiagent decision-making problem, in which privacy is an issue: players do not necessarily want
to reveal their attendance costs, nor whether they like or dislike another invitee.
1.2 Four Types of Private Information
As can be seen in the previous examples, the information that participants would like to
keep private can differ in nature; we propose to classify it into four privacy types. We only
briefly introduce and illustrate them here; more formal definitions are given in Section 2.2.1.
650

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

1. Agent privacy relates to the identities of the participants. Consider for instance a
CEO who wants to schedule two meetings respectively with a journalist and with
another companys CEO. Revealing to the journalist the other CEOs involvement in
the decision-making problem could leak out the companies plans to merge. In this
case agent privacy can be considered critical.
2. Topology privacy covers information about the presence of constraints. This is the
type of critical information that airline companies want to keep secret in the airport
slot allocation problem: the presence of a constraint between an airline and a specific
airport reveals the airlines strategic plans to offer flights to and from this airport.
3. Constraint privacy is about the nature of the constraints. This covers for instance
the participants availability schedules in the meeting scheduling problem, and, in the
party game, whether a player likes or dislikes other invitees.
4. Decision privacy has to do with the solution that is eventually chosen to the problem.
Depending on the problem class, this type of privacy may or may not be relevant. In
the meeting scheduling problem, the time chosen for each meeting necessarily has to
be revealed to all participants of the meeting; however it can be desirable to hide this
information from non-attendees.
Like in previous work on privacy in DisCSP, we assume that the participants are honest,
but curious (Goldreich, 2009), in that they honestly follow the algorithm, but are interested
in learning as much as possible from other agents private information based on the messages exchanged. Note that this honesty assumption does not mean that all agents are
assumed to faithfully report their true constraints to the algorithm; they may be tempted
to strategize by reporting slightly different constraints, hoping that this would lead the algorithm to select a solution to the problem that they deem preferable to them. This issue of
incentive-compatibility has been addressed in related work such as by Petcu, Faltings, and
Parkes (2008), and is orthogonal to the issue of privacy addressed in this paper. Furthermore, an agent would take a risk in reporting constraints different from its true constraints:
reporting relaxed constraints could yield a solution that violates its true constraints and
would therefore not be viable, while reporting tighter constraints could make the overall
problem infeasible and the algorithm fail to find any solution at all.
On the other hand, our algorithms depart from previous work in two respects. First,
previous work almost exclusively focused on constraint privacy, most often ignoring agent,
topology and decision privacy. We show how to address all four types, and the algorithms
we propose correspond to various points in the tradeoff between different levels of privacy
and efficiency. Second, while most of the literature focuses on quantitatively measuring
and reducing the amount of privacy loss in various DisCSP algorithms, we have developed
algorithms that give strong guarantees that certain pieces of private information will not be
leaked. In contrast, in previous privacy-protecting algorithms, it is typically the case that
any piece of private information may be leaked with some (small) probability.
The rest of this paper is organized as follows. Section 2 first formally defines the DisCSP
framework and the four aforementioned types of privacy. Section 3 then presents a first
algorithm, called P-DPOP+. Section 4 then describes the P3/2 -DPOP+ algorithm, which is
651

fiLeaute & Faltings

a variant that achieves a higher level of decision privacy, at the expense of an additional
computational overhead. Another variant, called P2 -DPOP+, is introduced in Section 5 in
order to further improve constraint privacy. Finally, Section 6 compares the performance
of these algorithms with the previous state of the art, on several classes of benchmarks.

2. Preliminaries
This section first formally defines the DisCSP framework (Section 2.1), and then introduces
four types of privacy (Section 2.2).
2.1 Distributed Constraint Satisfaction
After providing a formal definition of Distributed Constraint Satisfaction (Section 2.1.1),
we recall some existing algorithms for DisCSP and its optimization variant (Section 2.1.2).
2.1.1 Definition
A Distributed Constraint Satisfaction Problem can be formally defined as follows.
Definition 1 (DisCSP). A discrete DisCSP is a tuple < A, X , a, D, C >:
 A = {a1 , ..., ak } is a set of agents;
 X = {x1 , ..., xn } is a set of variables;
 a : X  A is a mapping that assigns the control of each variable xi to an agent a(xi );
 D = {D1 , ..., Dn } is a set of finite variable domains; variable xi takes values in Di ;
 C = {c1 , ..., cm } is a set of constraints, where each ci is a s(ci )-ary function of scope
(xi1 ,    , xis(ci ) ), ci : Di1  ..  Dis(ci )  {false, true}, assigning false to infeasible
tuples, and true to feasible ones.
V
A solution is a complete assignment such that the conjunction ci C ci = true, which is
the case exactly when the assignment is consistent with all constraints.
Some of the important assumptions of the DisCSP framework are the following. First,
we assume that all the details of a given constraint ci are known to all agents involved; if an
agent wants to keep some constraints private, it should formulate them in such a way that
they only involve variables it controls. Furthermore, we assume that two neighboring agents
(i.e. agents that share at least one constraint) are able to communicate with each other
securely, and that messages are delivered in FIFO order and in finite time. On the other
hand, we assume that two non-neighboring agents initially ignore everything about each
other, even including their involvement in the problem. In particular, a DisCSP algorithm
that protects agent privacy should not require them to communicate directly, nor should it
even allow them to discover each others presence. Finally, we assume each agent honestly
follows the protocol, and we focus on preventing private information leaks to other agents.
Figure 1 introduces a simple graph coloring problem instance that will be used to illustrate the algorithms throughout the rest of this paper. We assume that the five nodes in
652

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

R 6= x1

6=

6=
x2

x4 6= B
6=

6=

x3

6=

x5 6 {B, R}

Figure 1: The DisCSP constraint graph for a simple graph coloring problem instance.

the graph correspond to five different agents, which must each choose a color among red,
blue and green. These decisions are modeled by the five variables x1 , . . . , x5 with domains
{R, B, G}. Each agent may express a secret, unary constraint on its variable; for instance,
x1 does not want to be assigned the color red. Binary, inequality constraints are imposed
between each pair of neighboring nodes, and are only known to the two agents involved.
Distributed Constraint Optimization (DCOP) is an extension of the DisCSP formalism,
in which constraints specify not only which variable assignments are feasible or infeasible,
but also assign costs (or utilities) to these assignments. An (optimal) solution to such a
DCOP is then one that minimizes the sum of all costs (or maximizes the sum of all utilities).
The algorithms in this paper can easily be generalized to solve DCOPs, with a complexity
increase that is at most linear in an upper bound on the (assumed integer) cost of the
optimal solution. Such a generalization is left outside the scope of this paper for the sake
of conciseness, and has been addressed by Leaute and Faltings (2011) and Leaute (2011).
2.1.2 Complete Algorithms for DisCSPs
A range of distributed algorithms exist in the literature to solve DisCSPs and DCOPs.
They can be seen as belonging to two classes, depending on how they order variables. The
largest class consists of algorithms that order the variables along a linear order, such as
ABT (Yokoo, Durfee, Ishida, & Kuwabara, 1992), AWC (Yokoo, 1995), SynchBB (Hirayama & Yokoo, 1997), AAS (Silaghi, Sam-Haroud, & Faltings, 2000), AFC (Meisels &
Zivan, 2003), DisFC (Brito & Meseguer, 2003), (Comp)APO (Mailler & Lesser, 2003; Grinshpoun & Meisels, 2008), ConcDB (Zivan & Meisels, 2004), AFB (Gershman, Meisels, &
Zivan, 2006) and ConcFB (Netzer, Meisels, & Grubshtein, 2010). The linear order may be
chosen and fixed initially before the algorithm is run, or dynamically revised online.
In the second class, variables are ordered along a tree-based partial order. This includes
ADOPT (Modi, Shen, Tambe, & Yokoo, 2005) and its variants such as BnB-ADOPT (Yeoh,
Felner, & Koenig, 2010) and BnB-ADOPT+ (Gutierrez & Meseguer, 2010), DPOP (Petcu
& Faltings, 2005) and its countless variants, and NCBB (Chechetka & Sycara, 2006), which
all order the variables following a pseudo-tree (Definition 2). Among the aforementioned
pseudo-tree-based algorithms, DPOP is the only one using Dynamic Programming (DP),
while all others are based on search. Other algorithms have been proposed that perform DP
on different partial variable orders: Action-GDL uses junction trees (Vinyals, RodrguezAguilar, & Cerquides, 2010), and DCTE cluster trees (Brito & Meseguer, 2010).
653

fiLeaute & Faltings

2.1.3 The DPOP Algorithm
The DPOP algorithm was originally designed to solve optimization problems (DCOPs) and
described in terms of utility maximization. One way to apply it to pure satisfaction problems
(DisCSPs) is to first reformulate the DisCSP into a Max-DisCSP, in which the constraints
are no longer boolean but rather take values in {0, 1}, where 0 stands for feasibility and
1 for infeasibility. The cost-minimizing variant of DPOP (described below) can then be
applied to find a solution with minimal cost, where the cost (hereafter called feasibility
value) corresponds to the number of constraint violations (which we want to be equal to 0).
Overview of the Algorithm DPOP is an instance of the general bucket elimination
scheme by Dechter (2003), performed distributedly (Algorithm 1). It requires first arranging
the constraint graph into a pseudo-tree, formally defined as follows.
Definition 2 (Pseudo-tree). A pseudo-tree is a generalization of a tree, in which a node is
allowed to have links (back-edges) with remote ancestors (pseudo-parents) and with remote
descendants (pseudo-children), but never with nodes in other branches of the tree.
A pseudo-tree arrangement of the constraint graph in Figure 1 is illustrated in Figure 2.
This pseudo-tree naturally decomposes the original problem into two, loosely coupled subproblems, corresponding to the two branches, which will perform the rest of the algorithm
in parallel. Figure 2 also shows the FEAS messages (originally called UTIL messages in the
context of utility maximization) that are exchanged during the propagation of feasibility
values, following a multi-party dynamic programming computation (lines 1 to 12). In this
Algorithm 1 Overal DPOP algorithm, for variable x
Require: a pseudo-tree ordering of the variables; px denotes xs parent
1: // (UTIL propagation) Propagate feasibility values up the pseudo-tree:
2: m(x, px , )  c{c C | xscope(c)  scope(c )(childrenx pseudo childrenx )=} c(x, )
// Join with received messages:
for each yi  childrenx do
5:
Wait for the message (FEAS, mi (x, )) from yi
6:
sepyi  scope(mi )
7:
m(x, px , )  m(x, px , ) + mi (x, )
3:
4:

// Project out x:
if x is not the root variable then
10:
x (px , )  arg minx {m(x, px , )}
11:
Send the message (FEAS, m(x (px , ), px , )) to px
12: else x  arg minx {m(x)} // m(x, px , ) actually only depends on x
8:

9:

13:
14:
15:
16:
17:

// (VALUE propagation) Propagate decisions top-down along the pseudo-tree:
if x is not the root then
Wait for message (DECISION, px , ) from parent px
x  x (px = px , )
for each yi  childrenx do send message (DECISION, sepyi ) to yi
654

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

x2

x3  x2
R B G
0 1 0
x5  x3
x3
R
0
B
0
G
1

x4  x3
x2
x3 R B G
R
0 1 0
B
0 0 0
G
0 0 0

x2

x3

x5

x4

x1

x1  x4
x2
x4 R B G
R
0 0 0
B
0 0 1
G
0 1 0

Figure 2: Multiparty dynamic programming computation of a feasible value for x2 , based
on a pseudo-tree arrangement of the constraint graph in Figure 1. The dashed
edge represents a back-edge with a pseudo-parent in the pseudo-tree.

part of the algorithm, all messages travel bottom-up along tree edges. Consider for instance
the message sent by agent a(x5 ) to its parent agent a(x3 ). This message is the result of
the projection (lines 10 and 11) of variable x5 out of the conjunction (line 2) of x5 s two
constraints x5 6= x3 and x5 6= B, and summarizes the minimal number of constraint violations that a(x5 ) can achieve, as a function of the ancestor variable x3 . More generally,
each message sent by a variable x summarizes the minimal number of constraint violations
achievable for the aggregate subproblem owned by the entire subtree rooted at x, as a
function whose scope is called the separator of x (line 6). In DPOP, the separator of x necessarily includes xs parent px , and potentially other ancestor variables; this is indicated by
the notation m(px , ). For instance, the message x4  x3 summarizes the minimal number
of constraint violations achievable for the entire subtree rooted at x4 , as a function of x4 s
separator {px4 = x3 , x2 }. Notice that the separator of a variable x can contain variables
that are not neighbors of x; for example, x2 is in x4 s separator because a descendent of x4
has a constraint with x2 . In the privacy-aware algorithms presented later in this paper, this
notion of separator is extended to allow for separators that do not necessarily include the
parent variable, and that may include multiple codenames referring to the same variables,
which might not necessarily be ancestors in the pseudo-tree.
Upon receiving the messages x5  x3 and x4  x3 (line 5), agent a(x3 ) joins them
(line 7) with its constraint x3 6= x2 . Variable x3 is then projected out of the resulting joint
table, which produces the message x3  x2 (lines 10 and 11). At the end of this feasibility
propagation (line 12), the root variable x2 chooses a value x2 for itself that minimizes the
number of constraint violations over the entire problem (e.g. x2 = R). This decision can
then be propagated downwards along tree-edges via DECISION messages (originally called
VALUE messages) until all variables have been assigned optimal values (lines 13 to 17).
655

fiLeaute & Faltings

Complexity Given a pseudo-tree ordering of the n variables, DPOPs bottom-up and
top-down phases each exchange exactly (n  1) messages (one through each tree edge).
However, while each DECISION message contains at most (n  1) variable assignments, the
FEAS message sent by a given variable x can contain exponentially many feasibility values,
because it contains a table representation of a function m of |sepx | variables. The size of
sepmax
the largest FEAS message is therefore O(Dmax
), where Dmax is the size of the largest
variable domain, and sepmax = maxx |sepx | < n  1 is the width of the pseudo-tree. In the
best case, the width is equal to the treewidth of the constraint graph; however finding a
pseudo-tree that achieves this minimal width is NP-hard. In practice, the pseudo-tree is
generated by a heuristic, distributed, depth-first traversal of the constraint graph (Online
Appendix 1), producing a so-called DFS tree that is a pseudo-tree in which all parentchild relationships are between neighbors in the constraint graph. Since DPOP exchanges
(n  1) FEAS messages, its overall complexity in terms of runtime (measured in number of
sepmax
constraint checks), memory, and information exchange is O(n  Dmax
).
Privacy Properties The privacy-aware algorithms in Section 3 are based on DPOP,
because of two desirable properties that allow for higher levels of privacy. First, DPOP
only requires message exchanges between neighboring agents, provided that the pseudotree used is a DFS tree; this is necessary to protect agent privacy. Greenstadt, Pearce,
and Tambe (2006) made the opposite claim that pseudo-trees are detrimental to privacy
compared to linear orderings; however this claim is only valid if the only type of privacy
considered is constraint privacy, and does not hold if agent privacy and topology privacy
are guaranteed, i.e. if the pseudo-tree is not publicly known to all agents. The second, DPinherited property is that DPOPs performance does not depend on constraint tightness,
i.e. how easy or hard it is to satisfy each constraint. For all other, search-based algorithms,
inferences on the constraint tightness can be made by observing the runtime or the amount of
information exchanged (Silaghi & Mitra, 2004). In the case of meeting scheduling problems,
constraint tightness maps directly to the participants levels of availability, which is private
information. In application domains where this leak of constraint tightness is tolerable,
algorithms based on search rather than DP can be used, and many of the privacy-enhancing
techniques presented in this paper for DPOP are also applicable to search-based algorithms.
2.2 Privacy in DisCSPs
Section 2.2.1 formally defines the four types of privacy considered in this paper. Section 2.2.2
then recalls previous work that attempted to address various subsets of these privacy types.
2.2.1 Privacy Definitions
Definition 3 introduces the concept of semi-private information (Faltings, Leaute, & Petcu,
2008), which may inevitably be leaked by any DisCSP algorithm.
Definition 3 (Semi-private information). Semi-private information refers to information
about the problem and/or its solution that an agent might consider private, but that can
inevitably be leaked to other agents by their views of the chosen solution to the DisCSP.
In other words, semi-private information covers everything a given agent can discover
about other agents by making inferences simply based on its initial knowledge of the problem
656

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

and on the values its variables take in the solution. For instance, in a graph coloring problem
involving only two colors, each node can infer the color of each of its neighbors from the
color it was assigned in the chosen solution, provided that the solution is correct. Excluding
semi-private information, we now distinguish four types of private information that agents
may desire to protect (Faltings et al., 2008).
Definition 4 (Agent privacy). No agent should be able to discover the identity, or even the
existence of non-neighboring agents. A particular consequence of this type of privacy is that
two agents should only be allowed to communicate directly if they share a constraint.
In Figure 1, this means for instance that agent a(x1 ) should not be able to discover the
existence and identities of agents a(x3 ) and a(x5 ). Even if no two non-neighboring agents
communicate directly, agent privacy might still be leaked by the contents of messages; in
this paper we propose a method based on codenames to fully protect agent privacy.
Definition 5 (Topology privacy). No agent should be able to discover the existence of
topological constructs in the constraint graph, such as nodes (i.e. variables), edges (i.e.
constraints), or cycles, unless it owns a variable involved in the construct.
In Figure 1, topology privacy means for instance that agent a(x1 ) should not discover
how many other neighbors x2 has besides itself. However, a(x1 ) might discover the existence
of a cycle involving x1 , x2 and x4 . This is tolerated because x1 is involved in this cycle, but
a(x1 ) should not discover the length of the cycle (i.e. that x2 and x4 share a neighbor).
Definition 6 (Constraint privacy). No agent should be able to discover the nature of a
constraint that does not involve a variable it owns.
In Figure 1, an example of a breach in constraint privacy would be if agent a(x1 ) were
able to discover that agent a(x4 ) does not want to be assigned the color blue. This is the
type of privacy that the DisCSP literature mostly focuses on.
Definition 7 (Decision privacy). No agent should be able to discover the value that another
agents variable takes in the chosen solution (modulo semi-private information).
In a distributed graph coloring problem, this means that no agent can discover the color
of any neighbor (let alone any non-neighboring agent) in the solution chosen to the problem.
2.2.2 Previous Work on Privacy in DisCSP
Before discussing what information may be leaked by a given algorithm, and how to prevent
it, it is important to clarify what information is assumed to be initially known to each agent.
Initial Knowledge Assumptions In this paper, we use the following three assumptions,
which are currently the most widely used in the DisCSP literature.
1. Each agent a knows all agents that own variables that are neighbors of as variables,
but does not know any of the other agents (not even their existence);
2. A variable and its domain are known only to its owner agent and to the agents owning
neighboring variables, but the other agents ignore the existence of the variable;
657

fiLeaute & Faltings

3. A constraint is fully known to all agents owning variables in its scope, and no other
agent knows anything about the constraint (not even its existence).
Brito and Meseguer (2003) introduced Partially Known Constraints (PKCs), whose
scopes are known to all agents involved, but the knowledge of whose nature (which assignments are allowed or disallowed) is distributed among these agents. This is a relaxation of
Assumption 3; however it is worth noting that the algorithms presented in this paper can
still support PKCs without introducing privacy leaks by enforcing this assumption, because
any PKC can be decomposed into a number of constraints over copy variables such that
Assumption 3 holds. For instance, if agents a1 . . . an share the knowledge of a unary PKC
over variable x, then this constraint can be decomposed into n unary constraints, such
that each constraint ci is known fully and only to agent ai and is expressed over a copy
variable xi owned by ai . Equality constraints are added to the problem to enforce equality
of all copy variables. However, the introduction of copy variables can be detrimental to
decision privacy. Grubshtein, Grinshpoun, Meisels, and Zivan (2009) later proposed the
similar concept of asymmetric constraints, which can also be reformulated as symmetric
constraints over copy variables for the purpose of applying our algorithms.
Other previous work adopted a dual approach, assuming that variables are public and
known to all agents, but each constraint is known to only one agent (Silaghi et al., 2000;
Yokoo, Suzuki, & Hirayama, 2002; Silaghi, 2005a). Silaghi (2005b) even proposed a framework in which the constraints are secret to everyone. This dual approach has the disadvantage of necessarily violating topology privacy, since all variables are public.
Measuring Constraint Privacy Loss Most of the literature on privacy in DisCSPs
focuses on constraint privacy. Metrics have been proposed to evaluate constraint privacy
loss in algorithms, in particular for distributed meeting scheduling (Franzin, Freuder, Rossi,
& Wallace, 2004; Wallace & Freuder, 2005). Maheswaran, Pearce, Bowring, Varakantham,
and Tambe (2006) designed a framework called Valuation of Possible States (VPS) that
they used to measure constraint privacy loss in the OptAPO and SynchBB algorithms, and
they considered the impact of whether the problem topology is public or only partially
known to the agents. Greenstadt et al. (2006) also applied VPS to evaluate DPOP and
ADOPT on meeting scheduling problems, under the assumption that the problem topology
is public. Doshi, Matsui, Silaghi, Yokoo, and Zanker (2008) proposed to consider the cost of
privacy loss in optimization problems, in order to elegantly balance privacy and optimality.
Preventing Constraint Privacy Loss Some previous work also proposed approaches to
partially reduce constraint privacy loss. For instance, Brito and Meseguer (2007) described
a modification of the Distributed Forward Checking (DisFC) algorithm for DisCSPs in which
agents are allowed to lie for a finite time in order to achieve higher levels of privacy. However, the performance of most search-based algorithms like DisFC leaks information about
constraint tightness, as explained at the end of Section 2.1.3. To avoid this subtle privacy
leak, one must either perform full exhaustive search, which is the option chosen by Silaghi,
or resort to Dynamic Programming, which is the option we have chosen in this paper.
The cryptographic technique of secret sharing (Shamir, 1979; Ben-Or, Goldwasser, &
Wigderson, 1988) was also applied by Silaghi, Faltings, and Petcu (2006) and Greenstadt,
Grosz, and Smith (2007) to lower constraint privacy in DPOP, assuming that the constraint
graph topology is public knowledge. Cryptography has also been applied to provide strong
658

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

guarantees on constraint privacy preservation in multi-agent decision making. For instance,
Yokoo and Suzuki (2002), Yokoo et al. (2002) and Yokoo, Suzuki, and Hirayama (2005)
showed how a public key encryption scheme can be used to solve DisCSPs using multiple
servers, while protecting both constraint privacy and decision privacy. Bilogrevic, Jadliwala,
Hubaux, Aad, and Niemi (2011) solved single-meeting scheduling problems using similar
techniques, and one semi-trusted server. In this paper however, we only consider algorithms
that do not make use of third parties, as such third parties might not be available. Herlea,
Claessens, Preneel, Neven, Piessens, and Decker (2001) showed how to use Secure Multiparty
Computation (SMC) 1 to securely schedule a single meeting, without relying on servers. In
SMC, agents collaboratively compute the value of a given, publicly known function on
private inputs, without revealing the inputs. For Herlea et al. (2001), the inputs are each
participants availability at a given time, and the function outputs whether they are all
available.
The MPC-DisCSP4 Algorithm Silaghi (2005a) also applied SMC to solve general
DisCSPs, where the private inputs are the agents constraint valuations, and the function
returns a randomly chosen solution. The algorithm proceeds as follows (Leaute, 2011). Each
agent ai first creates a vector Fi with one entry per candidate solution to the DisCSP, equal
to 1 if the candidate solution satisfies ai s private constraints, and to 0 otherwise. To reduce
the size of Fi , the candidate solutions may be filtered through publicly known constraints,
if there exists any. Using Shamirs polynomial secret sharing technique (Shamir, 1979; BenOr et al., 1988), agent ai then sends one secret share Fij of its vector Fi to each other
agent aj , and receives corresponding secret shares Fji of their respective vectors. Agent ai
then multiplies together all the secret shares it received. The multiplication of Shamir secret
shares is a non-trivial operation, because each secret share is the value of a polynomial, and
multiplying two polynomials increases the degree of the output, which must always remain
lower than the number |A| of agents to be resolvable. Therefore, after each multiplication
of two secret shares, agent ai must perform a complex sequence of operations involving the
exchange of messages in order to reduce the degree of the output.
After performing (|A|  1) such pairwise multiplications of secret shares, agent ai s vector Fi contains secret shares of 1 at the entries corresponding to globally feasible solutions.
Agent ai then performs a transformation on Fi so that only one such secret share of 1
remains, identifying one particular feasible solution (if there exists one). Just selecting the
first such entry would a posteriori reveal that all previous entries correspond to infeasible
solutions to the DisCSP; to prevent this privacy leak, the vector Fi is first collaboratively,
randomly permuted using a mix-net. Agent ai then performs a sequence of iterative operations on Fi (including communication-intensive multiplications) to set all its entries to
secret shares of 0, except for one secret share of 1 corresponding to the chosen solution
to the DisCSP (if any). The vector Fi is then un-shuffled by re-traversing the mix-net in
reverse. Finally, agent ai can compute secret shares of the domain index of each variables
chosen assignment, and reveal these secret shares only to the owners of the variables.
This algorithm has numerous drawbacks. First, each agent must know all variables and
their domains to construct its initial vector Fi , which immediately violates agent privacy
and topology privacy (Table 3.3, page 664). Second, Shamirs secret sharing scheme is a
1. Silaghi uses the different acronym MPC for the same concept.

659

fiLeaute & Faltings

majority threshold scheme, which means that if at least half of the agents collude, they
can discover everyones private information. Even though, in this paper, we are assuming
that agents are honest and do not collude, a consequence of this threshold is that this
scheme does not provide any privacy guarantee when the problem involves only two agents.
Third, this algorithm is often only practical for very small problems, because it performs
full exhaustive search; this is demonstrated by our experimental results in Section 6.

3. P-DPOP+ : Full Agent Privacy and Partial Topology, Constraint and
Decision Privacy
This section describes a variant of the DPOP algorithm that guarantees full agent privacy.
It also partially protects topology, constraint, and decision privacy. Algorithm 2 is an
improvement over the P-DPOP algorithm we originally proposed (Faltings et al., 2008).
Like DPOP, the algorithm performs dynamic programming on a DFS-tree ordering of the
variables (Figure 2). Algorithms to first elect one variable, and then generate a DFS tree
rooted at this variable are given in Online Appendices 1 and 2. These algorithms do not
reveal the pseudo-tree in its entirety to any agent; instead, each agent only discovers the
(pseudo-)parents and (pseudo-)children of its own variables. For the sake of simplicity, we
will hereafter assume without loss of generality that the constraint graph consists of a single
component. If the problem actually consisted of two or more fully decoupled subproblems,
then each subproblem would be solved in parallel, independently from the others.
Algorithm 2 Overal P-DPOP+ algorithm, for variable x
Require: a DFS-tree ordering of the variables
1: // Choose and exchange codenames for x and its domain Dx :
2: Wait for a message (CODES, yix , Dyxi , yxi ) from each yi  {parentx }  pseudo parentsx
3: for each yi  childrenx  pseudo childrenx do
4:
xyi  large random number
5:
Dxyi  list of |Dx | random, unique identifiers
6:
xyi  random permutation of [1, . . . , |Dx |]
7:
Send message (CODES, xyi , Dxyi , xyi ) to yi

12:

// Choose and exchange obfuscation key for x:
Wait for and record a message (KEY, keyyxi ) from each yi  pseudo parentsx (if any)
for each yi  pseudo childrenx do
keyxyi  vector of large random numbers of B bits, indexed by Dx
Send message (KEY, keyxyi ) to yi

13:

Propagate feasibility values up the pseudo-tree (Algorithm 3, Section 3.1)

14:

// Propagate decisions top-down along the pseudo-tree (Section 3.2):
if x is not the root then
Wait for message (DECISION, px , ) from parent px
x  x (px = px , ) // where x () was computed in Algorithm 3, line 21
for each yi  childrenx do
 y ) to yi , with sepyi from Algorithm 3, line 12
Send message (DECISION, sep
i

8:
9:
10:
11:

15:
16:
17:
18:
19:

660

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

3.1 Finding a Feasible Value for the Root Variable
As already illustrated for DPOP in Section 2.1.3, the agents perform a bottom-up propagation of feasibility values along the pseudo-tree. This is done in Algorithm 3, which is an
extension of DPOPs UTIL propagation phase (the extensions are indicated by comments
in bold), and improves over the algorithm we originally proposed (Faltings et al., 2008)
by patching an important constraint privacy leak in the single-variable FEAS messages
sent by variables with singleton separators. The following sections describe the obfuscation
techniques used to protect the private information that could be leaked by the feasibility
messages, using codenames (Section 3.1.1) and addition of random numbers (Section 3.1.2).
Algorithm 3 Algorithm to find a feasible value for the root of a DFS tree, for variable x
Require: a DFS-tree ordering of the variables; px denotes xs parent
1: // Join local constraints:
2: m(x, px , )  c{c C | xscope(c)  scope(c )(childrenx pseudo childrenx )=} c(x, )
3:
4:
5:

// Apply codenames:
for each yi  {px }  pseudo parentsx do
m(x, px , )  replace (yi , Dyi ) in m(x, px , ) with (yix , Dyxi ) from Algorithm 2, line 2,
and apply the permutation yxi to Dyxi

// Obfuscate infeasible entries:
r  large, positive,
random number of B bits

m(x, px , )
if m(x, px , ) = 0
8: m(x, px , ) 
m(x, px , ) + r if m(x, px , ) > 0
6:
7:

9:
10:
11:
12:
13:
14:
15:

// Join with received messages:
for each yi  childrenx do
Wait for the message (FEAS, mi (x, )) from yi
sepyi  scope(mi )
for each z  childrenx  pseudo childrenx do // resolve codenames
mi (x, )  identify (xz , Dxz ) as (x, Dx ) in mi (x, ) (if xz is present)
m(x, px , )  m(x, px , ) + mi (x, )

// De-obfuscate feasibility values with respect to x:
17: for each yi  pseudo childrenx do
18:
m(x, px , )  m(x, px , )  keyxyi (x) // with keyxyi from Algorithm 2, line 11
16:

// Project out x:
if x is not the root variable then
21:
x (px , )  arg minx {m(x, px , )}
22:
m(px , )  minx {m(x, px , )}
19:

20:

23:
24:
25:
26:
27:

// Obfuscate feasibility values:
for each yi  pseudo parentsx do
m(px , )  m(px , ) + keyyxi (yix ) // with keyyxi from Algorithm 2, line 9
Send the message (FEAS, m(px , )) to px
else x  arg minx {m(x)} // m(x, px , ) actually only depends on x
661

fiLeaute & Faltings

3.1.1 Hiding Variable Names and Values Using Codenames
Consider the feasibility message x1  x4 sent by agent a(x1 ) to its parent variable x4 in
Figure 2. This message is recalled in Figure 3(a), reformulated in terms of minimizing the
number of constraint violations. If this message were actually received in cleartext, it would
breach agent privacy and topology privacy: agent a(x4 ) would be able to infer from the
dependency of the message on variable x2 both the existence of agent a(x2 ) (which violates
agent privacy) and the fact that x2 is a neighbor of one or more unknown nodes below x1 .
x1  x4
x2
x4 R B G
R
0 0 0
B
0 0 1
G
0 1 0
(a) in cleartext

x4
R
B
G

x1  x4
928372
  
0 0 0
0 0 1
0 1 0

(b) partly obfuscated

x4
R
B
G

x1  x4
928372


620961 983655
620961 983655
620961 983656


534687
534688
534687

(c) fully obfuscated

Figure 3: The message sent by agent a(x1 ) to its parent variable x4 in Figure 2.

In order to patch these privacy leaks, variable x2 and its domain D2 = {R, B, G} are replaced with random codenames xx2 1 = 928372 and D2x1 = {, , } (Figure 3b) preliminarily
generated by a(x2 ) and communicated directly to the leaf of the back-edge (Algorithm 2,
lines 2 to 7). The leaf applies these codenames to its output message (Algorithm 3, line 5),
and they are only resolved once the propagation reaches the root of the back-edge (Algorithm 3, line 14). Not knowing these codenames, the agents in between, such as a(x4 ), can
only infer the existence of a cycle in the constraint graph involving some unknown ancestor and descendent. This is tolerated by the definition of topology privacy (Definition 5)
since they are also involved in this cycle. A secret, random permutation 2x1 is also applied
to D2x1 ; this is useful for problem classes in which variable domains are public. Notice that
if x4 also had a constraint with x2 , the above reasoning would still hold, because x2 would
then have sent a different codename xx2 4 to x4 , which would then not be able to resolve the
unknown codename xx2 1 to x2 . In this case, x4 s separator would be {x3 , xx2 1 , xx2 4 }, and its
message sent to x3 would be three-dimensional instead of two-dimensional.
3.1.2 Obfuscating Feasibility Values
Hiding variable names and values using codenames addresses the leaks of agent and topology
privacy. However, this does not address the fact that the feasibility values in the message
x1  x4 in Figure 3(b) violate constraint privacy, because they reveal to x4 that its subtree
can always find a feasible solution to its subproblem when x4 = R, regardless of the value of
the obfuscated variable 928372. To patch this privacy leak, feasibility values are obfuscated
by adding large, random numbers that are generated by the root of the back-edge (x2 )
and sent over a secure channel to the leaf of the back-edge (Algorithm 2, lines 9 to 12).
The number of bits B of the random numbers is a problem-independent parameter of the
algorithm. The obfuscation is performed in such a way that a different random number is
662

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

added to all feasibility values associated with each value of x2 , as in Figure 3(c), using the
obfuscation key [620961, 983655, 534687]. These random numbers are added by the leaf of
the back-edge to its outgoing message (Algorithm 3, line 25), and they are only eventually
subtracted when the propagation reaches the root of the back-edge (Algorithm 3, line 18).
Notice that this obfuscation scheme achieves two objectives: 1) it hides from x4 the
absolute feasibility values of its subtree, and 2) it hides the relative dependencies of these
values on the obfuscated variable 928372, because different random numbers were used for
each value in its obfuscated domain {, , }. Agent a(x4 ) is still able to infer the relative
dependencies on its own variable x4 , which is necessary to perform the projection of this
variable, but it is unable to tell, for each value of the other (obfuscated) variable, whether
the subtrees problem is feasible, and if not, how many constraints are violated. Notice in
particular that, for a given value of the obfuscated variable (i.e. for any column), agent a(x4 )
does not know whether any of the assignments to x4 is feasible, and therefore it would be
incorrect to simply assume that the lowest of the obfuscated feasibility entries decrypts to 0.
Similarly, equal entries in the same column correspond with a high probability to entries
that have the same number of constraint violations, but this number is not necessarily 0,
so it would be incorrect to infer they correspond to feasible entries.
Notice also that this obfuscation scheme is only applicable in the presence of a backedge, i.e. when the message contains more than just the parent variable. Consider for
instance the single-variable message x5  x3 , recalled in Figure 4(a). If agent a(x3 ) knew
that x5 is a leaf of the pseudo-tree, the cleartext message would reveal agent a(x5 )s private
local constraint x5 6 {B, R} to agent a(x3 ), and the previous obfuscation scheme does not
apply because of the absence of back-edges. Notice that this threat to constraint privacy
is tempered by the fact that P-DPOP+ s guarantees in terms of topology privacy prevent
agent a(x3 ) from discovering that x5 is indeed a leaf. From a(x3 )s point of view, a larger
subproblem might be hanging below variable x5 in Figure 2, and the message could actually
be an aggregation of multiple agents subproblems.

x3
R
B
G

x5  x3
# conflicts
0
0
1

(a) in cleartext

x3

x5  x3
# conflicts

R
B
G

0
0
730957

(b) obfuscated

Figure 4: The message received by agent a(x3 ) in Figure 2.

To reduce this privacy leak present in the original algorithm (Faltings et al., 2008), we
propose a new additional obfuscation scheme that consists in adding large (B-bit), positive,
random numbers to positive entries in single-variable messages, in order to obfuscate the
true numbers of constraint violations (Algorithm 3, line 8 and Figure 4(b)). Because these
random numbers are never subtracted back, they must not be added to zero entries, otherwise the algorithm would fail to find a solution with no violation. Feasible entries are still
revealed, but the numbers of constraint violations for infeasible entries remain obfuscated.
663

fiLeaute & Faltings

3.2 Propagating Final Decisions
Once the feasibility values have propagated up all the way to the root of the pseudotree, and a feasible assignment to this root variable has been found (if there exists one),
this assignment is propagated down the pseudo-tree (Algorithm 2, lines 14 to 19). Each
variable uses the assignments contained in the message from its parent, in order to look up
a corresponding assignment for itself (line 17). It then sends to each child the assignments
for the variables in its separator (line 19), using the same codenames as before so as to
protect agent and topology privacy. Decision privacy is only partially guaranteed, because
each variable learns the values chosen for its parent and pseudo-parents  but not for other,
non-neighboring variables in its separator because they are hidden by unknown codenames.
3.3 Algorithm Properties
This section first formally proves that the algorithm is complete, and analyses its complexity.
We then present an algorithm variant with lower complexity. Finally, the privacy guarantees
provided by both algorithms (summarized in Table 3.3) are formally described.
privacy type:

agent

topology

constraint

decision

P-DPOP(+)

full
full
full
-

partial
partial
partial
partial

partial
partial
full
partial

partial
full
full
partial

3/2

-DPOP(+)

P
P2 -DPOP(+)
MPC-DisCSP4

Table 1: Privacy guarantees of various algorithms.

3.3.1 Completeness and Complexity
Theorem 1. Provided that there are no codename clashes, P-DPOP+ (Algorithm 2) terminates and returns a feasible solution to the DisCSP, if there exists one.
Proof. After exchanging codenames and obfuscation keys, which is guaranteed to require
a number of messages at most quadratic in the number n of variables, the bottom-up
propagation of feasibility values (Algorithm 3) terminates after sending exactly (n  1)
messages (one up each tree-edge). One can prove by induction (left to the reader) that
this multi-party dynamic programming computation almost surely correctly reveals to each
variable x the (obfuscated) feasibility of its subtrees subproblem, as a function of x and
possibly of ancestor variables in the pseudo-tree. This process may only fail in case of
collisions of codenames, when the roots of two overlapping back-edges choose the same
codenames. Such codename clashes are inherent to most privacy-protecting algorithms,
and can be made as improbable as desired by augmenting the size of the codename space.
Finally, the top-down decision propagation phase (Algorithm 2, lines 14 to 19) is guaranteed to yield a feasible assignment to each variable (if there exists one), after the exchange
of exactly (n  1) messages (one down each tree-edge).
664

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

When it comes to the complexity of the algorithm in terms of number of messages
exchanged, the bottleneck is in the election of the root variable (Online Appendix 1), which
requires O(  d  n2 ) messages, where  is the diameter of the constraint graph, d its degree,
and n is the number of variables. However, the (n  1) messages containing feasibility
values can be exponentially large: the message sent by variable x is expressed over |sepx |
|sep |
variable codenames (Algorithm 3, line 12), and therefore contains O(Dmaxx ) feasibility
values, where Dmax is the size of the largest variable domain. The overall complexity in
terms of information exchange, memory and runtime (measured in number of constraint
sepmax
checks) is therefore O(n  Dmax
), where sepmax = maxx |sepx |. This is the same as DPOP,
except that in P-DPOP+ each variable may appear multiple times under different codenames
in the same separator, hereby increasing the value of sepmax . However, this increase is only
by a multiplicative factor that is upper bounded by the degree of the constraint graph, since
the number of codenames for a given variable is at most equal to its number of neighbors.
Empirically, our experimental results in Section 6 suggest that, on almost all the problem
classes we considered, the median value of sepmax tends to grow rather linearly in n.
3.3.2 P-DPOP: Trading off Topology Privacy for Performance
It is possible to reduce the sizes |sepxi | of the separators, by enforcing that each agent a(x)
send the same codename x for x to all of xs (pseudo-)children, unlike in Algorithm 2 (lines
2 to 7). This variant will be identified by the absence of the plus sign in exponent; P-DPOP
is the version of the algorithm that was initially proposed by Faltings et al. (2008).
As a result of this change, variables that previously may have occurred multiple times
in the same feasibility message under different codenames can now only appear at most
once, such that we now have sepmax < n. The worst-case complexity of P-DPOP then
becomes the same as DPOP (Petcu & Faltings, 2005), in which sepmax is equal to the
width of the pseudo-tree, which is bounded below by the treewidth of the constraint graph.
However, privacy considerations prevents the use in P-DPOP of DPOPs more efficient, but
less privacy-aware pseudo-tree generation heuristics, resulting in higher-width pseudo-trees.
While the complexity of P-DPOP is hereby decreased compared to P-DPOP+ , sending
the same codename x for variable x to all its (pseudo-)children has drawbacks in terms of
topology privacy, as analyzed below.
3.3.3 Full Agent Privacy
There are only two ways the identity of an agent A could be leaked to a non-neighbor B:
1) the algorithm can require A and B to exchange messages with each other, or 2) Agent A
can receive a message whose content refers identifiably to B. Case 1 can never happen in
any of our algorithms, because they only ever involve exchanging messages with neighboring
agents. Case 2 is addressed mainly through the use of codenames.
Theorem 2. The P-DPOP(+) algorithms guarantee full agent privacy.
Proof. The P-DPOP(+) algorithms proceed in the following sequential phases (the preliminary phases of root election and pseudo-tree generation are addressed in online appendices):
Bottom-up feasibility propagation (Algorithm 3) Each feasibility message contains
only a function (line 11) over a set of variables, whose names, if transmitted in clear
665

fiLeaute & Faltings

text, could identify their owner agent. To prevent this agent privacy leak, P-DPOP(+)
replaces all variable names with secret, random codenames, as follows.
Consider a variable x in the pseudo-tree. Note that no feasibility message sent by x or
by any ancestor of x can be a function of x. The message sent by x is not a function
of x, because x is projected out before the message is sent (line 22). Variable x cannot
re-appear in any feasibility message higher in the pseudo-tree, because no agents local
problem can involve any variable lower in the pseudo-tree (line 2).
Similarly, consider now the feasibility message sent by a descendant y of x in the
pseudo-tree, and assume first that y is a leaf of the pseudo-tree. Since y has no
children, the feasibility message it sends can only be a function of the variables in its
local problem. If this local problem involves x, y will replace x by its codename xy
(line 5) before it sends its feasibility message. One can then prove by inference that
no feasibility message sent by any variable between y and x will contain x either; it
can only (and not necessarily) contain one or several of its codenames xyi .
Since the codenames xyi are random numbers chosen by x (Algorithm 2, line 4),
and only communicated (through channels that are assumed secure) to the respective
neighbors yi of x (Algorithm 2, line 7), no non-neighbor of x receiving a message
involving any xyi can discover the identity of its owner agent.
The domain Dx of variable x could also contain values that might identify its owner
agent. To fix this privacy risk, xs domain is also replaced by obfuscated domains Dxyi
of random numbers, similarly to the way variable names are obfuscated. In this
paper, we make the simplifying assumption that all variables have the same domain
size (which naturally holds in many problem classes), so that one variables domain
size does not give any information about its owner agent. Otherwise, variable domains
can be padded with fake values in order to make them all have the same size.
Top-down decision propagation (Section 3.2) The messages contain assignments to
variables (Algorithm 2, line 19), which are also obfuscated using codenames.
This concludes the proof that, in the P-DPOP(+) algorithms, no agent can receive any
message from which it can infer the identity of any non-neighboring agent.
3.3.4 Partial Topology Privacy
Theorem 3. P-DPOP guarantees partial topology privacy. The minor leaks of topology
privacy lie in the fact that a variable might be able to discover a lower bound on a neighbor
variables degree in the constraint graph, and a lower bound on the total number of variables.
Proof. Root election and pseudo-tree generation are left to the online appendices.
Bottom-up feasibility propagation (Algorithm 3) Each variable x receives a FEAS
message from each child, containing a function whose scope might reveal topological
information. Each variable y in this scope is represented by a secret codename y,
however x may be able to decrypt the codename y, if and only if y is a neighbor of x
(or is x itself), because y has sent the same codename y to all its neighbors. This
results in a leak of topology privacy: x discovers, for each neighboring ancestor y,
666

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

whether y has at least one other neighbor below a given child of x. But it cannot
discover exactly how many of these other neighbors there are.
Furthermore, in the case where x and y are not neighbors, x cannot decrypt y, but
it can still infer there exists another, non-neighboring ancestor corresponding to this
codename. This is another breach of topology privacy. Because y sent the same
codename y to all its neighbors, x can also discover whether that other ancestor has
at least one neighbor below each of xs children. Moreover, since codenames are
large random numbers that are almost surely unique, x may discover the existence of
several, distinct such non-neighboring ancestors.
Top-down decision propagation (Section 3.2) Each variable receives a message from
its parent, which can only contain codenames for variables and variable values that
were already present in the FEAS message received during the previous phase.
This concludes the proof that P-DPOP only partially protects topology privacy. The limited
topology information leaked to a variable only concerns its branch in the pseudo-tree; no
information can be leaked about any other branch, not even their existence.
Theorem 4. The use of different codenames for each (pseudo-)child improves the topology
privacy in P-DPOP+ compared to P-DPOP, but the same bounds can still be leaked.
Proof. Consider a variable x that receives a FEAS message including a secret codename y
corresponding to variable y (6= x). Because y now sent a different codename to each of its
neighbors, x is no longer able to decrypt y, even if y is a neighbor of x. As a consequence,
x is no longer able to infer whether y refers to a known neighbor of x, or to an unknown,
non-neighboring variable. However, since each codename now corresponds to a unique backedge in the pseudo-tree, for each pair (, ) of unknown codenames in xs received FEAS
message (if such a pair exists), at least one of the following statements must hold:
  and  refer to two different ancestors of x, and therefore x discovers at it has at least
two ancestors (which it might not have known, if it has no pseudo-parent); and/or
  and  were sent to two different descendants of x below (and possibly including) the
sender child y, and therefore x discovers that it has at least two descendants below
(and including) y (which it might not have known, if it has no pseudo-child below y).
Therefore x might be able to refine its lower bound on the total number of variables.
3.3.5 Partial Constraint Privacy
Theorem 5. The P-DPOP(+) algorithms guarantee partial constraint privacy. The local
feasibility of a subproblem for a partial variable assignment X  may be leaked, even if X 
cannot be extended to an overall feasible solution (i.e. this is not semi-private information).
Proof. Information about constraints is only transmitted during feasibility propagation (Algorithm 3). Based on the knowledge of the optimal variable assignments transmitted during
the last phase (Section 3.2), some of the feasibility information may be decrypted.
667

fiLeaute & Faltings

Single-variable feasibility messages When a variable px receives a feasibility message
involving only px , the message has been obfuscated only by adding secret random
numbers to its infeasible entries (line 8). Feasible entries remain equal to 0, and px
can identify which entries refer respectively to feasible or infeasible assignments to px .
However, the addition of a secret, positive, random number to each infeasible entry
ensures only an upper bound on the number of constraint violations is leaked, which
can be made as loose as desired by choosing random numbers as large as necessary.
Multi-variable feasibility messages If the FEAS message involves at least one other
variable yi , then all message entries have been obfuscated by adding large random
numbers keyyxi (yix ) of B bits (line 25). Furthermore, keyyxi (yix ) is only known to the
sender x of the message and to its pseudo-parent yi , but not to the recipient px , which
therefore cannot subtract it to de-obfuscate the entries.
Assume, for simplicity, that the message m(px , yix ) involves only the two variables
px and yix ; the argument extends easily to more variables. The recipient px might
be able to make inferences: 1) by fixing yix and comparing the obfuscated entries
corresponding to different values for px ; or 2) by fixing px and varying yix instead.
1. For a given value of yix , all entries have been obfuscated by adding the same
random number keyyxi (yix ) (line 25), so px can compute the relative differences of
feasibility values for various assignments to px . However, it cannot decrypt the
absolute values without knowing keyyxi (yix ). In particular, the lowest obfuscated
value is not necessarily equal to keyyxi (yix ), because it does not necessarily decrypt
to 0: all values of px may be infeasible for this particular value of yix .
There is one exception: if a feasible solution is found to the problem in which


yix = yix and px = px  , then m(px  , yix ) necessarily decrypts to 0, and therefore


px will be able to infer keyyxi (yix ). After fixing yix = yix in the message and

subtracting keyyxi (yix ), the same reasoning can be made as for the single-variable
case, in which feasible and infeasible entries are identifiable, but the numbers of
constraint violations for infeasible entries remain obfuscated.
2. For a given value of px , each feasibility value m(px , yix ) has been obfuscated
by adding a different, secret random number keyyxi (yix ). Choosing the number
of bits B sufficiently large makes sure that no useful information (relative, or
absolute) can be obtained by comparing the obfuscated feasibility values.
This concludes the proof that P-DPOP(+) guarantees partial constraint privacy.
3.3.6 Partial Decision Privacy
Theorem 6. The P-DPOP(+) algorithms guarantee partial decision privacy. The leak lies
in the fact that a variable might discover the values chosen for some or all of its neighbors.
Proof. First notice that the algorithm cannot leak any information about the chosen values
for variables that are lower in the pseudo-tree, since these variables have been projected out
of the feasibility messages received. However, during the decision propagation phase, each
variable receives a message from its parent that contains the chosen values for its parent and
668

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

pseudo-parents. The message may also contain codenames for the assignments to other, nonneighboring variables, which the recipient will not be able to decode. Furthermore, domains
are shuffled using secret permutations, making it impossible to decode the codename for
the value of a non-neighboring variable from its index in the variables domain.

4. P3/2 -DPOP+ : Adding Full Decision Privacy
This section presents another variant of the P-DPOP+ algorithm that achieves full decision
privacy. This results in a novel algorithm, which can be seen as a hybrid between the
P-DPOP+ and P2 -DPOP (Leaute & Faltings, 2009) algorithms, and is called P3/2 -DPOP+.
4.1 Overview of the Algorithm
Algorithm 4 patches the decision privacy leak in P-DPOP+ by removing its decision propagation phase. Only the root variable is assigned a value, and in order for all variables to be
assigned values, each variable is made root in turn (unless the first feasibility propagation
has revealed that the problem is infeasible, in which case the algorithm can terminate early).
3
The intuition behind this P /2 -DPOP+ algorithm is therefore that P-DPOP+ s bottom-up
feasibility propagation phase is repeated multiple times, each time with a different variable x
as the root of the pseudo-tree (lines 10 to 15). At the end of each iteration, a constraint
x = x is added to the problem to enforce consistency across iterations (line 16).
Algorithm 4 Overall P3/2 -DPOP+ algorithm with full decision privacy, for variable x
Require: a first temporary DFS tree, a unique ID idx , a tight strict lower bound on the
+
next unique ID id+
x , and an upper bound n on the total number of variables
idx

id+
x idx

z }| { z }| {
1: vectorx  [1, . . . , 1, 0, 1, . . . , 1, 1, . . . , 1]
|
{z
}
n+

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

// Exchange public key shares:
privatex  generate a private ElGamal key for x
publicx  generate a set of (id+
x  idx + 1) public key shares corresponding to privatex
for each share  publicx do ToPrevious((SHARE, share)) as in Algorithm 9
for i = 1 . . . n+ do
Wait for and record one message (SHARE, share)
if share 6 publicx then ToPrevious((SHARE, share)) as in Algorithm 9
Generate the compound ElGamal public key based on all the public key shares
while vectorx 6=  do
Choose a new root (Algorithm 5, Section 4.2)
Construct a new pseudo-tree rooted at the new root (Online Appendix 2)
Exchange codenames for x and its domain Dx (Algorithm 2, lines 2 to 7)
Choose and exchange obfuscation key for x (Algorithm 2, lines 9 to 12)
Propagate feasibility values up the pseudo-tree (Algorithm 3, except line 21)
if x is root then Add local constraint x = x , with x from Algorithm 3, line 27
669

fiLeaute & Faltings

4.2 Choosing a New Root Variable
To iteratively reroot the pseudo-tree, we propose to use an improved version of the rerooting
procedure we initially introduced for the P2 -DPOP algorithm (Leaute & Faltings, 2009).
This procedure requires that each of the n variables be assigned a unique ID; an algorithm
to achieve this is presented in Online Appendix 3. This algorithm reveals to each variable x
its unique ID idx , as well as a tight strict lower bound on the next unique ID id+
x (i.e. the
+ on the total number of variables.
+
1),
and
an
upper
bound
n
next unique ID equals id+
x
Each variable x then creates a Boolean vector vectorx with a single zero entry at the index
corresponding to its unique ID idx (Algorithm 4, line 1); this vector is then shuffled using
a random permutation used to hide the sequence in which variables become roots.
To keep the permutation secret, the vector is first encrypted using ElGamal encryption
(Appendix A), based on a compound public key jointly produced by the agents (Algorithm 4,
lines 2 to 9). This asymmetric encryption scheme enables each agent to (re-)encrypt the
entries in the vectors using a common public key, such that the decryption can only be
performed collaboratively by all the agents, using their respective private keys.
Algorithm 5 Algorithm to choose a new root, for variable x
Procedure: ShuffleVectors() for variable x
1: myID  large random number
2: px  random permutation of [1 . . . n+ ]
// Propagate xs encrypted vector backwards along the circular ordering
vectorx  E(vectorx ) // encrypts the vector using the compound public key
5: ToPrevious((VECT, myID, vectorx , 1)) as in Algorithm 9 in Appendix B
3:

4:

// Process all received vectors
7: while true do
8:
Wait for a message (VECT, id, vector, round) from the next variable
6:

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

if round = 1 then
if id 6= myID then for j = (idx + 1) . . . id+
x do vector[j]  1
else round  round + 1 // xs vector; move to next round
if round > 1 and x is the current root then
round  round + 1 // the root starts each round except the first
if round = 3 then vector  px (vector) // shuffle the vector
if round = 4 and id = myID then // done processing vectorx
vectorx  vector
continue
// Pass on the vector backwards along the circular ordering
vector  E(vector) // re-encrypts the vector using the compound public key
ToPrevious((VECT, id, vector, round)) as in Algorithm 9 in Appendix B

Procedure: Reroot() for variable x
21: repeat entry  Decrypt(pop(vectorx )) while entry 6= 1 // as in Algorithm 6
22: if entry = 0 then x is the new root
670

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

The agents then proceed as in Algorithm 5. Each variable x first starts the procedure
ShuffleVectors(), which is run only once  this is a performance improvement over our
previous work (Leaute & Faltings, 2009), where it was performed at each iteration. All the
vectors are passed from variable to variable in a round-robin fashion, using a circular message
routing algorithm presented in Appendix B. Each agent applies a secret permutation to
each vector to shuffle it. ShuffleVectors() proceeds in four rounds. During round 1
(started on line 5, Algorithm 5), each vector makes a full round along the circular ordering,
during which each variable x overwrites some of the entries with 1 (line 10), at the same
positions it did for its own vectorx (Algorithm 4, line 1). These 1 entries account for the
IDs in [idx + 1, id+
x ] that have not been assigned to any variable (Online Appendix 3). Once
x has received back its own vectorx , it enters the incomplete round 2 (line 11) during which
vectorx is passed on until it reaches the current root (line 12). The root then starts round 3
(line 13), during which each variable x shuffles each vector using its secret permutation px
(line 14). The incomplete round 4 returns the fully shuffled vector to its owner (line 16).
To reroot the variable ordering at the beginning of each iteration of P3/2 -DPOP+ , each
variable x calls the procedure Reroot(), which removes and decrypts the first element of
vectorx . Entries that decrypt to 1 correspond to unassigned IDs and are skipped. The
single entry that decrypts to 0 identifies the new root. The decryption process (Algorithm 6)
is a collaborative effort that involves each variable using its private ElGamal key to partially
decrypt the cyphertext, which travels around the circular variable ordering in the same way
as the vectors, until it gets back to its sender variable, which can finally fully decrypt it.
Algorithm 6 Collaborative decryption of a multiply-encrypted cyphertext e
Procedure: Decrypt(e) for variable x
1: codename  large random number used as a secret codename for x
2: codenamesx  codenamesx  {codename}
3: ToPrevious((DECR, codename, e)) as in Algorithm 9
4: Wait for message (DECR, codename, e ) from next variable in the ordering
5: return decryption of e using xs private key
Procedure: CollaborativeDecryption() for variable x
6: loop
7:
Wait for a message (DECR, c, e) from next variable in the ordering
8:
if c 6 codenamesx then
9:
e  partial decryption of e using xs private key
10:
ToPrevious((DECR, c, e )) as in Algorithm 9

4.3 Algorithm Properties
3

We first analyze the completeness and complexity properties of the P /2 -DPOP(+) algorithms, and then we move on to their privacy properties.
4.3.1 Completeness and Complexity
Theorem 7. Provided that there are no codename clashes, the P3/2 -DPOP+ algorithm
terminates and returns a feasible solution to the DisCSP, if there exists one.
671

fiLeaute & Faltings

Proof. On the basis of Theorem 1, it remains to prove that the rerooting Algorithm 5
terminates and is correct, and that the overall algorithm remains correct. The latter is easy
to prove: at each iteration, a feasible value is found for the root variable (if there exists
one), and that value is necessarily consistent with the chosen assignments to previous roots
since these assignments are enforced by new, additional constraints (Algorithm 4, line 16).
When it comes to the rerooting procedure, the unique ID assignment algorithm (Online
Appendix 3) ensures that each of the n variables gets a unique ID in 0 . . . (n+ 1). Therefore,
each variable has a 0 entry at a unique position in its vector (Algorithm 4, line 1). Round 1
of Algorithm 5 also makes sure that all vectors have 1 entries at the same positions. This
ensures that exactly one variable will become the new root at each iteration, since all vectors
are applied the same sequence of permutations, and no variable will be root twice.
3

In terms of complexity, P /2 -DPOP+ proceeds in a similar way to P-DPOP+ (Section 3.3), except that the bottom-up feasibility propagation phase is repeated n times (each
time with a different root variable). The overall complexity in information exchange theresep
fore becomes O(n2 Dmaxmax ), where sepmax is the maximum separator size over all variables,
and over all iterations, which therefore is likely to be higher than the exponent for PDPOP+ . The information exchanged by the rerooting protocol is negligible in comparison.
sep
The runtime complexity (measured in number of constraint checks) is also O(n2  Dmaxmax ),
sep
but the memory complexity is only O(n  Dmaxmax ), because removing the decision propagation phase makes it become unnecessary to compute and record x (px , ) (Algorithm 3,
line 21). Our experimental results on graph coloring benchmarks (Section 6.1) suggest that
the median value of sepmax may only be greater than the median value of sepmax in PDPOP+ by a small multiplicative factor. In terms of number of ElGamal cryptographic
operations, the rerooting procedure requires a total of n(3n  1)n+  O(n3 ) encryptions:
each of the n variables (re-)encrypts (3n  1) vectors of size n+ (each variables vector
performs 3 full rounds, except for the roots vector, which performs only 2 full rounds),
with n+  n + n  2incrmin , where incrmin is a constant input parameter of the algorithm.
The procedure also requires a total of n2 n+  O(n3 ) collaborative decryptions: each of the
n variables (partially) decrypts n vectors or size n+ .
4.3.2 Full Agent Privacy
Theorem 8. The P3/2 -DPOP(+) algorithms guarantee full agent privacy.
Proof. The unique ID assignment and circular routing algorithms guarantee full agent privacy, as demonstrated respectively in Online Appendix 3 and Appendix B.
Pseudo-tree rerooting (Algorithm 5) The messages sent by ShuffleVectors() contain a variable ID, a vector of ElGamal cyphertexts, and a round number. The ID
is used by the recipient to detect whether the vector is its own vector; it is a large
random number chosen by the owner agent (Algorithm 5, line 1), and therefore it
cannot be linked to the identity of this owner agent by any other agent. The ElGamal
vector and the round number also do not contain any information that could be used
to identify the agent. Also note that the procedure used to exchange ElGamal public
key shares (Algorithm 4, lines 2 to 9) does not leak any information about agents
672

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

identities. The Reroot() procedure then makes use of the collaborative decryption
algorithm, whose properties in terms of agent privacy are discussed below.
Collaborative decryption (Algorithm 6) The procedure exchanges messages that contain an ElGamal cyphertext, and a codename used like the variable ID in Algorithm 5.
This codename is similarly set to a large random number chosen by the current agent,
and cannot be linked to the identity of this agent by any other agent.
3

This concludes the proof that the P /2 -DPOP(+) algorithms guarantee agent privacy.
4.3.3 Partial Topology Privacy
3

The topology privacy in P /2 -DPOP(+) is only slightly worse than in P-DPOP(+) .
Theorem 9. The P3/2 -DPOP(+) algorithms guarantee partial topology privacy. Each variable unavoidably discovers the total number of variables in the problem, and might also
discover a lower bound on a neighbor variables degree in the constraint graph. The advan3
3
tages of P /2 -DPOP+ over P /2 -DPOP are the same as P-DPOP+ over P-DPOP.
Proof. Since there is one feasibility propagation phase per variable in the problem, the total
number of variables inevitably becomes public. The following analyzes the topology privacy
properties of each phase of P3/2 -DPOP(+) that is not already present in P-DPOP(+) , except
for unique ID assignment (Online Appendix 3) and secure message routing (Appendix B).
Exchange of ElGamal key shares (Algorithm 4, lines 29) The messages containing ElGamal key shares do not contain any information that could be used to make
inferences about the topology of the constraint graph.
Pseudo-tree rerooting (Algorithm 5) Each message travels along a circular variable
ordering using the message routing algorithm in Appendix B, and contains:
 a vector that is encrypted (and re-encrypted after each operation) and that therefore cannot provide any topological information;
 an id that identifies the owner of the vector; being a secret, large random number,
only the owner of the vector can identify itself;
 a round number can take the following values:
 round = 1 only indicates that the vector is being modified, each variable
setting in turn some of the values to 1;
 round = 2 only indicates that the vector is being sent to the root of the
pseudo-tree. This does not happen for the vector of the (unknown) root;
 round = 3 only indicates that the vector is being shuffled by each variable;
 round = 4 only indicates that the vector is on its way back to its owner.
This does not happen for the vector belonging to the (unknown) root.
Reroot() then uses the decryption algorithm whose properties are described below.
673

fiLeaute & Faltings

Collaborative decryption (Algorithm 6) DECR messages are passed along the circular variable ordering, containing a secret codename for the original sender variable,
which is the only variable capable of deciphering this codename. The last part of the
message payload is an ElGamal cyphertext, which remains encrypted until it reaches
back the original sender, and therefore does not leak any topological information.
3

This concludes the proof that P /2 -DPOP(+) guarantees partial topology privacy.
4.3.4 Partial Constraint Privacy
3

The constraint privacy properties of the P /2 -DPOP(+) algorithms differ from those of PDPOP(+) , because the former protect decision privacy (which benefits constraint privacy),
but also reveal the total number of variables in the problem (which hurts constraint privacy).
Theorem 10. The P3/2 -DPOP(+) algorithms guarantee partial constraint privacy. The
leaks are the same as in P-DPOP(+) (Section 3.3.5), but they happen less frequently.
Proof. Single-variable feasibility messages leak the same amount of constraint privacy as in
P-DPOP(+) ; notice however that, since the P3/2 -DPOP(+) algorithms now reveal the total
number of variables, in some circumstances it may be possible for a variable to discover that
a child is a leaf, and that the feasibility message it sends therefore contains information about
its local subproblem only. However, multi-variable feasibility messages leak potentially much
less information than in P-DPOP(+) : consider again the simpler and non-restrictive case
3
of a two-variable message m(px , yix ) received by px . Because P /2 -DPOP(+) now protects

decision privacy, px no longer discovers the value yix chosen for yix , and is therefore no
longer able to infer which of the entries corresponding to px = px  decrypts to 0.
One exception is when the following three conditions simultaneously hold: 1) P3/2 -DPOP

is used, 2) the codename yix refers to a variable yix that is a neighbor of px , and 3) yix is

semi-private information to px ; then px will still discover yix , and will be able to make the
3
same inferences as in P-DPOP(+) . If the first condition is not satisfied, i.e. P /2 -DPOP+ is
3
used instead of P /2 -DPOP, then px will not be able to link the codename yix to any known
3
variable. This is also the case if P /2 -DPOP is used, but the second condition does not

hold. Finally, if the first two conditions hold, px will only be able to discover yix if it is
semi-private information, i.e. if it can infer it only from its knowledge of the problem, and
of its own chosen value px  .
4.3.5 Full Decision Privacy
Theorem 11. The P3/2 -DPOP(+) algorithms guarantee full decision privacy.
Proof. The leak of decision privacy in P-DPOP(+) is fixed by removing the decision propagation phase. Instead, the variable ordering is rerooted, and the feasibility propagation
phase is restarted. It is not possible to compare the feasibility messages received from one
iteration to the next to infer the decision that has been made at the previous iteration: the
messages are not comparable, since different codenames and obfuscation keys are used.
674

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

5. P2 -DPOP+ : Adding Full Constraint Privacy
We now describe how the previous, non-fully secure obfuscation scheme can be replaced
with ElGamal homomorphic encryption (Appendix A) to achieve full constraint privacy,
which corresponds to the original P2 -DPOP algorithm (Leaute & Faltings, 2009), improved
by the use of multiple codenames. An important limitation of the ElGamal scheme is that
it is not fully homomorphic: it is possible to compute the OR of two encrypted Booleans,
but it is only possible to compute the AND of an encrypted Boolean with a cleartext
Boolean. As a consequence, the bottom-up feasibility propagation has to be performed on
a variable ordering such that each variable can have only one child, i.e. a linear variable
ordering (Figure 5), using the message routing algorithm in Appendix B. Otherwise, in a
pseudo-tree variable ordering, a variable with two children would not be able to join the
two encrypted feasibility messages sent by the children. This could be addressed using the
fully homomorphic encryption scheme by Gentry (2009), however it is unclear whether this
scheme would be practically applicable and would have sufficient performance.
x2
x3

x1

x5

x4

Figure 5: The (counter-clock-wise) circular variable ordering corresponding to Figure 2.

5.1 Propagating Encrypted Feasibility Values along a Linear Variable Order
In contrast to Figure 2, which illustrates multi-party dynamic programming on a pseudotree variable ordering (counting constraint violations), Figure 6 shows (in cleartext) how
it can be carried out on a linear ordering (in the Boolean domain). This assumes that a
circular communication structure has preliminarily been set up as described in Appendix B.
Algorithm 7 gives the detailed pseudocode for this procedure, and is intended as a
replacement for line 15 in Algorithm 4. The differences with the pseudo-tree-based Algorithm 3 are the following. First, while Algorithm 3 initially reformulated the DisCSP into
a Max-DisCSP so as to minimize the number of constraint violations, Algorithm 7 works
directly on the original DisCSP problem. This means that the conjunction operator 
replaces the sum operator (lines 2 and 10), and the disjunction operator  replaces the
operator min (line 13). Notice also that, in the case of the linear ordering, a variables local
subproblem no longer necessarily involves its parent variable in the ordering (line 2), just
like x4 shares no constraint with x5 in Figures 5 and 6.
The next difference is that variable x no longer partially de-obfuscates its feasibility
matrix before projecting itself (Algorithm 3, line 18). The reason is that the ElGamal
scheme is homomorphic, and therefore it is no longer necessary to first (partially) decrypt
675

fiLeaute & Faltings

x2

x3  x2
R
B
true false

G
true

x2

x3

x3
R
B
G

x5  x3
x2
R
B
true false
true
true
false false

x4  x5
x2
R
B
true false
true true
true true

x3
R
B
G
x5

G
true
true
false

x4

x4
R
B
G

G
true
true
true
x1
x1  x4
x2
R
B
true true
true true
true false

G
true
false
true

Figure 6: Multiparty dynamic programming computation (in cleartext) of a feasible value
for variable x2 , using a linear variable ordering based on Figure 5.

Algorithm 7 Propagating feasibility values along a linear ordering, for variable x
1: // Join local constraints:
V
2: m(x, )  c{c C | xscope(c )  scope(c )(childrenx pseudo childrenx )=} c(x, )

// Apply codenames:
4: for each yi  {parentx }  pseudo parentsx do
5:
m(x, )  replace (yi , Dyi ) in m(x, ) with (yix , Dyxi ) from Algorithm 2, line 2, and
apply the permutation yxi to Dyxi
3:

6:
7:
8:
9:
10:

// Join with received message:
Wait for the message (FEAS, m ()) from the next variable in the ordering
for each z  childrenx  pseudo childrenx do
m ()  identify (xz , Dxz ) as (x, Dx ) in m () (if xz is present)
m(x, )  m(x, )  m ()

// Project out x:
if x is not the root
W variable then
13:
m()  E ( x m(x, )) // re-encrypts using the compound public key
14:
ToPrevious((FEAS, m())) as in Algorithm 9
15: else x  FeasibleValue(m(x, )) as in Algorithm 8
11:

12:

676

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

the feasibility values to project x using the operator x . Only the root variable requires
decryption (Algorithm 7, line 15) to find a value x for its variable x whose encrypted
feasibility value decrypts to true (if any). This is described in the following section.
5.2 Decrypting a Feasible Value for the Root Variable
The decryption of feasibility values at the root is a collaborative process in which each variable partially decrypts the cyphertext using its private key (Algorithm 6). The dichotomy
procedure in Algorithm 8 uses at least log2 |Dx | and at most log2 |Dx | + 1 decryptions
to find a feasible assignment to the root variable, or to detect infeasibility.
Algorithm 8 Finding a feasible value in the encrypted feasibility matrix m(x)
Procedure: FeasibleValue(m (x = xil . . . xir ))
1: if il < ir hthen
ki in half the remaining subdomain:
j // cut
il +ir
2:
I  il ,
2

W
3:
f easible  Decrypt iI m (x = xi ) as in Algorithm 6
4:
if f easible = true then return FeasibleValue(m
 (x = xiI ))
5:
else return FeasibleValue m x = xi[il ,ir ]I
else // only one value remains for x
7:
f easible  Decrypt(m (x = xil )) as in Algorithm 6
8:
if f easible = true then return xil else return null
6:

5.3 Algorithm Properties
We first analyze the completeness and complexity properties of the P2 -DPOP(+) algorithms,
and then we move on to their privacy properties.
5.3.1 Completeness and Complexity
Theorem 12. Provided that there are no codename clashes, the P2 -DPOP+ algorithm
terminates and returns a feasible solution to the DisCSP, if there exists one.
Proof. Termination follows from Theorem 7, and from the fact that the message routing
procedure in Appendix B guarantees all feasibility messages eventually reach their destinations. When it comes to completeness, the homomorphic property of the ElGamal scheme
ensures the projection of a variable x out of an encrypted feasibility matrix is correct, and
that the feasibility message received by each variable in the linear ordering summarizes the
(encrypted) feasibility of the lower agents aggregated subproblems, as a function of higher
variables. In particular, the feasibility message received by the root allows it to find a value
for its variable that satisfies the overall problem, if there exists one.
The analysis of the complexity of the algorithm remains similar to the analysis in Secsep
tion 4.3: it is O(n2  Dmaxmax ) in information exchange and in number of constraint checks,
sep
and O(n  Dmaxmax ) in memory, but sepmax is now the maximum separator size along the
successive linear variable orderings, instead of along the pseudo-trees. The requirement that
677

fiLeaute & Faltings

each variable may have at most one child tends to make this exponent increase significantly,
as illustrated empirically in Section 6. In terms of number of ElGamal cryptographic operations, in addition to the cost of rerooting the variable ordering (Section 4.3), the algorithm
sep
also requires O(n2  Dmaxmax ) encryptions, and only O(n log Dmax ) collaborative decryptions.
5.3.2 Full Agent Privacy
Theorem 13. The P2 -DPOP(+) algorithms guarantee full agent privacy.
3

Proof. The only changes introduced in P2 -DPOP(+) with respect to P /2 -DPOP(+) are in
feasibility propagation, and in finding a feasible value for the root variable.
ElGamal feasibility propagation (Algorithm 7) From the point of view of agent privacy, this is the same procedure as Algorithm 3, but using Algorithm 9 for message
routing, both of which algorithms guarantee agent privacy.
Root variable assignment (Algorithm 8) This consists in iteratively calling the procedure in Algorithm 6, which has already been shown to guarantee agent privacy.
This concludes the proof that the P2 -DPOP(+) algorithms guarantee agent privacy.
5.3.3 Partial Topology Privacy
Theorem 14. The P2 -DPOP(+) algorithms guarantee partial topology privacy. In addition
to the limited leaks of topology privacy in P3/2 -DPOP(+) , an agent might also be able to
discover that there exists another branch in the constraint graph that it is not involved in.
3

Proof. There are only two relevant differences with P /2 -DPOP(+) : the linear variable ordering, and the choice of a value for the root variable that requires collaborative decryption.
ElGamal feasibility propagation (Algorithm 7) To exchange FEAS messages along
a linear variable ordering, the algorithm makes use of the circular message routing
procedure, which is shown in Appendix B to guarantee full topology privacy. However,
the last variable in the linear ordering needs to know it is the last in order to initiate
the feasibility propagation; therefore, by contraposition, non-last variables know they
are not the last, and, in particular, non-last leaves of the pseudo-tree discover the
existence of another branch. This minor leak of topology privacy is already present
in the unique variable ID assignment algorithm (Online Appendix 3). Besides this,
the topology privacy properties of the feasibility propagation phases in P2 -DPOP and
P2 -DPOP+ are the same as in P-DPOP and P-DPOP+ , respectively.
Root variable assignment (Algorithm 8) This algorithm involves recursively calling
the collaborative decryption procedure, shown to guarantee full topology privacy.
This concludes the proof that P2 -DPOP(+) guarantees partial topology privacy.
678

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

5.3.4 Full Constraint Privacy
Theorem 15. The P2 -DPOP(+) algorithms guarantee full constraint privacy.
Proof. The P2 -DPOP(+) algorithms fix all the leaks of constraint privacy in P<2 -DPOP(+) ,
by replacing the cryptographically insecure obfuscation through addition of random numbers, by the cryptographically secure ElGamal encryption (Appendix A). This makes it no
longer possible to compare two encrypted feasibility values without decrypting them, which
would require the collaboration of all agents (or an amount of computation to break the
encryption that can be made arbitrarily high in the worst case by increasing the ElGamal
key size). In particular, while it is possible to compute the logical OR of two cyphertexts
without decrypting them, the result remains encrypted, and cannot be compared to the two
inputs to decide which one is true, if any.
5.3.5 Full Decision Privacy
Theorem 16. The P2 -DPOP(+) algorithms guarantee full decision privacy.
Proof. The same proof applies as to Theorem 11.

6. Experimental Results
We report the empirical performance of our algorithms against the state-of-the-art MPCDisCSP4 algorithm, on four classes of benchmarks: graph coloring, meeting scheduling,
resource allocation, and game equilibrium. We only compare to MPC-DisCSP4, because to
our knowledge it is the only other general DisCSP algorithm that provides strong privacy
guarantees. For each problem class, the choice of the DisCSP formulation is crucial, because
it dictates how the four types of privacy defined based on the DisCSP constraint graph will
relate to the actual privacy of the original problem. In particular, the P -DPOP(+) algorithms use the standard DisCSP assumption that each constraint is known to all agents
owning a variable in its scope (Section 2.2.2). Therefore, when an agent wants to hide a
constraint from neighboring agents, it must express its constraint over copies of its neighbors variables. Additional equality constraints must be introduced to make copy variables
equal to their respective original variables. In contrast, MPC-DisCSP4 does not make use of
this DisCSP assumption, and therefore it does not need the introduction of copy variables.
Our first performance metric is simulated time (Sultanik, Lass, & Regli, 2007), which
is used, when all agents are simulated on a single machine, to estimate the time it would
have taken to solve the problem if they had run in parallel on dedicated machines (ignoring
communication delays). The two other metrics are the number of messages and the amount
of information exchanged. For each metric, we report the median over at least 100 problem
instances, with 95% confidence intervals. For the obfuscation in P<2 -DPOP(+) , we used
random numbers of B = 128 bits, while P2 -DPOP(+) used 512-bit ElGamal encryption.
MPC-DisCSP4 also used 512 bits for its Paillier encryption. For the unique variable ID
generation procedure in P>1 -DPOP(+) , the parameter incrmin was set to 10. All algorithms
were implemented inside the Java-based FRODO platform for DisCSP (Leaute, Ottens, &
Szymanek, 2009), coupled with the CSP solver JaCoP (Kuchcinski & Szymanek, 2011).
The experiments were run on a 2.2-GHz, dual-core computer, with Java 1.6 and a Java
heap space of 2 GB. The timeout was set to 10 min (wall-clock time).
679

fiLeaute & Faltings

6.1 Graph Coloring
We first report the performance of the algorithms on distributed, 3-color graph coloring
problems. The graphs were randomly generated with varying numbers of nodes, and an
edge density fixed to 0.4. Notice that, with a fixed number of colors and a fixed edge
density, increasing the number of nodes increases the degree of the graph, and therefore
reduces the number of feasible solutions; this explains the trends in some of the following
graphs. The DisCSP formulation involves one decision variable per node, and assumes that
each variable is controlled by a single-variable agent. Notice that inter-agent constraints are
binary inequality constraints, and therefore decision privacy is relevant to this problem class:
knowing ones chosen color is insufficient to infer the respective colors of ones neighbors.
To study the tradeoff between privacy and performance in MPC-DisCSP4, we considered
a variant denoted MPC-DisCSP4 , which assumes all inter-agent inequality constraints (i.e.
node neighborhoods) are public, and only the final choice of colors is protected. Each agent

Simulated time (in ms)

106

Induced width

10
MPC

5

MPC

10

8

P2 -DPOP+
P2 -DPOP

104
3

10

3
P2

-DPOP

3
P2

-DPOP

6
+

4

P-DPOP+

102

P-DPOP

2

DPOP

101

0
3

4

5
6
7
8
Number of nodes

9

10

3

Number of messages

106

4

5
6
7
8
Number of nodes

9

10

Information exchanged (in bytes)

108

105

107

MPC
MPC

104



P>1 -DPOP+
3

106

P>1 -DPOP

10

P-DPOP+

102

105

P-DPOP

101
100

DPOP

3

4

5
6
7
8
Number of nodes

9

104
103

10

3

4

5
6
7
8
Number of nodes

Figure 7: Performance on graph coloring problems.

680

9

10

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

first enumerates all feasible solutions to the overall problem (Section 2.2.2), and then uses
cryptographic techniques to securely and randomly choose one of the feasible solutions. If
there exists none, the algorithm therefore terminates without any cryptographic operations
nor exchanging messages. This explains the phase transition for MPC-DisCSP4 in the
following graphs, since the probability of infeasibility increases with the problem size.
Figure 7 shows that MPC-DisCSP4 (denoted as MPC in these and all subsequent figures)
scales very poorly, timing out on problems with more than 6 nodes. MPC-DisCSP4
performs better; however, as mentioned before, it only protects the final choices of colors.
For small numbers of nodes, the total state space is small, and MPC-DisCSP4 performs
relatively well; for numbers of nodes above 9, the problem instances are mostly infeasible,
and MPC-DisCSP4 quickly detects infeasibility without having to exchange any message.
The most efficient algorithms by far are P-DPOP(+) , whose performance curves are at
least one order of magnitude below all other algorithms. In particular, P-DPOPs runtime
is sensibly the same as DPOP (the communication overhead is almost solely due to the root
Simulated time (in ms)

105

Induced width
14
12

104
P-DPOP+

10
P-DPOP
DPOP

103

8
6

102

4
12

14

16
18
20
Number of nodes

22

12

Number of messages

105

14

16
18
20
Number of nodes

22

Information exchanged (in bytes)

109
108

104
107
106
103
105
102

12

14

16
18
20
Number of nodes

104

22

12

14

16
18
20
Number of nodes

Figure 8: Performance on larger graph coloring problems.

681

22

fiLeaute & Faltings

election algorithm). The cost of improved topology privacy in P-DPOP+ vs. P-DPOP only
starts to show for problem sizes above 7, when the induced widths of P-DPOP+ s pseudotrees start to deviate from P-DPOP and DPOP. Full decision privacy comes at much higher
costs: P3/2 -DPOP(+) s curve is between 1 and 3 orders of magnitude above P-DPOP(+) s,
even though their induced widths remain sensibly the same. This suggests that rerooting
the pseudo-tree (which involves expensive cryptographic operations) is by far the complexity
bottleneck, even when full constraint privacy is additionally guaranteed as in P2 -DPOP(+) ,
whose linear variable orderings nevertheless have significantly higher induced widths than
P<2 -DPOP(+) s pseudo-tree orderings. Notice that the slope of the runtime curve decreases
as the problem size increases; this is due to the fact that more and more problems become
infeasible, and the P>1 -DPOP(+) algorithms are able to terminate after the first iteration
on infeasible problems. Similarly to P-DPOP+ vs. P-DPOP, the cost of improved topology
privacy is only visible above 7 nodes; P2 -DPOP+ even timed out on problems of size 10.
Finally, Figure 7 illustrates the fact that MPC-DisCSP4 tends to send large numbers of
small messages, while the P>1 -DPOP(+) algorithms send lower numbers of larger messages.
Figure 8 compares the performance of P-DPOP(+) against DPOP on larger graph coloring problem instances. On such larger problems, the improved topology privacy in PDPOP+ comes at a complexity price that is too high to scale above 12 nodes. On the other
hand, P-DPOPs curves are only between one and two orders of magnitude above DPOP,
and P-DPOPs median runtime on problem instances of size 22 is below 30 s.
6.2 Meeting Scheduling
We now report experimental results on random meeting scheduling benchmarks. We varied
the number of meetings, while keeping the number of participants per meeting to 2. For each
meeting, participants were randomly drawn from a common pool of 3 agents. The goal is to
assign a time to each meeting among 8 available time slots, such that no agent is required to
attend simultaneous meetings. The pool of agents was deliberately chosen small to increase
the complexity of the problems, by increasing the probability that each agent take part
in multiple meetings. Note that fixing the pool size and the number of participants per
meeting still generates an unbounded number of different problem instances as we increase
the number of meetings, since the state space (the Cartesian product of the domains of the
decision variables) keeps increasing with the number of meetings/decisions to be made.
The DisCSP formulation for this problem class was the following. Each agent owns
one variable of domain size 8 for each meeting it participates in. There is an allDifferent
constraint over all its variables to enforce that all its meetings are scheduled at different
times. For each meeting, a binary equality constraint expressed over the corresponding
variables owned by the two participants enforces that the participants agree on the time
for the meeting. Notice that all inter-agent constraints are binary equality constraints,
3
and therefore P /2 -DPOP(+) do not bring any additional privacy compared to P-DPOP(+) ,
since the values of neighboring variables are semi-private information; therefore, we do not
report the performance of P3/2 -DPOP(+) . For MPC-DisCSP4, we simplified the formulation
by only introducing one variable per meeting, owned by its initiator. This way, for each
meeting, only its initiator is made public, but its exact list of participants remains secret
(it is only revealed a posteriori to the participants of the meeting when they attend it).
682

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

Simulated time (in ms)

106

5

105

P2 -DPOP+
P2 -DPOP

104

MPC

4
3

P-DPOP+

103

P-DPOP

2

DPOP

102
101

Induced width

6

1
0
1

2
3
4
5
Number of meetings

6

1

Number of messages

106

2
3
4
5
Number of meetings

6

Information exchanged (in bytes)

108

105

107

104
106
3

10

105
102
104

101
100

1

2
3
4
5
Number of meetings

103

6

1

2
3
4
5
Number of meetings

6

Figure 9: Performance on meeting scheduling problems.

As can be seen in Figure 9, P2 -DPOPs performance is comparable to that of MPCDisCSP4 (but with much stronger privacy guarantees), although the former sends significantly more information on the smallest problems, but significantly fewer messages on the
largest problems they could solve within the timeout limit. On the other hand, because
it is a majority threshold scheme, MPC-DisCSP4 actually could not provide any privacy
guarantees on problems of size 1, since they only involved 2 agents. Both algorithms could
only scale up to problems of size 4, and timed out on larger problems. P2 -DPOP+ s increased topology privacy comes at a price that made it time out earlier than P2 -DPOP;
this complexity increase is due to P2 -DPOP+ s steeper induced width curve.
The P-DPOP(+) algorithms remain the most efficient by far: they perform between 1
and 2 orders of magnitude better than all others, both in terms of runtime and information
exchanged. And like for graph coloring, the improved topology privacy in P-DPOP+ comes
at a price that is negligible for small problems, but can grow to one order of magnitude on
problems of size 6, even if its induced width remains close to that of P-DPOP. In terms of
683

fiLeaute & Faltings

runtime and information exchange, P-DPOP is only worse than DPOP by a small factor
(since it has the same median induced width); however it sends approximately one order of
magnitude more messages (which is mostly due to the pseudo-tree root election mechanism).
6.3 Resource Allocation
Next, we performed experiments on distributed resource allocation benchmarks. Problem
instances were produced using the combinatorial auction problem generator CATS (LeytonBrown, Pearson, & Shoham, 2000), ignoring bid prices. We used the temporal matching
distribution modeling the allocation of airport takeoff/landing slots, fixing the total number
of slots (i.e. resources) to 8, and varying the numbers of bids. Each bid is a request for a
bundle of 2 resources (a takeoff slot and a corresponding landing slot). Multiple requests
may be placed by the same airline company; each airline should have exactly one fulfilled.

Simulated time (in ms)

106

Induced width

5

105

4
MPC

104

3

P2 -DPOP(+)
P-DPOP(+)

103

2

DPOP

102
101

1
0
1

2

3
4
5
6
Number of bids

7

8

1

3
4
5
6
Number of bids

7

8

Information exchanged (in bytes)

Number of messages

106

2

109
108

105

107
104
106
103
105
102
101

104

1

2

3
4
5
6
Number of bids

7

103

8

1

2

3
4
5
6
Number of bids

Figure 10: Performance on resource allocation problems.

684

7

8

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

This problem was modeled as a DisCSP as follows (Leaute & Faltings, 2009). One
agent is introduced for each bidder/airline and for each resource/slot, assuming that each
resource is controller by a different resource provider/airport2 . For each resource X, and
for each bidder B that requests the resource, there is one binary variable xb controlled by
the resource provider, which models whether B is allocated the
P resource (xb = 1) or not
(xb = 0). The resource provider also expresses one constraint
 1 over all her variables
to enforce that her resource can only be allocated to at most one of the interested bidders.
For each variable xb , we also introduce one copy variable bx owned by the bidder B, with
the constraint xb = bx . Each bidder B then expresses a constraint over all her variables,
enforcing that she should only be allocated two resources that correspond exactly to one of
her requests. The introduction of copy variables is motivated by the DisCSP assumption
that each agent knows all constraints involving its variables, and serves two privacy-related
purposes: 1) the full list of agents placing requests on a given resource is only known to the
resource provider, and 2) the full list of resources requested by a given agent (and in which
bundles) is only known to the agent itself. Like for the meeting scheduling problem class, all
inter-agent constraints are equality constraints, therefore we do not report the performance
of P3/2 -DPOP(+) , whose privacy guarantees are the same as P-DPOP(+) .
For MPC-DisCSP4, the DisCSP formulation was simplified by not introducing copy
variables hold by bidders, since they are not necessary to protect constraint privacy: bidders
can request resources by expressing constraints directly over the variables owned by the
resource providers. However, since MPC-DisCSP4 assumes that all variables are public, in
order to increase topology privacy we introduced, for each resource, as many variables as
bidders, regardless of whether they are actually interested in the resource. To reduce the
size of the search space, we assumed that the   1 constraints were public.
Figure 10 shows that the performance of MPC-DisCSP4 decreases very fast with the
number of requests, such that the algorithm was not able to scale beyond problems of
size 4. The P2 -DPOP(+) algorithms seem to scale better, and were able to solve problems
involving 5 requests. On all three metrics, both algorithms were largely outperformed by
P-DPOP(+) , whose runtime curve is remarkably flat, and almost overlaps with the runtime
curve of DPOP, which is consistent with their undistinguishable induced width curves.
The overhead of P-DPOP(+) compared to DPOP is slightly larger in terms of information
exchanged, and goes up to one order of magnitude in terms of number of messages. PDPOP+ and P2 -DPOP+ performed the same as their respective non-plus variants.
6.4 Strategic Game Equilibria
Finally, we report experimental results on one last class of problem benchmarks, which
corresponds to the distributed computation of pure Nash equilibria in strategic games. We
used the particular example of the party game introduced by Singh et al. (2004), which is
a one-shot, simultaneous-move, graphical game (Kearns, Littman, & Singh, 2001) in which
all players are invited to a common party, and each players possible strategies are whether
to attend the party or not. Players are arranged in an undirected social graph, which
defines which other invitees each player knows. Each players reward for attending the
2. CATS assumes there is a single auctioneer, and does not specify which slot is at which airport; this is
why we have assumed that each resource was provided by a separate resource provider.

685

fiLeaute & Faltings

party depends on whether her acquaintances also decide to attend, and on whether she
likes them or not. The reward is 1 per attendee she likes, minus 1 per attendee she dislikes,
and minus a constant cost of attendance in [0, 1]. The reward for not attending is 0.
The problem of computing a Nash equilibrium to such a game can be formulated into
a DisCSP as follows. Each player is an agent, which owns one binary variable for its
strategy, and one copy variable for the strategy of each of its acquaintances. Each variable
is constrained to be equal to each of its copy variables, using binary equality constraints like
for resource allocation problems (Section 6.3). Each agent also expresses one constraint over
all its variables, which only allows a particular strategy for the agent if it is a best response to
its neighbors joint strategies. Notice that the resulting constraint graph is not the same as
the game graph, due to the presence of copy variables. A solution to the DisCSP therefore
yields a joint strategy profile for all players that is a pure Nash equilibrium, since each
player plays best-response to her neighbors. Notice also that, since each player holds a copy

Simulated time (in ms)

106

Induced width

10
9

105

8

104

P2 -DPOP(+)

7

MPC

6

P-DPOP

103

(+)

5
4

DPOP

3

2

10

2
1

10

1
2

3

4
5
6
Number of players

7

2

Number of messages

105

3

4
5
6
Number of players

7

Information exchanged (in bytes)

108
107

104

106
3

10

105
102

101

104

2

3

4
5
6
Number of players

103

7

2

3

4
5
6
Number of players

Figure 11: Performance on party games.

686

7

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

variable for each of her neighbors strategy, these strategies are semi-private information
that cannot be protected, which is why we do not report the performance of P3/2 -DPOP(+) .
For MPC-DisCSP4, the DisCSP formulation can be simplified by not introducing copy
variables (Vickrey & Koller, 2002). An interesting consequence of this difference is that,
contrary to P1 -DPOP(+) , MPC-DisCSP4 is then able to hide each players chosen strategy
from her neighbors. In the context of the party game, this is not very useful to players who
decide to attend the party, since they will necessarily eventually discover whether their
acquaintances also decided to attend or not. On the other hand, a player who declines the
invitation does not directly discover anything about the list of attendees. She might still
be able to make indirect inferences about the decisions of her acquaintances, based on the
fact that her decision to decline is a best response to their respective chosen strategies.
Figure 11 reports on the performance of the algorithms on random acyclic game graphs
of degree 2 (i.e. trees in which each node has at most 2 children), with varying numbers
of players. The P2 -DPOP(+) algorithms were only able to scale up to problems of size 5
due to the rapidly increasing induced width, and were outperformed by MPC-DisCSP4 by
at least one order of magnitude across all three metrics. Both algorithms still performed
largely worse than the P-DPOP(+) algorithms, which are capable of scaling to much larger
problems. This is because, in this setting, the induced width remains bounded: since the
game graphs are acyclic, DPOPs induced width is constantly equal to 2, because each
FEAS message sent by agent ax to its parent agent ay is expressed only over ay s strategy
variable and the copy of ax s strategy variable held by ay . P-DPOP(+) s induced width
is increased by 2 because agent ay has at most 2 children in the pseudo-tree, each using
a different codename for ay s strategy variable. As a result, the performance overhead in
P-DPOP(+) compared to DPOP is minimal in terms of runtime; it is slightly larger in
information exchanged, and reaches one order of magnitude in number of messages.

7. Conclusion
In this paper, we have addressed the issue of providing strong privacy guarantees in Distributed Constraint Satisfaction Problems (DisCSPs). We have defined four types of information about the problem that agents might want to hide from each other: agent privacy
(hiding an agents identity from non-neighbors), topology privacy (keeping the topology
of the constraint graph private), constraint privacy (protecting the knowledge of the constraints), and decision privacy (the final value of each variable should only be known to
its owner agent). Departing from previous work in the literature, which only addressed
subsets of these privacy types, and often focused on quantifying the privacy loss in various algorithms, we have proposed a set of algorithms with strong guarantees about what
information provably will not be leaked.
We have carried out performance experiments on four different classes of benchmarks:
graph coloring, meeting scheduling, resource allocation, and game equilibrium computation.
The results show that our algorithms not only provide stronger privacy guarantees, but also
scale better than the previous state of the art. We have explored the tradeoff between
privacy and performance: the P-DPOP+ variant was shown to scale much better than the
others, but can only guarantee partial constraint and decision privacy, which may still be
considered sufficient in many problem classes. Full decision privacy (P3/2 -DPOP+ ) and full
687

fiLeaute & Faltings

constraint privacy (P2 -DPOP+ ) come at significantly higher prices in computation time and
information exchange, which, with todays hardware, limits their applicability to smaller
problem instances. We have compared the performance of our algorithms against the MPCDisCSP4 algorithm, which can be considered the previous state of the art in DisCSP with
strong privacy guarantees. On the first three classes of benchmarks, all our algorithms
almost systematically outperformed MPC-DisCSP4 in terms of runtime and number of
messages exchanged; however, MPC-DisCSP4 proved to exchange less information than
P>1 -DPOP+ . On game equilibrium computation, MPC-DisCSP4 scaled much better than
P2 -DPOP+ along all three metrics, but was still largely outperformed by P-DPOP+ . In
terms of practical applicability, we have shown that some of our algorithms scale to mediumsize problems that are beyond reach of the previous state of the art in general DisCSP with
strong privacy guarantees. We have also investigated the application of these algorithms to
real-life meeting scheduling, in collaboration with the Nokia Research Center in Lausanne.
Future work could extend the techniques in this paper along several directions. First,
while we have restricted ourselves to pure satisfaction problems for the sake of simplicity,
our algorithms can be easily extended to solve Distributed Constraint Optimization Problems (DCOPs). In fact, our P<2 -DPOP+ algorithms already are optimization algorithms;
only P2 -DPOP+ requires some changes to be applied to DCOPs. These changes involve
replacing ElGamal-encrypted Boolean feasibility values with ElGamal-encrypted, bit-wise
vector representations of integer cost values, as described by Yokoo and Suzuki (2002). This
would incur an increase in complexity that is only linear in an upper bound on the cost of
the optimal solution. An optimization variant of MPC-DisCSP4, called MPC-DisWCSP4,
was also already proposed by Silaghi and Mitra (2004); we report performance comparisons
with our algorithms in other publications (Leaute & Faltings, 2011; Leaute, 2011).
Further avenues of future research could result from relaxing our assumption that agents
are honest, but curious. A number of challenging issues arise when attempting to apply
the techniques in this paper to self-interested agents that can manipulate the protocol in
order to achieve solutions that better suit their selfish preferences. One such issue is that of
verifiability, which involves making it possible to check whether the protocols were executed
as designed, without the need to decrypt the messages exchanged. Another interesting issue
is whether it is possible to modify the algorithms to make them incentive-compatible, such
that it is in each agents best interest to honestly follow the protocol.

Appendix A. Cooperative ElGamal Homomorphic Encryption
Homomorphic encryption is a crucial building block of the privacy-preserving algorithms
introduced in this paper. Encryption is the process by which a message  in this appendix,
a Boolean  can be turned into a cyphertext, in such a way that decrypting the cyphertext
to retrieve the initial cleartext message is impossible (or, in this case, computationally very
hard in the worst case) without the knowledge of the secret encryption key that was used to
produce the cyphertext. An encryption scheme is said to be homomorphic if it is possible
to perform operations on cyphertexts that translate to operations on the initial cleartext
messages, without the need to know the encryption key. ElGamal encryption (Elgamal,
1985) is one such encryption scheme that possesses this homomorphic property.
688

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

A.1 Basic ElGamal Encryption of Booleans
ElGamal encryption can be used to encrypt Booleans such that performing the following
operations on encrypted Booleans is possible without the knowledge of the decryption key:
 the AND of an encrypted and a cleartext Boolean;
 the OR of two encrypted Booleans.
ElGamal encryption is a homomorphic, public key cryptography system based on the
intractability of the Diffie-Hellman problem (Tsiounis & Yung, 1998), which proceeds as
follows. Let p be a safe prime of the form 2rt + 1, where r is a large random number, and
t is a large prime. All numbers and all computations will be modulo p. Let g be a generator
of Zp , i.e. g is such that its powers cover [1, p  1]. With p and g assumed public knowledge,
the ElGamal private key is a chosen random number x  [1, p  2], and the associated public
key is y = gx . A cleartext number m is then encrypted as follows:
E(m) = (, ) = (my r , gr )

(1)

where r is a random number chosen by the encryptor. Decryption proceeds as follows:
my r

=
=m.
x
(gr )x
A useful feature of ElGamal encryption is that it allows to randomize an encrypted value
to generate a new encryption bearing no similarity with the original value. Randomizing
E(m) in Eq. (1) yields:








E 2 (m) = (y r , g r ) = (my r+r , gr+r )
which still decodes to m. To encrypt Booleans, we represent false by 1, and true by a
value z 6= 1, which allows us to compute the AND and OR operations:
E(m)  true = E 2 (m) ;

E(m)  false = E(1)

E(m1 )  E(m2 ) = (1  2 , 1  2 ) = E(m1  m2 ) .
A.2 Cooperative ElGamal Encryption
In the previous ElGamal encryption scheme, decryption can be performed in a single step,
using the private key, which is a secret of the agent that originally encrypted the message.
However, it is also possible to perform ElGamal encryption in such a way that all agents
need to cooperate in order to perform decryption. This is possible through the use of a
compound ElGamal key (x, y) that is generated cooperatively by all agents (Pedersen, 1991):
Distributed Key Generation The ElGamal key pairs (xi , yi ) of n agents can be combined in the following fashion to obtain the compound key pair (x, y):
x = ni=1 xi

y = ni=1 yi .

Distributed Decryption If each agent publishes its decryption share  xi , the message
can be decrypted as follows:


= x =m.
ni=1  xi

689

fiLeaute & Faltings

Appendix B. Routing of Messages along a Circular Variable Ordering
In order to implement the round-robin exchange of vectors briefly presented in Section 4.1,
the variables are ordered along a circular ordering that is mapped to the chosen pseudo-tree,
as illustrated in Figure 5 (page 675) . Each variable needs to be able to send a message to
the previous variable (i.e. clock-wise) in the ordering, which is a challenge in itself because
only neighboring variables should communicate directly. Furthermore, to protect agent and
topology privacy, no agent should know the overall circular ordering. To solve this issue,
Algorithm 9 is the algorithm used in P2 -DPOP (Leaute & Faltings, 2009) to route messages.
Algorithm 9 Sending a message M clock-wise in the circular variable ordering.
Procedure: ToPrevious(M ) for variable x
1: if x is the root of the pseudo-tree then Send the message (LAST, M ) to xs last child
2: else Send the message (PREV, M ) to xs parent
Procedure: RouteMessages() for variable x
3: loop
4:
Wait for an incoming message (type, M ) from a neighbor yi
5:
if type = LAST then
6:
if x is a leaf then Deliver message M to x
7:
else Send the message (LAST, M ) to xs last child
8:
else if type = PREV then
9:
if yi is xs first child then Deliver the message M to x
10:
else Send the message (LAST, M ) to the child before yi in xs list of children
Consider for instance a message M that agent a(x1 ) wants to send to the previous
variable  which is x4 , but a(x1 ) does not know it. Agent a(x1 ) wraps M into a PREV
message that it sends to its parent variable x4 (line 2). Because the sender variable x1 is
x4 s first (and only) child, a(x4 ) infers that it should deliver M to itself (line 9). Consider
that a(x4 ) now wants to forward M to its previous variable  x5 , which a(x4 ) does not
know. Like before, a(x4 ) sends a message (PREV, M ) to its parent variable x3 , which then
reacts by sending a message (LAST, M ) to its last child preceding x4 in its list of children,
which is x5 (line 10). LAST messages indicate that the payload M should be delivered to
the last leaf of the current subtree (line 7); therefore, a(x5 ) delivers M to itself (line 6) since
it has no children. If the root wants to send a message to its previous variable, it also uses
a LAST message to forward it to the last leaf of the overall pseudo-tree (line 1).
Theorem 17. Algorithm 9 guarantees full agent privacy.
Proof. The goal of this algorithm is precisely to address agent privacy issues in the pseudotree rerooting procedure, which involves each variable sending a message to the previous
variable in a circular ordering of the variables. There is no guarantee that there exist a
circular ordering such that any two consecutive variables are owned by neighboring agents,
which is necessary to protect agent privacy. Therefore, Algorithm 9 is responsible for routing
these messages through paths that only involve communication between neighboring agents.
The routing procedure itself only involves encapsulating the routed messages inside
PREV or LAST messages, which do not contain any other payload. Therefore, as long
690

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

as the routed messages do not contain information that can be used to identify a nonneighboring agent, the routing procedure guarantees agent privacy.
Theorem 18. Algorithm 9 guarantees full topology privacy.
Proof. The purpose of this algorithm is to enable variables to propagate messages along
a circular variable ordering, without the need to know any topological information about
the constraint graph, other than the knowledge of their respective (pseudo-)parents and
(pseudo-)children in the pseudo-tree. ToPrevious() makes it possible to send a message
to the previous variable in the circular ordering, without knowing which variable this is.
 The reception of a (PREV, M ) message only indicates that the sender child wants
the included message M to be delivered to its previous variable, which is either the
recipient of the PREV message, or an unknown descendant thereof.
 The reception of a (LAST, M ) message from ones parent indicates that an unknown
variable (either the unknown root of the pseudo-tree, or the unknown child of an
unknown ancestor, in another branch) wants M to be delivered to its previous variable,
which is ones descendant in the pseudo-tree.

References
Ben-Or, M., Goldwasser, S., & Wigderson, A. (1988). Completeness theorems for noncryptographic fault-tolerant distributed computation (extended abstract). In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing (STOC88),
pp. 110.
Bilogrevic, I., Jadliwala, M., Hubaux, J.-P., Aad, I., & Niemi, V. (2011). Privacy-preserving
activity scheduling on mobile devices. In Proceedings of the First ACM COnference
on Data and Application Security and PrivacY (CODASPY11), pp. 261272.
Brito, I., & Meseguer, P. (2003). Distributed forward checking. In Proceedings of the
Ninth International Conference on Principles and Practice of Constraint Programming
(CP03), Vol. 2833 of Lecture Notes In Computer Science, pp. 801806.
Brito, I., & Meseguer, P. (2007). Distributed forward checking may lie for privacy. In
Proceedings of the Ninth International Workshop on Distributed Constraint Reasoning
(CP-DCR07).
Brito, I., & Meseguer, P. (2010). Cluster tree elimination for distributed constraint optimization with quality guarantees. Fundamenta Informaticae, 102, 263286.
Chechetka, A., & Sycara, K. (2006). No-commitment branch and bound search for distributed constraint optimization. In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS06), pp. 1427
1429.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
691

fiLeaute & Faltings

Doshi, P., Matsui, T., Silaghi, M.-C., Yokoo, M., & Zanker, M. (2008). Distributed private
constraint optimization. In Proceedings of the 2008 IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT08), pp. 277281.
Elgamal, T. (1985). A public key cryptosystem and a signature scheme based on discrete
logarithms. IEEE Transactions on Information Theory, 31 (4), 469472.
Faltings, B., Leaute, T., & Petcu, A. (2008). Privacy guarantees through distributed constraint satisfaction. In Proceedings of the 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT08), pp. 350358.
Franzin, M. S., Freuder, E. C., Rossi, F., & Wallace, R. J. (2004). Multi-agent constraint
systems with preferences: Efficiency, solution quality, and privacy loss. Computational
Intelligence, 20 (2), 264286.
Gentry, C. (2009). Fully homomorphic encryption using ideal lattices. In Proceedings of the
Forty-first Annual ACM Symposium on Theory of Computing (STOC09), pp. 169
178. ACM Special Interest Group on Algorithms and Computation Theory (SIGACT).
Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding for distributed constraints optimization. In Proceedings of the Seventeenth European Conference on Artificial Intelligence (ECAI06), pp. 103107.
Goldreich, O. (2009). Foundations of Cryptography, Vol. 2, Basic Applications. Cambridge
University Press.
Greenstadt, R., Grosz, B., & Smith, M. D. (2007). SSDPOP: Using secret sharing to
improve the privacy of DCOP. In Proceedings of the Ninth International Workshop
on Distributed Constraint Reasoning (CP-DCR07).
Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis of privacy loss in distributed
constraint optimization. In Proceedings of the Twenty-First National Conference on
Artificial Intelligence (AAAI06), pp. 647653.
Grinshpoun, T., & Meisels, A. (2008). Completeness and performance of the APO algorithm.
Journal of Artificial Intelligence Research (JAIR), 33, 223258.
Grubshtein, A., Grinshpoun, T., Meisels, A., & Zivan, R. (2009). Asymmetric distributed
constraint optimization. In Proceedings of the IJCAI09 Distributed Constraint Reasoning Workshop (DCR09), pp. 6074.
Gutierrez, P., & Meseguer, P. (2010). BnB-ADOPT+ with several soft arc consistency
levels. In Proceedings of the Nineteenth European Conference on Artificial Intelligence
(ECAI10), No. 215 in Frontiers in Artificial Intelligence and Applications, pp. 6772.
Herlea, T., Claessens, J., Preneel, B., Neven, G., Piessens, F., & Decker, B. D. (2001). On securely scheduling a meeting. In Proceedings of the Sixteenth International Conference
on Information Security  Trusted information: the new decade challenge (SEC01),
International Federation For Information Processing (IFIP) Series, pp. 183198.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem.
In Proceedings of the Third International Conference on Principles and Practice of
Constraint Programming (CP97), Vol. 1330 of Lecture Notes in Computer Science,
pp. 222236.
692

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

Kearns, M. J., Littman, M. L., & Singh, S. P. (2001). Graphical models for game theory.
In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence
(UAI01), pp. 253260.
Kuchcinski, K., & Szymanek, R. (2011). Java library: JaCoP Java constraint programming
solver. http://jacop.osolpro.com/.
Leaute, T. (2011). Distributed Constraint Optimization: Privacy Guarantees and Stochastic
Uncertainty. PhD thesis, Ecole Polytechnique Federale de Lausanne (EPFL).
Leaute, T., & Faltings, B. (2009). Privacy-preserving multi-agent constraint satisfaction.
In Proceedings of the 2009 IEEE International Conference on PrivAcy, Security, riSk
And Trust (PASSAT09), pp. 1725.
Leaute, T., & Faltings, B. (2011). Coordinating logistics operations with privacy guarantees. In Proceedings of the Twenty-Second International Joint Conference on Artificial
Intelligence (IJCAI11), pp. 24822487.
Leaute, T., Ottens, B., & Szymanek, R. (2009). FRODO 2.0: An open-source framework for
distributed constraint optimization. In Proc. of the IJCAI09 Distributed Constraint
Reasoning Workshop (DCR09), pp. 160164. http://frodo2.sourceforge.net.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards a universal test suite for
combinatorial auction algorithms. In Proceedings of the Second ACM Conference on
Electronic Commerce (EC00), pp. 6676. ACM Special Interest Group on Electronic
Commerce (SIGEcom). http://www.cs.ubc.ca/~kevinlb/CATS.
Maheswaran, R. T., Pearce, J. P., Bowring, E., Varakantham, P., & Tambe, M. (2006).
Privacy loss in distributed constraint reasoning: A quantitative framework for analysis
and its applications. Autonomous Agents and Multi-Agent Systems (JAAMAS), 13 (1),
2760.
Maheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004).
Taking DCOP to the real world: Efficient complete solutions for distributed multievent scheduling. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS04), Vol. 1, pp. 310317. ACM
Special Interest Group on Artificial Intelligence (SIGART).
Mailler, R., & Lesser, V. R. (2003). A mediation based protocol for distributed constraint
satisfaction. In Proceedings of the Fourth International Workshop on Distributed Constraint Reasoning (DCR03).
Meisels, A., & Zivan, R. (2003). Asynchronous forward-checking on DisCSPs. In Proceedings
of the Fourth International Workshop on Distributed Constraint Reasoning (DCR03).
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraint optimization with quality guarantees. Artificial Intelligence, 161,
149180.
Netzer, A., Meisels, A., & Grubshtein, A. (2010). Concurrent forward bounding for DCOPs.
In Proceedings of the Twelfth International Workshop on Distributed Constraint Reasoning (DCR10), pp. 6579.
693

fiLeaute & Faltings

Pedersen, T. P. (1991). A threshold cryptosystem without a trusted party (extended abstract). In Advances in Cryptology  EUROCRYPT91, Workshop on the Theory and
Application of Cryptographic Techniques, Proceedings, Vol. 547 of Lecture Notes in
Computer Science, pp. 522526.
Petcu, A., & Faltings, B. (2005). DPOP: A Scalable Method for Multiagent Constraint
Optimization. In Proceedings of the Nineteenth International Joint Conference on
Artificial Intelligence (IJCAI05), pp. 266271.
Petcu, A., Faltings, B., & Parkes, D. C. (2008). M-DPOP: Faithful distributed implementation of efficient social choice problems. Journal of Artificial Intelligence Research
(JAIR), 32, 705755.
Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). A combinatorial auction mechanism
for airport time slot allocation. The Bell Journal of Economics, 13 (2), 402417.
Shamir, A. (1979). How to share a secret. Communications of the ACM, 22 (11), 612613.
Silaghi, M.-C. (2005a). Hiding absence of solution for a distributed constraint satisfaction
problem (poster). In Proceedings of the Eighteenth International Florida Artificial
Intelligence Research Society Conference (FLAIRS05), pp. 854855.
Silaghi, M.-C. (2005b). Using secure DisCSP solvers for generalized Vickrey auctions 
complete and stochastic techniques. In Proceedings of the IJCAI05 Distributed Constraint Reasoning Workshop.
Silaghi, M.-C., Faltings, B., & Petcu, A. (2006). Secure combinatorial optimization simulating DFS tree-based variable elimination. In Proceedings of the Ninth International
Symposium on Artificial Intelligence and Mathematics.
Silaghi, M.-C., & Mitra, D. (2004). Distributed constraint satisfaction and optimization
with privacy enforcement. In Proceedings of the 2004 IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT04), pp. 531535.
Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search with aggregations. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence
(AAAI/IAAI00), pp. 917922.
Singh, S., Soni, V., & Wellman, M. P. (2004). Computing approximate Bayes-Nash equilibria
in tree-games of incomplete information. In Proceedings of the Fifth ACM Conference
on Electronic Commerce (EC04), pp. 8190.
Sultanik, E. A., Lass, R. N., & Regli, W. C. (2007). DCOPolis: A framework for simulating
and deploying distributed constraint optimization algorithms. In Proceedings of the
Ninth International Workshop on Distributed Constraint Reasoning (CP-DCR07).
Tsiounis, Y., & Yung, M. (1998). On the security of Elgamal-based encryption. In Proceedings of the First International Workshop on Practice and Theory in Public Key
Cryptography (PKC98), Vol. 1431 of Lecture Notes in Computer Science, pp. 117134.
Vickrey, D., & Koller, D. (2002). Multi-agent algorithms for solving graphical games. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI02),
pp. 345351.
694

fiProtecting Privacy thru Distributed Computation in Multi-agent Decision Making

Vinyals, M., Rodrguez-Aguilar, J. A., & Cerquides, J. (2010). Constructing a unifying
theory of dynamic programming DCOP algorithms via the generalized distributive
law. Autonomous Agents and Multi-Agent Systems (JAAMAS), 22 (3), 439464.
Wallace, R. J., & Freuder, E. C. (2005). Constraint-based reasoning and privacy/efficiency
tradeoffs in multi-agent problem solving. Artificial Intelligence, 161 (12), 209227.
Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: An asynchronous branch-andbound DCOP algorithm. Journal of Artificial Intelligence Research (JAIR), 38, 85
133.
Yokoo, M. (1995). Asynchronous weak-commitment search for solving distributed constraint
satisfaction problems. In Proceedings of the First International Conference on Principles and Practice of Constraint Programming (CP95), No. 976 in Lecture Notes In
Computer Science, pp. 88102.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction for formalizing distributed problem solving. In Proceedings of the Twelfth
International Conference on Distributed Computing Systems (ICDCS92), pp. 614
621.
Yokoo, M., & Suzuki, K. (2002). Secure multi-agent dynamic programming based on homomorphic encryption and its application to combinatorial auctions. In Proceedings
of the First International Joint Conference on Autonomous Agents and Multi-Agent
Systems (AAMAS02), pp. 112119.
Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction:
Reaching agreement without revealing private information. In Proc. 8th Intl. Conf. on
Principles and Practice of Constraint Prog. (CP02), Vol. 2470 of LNCS, pp. 387401.
Yokoo, M., Suzuki, K., & Hirayama, K. (2005). Secure distributed constraint satisfaction:
Reaching agreement without revealing private information. Artificial Intelligence,
161 (12, Distributed Constraint Satisfaction), 229245.
Zivan, R., & Meisels, A. (2004). Concurrent dynamic backtracking for distributed CSPs.
In Proceedings of the Tenth International Conference on Principles and Practice of
Constraint Programming (CP04), Vol. 3258 of Lecture Notes In Computer Science,
pp. 782787.

695

fiJournal of Artificial Intelligence Research 47 (2013) 157-203

Submitted 11/12; published 05/13

A Survey on Latent Tree Models and Applications
Raphael Mourad

raphael.mourad@aliceadsl.fr

LINA, UMR CNRS 6241,
Ecole Polytechnique de lUniversite de Nantes
Nantes, Cedex 3, 44306 France

Christine Sinoquet

christine.sinoquet@univ-nantes.fr

LINA, UMR CNRS 6241, Universite de Nantes
Nantes, Cedex, 44322 France

Nevin L. Zhang
Tengfei Liu

lzhang@cse.ust.hk
liutf@cse.ust.hk

Department of Computer Science & Engineering, HKUST
Clear Water Bay Road, Kowloon, Hong Kong

Philippe Leray

philippe.leray@univ-nantes.fr

LINA, UMR CNRS 6241,
Ecole Polytechnique de lUniversite de Nantes
Nantes, Cedex 3, 44306 France

Abstract
In data analysis, latent variables play a central role because they help provide powerful
insights into a wide variety of phenomena, ranging from biological to human sciences. The
latent tree model, a particular type of probabilistic graphical models, deserves attention.
Its simple structure - a tree - allows simple and efficient inference, while its latent variables
capture complex relationships. In the past decade, the latent tree model has been subject
to significant theoretical and methodological developments. In this review, we propose a
comprehensive study of this model. First we summarize key ideas underlying the model.
Second we explain how it can be efficiently learned from data. Third we illustrate its use
within three types of applications: latent structure discovery, multidimensional clustering,
and probabilistic inference. Finally, we conclude and give promising directions for future
researches in this field.

1. Introduction
In statistics, latent variables (LVs), as opposed to observed variables (OVs), are random
variables which are not directly measured. A wide range of statistical models, called latent
variable models, relate a set of OVs to a set of LVs. In these models, LVs explain
dependences among OVs and hence offer compact and intelligible insights of data. Moreover
LVs allow to reduce data dimensionality and to generate conditionally independent variables,
which considerably simplifies downstream analysis. Applications are numerous and cover
many scientific fields. This is typically the case in domains such as psychology, sociology,
economics, but also biological sciences and artificial intelligence, to cite some examples.
Such fields may need complex constructs that cannot be observed directly. For instance,
human personality in psychology and social class in socio-economics refer to higher level
abstractions than observed reality.
c
2013
AI Access Foundation. All rights reserved.

fiMourad, Sinoquet, Zhang, Liu, & Leray

1.1 Context
Latent tree model (LTM)1 is a class of latent variable models which has received considerable attention. LTM is a probabilistic tree-structured graphical model where leaf nodes
are observed while internal nodes can be either observed or latent. This model is appealing
since its simple structure - a tree - allows simple and efficient inference, while its latent
variables capture complex relationships.
A subclass of LTMs was first developed in the phylogenetic community (Felsenstein,
2003). In this context, leaf nodes are observed taxa while internal nodes represent unobserved taxum ancestors. For instance, in molecular genetic evolution, which is the process
of evolution at DNA scale, it is obviously hopeless to study DNA sequences in some dead
species from collecting their DNA. Nevertheless, evolutionary latent models can be used to
infer the most probable ancestral sequences knowing contemporary living species sequences.
Neighbor joining represents one of the first algorithms developed for LTM learning in the
phylogenetic context (Saitou & Nei, 1987; Gascuel & Steel, 2006). It is still very popular
as it is quick to compute and it allows to find the optimal model in polynomial time under
certain assumptions.
During the past decade, LTMs in their general form have been under extensive investigation and have been applied to many fields. For instance they have been applied in human
interaction recognition. Human interaction recognition is a challenging task, because of
multiple body parts and concomitant inclusions (Aggarwal & Cai, 1999). For this purpose,
the use of LTM allows to segment the interaction in a multi-level fashion (Park & Aggarwal,
2003): body part positions are estimated through low-level LVs, while overall body position
is estimated by a high-level LV. LTMs have also been used in medical diagnosis (Zhang,
Yuan, Chen, & Wang, 2008). In this context, LTMs provide a way to identify, through the
LVs, the different syndrome factors which cannot be directly observed by the physician.
1.2 Contributions
In this paper, we present a comprehensive study of LTM and a broad-brush view of its recent
theoretical and methodological developments. LTM must be paid attention because (i) it
offers deep insights for latent structure discovery (Saitou & Nei, 1987), (ii) it can be applied
to multidimensional clustering (Chen, Zhang, Liu, Poon, & Wang, 2012) and (iii) it allows
efficient probabilistic inference (Wang, Zhang, & Chen, 2008). Somewhat surprisingly, no
extensive review on this research area has been published.
In addition to the reviewing of the LTM research area, we also contribute an analysis and
a perspective that advance our understanding of the subject. We establish a categorization
of learning methods. We present generic learning algorithms implementing fundamental
principles. These generic algorithms are partly different from those of the literature because
they have been adapted to a broader context. Besides, the performances of all the algorithms
of the literature are compared in the context of small, large and very large simulated and
real datasets. Finally, we discuss future directions, such as the adaptation of LTM for
continuous data.
1. LTM has been previously called hierarchical latent class model (Zhang, 2004), but this name has been
discarded because the model does not inherently reveal a hierarchy.

158

fiLatent Tree Models

Figure 1: Illustration of graph theory terminology.

1.3 Paper Organization
This paper is organized as follows. Section 2 presents the latent tree model and related
theoretical developments. In Section 3, we review methods developed to learn latent tree
models for the two main situations: learning when structure is known and learning when it
is not the case. Then, Section 4 presents and details three types of applications of latent tree
models: latent structure discovery, multidimensional clustering and probabilistic inference.
Other applications such as classification are also discussed. Finally, the last two sections 5
and 6 conclude and point out future directions.

2. Theory
In this section, we first introduce graph terminology and then present LTM. Latent classes
and probabilistic inference for clustering are next presented. Scoring LTMs is discussed. We
also present the concepts of marginal equivalence, equivalence and model parsimony, useful
for LTM learning. Then, we explain the necessity of a trade-off between latent variable
complexity and partial structure complexity.
Variables are denoted by capital letters, e.g. A, B and C, whereas lower-case letters
refer to values that variables can take, e.g. a, b and c. Bold-face letters represent sets
of objects, that is A, B and C are sets of variables while a, b and c are value sets. An
observed variable is denoted X whereas a latent variable is denoted H. A variable about
which we do not know if it is observed or latent is denoted V .
2.1 Graph Theory Terminology
Before presenting LTM, we first need to define graph-related terms, which are illustrated in
Figure 1. A graph G(V, E) is composed of a set of nodes V and a set of edges E  V  V.
An edge is a pair of nodes (Va , Vb )  E. The edge is undirected (noted Va Vb ) if (Vb , Va )  E
and directed (noted Va  Vb ) if not.
159

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 2: (a) Directed tree. (b) Undirected tree. The light shade (blue) indicates the
observed variables whereas the dark shade (red) points out the latent variables.

A directed graph is a graph whose all edges are directed. In a directed graph, a node
Va is a parent of a node Vb if and only if there exists an edge from Va to Vb . The node Vb is
then called the child of node Va . Nodes are siblings if they share the same parent. A node
Vc is a root if it has no parent. A directed path from a node Vd to a node Va is a sequence
of nodes such that for each node except the last one, there is an edge to the next node in
the sequence. A node Va is a descendant of a node Vd if and only if there is a directed path
from Vd to Va . The node Vd is then called an ancestor of node Va .
An undirected graph only contains undirected edges. In an undirected graph, a node Va
is a neighbor of another node Vb if and only if there is an edge between them. A leaf is a
node having only one neighbor. An internal node is a node having at least two neighbors.
An undirected path is a path for which the edges are not all oriented in the same direction.
A clique is a set of pairwise connected nodes (in a tree, a clique is simply an edge).
A separator is a set of nodes whose removal disconnect two or more cliques (in a tree, a
separator is simply an internal node). A tree T is a graph for which any two nodes are
connected by exactly one path.
2.2 Latent Tree Model
LTM is a tree-structured graphical model with latent variables. It is composed of a tree the structure - T (V, E), and a set of parameters, . The tree can be either directed (i.e.
a Bayesian network; Zhang, 2004) or undirected (i.e. a Markov random field; Choi, Tan,
Anandkumar, and Willsky, 2011). Both representations are described in Figure 2. The set of
nodes V = {V1 , ..., Vn+m } represents n + m observed and latent variables. X = {X1 , ..., Xn }
is the set of observed variables and H = {H1 , ..., Hm } is the set of latent variables. Leaf
nodes are OVs while internal nodes can be either observed or latent. Variables can be either
discrete or continuous. The set of k edges E = {E1 , ..., Ek } captures the direct dependences
between these variables.
In the directed setting (Figure 2a), the set of parameters  consists of probability distributions, one for each variable. Given a variable Vi with parents P aVi , a conditional
distribution P (Vi |P aVi ) is defined. For a variable Vi that has no parent, a marginal distribution P (Vi ) is defined instead. The joint probability distribution (JPD) of this model is
160

fiLatent Tree Models

formulated as:
n+m
P (V) = i=1
P (Vi |P aVi ).

(1)

To illustrate the model, let us take the example in Figure 2a. It is composed of a set of
OVs {X1 , ..., X7 } and a set of LVs {H1 , H2 }. Its JPD writes as:
P (X1 , ..., X7 , H1 , H2 ) = P (X1 |H1 ) P (X2 |H1 ) P (X3 |H1 ) P (H1 |X7 ) P (X7 )
 P (X4 |X7 ) P (H2 |X7 ) P (X5 |H2 ) P (X6 |H2 ).

(2)

In the undirected setting (Figure 2b), the set of parameters  consists of probability
distributions, one for each clique and separator. In LTM, cliques are edges while separators
are internal nodes. Let {I1 , ..., Ij } be the separators. The JPD of this model is formulated
as:
(V ,V )E P (Va , Vb )
,
(3)
P (V) = ma b
j=1 P (Ij )(d(Ij )1)
where d(Ij ) is the degree of internal node Ij . The JPD of the undirected model in Figure
2b writes as:
P (X1 , H1 ) P (X2 , H1 ) P (X3 , H1 ) P (H1 , X7 ) P (X4 , X7 )
P (H1 ) P (H1 ) P (H1 ) P (X7 )
P (X7 , H2 ) P (X5 , H2 ) P (X6 , H2 )

.
P (X7 ) P (H2 ) P (H2 )

P (X1 , ..., X7 , H1 , H2 ) =

(4)

In the following, for the seek of simplicity, we restrain the study to categorical variables,
i.e. random variables which have a finite number of states. We also mainly focus on LTM
whose internal nodes are all LVs. Most works on LTM have been developed for these two
model settings.
2.3 Latent Classes and Clustering
An LV has a number of states, each of them representing a latent class. All latent classes
together represent a soft partition of the data and define a finite mixture model (FMM).
LTM can be seen as multiple FMMs connected to form a tree. Given a data point, the
probability of belonging to a particular class can be computed using the Bayes formula.
This computation is called class assignment.
In an LTM, each LV Hj represents a partition of the data. For an observation ` and the
vector of its values x` = {x`1 , ..., x`n } on the set of OVs X, the probability of its membership
to a class c of an LV Hj can be computed as follows:
P (x` |Hj = c) P (Hj = c)
P (x` )
P
0
`
0 P (x , H |Hj = c) P (Hj = c)
= Pk HP
0
`
c=1
H0 P (x , H |Hj = c) P (Hj = c)

P (Hj = c|x` ) =

(5)

with H0 = H\{Hj } and k the cardinality of Hj . From the last formula, the reader might get
the impression that the complexity of class assignment is exponential. However, in trees,
161

fiMourad, Sinoquet, Zhang, Liu, & Leray

linear2 - thus efficient - probabilistic inference, using message passing (Kim & Pearl, 1983),
can be used to compute P (Hj = c|x` ).
Probabilistic inference of LV values has two applications: clustering and latent data
imputation. Clustering using LTMs will be illustrated and seen in detail in Section 4.2.
Latent data imputation is the process of inferring, for each observation `, the values of
LVs. These variables are called imputed LVs. In Section 3.2.2, we will see that latent data
imputation is at the basis of fast variable clustering-based LTM learning methods.
2.4 Scoring Latent Tree Models
In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and
Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring
LTMs. In practice, the BIC score is often used for LTMs. Let consider a set of n OVs
X = {X1 , ..., Xn } and a collection of N identical and independently distributed (i.i.d.)
observations Dx = {x1 , ..., xN }. BIC is composed of two terms:
1
BIC(T, Dx ) = log P (Dx | M L , T )  dim(T ) log N,
2

(6)

with  M L the maximum likelihood parameters, dim(T ) the model dimension and N the
number of observations. The first term evaluates the fit of the model to the data. It is
computed through probabilistic inference of P (x` ) for each observation `. The second term
of the score penalizes the model according to its dimension, to prevent overfitting.
In models without latent variables, the dimension is simply calculated as the number
of free parameters. This is sometimes called standard dimension. When LVs are present,
standard dimension is no longer an appropriate measure of model complexity, and effective
dimension should be used instead (Geiger, Heckerman, & Meek, 1996). Effective dimension
is computed as the rank of the Jacobian matrix of the mapping from the model parameters
to the OV joint distribution.
2.5 Model Parsimony
Let us consider two LTMs, M = (T, ) and M0 = (T 0 ,  0 ), built on the same set of n
OVs, X = {X1 , ..., Xn }. We say that M and M0 are marginally equivalent if their joint
distributions on OVs are equal:
P (X1 , ..., Xn |T, ) = P (X1 , ..., Xn |T 0 ,  0 ).

(7)

If two marginally equivalent models have the same dimension, they are equivalent models.
A model M is parsimonious3 if there does not exist another model M0 that is
marginally equivalent and has a smaller dimension. A parsimonious model has the best
possible score. It does not contain any redundant LVs or any redundant latent classes. It
represents the model to infer from data. Two conditions ensure that an LTM does not
include any redundant LVs (Pearl, 1988):
2. Actually, in trees, inference is linear with the number of edges |E|, and is thus also linear with the number
n + m of observed and latent variables, because |E|  n + m  1.
3. The notion of parsimony is also called minimality by Pearl (1988).

162

fiLatent Tree Models

Figure 3: Illustration of the trade-off between latent variable complexity and partial structure complexity in latent tree models. Superscript represents LV cardinality. See
Figure 2 for node color code.

 An LV must have at least three (observed or latent) neighbors. If it has only two
neighbors, it can simply be replaced by a direct link between the two.
 Any two variables connected by an edge in the LTM are neither perfectly dependent
nor independent.
There is also a condition ensuring that an LTM does not include any redundant latent
classes. Let H be an LV in an LTM. The set of k variables Z = {Z1 , ..., Zk } are the
neighbors of H. An LTM is regular (Zhang, 2004) if for any LV H:
|H| 

ki=1 |Zi |
.
maxki=1 |Zi |

(8)

Zhang (2004) showed that all parsimonious models are necessarily regular. Thus the
model search can be restricted to the space of regular models. Zhang also demonstrated
2
that the space of regular models is upper bounded by 23n , where n is the number of OVs.
2.6 Trade-off between Latent Variable Complexity and Partial Structure
Complexity
Zhang and Kocka (2004b) distinguished two kinds of model complexity in LTM: latent
variable complexity refers to LV cardinalities while partial structure complexity4 is
4. In their paper, Zhang and Kocka (2004b) called it structure complexity. For a better understanding, we
prefer to make the distinction between (complete) structure which includes LV cardinalities and partial
structure which does not.

163

fiMourad, Sinoquet, Zhang, Liu, & Leray

the edges and number of LVs in the graph. The trade-off between these two complexities has
an important role to play when one wants to choose a model. This trade-off is illustrated
in Figure 3. For instance, let us consider a latent class model (i.e. a model with only one
LV, abbreviated LCM) versus an LTM having the same marginal likelihood (marginally
equivalent models). The LCM is the model showing the highest LV complexity and the
lowest partial structure complexity. It might have a low score if local dependences are
present between OVs. At the opposite, the model with a low LV complexity and a high
partial structure complexity, a binary tree with binary LVs, would also have a low score,
because some LVs would be unnecessary. Depending on the application, a model showing a
good trade-off between the two complexities should be preferred, because it would present
a better score and might be easier to interpret.

3. Statistical Learning
In this section, we present generic algorithms implementing fundamental principles for learning LTMs. These algorithms are partly different from those proposed in the literature,
because they have been adapted to a broader context. Moreover we provide a unified presentation of algorithms, in the context of this survey. When learning a model from data,
two main situations have to be distinguished: when structure is known and only parameters
have to be learned, and the more complicated situation where both are unknown.
3.1 Known Structure
In the simplest situation, structure is known, i.e. not only the dependences between variables but also the number of LVs and their respective cardinalities (i.e. the numbers of
latent classes). The problem is to estimate probability parameters. To solve the problem,
one can use expectation-maximization (EM), the most popular algorithm for learning parameters in the face of LVs (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995). Because
EM leads to computational burden for large LTMs, a more efficient procedure, that we
call LCM-based EM, can be used. Other methods different from EM, such as spectral
techniques, have also been developed.
3.1.1 Expectation-maximization
Ideally, when learning parameters, we would like to maximize the log-likelihood for a set of
N i.i.d. observed data Dx = {x1 , ..., xN }:
X
L(; Dx ) = log P (Dx |) = log
P (Dx , H|).
(9)
H

However, directly maximizing L(; Dx ) in Equation (9) is often intractable because it involves the logarithm of a (large) sum. To overcome the difficulty, EM implements an iterative approach. At each iteration, it optimizes instead the following expected log-likelihood
conditional on current parameters  t :
Q(;  t ) = EDh |Dx ,t [log P (Dx , Dh |)]

(10)

where Dx is completed by the missing data Dh = {h1 , ..., hN } inferred using  t . Note that
by completing the missing data, EM can easily deal with partially observed variables. An
164

fiLatent Tree Models

Algorithm 1 LCM-based EM parameter learning (LCMB-EM, adapted from Harmeling
and Williams, 2011)
INPUT:
T , the tree structure of the LTM.
OUTPUT:
, the parameters of the LTM.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

T 0  graph rooting(T ) /* choose an LV as a root of T */
Ho   /* initialization of the set of imputed latent variables */
0  
loop
TLCM = {TLCM1 , ..., TLCMk }  identif y LCM s of graph(T 0 , Ho )
LCM = {LCM1 , ..., LCMk }  EM (TLCM )
if |TLCM | > 1 then
 0   0  children parameters(LCM )
else
 0   0  children and parent parameters(LCM)
break
end if
Ho  Ho  impute LV data(LCM ) /* now parents are observed */
end loop
  EM ( 0 ) /* global EM using  0 as a starting point */

important drawback of EM is that it does not guarantee to reach the global optimum. To
reduce the probability of getting trapped into a local maximum, random restarts (multiple
starts with different random initial parameters) or simulated annealing represent well-used
solutions. Wang and Zhang (2006) showed that a few random restarts suffice when the LTM
is small and variables are strongly dependent with each other. In Supplemental material
B.1, we also present our experiments on random restarts for EM. It appears that it is not
possible to give a simple answer to how many restarts should be used because it depends on
the model. Besides, convergence is sometimes not reached even after a very large number
of restarts.
3.1.2 LCM-based EM
Although inference is linear in LTMs, running EM might be prohibitive. One solution
to speed up EM computations consists in chaining two steps: a first step of divide-andconquer strategy through local - LCM - learning, followed by a final step carrying out global
learning. This LCM-based EM (LCMB-EM) parameter learning method5 is presented in
Algorithm 1 and illustrated in Figure 4. In the first step, parameters are locally learned
through a bottom-up LCM-based learning procedure explained as follows. An LV is first
5. This learning procedure is very similar to the one proposed for binary trees by Harmeling and Williams
(2011).

165

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 4: Illustration of LCM-based EM parameter learning algorithm (adapted from
Harmeling and Williams, 2011). See Figure 2 for node color code.

chosen as a root of the LTM (line 1; note that this point will be discussed in detail in
Section 3.2.5). In the rooted LTM, every LCM {X, H} is identified (line 5), i.e. every LV
H whose all children are OVs X. Then, LCM parameters can be quickly learned through
EM (line 6) and used to update current LTM parameters (lines 7 to 12). Once parameters
have been learned for such an LCM, the distributions of unknown values of LV H can be
probabilistically inferred for each observation6 (line 13; for more details, see Section 2.3,
paragraph 2). These distributions can then be used as weighted observations. In their turn,
these weighted observations will seed the learning processes for the LCMs on the (now
observed) LVs Ho and remaining OVs which have not been used for LCM learning yet.
Iterating the two operations (LCM learning and latent data imputation) leads to a bottomup LCM-based EM procedure enabling local and fast parameter learning of LTMs. Finally,
LTM parameters are refined through global EM using as starting point the locally learned
parameters (line 15). As with EM, we also provide the results of experiments evaluating
the efficiency of random restarts (see Supplemental material B.1). Our results show that
LCMB-EM converges better than EM. However convergence is not achieved for the largest
dataset studied.
3.1.3 Spectral Methods
Recently, Parikh et al. (2011) applied spectral techniques to LTM parameter learning. The
method directly estimates the joint distribution of OVs without explicitly recovering the
6. We recall that this process is named latent data imputation.

166

fiLatent Tree Models

LTM parameters. Their algorithm is useful when LTM parameters are not required, for
instance, for probabilistic inference over OVs only. The work of Parikh et al. alleviates the
restriction of the approach of Mossel et al. (2006) requiring that all conditional probability
tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to
hidden Markov models.
Parikh et al. (2011) reformulated the message passing algorithm using an algebraic
formulation:
X
n+m
P (x) =
i=1
P (vi |P avi )
H
>

= r (Mj1 Mj2 ... MjJ 1r ),

(11)

where x = {x1 , ..., xn } is an observation, Xr is a root, r is the marginal probability vector of
the root and Mj1 , Mj2 , ..., MjJ are the incoming message matrices from the roots children.
Children message matrices are calculated in a similar manner:
 1 (Mj1 Mj2 ... MjJ 1i ),
Mi = Ti 

(12)

where Xi is a root child, Mi is the outgoing message matrix from Xi , Ti is a third order tensor related to the conditional probability matrix between Xi and P aXi (i.e. Xr ),
 1 is the mode-1 vector
Mj1 , Mj2 , ..., MjJ are the Xi s children incoming messages and 
product. Such as in the original message passing algorithm, all messages are recursively
calculated starting from the leaves and going up to the root.
The drawback of the previous representation is that message passing still needs model
parameters. To tackle this issue, the key is to recover P (x) using invertible transformations.
Message matrices are then calculated by transforming each message Mj by two invertible
matrices Lj and Rj (Lj R1
j = I):
1
1
1
 1 (Lj1 L1
Mi = Ti 
j1 Mj1 Rj1 Lj2 Mj2 Rj2 ... LjJ MjJ RjJ RjJ 1i ).

(13)

The matrices Lj , Mj and Rj can be recovered from singular vectors Uj of the empirical
probability matrices P (Xj , Xj ) of OVs Xj and their left neighbor OV Xj . This leads to
a very efficient computation of message passing only involving a sequence of singular value
decompositions of empirical pairwise joint probability matrices. We refer to the work of
Parikh et al. (2011) for more details about the singular value decomposition and spectral
algorithm. Compared to EM, this spectral method does not entail the problem of getting
trapped in local maxima. Moreover, it performs comparable to or better than EM while
being orders of magnitude faster.
3.1.4 Other Methods
Other methods exist for parameter learning. Gradient descent (Kwoh & Gillies, 1996;
Binder, Koller, Russel, & Kanazawa, 1997) and variations of the Gauss-Newton method
(Xu & Jordan, 1996) help accelerate the sometimes slow convergence of EM. However, they
require the evaluation of first and/or second derivatives of the likelihood function. For a
Bayesian learning, variational Bayes (Attias, 1999) offers a counterpart of EM.
167

fiMourad, Sinoquet, Zhang, Liu, & Leray

3.2 Unknown Structure
Regrettably, most of the time, there is no a priori information on the LTM structure. This
compels to learn every part of the model, i.e. the number of LVs, their cardinalities, the
dependences and the parameters. This learning task represents a challenging issue, for
which various methods have been conceived. In this section, we provide a survey of those
algorithms. The determination of LV cardinalities, as well as the time complexity and
scalability of algorithms are also discussed. We end by establishing a summary relative to
these learning methods.
Structure learning approaches fall into three categories. The first one is comprised of
search-based methods, inspired from standard Bayesian network learning. The second one
is based on variable clustering and is related to hierarchical procedures. The last category
relies on the notion of distances and comes from phylogenetics.
3.2.1 Search-based Methods
Search-based methods aim at finding the model that is optimal according to some scoring
metric. For BNs without LVs, the BIC score is often used. In the context of LTM, BIC
suffers from a theoretical shortcoming as pointed out in Section 2.4. However, empirical
results indicate that the shortcoming does not seem to compromise model quality in practice (Zhang & Kocka, 2004a). So, researchers still use BIC when it comes to learning LTM.
Many search procedures have been proposed. They all explore the space of regular LTMs.
Here we focus on: (i) the most naive one which is conceptually simple but very computationally expensive and (ii) the most advanced one which reduces the search space and
implements fast parameter learning through local EM.

Naive Greedy Search
Naive greedy search (NGS) consists in starting from an LCM and then visiting the space
of regular LTM partial structures. The neighborhood of the current model is explored by
greedy search through operations such as addition or removal of a latent node, and node
relocation7 . For each partial structure neighbor, the cardinalities of all LVs are optimized
through addition or dismissal of a state relative to an LV. During the model search (partial
structure and LV cardinality), candidate models are learned with EM and evaluated through
a score. If the best candidate model shows a score superior to the current model score, then
the former is used as a seed for the next step. Otherwise, NGS stops and the current model
is considered as the best model. Therefore, at each step of the search, the learning approach
necessitates to evaluate the score of a very large number of candidate models. This leads to
a huge computational burden because, for each candidate model evaluation, the likelihood
has to be computed through EM.

7. Node relocation picks a child of an LV and then grafts it as a child of another LV which is connected to
the former LV.

168

fiLatent Tree Models

Advanced Greedy Search
Advanced greedy search (AGS) relies on three strategies to reduce the complexity.
Advanced greedy search is presented in Algorithm 2. First, AGS focuses on a smaller
space of models to explore than NGS (Zhang & Kocka, 2004b). The algorithm performs
partial structure search and LV cardinality exploration simultaneously. For this purpose,
two additional operators are used: addition and removal of a latent state for an LV.
Second, AGS follows a grow-restructure-thin strategy to reduce again the complexity of
the search space (Chen, Zhang, & Wang, 2008; Chen et al., 2012). The strategy consists
in dividing the five operators into three groups. Each group is applied at a given step of
the model search. First, latent node and latent state introduction (NI and SI, respectively)
are used to make the current model more complex8 (grow, line 3). Then, node relocation
(NR) rearranges connections between variables (restructure, line 4). Finally, latent node
and latent state deletion (ND and SD, respectively) make the current model simpler (thin,
line 5).
Third, one needs to assess the BIC score of candidate models. Learning parameters of
the new models is thus required. To achieve fast learning, Chen et al. (2008, 2012) do
not compute likelihood but instead the so-called restricted likelihood through the local EM
procedure (line 12). The principle relies only on optimizing parameters of variables whose
connection or cardinality was changed in the candidate model. Parameters of remaining
variables are kept identical as in the current model.
Operation Granularity
When starting from the simplest solution (an LCM), Zhang and Kocka (2004b) observed
that the comparison of the BIC scores between the candidate model T 0 and the current one
T might not be a relevant criterion. The problem is that this strategy always leads to
increase the cardinality of the LCM, without introducing LVs in the model (see trade-off
between LV complexity and partial structure complexity in Section 2.6). To tackle this
issue, they propose instead to assess the so-called improvement ratio during the grow step:
IRBIC (T 0 , T |DX ) =

BIC(T 0 , DX )  BIC(T, DX )
,
dim(T 0 )  dim(T )

(14)

that is the difference of the BIC scores between candidate model T 0 and current model T
divided by the difference of their respective dimensions.
3.2.2 Methods Based on Variable Clustering
The major drawback of search-based methods is that the evaluation of maximum likelihood
in presence of LVs, as well as the large space to explore through local search, still entails
computational burden. Approaches relying on variable clustering represent efficient and
much faster alternatives. All of them rely on two key points: grouping variables to identify
LVs and constructing model through a bottom-up strategy. Three main categories have
been developed, depending on the structures learned: binary trees, non-binary trees and
8. Note that node relocation is used locally after each NI to increase the number of its children.

169

fiMourad, Sinoquet, Zhang, Liu, & Leray

Algorithm 2 Advanced greedy search for LTM learning (AGS, adapted from EAST, Chen
et al., 2012)
INPUT:
X, a set of n observed variables {X1 , ..., Xn }.
OUTPUT:
T and , respectively the tree structure and the parameters of the LTM constructed.
1:
2:
3:
4:
5:
6:

(T 0 ,  0 )  latent class model(X) /* LCM learning using EM */
loop for i = 0, 1,... until convergence
0
0
(T i ,  i )  local search(N I  SI, T i ,  i ) /* grow */
0
0
00
00
(T i ,  i )  local search(N R, T i ,  i ) /* restructure */
00
00
(T i+1 ,  i+1 )  local search(N D  SD, T i ,  i ) /* thin */
end loop

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

/* description of the function local search(operators, T, ) */
(T 0 ,  0 )  (T, )
loop for j = 0, 1,... until convergence
S
T = {T1 , ..., T` }  neighborhood(operators, T j ) T j
 = {1 , ..., ` }  
local EM (T,  j ) /* except for T j */
BIC(T, )/dim(T, ) if operators = N I  SI
T j+1  arg maxT
BIC(T, )
otherwise
 j+1  EM (T j+1 )
if T j+1  neighborhood(N I, T j )
(T j+1 ,  j+1 )  local search(N R, T j+1 ,  j+1 )
end if
end loop

forests.
Binary Trees
Binary tree learning represents the most simple situation. For instance, one can simply compute an agglomerative hierarchical clustering (Xu & Wunsch, 2005) to learn the
LTM structure. This algorithm is called agglomerative hierarchical cluster-based learning (AHCB). For the purpose of clustering, pairwise mutual information (MI) represents
a well-suited similarity measure between variables (Cover & Thomas, 1991; Kraskov &
Grassberger, 2009). The MI between two variables Vi and Vj can be defined as follows:
I(Vi ; Vj ) =

X X

p(vi , vj ) log

vi Vi vj Vj

p(vi , vj )
.
p(vi ) p(vj )

(15)

Single, complete or average linkage is used, depending on the cluster compactness required.
The inferred hierarchy provides a binary tree (the partial structure) where leaf nodes are
170

fiLatent Tree Models

Figure 5: Illustration of the LCM-based LTM learning procedure for a set of 4 variables
{X1 , X2 , X3 , X4 }.

OVs and internal nodes are LVs (Wang et al., 2008; Harmeling & Williams, 2011). Then,
LV cardinalities and parameters are learned (for details, see Section 3.2.4 and Section 3.1,
respectively).
In AHCB, each cluster of the hierarchy represents an LV. The MI between an LV and
the other variables (observed or latent) is approximated through a linkage criterion. Instead
of this approximation, another solution consists in directly computing the MI between variables, whatever their status, i.e. observed or latent (Hwang, Kim, & Zhang, 2006; Harmeling & Williams, 2011). To compute MI, values of LVs are imputed through LCM-based
learning. Algorithm 3 presents the LCM-based LTM learning (LCMB-LTM) algorithm9
implementing this solution. First, the working node set W is initialized with the set of OVs
X = {X1 , ..., Xn } (line 1). An empty graph is created on W (line 2). Then the pair of
variables showing the highest MI, {Wi , Wj }, is selected (line 5). An LCM ({Wi , Wj }, H) is
learned (line 6), allowing to locally estimate the LV cardinality (through greedy search) and
parameters, and then to impute values for H (line 7). Once values of H are known, it can
be used as an observed variable (line 8). A new step of clustering, followed by LCM learning
and LV value imputation, can then be performed. Iterating this step into a loop allows to
construct an LTM through a bottom-up recursive procedure. At each step of the procedure,
the parameters lcm of the current LCM are used to update the current parameters  0 of
the LTM (line 10). If only two nodes are remaining, the two nodes are connected10 (line
12), the corresponding parameters are learned using maximum likelihood (line 13) and the
loop is broken. After constructing the LTM, a final step globally learns LTM parameters
using  0 as a starting point (line 17). LCMB-LTM yields slightly better BIC results than
AHCB for large datasets (Harmeling & Williams, 2011). LCMB-LTM is illustrated for a
set of 4 variables {X1 , X2 , X3 , X4 } in Figure 5.
Harmeling and Williams (2011) justified selecting the pair of variables showing the
highest MI at each step of LCMB-LTM. Let us consider a working node set of (observed or
imputed latent) variables W = {W1 , ..., W` }. At each step of LCMB-LTM, the unknown
9. This algorithm is called LCMB-LTM to distinguish it from LCMB-EM for parameter learning (Algorithm
1). Both algorithms are similar and rely on LCM-based learning.
10. It prevents the introduction of a redundant LV, see Section 2.5, second paragraph.

171

fiMourad, Sinoquet, Zhang, Liu, & Leray

Algorithm 3 LCM-based LTM learning (LCMB-LTM, adapted from BIN-T, Harmeling
and Williams, 2011)
INPUT:
X, a set of n observed variables {X1 , ..., Xn }.
OUTPUT:
T (V, E) and , respectively the tree structure and the parameters of the LTM constructed.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

W  X /* Initialization of the working set of variables */
T (V, E)  empty tree(W) /* tree on W with no edges */
0  
loop
{Wi , Wj }  pair with highest M I(W)
lcm  latent class model({Wi , Wj })
H  impute LV data(lcm)
W  W\{Wi , Wj }  H /* remove children and add imputed parent */
E  E  edges(lcm); V  V  H
 0   0  children parameters(lcm)
if |W| = 2 then
E  E  edge between the two remaining nodes(W)
 0   0  learn remaining parameters(W) /* max likelihood estimation */
break
end if
end loop
  EM ( 0 ) /* global EM using  0 as a starting point */

JPD P (W) is approximated by a JPD Q(W):
Q(W) = P (Wi , Wj ) Wk W\{Wi ,Wj } P (Wk )
=

P (Wi , Wj )
Wk W P (Wk ),
P (Wi ) P (Wj )

(16)

where only Wi and Wj are dependent. A proper measure to assess the divergence between
P (W) and Q(W) is the Kullback-Leibler (KL) divergence, which is easy to calculate in this
situation:
X
X
KL(P ||Q) =
P (W) log P (W) 
P (W) log Q(W)
W

= I(Wi ; Wj ) +

W

X
W

P (W) log

P (W)
,
Wk W P (Wk )

(17)

where I(Wi ; Wj ) is the mutual information between Wi and Wj . As the last term is constant, the maximization of the KL between P and Q simply consists in selecting the pair
of variables with the highest MI before introducing an LV into the model under construction.

172

fiLatent Tree Models

Non-binary Trees
Although modeling with binary trees performs well in practice (Harmeling & Williams,
2011), it would be worth alleviating the binarity restriction. Indeed it might provide a better
model faithfulness and interpretation because less LVs would be required. There are several
ways to learn non-binary trees without necessitating too much additional computational
cost. For instance, Wang et al. (2008) first learn a binary tree. Then, they check each pair
of neighbor LVs in the tree. If the information is redundant between the two LVs (i.e. the
model is not parsimonious), then the LV node which is the child of the other LV node is
removed and the remaining node is connected to every child of the removed node. Although
the approach of Wang et al. is rigorous, it can lead in practice to find trees which are very
close to binary trees.
Another solution consists in identifying cliques of pairwise dependent variables to detect
the presence of LVs (Martin & Vanlehn, 1995; Mourad, Sinoquet, & Leray, 2011). For this
purpose, Mourad et al. propose to alternate two main steps: (i) at each agglomerative step,
a clique partitioning method is used to identify disjoint cliques of variables; (ii) each such
clique, containing at least two nodes, is connected to an LV through an LCM. For each
LCM, parameters are learned using EM and LV data are imputed through probabilistic inference. In Algorithm 3 (line 5), it is possible to replace the selection of the pair of variables
having the highest MI by a clique partitioning step where each clique leads to construct an
LCM and to impute corresponding LV values.
Flat Trees
All the algorithms discussed so far in this section are based on the idea of hierarchical
variable clustering. The Bridged Island (BI) algorithm by Liu et al. (2012) takes a slightly
different approach. It first partitions the set of all observed variables into subsets that are
called sibling clusters. Then it creates an LCM for each sibling cluster by introducing a
latent variable and optimizing its cardinality as well as the model parameters. After that,
it imputes the values of the latent variables and links the latent variables up to form a
tree structure using Chow-Lius algorithm. The EM algorithm is run once at the end to
optimize the parameters of the whole model. To highlight the difference between BI and
the other variable clustering algorithms, we call the models it produces flat LTMs. Sibling
cluster determination is the key step in BI. BI determines the sibling clusters one by one.
To determine the first sibling cluster, it starts with the pair of variables with the maximum
empirical MI. The cluster is expanded by adding other variables one after another. At each
step, the variable that is the most dependent with the variables already in the cluster is
added to the cluster. After that, a unidimensionality test (UD test) determines whether the
dependences among all the variables in the cluster can be properly modeled with one latent
variable. If the test fails, the expansion process is terminated and the first sibling cluster
is determined. Thereafter, the same process repeats on the remaining observed variables
until they are all grouped into sibling clusters.

173

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 6: Latent forest models. See Figure 2 for node color code.

Forests
When the number of variables to analyze is very large (e.g. 1000 variables), it might
be more reasonable to learn a forest instead of a tree because many variables might not
be significantly dependent of each other (see Figure 6). We call this model latent forest
model (LFM). It has many advantages over LTM, such as reducing the complexity of
probabilistic inference (which depends on the number of edges). To learn LFM, there exists
multiple approaches. For instance, in AHCB, one can use a cluster validation criterion to
decide where to cut the hierarchy. Regarding LCMB-LTM (Algorithm 3), there are two
options. On the one hand, Harmeling and Williams (2011) check the optimal cardinality
of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this
means that the LV is not useful to the model under construction and the algorithm stops.
On the other hand, after partitioning variables in cliques (which replaces the step in line
5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are
discovered.
To build an LFM, one can also first construct an LTM and then use the independence
testing method of Tan et al. (2011) for pruning non-significant edges. This method provides
guarantees to satisfy structural and risk consistencies. Similar works for non-parametric
analysis have also been developed by Liu et al. (2011). It is worth mentioning that, to
ensure model parsimony, the pruning of non-significant edges in an LTM should be followed
by the removal of latent nodes which are no longer connected to a minimum of three nodes.
3.2.3 Distance-based Methods
This class of methods has been originally developed for phylogenetics (Felsenstein, 2003). A
phylogenetic tree is a binary LTM showing the evolutionary relations among a set of taxa.
Compared to the other LTM learning methods, distance-based ones provide strong guarantees for the inference of the optimal model. In this section, we first define distances and then
present learning algorithms: neighbor joining (phylogenetic tree inference), distance-based
174

fiLatent Tree Models

Figure 7: Illustration for ascertaining child-parent (a and b) and sibling relations (c). A
dotted edge between two nodes means that the two nodes are linked by a path of
unknown length. In this figure, some nodes are colored in white as they can be
either observed or latent.

method dedicated to general LTM learning and recent spectral methods.
Distances between Variables
Distances are restricted to LTMs whose all variables share the same state space X ,
e.g. binary variables (Lake, 1994). Distances are functions of pairwise distributions. For a
discrete tree model G(V, E) (e.g. an LTM), the distance between two variables Vi and Vj
is:
| det(Jij )|
dij =  log q
,
(18)
det(Mi ) det(Mj )
ij
with Jij the joint probability matrix between Vi and Vj (i.e. Jab
= p(Vi = a, Vj = b), a, b 
i
i = p(V = a)).
X ), and M the diagonal marginal probability matrix of Vi (i.e. Maa
i
For a special case of discrete tree models, called symmetric discrete tree models (Choi
et al., 2011), distance has a simpler form. Symmetric discrete tree models are characterized
by the fact that every variable has a uniform marginal distribution and that any pair of
variables Vi and Vj connected by an edge in E verifies the following property:

1  (K  1) ij if vi = vj
p(vi |vj ) =
ij
otherwise,

with K the cardinality common to Vi and Vj , and ij  (0, 1/K), known as the crossover
probability. For symmetric discrete tree models, the distance between two variables Vi and
Vj is then:
dij = (K  1) log (1  Kij ).
(19)
Note that there is a one-to-one correspondence between distances and model parameters
for symmetric discrete tree models (for more details, see Choi et al., 2011).
175

fiMourad, Sinoquet, Zhang, Liu, & Leray

The aforementioned distances are additive tree metrics (Erdos, Szekely, Steel, & Warnow,
1999):
X
dk` =
dij , k, `  V.
(20)
(Vi ,Vj )P ath((k,`);E)

Choi et al. (2011) showed that additive tree distances allow to ascertain child-parent
and sibling relationships between variables in a parsimonious LTM. Let us consider any
three variables Vi , Vj , Vk  V. Choi et al. define ijk as the difference between distances
dik and djk (ijk = dik  djk ). For any distance dij between Vi and Vj , the following two
properties on ijk hold:
 ijk = dij , Vk  V\{Vi , Vj }, if and only if Vi is a leaf node and Vj is its parent node;
 ijk = dij , Vk  V\{Vi , Vj }, if and only if Vj is a leaf node and Vi is its parent
node;
 dij < ijk = ijk0 < dij , Vk , Vk0  V\{Vi , Vj }, if and only if Vi and Vj are leaf nodes
and they share the same parent node, i.e. they belong to the same sibling group.
Neighbor Joining
The principle of neighbor joining (NJ) is quite simple (Saitou & Nei, 1987; Gascuel &
Steel, 2006). NJ starts with a star-shaped tree. Then it iteratively selects two taxa i and
j, and it creates a new taxum u to connect them. The selection of a pair seeks to optimize
the following Q criterion:
Q(i, j) = (n  2) dij 

n
X
k=1

dik 

n
X

djk ,

(21)

k=1

where n is the number of taxa and dij is the additive tree distance between i and j. The
distance between i and the new taxum u is estimated as follows:
!
n
n
X
X
1
1
diu = dij +
dik 
djk ,
(22)
2
2(n  2)
k=1

k=1

and dju is calculated by symmetry. The distances between the new taxum u and the other
taxa in the tree are computed as:
duk =

1
1
(dik  diu ) + (djk  dju ) .
2
2

(23)

The success of distance methods such as NJ comes from the fact that they have been
proved very efficient in terms of sample complexity. Under the Cavender-Farris model
of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to
guarantee that maxi,j |dij  dij | <  with a probability at least  if:



2
2 ln 2n

exp max dij
,
(24)
N
i,j
(1  exp())2
176

fiLatent Tree Models

with N the number of mutation sites (i.e. the number of observations) and n the number
of taxa (i.e. the number of OVs). Erdos et al. (1999) then demonstrated that under any
evolutionary model and for any reconstruction method, N grows at least as fast as log n,
and for any model assuming i.i.d. observations, it grows at least as n log n. Erdos et al.
also proposed a new algorithm, called Dyadic Closure Method, with a sample complexity
of a power of log n, when the mutation probabilities lie in a fixed interval. Daskalakis et
al. (2009) proved the Steels conjecture (Steel, 2001) which
states that if the mutation


probabilities on all edges of the tree are less than p = ( 2  1)/23/2 and are discretized,
then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log2 n
suffices when discretization is not assumed.
In phylogenetics, the scientist is often faced with a set of different trees11 and the
construction of a consensus tree is thus required. The computational complexity of this
construction has been studied and a polynomial algorithm has been proposed by Steel et
al. (1992).
Learning Dedicated to General LTM
In this subsection, we present the latest developments of Choi et al. (2011) for general
LTM learning, i.e. learning not restricted to phylogenetic trees. It is restricted to the
analysis of data whose all variables share the same state space, for instance binary data.
Assuming data generated by a parsimonious LTM, the additive tree metric property allows
to exactly recover child-parent and sibling relations from distances (see Subsection Distances
between Variables, last paragraph). Another advantage is that OVs are not necessarily
constrained to be leaf nodes (this will be seen in the next paragraph).
The distance-based general LTM learning (DBG) is implemented in Algorithm 4, detailed as follows. First, the working node set W is initialized with the set of observed
variables X = {X1 , ..., Xn } (line 1). Distances are computed for any three variables in W
(line 2). An empty tree is created on W (line 3). Then the following steps are successively
iterated (lines 4 to 16):
 A procedure (based on properties described in Subsection Distances between Variables, last paragraph) allows to identify nodes that have a parent-child relation and
nodes that are siblings (line 5). This procedure generates three different sets of node
groups: a set of parent-child groups, PC = {P C1 , ..., P Cp }, a set of sibling groups,
S = {S1 , ..., Sq } and a set of remaining single nodes, R = {R1 , ..., Rr };
 The content of the working node set W is replaced with parent nodes belonging to
parents(PC) and remaining single nodes belonging to R (line 6);
 For each group of sibling nodes  S, a new parent LV H is created and added to W
(lines 7 and 8);
 To update the distances of the working node set (line 9), the distances between the
new LVs and the remaining variables in W are calculated (the calculation is easily
derived from previously computed distances; for more details, see Choi et al., 2011);
11. For instance when different genes are used to infer trees.

177

fiMourad, Sinoquet, Zhang, Liu, & Leray

Algorithm 4 Distance-based general LTM learning (DBG, adapted from RG, Choi et al.,
2011)
INPUT:
X, a set of n observed variables {X1 , ..., Xn }.
OUTPUT:
T and , respectively the tree structure and the parameters of the LTM constructed.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

W  X /* Initialization of the working set of variables */
D  inf o dist computation(W) /* for any three variables in W */
T (V, E)  empty tree(W) /* tree on W with no edges */
loop
(PC,S,R)  test relations(D,W) /* see paragraph 3 of the section */
W  (parents(PC), R) /* parents and singles */
LCMT  LCM trees(S) /* an LCM tree for each sibling group */
W  W  latent variables(LCMT)
D  D  inf o dist computation(W) /* only for LVs  W */
E  E  edges(PC)  edges(LCMT)
V  V  latent variables(LCMT)
if |W| = 2 then
E  E  edge between the two remaining nodes(W)
break
else if |W| = 1 then break end if
end loop
  EM (T ) /* See Section 3.1 */
 If the working node set W contains strictly more than two nodes, then a new step
is started. Otherwise, there are two possible situations: if the working node set W
contains two nodes, then these nodes are connected12 and the procedure is completed
(lines 12 to 14); if the working node set W only contains one node, then the iteration
stops (line 15).

After learning the partial structure, model parameters (line 17) are learned (see Section
3.1).
In practice, Choi et al. (2011) restricted the learning to symmetric discrete distributions13 (see definition in Subsection Distances between Variables). This restriction presents
a major advantage: it allows to derive model parameters through the use of distances previously learned by structure recovery. In other words, after learning the structure, there is
no need to recover parameters through EM. This is due to the one-to-one correspondence
between distance and model parameters (for more details, see Choi et al., 2011).
12. It prevents the introduction of a redundant LV, see Section 2.5, second paragraph.
13. Nevertheless, the algorithm can be applied to non-symmetric discrete distributions. The only requirement
is that all variables share the same state space.

178

fiLatent Tree Models

To diminish the computational complexity of DBG, Choi et al. (2011) propose to first
learn a minimum spanning tree (MST) based on distances between OVs. Then, in the tree,
they identify the set of internal nodes. For each internal node Vi and its neighbor nodes
nbd(Vi ), they apply DBG which outputs a latent tree. In the global model, each subtree
{Vi , nbd(Vi )} is then replaced by the corresponding latent tree. This strategy allows to
reduce the computational complexity of latent tree construction because MST is fast to
compute and DBG is only applied to a restricted number of variables. In term of sample
complexity, DBG and its derived algorithms only require log n observations for recovering
the model with high probability. This sample complexity is equal to the one of the Chow-Liu
algorithm (Tan et al., 2011).
Another version of the DBG algorithm was developed when it is not assumed that observations have been generated by a genuine LTM. To prevent the incorporation of irrelevant
LVs, after applying DBG on subtrees {Vi , nbd(Vi )}, only are integrated in the model those
latent trees that increase the BIC score.
Spectral Methods
Recent works extended previous distance methods following a spectral approach. On
the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed
and latent nodes are random vectors rather than scalars. Their approach can deal with
general linear models containing both categorical and continuous variables. Another important improvement of their method is that sample complexity bound is given in terms
of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011). The proposed extension
consists in replacing the step in line 5 of Algorithm 4 by a quartet test14 relying on spectral techniques (more specifically, canonical correlation analysis; Hair, Black, Babin, and
Anderson, 2009). Given four observed variables {X1 , X2 , X3 , X4 }, the spectral quartet test
distinguishes between the four possible tree topologies (see Figure 8). The correct topology
is {{Xi , Xj }, {Xi0 , Xj 0 }} if and only if:
|E[Xi Xj> ]| |E[Xi0 Xj>0 ]| > |E[Xi0 Xj> ]| |E[Xi Xj>0 ]| ,

(25)

where |M | := k`=1 ` (M ) is the product of the k largest singular values of matrix M and
E[M ] is the expectation of M (estimated using the covariance matrix).
On the other hand, Song et al. (2011) proposed a non-parametric learning based on
kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962). KDE is particularly
relevant for model learning, in the case of non-Gaussian continuous variables showing multimodality and skewness. Given a set of N i.i.d. observed data Dx = {x1 , ..., xN }, the joint
distribution is modeled as :
N n
1 XY
k(xj , xij ),
P (x) =
N

(26)

i=1 j=1

with x = {x1 , ..., xn } an observation, k(x, x0 ) the Gaussian radial basis function and N the
number of observations. Kernels k(x, x0 ) are represented as inner products h(x), (x0 )iF
14. Quartet tests are widely used for phylogenetic tree inference. The first authors to adapt them to LTM
learning were Chen and Zhang (2006).

179

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 8: The four possible tree topologies for the quartet test. See Figure 2 for node color
code.

through a feature map  : R  F. As products of kernels are also kernels, the product
nj=1 k(xj , x0j ) is written as a single inner product hnj=1 (xj ), nj=1 (x0j )iF n , where  is
the tensor product. Let CX := EX [nj=1 (Xj )] be the Hilbert space embedding of the KDE
distribution P (X). The expectation of P (X) can be formulated as hCX , nj=1 (xj )iF n . By
exploiting a given latent tree structure, the key is that Hilbert space embedding allows to
decompose CX into simpler tensors. Similarly to the work of Parikh et al. (2011), message
passing and parameter estimation are reformulated through tensor notation (see the work
of Song et al., 2011, for more details). To learn the structure, a non-parametric additive
tree metric is used. For two variables Vi and Vj , the distance is:
1
1
1
>
>
dij =  log |Cij Cij
|? + log |Cii Cii> |? + log |Cjj Cjj
|? .
2
4
4

(27)

Then, given the distances between variables, NJ or DBG can be used to learn the structure.
3.2.4 Determination of Latent Variable Cardinalities
During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion,
such as BIC (Raftery, 1986). For each cardinality value, parameters are required to calculate
the likelihood appearing in the optimization criterion. For this purpose, random restarts
of EM are generally used to learn parameters with a low probability of getting trapped in
local maxima. The drawback is that this method cannot be applied to LTM learning, because EM becomes time-consuming when there are several LVs. A better solution consists
in using a greedy search approach, starting with a preset value of LV cardinality (generally
equal to 2) and incrementing it to meet the optimal criterion. However, this solution still
remains computationally demanding (Zhang, 2004).
To tackle the issue of computational burden, several strategies have been proposed. For
instance, one can simply set a small value for the LV cardinalities. Following this idea,
Hwang and collaborators (2006) constrain LVs to binary variables. Because they worked on
binary trees whose OVs are also binary, this restriction is not severe in practice. Nevertheless, in the case of non-binary OVs and/or non-binary trees, this very fast method presents
several drawbacks: on the one hand, a too small cardinality can lead to an important loss
180

fiLatent Tree Models

of information; on the other hand, a too large cardinality can entail model overfitting and
unnecessary computational burden.
More rigorous, Wang et al.s approach (2008) relies on regularity (see Section 2.5). Knowing the cardinality of its neighbor variables Zi , the cardinality of an LV H is determined
as follows:
ki=1 |Zi |
.
(28)
|H| =
maxki=1 |Zi |
This very fast approach is efficient for LVs owning a few number of neighbors. Thus this
is only practicable for binary trees or trees whose LV degrees are small (close to 3). In the
context of large scale data analysis (several thousands of variables), Mourad et al. (2011)
proposed to estimate the cardinality of an LV given the number of its children. The rationale
underlying this approach is the following: the more child nodes an LV has, the larger the
number of combinations is for the values of the child variables. Therefore, the cardinality
of a latent variable should depend on the number of its child nodes. Nonetheless, to keep
the model complexity within reasonable limits, a maximum cardinality is fixed.
Two additional methods have been proposed to offer a better trade-off between accuracy
and computational cost. The first one uses a search-based agglomerative state-clustering
procedure (Elidan & Friedman, 2001). The idea relies on the Markov blanket of an LV.
In LTMs, the Markov blanket of an LV H, noted MBH , is composed of its parent and its
children. The Markov blanket represents the set of variables that directly interact with H.
Elidan and Friedmans method sets the initial cardinality of H based on the empirical joint
distribution of MBH , noted P (MBH ). H is initialized to have a state for each configuration
found in P (MBH ). Then the cardinality is repeatedly decreased through successive merging
operations: states hi and hj , whose merging entails the best optimization of a given score,
are merged. After repeating these operations till H has only one state, the cardinality value
leading to the best score is selected. The second method relies on local and fast parameter
estimation through LCM learning (Harmeling & Williams, 2011). As presented in the first
paragraph of this section, a greedy search approach can be used. It starts with a preset
value and increments it to meet an optimal criterion. This greedy search becomes efficient
because, for each cardinality value to test, parameters are quickly learned in constant time.
3.2.5 Choosing a Root
The LTM root cannot be learned from data. However there is sometimes a need to determine
the root. For instance, LCM-based parameter learning (see Algorithm 1) can easily be
performed when a root is chosen.
The root can be determined from a priori knowledge on data. For instance, we can
consider that the latent structure of LTM represents a hierarchy of concepts (i.e. a taxonomy
in ontology). Thus, the LV root corresponds to the highest abstract level, whereas an LV
node only having OVs as children is interpreted as the lowest abstract level. Actually,
variable clustering-based algorithms implicitly implement this a priori knowledge.
3.2.6 Time Complexity and Scalability
The time complexity of generic LTM learning algorithms is summarized in Table 1. In
the table, we compare algorithms, approaches, models and time complexities. We also
181

fiMourad, Sinoquet, Zhang, Liu, & Leray

Algorithm
CL

Approach
-

Model
Tree

Complexity
O(n2 N )

Instantiation
 (Chow & Liu, 1968)

NGS

Score

Tree

O(sn5 N )

DHC (Zhang, 2004)

AGS, Alg. 2

Score

Tree

O(sn2 N )

AHCB

Variable
clustering

Forest

O(n2 N )

LCMB-LTM, Alg. 3

Variable
clustering

Forest

O(n2 N )

HSHC (Zhang & Kocka, 2004b)
EAST (Chen et al., 2012)
LTAB (Wang et al., 2008)
BIN-A (Harmeling & Williams, 2011)
BIN-G (Harmeling & Williams, 2011)
CFHLC (Mourad et al., 2011)
BI (Liu et al., 2012)

NJ

Information
distance

Tree

O(n3 N )

DBG, Alg. 4

Information
distance

O(n3 N
Tree

O(n2 N

+

n4 )

+

n4 )

NINJA (Saitou & Nei, 1987; Wheeler, 2009)
RG (Choi et al., 2011)
CLRG (Choi et al., 2011)
regCLRG (Choi et al., 2011)

Table 1: Computational time complexities of generic algorithms dedicated to latent tree
model (LTM) learning. The number of observed variables, the number of observations and the number of steps (in search-based algorithms) are denoted n, N
and s, respectively. CL: Chow-Lius algorithm; NGS: naive greedy search (Section
3.2.1); AGS: advanced greedy search (Section 3.2.1); LCMB-LTM: latent class
model-based LTM learning (Section 3.2.2); NJ: neighbor joining (Section 3.2.3);
DBG: distance-based general LTM learning (Section 3.2.3).

give examples of instantiations for generic algorithms. Online resources are summarized in
Appendix A. In order to simplify the comparison of time complexities, we only consider
the number n of variables (input data), the number N of observations and the number s
of steps (for search-based algorithms). The LTM learning algorithms are compared with
the Chow-Liu algorithm for learning a tree without LVs. Details about the complexity
calculation of LTM learning algorithms are provided in Supplemental material B.2.
When the tree does not contain any LV, learning the model can be done efficiently
in O(n2 N ) using the Prims algorithm (1957). The situation is more complicated when
the tree contains LVs. The complexity of finding the regular LTM with the lowest score
2
is O(23n ). Search-based methods implement heuristics to reduce this large complexity.
Their overall complexity can be decomposed into a product of three main terms: number of
steps, structure learning complexity and parameter learning complexity. At the opposite, in
variable clustering- and distance-based methods, the overall complexity can be decomposed
into a sum. Nevertheless, the development of new operators for greedy search and the
application of local EM have led to significant improvements (from O(sn5 N ) to O(sn2 N )).
Variable clustering-based methods are computationally more efficient for multiple reasons.
They rely on pairwise dependence computation to identify LVs and their connections, and
on LCM-based learning to determine LV cardinality. Regarding distance-based methods,
NJ provides a reasonable complexity of O(n3 N ), whereas DBG presents a high complexity
of O(n3 N + n4 ). However, this last complexity corresponds to the worst case, when the tree
to learn is a hidden Markov model. Besides, Choi et al. provide a modified DBG which
reduces the complexity to O(n2 N + n4 ).
182

fiLatent Tree Models

Algorithm
Number of variables
Number of observations
Type of data

CL1
LCM
DHC
SHC
HSHC
EAST
LTAB
BIN-A
BIN-G
CFHLC
BI
NJ
RG2
CLRG2
regCLRG2
Algorithm
Number of variables
Number of observations
Type of data

CL1
LCM
DHC
SHC
HSHC
EAST
LTAB
BIN-A
BIN-G
CFHLC
BI
NJ
RG2
CLRG2
regCLRG2

BinTree

BinForest
5
500
simu
0.01
5.83
123.83
43.86
15.14
20.17
5.97
2.68
2.61
1.3
37.66
non-bin
non-bin
non-bin
non-bin
Alarm
37
1000
simu
0.15
34.32
time
time
3729.78
158.64
4366.93
277.01
6388.69
83.31
574.57
2.03
86.12
1.28
99.93
5.4
21
223.19
319.62
non-bin
non-bin
non-bin
non-bin
non-bin
non-bin
non-bin
non-bin

4
500
simu
0.01
1.43
18.79
27.07
13.85
12.56
2.8
3.00
3.06
1
19.38
non-bin
non-bin
non-bin
non-bin
Forest
20
500
simu
0.04
0.97
time

Asia

Hannover
8
5
100
3589
simu
real
0.01
0.00
1.02
58.58
16.23
9.86
4.55
5.39
1.5
1.9
3.79
5.02
0.97
1
0.22
16.70
0.23
17.87
0.6
18.6
7.87
7.65
0.15
4.75
0.16
3.49
0.06
3.63
0.04
6.54
Coil-42 NewsGroup
42
100
4000
8121
real
real
0.45
2.17
678.19
1467.10
time
time
time
time
time
time
time
751683
time
2197.69
387.21
1152.70
436.41
1302.10
560.9
1291.4
1193.25
6311.99
non-bin
1325.38
non-bin
274.37
non-bin
927.22
non-bin
345.09

Car
7
869
real
0.01
2.54
1609.72
150.23
18.79
63.55
35.36
3.72
3.72
2.7
69.26
non-bin
non-bin
non-bin
non-bin
HapGen
1000
1000
simu
121.36
bug
time
time
time
time
time
3573.20
7671.20
787.2
18977.02
non-bin
non-bin
non-bin
non-bin

Tree
19
500
simu
0.04
1.61
time
4258.06
87.04
309.47
86.52
0.63
0.29
6.6
183.34
non-bin
non-bin
non-bin
non-bin
HapMap
10000
116
real
memory
memory
time
time
time
time
time
memory
memory
2852.6
time
memory
memory
memory
memory

Long running time

Short running time

Table 2: Comparison of running times between algorithms from the literature on small,
large and very large simulated and real datasets. 1: CL learns a tree without LVs;
2: RG, CLRG and regCLRG learn a tree whose internal nodes can be observed
or latent. time: very long running time; memory: out-of-memory; non-binary:
impossible to process non-binary data. 3: results from the work of Chen (2008).

183

fiMourad, Sinoquet, Zhang, Liu, & Leray

Algorithm
Number of variables
Number of observations
Type of data

BinTree

BinForest

4
500
simu

Asia

5
500
simu

Hannover

8
100
simu

5
3589
real

Car

Tree
7
869
real

19
500
simu

CL1

-3222.80

-3350.20

-281.430

-7859.60

-7161.20

-106280

LCM

-3361.830

-3646.98

-346.2310

-7754.63

-7127.818

-100100

DHC

-2825.110

-3038.660

-269.326

-7710.961

-7049.9822

time

SHC

-2825.110

-3056.770

-268.044

-7709.710

-7056.184

-10095.9217

HSHC

-2825.10

-3056.770

-270.590

-7709.710

-7057.493

-10087.548

EAST

-2825.110

-3056.760

-283.826

-7709.690

-7051.977

-10092.455

LTAB

-3332.948

-379147

-727.323

-7876.480

-8084.8857 -13410.27390

BIN-A

-29910

-31460

-296.010

-77560

-7137.741

-100100

BIN-G

-29910

-31460

-296.010

-77560

-7133.843

-100100

CFHLC

-3682.860

-3302.560

-280.862

-8032.390

-7200.7626

-10070.4310

BI

-28500

-30740

-2830

-77111

-706317

-100732

NJ

non-bin

non-bin

-288.391

-77143

non-bin

non-bin

RG2

non-bin

non-bin

-287.132

-7711.44

non-bin

non-bin

CLRG2

non-bin

non-bin

-271.810

-7710.12

non-bin

non-bin

regCLRG2

non-bin

non-bin

-266.140

-7742.423

non-bin

non-bin

Algorithm

Forest

Alarm

Coil-42

NewsGroup

HapGen

HapMap

20

37

42

100

1000

10000

Number of observations

500

1000

4000

8121

1000

116

Type of data

simu

simu

real

real

simu

real

CL1

-113300

-112810

-364440

-1214000

-1912500

memory

Number of variables

LCM

-107090

-218593489

-49581491

-124390768

bug

memory

DHC

time

time

time

time

time

time

SHC

-10777.811

time

time

time

time

time

HSHC

-10775.250

-11322.8384

time

time

time

time

-10777.085 -12315.66589

-35982.43

time

time

time

-14207.87392 -17733.87239 -43655.3746

time

bug

time

EAST
LTAB
BIN-A

-107080

-17640551

-37380104

-11906033

-3010103962

memory

BIN-G

-107080

-17600589

-3740478

-120230231

-3017903006

memory

-17856.0418 -51878.73151 -129101.4488 -367875.31706

-373523876

CFHLC

-10762.38

BI

-107616

-12296125

-36682152

NJ

non-bin

non-bin

RG2

non-bin

CLRG2
regCLRG2

-117278134

-2678811347

time

non-bin

-11761039

non-bin

memory

non-bin

non-bin

-120274380

non-bin

memory

non-bin

non-bin

non-bin

-11758046

non-bin

memory

non-bin

non-bin

non-bin

-118938161

non-bin

memory

High BIC

Low BIC

Table 3: Comparison of BIC scores between algorithms from the literature on small, large
and very large simulated and real datasets. 1: CL learns a tree without LVs;
2: RG, CLRG and regCLRG learn a tree whose internal nodes can be observed
or latent. time: very long running time; memory: out-of-memory; non-binary:
impossible to process non-binary data. 3: results from the work of Chen (2008).

184

fiLatent Tree Models

Some of the algorithms proposed in the literature differ from the generic algorithms
presented. In Tables 2 and 3, we compare the algorithms from the literature on small, large
and very large simulated and real datasets15 . Moreover we provide results for standard
algorithms: CL model-based and LCM-based approaches. For each dataset, we learned
the model from training data and evaluated the BIC score on test data. We repeated the
experiments 10 times. The programs were allowed to run in maximum 6 hours. Datasets
are described in Supplemental material B.316 . We report the BIC score with its standard
deviation and the running time.
On small datasets (n  10 variables), search-based methods lead to the best BIC values,
except for the Asia dataset for which a distance-based method, regCLRG, is the best one.
This is not surprising since search-based methods evaluate a large number of models to find
the optimal one. Nevertheless, on large datasets (10  n  100), search-based methods
require long running times and thus cannot be used for some datasets such as the Coil-42
and NewsGroup ones. In this context, variable clustering-based and distance-based methods
are much more efficient while yielding accurate results. Regarding the very large dataset
context (n > 100), only variable clustering-based and distance-based methods can learn
LTMs17 . CFHLC18 is the only approach able to process the HapMap data containing 117
observations for 10k variables. For all datasets, we observe that using CL model and LCM
predominantly leads to lower BIC scores than when using LTM, except for large and very
large datasets.
3.2.7 Summary
LTM learning has been subject to many methodological developments. When structure
is known, EM is often preferred. Nevertheless, for large LTMs, EM leads to considerable
running times and to local maxima. To address this problem, LCM-based EM allows
to quickly learn parameters, while spectral methods help find the optimal solution when
LTM parameters are not required. When structure is unknown, search-based approaches
represent standard methods from Bayesian network learning. However they are only suitable
for learning small LTMs. To tackle this issue, variable clustering-based methods represent
efficient alternatives. These methods are based on the idea of grouping variables to identify
LVs in a bottom-up manner. Recently, phylogenetic algorithms have been adapted to
general LTM learning. Compared to the other methods, they guarantee to exactly recover
the generative LTM structure under some conditions.

15. For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011).
16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space
(binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process
binary data. Hence the algorithms have not been applied to some datasets. We recall that RG, CLRG
and regCLRG do not exactly learn an LTM but instead a tree whose all internal nodes are not compelled
to be latent.
17. Although it is not shown in Tables 2 and 3, NJ, RG, CLRG and regCLRG were able to process 1000
binary variables in our experiments.
18. In the work of Mourad et al. (2011), CFHLC implements a window-based approach to scale very large
datasets (n  100k variables). Here for a fair comparison, the window-based approach has not been
used.

185

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 9: Illustration of phylogenetic tree reconstruction.

4. Applications
In this section, we discuss and illustrate three types of applications of LTMs: latent structure
discovery, multidimensional clustering and probabilistic inference. At the end of the section,
we also briefly present other applications such as classification.
4.1 Latent Structure Discovery
Latent structure discovery aims at revealing: (i) latent information underlying data, i.e.
unobservable variables or abstract concepts which have a role to play in data analysis, and
(ii) latent relationships, i.e. relationships existing between observed and latent information,
and also between pieces of latent information themselves. For this purpose, LTM analysis
represents a powerful tool where latent information and latent relationships are modeled
by LVs and graph edges, respectively. Thanks to LTMs, latent structure discovery has
been applied to several fields: marketing (Zhang, Wang, & Chen, 2008), medicine (Zhang,
Nielsen, & Jensen, 2004; Zhang et al., 2008), genetics (Hwang et al., 2006; Mourad et al.,
2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Peer, & Pupko, 2002). Let
us take the example of phylogenetics which is the major application of LTMs in structure
discovery.
In phylogenetics, the purpose is to infer the tree representing the evolutionary connections between observed species. Let us consider human and its closest living relatives:
orangutan, gorilla, bonobo and chimpanzee. From their DNA sequences, it is possible to
reconstruct the phylogenetic tree. DNA sequences are sequences of letters A, T , G and C.
During the evolution of species, DNA sequences are modified by mutational processes. Each
186

fiLatent Tree Models

Figure 10: Latent tree model learned from the dataset on Danish beer consumption. Edge
widths are proportional to mutual information between nodes. For each latent
variable, the number of latent classes is indicated in brackets. See Figure 2 for
node color code. Lantern software.

species can then be characterized by its DNA sequence. One of the most popular algorithms
for phylogenetic tree reconstruction is NJ (described in Section 3.2.3, Neighbor Joining). It
starts by considering the tree as a star linking all species (see illustration in Figure 9). Then
the distances between all species are calculated based on the DNA sequences. Chimpanzee
and bonobo present the shortest distance and are thus regrouped under a new latent node.
Then distances are updated and the last previous step is reiterated until the construction
of the final phylogenetic tree. The tree first links chimpanzee and bonobo, then human,
gorilla and orangutan. The success of NJ comes from the fact that, compared to previous
hierarchical clustering methods such as UPGMA (Unweighted Pair Group Method with
Arithmetic Mean), it does not assume all species evolve at the same rate. The length of an
edge represents the time separating two species. Moreover, assuming that the distances are
correct, NJ outputs the correct tree.
4.2 Multidimensional Clustering
Cluster analysis, also called clustering, aims at assigning a set of observations to several
groups (called clusters) so that observations belonging to the same cluster are similar in
some sense (Xu & Wunsch, 2008). LTMs are particular tools which can produce multiple
clusterings: each LV represents a partition of data, which is most related to a specific subset
of variables. This application is called multidimensional clustering and has been mainly
explored by Chen et al. (2012).
Let us illustrate LTM-based multidimensional clustering using dataset from a survey
on Danish beer market consumption. For this purpose, we use the user-friendly software
Lantern. The dataset consists of 11 variables and 463 consumers. Each variable represents a
beer brand which is evaluated through the four possible answers to a survey questionnaire:
never seen the brand before (s0); seen before, but never tasted (s1); tasted, but do not
drink regularly (s2) and drink regularly (s3).
187

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 11: Information curves for latent variable H1. Lantern software.

The learned model is presented in Figure 10. It has a BIC score of 4851.99. The model
contains 3 LVs: H0 , H1 and H2 . These LVs have 2, 3 and 2 latent classes, respectively. Let
us start with H1 which is the simplest interpretable LV. Let X1 , X2 , ..., Xn be the OVs sorted
by decreasing values of pairwise MI between H1 and each OV Xi . Two different information
curves are analyzed in Figure 11: (i) the curve of pairwise mutual information I(H1 ; Xi )
between H1 and each OV Xi , and (ii) the curve of cumulative information I(H1 ; X1 , ..., Xi )
representing MI between H1 and the first i OVs X1 , ..., Xi . The curve of pairwise mutual
information shows that TuborgClas, followed by CarlSpec and Heineken, are the beers most
related to H1 . The curve of cumulative information presents complementary information.
The cumulative information curve increases monotonically with i and reaches the maximum
for n. The ratio I(H1 ; X1 , ..., Xi )/I(H1 ; X1 , ..., Xn ) is the information coverage of the first i
OVs. If this ratio is equal to 1, it means that H1 is conditionally independent of Xi+1 , ..., Xn
given the first i OVs. In practice only the first OVs whose information coverage is less than
95% are considered relevant. Using the cumulative information curve, we observe that H1
is only related to TuborgClas, CarlSpec and Heineken, which represent a group of beers
different from the others. TuborgClas and CarlSpec are frequent beers, being a bit darker
in color and more different in taste than the two main mass-market beers, GronTuborg
and Carlsberg. Although not Danish, Heineken is one of the largest brand in the world
that most Danes would have tasted sometimes during travels abroad. Results for other
LVs are discussed but not shown (interpretation remains the same as for H1 ). H0 is more
related to CeresTop, CeresRoyal, Pokal, Fuglsang, CarlsSpec and FaxeFad (i.e. minor local
beers), whereas H2 is more connected to GronTuborg and Carlsberg (i.e., the two main
mass-market beers).
188

fiLatent Tree Models

s0
s1
s2
s3

Class1, prior
Tub Carl
0.03
0
0.07 0.12
0.89 0.81
0.02 0.07

= 0.36
Hein
0.08
0.3
0.57
0.05

Class2, prior
Tub Carl
0.06 0.15
0.56 0.74
0.32 0.11
0.06
0

= 0.27
Hein
0.36
0.4
0.23
0.01

Class3, prior
Tub Carl
0
0
0
0.01
0.14 0.39
0.86 0.61

= 0.37
Hein
0.02
0.17
0.66
0.16

Table 4: Class conditional probability tables for latent variable H1. Tub: TuborgClas;
Carl: CarlsSpec; Hein: Heineken.

H2

Class1
Class2

Class1
0.55
0.45

H1
Class2
0.71
0.29

Class3
0.11
0.89

Table 5: Conditional probability distributions of latent variable H2 given H1 .
Class conditional probability distributions (CCPDs) of H1 are presented in Table 4.
Using this table, it is easy to interpret latent classes. For instance, the first class (Class1)
represents 36% of the consumers (prior = 0.36). For this class, all conditional probabilities
of s2 are higher than 0.5. This means that these consumers tasted the beers, but do
not drink them regularly. The second class (Class2) contains 27% of the consumers and
represents people who saw the beers before or only tasted them. The last class (Class3)
includes 37% of the consumers and represents people who just tasted the beers or drink
them regularly. Results for other LVs are discussed but not shown. The CCPDs relative to
H0 show a division into a group of consumers who just tasted the beers (Class1) and a more
complicated group of consumers who never saw the brands, just saw them or just tasted
them (Class2). Regarding H2 , the CCPDs report consumers who just tasted the beers
(Class1) and consumers who drink them regularly (Class2). Using the LTM, we can also
analyze relations between the different partitions. The conditional probability distribution
P (H2 |H1 ) is given in Table 5. We observe that consumer behaviors are similar for the two
groups of beers. For instance, consumers who just tasted or regularly drink the H1 group of
beers (Class3 of H1 ) generally also drink regularly the H2 group of beers (Class2 of H2 ).
In this example, we were able to find consumer profiles specific to beer brands. Multidimensional clustering thanks to LTMs helps discover multiple facets of data (i.e. LVs)
and partition data along each facet (i.e. latent class). Moreover, general relations between
multiple facets are highlighted through connections between LVs.
4.3 Probabilistic Inference
Probabilistic inference is the process of answering probabilistic queries of the form p(x|y),
for an event x given some knowledge y (Koller & Friedman, 2009), using the Bayes formula:
p(x|y) =

p(y|x)p(x)
.
p(y)
189

(29)

fiMourad, Sinoquet, Zhang, Liu, & Leray

10
10
10
10
10

2

N=1k
N=10k
N=100k

1

10
Time (hour)

10

0

1

10

10

2

3

1

2

4

10

8

2

0

2

4

1

4

16

64

4

16

64

4

16

64

C

10

10

10

0

0

10

1

1

2

10
LTM (1k)
LTM (10k)
LTM (100k)
LBP
CL (100k)
LCM (100k)

1

2

2

4

10

8

1

6

1

10

10

LTM (1k)
LTM (10k)
LTM (100k)
CTP
LBP
CL (100k)
LCM (100k)

0

10

4

10

2

10

0

10
1

10

2

1

2

4

10

8

1

Figure 12: Experiments on ALARM and BARLEY networks: a) running times for LTM
learning using the LTAB algorithm (Wang et al., 2008), b) approximation accuracy of probabilistic inference and c) running time for inference. Approximation
accuracy is measured by the Kullback-Leibler divergence between approximate
inferred distributions and exact inferred distributions obtained from clique tree
propagation on the original BN. N and C designate the sample size and the
maximal cardinality of latent variables, respectively. These results come from
the work of Wang et al. (2008).

Probabilistic inference is used in many circumstances, such as in credit card fraud detection
(Ezawa & Norton, 1996) or disease diagnostic (McKendrick, Gettinbya, Gua, Reidb, &
Revie, 2000).
190

fiLatent Tree Models

Probabilistic inference in a general BN is known to be an NP-hard task (Cooper, 1990).
To tackle this issue, one can approximate the original BN using a maximum weight spanning
tree learned relying on Chow and Lius algorithm (1968). The drawback is the risk of
inaccuracy in inference results. In this context, LTM provides an efficient and simple
solution, because: (i) thanks to its tree structure, the model allows linear computations with
respect to the number of OVs, and at the same time, (ii) it can represent complex relations
between OVs through multiple LVs. Because learning LTM before performing inference can
be time-consuming Wang et al. (2008) propose the following strategy: first, offline model
learning is performed, then answers to probabilistic queries are quickly computed online.
However, recent spectral methods (Parikh et al., 2011) considerably reduced model learning
phase, because they do not require to learn the model structure. This makes inference
through large LTMs possible (around several hundred OVs) and thus renders them even
more attractive.
Inferential complexity does not only depend on the number of OVs, but also on LV
cardinalities: the higher the cardinality, the higher the complexity. Hence Wang et al.
(2008) propose a tradeoff between inferential complexity and model approximation accuracy
by fixing a maximal cardinality C for LVs.
Wang et al. (2008) empirically demonstrated the high performance of LTM-based inference on 8 well-known Bayesian networks from the literature. The principle consists in
learning the LTM which will provide the best approximation of the original BN. For this
purpose, data are sampled from the original BN and then an LTM is learned from the data.
To illustrate inference, let us only consider two examples, the ALARM and the BARLEY
networks which show the lowest and highest inferential complexities among the aforementioned networks, respectively. The ALARM network contains 37 nodes, and is characterized
by an average indegree of 1.24 (max: 4) and an average cardinality of 2.84 (max: 4). The
BARLEY network contains 48 nodes; its average indegree is 1.75 (max: 4) and its average
cardinality is 8.77 (max: 67). Two parameters are central for the user: (i) N , the sample
size which entails long model learning running times but leads to better model accuracies,
and (ii) C, the maximal cardinality of LVs which entails long model learning and inference
running times but leads to higher model accuracies.
In Figure 12, the LTM-based method is compared to other standard inference approaches: the LCM-based approach, the Chow-Liu (CL) model-based approach and loopy
belief propagation (LBP) (Pearl, 1988). Exact inference through clique tree propagation
(CTP) (Lauritzen & Spiegelhalter, 1988) on the original BN is considered as the reference.
Figure 12a reports running times for LTM learning using the LTAB algorithm (Wang et al.,
2008). Running times almost linearly increase with N and C. Regarding inference accuracy (Figure 12b), the LTM-based method outperforms other methods when N and C are
high, e.g. N = 100k and C = 8 for the ALARM network. As regards inference running
times (Figure 12c), the benefits of using the LTM-based method are high for the ALARM
network, a high inferential complexity network. In these experiments, we note that CL is
also very interesting, and the choice between the latter and the LTM will depend on the
online inference time allowed. If time is very limited, CL would be preferred. In the other
case, the LTM would be chosen.
191

fiMourad, Sinoquet, Zhang, Liu, & Leray

4.4 Other Applications
Beside latent structure discovery, multidimensional clustering and probabilistic inference,
there are other interesting applications of LTMs.
A simple but efficient classifier is naive Bayes. This model assumes that OVs are independent conditional on the class variable. This assumption is often violated by data and
hence numerous adaptations have been developed to improve the classifier performance.
Naive Bayes has been generalized by introducing latent nodes as internal discrete nodes
(Zhang et al., 2004) or continuous nodes (Langseth & Nielsen, 2009), mediating the relation between leaves and the class variable. The model is identical to an LTM except that
the root is observed. Recently, Wang et al. (2011) proposed a classifier based on LTM.
For each class, a specific LTM is learned and a latent tree classifier is built by aggregating
all LTMs. This classifier outperforms naive Bayes and many other successful classifiers
such as tree augmented naive Bayes (Friedman, Geiger, & Goldszmidt, 1997) and averaged
one-dependence estimator (Webb, Boughton, & Wang, 2005).
More specifically to some research fields, LTM has been used for human interaction
recognition, haplotype inference in genetics and diagnosis in traditional medicine. Human
interaction recognition is a challenging task, because of multiple body parts and concomitant
inclusions (Aggarwal & Cai, 1999). For this purpose, the use of LTM allows to segment
the interaction in a multi-level fashion (Park & Aggarwal, 2003): body part positions
are estimated through low-level LVs, while overall body position is estimated by a highlevel LV. In genetics, there is a need for inferring haplotypic data (i.e. latent genetic
DNA sequences) from genotypic data (observed data). Kimmel and Shamir (2005) perform
efficient haplotypic inference using a two-layer LFM. Finally, Zhang et al. (2008) applied
LTMs to traditional Chinese medicine (TCM). They discovered that the latent structure
obtained matches TCM theories. The model provides an objective justification for the
ancient theories.

5. Discussion
In data analysis, LTM represents an emerging popular topic as it offers several advantages:
 The model allows to discover interpretable latent structure.
 Each latent variable is intended to represent a way to cluster categorical data, and connections between latent variables are meant to express relations between the multiple
clustering ways.
 Multiple latent variables organized into a tree greatly improve the flexibility of probabilistic modeling while, at the same time, ensuring linear - thus fast - probabilistic
inference.
Applications of LTMs are summarized in Table 6, which recapitulates three types of applications with details, examples, references to generic algorithms, scalability to large datasets,
software and bibliographical references.
In the past decade, extensive research efforts have been done in LTM learning. When
structure is known, standard EM and LCM-based EM or spectral methods can be used.
192

fiLatent Tree Models

Table 6: Summary for main applications of the latent tree model.
When structure is unknown, three classes of methods have been proposed: search-based,
variable clustering-based and distance-based methods. The first one is slow but leads to
accurate models. The second one drastically decreases running times. Finally, the last class
guarantees to exactly recover the generative LTM structure under the assumption that all
LVs have the number of states and this number is known.
In spite of the aforementioned advances, the use of LTM presents some drawbacks. For
example, when the data dimension is large or very large, model learning still remains prohibitive. Regarding probabilistic inference, LTM provides better results than the standard
Chow-Lius approach, but leads to more computational burden.

6. Future Directions
Progress on LTM has been made, but there is still much to be done. There are multiple
promising directions. For instance, a recent work developed LTM for continuous data analysis (Poon, Zhang, Chen, & Wang, 2010; Choi et al., 2011). Other authors investigated the
relationships between LTM and ontology (Hwang et al., 2006), and LTM-based dependence
visualization (Mourad, Sinoquet, Dina, & Leray, 2011). Although no research has been
carried out on the application to causal discovery and latent trait analysis, we argue that
LTM might represent interesting avenues of research.
LTM for Continuous Data: Recently, LTM modeling has been applied to continuous
data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).
For instance, Poon et al. (2010) proposed a new model, called pouch latent tree model
(PLTM). PLTM is identical to LTM, except that each observed node in an LTM is replaced
with a pouch node representing an aggregation of multiple continuous OVs. PLTM
represents a generalization of the Gaussian mixture model (GMM) when more than one LV
is allowed. The proposal of Poon et al. originates from the fact that model-based clustering
of continuous data is sensitive to the selected variables. Similarly to categorical clustering
(Section 4.2), in high-dimensional continuous data, there are multiple ways to partition
the data, and the multiple LVs of PLTM are able to take this into account. Poon et al.
193

fiMourad, Sinoquet, Zhang, Liu, & Leray

developed a search-based algorithm for PLTM learning. The latter algorithm is closely
related to the EAST algorithm (Zhang & Kocka, 2004b) dedicated to LTM learning and
is thus quite slow. Hence, the development of new methods dedicated to efficient PLTM
learning certainly represents interesting perspectives of research. Besides, a next step would
also be the development of LTM dedicated to mixed data analysis, i.e. combining categorical
and continuous data.
LTM Structure and Ontology: It is possible to relate LTM to ontology, in particular
taxonomy (tree-structured ontology). For instance, when applying LTM to a microarray
dataset of yeast cell-cycle, Hwang et al. (2006) showed that some LVs are significantly related to specific gene ontology terms, such as organelle organization or cellular physiological
process. Thus taxonomy could help interpret LVs. Moreover the taxonomy structure could
be used as a priori structure.
Dependence Visualization: LTM provide a compact and interpretable view of dependences between variables, thanks to its graphical nature and its latent variables (Mourad
et al., 2011). Compared to heat map (Barrett, Fry, Maller, & Daly, 2005) which can only
display pairwise dependences between variables, LTM helps visualize both pairwise and
higher-order dependences. Pairwise dependence can be represented by the chain length
linking two leaf variables, whereas higher-order dependences are simply represented by a
set of leaf variables connected to a common LV.
Causal Discovery: We argue that LTM represents simple but efficient model for causal
discovery, for the following reasons:
 If the LTM root is known, then the model can be interpreted as a hierarchy. Into
this hierarchy, LVs are distributed into multiple layers. This multiple LV layers represent different degrees of information compactness (i.e. data dimension reduction),
since each LV captures the patterns of its child variables. The connexion of variables
through parent-child relationships allows easy and natural moves from general (top
layers) to specific (bottom layers) causes, and vice-versa. Thus, causal discovery can
be guided by the hierarchical model feature.
 After constructing the model on variables X = {X1 , ..., Xn }, if one wants to test the
direct dependence between Xi and another variable Y not present in the network, it
can be easily computed through a test for independence between Xi and Y conditional
on the parent of Xi . A practical advantage of this conditional test meant to assess
direct dependence is that the number of degrees of freedom required is low (because
only the parent of Xi is used to condition the test), which ensures a good power.
Latent Trait Analysis: Similarly to the generalization of LCM by LTM, it would be
worth developing an extension of the latent trait model by a tree-structured model where
internal nodes are continuous LVs. For instance, this would alleviate the drawbacks of local
independence in the latent trait model and would provide multiple facets thanks to LVs
when dealing with high-dimensional data.

194

fiLatent Tree Models

Acknowledgments
The authors are deeply indebted to four anonymous reviewers for their invaluable comments and for helping to improve the manuscript. This work was supported by the BIL
Bioinformatics Research Project of Pays de la Loire Region, France. The authors are also
grateful to Carsten Poulsen (Aalborg University, Denmark) for providing the Danish beer
data, Yi Wang (National University of Singapore) for the LTAB algorithm, Tao Chen (EMC
Corporation, Beijing, China) for the EAST algorithm, Stefan Harmeling (Max Planck Institute, Germany) for the BIN-A and BIN-G algorithms, and Myung Jin Choi (Two Sigma
Investments, USA) and Vincent Tan (University of Wisconsin-Madison, USA) for the RG,
CLRG and regCLRG algorithms.

Appendix A. Online Resources Mentioned
Software:
 BIN-A, BIN-G, CL and LCM:
http://people.kyb.tuebingen.mpg.de/harmeling/code/ltt-1.4.tar
 CFHLC:
https://sites.google.com/site/raphaelmouradeng/home/programs
 DHC, SHC and HSHC:
http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (hlcm-distribute.zip)
http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (toolBox.zip)
 EAST:
http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (EAST.zip)
 Lantern:
http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (Lantern2.0-beta.exe)
 NJ, RG, CLRG and regCLRG:
http://people.csail.mit.edu/myungjin/latentTree.html
 NJ (fast implementation):
http://nimbletwist.com/software/ninja
All datasets used for the algorithm comparison are available at :
https://sites.google.com/site/raphaelmouradeng/home/programs

Appendix B. Supplemental Material
B.1 Experiments on Parameter Learning with EM
We studied the number of random restarts required, in practice, to obtain the convergence
of EM (Section 3.1.1) and LCMB-EM (Section 3.1.2) to the optimal solution. BIC scores
are presented in Figure 13. For EM, we used the method of Chickering and Heckerman
(1997) which is implemented in the software LTAB (Wang et al., 2008). We analyzed three
195

fiMourad, Sinoquet, Zhang, Liu, & Leray

Figure 13: The impact of the number of random restarts on the convergence of expectationmaximization (EM). a) EM. b) LCMB-EM. The number of restarts is reported
on the x-axis while the y-axis indicates BIC scores.

datasets: two small ones (BinTree and Asia) and a large one (Tree). For the first two,
convergence was achieved after 20 and 2000 restarts, respectively. For the large dataset,
convergence is never achieved, even after 5000 restarts. For LCMB-EM, we used the software
BIN-A (Harmeling & Williams, 2011). We analyzed three datasets: one small one (Asia)
and two large ones (Tree, Alarm). Convergence is achieved for the first two datasets with
only one parameter initialization, whereas for the later which is the largest, convergence
is never achieved (we were not able to assess with more than 1000 restarts because of a
prohibitive running time).
B.2 Time Complexity
We recall the reader that n is the number of variables (input data), N the number of
observations and s the number of steps (for search-based algorithms). The time complexities
of generic algorithms for LTM learning are detailed as follows:
 Naive greedy search (NGS). There are O(s) steps needed for the convergence of
search-based methods. For each step, there are O(n2 ) new models generated through
the use of 3 operators: addition/removal of an LV and node relocation (Zhang, 2004).
For each model, the cardinality is optimized for each LV, so that O(n2 ) new models are
generated (Zhang, 2004). For each model, parameters are learned using EM, which is
196

fiLatent Tree Models

achieved in O(nN ). Thus, the overall complexity is : O(s)  O(n2 )  O(n2 )  O(nN ) =
O(sn5 N ).
 Advanced greedy search (AGS), Algorithm 2. There are O(s) steps needed for
the convergence of search-based methods. For each step, there are O(n2 ) new models
generated through the use of 5 operators: addition/removal of an LV, node relocation
and addition/dismissal of a state relative to an LV (Zhang & Kocka, 2004b). For
each model, model evaluation is realized through local EM in O(N ). After choosing
the best model at each step, parameters are learned using EM, which is achieved in
O(nN ). Thus, the overall complexity is : O(s)  (O(n2 )  O(N ) + O(nN )) = O(sn2 N ).
 Agglomerative hierarchical clustering-based learning (AHCB). The agglomerative hierarchical clustering is achieved in O(n2 N )19 . LV cardinality and parameters
can be learned in O(N ) thanks to LCM parameter learning. There are O(n) LVs, thus
the complexity is (O(N )  O(n)). A final global EM parameter learning is done in
O(nN ). Thus, the overall complexity is : O(n2 N )+(O(N )O(n))+O(nN ) = O(n2 N ).
 Latent class model-based LTM learning (LCMB-LTM), Algorithm 3. Pairwise mutual information values are computed in O(n2 N ). LCM is learned in O(N ).
LCM learning is called O(n) times, i.e. for each new LV added to the model. LV
cardinality and parameters are learned during LCM learning. A final global EM parameter learning is done in O(nN ). Thus, the overall complexity is : O(n2 N ) +
(O(N )  O(n)) + O(nN ) = O(n2 N ).
 Neighbor joining (NJ). To learn the structure, there are O(n) steps. At each step,
pairwise distances are computed in O(n2 N ). Structure learning thus requires O(n3 N )
computations. After learning the structure, parameters can be learned with EM or
LCMB-EM in O(nN ). Thus, the overall complexity is : O(n3 N ) + O(nN ) = O(n3 N ).
 Distance-based general LTM learning (DBG), Algorithm 4. First the distances are computed in O(n3 N ). If the minimum spanning tree is learned before, the
complexity is reduced to O(n2 N ). Then, to learn the structure, testing child-parent
and sibling relations necessitates O(n4 ) operations in the worst case, i.e. when the
tree is a hidden Markov model. Parameters can be learned with EM or LCMB-EM in
O(nN ). Thus, the overall complexity is : O(n3 N ) + O(n4 ) + O(nN ) = O(n3 N + n4 )
or O(n2 N ) + O(n4 ) + O(nN ) = O(n2 N + n4 ).
B.3 Description of Datasets Used for Literature Algorithm Comparison
Small datasets (n  10 variables):
 BinTree. Datasets generated using a binary tree on 7 variables (4 leaves and 3
internal nodes), each having eight states. Only leaf data are used. The train and
test datasets both consist of 500 observations. The model comes from the work of
Harmeling and Williams (2011).
19. The complexity of agglomerative hierarchical clustering is O(n2 N ) using the single linkage criterion. The
complexity is higher for other criteria.

197

fiMourad, Sinoquet, Zhang, Liu, & Leray

 BinForest. Datasets generated from a binary forest composed of two trees. One
tree has 3 variables (2 leaves and 1 internal node), the other one has 5 variables (3
leaves and 2 internal nodes). Only leaf data are used. The train and test datasets
both consist of 500 observations. The model comes from the work of Harmeling and
Williams (2011).
 Asia. Datasets generated using the well-known Asia network containing 8 binary
OVs. The train and test datasets both consist of 100 observations.
 Hannover. Real dataset containing 5 binary variables. The dataset has been split
into a train dataset and a test dataset. They consist of 3573 and 3589 observations,
respectively. The dataset comes from the work of Zhang (2004).
 Car. Real dataset containing 7 variables. The dataset has been split into a train
dataset and a test dataset. They consist of 859 and 869 observations, respectively.
The dataset is available at :
http://archive.ics.uci.edu/ml/.
Large datasets (10  n  100 variables):

 Tree. Datasets generated using a tree on 50 variables (19 leaves and 31 internal
nodes). Only leaf data are used. The train and test datasets both consist of 500
observations.
 Forest. Datasets generated using a tree on 50 variables (20 leaves and 30 internal
nodes). Only leaf data are used. The train and test datasets both consist of 500
observations.
 Alarm. Datasets generated using the well-known Alarm network containing 37 OVs.
The train and test datasets both consist of 1000 observations.
 Coil-42. Real dataset containing 42 variables. The dataset has been split into a train
dataset and a test dataset. They consist of 5822 and 4000 observations, respectively.
The dataset comes from the work of Zhang and Kocka (2004b).
 NewsGroup. Real dataset containing 100 binary variables. The dataset has been
split into a train dataset and a test dataset. They both consist of 8121 observations.
The dataset is available at :
http://cs.nyu.edu/roweis/data/20news w100.mat.

Very large datasets (n > 100 variables):
 HapGen. Datasets generated over 1000 genetic variables using the HAPGEN software (Spencer, Su, Donnelly, & Marchini, 2009). The train and test datasets both
consist of 1000 observations.
 HapMap. Real dataset containing 10000 binary variables. The dataset has been
split into a train dataset and a test dataset. They consist of 118 and 116 observations,
respectively. The dataset comes from HapMap phase III (The International HapMap
Consortium, 2007) and concerns Utah residents with Northern and Western European
ancestry (CEU).

198

fiLatent Tree Models

References
Aggarwal, J., & Cai, Q. (1999). Human motion analysis: A review. Computer Vision and
Image Understanding, 73 (3), 428440.
Akaike, H. (1970). Statistical predictor identification. Annals of the Institute of Statistical
Mathematics, 22 (1), 203217.
Anandkumar, A., Chaudhuri, K., Hsu, D., Kakade, S. M., Song, L., & Zhang, T. (2011).
Spectral methods for learning multivariate latent tree structure. In Twenty-Fifth
Conference in Neural Information Processing Systems (NIPS-11).
Atteson, K. (1999). The performance of neighbor-joining methods of phylogenetic reconstruction. Algorithmica, 25 (2), 251278.
Attias, H. (1999). Inferring parameters and structure of latent variable models by variational
Bayes. In Proceedings of the 15th Conference on Uncertainty and Artificial Intelligence
(UAI-99), pp. 2130.
Barrett, J. C., Fry, B., Maller, J., & Daly, M. J. (2005). Haploview: analysis and visualization
of LD and haplotype maps. Bioinformatics, 21 (2), 263265.
Binder, J., Koller, D., Russel, S., & Kanazawa, K. (1997). Adaptive probabilistic networks
with hidden variables. Machine Learning, 29 (2-3), 213244.
Cavender, J. A. (1978). Taxonomy with confidence. Mathematical Biosciences, 40 (3-4),
271280.
Chen, T., & Zhang, N. L. (2006). Quartet-based learning of hierarchical latent class models:
Discovery of shallow latent variables. In Proceedings of 9th International Symposium
on Artificial Intelligence and Mathematics.
Chen, T. (2008). Search-based learning of latent tree models. Ph.D. thesis, The Hong Kong
University of Science and Technology.
Chen, T., Zhang, N. L., Liu, T., Poon, K. M., & Wang, Y. (2012). Model-based multidimensional clustering of categorical data. Artificial Intelligence, 176 (1), 22462269.
Chen, T., Zhang, N. L., & Wang, Y. (2008). Efficient model evaluation in the searchbased approach to latent structure discovery. In Proceedings of the Fourth European
Workshop on Probabilistic Graphical Models (PGM-08), pp. 5764.
Chickering, D. M., & Heckerman, D. (1997). Efficient approximations for the marginal
likelihood of Bayesian networks with hidden variables. Machine Learning, 29 (2-3),
181212.
Choi, M. J., Tan, V. Y., Anandkumar, A., & Willsky, A. S. (2011). Learning latent tree
graphical models. Journal of Machine Learning Research, 12, 17711812.
Chow, C. K., & Liu, C. N. (1968). Approximating discrete probability distributions with
dependence trees. IEEE Transactions on Information Theory, 14 (3), 462467.
Cooper, G. F. (1990). The computational complexity of probabilistic inference using
Bayesian belief networks. Artificial Intelligence, 42 (2-3), 393405.
Cover, T. M., & Thomas, J. A. (1991). Elements of information theory. Wiley-Interscience.
199

fiMourad, Sinoquet, Zhang, Liu, & Leray

Daskalakis, C., Mossel, E., & Roch, S. (2009). Evolutionary trees and the Ising model on
the Bethe lattice: A proof of Steels conjecture. Probability Theory and Related Fields,
149 (1-2), 149189.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological),
39, 138.
Elidan, G., & Friedman, N. (2001). Learning the dimensionality of hidden variables. In
Proceedings of the 17th Conference on Uncertainty and Artificial Intelligence (UAI01), pp. 144151.
Erdos, P. L., Szekely, L. A., Steel, M. A., & Warnow, T. J. (1999). A few logs suffice to
build (almost) all trees: Part II. Theoretical Computer Science, 221 (1-2), 77118.
Ezawa, K. J., & Norton, S. W. (1996). Constructing Bayesian networks to predict uncollectible telecommunications accounts. IEEE Expert, 11 (5), 4551.
Farris, J. S. (1973). A probability model for inferring evolutionary trees. Systematic Zoology,
22 (3), 250256.
Felsenstein, J. (2003). Inferring phylogenies (2 edition). Sinauer Associates.
Friedman, N., Ninio, M., Peer, I., & Pupko, T. (2002). A structural EM algorithm for
phylogenetic inference.. Journal of Computational Biology, 9 (2), 331353.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
Learning, 29 (2-3), 131163.
Gascuel, O., & Steel, M. (2006). Neighbor-joining revealed. Molecular Biology and Evolution, 23 (11), 19972000.
Geiger, D., Heckerman, D., & Meek, C. (1996). Asymptotic model selection for directed
networks with hidden variables. In Proceedings of Twelfth Conference on Uncertainty
in Artificial Intelligence (UAI-96), pp. 283290. Morgan Kaufmann.
Hair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2009). Multivariate data analysis
(7 edition). Prentice Hall.
Harmeling, S., & Williams, C. K. I. (2011). Greedy learning of binary latent trees. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 33 (6), 10871097.
Hsu, D., Kakade, S., & Zhang, T. (2009). A spectral algorithm for learning hidden Markov
models. In The 22nd Annual Conference on Learning Theory (COLT 2009).
Hwang, K.-B., Kim, B.-H., & Zhang, B.-T. (2006). Learning hierarchical Bayesian networks
for large-scale data analysis. In International Conference on Neural Information Processing (ICONIP-06), pp. 670679.
Kim, J. H., & Pearl, J. (1983). A computation model for causal and diagnostic reasoning
in inference systems. In Proceedings of the 8th International Joint Conference on
Artificial Intelligence.
Kimmel, G., & Shamir, R. (2005). GERBIL: Genotype resolution and block identification
using likelihood. Proceedings of the National Academy of Sciences of the United States
of America, 102 (1), 158162.
200

fiLatent Tree Models

Kirshner, S. (2012). Latent tree copulas. In Proceedings of the Sixth European Workshop
on Probabilistic Graphical Models (PGM-12).
Koller, D., & Friedman, N. (2009). Probabilistic graphical models: Principles and techniques
(adaptive computation and machine learning). The MIT Press.
Kraskov, A., & Grassberger, P. (2009). Information theory and statistical learning, chap.
MIC: Mutual information based hierarchical clustering, pp. 101123. Springer.
Kwoh, C.-K., & Gillies, D. F. (1996). Using hidden nodes in Bayesian networks. Artificial
Intelligence, 88 (1-2), 138.
Lake, J. A. (1994). Reconstructing evolutionary trees from DNA and protein sequences:
Paralinear distances. Proceedings of the National Academy of Sciences of the United
States of America, 91 (4), 14551459.
Langseth, H., & Nielsen, T. D. (2009). Latent classification models for binary data. Pattern
Recognition, 42 (11), 27242736.
Lauritzen, S. L. (1995). The EM algorithm for graphical association models with missing
data. Computational Statistics & Data Analysis, 19 (2), 191201.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on
graphical structures and their application to expert systems. Journal of the Royal
Statistical Society. Series B (Methodological), 50 (2), 157224.
Liu, H., Xu, M., Gu, H., Gupta, A., Lafferty, J., & Wasserman, L. (2011). Forest density
estimation. Journal of Machine Learning Research, 12, 907951.
Liu, T. F., Zhang, N. L., Liu, A. H., & Poon, L. K. M. (2012). A novel LTM-based method
for multidimensional clustering. In Proceedings of the Sixth European Workshop on
Probabilistic Graphical Models (PGM-12).
Martin, J., & Vanlehn, K. (1995). Discrete factor analysis: Learning hidden variables in
Bayesian network. Tech. rep., Department of Computer Science, University of Pittsburgh.
McKendrick, I. J., Gettinbya, G., Gua, Y., Reidb, S. W. J., & Revie, C. W. (2000). Using
a Bayesian belief network to aid differential diagnosis of tropical bovine diseases.
Preventive Veterinary Medicine, 47 (3), 141156.
Mossel, E., & Roch, S. (2006). Learning nonsingular phylogenies and hidden Markov models.
The Annals of Applied Probability, 16 (2), 583614.
Mossel, E., Roch, S., & Sly, A. (2011). Robust estimation of latent tree graphical models:
Inferring hidden states with inexact parameters. Submitted.
Mourad, R., Sinoquet, C., Dina, C., & Leray, P. (2011). Visualization of pairwise and
multilocus linkage disequilibrium structure using latent forests. PLoS ONE, 6 (12),
e27320.
Mourad, R., Sinoquet, C., & Leray, P. (2011). A hierarchical Bayesian network approach for
linkage disequilibrium modeling and data-dimensionality reduction prior to genomewide association studies. BMC Bioinformatics, 12, 16.
201

fiMourad, Sinoquet, Zhang, Liu, & Leray

Parikh, A. P., Song, L., & Xing, E. P. (2011). A spectral algorithm for latent tree graphical
models. In Proceedings of the 28th International Conference on Machine Learning
(ICML-2011).
Park, S., & Aggarwal, J. K. (2003). Recognition of two-person interactions using a hierarchical Bayesian network. In The first ACM International Workshop on Video
Surveillance (IWVS03), pp. 6575.
Parzen, E. (1962). On estimation of a probability density function and mode. Annals of
Mathematical Statistics, 33, 10651076.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan Kaufmann, Santa Mateo, CA, USA.
Poon, L. K. M., Zhang, N. L., Chen, T., & Wang, Y. (2010). Variable selection in modelbased clustering: To do or to facilitate. In Proceedings of the 27th International Conference on Machine Learning (ICML-2010).
Prim, R. C. (1957). Shortest connection networks and some generalizations. Bell System
Technical Journal, 36, 13891401.
Raftery, A. E. (1986). Choosing models for cross-classifications. American Sociological
Review, 51 (1), 145146.
Rosenblatt, M. (1956). Remarks on some nonparametric estimates of a density function.
Annals of Mathematical Statistics, 27, 832837.
Saitou, N., & Nei, M. (1987). The neighbor-joining method: A new method for reconstructing phylogenetic trees.. Molecular Biology and Evolution, 4 (4), 406425.
Schwartz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6 (2),
461464.
Song, L., Parikh, A., & Xing, E. (2011). Kernel embeddings of latent tree graphical models.
In Twenty-Fifth Conference in Neural Information Processing Systems (NIPS-11).
Spencer, C. C., Su, Z., Donnelly, P., & Marchini, J. (2009). Designing genome-wide association studies: sample size, power, imputation, and the choice of genotyping chip..
PLoS Genetics, 5 (5), e1000477.
Steel, M. (2001).
My favourite conjecture.
m.steel/files/misc/conjecture.pdf.

http://www.math.canterbury.ac.nz/-

Steel, M. (1992). The complexity of reconstructing trees from qualitative characters and
subtrees. Journal of Classification, 9 (1), 91116.
Tan, V. Y. F., Anandkumar, A., & Willsky, A. (2011). Learning high-dimensional Markov
forest distributions: Analysis of error rates. Journal of Machine Learning Research,
12, 16171653.
The International HapMap Consortium (2007). A second generation human haplotype map
of over 3.1 million SNPs. Nature, 449 (7164), 851861.
Wang, Y., Zhang, N. L., Chen, T., & Poon, L. K. M. (2011). Latent tree classifier. In
European Conferences on Symbolic and Quantitative Approaches to Reasoning with
Uncertainty (ECSQARU2011), pp. 410421.
202

fiLatent Tree Models

Wang, Y., & Zhang, N. L. (2006). Severity of local maxima for the EM algorithm: Experiences with hierarchical latent class models. In Proceedings of the Third European
Workshop on Probabilistic Graphical Models (PGM-06).
Wang, Y., Zhang, N. L., & Chen, T. (2008). Latent tree models and approximate inference
in Bayesian networks. Journal of Articial Intelligence Research, 32, 879900.
Webb, G. I., Boughton, J. R., & Wang, Z. (2005). Not so naive Bayes: Aggregating onedependence estimators. Machine Learning, 58 (1), 5 24.
Wheeler, T. J. (2009). Large-scale neighbor-joining with NINJA. In Proceedings of the 9th
Workshop on Algorithms in Bioinformatics.
Xu, L., & Jordan, M. I. (1996). On convergence properties of the EM algorithm for Gaussian
mixtures. Neural Computation, 8 (1), 129151.
Xu, R., & Wunsch, D. (2005). Survey of clustering algorithms. IEEE Transactions on
Neural Networks, 16 (3), 645678.
Xu, R., & Wunsch, D. C. (2008). Clustering (illustrated edition). Wiley-IEEE Press.
Zhang, N. L. (2004). Hierarchical latent class models for cluster analysis. The Journal of
Machine Learning Research, 5, 697723.
Zhang, N. L., & Kocka, T. (2004a). Effective dimensions of hierarchical latent class models.
Journal of Articial Intelligence Research, 21, 117.
Zhang, N. L., & Kocka, T. (2004b). Efficient learning of hierarchical latent class models.
In Proceedings of the 16th IEEE International Conference on Tools with Artificial
Intelligence (ICTAI), pp. 585593.
Zhang, N. L., Nielsen, T. D., & Jensen, F. V. (2004). Latent variable discovery in classification models. Artificial Intelligence in Medicine, 30 (3), 283299.
Zhang, N. L., Wang, Y., & Chen, T. (2008). Discovery of latent structures: Experience
with the CoIL Challenge 2000 data set*. Journal of Systems Science and Complexity,
21 (2), 172183.
Zhang, N. L., Yuan, S., Chen, T., & Wang, Y. (2008). Latent tree models and diagnosis in
traditional Chinese medicine. Artificial Intelligence in Medicine, 42 (3), 229245.

203

fiJournal of Artificial Intelligence Research 47 (2013) 393-439

Submitted 9/12; published 7/13

Lifted Variable Elimination:
Decoupling the Operators from the Constraint Language
Nima Taghipour
Daan Fierens
Jesse Davis
Hendrik Blockeel

nima.taghipour@cs.kuleuven.be
daan.fierens@cs.kuleuven.be
jesse.davis@cs.kuleuven.be
hendrik.blockeel@cs.kuleuven.be

KU Leuven, Department of Computer Science
Celestijnenlaan 200A, 3001 Leuven, Belgium

Abstract
Lifted probabilistic inference algorithms exploit regularities in the structure of graphical
models to perform inference more efficiently. More specifically, they identify groups of
interchangeable variables and perform inference once per group, as opposed to once per
variable. The groups are defined by means of constraints, so the flexibility of the grouping
is determined by the expressivity of the constraint language. Existing approaches for exact
lifted inference use specific languages for (in)equality constraints, which often have limited
expressivity. In this article, we decouple lifted inference from the constraint language. We
define operators for lifted inference in terms of relational algebra operators, so that they
operate on the semantic level (the constraints extension) rather than on the syntactic
level, making them language-independent. As a result, lifted inference can be performed
using more powerful constraint languages, which provide more opportunities for lifting. We
empirically demonstrate that this can improve inference efficiency by orders of magnitude,
allowing exact inference where until now only approximate inference was feasible.

1. Introduction
Statistical relational learning or SRL (Getoor & Taskar, 2007; De Raedt, Frasconi, Kersting, & Muggleton, 2008) focuses on combining first-order logic with probabilistic graphical
models, which permits algorithms to reason about complex, uncertain, structured domains.
A major challenge in this area is how to perform inference efficiently. First-order logic can
reason on the level of logical variables: if a model states that for all X, P (X) implies Q(X),
then whenever P (X) is known to be true, one can infer Q(X), without knowing what X
stands for. Many approaches to SRL, however, transform their knowledge into a propositional graphical model before performing inference. By doing so, they lose the capacity to
reason on the level of logical variables: standard inference methods for graphical models
can reason only on the ground level, repeating the same inference steps for each different
value x of X, instead of once for all x.
To address this problem, Poole (2003) introduced the concept of lifted inference for
graphical models. The idea is to group together interchangeable objects, and perform
the inference operations once for each group instead of once for each object. Multiple
different algorithms have been proposed, based on variable elimination (Poole, 2003; de
Salvo Braz, Amir, & Roth, 2005; Milch, Zettlemoyer, Kersting, Haimes, & Kaelbling, 2008;
Sen, Deshpande, & Getoor, 2009b, 2009a; Choi, Hill, & Amir, 2010; Apsel & Brafman,
c
2013
AI Access Foundation. All rights reserved.

fiTaghipour, Fierens, Davis, & Blockeel

2011), belief propagation (Kersting, Ahmadi, & Natarajan, 2009; Singla & Domingos, 2008),
or various other approaches (Van den Broeck, Taghipour, Meert, Davis, & De Raedt, 2011;
Jha, Gogate, Meliou, & Suciu, 2010; Gogate & Domingos, 2011).
A group of interchangeable objects is typically defined by means of a constraint that
an object must fulfill in order to belong to that group. The type of constraints that are
allowed, and the way in which they are handled, directly influence the granularity of the
grouping, and hence, the efficiency of the subsequent lifted inference (Kisynski & Poole,
2009a). Among the approaches based on variable elimination, the most advanced system
(C-FOVE) uses a specific class of constraints, namely, conjunctions of pairwise (in)equalities.
This is the bare minimum required to be able to perform lifted inference. However, as we
will show, it unnecessarily limits the symmetries the model can capture and exploit.
In this article, we present an algorithm for lifted variable elimination that is based
on C-FOVE, but uses a constraint language that is extensionally complete, that is, for
any group of variables a constraint exists that defines exactly that group. To this aim,
C-FOVEs constraint manipulation is redefined in terms of relational algebra operators.
This decouples the lifted inference algorithm from the constraint representation mechanism.
Consequently, any constraint language that is closed under these operators can be plugged
into the algorithm to obtain a working system. Apart from redefining existing operators,
we also define a novel operator, called lifted absorption, in this way. Furthermore, we
propose a concrete mechanism for constraint representation that is extensionally complete,
and briefly discuss how the operators can be implemented for this particular mechanism.
The new lifted inference algorithm, with this constraint representation mechanism, can
perform lifted inference with a much coarser granularity than its predecessors. Due to this,
it outperforms existing systems by several orders of magnitude on some problems, and solves
inference problems that until now could only be solved by approximate inference methods.
The basic ideas behind our approach have been explained in an earlier conference paper
(Taghipour, Fierens, Davis, & Blockeel, 2012). This article extends that paper by providing
precise and motivated definitions for the operators, up to the level where they can be
implemented. These definitions, at the same time, may help understand on a more intuitive
and semantic level how lifted variable elimination works, and can serve as a kind of gold
standard for other implementations of lifted variable elimination, as they provide a semantics
based reference point.
The paper is structured as follows. Section 2 illustrates the principles of lifted variable
elimination by example, and briefly states how this work improves upon the state of the art,
C-FOVE (Milch et al., 2008). Section 3 introduces formal notation and terminology, and
Section 4 provides a high-level outline of our lifted variable elimination algorithm. Section
5 describes in detail all the operators that the algorithm uses. Section 6 briefly discusses an
efficient representation for the constraints themselves. Section 7 empirically compares our
systems performance with that of C-FOVE, and Section 8 concludes.

2. Lifted Variable Elimination by Example
Although lifted variable elimination builds on simple intuitions, it is relatively complicated,
and an accurate description of it requires a level of technical detail that is not conducive to a
clear understanding. For this reason, we first illustrate the basic principles of lifted inference
394

fiDecoupling Lifted Variable Elimination from the Constraint Language

on a simple example, and without referring to the technical terminology that is introduced
later. We start with describing the example; next, we illustrate variable elimination on this
example, and show how it can be lifted.
2.1 The Workshop Example
This example is from Milch et al. (2008). Suppose a new workshop is organized. If the
workshop is popular (that is, many people attend), it may be the start of a series. Whether
a person is likely to attend depends on the topic.
We introduce a random variable T , indicating the topic of the workshop, and a random
variable S, indicating whether the workshop becomes a series. We consider N people, and
for each person i, we include a random variable Ai that indicates whether i attends. Each
random variable has a finite domain from which it takes on values, i.e., {ai, ml, . . . } for T ,
{yes, no} for S, and {true, f alse} for each Ai .
The joint probability distribution of these variables can be specified by an undirected
graphical model. A set of factors captures dependencies between the random variables in
such a model. In our model, there are two kinds of factors. For each person i, there is a
factor 1 (Ai , S) that states how having a series depends on whether person i attends, and
a factor 2 (T, Ai ) that states how is attendance depends on the topic. Note that all N
factors of the first type have the same potential function 1 , and all factors of the second
type have potential function 2 . This imposes a certain symmetry on the model: it implies
that S depends on each persons attendance in exactly the same way, and all people have
the same topic preferences.
The model defines a joint probability distribution over the variables that is the normalized product of the factors (normalized such that all joint probabilities sum to one):
Pr(T, S, A1 , . . . , AN ) =

n
n
Y
1 Y
2 (T, Ai )
1 (Ai , S)
Z
i=1

i=1

where Z is the normalization constant.
Undirected graphical models can be visualized as factor graphs (Kschischang, Frey, &
Loeliger, 2001), which have a node for each random variable and each factor, and an edge
between a factor and a random variable if that variable occurs in the factor. Figure 1 shows
a factor graph for our example.
2.2 Variable Elimination
From now on, we refer to the values taken by a variable by the corresponding lowercase
symbols (e.g., ai as shorthand for Ai = ai ).
Suppose we want to compute the marginal probability distribution Pr (S).
Pr (S) =

XX
T

=



A1

X

Pr (T, S, A1 , . . . , AN )

N
N
Y
1 XX XY
2 (T, Ai )
1 (Ai , S)

Z
T

A1

(1)

AN

AN i=1

395

i=1

(2)

fiTaghipour, Fierens, Davis, & Blockeel

S
1

1

A1

A2

...

A3

An
2

2

T
Figure 1: A factor graph for the workshop example. Square nodes represent factors, round
nodes variables. Variables are labeled with their name, factors with their potential
function.

Usually, the normalization constant Z is ignored during the computations, and normalization happens only at the very end. So, we can focus on how to compute
 (S) =
Pr

XX
T

A1



N
XY

1 (Ai , S)

N
Y

2 (T, Ai ).

(3)

i=1

AN i=1

 (S) is to compute Pr
 (s) for each possible value s
A straightforward way of computing Pr

of S, and tabulate the results. We can compute Pr(true) by iterating
over all possible
Q
QN value
combinations (t, a1 , . . . , an ) of (T, A1 , . . . , An ) and computing N

(a
,
true)
i=1 1 i
i=1 2 (t, ai )
 alse). If all variables are binary, there are 2N +1
for each combination, and similarly for Pr(f
such combinations, and for each combination 2N  1 multiplications are performed. This
clearly does not scale.
However, we can improve efficiency by rearranging the computations. In the above computation, the same multiplications are performed repeatedly. Since 1 (A1 , S) and 2 (T, A1 )
are constant in all Ai except A1 , they can be moved out of the summations over Ai , i > 1,
so the right hand side of Equation 3 becomes:
XX
T

1 (A1 , S)2 (T, A1 )

X



1 (Ai , S)

N
Y

2 (T, Ai )

(4)

i=2

AN i=2

A2

A1

N
XY

P
Conversely, the factor starting with A2 is independent of A1 , so it can be moved outside
of the summation over A1 , giving



N
N
Y
X
XY
X X

2 (T, Ai ) 
1 (Ai , S)
1 (A1 , S)2 (T, A1 )

(5)
T

A2

AN i=2

i=2

A1

Repeating this for each Ai eventually yields




X
X X

1 (AN , S)2 (T, AN )
1 (A1 , S)2 (T, A1 ) . . . 
T

AN

A1

396

(6)

fiDecoupling Lifted Variable Elimination from the Constraint Language

12 (T, A1 , S)
1 (A1 , S)
A1
true
false
true
false

S
true
true
false
false

2 (T, A1 )
1
1
2
2
1

T
SRL
SRL
DB
DB



A1
true
false
true
false

2
3
1
2
2

=

T
SRL
SRL
SRL
SRL
DB
DB
DB
DB

A1
true
true
false
false
true
true
false
false

S
true
false
true
false
true
false
true
false

12
3
6
2
1
2
4
4
2

12 (T, S)

P

A1

12 (T, A1 , S)

=

T
SRL
SRL
DB
DB

S
true
false
true
false

12
5
7
6
6

Figure 2: Two example factors, their product, and the result of summing out A1 from the
product. The values of 1 and 2 are chosen arbitrarily for this illustration.

which shows that for each Ai , the product 1 (Ai , S)2 (T, Ai ) needs to be computed only
once for each combination of values for (T, S, Ai ). When T is binary, there are eight such
combinations, reducing the total number of multiplications to 8N .
Note that the result of Formula 6 is a function of S; it can yield a different value for each
value s of S. In other words, it is a factor over S. Similarly, the result of 1 (A1 , S)2 (T, A1 )
depends on the values of S, T and A1 (is a factor over these variables), and after summation
over A1 a factor over S and T is obtained. Thus, the multiplications and summations in
Formula 6 are best seen as operating on factors, not individual numbers. Figure 2 illustrates
the process of multiplying and summing factors.
The result of Formula 6 can be computed as follows. First, multiply the factors 1 (A1 , S)
and 2 (T, A1 ) for each value of A1 , and sum out A1 from the product. This is exactly the
computation illustrated in Figure 2. After summing over all values of A1 , the result depends
on T and S only; A1 no longer occurs in this factor, nor in any other factors. We say that
A1 has been eliminated. Note that the elimination consisted of first gathering all factors
containing A1 , multiplying them, then summing over all possible values of A1 .
After eliminating A1 , we can repeat the process for all other Ai , each time obtaining a
factor over T and S. All those factors can then be multiplied and the result summed over
 (S).
T , at which point a single factor over S is obtained. This factor equals Pr
The above computation is exactly what Variable Elimination (VE) does. Generally, VE
works as follows. It considers one variable at a time, in an order called the elimination
order. For each considered variable X, VE first retrieves all factors that involve X, then
multiplies these factors together into a single joint factor, and finally sums out X, thereby
397

fiTaghipour, Fierens, Davis, & Blockeel

eliminating X from the factor. Hence, in each step, the number of remaining variables
strictly decreases (by 1) and also the number of factors decreases (because the set of factors
involving X is replaced by a single factor).
The elimination order can heavily influence runtime. Unfortunately, finding the optimal
order is NP-hard. In the above example, the elimination order was A1 , A2 , . . . , AN , T , and
this resulted in a computation with 8N multiplications, which is O(N ).
2.3 Lifted Inference: Exploiting Symmetries Among Factors
In the above example, by avoiding many redundant computations, VE obtained an exponential speedup compared to the naive computation discussed before, reducing computation
time from O(2N ) to O(N ). N can still be large. Even more efficiency can be gained when
we know that certain factors have the same potential function.
In our example, VE computes the same product N times: in Expression 6, factors
1 (Ai , S) and 2 (T, Ai ) are the same for allPi, and so is their product 12 (Ai , S, T ) =
1 (Ai , S)2 (T, Ai ). It also computes the sum Ai 12 (Ai , S, T ) N times. This redundancy
arises because in our probabilistic model all N people behave in the same way, i.e., all
Ai are interchangeable. The idea behind lifted inference is to exploit such symmetries,
and compute the product and sum only once. From the algorithmic perspective, lifted
variable elimination eliminates only one Ai variable, then exponentiates the resulting factor
(see formula below), and then sums out T . Mathematically, Expression 6 is computed as
follows:


N

X
T

X
 (1 (A1 , S)2 (T, A1 ))

(7)

A1

The way in which lifted variable elimination manipulates the set of variables {A1 , . . . , AN }
is called lifted multiplication and lifted summing-out (a.k.a. lifted elimination). Note that
the number of operations required is now constant in N . Assuming N is already known,
the main operation here is computing the N -th power, which is O(log N ) (logarithmic in
N if exact arithmetic is used, constant for floating point arithmetic). Thus, lifted variable
elimination runs in O(log N ) time in this case.
2.4 Lifted Inference: Exploiting Symmetries within Factors
Now consider a second elimination order, where we first eliminate T and then the Ai :
 (S) =
Pr

X

N
XY

A1 ,...,AN T

1 (Ai , S)

i=1

N
Y

2 (T, Ai ) =

X

N
Y

A1 ,...,AN i=1

i=1

1 (Ai , S)

N
XY
T

i=1

!

2 (T, Ai )

(8)
With this order, regular variable elimination works as follows. The inner summation (elimination of T ) first multiplies all factors 2 (T, Ai ) into a factor 3 (T, A1 , . . . , AN ), and then
sums out T :
N
XY
T i=1

2 (T, Ai ) =

X

3 (T, A1 , . . . , AN ) = 3 (A1 , . . . , AN )

T

398

fiDecoupling Lifted Variable Elimination from the Constraint Language

Note that 3 is a function of N + 1 binary variables, so its tabular representation has 2N +1
entries, which makes the cost of this elimination O(2N +1 ). Substituting the computed 3
into Equation (8) yields:

Pr(S)
=

X

A1 ,...,AN

N
Y

!

1 (Ai , S)

i=1

3 (A1 , . . . , AN )

Now we can multiply 3 (A1 , . . . , AN ) by 1 (A1 , S) and sum out A1 , then multiply the result
by 1 (A2 , S) and sum out A2 , and so on, until we obtain a factor 4 (S):
 (S) = 4 (S)
Pr
This again involves N multiplications and summations with exponential complexity. In
summary, variable elimination computes the result in O(2N +1 ).
This elimination order also has symmetries that lifted inference can exploit. Let us
examine 3 (T, A1 , . . . , AN ), the product of factors 2 (T, Ai ). For each assignment T = t
and (A1 , . . . , AN ) = (a1 , . . . aN )  {true, f alse}N :
3 (t, a1 , . . . , aN ) = 2 (t, a1 ) . . . 2 (t, aN )
Note that, since each ai  {true, f alse}, the multiplicands on the right hand side can
have only one of two values, 2 (t, true) or 2 (t, f alse). That is, for each ai = true there
is a 2 (t, true), and similarly for each ai = f alse, a 2 (t, f alse). This means that, with
At = {Ai |ai = true} and Af = {Ai |ai = f alse}, we can rewrite the above expression as:
3 (t, a1 , . . . , aN ) =

Y

2 (t, true)

ai At

Y

2 (t, f alse) = 2 (t, true)|At | 2 (t, f alse)|Af | .

ai Af

This shows that to evaluate 3 (T, A1 , . . . , AN ) it suffices to know how many Ai are true (call
this number nt ) and false (nf ); we do not need to know the value of each individual Ai . We
can therefore restate 3 in terms of a new variable #[A], called a counting variable, the value
of which is the two-dimensional vector (nt , nf ). Generally, #[A] can take any value (x, y)
with x, y  N and x + y = N . We call such a value a histogram. It captures the distribution
of values among A = {A1 , . . . , AN }. The reformulation of a factor in terms of a counting
variable is called counting conversion. Rewriting 3 (T, A1 , . . . , AN ) as 3 (T, #[A]), we have
3 (t, (nt , nf )) = 2 (t, true)nt 2 (t, f alse)nf .
3 has 2(N + 1) possible input combinations (two values for t and N + 1 values for (nt , nf ),
since nt +nf = N ). It can be tabulated in time O(N ), using the recursive formula 3 (t, (nt +
1, nf  1)) = 3 (t, (nt , nf ))  1 (t, true)/2 (t, f alse). Note that VEs computation of 3 was
O(2N ).
Because 3 has only 2(N +1) possible input states, instead of 2N +1 , we can now eliminate
T in O(N ):
N
X
XY
2 (T, Ai ) =
3 (T, #[A]) = 3 (#[A])
T

i=1

T

399

fiTaghipour, Fierens, Davis, & Blockeel

Using this result, we continue with the elimination:
 (S) =
Pr

X

N
Y

1 (Ai , S) 3 (#[A])

A1 ,...,AN i=1

Using counting conversion a second time, we can reformulate the result of
as 4 (#[A], S), which gives:
 (S) =
Pr

X

4 (#[A], S) 3 (#[A]) =

A1 ,...,AN

X

QN

i=1 1 (Ai , S)

43 (#[A], S)

(9)

A1 ,...,AN

In itself, the final summation still enumerates all 2N joint states of variables A, computes
the histogram (nt , nf ) and 43 ((nt , nf ), S) for each state, and adds up all the 43 . But we
can do better: all states that result in the same histogram (nt , nf ) have the same value for

43 ((nt , nf ), S), and we know exactly how many such joint states there are, namely nNt =
N!
nt !nf ! . We will call this the multiplicity of the histogram (nt , nf ), denoted Mul((nt , nf )).
Thus, we can compute 43 ((nt , nf ), S) just once for each histogram (nt , nf ) and multiply it
by its multiplicity:
X

43 (#[A], S) =

A1 ,...,AN

X

Mul(#[A])  43 (#[A], S)

#[A]

This way we enumerate over N + 1 possible values of #[A] instead of 2N possible states of
A. To summarize, we can reformulate Equation (9) as
 (S) =
Pr

X

43 (#[A], S) =

A1 ,...,AN

X

Mul(#[A])  43 (#[A], S) = 5 (S)

#[A]

which shows that #[A] can be eliminated with O(N ) operations.
 (S) thus has complexity O(N ), instead of O(2N ) for VE
The whole computation of Pr
with this elimination order. This reduction in complexity is possible due to symmetries in
the model that allow us to treat all variables A as one unit #[A].
2.5 Capturing the Symmetries
It is clear that lifting can yield important speedups, if certain symmetries among factors or
among the inputs of a single factor are present. To exploit these, it is essential that one can
indicate which variables are interchangeable and hence induce these symmetries.
In our workshop example, assume, for instance, that not every person has the same
preferences with respect to topics, but there are two types of people, and different potentials
(2a and 2b ) are associated with each type of person. It is clear that instead of Formula 7,
X
T




X
A1

N

1 (A1 , S)2 (T, A1 ) ,
400

fiDecoupling Lifted Variable Elimination from the Constraint Language

we then need to compute
 Na 

 Nb
X X
X

1 (Ak , S)2a (T, Ak ) 
1 (Al , S)2b (T, Al )
T

Ak

Al

where Ak and Al are random members from the first and second group, and Na and Nb the
cardinality of these groups. In order to do this, we need to be able to state for which Ai
2a is relevant, and for which 2b is. (For this particular computation, it actually suffices
to know the size of each group, but that is not true in general; for instance, to compute the
marginal distribution of A5 , we need to know which group A5 is in.)
Our main contribution is related to this particular point. At the time of writing, the
C-FOVE system (Milch et al., 2008) is considered the state of the art in lifted variable elimination. By introducing counting variables, it can capture within-factor symmetries better
than its predecessor, FOVE. However, as it turns out, C-FOVE is less good at capturing
symmetries among multiple factors, compared to FOVE. This is because groups of variables
or factors are defined by means of constraints, and C-FOVE uses a constraint language that
is more limited than FOVEs; essentially, it only allows for conjunctive constraints.
There are two reasons why it is important to be able to group variables with as much
flexibility as possible. First, it gives more flexibility to the user who has to specify the
graphical model itself. Second, during inference, it may become necessary to split up
groups into subgroups.
We cannot go into detail about the constraint based representation at this point (we will
do that later), but basically, during lifted inference, one may have a set of interchangeable
variables that could in principle be treated as one group, but are not because the system
cannot represent this group. It then needs to partition the group into smaller groups,
possibly up to the level of individuals. For instance, assume the groups in our above example
are {A1 , A2 , A5 , A6 , A7 } and {A3 , A4 , A8 }. Further assume that the constraint language is
such that sets of variables Ai are defined using constraints of the form {Ai |l  i  u}.
Neither group can be represented using one single constraint. For instance, the first group
consists of the union of {Ai |1  i  2} and {Ai |5  i  7}. Using this constraint language,
we get four groups of size 2, 3, 2 and 1 instead of two groups of size 5 and 3. As a result,
the computation actually performed will contain four exponentiated factors instead of two:
3
2 

X
X
X

1 (A5 , S)2a (T, A5 )
1 (A1 , S)2a (T, A1 ) 
T

A5

A1

1
2 

X
X

1 (A8 , S)2b (T, A8 )
1 (A3 , S)2b (T, A3 ) 
A8

A3

Generally, during lifted inference, groups may be split repeatedly. Unnecessary splits can
substantially hurt efficiency, as each one causes a duplication of work. Since the duplicated
work may include further splitting, the overall effect can be exponential in the number of
consecutive splits.
Ideally, the constraint language should have the property that for each group of variables,
there exists a constraint that represents exactly that group of variables. In that case, it is
401

fiTaghipour, Fierens, Davis, & Blockeel

never necessary to split a group into subgroups just because the group cannot be represented.
We call such a language extensionally complete. The main contribution of this article
is that it shows how to perform lifted variable elimination with an extensionally complete
constraint language. To this aim, first, a lifted variable elimination algorithm is defined
in a way that is independent of the constraint representation mechanism, by defining its
operators in terms of relational algebra expressions. We call this algorithm GC-FOVE. To
make GC-FOVE operational, some kind of constraint representation mechanism is of course
needed. Any constraint language L can be plugged into GC-FOVE, as long as it is closed
with respect to the relational algebra operators used by GC-FOVE. Second, we propose an
extensionally complete constraint representation language that is based on trees. Such a
language is necessarily closed with respect to the relational algebra operators, and therefore
suitable for GC-FOVE. The resulting system, GC-FOVETREES , can perform inference at
a higher level of granularity, and therefore more efficiently, than C-FOVE, which does not
use an extensionally complete constraint language. The effect of this is visible in particular
when evidence is given (which breaks symmetries and hence causes group splitting); in such
cases, GC-FOVE achieves exponential speedups compared to C-FOVE.
This ends our informal introduction to lifted variable elimination and the main contribution this articles makes to it. In the following sections, we first introduce formal notation
and terminology, then present our contributions in more detail.

3. Representation
Lifted inference exploits symmetries in a probabilistic model. Such symmetries often occur
in models that have repeating structures, such as plates (Getoor & Taskar, 2007, Ch. 7),
or, more generally, in probabilistic-logical models. Probabilistic-logical modeling languages
(also called probabilistic-relational languages) combine the representational and inferential
aspects of first-order logic with that of probability theory.
First-order logic languages refer to objects (possibly of various types) in some universe,
and properties of, or relationships between, these objects. Formulas in these languages can
express that some property holds for a particular object, or for an entire set of objects.
For instance, the fact that all humans are mortal could be written as x : Human(x) 
M ortal(x). Probabilistic-logical models can, in a similar way, express probabilistic knowledge about all objects. For instance, they could state that for each human, there is a prior
probability of 20% that he or she smokes: x : P (Smokes(x)|Human(x)) = 0.2. It is
this ability to make (probabilistic) statements about entire sets of objects that allows these
languages to compactly express symmetries in a model. Many different languages exist for
representing probabilistic-logical models (e.g., see Getoor & Taskar, 2007). We use a representation formalism based on undirected graphical models that is closely related to the one
used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch
et al., 2008).
The concepts introduced in this section have also been introduced in earlier work (de
Salvo Braz, 2007; Milch et al., 2008). Differences arise in terminology and notation as we
emphasize the constraint part.
402

fiDecoupling Lifted Variable Elimination from the Constraint Language

3.1 A Constraint-based Representation Formalism
An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001). Given a set of random variables X = {X1 , X2 , . . . , Xn },
a factor consists of a potential function  and an assignment of a random variable to each
of s inputs. For instance, the factorization f (X1 , X2 , X3 ) = (X1 , X2 )(X2 , X3 ) contains
two different factors (even if their potential functions are the same).
Likewise, in our probabilistic-logical representation framework, a model is a set of factors. The random variables they operate on are properties of, and relationships between,
objects in the universe. We now introduce some terminology to make this more concrete.
We assume familiarity with set and relational algebra (union , intersection , difference
\, set partitioning, selection C , projection X , attribute renaming , join ); see, for
instance, the work of Ramakrishnan and Gehrke (2003).
The term variable can be used in both the logical and probabilistic context. To
avoid confusion, we use the term logvar to refer to logical variables, and randvar to refer to
random variables. We write variable names in uppercase, and their values in lowercase. Sets
or sequences of logvars are written in boldface, sets or sequences of randvars in calligraphic;
their values are written in boldface lowercase.
The vocabulary of our representation includes a finite set of predicates and a finite set
of constants. A constant represents an object in our universe. A term is either a constant
or a logvar. A predicate P has an arity n and a finite range (range(P )); it is interpreted
as a mapping from n-tuples of objects (constants) to the range. An atom is of the form
P (t1 , t2 , . . . , tn ), where the ti are terms. A ground atom is an atom where all ti are constants.
A ground atom represents a random variable; this implies that its interpretation, an element
of range(P ), corresponds to the assignment of a value to the random variable. Hence, the
range of a predicate corresponds to the range of the random variables it can represent, and
is not limited to {true, f alse} as in logic.
Logvars have a finite domain, which is a set of constants. The domain of a logvar
X is denoted D(X). A constraint is a relation defined on a set of logvars, i.e., it is a
pair (X, CX ), where X = (X1 , X2 , . . . , Xn ) is a tuple of logvars, and CX is a subset of
D(X) = i D(Xi ) (Dechter, 2003). Hence, CX is a set, whose elements (tuples) indicate the
allowed combinations of value assignments for the variables in X. For ease of exposition, we
identify a constraint with its relation CX , and write C instead of CX when X is apparent
from the context. We assume an implicit ordering of values in CX s tuples according to the
order of logvars in X. For instance with X = (X1 , X2 ), the constraint CX = {(a, b), (c, d)}
indicates that there are two possibilities: either X1 = a and X2 = b, or X1 = c and X2 = d.
A constraint that contains only one tuple is called singleton.
A constraint may be defined extensionally, by listing the tuples that satisfy it, or intensionally, by means of some logical condition, expressed in a constraint language. We call a
constraint language L extensionally complete if it can express any relation over logvars X,
i.e., for any subset of D(X), there is a constraint CX  L whose extension is exactly that
subset.
A constrained atom is of the form P (X)|C, where P (X) is an atom and C is a constraint
on X. A constrained atom P (X)|C represents a set of ground atoms {P (x)|x  C}, and
hence a set of randvars. For consistency with the literature, we call such a constrained atom
403

fiTaghipour, Fierens, Davis, & Blockeel

a parametrized randvar (PRV), and use calligraphic notation to denote it. Given a PRV V,
we use RV (V) to denote the set of randvars it represents; we also say these randvars are
covered by V.
A valuation of a randvar (set of randvars) is an assignment of a value to the randvar
(an assignment of values to all randvars in the set).
Example 1. The PRV V = Smokes(X)|C, with C = {x1 , . . . , xn }, represents n randvars
{Smokes(x1 ), . . . Smokes(xn )}.

A factor f = f (Af ) consists of a sequence of randvars Af = (A1 , . . . , An ) and a
potential function f : ni=1 range(Ai )  R+ . The product of two factors, f1  f2 , is
defined as follows. Factor f = (A) is the product of f1 = 1 (A1 ) and f2 = 2 (A2 ) if
(a) = ai for
and only if A = A1  A2 and for all a  D(A): (a) = 1 (a1 )2 (a2 ) with AiQ
i = 1, 2. That is, a assigns to each randvar in Ai the same value as ai . We use
to denote
multiplication of multiple factors. Multiplying a factor by a scalar c means replacing its
potential  by  : x 7 c  (x).
An undirected S
model is a set of factors F . It represents
a probability distribution PF
1 Q

(A
on randvars A = P
A
as
follows:
P
(A)
=
F
f ), with Z a normalization
f
f F f
f F
Z
constant such that arange(A) PF (a) = 1.
A parametric factor or parfactor has the form (A)|C, with A = {Ai }ni=1 a sequence
of atoms,  a potential function on A, and C a constraint on the logvars appearing in A.1
The set of logvars occurring in A is denoted logvar(A); the set of logvars in C is denoted
logvar(C). A factor (A ) is a grounding of a parfactor (A) if A can be obtained by
instantiating X = logvar(A) with some x  C. The set of groundings of a parfactor g is
denoted gr(g).
Example 2. Parfactor g1 = 1 (Smokes(X))|X  {x1 , . . . , xn } represents the set of factors

gr(g1 ) = {1 (Smokes(x1 )), . . . , 1 (Smokes(xn ))}.
A set of parfactors G is a compact way of defining a set of factors
Q F = {f |f  gr(g)g 
G} and the corresponding probability distribution PG (A) = Z1 f F f (Af ).
3.2 Counting Formulas

Milch et al. (2008) introduced the idea of counting formulas and (parametrized) counting
randvars.
A counting formula is a syntactic construct of the form #Xi C [P (X)], where Xi  X is
called the counted logvar.
A grounded counting formula is a counting formula in which all arguments of the atom
P (X), except for the counted logvar, are constants. It defines a counting randvar (CRV),
the meaning of which is as follows. First, we define the set of randvars it covers as
RV (#XC [P (X)]) = RV (P (X)|X  C). The value of the CRV is determined by the values
of the randvars it covers. More specifically, it is a histogram that indicates, given a valuation of RV (P (X)|X  C), how many different values of X occur for each r  range(P ).
Thus, its value is of the form {(r1 , n1 ), (r2 , n2 ), . . . , (rk , nk )}, with ri  range(P ) and ni the
1. We use the definition of Kisynski and Poole (2009a) for parfactors, as it allows us to simplify the notation.

404

fiDecoupling Lifted Variable Elimination from the Constraint Language

corresponding count. Given a histogram h, we will also write h(v) for the count of v in h.
Note that the range of a CRV, i.e., the set of all possible histograms it can take as a value,
is determined by k = |range(P )| and |C|.
Example 3. #X{x1 ,x2 ,x3 } [P (X, y, z)] is a grounded counting formula. It covers the rand-

vars P (x1 , y, z), P (x2 , y, z) and P (x3 , y, z). It defines a CRV, the value of which is determined by the values of these three randvars; if P (x1 , y, z) = true, P (x2 , y, z) = f alse and
P (x3 , y, z) = true, the CRV takes the value {(true, 2), (f alse, 1)}.
The concept of a CRV is somewhat complicated. A CRV behaves like a regular randvar
in some ways, but not all. It is a construct that can occur as an argument of a factor, like
regular randvars, but in that role it actually stands for a set of randvars, all of which are
arguments of the factor. A factor of the form  (   , #XC [P (X)],    ) is equivalent to a
factor of the form (   , P (X1 ), P (X2 ), . . . , P (Xk ),    ), with P (Xi ) all the instantiations
of X obtainable by instantiating X with a value from C, and with  returning for any
valuation of the P (Xi ) the value that  returns for the corresponding histogram.
Example 4. The factor  (#X{x1 ,x2 ,x3 } [P (X, y, z)]) is equivalent to a factor

(P (x1 , y, z), P (x2 , y, z), P (x3 , y, z)). If  ({(true, 2), (f alse, 1)}) = 0.3, this implies that
(f alse, true, true) = (true, f alse, true) = (true, true, f alse) = 0.3.
As illustrated in Section 2.4, counting formulas are useful for capturing symmetries
within a potential function. Recall the workshop example. Whether a person attends
a workshop depends on its topic, and this dependence is the same for each person. We
can represent this with a single parfactor (T, A(X))|X  {x1 , . . . , xn } that represents
n ground factors. Eliminating T requires multiplying these n factors into a single factor
 (T, A(x1 ), A(x2 ), . . . , A(xn )) before summing out T . The potential function  is highdimensional, so a tabular representation for it would be very costly. However, it contains a
certain symmetry:  depends only on how many times each possible value for A(xi ) occurs,
not on where exactly these occur. By representing the factor using a potential function 
that has only two arguments, T and the CRV #X{x1 ,...,xn} [A(X)], it can be represented
more concisely, and computed more efficiently. For instance, to sum out A(X), we do not
need to enumerate all possible (2n ) value combinations of the A(xi ) and sum the corresponding  (T, A(x1 ), . . . , A(xn )), we just need to enumerate all possible (n + 1) values for the
histogram of #X{x1 ,...,xn } [A(X)] and sum the corresponding  (T, #X{x1 ,...,xn } [A(X)]),
each multiplied by its multiplicity.
Note the complementarity between PRVs and CRVs. While the randvars covered by a
PRV occur in different factors, the randvars covered by a CRV occur in one and the same
factor. Thus, PRVs impose a symmetry among different factors, whereas CRVs impose a
symmetry within a single factor.
A parametrized counting randvar (PCRV) is of the form #X [P (X)] |CX . In this notation
we write the constraint on the counted logvar X as part of the constraint CX on all variables
in X. Similar to the way in which a PRV defines a set of randvars through its groundings,
a PCRV defines a set of CRVs through its groundings of all variables in X \ {X}.
Example 5. #Y [F riend(X, Y )] |C represents a set of CRVs, one for each x  X (C),
indicating the number of friends x has. If C = D(X)  D(Y ) with D(X) = D(Y ) =
405

fiTaghipour, Fierens, Davis, & Blockeel

{ann, bob, carl}, we might for instance have #Y [F riend(ann, Y )]|C = {(true, 1), (f alse, 2)}
(Ann has one friend, and two people are not friends with her).
Some definitions from the previous section need to be extended slightly in order to
accommodate PCRVs. First, because CRVs are not regular randvars, they are not included
in the set of randvars covered by the PRCV; that is, RV (#Xi [P (X)]|C) = RV (P (X)|C).
Second, since a counting formula binds the counted logvar (it is no longer a parameter of
the resulting PCRV), we define logvar(#Xi [P (X)]) = X \ {Xi }. Thus, generally, logvar(A)
refers to all the logvars occurring in A, excluding the counted logvars. Note that logvar(C)
remains unchanged: it refers to all logvars in C, whether they appear as counted or not.
We end this section with two definitions that will be useful later on.
Definition 1 (Count function) Given a constraint CX , for any Y  X and Z  XY,
the function CountY|Z : CX  N is defined as follows:
CountY|Z (t) = |Y (Z=Z (t) (CX ))|
That is, for any tuple t, this function tells us how many values for Y co-occur with ts value
for Z in the constraint. We define CountY|Z (t) = 1 when Y = .
Definition 2 (Count-normalized constraint) For any constraint CX , Y  X and Z 
X  Y, Y is count-normalized w.r.t. Z in CX if and only if
n  N : t  CX : CountY|Z (t) = n.
When such an n exists, we call it the conditional count of Y given Z in CX , and denote it
CountY|Z (CX ).
Example 6. Let X be {P, C} and let the constraint CX be (P, C)  {(ann, eric), (bob, eric),

(carl, f inn), (debbie, f inn), (carl, gemma), (debbie, gemma)}. Suppose CX indicates the parent relationship: Ann is a parent of Eric, etc. Then {P } is count-normalized w.r.t. {C}
because all children (i.e., all instantiations of C in CX : Eric, Finn and Gemma) have two parents according to CX , or formally, for all tuples t  CX it holds that Count{P }|{C} (t) = 2.
Conversely, {C} is not count-normalized w.r.t. {P } because not all parents have equally
many children. For instance, Count{C}|{P } ((ann, eric)) = 1 (Ann has 1 child), but
Count{C}|{P } ((carl, f inn)) = 2 (Carl has 2 children).

4. The GC-FOVE Algorithm: Outline
We now turn to the problem of performing lifted inference on models specified using the
above representation. The algorithm we introduce for this is called GC-FOVE (for Generalized C-FOVE). At a high level, it is similar to C-FOVE (Milch et al., 2008), the current
state-of-the-art system in lifted variable elimination, but it differs in the definition and
implementation of its operators.
Recall how standard variable elimination works. It eliminates randvars one by one, in a
particular order called the elimination order. Elimination consist of multiplying all factors
the randvar occurs in into one factor, then summing out the randvar.
406

fiDecoupling Lifted Variable Elimination from the Constraint Language

Similarly, GC-FOVE visits PRVs (as opposed to individual randvars) in a particular
order. Ideally, it eliminates each PRV by multiplying the parfactors in which it occurs
into one parfactor, then summing out the PRV, using the lifted multiplication and lifted
summing-out operators. However, these operators are not always immediately applicable:
it may be necessary to refine the involved parfactors and PRVs to make them so. This is
done using other operators, which we call enabling operators.2
A high-level description of GC-FOVE is shown in Algorithm 1. Like C-FOVE, it makes
use of a number of operators, and repeatedly selects and performs one of the possible
operators on one or more parfactors. It uses the same greedy heuristic as C-FOVE for this
selection, choosing the operation with the minimum cost, where the cost of each operation
is defined as the total size (number of rows in tabular form) of all the potentials it creates.
The main difference between C-FOVE and GC-FOVE is in the operators used. Four of
GC-FOVEs operators (multiply, sum-out, count-convert and ground-logvar) are
a straightforward generalization of a similar operator in C-FOVE, the difference being that
we provide definitions that work for any constraint representation language that is closed
under relational algebra, instead of definitions that are specific for the constraint language
used by C-FOVE. Three other operators (expand, count-normalize and split) also have
counterparts in C-FOVE, but need to be redefined more substantially because they directly
concern constraint manipulation. The lifted absorption operator (absorb) is completely
new.
GC-FOVE in itself does not specify a particular constraint language. In practice, constraints have to be represented one way or another, so some constraint representation mechanism has to be plugged in. In this article, we propose a tree-based representation mechanism
for constraints. Important advantages of this mechanism are that, on the one hand, any
extensional set can be represented by these trees, and on the other hand, constraints can
still be manipulated efficiently.
The generalization of the operators, the new absorption operator, and the tree-based
constraint language are the main contributions of this paper. Together, they greatly improve
the efficiency of inference, as will be clear from the experimental section. Before describing the operators in detail, we illustrate the importance of using an expressive constraint
language.
4.1 Constraint Language
In C-FOVE, a constraint is a set of pairwise (in)equalities between a single logvar and a
constant, or between two logvars. Thus, in a single parfactor, C-FOVE can represent, for instance, F riend(X, Y )|X 6= ann, but not F riend(X, Y )|(X, Y )  {(ann, bob), (bob, carl)}).
Table 1 provides some more examples of PRVs that C-FOVE can/cannot represent, and
Figure 3 illustrates this visually. Basically, C-FOVE can only use conjunctive constraints,
not disjunctive ones, and C-FOVEs operators are defined to operate directly on this representation. GC-FOVE, on the other hand, allows a constraint to be any relation on the
logvars, and can therefore handle all these PRVs. Because it has no restrictions whatsoever
regarding the constraints it can handle, it can maximally exploit opportunities for lifting.
2. Technically speaking, multiplication is also an enabling operator as summing-out can only be applied
after multiplication.

407

fiTaghipour, Fierens, Davis, & Blockeel

GC-FOVE
Inputs:
G: a model
Q: the query randvar
Algorithm:
while G contains other randvars than Q:
if there is a PRV V that can be eliminated by lifted absorption
G  apply operator absorb to eliminate V in G
else if there is a PRV V that can be eliminated by lifted summing-out
G  apply sum-out to eliminate V in G
else apply an enabling operator (one of multiply, count-convert, expand,
count-normalize, split or ground-logvar) on some parfactors in G
end while
return G
Algorithm 1: Outline of the GC-FOVE algorithm.
PRV
F riend(X, Y )
F riend(ann, Y )
F riend(X, Y )|X 6= ann
F riend(X, Y )|X  {ann, bob}
F riend(X, Y )|(X, Y )  {(ann, bob), (bob, carl)})

C-FOVE
yes
yes
yes
yes
no

GC-FOVE
yes
yes
yes
yes
yes

Table 1: Examples of parametrized random variables that can / cannot be represented using
a single constraint by C-FOVE. Though the fourth constraint (yes ) is disjunctive,
C-FOVE can represent it using a conjunction of inequality constraints. This is not
the case for the fifth constraint. GC-FOVE can represent all constraints.

The expressiveness of the constraint representation language, and the way the constraints are handled by the operators, are crucial to the efficiency of lifted variable elimination. The reason is that variables continuously need to be re-grouped (i.e., constraints need
to be rewritten) during inference. For instance, we can multiply 1 (P (X))|{x1 , x2 , x3 } and
2 (P (X))|{x1 , x2 , x3 } directly, resulting in a parfactor of the form 12 (P (X))|{x1 , x2 , x3 },
but we cannot multiply 1 (P (X))|{x1 , x2 , x3 } and 2 (P (X))|{x2 , x3 , x4 , x5 } into a single
parfactor because their PRVs do not match. The solution is to split constraints and parfactors so that matching parfactors arise. In this particular case, a model with three parfactors
arises: 1 (P (x1 )), 12 (P (X))|{x2 , x3 } and 2 (P (X))|{x4 , x5 }. GC-FOVEs operations result in this model. C-FOVE, however, when splitting constraints, separates off one tuple at a
time (splitting based on substitution, Milch et al., 2008), which here results in four parfactors: 1 (P (x1 )); 12 (P (X))|X 6= x1 , X 6= x4 , X 6= x5 ; 2 (P (x4 )); and 2 (P (x5 )) (assuming
the domain of X is {x1 , x2 , . . . , x5 }). In this case, C-FOVE could in fact represent the separate factors 2 (P (x4 )) and 2 (P (x5 )) as one parfactor 2 (P (X))|X 6= x1 , X 6= x2 , X 6= x3 ,
but it does not do so (only the intersection of two constraints is kept on the lifted level),
408

fiDecoupling Lifted Variable Elimination from the Constraint Language

a
b
c
d
e
f

a
b
c
d
e
f
a b c d e f

a
b
c
d
e
f

a
b
c
d
e
f
a b c d e f

a
b
c
d
e
f
a b c d e f

a b c d e f
a
b
c
d
e
f

a b c d e f

a b c d e f

Figure 3: In each schema, the gray area indicates a PRV of the form F riend(X, Y )|CXY
(with a standing for ann, b for bob, etc.) C-FOVE can only handle PRVs that
can be defined by conjunctive constraints; this includes the top three schemas,
but not the bottom ones. GC-FOVE can handle all PRVs.

and in general, for non-unary predicates, this is not possible, as Table 1 shows. Because of
its restricted constraint language, C-FOVE often has to create finer-grained partitions than
necessary. GC-FOVE, because it uses an extensionally complete constraint language, does
not suffer from this problem.
4.2 Lifted Absorption
Absorption (van der Gaag, 1996) is an additional operator in VE that is known to increase
efficiency. It consists of removing a random variable from a model when its valuation is
known, and rewriting the model into an equivalent one that does not contain the variable.
C-FOVE, like its predecessors, does not use absorption, and including it might in fact have
detrimental effects due to breaking of symmetries. GC-FOVEs extensionally complete
constraint language, however, not only makes it possible to use absorption more effectively,
it even allows for lifting it.
4.3 Summary of Contributions
We are now at a point where we can summarize the contributions of this work more precisely.
1. We present the first description of lifted variable elimination that decouples the lifted
inference algorithm from the constraint representation it uses. This is done by taking
the C-FOVE algorithm and redefining its operators so that they become independent
from the underlying constraint mechanism. This is achieved by defining the operators
409

fiTaghipour, Fierens, Davis, & Blockeel

in terms of relational algebra operators. This redefinition generalizes the operators
and clarifies on a higher level how they work.
2. We present a mechanism for representing constraints that is extensionally complete.
It is closed under the relational algebra operators, and allows for executing them
efficiently. In itself, this is a minor contribution, but it is necessary in order to obtain
an operational system.
3. We present a new operator, called lifted absorption.
4. We experimentally demonstrate the practical impact of the above contributions.
5. We contribute the software itself.
Contributions 1 and 3 (our main contributions) are the subject of Section 5. Contribution 2 is detailed in Section 6, and Contribution 4 in Section 7. Contribution 5 is at
http://dtai.cs.kuleuven.be/ml/systems/gc-fove.

5. GC-FOVEs Operators
This section provides detailed information on GC-FOVEs operators. These can conceptually be split into two categories: operators that manipulate potential functions, and
operators that refine the model so that the first type of operators can be applied. We
will start with three operators that belong to the first category: lifted multiplication, lifted
summing-out and counting conversion. These can be seen as generalized versions of the
corresponding C-FOVE operators; algorithmically, they are similar. Next, we discuss splitting, shattering, expansion, and count normalization. Because they operate specifically on
the constraints, these differ more strongly from C-FOVEs operators. We will systematically compare them to the latter, showing each time that C-FOVEs constraint language
and operators force it to create more fine-grained models than necessary, while GC-FOVE,
because of its extensionally complete constraint language, can always avoid this: whatever
the set of interchangeable randvars is, this set can be represented by one constraint. Finally,
we discuss lifted absorption, which is completely new, and grounding, which is again similar
to its C-FOVE counterpart.
In the following, G refers to a model (i.e., a set of parfactors), and G1  G2 means that
models G1 and G2 define the same probability distribution.
5.1 Lifted Multiplication
The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch
et al., 2008). Figure 4 illustrates this for two parfactors g1 = 1 (S(X))|C and g2 =
2 (S(X), A(X))|C, where C = (X  {x1 , . . . , xn }). Lifted multiplication is equivalent
to n multiplications on the ground level.
The above illustration is deceptively simple, for several reasons. First, the naming of
the logvars suggests that logvar X in g1 corresponds to X in g2 . In fact, g2 could have
multiple logvars, with different names. An alignment between the parfactors is necessary,
showing how logvars in different parfactors correspond to each other (de Salvo Braz, 2007).
410

fiDecoupling Lifted Variable Elimination from the Constraint Language

1 (S(X))




1 (S(x1 ))

2 (S(X), A(X))
2 (S(x1 ), A(x1 ))



S(x1 )

1

3 (S(x1 ), A(x1 ))
..
.

Multiplications



2 (S(xn ), A(xn ))
2

3 (S(X), A(X))

Ground

..
.

..
.
1 (S(xn ))

Lifted
Multiplication



3 (S(xn ), A(xn ))

A(x1 )

S(x1 )

3

A(x1 )

3

A(xn )



..
.
1

Ground
Multiplications

..
.


S(xn )

2

A(xn )

S(xn )



Figure 4: Lifted Multiplication with a 1:1 alignment between parfactors. The equivalent of
the lifted operation (top), is shown at the level of ground factors (middle), and
also in terms of factor graphs (bottom).  denotes (par)factor multiplication.

The alignment must constrain the aligned logvars to exactly the same values in g1 and
g2 (otherwise, they cannot give identical PRVs in both parfactors). We formalize this as
follows.
Definition 3 (substitution) A substitution  = {X1  t1 , . . . , Xn  tn } = {X 
t} maps each logvar Xi to a term ti , which can be a constant or a logvar. When all ti
are constants,  is called a grounding substitution, and when all are different logvars, a
renaming substitution. Applying a substitution  to an expression  means replacing each
occurrence of Xi in  with ti ; the result is denoted .
Definition 4 (alignment) An alignment  between two parfactors g = (A)|C and g  =
 (A )|C  is a one-to-one substitution {X  X }, with X  logvar(A) and X  logvar(A ),
such that  (X (C)) = X (C  ) (with  the attribute renaming operator).
An alignment tells the multiplication operator that two atoms in two different parfactors
represent the same PRV, so it suffices to include it in the resulting parfactor only once.
Including it twice is not wrong, but less efficient: some structure in the parfactor is then
lost. For this reason, it is useful to look for maximal alignments which map as many
PRVs to each other as possible.
Example 7. Consider g1 = 1 (S(X), F (X, Y ))|CX,Y and g2 = 2 (S(X  ), F (X  , Y  ))|CX  ,Y 


with CX,Y = CX  ,Y  = {xi }n1  {yj }m
1 . Using the maximal alignment {X  X , Y 

411

fiTaghipour, Fierens, Davis, & Blockeel

Y  )}, we get the product parfactor 3 (S(X), F (X, Y ))|CX,Y . This alignment establishes
a 1:1 association between each ground factor 1 (S(xi ), F (xi , yj )) and the corresponding
2 (S(xi ), F (xi , yj )). If, however, we multiply g1 and g2 with the alignment {X  X  },
m
the result is a parfactor 3 (S(X), F (X, Y ), F (X, Y  ))|(X, Y, Y  )  {xi }n1  {yj }m
1  {yk }1 ,
which for each xi unnecessarily multiplies each factor 1 (S(xi ), F (xi , yj )) with all factors
2 (S(xi ), F (xi , yk )), k = 1, . . . , m. In other words, it unnecessarily creates a direct dependency between all pairs of randvars F (xi , yj ), F (xi , yk ).
A second complication is that a single randvar may participate in multiple factors within
a certain parfactor, and the number of such factors it appears in may differ across parfactors. Consider parfactors g1 = 1 (S(X))|X  {xi }n1 and g2 = 2 (S(X), F (X, Y ))|(X, Y ) 
{xi }n1 {yi }m
1 . For each xi , 1 (S(xi )) shares randvar S(xi ) with m factors 2 (S(xi ), F (xi , yj )),
j = 1, . . . , m. Multiplication should result in a single parfactor 3 (S(xi ), F (xi , Y ))|Y 
{yi }m
1 that covers m factors 3 (S(xi ), F (xi , yj )), and is equivalent to the product of one
factor 1 (S(xi )) and m factorsQ2 (S(xi ), F (xi , yj )). This means we must find a 3 such
1/m  (v, w). The
that v, w : 3 (v, w)m = 1 (v) m
2
i=1 2 (v, w). This gives 3 (v, w) = 1 (v)
exponentiation of 1 to the power 1/m is called scaling. The result of this multiplication
for a single xi is the same regardless of xi , so finally, the product of the parfactors g1 and
g2 will be the parfactor
3 (S(X), F (X, Y )) = 1 (S(X))1/m  2 (S(X), F (X, Y )) | (X, Y )  {xi }n1  {yj }m
1 .
Figure 5 illustrates this multiplication graphically.
An alignment between parfactors is called 1 : 1 if all non-counted logvars in the parfactors
are mapped to each other, and is called m:n otherwise. Multiplication based on an m:n
alignment involves scaling, and requires that the non-aligned logvars be count-normalized
(Definition 2, p. 406) with respect to the aligned logvars in the constraints (otherwise there
is no single scaling exponent that is valid for the whole parfactor).
Operator 1 formally defines the lifted multiplication. Note that this definition does not
assume any specific format for the constraints.
5.2 Lifted Summing-Out
Once a PRV occurs in only one parfactor, it can be summed out from that parfactor (Milch
et al., 2008). We begin with an example of lifted summing-out, which will help motivate
the formal definition of the operator.
Example 8. Consider parfactor g = (S(X), F (X, Y ))|C, in which C = {(xi , yi,j ) : i 

{1, . . . , n}, j  {1, . . . , m}} (Figure 6). Note that Y is count-normalized w.r.t X in C. Assume we want to sum out randvars F (xi , yi,j )  RV (F (X, Y )|C) on the ground level. Each
randvar F (xi , yi,j ) appears in exactly one ground factor (S(xi ), F (xi , yi,j )) (see Figure 6
(middle)). We can therefore sum out each
P F (xi , yi,j ) from its factor independently from the
others, obtaining a factor  (S(xi )) = F (xi ,yi,j ) (S(xi ), F (xi , yi,j )). Since the m ground
factors (S(xi ), F (xi , yi,j )) have the same potential , summing out their second argument
always results in the same potential  , so we can compute  just once and, instead of storing m copies of the resulting factor  (S(xi )), store a single factor  (S(xi )) =  (S(xi ))m .
In the end, we obtain n such factors, one for each S(xi ), i = 1, . . . , n. We can represent
412

fiDecoupling Lifted Variable Elimination from the Constraint Language

1 (S(X))

1 (S(x1 ))





..
.

2 (S(X), F (X, Y ))

i=1

Lifted

1 (S(X))1/m  2 (S(X), F (X, Y ))



1/m


 2 (S(x1 ), F (x1 , y1 ))
 1 (S(x1 ))
Scaling 1
..
..







.
.




2 (S(x1 ), F (x1 , ym ))
1 (S(x1 ))1/m





1/m


 1 (S(xn ))
 2 (S(xn ), F (xn , y1 ))
..
Scaling 1
..
  .
.



1 (S(xn ))1/m
2 (S(xn ), F (xn , ym ))

2
S(x1 )

..
.
2

1/m

1

F (x1 , y1 )
Scaling 1


F (x1 , ym )

Multiplications





3 (S(X), F (X, Y ))



 3 (S(x1 ), F (x1 , y1 ))
..
.


3 (S(x1 ), F (x1 , ym ))

2 (S(x1 ), F (x1 , y1 ))
..
.
2 (S(x1 ), F (x1 , ym ))



Ground

..
.
1/m
1


S(x1 )






2 (S(xn ), F (xn , y1 ))
 3 (S(xn ), F (xn , y1 ))
..
..
.
.


3 (S(xn ), F (xn , ym ))
2 (S(xn ), F (xn , ym ))




2
..
.
2

F (x1 , y1 )
S(x1 )
F (x1 , ym )

..
.

..
.
2
1

!m

Multiplications

1 (S(xn ))

1

Scaling 1



S(xn )

..
.
2

Scaling 1

F (xn , ym )

..
.
3

F (x1 , ym )

3

F (xn , y1 )

..
.
3

F (xn , ym )

Ground
Multiplications
1/m



F (x1 , y1 )



1

F (xn , y1 )

3

..
.
1/m
1


S(xn )



2
..
.
2

F (xn , y1 )
S(xn )
F (xn , ym )

Figure 5: Lifted Multiplication with a m:n alignment between parfactors. The equivalent
of the lifted operation (top), is shown at the level of ground factors (middle), and
also in terms of factor graphs (bottom).

this result using a single parfactor g  =  (S(X))|C  , with C  = {x1 , . . . , xn } = X (C).
Lifted summing-out directly computes g  from g in one operation. Note that to have a
single exponent for all  , Y must be count-normalized w.r.t. X in C.
Like its C-FOVE counterpart, our lifted summing-out operator requires a one-to-one
mapping between summed-out randvars and factors; that is, each summed-out randvar
appears in exactly one factor, and all these factors are different. This is guaranteed when
the eliminated atom contains all the logvars of the parfactor, since there is a different
ground factor for each instantiation of the logvars. Further, lifted summing-out may result
in identical factors on the ground level, which is exploited by computing one factor and
exponentiating. This is the case when there is a logvar that occurs only in the eliminated
atom, but not in the other atoms (such as Y in F (X, Y ) in the above example).
As already illustrated in Section 2.4, counting randvars require special attention in
lifted summing-out. A formula like (#X [P (X)])|X  {x1 , . . . , xk } is really a shorthand
for a factor (P (x1 ), P (x2 ), . . . , P (xk )) whose value depends only on how many arguments
take particular values. In principle, we need to sum out over all combinations of values of
P (Xi ). We can replace this by summing out over all values of #X [P (X)], on the condition
that we take the multiplicities of the latter into account. The multiplicity of a histogram
413

fiTaghipour, Fierens, Davis, & Blockeel

Operator multiply
Inputs:
(1) g1 = 1 (A1 )|C1 : a parfactor in G
(2) g2 = 2 (A2 )|C2 : a parfactor in G
(3)  = {X1  X2 }: an alignment between g1 and g2
Preconditions:
(1) for i = 1, 2: Yi = logvar(Ai ) \ Xi is count-normalized w.r.t. Xi in Ci
Output: (A)|C, with
(1) C =  (C1 )  C2 .
(2) A = A1   A2 , and
(3) for each valuation a of A, with a1 = A1  (a) and a2 = A2 (a) :
1/r
1/r
(a) = 1 2 (a1 )  2 1 (a2 ), with ri = CountYi |Xi (Ci )
Postcondition: G  G \ {g1 , g2 }  {multiply(g1 , g2 , )}
Operator 1: Lifted multiplication. The definition assumes, without loss of generality, that
the logvars in the parfactors are standardized apart, i.e., the two parfactors do not share
variable names (this can always be achieved by renaming logvars).
h = {(r1 , n1 ), (r2 , n2 ), . . . , (rk , nk )} is a multinomial coefficient, defined as
Mul(h) = Qk

n!

i=1 ni !

.

As multiplicities should only be taken into account for (P)CRVs, never for regular PRVs,
we define for each PRV A and for each value v  range(A): Mul(A, v) = 1 if A is a regular
PRV, and Mul(A, v) = Mul(v) if A is a PCRV. This Mul function is identical to Milch
et al.s (2008) num-assign.
With all this in mind, the formal definition of the lifted summing-out in Operator 2
is mostly self-explanatory. Precondition (1) ensures that all randvars in the summed-out
P(C)RV occur exclusively in this parfactor. Precondition (2) ensures that each summed
out randvar occurs in exactly one, separate, ground factor. Precondition (3) ensures that
logvars occurring exclusively in the eliminated PRV are count-normalized with respect to
the other logvars in that PRV, so that there is one unique exponent for exponentiation.
5.3 Counting Conversion
Counting randvars may be present in the original model, but they can also be introduced
into parfactors by an operation called counting conversion (Milch et al., 2008) (see also
Section 2.4). To see why this is useful, consider a parfactor g = (S(X), F (X, Y ))|C, with
C = {xi }ni=1  {yj }m
j=1 , and assume we want to eliminate S(X)|C. To do that, we first need
to make sure each S(xi ) occurs in only one factor. On the ground level, this can be achieved
for a given S(xi ) by multiplying all factors (S(xi ), F (xQ
i , yj )) in which it occurs. This results
in a single factor  (S(xi ), F (xi , y1 ), . . . , F (xi , ym )) = j (S(xi ), F (xi , yj )) (see Figure 7).
This is a high-dimensional factor, but because it equals a product of identical potentials ,
its F (xi , yj ) arguments are mutually interchangeable: all that matters is how often values
v1 , v2 , . . . occur among them, not where they occur. This is exactly the kind of symmetry
414

fiDecoupling Lifted Variable Elimination from the Constraint Language

(S(X), F (X, Y ))
(S(x1 ), F (x1 , y1 ))
..
.
(S(x1 ), F (x1 , ym ))

..
.
(S(xn ), F (xn , y1 ))
..
.
(S(xn ), F (xn , ym ))

S(x1 )

P

RV (F (X,Y ))


P

F (x ,y )

1
1

..
.P
F (x ,ym )

1



P

F (xn ,y )

1
..
.

P

F (xn ,ym )



..
.
F (x1 , ym ) P

 (S(X))

 

  (S(x1 ))
..
.

 
 (S(x1 ))





 

  (S(xn ))
..
.

 
 (S(xn ))





S(x1 )

F (x ,ym )

1



F (xn , y1 ) P
..
.
F (xn , ym ) P

..
.

..
.
 (S(xn ))m




S(x1 )

( )m

S(xn )

( )m

..
.


F (xn ,y1 )


..
.


 (S(X))m

 (S(x1 ))m

..
.


..
.





Exponentiate





1 ,y1 )

..
.
S(xn )

i=1

F (x1 , y1 ) PF (x


..
.


!m

S(xn )

F (xn ,ym )

..
.




Figure 6: Lifted summing-out. The equivalent of the lifted operation (top), is shown at the
level of ground factors (middle), and also in terms of factor graphs (bottom).

that CRVs aim to exploit. The factor  (S(xi ), F (xi , y1 ), . . . , F (xi , ym )) can therefore be
replaced by a two-dimensional  (S(xi ), h) with h a histogram that indicates how often each
possible value in the range of F (xi , yj ) occurs. Thus, by introducing a CRV, we can define
a two-dimensional  with that CRV as an argument, as opposed to the high-dimensional
 . As argued in Section 2.4, this reduces the size of the potential function, and hence
computational complexity, exponentially.
In many situations where lifted elimination cannot immediately be applied, counting
conversion makes it applicable. The conditions of the sum-out operator (Section 5.2) state
that an atom Ai can only be eliminated from a parfactor g if Ai has all the logvars in
g. When an atom has fewer logvars than the parfactor, counting conversion modifies the
parfactor by replacing another atom Aj by a counting formula, which removes this counted
logvar from logvar(A). For instance, in the above example, S(X) does not have the logvar
Y in g = (S(X), F (X, Y ))|C and cannot be eliminated from the original parfactor g, but
a counting conversion on Y replaces F (X, Y ) with #Y [F (X, Y )], allowing us to sum out
S(X) from the new parfactor g  = (S(X), #Y [F (X, Y )])|C.
415

fiTaghipour, Fierens, Davis, & Blockeel

Operator sum-out
Inputs:
(1) g = (A)|C: a parfactor in G
(2) Ai : an atom in A, to be summed out from g1
Preconditions
(1) For all PRVs V, other than Ai |C, in model G: RV (V)  RV (Ai |C) = 
(2) Ai contains all the logvars X  logvar(A) for which X (C) is not singleton.
(3) Xexcl = logvar(Ai ) \ logvar(A \ Ai ) is count-normalized w.r.t.
Xcom = logvar(Ai )  logvar(A \ Ai ) in C
Output:  (A )|C  , such that
(1) A = A \ Ai
(2) C  = X com (C)
(3) for each assignment a = (. . . , ai1 , ai+1 , . . . ) to A ,
P

 (. . . , ai1 , ai+1 , . . . ) = ai range(Ai ) Mul(Ai , ai ) (. . . , ai1 , ai , ai+1 , . . . )r
with r = CountXexcl |Xcom (C)
Postcondition: PG\{g}{sum-out(g,Ai )} = RV (Ai |C) PG
Operator 2: The lifted summing-out operator.
Operator 3 formally defines counting conversion. It is mostly self-explanatory, apart
from the preconditions. Precondition 1 makes sure that counting conversion, on the ground
level, corresponds to multiplying factors that only differ in one randvar (i.e., are the same up
to their instantiation of the counted logvar). Precondition 2 guarantees that the resulting
histograms have the same range. Precondition 3 is more difficult to explain. It imposes
a kind of independence between the logvar to be counted and already occurring counted
logvars. Though not explicitly mentioned there, this precondition is also required for CFOVEs counting operation; it implies that no inequality constraint should exist between
X and any counted logvar X # . A similar condition for FOVEs counting elimination is
mentioned by de Salvo Braz (2007).
To see why precondition 3 is necessary, consider the parfactor g = (S(X), #Y [A(Y )])
|(X, Y )  {(x1 , y2 ), (x1 , y3 ), (x2 , y1 ), (x2 , y3 ), (x3 , y1 ), (x3 , y2 )}, which does not satisfy it.
This parfactor represents three factors of the form (S(xi ), #Y [A(Y )])|Y  {y1 , y2 , y3 }\{yi },
which contribute to the joint distribution with the product
(S(x1 ), #Y {y2 ,y3 } [A(Y )])  (S(x2 ), #Y {y1 ,y3 } [A(Y )])  (S(x3 ), #Y {y1 ,y2 } [A(Y )]).
Counting conversion on logvar X turns g into a factor of the form
 (#X [S(X)], #Y [A(Y )])
that should be equivalent. Note that  depends only on #X [S(X)] and #Y [A(Y )].
Now consider valuations V1 : [S(x1 ), S(x2 ), S(x3 ), A(y1 ), A(y2 ), A(y3 )] = [t, t, f, t, t, f ]
and V2 : [S(x1 ), S(x2 ), S(x3 ), A(y1 ), A(y2 ), A(y3 )] = [t, t, f, t, f, t]. For both valuations,
#X [S(X)] = (2, 1) and #Y [A(Y )] = (2, 1), so  (#X [S(X)], #Y [A(Y )]) must return the
same value under V1 and V2 . The original parfactor, however, returns (S(t), (1, 1)) 
(S(t), (1, 1))  (S(f ), (2, 0)) under V1 , and (S(t), (1, 1))  (S(t), (2, 0))  (S(f ), (1, 1))
416

fiDecoupling Lifted Variable Elimination from the Constraint Language

Counting Conversion

(S(X), F (X, Y ))

(S(x1 ), F (x1 , y1 ))
..
.

!





 (S(X), #Y [F (X, Y )])

 (S(x1 ), F (x1 , y1 ), . . . , F (x1 , ym )) =  (S(x1 ), #Y [F (x1 , Y )])

(S(x1 ), F (x1 , ym ))

..
.

..
.

(S(xn ), F (xn , y1 ))
..
.

!



..
.

 (S(xn ), F (xn , y1 ), . . . , F (xn , ym )) =  (S(xn ), #Y [F (xn , Y )])

(S(xn ), F (xn , ym ))

S(x1 )



F (x1 , y1 )

..
.




F (x1 , ym )

F (x1 , ym )
S(x1 )



..
.

Counting

S(x1 ) 

F (x1 , ym )
F (x1 , ym )

F (x1 , ym )

..
.

..
.
S(xn )

..
.

#





F (xn , y1 )

..
.




F (xn , y1 )
S(xn )

F (xn , ym )

..
.


..
.

F (xn , y1 )
S(xn )

Counting



F (xn , ym )



#

..
.
F (xn , ym )

Figure 7: Counting conversion. The equivalent of the lifted operation (top), is shown at
the level of ground factors (middle), and also in terms of factor graphs (bottom).

under V2 , which may be different. Since the original parfactor can distinguish valuations
that no factor of the form  (#X [S(X)], #Y [A(Y )]) can, counting conversion cannot be
applied in this case.
In contrast, consider g  = (S(X), #Y [A(Y )])|(X, Y )  {x1 , x2 , x3 }  {y1 , y2 , y3 }, which
is similar to g, except that its constraint satisfies precondition 3. All three factors represented by g differ only in their first argument, randvar S(xi ); they have the same counting
randvar #Y [A(Y )]|Y  {y1 , y2 , y3 } as their second argument (this was not the case for g).
Their product, thus, can be represented by a parfactor  (#X [S(X)], #Y [A(Y )])|(X, Y ) 
{x1 , x2 , x3 }  {y1 , y2 , y3 }, which is derived from g  by a counting conversion.
5.4 Splitting and Shattering
When the preconditions for lifted multiplication, lifted summing-out and counting conversion are not fulfilled, it is necessary to reformulate the model in terms of parfactors that
do fulfill them. For instance, if g1 = 1 (S(X))|X  {x1 , x2 , x3 } and g2 = 2 (S(X))|X 
{x1 , x2 , x3 , x4 , x5 }, we cannot multiply g1 and g2 directly without creating unwanted de417

fiTaghipour, Fierens, Davis, & Blockeel

Operator count-convert
Inputs:
(1) g = (A)|C: a parfactor in G
(2) X: a logvar in logvar(A)
Preconditions
(1) there is exactly one atom Ai  A with X  logvar(Ai )
(2) X is count-normalized w.r.t logvar(A) \ {X} in C
(3) for all counted logvars X # in g: X,X # (C) = X (C)  X # (C)
Output:  (A )|C, such that
(1) A = A \ {Ai }  {Ai } with Ai = #X [Ai ]
(2) for each assignment a to A with ai = h:
Q
 (. . . , ai1 , h, ai+1 , . . . ) = ai range(Ai ) (. . . , ai1 , ai , ai+1 , . . . )h(ai )
with h(ai ) denoting the count of ai in histogram h
Postcondition: G  G \ {g}  {count-convert(g, X)}.
Operator 3: The counting conversion operator.
pendencies. However, we can replace g2 with g2a = 2 (S(X))|X  {x1 , x2 , x3 } and g2b =
2 (S(X))|X  {x4 , x5 }. The resulting model is equivalent, but in this new model, we can
multiply g1 with g2a , resulting in g3 = 3 (S(X))|X  {x1 , x2 , x3 }.
The above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007;
Milch et al., 2008). Basically, splitting two parfactors partitions each parfactor into a part
that is shared with the other parfactor, and a part that is disjoint. The goal is to rewrite the
P(C)RVs and parfactors into a proper form. Two P(C)RVs (V1 , V2 ) are proper if RV (V1 )
and RV (V2 ) are either identical or disjoint; two parfactors are proper if all their P(C)RVs
are proper. A pair of parfactors can be written into proper form by applying the following
procedure, until all their P(C)RVs are proper. Choose a P(C)RV V1 from one parfactor,
compare it to a P(C)RV V2 from the other, and rewrite the first parfactor such that V1 is
split into two parts: one that is disjoint from V2 and one that is shared with V2 . All the
parfactors in the model can be made proper w.r.t. each other by repeatedly applying this
rewrite until convergence. This is called shattering the model.
It is simpler to rewrite a PRV into the proper form than a PCRV. We describe the operator that handles PRVs, namely split, in this section and discuss the operator that handles
PCRVs, namely expand, in the following section. Before defining the split operator, we
provide the following auxiliary definitions, which will also be used later on.
Definition 5 (Splitting on overlap) Splitting a constraint C1 on its Y-overlap with C2 ,
denoted C1 /Y C2 , partitions C1 into two subsets, containing all tuples for which the Y part
occurs or does not occur, respectively, in C2 . C1 /Y C2 = {{t  C1 |Y (t)  Y (C2 )}, {t 
C1 |Y (t) 
/ Y (C2 )}}.
Definition 6 (Parfactor partitioning) Given a parfactor g = (A)|C and a partition
C = {Ci }ni=1 of C, partition(g, C) = {(A)|Ci }ni=1 .
Operator 4 defines splitting of parfactors. Note that, in the operator definition, for
simplicity, we assume that A = A = P (Y), which means that the logvars used in A and
418

fiDecoupling Lifted Variable Elimination from the Constraint Language

Operator split
Inputs:
(1) g = (A)|C: a parfactor in G
(2) A = P (Y): an atom in A
(3) A = P (Y)|C  or #Y [P (Y)]|C 
Output: partition(g, C), with C = C/Y C  \ {}
Postcondition G  G \ {g}  split(g, A, A )
Operator 4: The split operator.
A must be the same, in the same order. We can always rewrite the model such that any
two PRVs with the same predicate are in this form. For this, we rewrite the parfactors as
follows: (i) if the parfactors share logvars, we first standardize apart the logvars between two
parfactors, (ii) linearize each atom in which some logvar occurs more than once, i.e., rewrite
it such that it has a distinct logvar in each argument, and (iii) apply a renaming substitution
on the logvars such that the concerned atoms have the same logvars. For instance, consider
the two parfactors g1 = 1 (P (X, X))|X  C1 and g2 = 2 (P (Y, Z))|(Y, Z)  C2 . The
logvars of the two parfactors are already different, so there is no need for standardizing
them apart. However, the atom P (X, X) in g1 is not linearized yet. To linearize it, we
rewrite g1 into the form 1 (P (X, X  ))|(X, X  )  C1 , where C1 = {(x, x)|x  C1 }. Finally,
we rename the logvars X and X  to Y and Z, respectively, to derive 1 (P (Y, Z))|(Y, Z)  C1 .
This brings the atom P (X, X) into the desired form P (Y, Z).
For ease of exposition, we will not explicitly mention this linearization and renaming;
whenever two PRVs from different parfactors are compared, any notation suggesting that
they have the same logvars is to be interpreted as have the same logvars after linearization
and renaming.
When GC-FOVE wants to multiply two parfactors, it first checks for all pairs A1 |C1 ,
A2 |C2 (one from each parfactor) whether they are proper. If a pair is found that is not
proper, this means A1 and A2 are both of the form P (Y), with different (but overlapping)
instantiations for Y in C1 and C2 . The pair is then split on Y.
Example 9. Consider g1 = 1 (N (X, Y ), R(X, Y, Z))|C1 with C1 = (X, Y, Z)  {xi }50
i=1 

5
25
50
{yi }50
i=1  {zi }i=1 , and g2 = 2 (N (X, Y ))|C2 with C2 = (X, Y )  {x2i }i=1  {yi }i=1 . First,
we compare the PRVs N (X, Y )|C1 and N (X, Y )|C2 . These PRVs partially overlap, so
splitting is necessary. To split the parfactors, we split C1 and C2 on their (X,Y)-overlap.
50
5
excl = C \
This partitions C1 into two sets: C1com = {x2i }25
1
i=1  {yi }i=1  {zi }i=1 , and C1
50
5
C1com = {x2i1 }25
i=1  {yi }i=1  {zi }i=1 . C2 does not need to be split, as it has no tuples
for which the (X,Y)-values do not occur in C1 . After splitting the constraints, we split the
parfactors accordingly: g1 is split into two parfactors g1com = (N (X, Y ), R(X, Y, Z))|C1com
and g1excl = (N (X, Y ), R(X, Y, Z))|C1excl , and parfactor g2 remains unmodified.

Our splitting procedure splits any two PRVs into at most two partitions each. Similarly, the involved parfactors are split into at most two partitions each. This strongly
contrasts with C-FOVEs approach to splitting. C-FOVE operates per logvar, and splits
off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch
et al., 2008). Thus, it may require many splits where GC-FOVE requires just one. In
419

fiTaghipour, Fierens, Davis, & Blockeel

the above example, instead of g1excl = (N (X, Y ), R(X, Y, Z))|C1excl , C-FOVE ends up
with 1250 parfactors (N (x1 , y1 ), R(x1 , y1 , Z))|{zi }5i=1 , (N (x1 , y2 ), R(x1 , y2 , Z))|{zi }5i=1 ,
. . . , (N (x3 , y1 ), R(x3 , y1 , Z))|{zi }5i=1 , . . . , (N (x49 , y50 ), R(x49 , y50 , Z))|{zi }5i=1 .
The reason why GC-FOVE can always split into at most two parfactors, yielding much
coarser partitions than C-FOVE, is that it assumes an extensionally complete constraint
language, whereas C-FOVE allows only pairwise (in)equalities, forcing it to split off each
element separately.
5.5 Expansion of Counting Formulas
When handling parfactors with counting formulas, to rewrite a P(C)RV into the proper
from, we employ the operation of expansion (Milch et al., 2008). When we split one group
of randvars RV (V) into a partition {RV (Vi )}m
i=1 , any counting randvar  that counts the
values of RV (V) needs to be expanded, i.e., replaced by a group of counting randvars {i }m
i=1 ,
where each i counts the values of randvars in RV (Vi ). In parallel with this, the potential
that originally had V as an argument must be replaced by a potential that has all the Vi as
arguments; we call this potential expansion.
Example 10. Suppose we need to split g1 = 1 (#X [S(X)])|C1 and g2 = 2 (S(X))|C2 , with

C1 = {x1 , . . . , x100 } and C2 = {x1 , . . . , x40 }. C1 is split into C1com = C1  C2 = {x1 , . . . x40 }
and C1excl = C1 \ C2 = {x41 , . . . x100 }. Consequently, the original group of randvars in
parfactor g1 , namely {S(x1 ), . . . S(x100 )}, is partitioned into V1com = {S(x1 ), . . . S(x40 )} and
V1excl = {S(x41 ), . . . S(x100 )}. To preserve the semantics of the original counting formula,
we now need two separate counting formulas, one for V1com and one for V1excl , and we need
to replace the original potential 1 (#X [S(X)]) by 1 (#Xcom [S(Xcom )], #Xexcl [S(Xexcl )]),
where 1 () depends only on the sum of the two new counting randvars #Xcom [S(Xcom )] and
#Xexcl [S(Xexcl )]. The end effect is that the parfactor g1 is replaced by the new parfactor
1 (#Xcom [S(Xcom )], #Xexcl [S(Xexcl )])|C1 , where C1 = C1com  C1excl .
To explain expansion, we begin with the case of (non-parametrized) CRVs and then
move to the general case of expansion for PCRVs.
5.5.1 Expansion of CRVs
First consider the simplest possible type of CRV: #X [P (X)]|C. It counts for how many
values of X in C, P (X) has a certain value. When C is partitioned, X must be counted
within each subset of the partition.
In the following, we assume C is partitioned into two non-empty subsets C1 and C2 . If
one of them is empty, the other equals C, which means the CRV can be kept as is and no
expansion is needed.
In itself, splitting #X [P (X)]|C into #X [P (X)]|C1 and #X [P (X)]|C2 is trivial, but a
problem is that both of the resulting counting formulas will occur in one single parfactor,
and a constraint is always associated with a parfactor, not with a particular argument
of a parfactor. Thus, we need to transform (#X [P (X)])|C into a parfactor of the form
 (#X1 [P (X1 )], #X2 [P (X2 )])|C  , where the single constraint C  expresses that X1 can take
only values in C1 , and X2 only values in C2 . It is easily seen that C  = XX1 C1 XX2 C2
420

fiDecoupling Lifted Variable Elimination from the Constraint Language

satisfies this condition. Further, to preserve the semantics,  should, for any count of X1
and X2 , give the same result as  with the corresponding count of X. The function
 (h1 , h2 ) = (h1  h2 ),
with  denoting summation of histograms, has this property. Indeed, the histogram for X1
(resp. X2 ) in C  is equal to that for X in C1 (resp. C2 ), and since {C1 , C2 } is a partition
of C, the sum of these histograms equals the histogram for X in C.
More generally, consider a non-parametrized CRV #X [P (X)]|C, with X  X meaning that X\{X} (C) is singleton. The constraint C  = X\{X} (C)  (X1 (XX1 C1 ) 
X2 (XX2 C2 )) joins this singleton with the Cartesian product of X (C1 ) and X (C2 ),
and is equivalent to the constraint XX1 (C1 )  XX2 (C2 ). The result is again such
that counting X1 (X2 ) in C  is equivalent to counting X in C1 (C2 ), while the constraint
on all other variables remains unchanged. This shows that a parfactor (A, #X [P (X)])|C,
for any partition {C1 , C2 } of C with C1 and C2 non-empty, can be rewritten in the form
 (A, #X1 [P (X)], #X2 [P (X)])|C  , where C  = XX1 (C1 )  XX2 (C2 ).
Note that the ranges of the counting formulas in  (the hi arguments) depend on the
cardinality of C1 and C2 , which we will further denote as n1 and n2 respectively.
5.5.2 Expansion of PCRVs
Consider the case where X\{X} (C) is not a singleton, i.e., we have a parametrized CRV
V that represents a group of CRVs, each counting the values of a subset of RV (V). Given
a partitioning of the constraint C, we need to expand each underlying CRV and the corresponding potential. The constraint C  = XX1 (C1 )  XX2 (C2 ) remains correct (for
non-empty C1 , C2 ), even when X\{X} (C) is no longer singleton: it associates the correct
values of X1 and X2 with each tuple in X\{X} (C). However, because the result of potential expansion depends on the size of the partitions, n1 and n2 , only those CRVs that have
the same (n1 ,n2 ) result in identical potentials after expansion, and can be grouped in one
parfactor. To account for this, PCRV expansion first splits the PCRV into groups of CRVs
that have the same joint count (n1 , n2 ), then applies for each group the corresponding
potential expansion.
To formalize this, we first provide the following auxiliary definitions.
Definition 7 (Group-by) Given a constraint C and a function f : C  R, GroupBy(C, f ) = C/ f , with x f y  f (x) = f (y) and / denoting set quotient. That is,
Group-By(C, f ) partitions C into subsets of elements that have the same result for f .
Definition 8 (Joint-count) Given a constraint C over variables X, partitioned into {C1 ,
C2 }, and a counted logvar X  X; then for any t  C, with L = X \ {X} and l = L (t),
joint-countX,{C1 ,C2 } (t) = (|X (L=l (C1 ))|, |X (L=l (C2 ))|).
When a PCRV V = #Xi [P (X)] |C in a parfactor g partially overlaps with another PRV
A |C  in the model, expansion performs the following on g: (1) partition C on its X-overlap
with C  , resulting in C/X C  ; (2) partition C into C = group-by(C, joint-countX,C/X C  )
(this corresponds to a partition of V into CRVs that have the same number of randvars in each of the common and exclusive partitions in C/X C  ); (3) split g, based on
421

fiTaghipour, Fierens, Davis, & Blockeel

Operator expand
Inputs:
(1) g = (A)|C: a parfactor in G
(2) A = #X [P (X)]: a counting formula in A
(3) A = P (X)|C  or #Y [P (X)]|C 
Output: {gi = i (Ai )|Ci }ni=1 where
(1) C/X C  = {C com , C excl }
(2) {C1 , . . . , Cn } = group-by(C, joint-countX,C/X C  )
(3) for all i where Ci  C com =  or Ci  C excl = : i = , Ai = A, Ci = Ci
(4) for all other i:
(5) Ci = logvar(A) (Ci )  (XXcom (C com )  XXexcl (C excl ))
(6) Ai = A \ {A}  {Acom , Aexcl } with com = {X  Xcom }, excl = {X  Xexcl }
(7) for each valuation (l, hcom , hexcl ) of Ai , i (l, hcom , hexcl ) = (l, hcom  hexcl )
Postcondition G  G \ {g}  expand(g, A, A )
Operator 5: The expansion operator.
C = {C1 , . . . , Cn }, resulting in parfactors g1 , . . . , gn that each require a distinct expanded
potential; (4) in each gi , replace potential  with its expanded version. The formal definition
of expansion is given in Operator 5.
Suppose we need to split parfactors g = (#Y [F (X, Y )])|C and g =
with C = {ann, bob, carl}  {dave, ed, f red, gina} and C  = {ann, bob} 
{dave, ed}. Assume F stands for friendship; #Y [F (X, Y )]|C counts the number of friends
and non-friends each X has in C. The random variables covered by PCRV #Y [F (X, Y )] |C
partially overlap with those of F (X, Y ) |C  . If we need to split C on overlap with C  , yielding C com and C excl , we need to replace the original PCRV with separate PCRVs for C com
and C excl . But PCRVs require count-normalization, and the fact that Y is count-normalized
w.r.t. X in C does not necessarily imply that the same holds in C com and C excl . That is
why, in addition to the split on overlap, we need an orthogonal partitioning of C according
to the joint counts. Within a subset Ci of this partitioning, Y will be count-normalized
w.r.t. X in Cicom and in Ciexcl .
We follow the four steps outlined above. Figure 8 illustrates these steps. First, we
find the partition C/X,Y C  = {C com , C excl } with C com = {ann, bob}  {dave, ed} and
C excl = {ann, bob}  {f red, gina}  {carl}  {dave, ed, f red, gina}. Inspecting the joint
counts, we see that C com contains 2 possible friends for Ann or Bob (namely Dave and
Ed), but 0 for Carl, whereas C excl contains 2 possible friends for Ann or Bob and 4
for Carl. Formally, joint-countY,C/X,Y C  (t) equals (2,2) for X (t) = ann or X (t) =
bob, and equals (0,4) for X (t) = carl. So, within C com and C excl , Y is no longer
count-normalized with respect to X. We therefore partition C into subsets {C1 , C2 } =
group-by(C, joint-countY,C/X,Y C  ), which gives C1 = {ann, bob} {dave, ed, f red, gina}
and C2 = {carl}{dave, ed, f red, gina}. For each Ci , we can now construct a Ci that allows
for counting the friends in Cicom and in Ciexcl separately, using the series of joins discussed
earlier. Where both Cicom and Ciexcl are non-empty, the original PCRV #Y [F (X, Y )] |C is
Example 11.

 (F (X, Y

))|C  ,

422

fiDecoupling Lifted Variable Elimination from the Constraint Language

C
ann dave
ann ed
bob dave
bob ed

C
ann
ann
ann
ann
bob
bob
bob
bob
carl
carl
carl
carl

dave
ed
fred
gina
dave
ed
fred
gina
dave
ed
fred
gina
C1

X
ann
ann
ann
ann
bob
bob
bob
bob

Ycom
dave
dave
ed
ed
dave
dave
ed
ed

Yexcl
fred
gina
fred
gina
fred
gina
fred
gina

C/X,Y C 
ann dave
ann ed
bob dave
bob ed
ann fred
ann gina
bob fred
bob gina
carl dave
carl ed
carl fred
carl gina

group-by(C,
joint-countY,C/X,Y C  )
ann dave
ann ed
ann fred
ann gina
C1
bob dave
bob ed
bob fred
bob gina
carl dave
carl ed
C2
carl fred
carl gina

C2
X
Y
carl dave
carl ed
carl fred
carl gina

Figure 8: Illustration of the PCRV expansion operator. (1) Y is count-normalized w.r.t. X
in C (with each X, four Y values are associated). Splitting C on overlap with C 
results in subsets in which Y is no longer count-normalized w.r.t. X: the joint
counts of Y for both subsets are (2,2) for Ann and Bob, and (0,4) for Carl. To
obtain count-normalized subsets, we need to partition C into a subset C1 for
Ann and Bob, and C2 for Carl; this is what the Group-By construct does. For
each of the subsets, a split on overlap with C  will yield subsets in which Y is
count-normalized w.r.t. X. C1 is the result of joining the common and exclusive
parts according to the join construct motivated earlier. C2 equals C2 because C2
has no overlap with C  and hence need not be split.

replaced by two PCRVs per Ci , #Ycom [F (X, Ycom )] |Ci and #Yexcl [F (X, Yexcl )] |Ci , and the
new potential  is defined such that  (hcom , hexcl ) = (hcom  hexcl ).
GC-FOVEs expansion improves over C-FOVEs in the following way. C-FOVE uses
expansion based on substitution (Milch et al., 2008). For instance, in Example 10, C-FOVE
splits off all the elements of C excl individually from C, adding each of these elements as
a separate argument of the parfactor and the involved potential function. This yields a
423

fiTaghipour, Fierens, Davis, & Blockeel

potential function 1 () with 61 arguments, namely the counting randvar #Xcom [S(Xcom )]
and the 60 randvars S(x41 ), . . . S(x100 ). This causes an extreme blow up in the size (number
of entries) of the potential function, which does not happen using our approach. In general,
C-FOVEs expansion yields a potential function of size O(r k  (n  k)r ), with n = |C1 |, k =
|C1excl |, and r the cardinality of the range of the considered randvars (e.g., r = |range(S(.))|
in Example 10). In contrast, GC-FOVEs expansion yields a potential function of size
O(kr  (n  k)r ). In the likely scenario that r  k, this is exponentially smaller than
C-FOVEs potential function. Given that this potential function will later be used for
multiplication or summing-out, it is clear that GC-FOVE can yield large efficiency gains
over C-FOVE.
5.6 Count Normalization
Lifted multiplication, summing-out and counting conversion all require certain variables
to be count-normalized (recall Definition 2, p. 406). When this property does not hold,
it can be achieved by normalizing the involved parfactor, which amounts to splitting the
parfactor into parfactors for which the property does hold (Milch et al., 2008). Concretely,
when Y is not count-normalized given Z in a constraint C, then C is simply partitioned
into C = Group-By(C, CountY|Z ), with CountY|Z as defined in Definition 1; next, the
parfactor is split according to C. The formal definition of count normalization is shown in
Operator 6.
Operator count-normalize
Inputs:
(1) g = (A)|C: a parfactor in G
(2) Y|Z: sets of logvars indicating the desired normalization property in C
Preconditions
(1) Y  logvar(A) and Z  logvar(A) \ Y
Output: partition(g, group-by(C, CountY|Z ))
Postconditions G  G \ {g}  count-normalize(g, Y|Z)
Operator 6: The count-normalization operator.
Example 12. Consider the parfactor g with A = (P rof (P ), Supervises(P, S)) and con-

straint C = {(p1 , s1 ), (p1 , s2 ), (p2 , s2 ), (p2 , s3 ), (p3 , s5 ), (p4 , s3 ), (p4 , s4 ), (p5 , s6 )}. Lifted
elimination of Supervises(P, S) requires logvar S (student) to be count-normalized with respect to logvar P (professor). Intuitively, we need to partition the professors into groups such
that all professors in the same group supervise the same number of students. In our example,
C needs to be partitioned into two, namely C1 = P {p3 ,p5 } (C) = {(p3 , s5 ), (p5 , s6 )} (tuples
involving professors with 1 student) and C2 = P {p1 ,p2,p4 } (C) = {(p1 , s1 ), (p1 , s2 ), (p2 , s2 ),
(p2 , s3 ), (p4 , s3 ), (p4 , s4 )} (professors with 2 students). Next, the parfactor g is split accordingly into two parfactors g1 and g2 with constraints C1 and C2 . These parfactors are now
ready for lifted elimination of Supervises(P, S).
C-FOVE requires a stronger normalization property to hold. For every pair of logvars X and
Y it requires either (1) X,Y (C) = X (C)  Y (C) or (2) X (C) = Y (C) and X,Y (C) =
424

fiDecoupling Lifted Variable Elimination from the Constraint Language

(X (C)Y (C))\{hxi , xi i : xi  X (C)}. To enforce this, C-FOVE requires finer partitions
than our approach does. In our example, C-FOVE requires C to be split into 5 subsets
{Ci }5i=1 with Ci = P {pi } (C), i.e., one group per professor. The coarser partitioning used
in our approach cannot be represented using C-FOVEs constraint language.
5.7 Absorption: Handling Evidence
When the value of a randvar is observed, this usually makes probabilistic inference more
efficient: the randvar can be removed from the model, which may introduce extra independencies in the model. However, in lifted inference, there is also an adverse effect:
observations can break symmetries among randvars. For this reason, it is important to
handle observations in a manner that preserves as much symmetry as possible. In order to
effectively handle observations in a lifted manner, we introduce the novel operator of lifted
absorption.
In the ground setting, absorption works as follows (van der Gaag, 1996). Given a factor
(A) and an observation Ai = ai with Ai  A, absorption replaces (A) with a factor  (A ),
with A = A \ {Ai } and  (a1 , . . . , ai1 , ai+1 , . . . , am ) = (a1 , . . . , ai1 , ai , ai+1 , . . . , am ).
This reduces the size of the factor and may introduce extra independencies in the model,
which is always beneficial.
If n randvars (built from the same predicate) have the same observed value, we can
perform absorption on the lifted level by treating these n randvars as one single group.
Consider a parfactor g = (S(X), F (X, Y ))|(X, Y )  {(x1 , y1 ), . . . , (x1 , y50 )}. Assume that
evidence atoms F (x1 , y1 ) to F (x1 , y10 ) all have the value true. This can be represented by
adding an evidence parfactor gE to the model: gE = E (F (X, Y ))|(X, Y )  {x1 }  {yj }10
1 ,
with E (true) = 1 (the observed value) and E (f alse) = 0. To absorb the evidence, g needs
to be split into two, namely g1 with C1 = {(x1 , y1 ), . . . , (x1 , y10 )} (the parfactor about which
we have evidence) and g2 with C2 = {(x1 , y11 ), . . . , (x1 , y50 )} (no evidence). Then, we can
absorb the evidence about F into g1 . Performing absorption on the ground level would
result in ten identical factors  (S(x1 )) (the logvar Y disappears in the absorption). Lifted
absorption computes the same  once, and raises it to the tenth power. Generally, with
Xexcl the logvars that occur exclusively in the atom being absorbed, the exponent is the
number of values Xexcl can take, so Xexcl must be count-normalized with respect to the
other logvars. Further, all logvars in Xexcl can be removed from the constraint C as they
disappear in the absorption.
For parfactors with counting formulas, essentially the same reasoning is used, but now
the exponent is determined by the non-counted logvars occurring exclusively in the atom
(Xnce ). These logvars, together with the counted logvar, can be removed from C. The value
for the absorbed counting formula, to be filled in in , is a histogram indicating how many
times each possible value has been observed in the absorbed PRV. Since there is only one
observed value in the evidence parfactor, this histogram maps that value to the number of
randvars being absorbed, and other values to zero. Lifted absorption is formally defined in
Operator 7. We provide a correctness proof for this operator in Appendix A, and analyze
its complexity in Appendix B.
GC-FOVE handles evidence by absorption as follows. It first creates one evidence parfactor per observed value for each predicate. Next, it compares each evidence parfactor with
425

fiTaghipour, Fierens, Davis, & Blockeel

Operator absorb
Inputs:
(1) g = (A)|C: a parfactor in G
(2) Ai  A with Ai = P (X) or Ai = #Xi [P (X)]
(3) gE = E (P (X))|CE : an evidence parfactor
Let Xexcl = X \ logvar(A \ Ai );
Xnce = Xexcl \ {Xi } if Ai = #Xi [P (X)], Xexcl otherwise;
L = logvar(A) \ Xexcl ;
o = the observed value for P (X) in gE ;
Preconditions
(1) RV (Ai |C)  RV (Ai |CE )
(2) Xnce is count-normalized w.r.t. L in C.
Output: g =  (A )|C  , with
(1) A = A \ {Ai }
(2) C  = logvar(C)\Xexcl (C)
(3)  (. . . , ai1 , ai+1 , . . . ) = (. . . , ai1 , e, ai+1 , . . . )r , with r = CountXnce |L (C), and
with e = o if Ai = P (X)
and e a histogram with e(o) = CountXi |logvar(A) (C), e(.) = 0 elsewhere, otherwise
(namely if Ai = #Xi [P (X)])
Postcondition
G  {gE } = G \ {g}  {gE , absorb(g, Ai , gE )}
Operator 7: Lifted absorption.
each PRV in the model, applying absorption when possible. Where necessary, parfactors
in the model are split to allow for absorption. (It is never necessary to split evidence parfactors, see precondition 1.) When no more absorptions are possible with a given evidence
parfactor, it is removed from the model: the evidence has been incorporated completely.
Like the sum-out operator, the absorb operator has the effect of eliminating PRVs
from the model. As the operators definitions show, however, absorb requires weaker
preconditions than sum-out, which means that it can be applied in more situations. Also,
the absorb operator easily lends itself to a splitting as needed constraint processing strategy
(Kisynski & Poole, 2009a), which keeps the model at a much higher granularity, by requiring
fewer splits on the parfactors compared to a preemptive shattering strategy. In the presence
of observations, which is often the case in real-world problems, these effects can result in
large computational savings.
Our approach to dealing with evidence differs from C-FOVEs in two important ways.
First, C-FOVE introduces a separate evidence factor for each ground observation A = a.
This causes extensive splitting: if there are n randvars with the same observed value, there
will be n separate factors, and C-FOVE will perform (at least) n eliminations on these
randvars. In addition, the splitting may cause further splitting as C-FOVE continues,
destroying even more opportunities for lifting. We show in Section 7 that this can make
inference impossible with C-FOVE in the presence of evidence.
Second, C-FOVE does not use absorption; during inference, the evidence factors are
used for multiplication and summing-out like any other factors. Absorption is advantageous
426

fiDecoupling Lifted Variable Elimination from the Constraint Language

Operator ground-logvar
Inputs:
(1) g = (A)|C: a parfactor in G
(2) X: a logvar in logvar(A)
Output: partition(g, group-by(C, X ))
Postcondition
G  G \ {g}  ground-logvar(g, X)
Operator 8: Grounding.
because it eliminates randvars from the model, so they no longer need to be summed out. As
a result, in our approach, evidence reduces the number of summing-out and multiplication
operations, while in C-FOVE it increases that number.
5.8 Grounding a Logvar
There is no guarantee that the enabling operators eventually result in PRVs and parfactors
that allow for any of the lifted operators. To illustrate this, consider a model consisting of
a single parfactor (R(X, Y ), R(Y, Z), R(X, Z))|C, which expresses a probabilistic variant
of transitivity. Since there is only one factor, no multiplications are needed before starting
to eliminate variables. Yet, because of the structure of the parfactor, no single PRV can
be eliminated (the preconditions for lifted summing out and counting conversion are not
fulfilled, and none of the other operators can change that).
In cases like this, when no other operators can be applied, lifted VE can always resort to a
last operator: grounding a logvar X in a parfactor g (de Salvo Braz, 2007; Milch et al., 2008).
Given a parfactor g = (A)|C and a logvar X  logvar(A) with X (C) = {x1 , . . . , xn },
grounding X replaces g with the set of parfactors {g1 , . . . , gn } with gi = (A)|X=xi (C).
This is equivalent to splitting g based on the partition group-by(C, X ), which yields the
definition shown in Operator 8. Note that in each resulting parfactor gi , logvar X can only
take on a single value xi , so in practice X can be replaced by the constant xi and removed
from the set of logvars.
Grounding can significantly increase the granularity of the model and decrease the opportunities for performing lifted inference: in the extreme case where all logvars are grounded,
inference is performed at the propositional level. It is therefore best used only as a last
resort. In practice, (G)C-FOVEs heuristic for selecting operators, which relies on the size
of the resulting factors, automatically has this effect.
Calling the ground-logvar operator should not be confused with the event of obtaining a ground model. ground-logvar grounds only one logvar, and does not necessarily
result in a ground model. Conversely, one may arrive at a ground model without ever calling
ground-logvar, simply because the splitting continues up to the singleton level.

6. Representing and Manipulating the Constraints
We have shown that using an extensionally complete constraint language instead of allowing
only pairwise (in)equalities can potentially yield large efficiency gains by allowing more
opportunities for lifting. The question remains how we can represent these constraints.
427

fiTaghipour, Fierens, Davis, & Blockeel

X
{x1 , x2 , x3 }
Y
{y1 , . . . , y10 }
Z
{z1 , . . . , z5 }



{x4 , x5 }

{x5 , . . . , x20 }

Y

Y

{y11 , y12 } {y1 , . . . , y20 } {y1 , y2 }
Z

Z

{z1 , . . . , z10 } {z4 , z5 }





Z
{z1 , . . . , z8 }



{y3 , . . . , y10 }

Z
{z10 , z15 }



Figure 9: A constraint tree representing a constraint on logvars X, Y, Z.
In principle, we could represent them extensionally, as lists of tuples. This allows any
constraint to be represented, but is inefficient when we have many logvars. Instead, we
employ a constraint tree, as also used in First Order Bayes-Ball (Meert, Taghipour, &
Blockeel, 2010). Hence, the lower-level operations on constraints (projection, splitting,
counting) must be implemented in terms of constraint trees. Below, we briefly explain how
this is done.
A constraint tree on logvars X is a tree in which each internal (non-leaf) node is labeled
with a logvar X  X, each leaf is labeled with a terminal label , and each edge e = (Xi , Xj )
is labeled with a (sub-)domain D(e)  D(Xi ). See Figure 9 for an example. We use ordered
trees, where all nodes in the same level of the tree are labeled with the same logvar, and each
logvar occurs on only one level. Each path from the root to a leaf through edges e1 , . . . , e|X|
represents the tuples in the Cartesian product i D(ei ). For example, in Figure 9, the left
most path represents the tuples {x1 , x2 , x3 }  {y1 , . . . , y10 }  {z1 , . . . , z5 }. The constraint
represented by the tree is the union of tuples represented by each root-to-leaf path.
Given a constraint (in terms of the set of tuples that satisfy it), we construct the
corresponding tree in a bottom-up manner by merging compatible edges. Different logvar
orders can result in trees of different sizes. A tree can be re-ordered by interchanging nodes
in two adjacent levels of the tree and applying the possible merges at those levels. We
employ re-ordering to simplify the various constraint handling operations. For projection
of a constraint, we move the projected logvars to the top of the tree and discard the parts
below these logvars. For splitting, we perform a pairwise comparison of the two involved
constraint trees. First, we re-order each tree such that the logvars involved in the split
are at the top of the trees. Then we process the trees top-down by comparing the edges
leaving the root in the two trees and partition their domains based on their overlap. We
recursively repeat this for their children until we reach the last logvar involved in the split.
For count normalization, we also first apply this re-ordering. Then we partition the tree
based on the number of tuples of counted logvars in each branch. For counting this number,
428

fiDecoupling Lifted Variable Elimination from the Constraint Language

we only need to consider the size of the domains associated with the edges. Finally, the
join of two constraints is computed by reordering the trees so that the join variables occur
at the top, merging the levels of the join variables in the same way as is done for splitting,
and extending each leaf in the resulting tree with the cross-product of the corresponding
subtrees of the original trees.
Constraint trees (and the way they are constructed) are close to the hypercube representation used in lifted belief propagation (Singla, Nath, & Domingos, 2010). However, for
a given constraint, the constraint tree is typically more compact. The constraint tree of
Figure 9 corresponds to a set of five hypercubes, one for each leaf. The hypercube representation does not exploit the fact that the first and second hypercube, for instance, share
the part {x1 , x2 , x3 }. In the constraint tree, this is explicit, which makes it more compact.
We stress that GC-FOVE can use any extensionally complete constraint representation
language. Constraint trees are just one such representation. Other representations can be
more compact in some cases, but in the choice of a representation we need to consider also
the tradeoff between compactness and ease of constraint processing. Consider a constraint
graph, which is similar to our trees, but in which parent nodes can share child nodes. This
representation is more compact than a constraint tree, but also requires more complicated
constraint handling operations. For instance, consider splitting, in which we might need
to split a child node for one parent but not for the others. Such operations become more
complicated on graphs, while they are trivial on trees.

7. Experiments
Using an extensionally complete constraint language, we can capture more symmetries
in the model, which potentially offers the ability to perform more operations at a lifted
level. However, this comes at a cost, as manipulating more expressive constraints is more
computationally demanding. We hypothesize that the ability to perform fewer computations
by capturing more symmetries will far outweigh this cost in typical inference tasks. In this
section, we compare the performances of C-FOVE and GC-FOVETREES (GC-FOVE using
the tree representation from Section 6) to empirically validate this hypothesis. In particular,
we study how the performances vary as a function of two parameters: (i) the domain size,
and (ii) the amount of evidence. We also empirically study whether GC-FOVETREES can
solve inference tasks that are beyond the reach of C-FOVE.
Throughout this section, GC-FOVE stands for GC-FOVETREES .
7.1 Methodology and Datasets
We compare C-FOVE and GC-FOVE on several inference tasks with synthetic and realworld data. We use the version of C-FOVE extended with general parfactor multiplication
(de Salvo Braz, 2007).3 For implementing GC-FOVE, we started from the publicly available C-FOVE code (Milch, 2008), so the implementations are maximally comparable.4 In
all experiments, the undirected model has parfactors whose constraints are all representable
3. This allows C-FOVE to handle some tasks in an entirely lifted way, where otherwise it would have to
resort to grounding, e.g., on the social network domain (Jha et al., 2010).
4. GC-FOVE is available from http://dtai.cs.kuleuven.be/ml/systems/gc-fove.

429

fiTaghipour, Fierens, Davis, & Blockeel

by C-FOVE. Thus, GC-FOVE has no initial advantage, which makes the comparison conservative.
In each experiment we compute the marginal probability of a query randvar given some
evidence. The query randvar is selected at random from the non-observed atoms. The
evidence is generated by randomly selecting randvars of a particular predicate and giving
them a value chosen randomly and uniformly from their domain. All the reported results
are averaged over multiple runs for different query and evidence sets.
7.1.1 Experiments with Synthetic Data
In terms of synthetic data, we evaluate our algorithm on three standard benchmark problems. The first domain is called workshop attributes (Milch et al., 2008). Here, m different
attributes (e.g., topic, date, etc.) describe the workshop, and a corresponding factor for each
attribute shows the dependency between the attendance of each person and the attribute.
The theory contains the following parfactors.
1 (Attends(X), Attr1 )
...
m (Attends(X), Attrm )
m+1 (Attends(X), Series)
The second domain is called competing workshops (Milch et al., 2008). It models the fact
that people are more likely to attend a workshop if it is on a hot topic and that the number
of attendees influences whether the workshop becomes a series. The theory contains the
following parfactors.
1 (Attends(X), Hot(Y ))
2 (Attends(X), Series)
In our experiments on both of the above domains, the query variable is Series, and all
evidence randvars are of the form Attends(x).
The third domain is called social network (Jha et al., 2010) and it models peoples
smoking habits, their chance of having asthma, and the dependence of a persons habits and
diseases on their friendships. The theory contains the following parfactors.
1 (Smokes(X))
2 (Asthma(X))
3 (F riends(X, Y ))
4 (Asthma(X), Smokes(X))
5 (Asthma(X), F riends(X, Y ), Smokes(Y ))
In this domain, the evidence randvars will be a mix of randvars of the form Smokes(x) or
Asthma(x), and the query randvar can be any randvar that is unobserved.
430

fiDecoupling Lifted Variable Elimination from the Constraint Language

7.1.2 Experiments with Real-World Data
We also used two other datasets from the field of statistical relational learning. The first,
WebKB (Craven & Slattery, 1997), contains data about more than 1200 webpages, including
their class (e.g., course page), textual content (set of words), and the hyperlinks between
the pages. The model consists of multiple parfactors, stating for instance how the classes of
two linked pages depend on each other. Our inference task concerns link prediction. Here,
the class information is observed for a subset of all pages and the task is to compute the
probability of having a hyperlink between a pair of pages. We use one Pageclass predicate
in the model for each run, and average the runtime over multiple runs for each class. We
used the following set of parfactors.
1 (P ageclass(P ))
2 (P ageclass(P ), HasW ord(P, W ))
3 (P ageclass(P1 ), Link(P1 , P2 ), P ageclass(P2 ))

The second dataset, Yeast (Davis, Burnside, de Castro Dutra, Page, & Costa, 2005), contains data about more than 7800 yeast genes, their functions and locations, and the interactions between these genes. The model and task are similar to those in WebKB (gene
functions correspond to page classes, gene-to-gene interactions to hyperlinks). In this task,
we observe the function information for a subset of all genes and query the existence of an
interaction between two genes. Similar to WebKB, we also use one function in the model
in each run and average the results over multiple runs. Here, we used the following set of
parfactors.

1 (F unction(G))
2 (Location(G, L))
3 (F unction(G), Location(G, L))
4 (F unction(G1 ), Interaction(G1 , G2 ), F unction(G2 ))

Motivation for evidence randvars. In all experiments, evidence randvars correspond to
atoms of a unary predicate; we call them unary randvars. This is done on purpose because
introducing evidence randomly for binary randvars, e.g., randvars of the type P (X, Y ), can
quickly break so many symmetries that lifted inference is not possible anymore. In fact,
there are recent theoretical results that show that lifted inference in the presence of arbitrary
evidence on binary randvars is simply not possible. This limitation is not unique to our
approach, but is true of any possible exact lifted inference approach (Van den Broeck &
Davis, 2012). Because of this, random insertion of evidence on binary randvars can quickly
cause any lifted inference algorithm to resort to ground inference, which would blur the
distinction between C-FOVE, GC-FOVE, and ground inference. We avoid this by placing
evidence only on unary randvars.
431

fiTaghipour, Fierens, Davis, & Blockeel

GC-FOVE
C-FOVE

10

1000

GC-FOVE
C-FOVE

GC-FOVE
C-FOVE
100
Time (s)

Time(s)

Time(s)

10

1

1

10

1

0.1
200

400
600
Domain Size

800

(a) Workshop Attributes

1000

200

400
600
Domain Size

800

(b) Competing Workshops

1000

!

400

600

' 

800

1000

(c) Social Network

Figure 10: Performance on synthetic data for varying domain sizes and proportion of observed randvars fixed at 20%. Y-axis (runtime) is drawn in log scale.

7.2 Influence of the Domain Size
In the first set of experiments, we use the synthetic datasets to measure the effect of domain
size (number of objects) on runtime. We vary the domain size from 50 to 1000 objects while
holding the proportion of observed randvars (relative to the number of observable randvars)
constant at 20%. Figures 10(a) through 10(c) show the performance on all three synthetic
datasets. On all three models, GC-FOVE outperforms C-FOVE on all domain sizes. As
the number of objects in the domain increases, the runtimes increase for both algorithms.
GC-FOVEs runtime increases at a much lower rate than C-FOVEs on all three models.
On the first two tasks, GC-FOVE is between one and two orders of magnitude faster than
C-GOVE, for the largest domain sizes. On the social network domain, the difference in
performance becomes more striking: C-FOVE cannot handle domain sizes of 100 objects or
more, while GC-FOVE handles the largest domain (1000 objects) in about 200 seconds. The
improvement in performance arises as GC-FOVE better preserves the symmetries present
in the model by treating all indistinguishable elements, observed or not, as a single unit.
The gain is more pronounced for larger domains. C-FOVE makes a separate partition
(and a separate evidence factor) for each observed randvar, thus, with a fixed evidence
ratio, the number of partitions induced by C-FOVE grows linearly with the domain size.
Moreover, it has a costly elimination operation for each partition. In contrast, GC-FOVE,
which employs lifted absorption, keeps the model at a higher granularity by grouping the
observations and handles whole groups of observations with a single lifted operation.
7.3 Influence of the Amount of Evidence
In the second set of experiments, we measure the effect of the proportion of observed
randvars on runtime, using the synthetic datasets. We fix the domain size, and vary the
percentage of observed randvars from 0% to 100%. Note that this is a percentage of all
observable randvars (e.g., all randvars of the form Smokes(x)), not of all randvars of
any type (so 100% does not mean there are no unobserved variables left). Figures 11(a)
through 11(c) show the performance on all three synthetic domains with domain size of
1000 objects. To better demonstrate C-FOVEs behavior on the social networks domain,
Figure 11(d) shows the performances on a domain with only 25 objects. Both algorithms
display similar trends across the three domains. Without evidence, GC-FOVE is comparable
432

fiDecoupling Lifted Variable Elimination from the Constraint Language

10000

GC-FOVE
C-FOVE

1000

100

100
Time (s)

Time(s)

GC-FOVE
C-FOVE

1000

10

10

1

1

0.1

0.1

0

20

40
60
Percentage of Evidence

80

0.01
0

100

(a) Workshop Attributes (domain size: 1000)

20

40
60
Percentage of Evidence

80

100

(b) Competing Workshops (domain size: 1000)
1000

1000
GC-FOVE
C-FOVE

GC-FOVE
C-FOVE

100

Time(s)

Time(s)

100

10

10

1

1
0.1

0.1
0

20

40
60
Percentage of Evidence

80

100

0

(c) Social Network (domain size: 1000)

20

40
60
Percentage of Evidence

80

100

(d) Social Network (domain size: 25)

Figure 11: Performance on synthetic data with varying amounts of evidence and a fixed
domain size. The Y-axis (runtime) is drawn in log scale.

to C-FOVE. This is the best scenario for C-FOVE as (i) the initial model only contains
(in)equality constraints, and (ii) there is no evidence, so no symmetries are broken when the
inference operators are applied. In this case, the only difference in runtime between the two
algorithms is the overhead associated with constraint processing, which is almost negligible.
As the proportion of observations increases, and the symmetries between the objects are
broken, GC-FOVE maintains a much coarser grouping, and so performs inference much more
efficiently, than C-FOVE. In all domains, C-FOVEs runtime increases dramatically with
an increase in the percentage of observations. As more evidence is added, C-FOVE induces
more partitions, which results in finer groupings of objects and leaves fewer opportunities
for lifting. GC-FOVE performs significantly better in comparison, due to coarser grouping
of observations and employing absorption for their elimination from the model. GC-FOVEs
runtime experiences a bump as the initial set of evidence is added, but then levels out or
gradually decreases (the more evidence, the more randvars are efficiently eliminated by
absorption). GC-FOVE consistently finishes in under 200 seconds, regardless of the setting.
In contrast, on the social network domain (Figure 11(c)) C-FOVE cannot handle portions
of evidence greater than 1% (it runs out of memory on machine configured with 30GB of
memory).
These results confirm that both the coarser groupings and the use of lifted absorption
contribute to the much better performance of GC-FOVE.
433

fiTaghipour, Fierens, Davis, & Blockeel

6
4

GC-FOVE
C-FOVE

0.4
Time(s)

Time(s)

0.5

GC-FOVE
C-FOVE

5
3
2

0.3
0.2
0.1

1
0

20

40
60
Percentage of Evidence

80

0

100

(a) Yeast

0

"	

40

60


fffiffff  ffff

%	

100

(b) WebKB

Figure 12: Performance on real-world data with varying amounts of evidence. The Yaxis (runtime) is drawn in log scale. C-FOVE only ran to completion for the
zero-evidence experiments.

7.4 Performance on Real-World Data
In the final set of experiments, we compared the algorithms on the two real-world datasets,
WebKB and Yeast. On both datasets, we varied the percentage of observed page classes
or functions from 0% to 100% in steps of 10%. Figures 12(a) and 12(b) illustrate the
results. C-FOVE could solve only the zero-evidence problems in these experiments; for the
other cases, it typically ran out of memory after up to an hour of computation time on a
machine configured with 30 GB of memory. Its failure is primarily due to the large number
of observations, which often forces it to resort to inference at the ground level for a large
number of objects. GC-FOVE, on the other hand, runs successfully for all experimental
conditions. Furthermore, GC-FOVE can consistently solve the problems in a few seconds.
As on the synthetic data, GC-FOVEs performance improves with increasing number of
observations. In these cases more randvars can be eliminated through absorption, instead
of the more expensive operations of multiplication and summation.

8. Conclusions
Constraints play a crucial role in lifted probabilistic inference as they determine the degree
of lifting that takes places. Surprisingly, most lifted inference algorithms use the same class
of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005;
Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011);
the main exception is the work on approximate inference using lifted belief propagation
(Singla & Domingos, 2008). In this paper we have shown that this class of constraints is
overly restrictive. We proposed using extensionally complete constraint languages, which
can capture more symmetries among the objects and allow for more operations to occur on
a lifted level. We defined the relevant constraint handling operations (e.g., splitting and
normalization) to work with extensionally complete constraint languages and implemented
them for performing lifted variable elimination. We made use of constraint trees to efficiently represent and manipulate the constraints. We empirically evaluated our system on
several domains. Our approach resulted in up to three orders of magnitude improvement
in runtime, as compared to C-FOVE. Furthermore, GC-FOVE can solve several tasks that
are intractable for C-FOVE.
434

fiDecoupling Lifted Variable Elimination from the Constraint Language

Future work includes generalizing other lifted inference algorithms that currently use
only inequality constraints, e.g., the works of Jha et al. (2010) and Van den Broeck et al.
(2011), and further optimizing constraint handling. With respect to the latter, an interesting direction is the recent work of de Salvo Braz, Saadati, Bui, and OReilly (2012)
that employs a logical representation for constraints, which is extensionally complete, and
presents specialized constraint processing methods for this representation. Finally, it is
possible to extend lifted absorption such that it works not only with evidence parfactors,
but more generally with deterministic parfactors. This is another promising direction for
future work.
Acknowledgments
Daan Fierens is supported by the Research Foundation of Flanders (FWO-Vlaanderen).
Jesse Davis is partially supported by the Research Fund KULeuven (CREA/11/015 and
OT/11/051), and EU FP7 Marie Curie Career Integration Grant (#294068). This work was
funded by GOA/08/008 Probabilistic Logic Learning of the Research Fund KULeuven.
The authors thank Maurice Bruynooghe and Guy Van den Broeck for interesting discussions
and comments on this work and text. They also thank the reviewers for their constructive
comments and very concrete suggestions to improve the article.

Appendix A. Correctness Proof for Lifted Absorption
In this appendix, we prove the correctness of the novel lifted absorption operator. We begin
by providing some lemmas.
Recall that a set of parfactors G is a compact way of defining a set of factors gr(G) =
{f |f  gr(g)  g  G} and the corresponding probability distribution
PG (A) =

1
Z

Y

f (Af ).

f gr(G)

Further, G  G means G and G define the same probability distribution. Thus, formally:
G  G  PG (A) = PG (A) 

1
Z

Y

f gr(G)

f (Af ) =

1
Z

Y

f (Af ).

f gr(G )

The following lemmas are easily proven by applying the above definition and keeping in
mind that gr(G  G ) = gr(G)  gr(G ).
Lemma 1 For all models G, G , G : G  G  G  G  G  G .
Lemma 2 Given a factor f = (A1 , A2 , . . . , An ) and an evidence factor fE = E (A1 ) with
E (a1 ) = 1 if a1 = o (the observed value) and E (a1 ) = 0 otherwise, {f, fE }  {f  , fE }
with f  =  (A2 , . . . , An ) and  (a2 , . . . , an ) = (o, a2 , . . . , an ).
Lemma 3 A model that consists of m identical factors, G = {(A1 , . . . , An )}m
i=1 , is



equivalent to a model with a single factor G = { (A1 , . . . , An )} where  (a1 , . . . , an ) =
(a1 , . . . , an )m .
435

fiTaghipour, Fierens, Davis, & Blockeel

We now prove that the Absorb operator is correct, i.e., its postconditions hold, given
the preconditions.
Theorem 1 Given a model G, a parfactor g  G and an evidence parfactor gE , if the
preconditions of the absorb operator are fulfilled, then
G  {gE }  G \ {g}  {absorb(g, Ai , gE ), gE }.
Proof: With G = G \ {g}, we can rewrite the above equivalence as
G  {g, gE }  G  {absorb(g, Ai , gE ), gE }.
Because of Lemma 1, it suffices to prove
{g, gE }  {absorb(g, Ai , gE ), gE }.
Let g = (A)|C with A = {A1 (X1 ), . . . , Ak (Xk )}, let gE = E (P (X))|CE , and let
L = logvar(A) (non-counted logvars in A), Xexcl = X \ logvar(A \ {Ai }) (logvars occurring
exclusively in Ai ) and L = logvar(A) \ Xexcl (non-counted logvars occurring (also) outside
Ai ). The operator returns a parfactor of the form  (A )|C  where A = {A2 , . . . , Ak } and
C  = logvar(C)\Xexcl (C) (see the operator definition). We need to prove that  is such that
the above equivalence holds. For ease of exposition, from now we assume that the atom or
counting formula that is to be absorbed (Ai in the operators input) is A1 . We first consider
the case where A1 is an atom P (X1 ), and then the case where A1 is a counting formula
#X [P (X1 )].
Absorption for atoms. In this case, we have  (a2 , . . . , ak ) = (o, a2 , . . . , ak )r with
r = CountXexcl |L (C) (see the operator definition, observing that Xnce = Xexcl ).
By definition, gr(g) = {(P (x1 ), A2 (x2 ), . . . , Ak (xk ))}lL (C) , with xi = Xi (l). Precondition 1 guarantees that for each (P (x1 ), A2 (x2 ), . . . , Ak (xk )), there exists an evidence
factor E (P (x1 )) in gr(gE ). By Lemma 2, we can therefore rewrite each factor in gr(g) into
the form  (A2 (x2 ), . . . ), with  (a2 , . . . ak ) = (o, a2 , . . . , ak ), with o the observed value
for P (x1 ).
The potential function  is the same for all factors, since there is only one observed value
o for the whole evidence parfactor. Therefore, any two factors (P (x1 ), A2 (x2 ), . . . , Ak (xk ))
and (P (x1 ), A2 (x2 ), . . . , Ak (xk )) that differ only in their first argument are rewritten to
the same factor. Because of Precondition 2, the number of factors rewritten to the same
factor is constant and equals CountXexcl |L (C) = r. By Lemma 3, each set of identical
factors can therefore be replaced by a single factor with potential function
 (a2 , . . . , ak ) =  (a2 , . . . , ak )r = (o, a2 , . . . , ak )r ,
which is exactly how  is defined by the operator.
Absorption for counting formulas. In this case, we have
 (a2 , . . . , ak ) = (e, a2 , . . . , ak )r
and r = CountXnce |L (C) with Xnce = Xexcl \ {X} (see the operator definition).
436

fiDecoupling Lifted Variable Elimination from the Constraint Language

We define X1 = X1 \ {X}, and use (x1 , X) to denote X1 with all logvars instantiated
except the counted logvar X. Now, by definition,
gr(g) = {(#XCl [P (x1 , X)], A2 (x2 ), . . . , Ak (xk ))}lL (C) ,
with xi = Xi (l), x1 = X1 (l) and Cl = X (L=l (C)). Each Cl is of the form {x1 , . . . , xn },
where n = CountX|L (C) (n exists because PCRVs are by definition count-normalized).
We show correctness of the operator in this case by showing that for each factor f in
gr(g), the evidence parfactor gE can be rewritten to contain an evidence factor that has
the same CRV as f , such that the same reasoning as above can be applied on f .
Precondition 1 guarantees that for each factor
f = (#X{x1 ,...,xn } [P (x1 , X)], A2 (x2 ), . . . , Ak (xk )),
gr(gE ) contains the group of evidence factors
Ef = {E (P (x1 , x1 )), . . . , E (P (x1 , xn ))}.
We can multiply all factors in Ef into
E (P (x1 , x1 ), . . . , P (x1 , xn )),
with E (o, o, . . . , o) = 1 and E (.) = 0 elsewhere, and then rewrite this as
fE = E (#X{x1 ,...,xn } [P (x1 , X)]),
where E is such that (i) E (e) = 1, for e the histogram with e(o) = n and e(.) = 0
elsewhere, and (ii) E (e ) = 0 for e 6= e.
Having formed fE , we can rewrite f into the form  (A2 (x2 ), . . . ), with  (a2 , . . . ak ) =
(e, a2 , . . . , ak )r and r = CountXnce |L (C), with the same argumentation as for regular
atoms. After this, we can replace fE with the equivalent Ef , thus restoring gE . Repeating
this for each f preserves equivalence and eventually yields the model that the operator
returns.


Appendix B. Computational Complexity of Lifted Absorption
Applying lifted absorption on a parfactor g = (A)|C, has complexity O(|C|) + O(Size() 
log |C|), where |C| is the cardinality (number of tuples) of the constraint
Q C, and Size()
equals the product of range sizes of the arguments A, i.e., Size() = Ai A |range(Ai )|.
The first term in the complexity, O(|C|), arises because absorption involves a projection of
the constraint C, which in the worst case (with an extensional representation) has complexity O(|C|). The second term, O(Size()  log |C|), is the complexity of computing the new
potential function, which involves manipulating , which has Size() entries (in a tabular
representation), and exponentiating it, which has complexity O(log |C|).
437

fiTaghipour, Fierens, Davis, & Blockeel

References
Apsel, U., & Brafman, R. I. (2011). Extended lifted inference with joint formulas. In
Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 1118.
Choi, J., Hill, D., & Amir, E. (2010). Lifted inference for relational continuous models. In
Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 126134.
Craven, M., & Slattery, S. (1997). Relational learning with statistical predicate invention:
Better models for hypertext. Machine Learning, 43(1/2), 97119.
Davis, J., Burnside, E. S., de Castro Dutra, I., Page, D., & Costa, V. S. (2005). An
integrated approach to learning Bayesian networks of rules. In Proceedings of 16th
European Conference on Machine Learning (ECML), pp. 8495.
De Raedt, L., Frasconi, P., Kersting, K., & Muggleton, S. (Eds.). (2008). Probabilistic inductive logic programming: Theory and applications. Springer-Verlag, Berlin, Heidelberg.
de Salvo Braz, R. (2007). Lifted first-order probabilistic inference. Ph.D. thesis, Department
of Computer Science, University of Illinois at Urbana-Champaign.
de Salvo Braz, R., Amir, E., & Roth, D. (2005). Lifted first-order probabilistic inference.
In Proceedings of the 19th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 13191325.
de Salvo Braz, R., Saadati, S., Bui, H., & OReilly, C. (2012). Lifted arbitrary constraint solving for lifted probabilistic inference. In Proceedings of the 2nd International Workshop
on Statistical Relational AI (StaRAI), pp. 18.
Dechter, R. (2003). Constraint processing. Morgan Kaufmann.
Getoor, L., & Taskar, B. (Eds.). (2007). An Introduction to Statistical Relational Learning.
MIT Press.
Gogate, V., & Domingos, P. (2011). Probabilistic theorem proving. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 256265.
Jha, A., Gogate, V., Meliou, A., & Suciu, D. (2010). Lifted inference seen from the other
side : The tractable features. In Proceedings of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS), pp. 973981.
Kersting, K., Ahmadi, B., & Natarajan, S. (2009). Counting belief propagation. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI), pp.
277284.
Kisynski, J., & Poole, D. (2009a). Constraint processing in lifted probabilistic inference.
In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 293302.
Kisynski, J., & Poole, D. (2009b). Lifted aggregation in directed first-order probabilistic models. In Proceedings of the 21st International Joint Conference on Artificial
Intelligence (IJCAI), pp. 19221929.
438

fiDecoupling Lifted Variable Elimination from the Constraint Language

Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs and the sum-product
algorithm. IEEE Transactions on Information Theory, 47 (2), 498519.
Meert, W., Taghipour, N., & Blockeel, H. (2010). First-order bayes-ball. In Proceedings of
the European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML PKDD), pp. 369384.
Milch, B. (2008). BLOG.. http://people.csail.mit.edu/milch/blog/.
Milch, B., Zettlemoyer, L. S., Kersting, K., Haimes, M., & Kaelbling, L. P. (2008). Lifted
probabilistic inference with counting formulas. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI), pp. 10621608.
Poole, D. (2003). First-order probabilistic inference.. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence (IJCAI), pp. 985991.
Ramakrishnan, R., & Gehrke, J. (2003). Database management systems (3. ed.). McGrawHill.
Sen, P., Deshpande, A., & Getoor, L. (2009a). Bisimulation-based approximate lifted inference. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence
(UAI09), pp. 496505.
Sen, P., Deshpande, A., & Getoor, L. (2009b). Prdb: managing and exploiting rich correlations in probabilistic databases. VLDB Journal, 18 (5), 10651090.
Singla, P., & Domingos, P. (2008). Lifted first-order belief propagation. In Proceedings of
the 23rd AAAI Conference on Artificial Intelligence (AAAI), pp. 10941099.
Singla, P., Nath, A., & Domingos, P. (2010). Approximate Lifted Belief Propagation. In
Proceedings of the 1st International Workshop on Statistical Relation AI (StaRAI),
pp. 9297.
Taghipour, N., Fierens, D., Davis, J., & Blockeel, H. (2012). Lifted variable elimination
with arbitrary constraints. In Proceedings of the 15th International Conference on
Artificial Intelligence and Statistics (AISTATS), pp. 11941202.
Van den Broeck, G., & Davis, J. (2012). Conditioning in first-order knowledge compilation
and lifted probabilistic inference. In Proceedings of the 26th AAAI Conference on
Artificial Intelligence (AAAI), pp. 17.
Van den Broeck, G., Taghipour, N., Meert, W., Davis, J., & De Raedt, L. (2011). Lifted
Probabilistic Inference by First-Order Knowledge Compilation. In Proceedings of the
22nd International Joint Conference on Artificial Intelligence (IJCAI), pp. 21782185.
van der Gaag, L. C. (1996). On evidence absorption for belief networks. Int. J. Approx.
Reasoning, 15 (3), 265286.

439

fiJournal of Artificial Intelligence Research 47 (2013) 575-611

Submitted 02/13; published 07/13

A Refined View of Causal Graphs and Component Sizes:
SP-Closed Graph Classes and Beyond
Christer Backstrom
Peter Jonsson

christer.backstrom@liu.se
peter.jonsson@liu.se

Department of Computer Science
Linkoping University
SE-581 83 Linkoping, Sweden

Abstract
The causal graph of a planning instance is an important tool for planning both in
practice and in theory. The theoretical studies of causal graphs have largely analysed the
computational complexity of planning for instances where the causal graph has a certain
structure, often in combination with other parameters like the domain size of the variables.
Chen and Gimenez ignored even the structure and considered only the size of the weakly
connected components. They proved that planning is tractable if the components are
bounded by a constant and otherwise intractable. Their intractability result was, however,
conditioned by an assumption from parameterised complexity theory that has no known
useful relationship with the standard complexity classes. We approach the same problem
from the perspective of standard complexity classes, and prove that planning is NP-hard
for classes with unbounded components under an additional restriction we refer to as SPclosed. We then argue that most NP-hardness theorems for causal graphs are difficult to
apply and, thus, prove a more general result; even if the component sizes grow slowly and
the class is not densely populated with graphs, planning still cannot be tractable unless the
polynomial hierachy collapses. Both these results still hold when restricted to the class of
acyclic causal graphs. We finally give a partial characterization of the borderline between
NP-hard and NP-intermediate classes, giving further insight into the problem.

1. Introduction
We will first briefly explain what a causal graph is and give a short survey of applications
as well as theoretical results reported in the literature. Following that, we give an overview
of the new results presented in this article.
1.1 Background
The causal graph for a planning instance is an explicit description of the variable dependencies that are implicitly defined by the operators. More precisely, it is a directed graph
such that there is an arc from a variable x to another variable y if either x appears in the
precondition of an operator with an effect on y or some operator has effects on both x and y.
This standard definition of the causal graph can be traced back to Knoblock (1994)
although he did not give it a name. He used the causal graph in the Alpine algorithm,
as a guidance for partitioning and ordering the variables in the process of automatically
deriving state abstraction hierarchies. The actual name causal graph can be traced back
to Williams and Nayak (1997). Their approach was both more general and more restricted
c
2013
AI Access Foundation. All rights reserved.

fiBackstrom & Jonsson

than Knoblocks. On the one hand, they generalized the concept from binary variables to
multi-valued variables, but on the other hand, they considered only acyclic causal graphs
which implies that all operators are unary, i.e. every operator changes only one variable.
The context of their work was the reactive planner Burton for onboard space-ship control.
A causal model was compiled into a transition system that could be efficiently exploited by a
reactive controller to choose appropriate operators to achieve given goals. The compilation
was done in such a way that all operators were unary, and they claimed that this is often
possible in real applications. The resulting acyclicity of the causal graph was then exploited
by Burton, which traversed the graph bottom up in order to issue operators in an order
consistent with their causal relationships.
Jonsson and Backstrom (1998b) also studied acyclic causal graphs, but referred to them
as dependency graphs. They considered a subclass of such graphs having a particular structure and used this to implicitly define a corresponding class of planning instances, the 3S
class. This class has the property that it is always possible to decide in polynomial time if
there is a solution or not, but the solutions themselves may be of exponential length, thus
necessarily taking exponential time to generate. Although only one single restricted case,
the 3S class is probably the first example of relating structural properties of the causal graph
to the computational complexity of planning. A more general and extensive such analysis
was done by Domshlak and Dinitz (2001a), who analysed the complexity of planning for
classes of instances corresponding to a number of different possible structures of acyclic
causal graphs. However, their work was done in the context of multi-agent coordination
and the term causal graph was never used.
The first two of these papers may be viewed as early examples of exploiting the causal
graph in practice, while the latter papers form the starting point of the subsequent theoretical research into the relationships between planning complexity and the structure of
causal graphs.
An important step forward in the usage of causal graphs was the paper by Helmert
(2004) where he demonstrated that the causal graph is particularly useful in the context of
multi-valued variables. Previous research on the complexity of planning with multi-valued
variables had focussed on the structure of the domain-transition graphs for the variables
(Jonsson & Backstrom, 1998a), rather than the causal graph. Helmert realized the power
of using both the domain-transition graphs and the causal graph in heuristic planning.
This was exploited in practice in his highly succesful Fast Downward planner (Helmert,
2006a). It translates PDDL planning instances with binary variables into a representation
with multi-valued variables and then removes carefully chosen edges in the resulting causal
graph to make it acyclic. The resulting causal graph is then used to compute a heuristic
by hierarchically computing and composing plan lengths for subgraphs having one of the
particular structures studied by Domshlak and Dinitz (2001a). Somewhat similarly, Katz
and Domshlak (2010) identified subgraphs of the causal graph that have certain structures
that make planning for them tractable. They exploited this to be able to use larger variables
sets when constructing pattern databases. A further example of exploiting the causal graph
to make planning more efficient is the paper on factored planning by Brafman and Domshlak
(2006). They showed that the structure of the causal graph can be used as a guide for
deciding if and how a planning instance can be solved more efficiently by dividing it into
loosely coupled subinstances and use constraint processing. The basic idea of the causal
576

fiA Refined View of Causal Graphs and Component Sizes

graph to represent variable dependencies is, of course, quite general and not necessarily
restricted to planning. For instance, Wehrle and Helmert (2009) transferred the causal
graph concept to the context of model checking.
As previously mentioned, the two papers by Jonsson and Backstrom (1998b) and by
Domshlak and Dinitz (2001a) can be viewed as the starting point for a successful line of
research into studying the relationships between planning complexity and the structure
of the causal graph. While the 3S class by Jonsson and Backstrom was a very limited
special case, Domshlak and Dinitz studied classes of planning instances corresponding to a
number of more general graph structures, like in-stars (aka. inverted forks), out-stars (aka.
forks), directed path graphs (aka. directed chain graphs), polytrees and singly-connected
DAGs. Further results followed, for instance, in articles by Brafman and Domshlak (2003),
and Gimenez and Jonsson (2008). The latter article additionally showed that although 3S
instances can have exponential-length plans, it is possible to generate a macro representation
of such a plan in polynomial time, a result they extended also to some other classes defined
by the structure of the causal graph. Many of the complexity results in these papers use
additional numerical parameters in conjunction with the graph structure. Examples of such
parameters are the maximum domain size of the variables and the maximum in-degree of
the graph. While increasing the number of possible cases to analyse, it does allow for a
more fine-grained analysis in many cases. Consider for instance the case of directed path
graphs. Domshlak and Dinitz (2001a) proved that it is tractable to decide if there is a
plan for this case when the domains are binary, while Gimenez and Jonsson (2009) proved
that a domain size of 5 is sufficient to make the problem NP-hard. Similarly, Gimenez and
Jonsson (2012) proved tractability for planning instances with binary variables, a constant
number of prevail conditions and where the causal graph is a polytree. Also the paper
by Brafman and Domshlak (2006) fits into this line of theoretical research, exhibiting a
planning algorithm that runs in time exponential in two parameters, the tree-width of the
undirected version of the causal graph and the maximum number of times a variable must
change value.
While most research has been based on the standard definition of causal graphs that
was set already by Knoblock, although often in the generalisation to multi-valued variables,
there are important exceptions. One potential problem with the standard defintion is that
whenever two variables are both affected by the same operator, then the causal graph must
necessarily contain cycles, which is the major reason why the focus has mainly been on
planning with unary operators. In an attempt to circumvent this problem, Jonsson (2009)
defined a more relaxed variant of the causal graph that does not always introduce cycles for
non-unary operators, which can sometimes allow for a more fine-grained complexity analysis.
The previous results relate the structure of the causal graph to the complexity of satisficing planning, i.e. deciding if there is a plan. There has also been a corresponding branch
of research relating the structure of the causal graph to the complexity of cost-optimal
planning (cf., Katz & Domshlak, 2007, 2008, 2010; Katz & Keyder, 2012).
1.2 Our Contributions
All of the theoretical research above studies the complexity of planning based on the structure of the causal graph, and possibly other parameters like domain sizes. An important
577

fiBackstrom & Jonsson

milestone that deviates from this line of research was an article by Chen and Gimenez
(2010) who did not even consider the structure of the causal graph but only a simple quantitative measure, the size of the weakly connected components. They proved that deciding
if there is a plan can be done in polynomial time if and only if the size of the weakly connected components in the causal graph is bounded by a constant. In one sense, this is a
very sharp and final result. However, the intractability result for unbounded components is
conditional on the assumtion that W[1] 6 nu-FPT. This assumption relies on the theory
of parameterised complexity theory and neither the complexity classes nor the assumption
itself can be related to ordinary complexity classes in a clear way. Chen and Gimenez acknowledge that the problems they prove conditionally intractable include NP-intermediate
problems. Hence, we take their result as a take-off point for further investigation of how
the component sizes reflect on the standard complexity classes. Since we know from Chen
and Gimenez that not all graph classes with unbounded components are NP-hard we must
consider further restrictions in order to find NP-hard classes. We do so by adding a new
type of closure property, SP-closure, which is incomparable to subset-closure but is a subset of minor-closure, and prove that planning is NP-hard for any SP-closed graph class
with unbounded components. It should be noted that this result still holds for the class
of all acyclic graphs, which is important considering the practical relevance of acyclicity
previously mentioned.
While many graph classes that have been studied in the literature are indeed SP-closed,
there also exists natural classes that lack this property. We present one way of handling
such classes with the aid of non-uniform complexity theory. In this case, we are not able to
show NP-hardness but we can show that the polynomial hierarchy collapses to its second
level. This is a fairly general result that can be applied even when the component sizes grow
very slowly and the graph class is not very densely populated with graphs. Also this result
holds even if restricted to acyclic graphs. This result can be used to demonstrate clearly
that complexity results for planning based only on the class of causal graphs does not necessarily have any connection to the complexity of a generic planning problem having the same
class of causal graphs. This result also raises the question of where to find (preferably natural) NP-intermediate planning problems. Chen and Gimenez state that NP-intermediate
problems can be obtained by using methods similiar to the ones employed by Bodirsky and
Grohe (2008). Such problems are hard to describe as natural, though. They are based on
Ladners (1975) diagonalization technique that removes a large fraction of input strings from
a problem. It is apparently difficult to connect graph classes constructed by this technique
with simple conditions on component growth. As an alternative, we show that graph classes
where the component sizes grow polylogarithmically are NP-intermediate under the double
assumption that W[1] 6 nu-FPT and that the exponential time hypothesis (Impagliazzo
& Paturi, 2001) holds. We also show that for every k > 1, there exists a class Gk of graphs
such that component size is bounded by |V (G)|1/k for all G  Gk and the corresponding
planning problem is NP-hard. These results coarsely stake out the borderline between
NP-hard and NP-intermediate classes.
A possible conclusion from this paper is that complexity analysis of planning based only
on the structure of the causal graph is of limited value, and that additional parameters are
needed to achieve more useful results. While this may be a fair conclusion in general, there
are cases where the graph structure is sufficient. For instance, Katz, Hoffmann, and Domsh578

fiA Refined View of Causal Graphs and Component Sizes

lak (2013) have applied the result by Chen and Gimenez (2010) in the context of so called
red-black planning, a variant of delete relaxation for computing heuristics. Furthermore,
even when the structure of the causal graph has to be combined with other parameters, it
is still important to know the behaviour of each parameter in isolation.
The remainder of the article is structured as follows. In Section 2 we set the notation
and terminology used for planning and for graphs, and in Section 3 we define causal graphs
and structural planning in general. Section 4 contains a number of NP-hardness results for
various special graph classes that we need for the main results. The first of the two main
theorems of the article appears in Section 5, where we define the concept of SP-closed graph
classes and prove that planning is NP-hard for such classes when the component size is
unbounded. Section 6 discusses some of the problems with both the previous theorem and
other similar results in the literature. As a way around these problems, our second main
theorem shows that even without any closure requirements, planning is likely to be hard
even when the components grow slowly and the graphs do not appear densely in the class.
Section 7 contains some observations concerning the borderline between NP-intermediate
and NP-hard planning problems. The article ends with a discussion section.

2. Preliminaries
This section sets the terminology and notation for planning and graphs used in this article.
We write |X| to denote the cardinality of a set X or the length of a sequence X, i.e. the
number of elements in X, and we write ||X|| to denote the size of the representation of an
object X.
2.1 Planning
Since this article has many connections with the one by Chen and Gimenez (2010) we
follow their notation and terminology for plannning, which is a notational variant of SAS+
(Backstrom & Nebel, 1995).
An instance of the planning problem is a tuple  = (V, init, goal, A) whose components
are defined as follows:
 V is a finite set of variables, where each variable v  V has an associated finite domain
D(v). Note that variables are not necessarily propositional, that is, D(v) may be any
finite set. A state is a mapping s defined on the variables V such that s(v)  D(v) for
all v  V . A partial state is a mapping p defined on a subset vars(p) of the variables V
such that for all v  vars(p), it holds that p(v)  D(v), and p is otherwise undefined.
 init is a state called the initial state.
 goal is a partial state.
 A is a set of operators; each operator a  A consists of a precondition pre(a) and
a postcondition post(a) which are both partial states. We often use the notation
hpre ; posti to define an operator with precondition pre and postcondition post. For
instance, a = hx = 0, y = 1 ; z = 1i defines an operator a which is applicable in any
state s such that s(x) = 0 and s(y) = 1, and which has the effect of setting variable
z to 1.
579

fiBackstrom & Jonsson

When s is a state or a partial state and W is a subset of the variable set V , we write
s  W to denote the partial state resulting from restricting s to W . We say that a state s is
a goal state if goal = s  vars(goal).
We define a plan (for an instance ) to be a sequence of operators P = a1 , . . . , an .
Starting from a state s, we define the state resulting from s by applying a plan P , denoted
by s[P ], inductively as follows. For the empty plan P = , we define s[] = s. For non-empty
plans P we define s[P ] as follows, where a is the last operator in P and P 0 is the prefix of
P up to, but not including, a:
 If pre(a) 6= s[P 0 ]  vars(pre(a)) (that is, the preconditions of a are not satisfied in the
state s[P 0 ]), then s[P 0 , a] = s[P 0 ].
 Otherwise, s[P 0 , a] is the state equal to post(a) on variables v  vars(post(a)), and
equal to s[P 0 ] on variables v  V \ vars(post(a)).
A plan P is a solution plan if init[P ] is a goal state.
We are concerned with the computational problem plan existence (PlanExist): given an
instance  = (V, init, goal, A), decide if there exists a solution plan.
2.2 Graphs
A directed graph is a pair (V, E) where V is the vertex set and E  V  V is the edge set.
An undirected graph is a pair (V, E) where V is the vertex set and E  {{u, v} | u, v  V }
is the edge set. We will often only say graph and edge if it is clear from the context whether
it is directed or undirected. The notation V (G) refers to the vertex set of a graph G and
E(G) refers to its edge set. If e = (u, v) or e = {u, v} is an edge, then the vertices u and
v are incident with e. Furthermore, the directed edge (u, v) is an outgoing edge of u and
an incoming edge of v. For a directed graph G = (V, E), we write U (G) to denote the
correspsonding undirected graph U (G) = (V, EU ) where EU = {{u, v} | (u, v)  E}. That
is, U (G) is the undirected graph induced by G by ignoring the orientation of edges.
Let G = (V, E) be a directed graph and let v0 , . . . , vk  V such that v1 , . . . , vk are
distinct and (vi1 , vi )  E for all i (1  i  k). Then the sequence v0 , . . . , vk is a directed
path of length k in G if v0 6= vk and it is a directed cycle of length k in G if v0 = vk . Paths
and cycles in undirected graphs are defined analogously, except that there is no direction
to consider. A graph is acyclic if it contains no cycles.
Let G = (V, E) be a directed graph and let v  V be a vertex. Then, v is isolated if it
has no incoming or outgoing edges, v is a source if it has at least one outgoing edge but no
incoming edge, v is a sink if it has at least one incoming edge but no outgoing edge and
otherwise v is intermediate.
Let G = (VG , EG ) and H = (VH , EH ) be two directed graphs. Then G and H are
isomorphic (denoted G ' H) if there exists a bijective function f : VG  VH such that
(u, v)  EG if and only if (f (u), f (v))  EH . Furthermore, H is a subgraph of G if VH  VG
and EH  EG  (VH  VH ). When EH = EG  (VH  VH ) we say that the subgraph H
is induced by the vertex set VH . Isomorphisms and subgraphs are analogously defined for
undirected graphs.
Let G be an undirected graph. Then G is connected if there is a path between every
pair of vertices in G. A connected component of G is a maximal subgraph of G that is
580

fiA Refined View of Causal Graphs and Component Sizes

connected. Let G be a directed graph. Then G is weakly connected if U (G) is connected.
A weakly connected component of G is a maximal subgraph of G that is weakly connected.
That is, in a weakly connected component there are paths between every pair of vertices
if we ignore the direction of edges. Let G = (VG , EG ) and H = (VH , EH ) be two directed
graphs such that VG and VH are disjoint. Then the (disjoint) union of G and H is defined
as G  H = (VG  VH , EG  EH ) and is a commutative operation. Note that if a graph G
consists of the (weakly) connected components G1 , . . . , Gn , then G = G1  G2  . . .  Gn .
We further define some numeric graph parameters. For a directed graph G and a vertex
v  V (G), the indegree of v is |{u  V (G) | (u, v)  E(G)}|, i.e. the number of incoming
edges incident with v, and the outdegree of v is |{u  V (G) | (v, u)  E(G)}|, i.e. the number
of outgoing edges incident with v. For an undirected graph G, the degree of v  V (G) is
|{u  V (G) | {v, u}  E(G)}|, i.e. the number of edges incident with v. We extend this to
graphs as follows. If G is an undirected graph, then deg(G) denotes the largest degree of
any vertex in V (G). Similarly, if G is a directed graph then in-deg(G) denotes the largest
indegree of any vertex in V (G) and out-deg(G) denotes the largest outdegree of any vertex in
V (G). Furthermore, if G is an undirected graph, then path-length(G) denotes the length of
the longest path in G and cc-size(G) denotes the size of the largest connected component in
G. If G is a directed graph, then path-length(G) denotes the length of longest directed path
in G. We also define upath-length(G) = path-length(U (G)) and cc-size(G) = cc-size(U (G)).
That is, upath-length(G) is the length of the longest path in G if ignoring the direction of
edges and cc-size(G) is the size of the largest weakly connected component in G. Note that
if G is an undirected connected graph, then path-length(G) equals the diameter of G. We
extend all such numeric graph properties (in-deg, path-length etc.) to sets of graphs such
that if C is a set of graphs and prop is a graph property, then prop(C) = maxGC prop(G).
2.3 Special Graph Types
In the literature on causal graphs, as well as in this article, there are certain types of graphs
that are of particular interest and that are thus useful to refer to by names. We distinguish
the following types of undirected graphs: A tree is an undirected graph in which any two
vertices are connected by exactly one path, i.e. it is acyclic and connected. A path graph
is a tree where all vertices have degree 1 or 2, i.e. it is a tree that does not branch. A star
graph is a tree where all vertices except one, the centre vertex, have degree 1.
For directed graphs, we distinguish the following types: An in-star graph is a directed
graph G such that U (G) is a star graph and all edges are directed towards the centre.
An out-star graph is a directed graph G such that U (G) is a star graph and all edges are
directed out from the centre. A directed path graph is a directed graph G such that U (G)
is a path graph, in-deg(G)  1 and out-deg(G)  1, i.e. G is a directed path over all its
vertices and contains no other edges. A polytree is a directed graph G such that U (G) is
a tree, i.e. G is a weakly connected directed graph that can be constructed from a tree by
giving a unique direction to every edge. A polypath is a directed graph G such that U (G)
is a path graph, i.e. G is a weakly connected directed graph that can be constructed from
a path graph by giving a unique direction to every edge. A fence is a polypath where every
vertex is either a source or a sink, i.e. the edges alternate in direction at every vertex.
581

fiBackstrom & Jonsson

It should be noted that the out-star graph is usually called a directed star graph in
graph theory, while the in-star graph appears to have no standard name. We hence deviate
sligthly from standard terminology in order to have logical names for both graph types.
Also the polypath appears to have no standard name, but polypath is a logical term in
analogy with polytree. It should be further noted that a parallel terminology for certain
graph types has evolved in the literature on causal graphs in planning. For instance, instars, out-stars and directed paths are commonly referred to as inverted forks, forks and
directed chains, respectively.
Note that the number of sinks and sources in a polypath differ by at most one, i.e. a
polypath with m sinks has m + c sources for some c  {1, 0, 1}. Furthermore, every fence
is a polypath, but not every polypath is a fence.
We define the following graphs and graphs classes:
 Skin denotes the in-star graph with one centre vertex and k sources. Also define the
class Sin = {Skin | k  0}.
 Skout denotes the out-star with one centre vertex and k sinks. Also define the class
Sout = {Skout | k  0}.
 dPk denotes the directed path on k vertices. Also define the class dP = {dPk | 1  k}.
c , for c  {1, 0, 1}, denotes the fence with m sinks and m + c sources. Also define
 Fm
c | 1  m}, for each c  {1, 0, 1}, and the class F = F1 F0 F+1 .
the class Fc = {Fm

Examples of these graph types are illustrated in Figure 1.

v1
v5

v1
v2

vc
v4

v5

v3

v2

vc
v4

v0

v1

v2

v1

v2
u1

v3
u2

F31

v4

v3
S5out

S5in

v3

dP5

v1
u0

v2
u1

v3
u2

v1
u0

F30

v2
u1

v3
u2

u3

F3+1

Figure 1: Examples of some important graph types.
The following observation about polypaths will be used later on.
Proposition 1. Let G be a polypath with at most m sinks and m + 1 sources such that
path-length(G)  k. Then |V (G)|  2mk + 1.
582

fiA Refined View of Causal Graphs and Component Sizes

Proof. There are at most 2m distinct paths from a source to a sink, each of these having at
most k  1 intermediate vertices. Hence |V (G)|  m + (m + 1) + 2m(k  1) = 2mk + 1.
This bound is obviously tight in the case where there are m sinks and m + 1 sources, and
every path from a source to a sink contains exactly k  1 intermediate vertices.

3. Structurally Restricted Planning
The topic of study in this article is causal graphs for planning, but before discussing this
concept we first define the concept of domain-transition graphs (Jonsson & Backstrom,
1998a). Although not used explicitly in any of our results, it is useful for explaining some
of the proofs later in the article. Let  = (V, init, goal, A) be a planning instance. For each
variable v  V , we define the domain-transition graph (DTG) for v as a directed graph
(D(v), E), where for all x, y  D(v), E contains the edge (x, y) if there is some operator
a  A such that post(a)(v) = y and either pre(a)(v) = x or v 6 vars(pre(a)).
The causal graph for a planning instance describes how the variables of the instance
depends on each other, as implicitly defined by the operators.
Definition 2. The causal graph of a planning instance  = (V, init, goal, A) is the directed
graph CG() = (V, E) where E contains the edge (u, v) for every pair of distinct vertices u, v  V such that u  vars(pre(a))  vars(post(a)) and v  vars(post(a)) for some
operator a  A.
The causal graph gives some, but not all, information about the operators. For instance,
if the causal graph is acyclic, then all operators must be unary, i.e. |vars(post)(a)| = 1 for
all operators, since any non-unary operator must necessarily introduce a cycle according
to the definition. However, the presence of cycles does not necessarily mean that there are
non-unary operators. For instance, if both the edges (u, v) and (v, u) are present in the
graph, then this can mean that there is some operator a such that both u  vars(post(a))
and v  vars(post(a)). However, it can also mean that there are two operators a and a0 such
that u  vars(pre(a)), v  vars(post(a)), v  vars(pre(a0 )) and u  vars(post(a0 )), which could
thus both be unary operators. Similarly, the degree of the vertices provides an upper bound
on the number of pre- and postconditions of the operators, but no lower bound. Suppose
there is a vertex u with indegree 2 and incoming edges (v, u) and (w, u). This could mean
that there is some operator a such that u  vars(post(a)) and both v  vars(pre(a)) and
w  vars(pre(a)). However, it can also mean that there are two different operators a and a0
such that v  vars(pre(a)), u  vars(post(a)), w  vars(pre(a0 )) and u  vars(post(a0 )).
The PlanExist problem is extended from planning instances to causal graphs in the
following way. For a class C of directed graphs, PlanExist(C) is the problem of deciding
for an arbitrary planning instance  such that CG()  C, whether  has a solution or
not. That is, the complexity of PlanExist(C) refers to the complexity of the set of planning
instances whose causal graphs are members of C.
There are a number of results in the literature on the computational complexity of
planning for various classes of causal graphs. However, these results usually assume that
the graph class has a restricted structure, e.g. containing only in-stars or only directed
paths. A more general and abstract result is the following theorem.
583

fiBackstrom & Jonsson

Theorem 3. (Chen & Gimenez, 2010, Thm. 3.1) Let C be a class of directed graphs.
If cc-size(C) is bounded, then PlanExist(C) is solvable in polynomial time. If cc-size(C) is
unbounded, then PlanExist(C) is not polynomial-time solvable (unless W[1]  nu-FPT).
While the theorem describes a crisp borderline between tractable and intractable graph
classes, it does so under the assumption that W[1] 6 nu-FPT1 . Both these complexity
classes are from the theory of parameterised complexity and cannot be immediately related
to the usual complexity classes. It is out of the scope of this article to treat parameterised
complexity and we refer the reader to standard textbooks (Downey & Fellows, 1999; Flum
& Grohe, 2006). The result in the theorem is not a parameterised result, however; it is only
the condition that is parameterised, so it suffices to note that the intractability result holds
under a condition that is difficult to relate to other common assumptions, such as P 6= NP.
One of the reasons why Chen and Gimenez were forced to state the theorem in this way was
that a classification into polynomial and NP-hard classes would not have been exhaustive,
since there are graph classes that are NP-intermediate. (A problem is NP-intermediate if
it is neither in P nor NP-complete, unless P = NP.)
This theorem might be viewed as the starting point for the research reported in this
article, where we investigate this problem from the perspective of standard complexity
classes. For instance, NP-hardness can be proved in the case of unbounded components if
adding further restrictions, which we will do in Section 5.

4. Basic Constructions
This section presents some results that are necessary for the theorems later in the article.
The first three results, that planning is NP-hard for in-stars (aka. inverted forks), out-stars
(aka. forks) and directed paths (aka. directed chains), are known from the literature, while
the NP-hardness result for fences is new. We will, however, provide new proofs also for the
in-star and out-star cases. The major reason is that in Section 6 we will need to refer to
reductions that have certain precisely known properties. Furthermore, the original proofs
are only published in a technical report (Domshlak & Dinitz, 2001b) and may thus be hard
to access.
Lemma 4. (Domshlak & Dinitz, 2001a, Thm. 3.IV) PlanExist(Sin ) is NP-hard. This result
holds even when restricted to operators with at most 2 preconditions and 1 postcondition.
Proof. (New proof) Proof by reduction from 3SAT to a class of planning instances with
causal graphs in Sin . The reduction constructs a planning instance where each source in the
causal graph corresponds to one of the variables in the formula and the centre corresponds
to the clauses. The construction is illustrated in Figure 2 and formally defined as follows.
Let F = c1  . . .  cm be an arbitrary 3SAT formula with variables x1 , . . . , xn and clauses
c1 , . . . , cm . Construct a corresponding planning instance F = (V, init, goal, A) as follows:
 V = {vc , v1 , . . . , vn }, where
D(vc ) = {0, . . . , m} and
D(vi ) = {u, f, t}, for all i (1  i  n).
1. The condition can be simplified to W[1] 6 FPT if the class C is recursively enumerable.

584

fiA Refined View of Causal Graphs and Component Sizes

vc
0

1

2

t
u

t
u

t
u

f
v1

m

f
v2

f
vn

Figure 2: The in-star causal graph and the DTGs for the construction in the proof of
Lemma 4.

 init(vi ) = u, for all i (1  i  n), and init(vc ) = 0.
 goal(vc ) = m and goal is otherwise undefined.
 A consists of the following operators:
 For each i (1  i  n), A contains the operators
set-f(i) = hvi = u ; vi = f i and
set-t(i) = hvi = u ; vi = ti.
 For each clause ci = (`1i  `2i  `3i ) and each j (1  j  3), there is some k such
that `ji = xk or `ji = xk , so let A contain either the operator
verify-clause-pos(i, j) = hvc = i  1, vk = t ; vc = ii, if `ji = xk ,
or the operator
verify-clause-neg(i, j) = hvc = i  1, vk = f ; vc = ii, if `ji = xk .
Clearly, the instance F can be constructed in polynomial time and CG(F ) = Snin , so it
remains to prove that F has a solution if and only if F is satisfiable.
Each source variable vi can be changed independently. It starts with the undefined
value u and can be set to either t or f , corresponding to true and false, respectively, for the
corresponding variable xi in F . Once it is set to either t or f , it cannot be changed again.
That is, variables v1 , . . . , vn can be used to choose and commit to a truth assignment for
x1 , . . . , xn . The centre variable vc has one value, i, for each clause ci in F , plus the initial
value 0. It is possible to reach the goal value m from the inital value 0 by stepping through
585

fiBackstrom & Jonsson

all intermediate values in numerical order. For each such step, from i  1 to i, there are
three operators to choose from, corresponding to each of the literals in clause ci . The step
is possible only if one of v1 , . . . , vn is set to a value consistent with one of the literals in ci .
That is, the goal vc = m can be achieved if and only if variables v1 , . . . , vn are set to values
corresponding to a truth assignment for x1 , . . . , xn that satisfies F .
The restricted case (with respect to pre- and post-conditions) is immediate from the
construction above.
The problem is known to be tractable, though, if the domain size of the centre variable is
bounded by a constant (Katz & Domshlak, 2010). Furthermore, the causal graph heuristic
by Helmert (2004) is based on identifying in-star subgraphs of the causal graph, and it
should be noted that he provided a variant of the original proof due to some minor technical
differences in the problem formulations.
Lemma 5. (Domshlak & Dinitz, 2001a, Thm. 3.III) PlanExist(Sout ) is NP-hard. This result
holds even when restricted to operators with at most 1 precondition and 1 postcondition.
Proof. (New proof) Proof by reduction from 3SAT to a class of planning instances with
causal graphs in Sout . The reduction constructs a planning instance where the centre vertex
of the causal graph corresponds to the variables in the formula and each sink corresponds
to one of the clauses. The construction is illustrated in Figure 3 and formally defined as
follows.
v1

u

v2

vm

s

u

s

u

t0

t1

t2

tn

f0

f1

f2

fn

s

vc

Figure 3: The out-star causal graph and the DTGs for the construction in the proof of
Lemma 5.

Let F = c1  . . .  cm be an arbitrary 3SAT formula with variables x1 , . . . , xn and clauses
c1 , . . . , cm . Construct a corresponding planning instance F = (V, init, goal, A) as follows:
586

fiA Refined View of Causal Graphs and Component Sizes

 V = {vc , v1 , . . . , vm }, where
D(vc ) = {f0 , . . . , fn , t0 , . . . , tn } and
D(vi ) = {u, s}, for all i (1  i  m).
 init(vi ) = u, for all i (1  i  m), and init(vc ) = f0 .
 goal(vi ) = s, for all i (1  i  m), and goal(vc ) is undefined.
 A consists of the following operators:
 For each i (1  i  n), A contains the operators
step-c(fi1 , fi ) = hvc = fi1 ; vc = fi i,
step-c(fi1 , ti ) = hvc = fi1 ; vc = ti i,
step-c(ti1 , fi ) = hvc = ti1 ; vc = fi i and
step-c(ti1 , ti ) = hvc = ti1 ; vc = ti i.
 For each clause ci = (`1i  `2i  `3i ) and each j (1  j  3), there is a k such that
`ji = xk or `ji = xk , so let A contain either the operator
verify-clause-pos(i, j) = hvc = tk ; vi = si, if `ji = xk ,
or the operator
verify-clause-neg(i, j) = hvc = fk ; vi = si, if `ji = xk .
Clearly, the instance F can be constructed in polynomial time and CG(F ) = Snout , so it
remains to prove that F has a solution if and only if F is satisfiable.
Variable vc can be changed independently and it has two values, ti and fi , for each
variable xi in F , corresponding to the possible truth values for xi . In addition there is an
initial value f0 (and a dummy value t0 in order to simplify the formal definition). Both the
values tn and fn are reachable from the initial value f0 , and each such plan will correspond
to a path f0 , z1 , z2 , . . . , zn where each zi is either ti or fi . That is, vc must pass either value
ti or fi , but not both, for each i. Hence, any such path will correspond to a truth assignment
for the variables x1 , . . . , xn in F . For each clause ci in F , there is a corresponding variable
vi that can change value from the initial value u, unsatisfied, to the goal value s, satisfied.
Each vi has three operators to do this, one for each literal in ci . That is, if ci contains
a literal xk (or xk ) then vi can change value from u to s while vc has value tk (or fk ).
Hence, the goal v1 = . . . = vm = s can be achieved if and only if there is a path for vc
that corresponds to a truth assignment for x1 , . . . , xn that satisfies F . (Note, though, that
vc must not always follow a path all the way to fn or tn since a partial assignment may
sometimes be sufficient to prove satisfiability.)
The restricted case (with respect to pre- and post-conditions) is immediate from the
construction above.
The problem is known to be tractable, though, if the domain size of the centre variable is
bounded by a constant (Katz & Keyder, 2012).
The following result on planning with directed-path causal graphs is also known from
the literature.
Lemma 6. (Gimenez & Jonsson, 2009, Prop. 5.5) PlanExist(dP) is NP-hard, even when
all variables have domain size 5 and the operators have at most 2 preconditions and 1 postcondition.
587

fiBackstrom & Jonsson

We refer to Gimenez and Jonsson for the proof. However, we will implicitly use their
proof later in this article so there are a few important observations to make about it. The
reduction is from SAT and, thus, works also as a reduction from 3SAT. Furthermore, the
reduction transforms a formula with n variables and m clauses to a planning instance with
(2m + 4)n variables. As a final remark, this problem is known to be tractable if all variables
have a domain of size 2 (Domshlak & Dinitz, 2001a).
While the three previous results are known in the literature, the following result is new
to the best of our knowledge.
Lemma 7. PlanExist(F+1 ) is NP-hard. This result holds even when restricted to operators
with at most 2 preconditions and 1 postcondition.
Proof. Proof by reduction from 3SAT to a class of planning instances with causal graphs
in F+1 .
The reduction constructs a planning instance where each sink of the causal graph corresponds to one of the clauses in the formula, while each source corresponds to all variables.
Furthermore, the source variables are synchronized to have the same behaviour. The construction is illustrated in Figure 4 and formally defined as follows.
Let F = c1  . . .  cm be an arbitrary 3SAT formula with variables x1 , . . . , xn and clauses
c1 , . . . , cm . Construct a corresponding planning instance F as follows:
 V = {u0 , . . . , um , v1 , . . . , vm }, where
D(ui ) = {f0 , . . . , fn , t0 , . . . , tn }, for all i (0  i  m), and
u , tu , . . . , tu , f s , . . . f s , ts , . . . , ts , s}, for all i (1  i  m).
D(vi ) = {f0u , . . . , fm
m 0
m 0
m
0
 init(ui ) = f0 , for all i (0  i  m), and init(vi ) = f0u , for all i (1  i  m).
 goal(vi ) = s, for all i (1  i  m), and goal is otherwise undefined.
 Let A consist of the following operators:
 For all i, j (1  i  n, 0  j  m), A contains the operators
step-x(j, fi1 , fi ) = huj = fi1 ; uj = fi i,
step-x(j, fi1 , ti ) = huj = fi1 ; uj = ti i,
step-x(j, ti1 , fi ) = huj = ti1 ; uj = fi i and
step-x(j, ti1 , ti ) = huj = ti1 ; uj = ti i.
 For all i, j, (1  i  n, 1  j  m), A contains the operators
u
u , f u ) = hv = f u , u
step-clause-u(j, fi1
j
i1 j1 = fi , uj = fi ; vj = fi i,
i
u
u
u
u
step-clause-u(j, fi1 , ti ) = hvj = fi1 , uj1 = ti , uj = ti ; vj = ti i,
step-clause-u(j, tui1 , fiu ) = hvj = tui1 , uj1 = fi , uj = fi ; vj = fiu i,
step-clause-u(j, tui1 , tui ) = hvj = tui1 , uj1 = ti , uj = ti ; vj = tui i,
s , f s ) = hv = f s , u
s
step-clause-s(j, fi1
j
i
i1 j1 = fi , uj = fi ; vj = fi i,
s , ts ) = hv = f s , u
s
step-clause-s(j, fi1
j
i
i1 j1 = ti , uj = ti ; vj = ti i,
s
s
s
step-clause-s(j, ti1 , fi ) = hvj = ti1 , uj1 = fi , uj = fi ; vj = fis i,
step-clause-s(j, tsi1 , tsi ) = hvj = tsi1 , uj1 = ti , uj = ti ; vj = tsi i,
 For each j (1  j  m), A contains the operators
finalize-clause-f(j) = hvj = fns ; vj = si and
finalize-clause-t(j) = hvj = tsn ; vj = si.
588

fiA Refined View of Causal Graphs and Component Sizes

vi1

vi+1

vi
tu0

tu1

tu2

tun

f0u

f1u

f2u

fnu

x1  ci

x2  ci

ts0

ts1

ts2

tsn

f0s

f1s

f2s

fns

s

t0

t1

t2

tn

t0

t1

t2

tn

f0

f1

f2

fn

f0

f1

f2

fn

ui1

ui

Figure 4: The fence causal graph and the DTGs for the construction in the proof of
Lemma 7. (This example assumes that clause ci contains the literals x1 and x2 ).

 For each clause ci = (`1i  `2i  `3i ) and for each j (1  j  3), there is a k such
that `ji = xk or `ji = xk so let A contain either the operator
verify-pos(i, j) = hvi = tuk ; vi = tsk i, if `ji = xk ,
or the operator
verify-neg(i, j) = hvi = fku ; vi = fks i, if `ji = xk .
+1 . Hence,
Clearly, the instance F can be constructed in polynomial time and CG(F ) = Fm
it remains to prove that F has a solution if and only if F is satisfiable.
First consider only variables ui and vi , for some i. The construction of the domain and
the operators for ui is identical to the one for vc in the proof of Lemma 5, i.e. there is a
directed path from value f0 to fn or tn for every possible truth assignment for the variables
x1 , . . . , xn in F . Variable vi , corresponds to clause ci and contains two copies of the DTG for
ui , where the values differ only in the extra superscript, u or s. The latter copy is extended
with the additional value s, denoting that the clause has been satisfied. There are operators
that allows vi to mimic the behaviour of ui ; it can follow the corresponding path in either
of its two copies. Furthermore, for each of the three literals in ci there is an operator that

589

fiBackstrom & Jonsson

makes it possible to move from value zku to value zks if value zk of ui is consistent with this
s or ts in order to reach the goal value
literal. Since vi starts at f0u and must reach either fm
m
s, it is necessary for vi to make such a transition for one of the literals in ci . That is, if ui
follows the path f0 , z1 , . . . , zn then vi must follow the path f0u , z1u , . . . , zku , zks , . . . , zns , s, for
some k such that xk occurs in a literal in ci and zk is a satisfying truth value for this literal.
Now consider also variable ui1 . Since each operator that affects the value of vi either
has the same precondition on both ui1 and ui or no precondition on either, it follows that
ui1 and ui must both choose the same path if vi is to reach its goal. Since every variable vj
forces synchronization of its adjacent variables uj1 and uj in this manner, it follows that
all of u0 , . . . , um must choose exactly the same path for any plan that is a solution. It thus
follows from this and from the argument for ui and vi that the goal v1 = . . . = vm = s can
be achieved if and only if there is a path that all of u0 , . . . , um can choose such that this
path corresponds to a satisfying truth assignment for F .
For the restriction, we first note that it is immediate from the construction that operators
with 3 preconditions and 1 postcondition are sufficient. To see that 2 preconditions are
sufficient, consider the following variation on the construction. Each step-clause-u and stepclause-t operator is replaced with two operators as follows. As an example, consider an
u , tu ). First introduce an extra value f tu in D(v ). Then replace
operator step-clause-u(j, fi1
j
i
i
the operator with two new operators
u , f tu ) = hv = f u , u
u
step-clause-u(j, fi1
j
i
i1 j1 = ti ; vj = f ti i and
step-clause-u(j, f tui , tui ) = hvj = f tui , uj = ti ; vj = tui i.
u
Consider the step in the DTG for vj from fi1
to tui . In the original construction, this is
u , tu ), which requires that both u
done by the single operator step-clause-u(j, fi1
j1 and uj
i
u
have value ti . The modified construction instead requires two steps, first a step from fi1
u
u
to the new intermediate value f ti and then a step from this value to ti . The previous
conjunctive constraint that uj1 = uj = ti is replaced by a sequential constraint that first
uj1 = ti and then uj = ti . Although it is technically possible for uj1 to have moved on
to a new value when the second step is taken, this does not matter; both uj1 and uj must
still choose exactly the same path in their respective DTGs.
Corollary 8. PlanExist(F1 ), PlanExist(F0 ) and PlanExist(F) are NP-hard.
Proof. Neither of the two outer source vertices, u0 and um , are necessary in the construction
in the previous proof. Hence, by omitting either or both of these the reduction works also
for F1 and F0 . Finally, PlanExist(F) is NP-hard since F+1  F.
We now have all the basic results necessary for the main theorems of the following
two sections.

5. Graph Classes and Closure Properties
Like most other results in the literature, the results in the previous section are about classes
consisting of some particular graph type, like the class Sin of all in-stars or the class F of
all fences. This section will depart from this and instead study graph classes with certain
closure properties. We will first discuss the standard concepts of subgraph closure and minor
closure, finding that the first does not contain all the graphs we need while the latter results
590

fiA Refined View of Causal Graphs and Component Sizes

in a set with too many graphs. For that reason, we will define a new concept, SP-closure,
which is incomparable with subgraph closure but is a subset of minor closure. We will then
show that this closure concept defines a borderline between the non-NP-hard graph classes
and large number of useful NP-hard classes.
5.1 Subgraph Closure and Minor Closure
Suppose C is a class of graphs which is closed under taking subgraphs. Then for every graph
G in C it is the case that every subgraph H of G must also be in C. Subgraph closure is
not sufficient for our purposes, though. For instance, a subgraph of a polypath will always
be either a polypath or a graph where every weakly connected component is a polypath.
However, a polypath need not have any subgraphs that are fences of more than trivial size.
We will need a closure property that guarantees that if C contains a polypath with m sinks,
then it also contains a fence with m sinks. An obvious candidate for this is the concept of
minor-closure, which is a superset of the subgraph-closure. The concepts of graph minors
and minor-closure has rapidly evolved into a very important and useful research area in
mathematical as well as computational graph theory (Lovasz, 2005; Mohar, 2006).
In order to define graph minors we first need the concept of edge contraction, which is
commonly defined as follows, although other definitions occur in the literature.
Definition 9. Let G = (V, E) be a directed graph and let e = (u, v)  E be an edge such
that u 6= v. Then the contraction of e in G results in a new graph G0 = (V 0 , E 0 ), such that
 V 0 = (V \ {u, v})  {w} and
 E 0 = {(f (x), f (y)) | (x, y)  E, (x, y) 6= (u, v) and (x, y) 6= (v, u)},
where w is a new vertex, not in V , and the function f : V  V 0 is defined such that
f (u) = f (v) = w and otherwise f (x) = x.
That is, when an edge (u, v) is contracted, the two vertices u and v are replaced with
a single new vertex w and all edges that were previously incident with either u or v are
redirected to be incident with w. Figure 5 shows an example of edge contraction. We say
that a graph H is a contraction of another graph G if H can result from contracting zero
or more edges in G.
The concept of graph minors can now be defined as follows.
Definition 10. A directed graph H is a minor of a directed graph G if H is isomorphic to
a graph that can be obtained by zero or more edge contractions of a subgraph of G.
An example is illustrated in Figure 6. The graph G in the figure is a weakly connected
directed graph, which also happens to be a polypath. If vertex v9 is removed from G,
then the restriction to the remaining vertices is still a weakly connected graph which is a
subgraph of G. Removing also v4 results in the graph H, which consists of two weakly
connected components H1 and H2 . All of H, H1 and H2 are subgraphs of G, but they are
also minors of G, since a subgraph is a minor, by definition. Contracting the edge (v1 , v2 )
in H1 results in the graph M1 , where w1 is the new vertex replacing v1 and v2 . Similarly,
contracting the edge (v8 , v7 ) in H2 results in M2 . The graph M1 is a minor of G since it is
591

fiBackstrom & Jonsson

v9

v9
v10

v8

v10
v8

v2
v7

v5

v7

v1

v6

v5

w

v6

v3

v4

a) A graph G

v3

v4

b) The result of contracting edge (v1 , v2 ) in G.

Figure 5: Edge contraction.

the result of an edge contraction in the subgraph H1 of G and the graph M2 is analogously
a minor of G too. Also the graph M , consisting of the two components M1 and M2 is a
minor of G, since it is the result of two contractions in the subgraph H of G. While the
graphs H, H1 and H2 are both subgraphs and minors of G, the graphs M , M1 and M2 are
only minors of G, not subgraphs.

v6
v3
v2

v5
v4

v6
v7

v3
v8

v1

a) A polypath

v7

v2
v9

G

v5

v3
v8

w1

v5
M1

v1
H1

v6
w2
M2

H2

b) A subgraph H of G
(where H = H1  H2 )

c) A minor M of G
(where M = M1  M2 )

Figure 6: Subgraphs and minors.

A trivial example of a minor-closed class is the class of all graphs, which is minor-closed
since it contains all graphs and every minor of a graph is itself a graph. More interestingly,
many commonly studied graph types result in minor-closed classes. For instance, the class
Sin of all in-stars is minor-closed, as is the class Sout of all out-stars and the class dP of all
592

fiA Refined View of Causal Graphs and Component Sizes

directed paths. Furthermore, a weakly connected minor of a polypath is a polypath and a
weakly connected minor of a polytree is a polytree. As an illustration, once again consider
Figure 6. The graph G is a polypath, and the weakly connected graphs H1 , H2 , M1 and
M2 are all minors of G, but they are also polypaths. In fact, M1 and M2 are also fences.
Note though, that neither H nor M is a polypath, since they both consist of more than one
weakly connected component. It is worth noting, however, that the class F of all fences is
not minor-closed although every fence is a polypath; a weakly connected minor of a fence
must be a polypath, but it is not necessarily a fence.
Requiring minor-closed graph classes is, however, overly strong. For instance, it would
be sufficient to require that for every graph G  C, also every weakly connected minor of G
is in C. That is, in the example in Figure 6 we would require that H1 , H2 , M1 and M2 are
all in C if G is in C, but we would not require that also H and M are in C. This is both
reasonable and desirable in the context of causal graphs. If the causal graph of a planning
instance consists of two or more weakly connected components, then these components
correspond to entirely independent subinstances that can be solved separately.
Furthermore, certain natural restrictions do not mix well with minor-closed classes.
Consider, for instance, the example in Figure 7, with an acyclic graph G = (V, E), where
V = {v1 , v2 , v3 , v4 } and E = {(v1 , v2 ), (v2 , v3 ), (v3 , v4 ), (v1 , v4 )}. If we contract the edge
(v1 , v4 ) to a new vertex w we get a cycle graph on the vertices w, v2 , v3 . That is, a class of
acyclic graphs is not minor-closed in general, which is problematic considering the importance of acyclic causal graphs.

v1

v2

v4

v3

v2
w

a) An acyclic graph G

v3
b) The contraction of (v1 , v4 ) in G.

Figure 7: Contracting an edge in an acyclic graph can result in a cycle.

5.2 SP-Closed Graph Classes
In order to avoid problems with acyclicity (and other similar problems) and to avoid defining
special variants of the contraction and minor concepts, we instead identify a set of minimal
requirements that a closure must satisfy in order to imply NP-hardness for the PlanExist
problem. We will focus on one such set of restrictions, defining a concept we refer to as
SP-closure (where SP denotes that the set is closed under stars and polypaths).
Definition 11. Let G and H be two directed graphs. Then H is an SP-graph of G if H is
weakly connected and either of the following holds:
1. H is an in-star that is a subgraph of G,
593

fiBackstrom & Jonsson

2. H is an out-star that is a subgraph of G or
3. H can be obtained by zero or more contractions of some polypath G0 such that G0 is
a subgraph of G.
A class C of graphs is SP-closed if it contains every SP-graph of every graph G  C.
SP-closure has a number of interesting properties, including the following:
Proposition 12. Let G and H be directed graphs and let C be a class of directed graphs.
1. If G is a polypath, then every SP-graph of G is a polypath.
2. Every SP-graph of G is acyclic.
3. If H is an SP-graph of G, then H is a minor of G.
4. If C is minor-closed, then C is SP-closed.
Proof. 1) Suppose G is a polypath. Obviously, G cannot contain an in-star or out-star
with higher degree than two, and any such star is also a polypath. Hence, we only need to
consider the third case in the definition. We note that any weakly connected subgraph G0 of
G must also be a polypath, and that doing contractions on a polypath results in a polypath.
2) Immediate since in-stars, out-stars and polypaths are all acyclic and contracting edges
cannot introduce a cycle in any of these cases.
3) Immediate from the definitions of minors and SP-graphs.
4) Immediate from 3.
This proposition says that it makes sense to talk about SP-closed classes of polypaths
and SP-closed classes of acyclic graphs. It also says that SP-closure and minor-closure
are comparable concepts; the SP-closure of a class is a subset of the minor-closure of the
same class.
We can now prove the following result about SP-closed classes of polypaths, which we
need for the main theorem.
Lemma 13. Let C be an SP-closed class of polypaths. If cc-size(C) is unbounded, then
PlanExist(C) is NP-hard. This result holds even when restricted to operators with at most
2 preconditions and 1 postcondition.
Proof. Proof by cases depending on whether the directed path length of C is bounded or not.
Case 1: Suppose that path-length(C) is unbounded. Let n > 1 be an arbitrary integer.
Then there must be some graph G  C such that G contains a subgraph H that is a directed
path graph and V (H) = n. Obviously, H is an SP graph of G, since a directed path is
also a polypath. It follows that H  C since C is SP-closed. Furthermore, H ' dPn so
NP-hardness of PlanExist(C) follows from Lemma 6, since n was choosen arbitrarily.
Case 2: Instead suppose that path-length(C)  k for some constant k  0. Let n > 1 be
an arbitrary integer. Since all graphs in C are polypaths and cc-size(C) is unbounded, there
must be some polypath G  C such that V (G)  n. It thus follows from the assumption
and Proposition 1 that G must have at least m sinks and m + 1 sources, for some m such
594

fiA Refined View of Causal Graphs and Component Sizes

that V (G)  2mk + 1. There must, thus, be some subgraph G0 of G that is a polypath with
exactly m sinks and m + 1 sources (i.e. G0 is weakly connected) and there must, thus, also
+1 .
be a graph H that can be obtained by zero or more contractions of G0 such that H ' Fm
It follows that H  C since C is SP-closed. NP-hardness of PlanExist(C) thus follows from
Lemma 7, since n was choosen arbitrarily and k is constant.
To see that the result holds even if the operators under consideration have at most 2
preconditions and 1 postcondition, simply note that this restriction holds for all reductions
used in the underlying NP-hardness proofs in Section 4.
Chen and Gimenez (2010, Thm. 3.19) proved a similar result: If C is a class of polypaths2
with unbounded components and unbounded number of sources, then PlanExist(C) is not
polynomial-time solvable unless W[1]  nu-FPT.
In order to prove the main result of this section, we also need the Moore bound (Biggs,
1993, p. 180), which is stated as follows: for an arbitrary connected undirected graph G,
the maximum number of vertices is
|V (G)|  1 + d

k1
X

(d  1)i ,

(1)

i=0

where d = deg(G) and k = path-length(G).
We can now prove that under the additional restriction that graph classes are SPclosed, we can avoid NP-intermediate problems and prove NP-hardness for graph classes
with unbounded components.
Theorem 14. Let C be an SP-closed class of directed graphs. If cc-size(C) is unbounded,
then PlanExist(C) is NP-hard. This result holds even when restricted to operators with at
most 2 preconditions and 1 postcondition and all graphs in C are acyclic.
Proof. First suppose there is some constant k such that in-deg(C)  k, out-deg(C)  k and
upath-length(C)  k. Consider an arbitrary graph G  C. Obviously, deg(U (G))  2k and
path-length(U (G))  k, P
so it follows from the Moore bound that no component in U (G) can
i
have more than 1 + 2k k1
i=0 (2k  1) vertices. However, since cc-size(G) = cc-size(U (G))
and G was choosen arbitrarily, it follows that cc-size(C) is bounded. This contradicts the
assumption so at least one of in-deg(C), out-deg(C) and upath-length(C) is unbounded. The
remainder of the proof is by these three (possibly overlapping) cases.
Case 1: Suppose that in-deg(C) is unbounded. Let n > 0 be an arbitrary integer. Then
there must be some graph G  C containing a vertex with indegree n or more, so there must
also be a subgraph H of G such that H ' Snin . Hence, H  C since C is SP-closed. It thus
follows from Lemma 4 that PlanExist(C) is NP-hard, since n was choosen arbitrarily.
Case 2: Suppose that out-deg(C) is unbounded. This case is analogous to the previous
one, but using Lemma 5 instead of Lemma 4.
Case 3: Suppose that upath-length(C) is unbounded. Let n > 0 be an arbitrary integer.
Then there must be some graph G  C such that U (G) contains a path of length n, and there
must, thus, also be a subgraph H of G such that H is a polypath of length n. Obviously, H
2. Chen and Gimenez use the term source-sink configuration for polypath.

595

fiBackstrom & Jonsson

is an SP-graph of G (doing zero contractions) so H  C since C is SP-closed. It thus follows
from Lemma 13 that PlanExist(C) is NP-hard, since n was choosen arbitrarily.
To see that the result holds even if the operators under consideration have at most 2
preconditions and 1 postcondition, simply note that this restriction holds for all reductions
used in the underlying NP-hardness proofs in Section 4. Similarly, the acyclicity restriction
holds since the result is based only on in-stars, out-stars and polypaths, which are all
acyclic graphs.
This theorem is somewhat more restricted than the one by Chen and Gimenez since it
requires the additional constraint that C is SP-closed. On the other hand, it demonstrates
that SP-closure is a sufficient condition to avoid graph classes such that PlanExist is NPintermediate and, thus, sharpen the result to NP-hardness. It should be noted, though,
that this is not an exact characterization of all graph classes that are NP-hard for PlanExist.
There are other such graph classes, but SP-closure captures a large number of interesting
graph classes. For instance, the class of all acyclic graphs is SP-closed (recall that this class
is not minor-closed), although not every subclass of it is SP-closed. As an opposite example,
any non-empty class that does not contain a single acyclic graph cannot be SP-closed.

6. Beyond SP-Closed Graph Classes
This section is divided into three parts. We first discuss why the previous results, as well as
most other similar NP-hardness results in the literature, are problematic, which motivates
us to switch over to non-uniform complexity theory. The second part contains a number of
preparatory results that are required for the main theorem in the third part.
6.1 Why NP-Hardness is Not Enough
We refer to a planning problem as generic if it has instances of varying size, depending on
one or more parameters. An archetypical example is the blocks world, where the natural
parameter is the number of blocks. For a particular encoding and a specified number of
blocks, the variables and operators will be the same whatever the inital state and goal is.
That is, if we fix the encoding then we get a planning frame n = (Vn , An ) for every number,
n, of blocks. That is, n is the same for all instances with n blocks and is thus a function of
n. All instances (Vn , init, goal, An ) with n blocks will be instantiations of n with different
init and goal components but with the same Vn and An components. An instance can thus
be specified with three unique parameters, n, init and goal, where only the first parameter,
n, affects the size of the instance. Furthermore, the causal graph for an instance depends
only on the variables and the operators, which means that all instantiations of a frame n
have the same causal graph, which we denote CG(n ). The class of causal graphs for blocks
world instances will be D = {CG(1 ), CG(2 ), CG(3 ), . . .}, although 1 , 2 , 3 , . . ., and
thus also D, will differ depending on the encoding.
It is often possible to analyse the complexity of a particular generic planning problem.
Examples of this are the complexity of blocks-world planning (Gupta & Nau, 1992) and
the complexity of various problems from the International Planning Competitions (IPC)
(Helmert, 2003, 2006b). In the context of this article, though, we are rather interested
in the complexity of the class of causal graphs corresponding to a generic problem, than
596

fiA Refined View of Causal Graphs and Component Sizes

the complexity of the specific problem itself. Suppose that a class D of causal graphs
happens to be a subset of some class C of graphs such that we know that PlanExist(C) is
tractable. Then we can infer that also PlanExist(D) is tractable, and thus also that all
generic planning problems with causal graphs in D are tractable. However, in order to
prove that PlanExist(D) is NP-hard (or hard for some other complexity class) we would
have to prove that there is some class C of graphs such that PlanExist(C) is NP-hard and C
is a subset of D. Finding such a class C may not be trivial, though.
One problem is that the encoding can have a large influence on how densely or sparsely
the causal graphs occur with respect to size. Consider, for instance, blocks world encodings
with multi-valued variables and with boolean variables respectively. A typical encoding with
multi-valued variables will use one variable for the status of the hand and two variables for
each block, one for the position of the block and one to flag whether the block is clear or
not. That is, such encodings will use 2n + 1 variables for an n-block frame. An encoding
with boolean variables, on the other hand, will typically represent the block position with
a number of boolean variables, one for each other block that a block can be on. A boolean
encoding will thus use n2 + 1 variables for an n-block frame. While D will contain a graph
for every odd number of vertices in the first case, it will be increasingly sparse in the second
case. The class D of causal graphs for a generic planning problem will, thus, typically not
be SP-closed, or even closed under taking subsets. Furthermore, since D will typically not
contain a member for every possible number of vertices, it cannot possibly contain any of
the known NP-hard sets Sin , Sout , dP etc. as a subset. Hence, in order to prove that
a class D of causal graphs is hard for NP (or some other complexity class), it will often
be necessary to make a dedicated proof for D. This is often doable, however. A generic
planning problem has a corresponding function f that takes a parameter value n, e.g. the
number of blocks in blocks world, such that f (n) = n . If f is furthermore polynomialtime computable in the value of n, which will often be the case, then also the corresponding
causal graph, CG(n ), is polynomial-time computable. However, even if this can be done
for many generic planning problems, it will be a specific proof for every specific encoding of
every particular generic planning problem. The same holds for particular classes of causal
graphs; every specific class will typically require its own dedicated proof.
In order to get around these problems and to be able to prove a more general result that
does not depend on the specific planning problems or causal graphs, we switch over to nonuniform complexity. This makes it possible to prove more powerful results, while retaining
natural connections with the ordinary complexity classes. The basic vehicle for proving nonuniform complexity results is the advice-taking Turing machine, which is defined as follows.
Definition 15. An advice-taking Turing machine M has an associated sequence of advice
strings A0 , A1 , A2 , . . ., a special advice tape and an advice function A, from the natural
numbers to the advice sequence, s.t. A(n) = An . On input x the advice tape is immediately
loaded with A(||x||). After that M continues like an ordinary Turing machine, except that
it also has access to the advice written on the advice tape.
If there exists a polynomial p s.t. ||A(n)||  p(n), for all n > 0, then M is said to use
polynomial advice. The complexity class P/poly is the set of all decision problems that can
be solved on some advice-taking TM that runs in polynomial time using polynomial advice.
597

fiBackstrom & Jonsson

Note that the advice depends only on the size of the input, not its content, and need
not even be computable. Somewhat simplistically, an advice-taking Turing machine is a
machine that has an infinite data-base with constant access time. However, for each input
size there is only a polynomial amount of information while there might be an exponential
number of instances sharing this information. The power of polynomial advice is thus still
somewhat limited and useful relationships are known about how the non-uniform complexity
classes relate to the standard ones are known. One such result is the following.
Theorem 16. (Karp & Lipton, 1980, Thm. 6.1) If NP  P/poly, then the polynomial
hierarchy collapses to the second level.
6.2 Preparatory Results
Before carrying on to the main theorem of this section, we need a few auxiliary results. We
first show that if a planning instance has a causal graph G that is a subgraph of some graph
H, then the instance can be extended to an equivalent instance with H as causal graph.
Lemma 17. Let  be a planning instance and let G be a directed graph such that CG()
is a subgraph of G. Then there is a planning instance G such that
 G can be constructed from  in polynomial time,
 CG(G ) = G and
 G has a solution if and only if  has a solution.
Furthermore, G has the same maximum number of pre- and postconditions for its operators
as  (or one more if this value is zero in ).
Proof. Let  = (V, init, goal, A) be a planning instance and let CG() = (V, E). Let
G = (VG , EG ) be a directed graph such that CG() is a subgraph of G. Let U = VG \ V .
Construct a planning instance G = (VG , initG , goalG , AG ) as follows:
 DG (u) = {0, 1}, for all u  U , and
DG (v) = D(v)  {?}, for all v  V , (where ? is a new value not in D(v)).
 initG (v) = init(v), for all v  V , and
initG (u) = 0, for all u  U .
 goalG (v) = goal(v), for all v  V , and
goalG (u) is undefined for all u  U .
 Let AG consist of the following operators:
 Let AG contain all a  A.
 For each edge (x, v)  EG \ E such that x  VG and v  V , let AG also contain
an operator star(x, v) = hx = 0 ; v = ?i.
 For each edge (x, u)  EG such that x  VG and u  U , let AG also contain
an operator set(x, u) = hx = init(x) ; u = 1i.
598

fiA Refined View of Causal Graphs and Component Sizes

Obviously G can be constructed in polynomial time and CG(G ) = G, so it remains
to prove that G has a solution if and only if  has a solution.
Suppose P = a1 , . . . , an is a plan for . Then P is also a plan for G since goalG (u) is
undefined for all u  U and a1 , . . . , an  AG . To the contrary, suppose P = a1 , . . . , an is a
plan for G . For each operator ai in P , there are three cases: (1) ai  A, (2) ai is a set
operator or (3) ai is a star operator. In case 2, operator ai serves no purpose since it only
modifies some variable in U , which has an undefined goal value. In case 3, operator ai sets
some variable v  V to ? and has no effect on any other variables. If goalG (v) is undefined,
then ai serves no purpose. Otherwise there must be some operator aj , j > i, such that aj
can change v from ? to some value in D(v), i.e. ai serves no purpose in this case either. It
follows that the operator sequence P 0 obtained from P by removing all operators that are
not in A is also a plan for G . Furthermore, since P 0 contains only operators from A it is
also a plan for . It follows that  has a plan if and only if G has a plan.
This construction increases the maximum domain size by one but has very little effect
on the maximum number of pre- and postconditions. This is suitable for our purpose, since
we do not consider the influence of domain sizes in this article. Other constructions are
possible if we want to balance the various factors differently.
In the proof of the forthcoming theorem we will also do the opposite of taking graph
minors, that is, starting from a minor G of some target graph H we will extend G to H.
In order to do so, we need an operation similar to the opposite of edge contraction. This is
satisfied by a graph operation known as edge subdivision.
Definition 18. Let G = (V, E) be a directed graph and let (u, v)  E be an edge such that
u 6= v. Then the subdivision of (u, v) in G is a graph G0 = (V  {w}, E 0 ) where w is a new
vertex and E 0 = (E \ {(u, v)})  {(u, w), (w, v)}.
Although one might consider other definitions, e.g. in the case where both (u, v) and
(v, u) are in E, this one is sufficient for our purpose and it follows the usual extension to
directed graphs (cf., Kuhn, Osthus, & Young, 2008). Usually an operation called smoothing
is considered as the inverse of edge subdivision. However, smoothing can be viewed as a
restricted case of edge contraction, so it is reasonable to think of edge subdivision as a sort
of inverse of edge contraction. An example of edge subdivision is illustrated in Figure 8.
We further note that just like an edge contraction of a polypath is a polypath, also an edge
subdivision of a polypath is a polypath.
We also need an operation on planning instances corresponding to edge subdivision
in their causal graphs. For that purpose, we need a concept of variable substitution for
operators. We denote the substitution of a variable w for a variable v in a partial state s
with a[v/w], defined as:

if x = w,
 s(v),
s(x),
if x  vars(s) \ {v, w},
s[v/w](x) =

undefined, otherwise.
If a is an operator, then the operator a0 = a[v/w] is defined such that pre(a0 ) = pre(a)[v/w]
and post(a0 ) = post(a)[v/w].
599

fiBackstrom & Jonsson

v9

v9
v10

v8

v10
v8

v2
v7

v5

v7

v1

v6
v3

a) A graph G

v2

v6
v4

w

v5

v1
v3

v4

b) The result of subdividing edge (v1 , v2 ) in G.

Figure 8: Edge subdivision.

We now have the necessary concepts for modifying an arbitrary planning instance such
that the result corresponds to subdividing an edge in the causal graph of the instance.
However, we will only need to do this for instances where the causal graph is a polypath.
Before proving that this can be done, we first need the following lemma, which states a
certain reordering property for plans when the causal graph is a polypath. If we choose an
arbitrary vertex v in a polypath G and remove v from G, then G falls apart into two weakly
connected components C1 and C2 . In other words, the vertices of G can be partitioned
into three sets C0 , C1 and C2 such that C0 = {v} and there is no edge directly between
a vertex in C1 and a vertex in C2 . It then follows from the definition of causal graphs
that no operator that changes some variable in C1 can have a precondition on a variable
in C2 and vice versa. The following lemma utilises this fact to prove that any sequence
of operators that does not change variable v can be reordered such that all operators that
change variables in C1 come before all operators that change variables in C2 .
Lemma 19. Let  = (V, init, goal, A) be a planning instance such that G = CG() is a
polypath. Let v be an arbitrary variable in V , let C0 = {v} and let C1 , C2  V be the two
(possibly empty) weakly connected components of G that result if vertex v is removed from
G. Define Ai = {a  A | vars(post(a))  Ci } for all i (0  i  2). Let P be a plan for .
Let P1 , P2 and Q be operator sequences such that P = P1 , Q, P2 and Q contains no operator
from A0 . Let Q1 be the subsequence of Q containing only operators from A1 and let Q2
be the subsequence of Q containing only operators from A2 . Then P1 , Q1 , Q2 , P2 is a plan
for .
Proof. Assume C0 , C1 and C2 as defined in the lemma and recall that C0 = {v}. First
note that G is acyclic since it is a polypath, so all operators in A are unary. It follows
that {A0 , A1 , A2 } is a partition of A and, thus, that A0  A1  A2 = A. Let s0 = init[P1 ].
Obviously, (vars(pre(a))C2 = (vars(post(a))C2 =  for all a in Q1 and (vars(pre(a))C1 =
(vars(post(a))  C1 =  for all a in Q2 , i.e. for any state s it holds that s[a]  C2 = s  C2 for
all a in Q1 and that s[a]  C1 = s  C1 for all a in Q2 . Furthermore, for any state s it holds
600

fiA Refined View of Causal Graphs and Component Sizes

that s[a](v) = s(v) for all a in Q, since a 6 A0 . It follows that s0 [Q]  C1 = s0 [Q1 ]  C1 and
s0 [Q]  C2 = s0 [Q2 ]  C2 . Hence,
s0 [Q1 , Q2 ]  C0 = s0 [Q]  C0 ,
s0 [Q1 , Q2 ]  C1 = s0 [Q1 ]  C1 = s0 [Q]  C1 and
s0 [Q1 , Q2 ]  C2 = s0 [Q2 ]  C2 = s0 [Q]  C2 .
That is, s0 [Q1 , Q2 ] = s0 [Q] and it follows that also P1 , Q1 , Q2 , P2 is a plan for .
We now prove that if  is a planning instance such that CG() is a polypath, then we
can subdivide any edge in CG() and create a planning instance 0 such that CG(0 ) is
this subdivision of CG() and 0 is solvable if and only if  is solvable.
Lemma 20. Let  be a planning instance such that CG() is a polypath and let e be an
edge in CG(). Then there is a planning instance 0 such that
 0 can be constructed from  in polynomial time,
 CG(0 ) is an edge subdivision of e in CG() and
 0 has a solution if and only if  has a solution.
Proof. Let  = (V, init, goal, A) be a planning instance such that CG() is a polypath and
let e = (u, v) be an edge in CG(). Construct a new instance 0 = (V 0 , init0 , goal0 , A0 )
as follows:
 V 0 = V  {w}, where D(w) = D(u) and w 6 V .
 init0 (v) = init(v), for all v  V , and
init0 (w) = init(u).
 goal0 = goal.
 Let A0 consist of the following groups of operators:
1. Let A0 contain all operators a  A such that u 6 vars(pre(a)) or v 6 vars(post(a)).
2. Let A0 contain the operator a[u/w] for every operator a  A such that
u  vars(pre(a)) and v  vars(post(a)).
3. Let A0 contain an operator copy(u, w, x) = hu = x ; w = xi for every value
x  D(v).
The operators in group 1 are the original operators from A corresponding to all edges in
CG() except (u, v). The operators in group 2 are the operators from A corresponding
to edge (u, v) but modified to instead correspond to the new edge (w, v). The operators
in group 3 correspond to the new edge (u, w) and are defined such that variable w can
mimic variable u. Clearly, this is a polynomial-time construction and CG(0 ) is an edge
subdivision of CG(). It remains to prove that 0 has a plan if and only if  has a plan.
If: Suppose P = a1 , . . . , an is a plan for . Construct a new operator sequence P 0
over A0 from P as follows: First, for each ai in P such that u  vars(pre(ai )) and v 
vars(post(ai )), replace ai with ai [u/w]. Then, for each ai in P such that u  vars(post(ai )),
601

fiBackstrom & Jonsson

let x = post(ai )(u) and add operator copy(u, w, x) between ai and ai+1 . The resulting
sequence P 0 is a plan for 0 .
Only if: Suppose P = a1 , . . . , an is a plan for 0 . Define the corresponding state sequence
s0 , . . . , sn such that s0 = init0 and si = s0 [a1 , . . . , ai ] for all i (1  i  n). Without losing
generality, assume that P is a shortest plan for 0 , which implies that ai is applicable in
si1 for every i (1  i  n). Define three variable sets C0 , C1 and C2 as in Lemma 19 such
that C0 = {w}, v  C1 and u  C2 . Also define the corrsponding partition {A0 , A1 , A2 } of
A0 , i.e. Ai = {a  A0 | vars(post(a))  Ci } for all i (0  i  2). Then A0 contains all copy
operators and nothing else. Before proving the main result of this direction, we first prove
the following auxiliary result:
According to Lemma 19 we can assume that every longest subsequence ak , . . . , a` that
does not contain any operator from A0 is on the form ak , . . . , am , am+1 , . . . , a` such that
ak , . . . , am  A1 and am+1 , . . . , a`  A2 . Since it is a longest such sequence, it must
hold that either (1) k = 1 or (2) ak1  A0 . In case (1) we have sk1 = s0 = init0 , so
sk1 (u) = sk1 (w) since init0 (u) = init0 (w). In case (2) operator ak1 = copy(u, w, x) for
some x such that sk1 (w) = sk2 (u) = x. Hence, sk1 (u) = sk1 (w) = x since ak1 does
not change u. That is, in either case we have sk1 (u) = sk1 (w). Furthermore, for all i
(k  i  m) it holds that si  (C0  C2 ) = sk1  (C0  C2 ) since ai  A1 . It follows that
si (u) = si (w) for all i (k  i  m). Now, for every i (k  i  `), if w  vars(pre(ai )) then
ai must be on the form a[u/w], for some a  A, so v  vars(pre(ai )) by definition. Hence,
ai  A1 so i  m and it follows that si1 (u) = si1 (w). Since this proof holds for all longest
subsequences not containing any operator from A0 we can conclude the following, which
will be used below:
(*) For any operator ai in P such that ai = a[u/w] for some a  A, it holds that
si1 (u) = si1 (w).
We now prove the main result of this direction, that also  has a plan since 0 has a
plan. We do so by constructing a plan P 00 for  from P in two steps. First we construct an
intermediate operator sequence P 0 and then construct the plan P 00 from P 0 . The sequence
P 0 is technically not a plan for either  or 0 , but this intermediate step makes the proof
clearer. Temporarily introduce a virtual dummy operator dum that has no precondition
and no postcondition, i.e. it is applicable in any state and has no effect. Then construct
the new operator sequence P 0 = b1 , . . . , bn over A  {dum} as follows:
 If ai  A, then bi = ai .
 If ai is a copy operator, then bi = dum.
 Otherwise, ai = a[u/w] for some operator a  A, so let bi be that operator a.
Define the corresponding state sequence t0 , . . . , tn such that t0 = init0 and ti = t0 [b1 , . . . , bi ]
for all i (1  i  n). We claim that ti  V = si  V for all i (0  i  n). Proof by induction
over i:
Basis: t0 = s0 by definition.
Induction: Suppose ti1  V = si1  V for some i (1  i  n). There are three cases:
(1) ai = bi and ai  A. Then w is not in the pre- or postcondition of either ai or bi so
bi is applicable in ti1 since ai is applicable in si1 and ti1  V = si1  V by assumption.
Furthermore, ti  V = ti1 [bi ]  V = si1 [ai ]  V = si  V .
602

fiA Refined View of Causal Graphs and Component Sizes

(2) ai is a copy operator and bi = dum. It is immediate from the definition of bi
that it is applicable in ti1 and that ti = ti1 . Furthermore, vars(post(ai ))  V =  so
si  V = si1  V . Since ti1  V = si1  V by assumption it thus follows that ti  V = si  V .
(3) ai is bi [u/w] and bi  A. It follows from (*) that si1 (w) = si1 (u), so si1 (w) =
ti1 (u) since u  V and ti1  V = si1  V by assumption. Since ai is applicable in si1 ,
pre(ai )(w) = pre(bi )(u) and pre(ai )(x) = pre(bi )(x) for all variables in V \{u}, it follows that
bi is applicable in ti1 . By definition, vars(post(bi )) = vars(post(ai )) = {v}, since both ai
and bi must be unary, and it thus also follows from the definition that post(bi ) = post(ai ).
Hence, it also follows that ti  V = si  V , since ti1  V = si1  V by assumption.
We have thus shown that ti  V = si  V for all i (0  i  n). Furthermore, clearly
ti = ti1 for all i such that bi = dum. It follows that we can create a plan P 00 for  by
removing all dummy operators from P 0 .
We conclude that  has a solution if and only if 0 has a solution.
We will finally need the following observations about 3SAT instances. Let F be a 3SAT
formula with n variables and m clauses. If it contains no repeated clauses, then
m
n
 m  8n3 and, thus, ( )1/3  n  3m.
3
8
Furthermore, F can be represented as a list of 3m literals which requires 3m(1 + log n) 
3m(1 + log 3m) bits, plus some overhead. Hence, F can be represented by at most cm2 bits,
for some constant c, and we will later use the upper bound 40m3 , which is safe.
We also note that the reduction used in the proof of Lemma 6 transforms a 3SAT
instance with n variables and m clauses to a planning instance with N = (2m + 4)n
variables. However, n  3m so N  (2m + 4)  3m = 6m2 + 12m, which can be safely
overestimated with N  18m2 .
6.3 The Main Theorem
We are now prepared to state and prove the main theorem of this section. It follows from the
proof of Theorem 14 that if in-deg(C), out-deg(C) and upath-length(C) are all bounded for a
class C of graphs, then cc-size(C) is bounded. In that case it is immediate from Theorem 3
that planning is tractable for C. This begs the question what happens if these parameters are
not bounded by a constant, yet bounded by some slow-growing function? We will consider
the case when they are allowed to grow slowly, as long as they are polynomially related to
the instance size. Since we have also noted that practical planning problems will typically
not have a causal graph of every size, we will only require that for every graph G in C there
must also be some larger graph G0 in C of size at most p(|G|), for some polynomial p. We
also define the parameter  (G) = max{upath-length(G), in-deg(G), out-deg(G)}, and require
that  (G) and ||G|| are polynomially related. It turns out that planning is still hard under
these restrictions, as the following theorem says.
Theorem 21. Let p and q be increasing polynomials on the natural numbers. Let C be
a class of directed graphs containing a subset of weakly connected graphs G1 , G2 , G3 , . . .
such that:
1. |V (G1 )|  p(q(1)),
|V (Gi1 )| < |V (Gi )|  p(|V (Gi1 )|), for all i > 1, and
603

fiBackstrom & Jonsson

2. |V (Gi )|  q( (Gi )), for all i  1.
If PlanExist(C) is polynomial-time solvable, then the polynomial hierarchy collapses. This result holds even when restricted to operators with at most 2 preconditions and 1 postcondition
and all graphs in C are acyclic.
Proof. Let G1 , G2 , G3 , . . . be a sequence of weakly connected graphs in C as assumed in the
theorem. Let H1 , H2 , H3 , . . . be a sequence of graphs defined as follows: for each i > 0,
Hi = Gj for the smallest j such that q(i)  |V (Gj )|.
We first prove that i underestimates  (Hi ). Combining the requirement that q(i) 
|V (Gj )| with condition 2 of the theorem, that |V (Gj )|  q( (Gj )), we get q(i)  |V (Gj )| 
q( (Gj )). Since Hi = Gj we get q(i)  |V (Hi )|  q( (Hi )), that is, that i   (Hi ). It
follows that also i  |V (Hi )| holds.
We then prove that |V (Hi )| is polynomially bounded by p(q(i)). Since j is choosen as the
smallest value satisfying that q(i)  |V (Gj )|, it must be that either j = 1 or |V (Gj1 )| <
q(i). If j = 1, then Hi = Gj = G1 and |V (G1 )|  p(q(1)) by condition 1 in the theorem.
Hence, |V (Hi )| = |V (G1 )|  p(q(1))  p(q(i)), since p and q are increasing. Otherwise,
when j > 1, condition 1 of the lemma says that |V (Gj )|  p(|V (Gj1 )|). Combining
this with the inequality |V (Gj1 )| < q(i) yields that |V (Gj )|  p(|V (Gj1 )|) < p(q(i)),
that is, |V (Hi )|  p(q(i)) since Hi = Gj . Combining this with the previous result that
i  |V (Hi )| and the construction of Hi yields that H1 , H2 , H3 is a sequence of graphs with
non-decreasing and unbounded size.
Now, define a sequence A0 , A1 , A2 , . . . of tuples such that for all i  0, either of the
following holds:
1. in-deg(Hi )  i and Ai = (in-deg, Hi , Xi ) such that Xi is a subgraph of Hi and Xi ' Siin .
2. out-deg(Hi )  i and Ai = (out-deg, Hi , Xi ) such that Xi is a subgraph of Hi and
Xi ' Siout .
3. upath-length(Hi )  i and Ai = (upath-length, Hi , Xi ) such that Xi is a subgraph of Hi
and Xi is a polypath of length i.
For every i > 0, at least one of these three cases must hold since i   (Hi ).
Define an advice-taking Turing machine M that uses the sequence A1 , A2 , A3 , . . . as
advice and takes 3SAT formulae as input. Assume that the representation of each formula
F is padded to size 40m3 bits, where m is the number of clauses. Although somewhat
redundant, this is still a reasonable encoding in the sense of Garey and Johnson (1979).
Let M work as follows. Let F be an input formula with n variables and m clauses and
let t = ||F || = 40m3 . Then the advice is At = (x, Ht , Xt ). First M constructs a planning
instance F . There are three cases depending on x:
x = in-deg: By construction, Xt is a subgraph of Ht such that Ht ' Stin . Since t = 40m3
and n  3m, it follows that n  t, so Xt contains a subgraph H 0 such that H 0 ' Snin .
Construct F in the same way as in the proof of Lemma 4, using the vertices of H 0
as variables. Then, CG(F ) = H 0 .
x = out-deg: Analogous to previous case, but constructing F according to the proof of
Lemma 5 instead.
604

fiA Refined View of Causal Graphs and Component Sizes

x = upath-length: By construction, Xt is a subgraph of Ht which is a polypath of length
t = 40m3 . Suppose that Xt contains less than m sinks and m + 1 sources and that
path-length(Xt ) < 18m2 . It then follows from Proposition 1 that
|V (Xt )| < 2m  18m2 + 1 = 36m3 + 1 < 40m3 = t.
However, this contradicts the construction so Xt must either contain a directed path
of length 18m2 or have at least m sinks and m + 1 sources.
1. If Xt contains a subgraph H 0 which is a directed path of length 18m2 , then
construct a planning instance F according to the proof of Lemma 6, using the
vertices from H 0 as variables. Then, CG(F ) ' H 0 .
2. If Xt contains a subgraph H 0 which is a polypath with m sinks and m+1 sources,
then construct a planning instance 
F according to the proof of Lemma 7, us0
+1
ing the variables of H as variables. Then, CG(
F ) ' Fm . This graph is a
fence, i.e. a polypath where all directed paths are of length 1. Each such path
can be stretched to a directed path of arbitrary length by repeatedly applying
Lemma 20. The graph H 0 is a polypath that can be used as a template for
which paths in CG(
F ) to stretch and how much in order to get a graph that
0
is isomorphic to H . Instance 
F can thus be modified into a new instance F
such that CG(F ) ' H 0 .
All these constructions can be done in polynomial time, and for all cases, F has a
solution if and only if F is satisfiable. Furthermore, CG(F ) is isomorphic to a subgraph
of Ht in all four cases. According to Lemma 17 it is thus possible to extend F to a new
+
+
planning instance +
F such that CG(F ) ' Ht and F has a solution if and only if  has
a solution. This extension can be done in polynomial time according to the same lemma.
Since PlanExist(C) can be solved in polynomial time by assumption in the theorem,
it thus follows that M can solve 3SAT in polynomial time. However, this implies that
NP  P/poly, which is impossible unless the polynomial hierarchy collapses (Theorem 16).
To see that the result holds even if the operators under consideration have at most 2
preconditions and 1 postcondition, simply note that this restriction holds for all reductions
used in the underlying NP-hardness proofs in Section 4. Similarly, the acyclicity restriction
holds since the result is based only on in-stars, out-stars and polypaths, which are all
acyclic graphs.
Recall the generic blocks world encoding that we discussed in the beginning of this
section. The class D of causal graphs for these blocks-world instances satisfies the requirements in Theorem 21, which means that PlanExist(D) is not likely to be tractable. However,
finding non-optimal plans for blocks world is tractable; a plan of length at most twice the
length of the optimal plan can be found in polynomial time (Gupta & Nau, 1992). That is,
there are most likely more difficult problems than blocks world that happen to have exactly
the same causal graphs, which illustrates that the complexity of a generic planning problem
cannot be deduced from its corresponding class of causal graphs alone.
605

fiBackstrom & Jonsson

7. NP-Hard and NP-Intermediate Classes
The theorem by Chen and Gimenez (2010) states a crisp complexity-theoretic borderline: if
the component sizes are bounded by a constant, then planning is polynomial-time solvable
and, otherwise, planning is not polynomial-time solvable. We have exploited an extra
constraint, SP-closure, to be able to prove NP-hardness, which leaves a greyzone between
the polynomial cases and the NP-hard ones. If we no longer require the classes to be SPclosed, then they are no longer obviously NP-hard even if the components are unbounded.
The natural question then arises, can we say something about this middle ground? For
instance, can we say something about what the NP-intermediate cases may look like and
where the borderline between NP-hard and NP-intermediate is? Although it does not seem
likely that we could find any results that characterize this borderline exactly, we can at
least give some partial answers to these questions. We will do this by proving two theorems
related to the growth rate of the components. The first of these shows that planning is still
NP-hard if the components grow as O(|V (G)|1/k ) for integers k, while the second one shows
that planning is likely to be NP-intermediate if the components grow polylogarithmically.
Theorem 22. For every constant integer k > 1, there is a class Gk of graphs such that
cc-size(G)  |V (G)|1/k for all G  Gk and PlanExist(Gk ) is NP-hard.
Proof. Let k > 1 be an arbitrary integer. Construct the graph class Gk = {G1 , G2 , G3 , . . .}
as follows. For each m > 0, let Gm have mk1 components, each of them isomorphic to dPm ,
i.e. |V (Gm )| = mk so all components are of size m = |V (Gm )|1/k . We prove NP-hardness
of PlanExist(Gk ) by reduction from PlanExist(dP). Let  be an arbitrary planning instance
such that CG()  dP. Then CG() = dPm for some m > 0. Construct a new instance 0
which consists of mk1 renamed copies of . This is clearly a polynomial time construction
since k is constant and m < ||||. Furthermore, CG(0 ) is isomorphic to Gm and 0 has a
solution if and only if  has a solution. Hence, this is a polynomial reduction so it follows
from Lemma 6 that PlanExist(Gk ) is NP-hard.
Obviously, the size of the graphs is exponential in k.
Our second result must be conditioned by the assumption that the exponential time
hypothesis (Impagliazzo & Paturi, 2001; Impagliazzo, Paturi, & Zane, 2001) holds. This
hypothesis is a conjecture stated as follows.
Definition 23. For all constant integers k > 2, let sk be the infimum of all real numbers
 such that k-SAT can be solved in O(2n ) time, where n is the number of variables of an
instance. The exponential time hypothesis (ETH) is the conjecture that sk > 0 for all k > 2.
Informally, ETH says that satisfiability cannot be solved in subexponential time. ETH
is not just an arbitrarily choosen concept, but a quite strong assumption that allows for
defining a theory similar to the one of NP-completeness. There is a concept called SERF
(subexponential reduction family) reduction which preserves subexponential time solvability. There is also a concept called SERF-completeness which is similar to NP-completeness,
but based on SERF reductions. That is, there is a subclass of the NP-complete problems
that are also SERF-complete, meaning that these can all be SERF reduced to each other.
Hence, if one of these can be solved in subexponential time, then all of them can.
606

fiA Refined View of Causal Graphs and Component Sizes

Theorem 24. For all constant integers k > 0 and all classes C of directed graphs, if
cc-size(G)  logk |V (G)| for all G  C, then PlanExist(C) is not NP-hard unless ETH
is false.
Proof. Let k > 0 be an arbitrary integer. Let  be an arbitrary planning instance with n
variables of maximum domain size d such that cc-size(CG())  c. The components correspond to independent subinstances, which can thus be solved separately. Each component
has a state space of size dc or less, so a plan for the corresponding subinstance can be found
in O(d2c ) time, using Dijkstras algorithm. Since there are at most n components, the whole
instance can be solved in O(nd2c ) time. However, it follows from the standard assumptions
of reasonable encodings that both n  |||| and d  ||||, so a looser bound is that  can
be solved in O(x  x2c ) = O(x1+2c ) time, where x = ||||.
Suppose PlanExist(C) is NP-hard. Then there is a polynomial reduction from 3SAT
to PlanExist(C). Furthermore, the size of a 3SAT instance is polynomially bounded in the
number of variables. Hence, there must be some polynomial p such that for a 3SAT instance
with n variables, the corresponding planning instance  has size ||||  p(n).
Since the number of variables in  is upper bounded by ||||, it follows from the assumption that the component size is upper bounded by logk ||||  logk p(n). Hence,  can
k
be solved in O(p(n)1+2 log p(n) ) time, according to the earlier observation, and
p(n)1+2 log

k

p(n)

= (2log p(n) )1+2 log

k

p(n)

 (2(1+2 log

k

p(n)) logk p(n)

)  23 log

2k

2k

p(n)

.

2k

Furthermore, logk p(n)  O(logk n), since p is a polynomial, so 23 log p(n)  2O(log n) and
2k
it follows that  can be solved in 2O(log n) time. However, then  can be solved in 2n
time for arbitrarily small , which contradicts ETH. It follows that PlanExist(C) cannot be
NP-hard unless ETH is false.
Since the components are unbounded, this problem is not likely to be solvable in polynomial
time either. It is thus an NP-intermediate problem under the double assumption that
W[1] 6 nu-FPT and that ETH holds.
Theorems 22 and 24 together thus tell us something about where the borderline between
NP-intermediate and NP-hard graph classes is. However, it is not a very crisp distinction;
asymptotically, there is quite a gap between the polylogarithmic functions and the root
functions (i.e. functions on the form x1/k ). One may, for instance, note that the function
1

f (n) = 2(log n)

1
(log log n)c

lies within this gap whenever 0 < c < 1.

8. Discussion
SP-closed graph classes have appealing properties and fit in well as a concept stronger than
subgraph-closed but weaker than minor-closed. They also give a partial characterization of
where the borderline to NP-hardness lies. However, as noted earlier, it is possible to define
other types of graph classes which also imply that planning is NP-hard. One example is
the family G1 , G2 , G3 , . . . of classes in the proof of Theorem 22. Another more specialized
and, perhaps, contrived class is the following, intended to give a contrast to the SP-closure
concept and the Gk classes.
607

fiBackstrom & Jonsson

A tournament is a directed graph formed by giving directions to each edge in a complete
graph. Let T denote the set of tournaments and note that T is not SP-closed. However,
tournaments are Hamiltonian graphs (Redei, 1934) so if T is a tournament on n vertices,
then path-length(T ) = n  1. Furthermore, the path of length n  1 can be computed in
polynomial time (Bar-Noy & Naor, 1990).
Assume we are given a 3SAT formula F with n variables and m clauses. Let ` =
(2m + 4)n, i.e. ` is polynomially bounded in F . According to Lemma 6 we can thus
construct a planning instance F in polynomial time such that
1. F contains ` variables,
2. CG(F ) ' dP` , and
3. F has a solution if and only if F is satisfiable.
Choose an arbitrary tournament T with ` vertices in T. Find the path of length `1 in T
and identify it with CG(F ). Then add dummy operators corresponding to the remaining
edges of T . We have thus shown that there is a polynomial-time transformation from 3SAT
to PlanExist(T), and that PlanExist(T) is NP-hard. One may also note that variations of
this technique can be used for proving that PlanExist(T0 ) is NP-hard for many different
T0  T.
While we have not considered domain sizes or tractable restrictions in this article, we
note that the Theorem 24 may give some ideas for where to look for tractable cases. Consider
the case where all variable domains are bounded in size by some constant k and where
cc-size(G)  log V (G). Using the first part of the proof, we see that planning can be solved
in O(n  k 2 log n ) time. However, k 2 log n = (2log k )2 log n = (2log n )2 log k = n2 log k , which is
polynomial since k is a constant. That is, planning is tractable for this restricted case. Even
though this observation is straightforward, it is interesting as a contrast to Theorem 24. It
also suggests that there are even larger tractable subgraphs if we also consider additional
restrictions on the planning instances.
While we have explicitly commented on the sufficient number of pre- and postconditions
for the various results, there are also alternative such characterizations that might be relevant. It would bear to far to list all such possibilities, so let it suffice with one example.
The concept of prevail conditions, i.e. preconditions on variables that are not changed by
the operator, originate from the SAS+ formalism (Backstrom & Nebel, 1995) but has more
recently been considered also in the context of causal graphs. Gimenez and Jonsson (2012)
refer to an operator as k-dependent if it has a precondition on at most k variables that it
does not also change. We may note that the proofs of Lemmata 17 and 20 only introduce
operators that are 1-dependent, at most. Since the proof of Theorem 21 does not impose
any further such restrictions on the original planning instance, it follows that this theorem
holds also when all operators are 1-dependent, at most.
As a final question, one might wonder if it is of any practical use at all to know that
planning is tractable, or NP-intermediate, for severely limited component sizes? After all,
most planning instances are likely to have a causal graph that is weakly connected, that
is, the whole graph is one single component. To answer that question, the first important
observation to make is that the complexity of planning for instances is directly related to
the complexity of planning for the components separately. This is because there can be at
608

fiA Refined View of Causal Graphs and Component Sizes

most linearly (in the number of variables) many components. If planning can be solved in
polynomial time for all components of an instance, then it can be solved in polynomial time
for the whole instance. Conversely, if planning cannot be solved in polynomial time for the
whole instance, then there is at least one component which is not polynomial-time solvable.
That is, the complexity results for instances and for components are directly related to each
other. In other words, the results are relevant for all methods that artificially split the causal
graph into components, in one way or another. Examples are the causal-graph heuristic by
Helmert (2006a), factored planning (Brafman & Domshlak, 2006) and structural pattern
data bases (Katz & Domshlak, 2010).

Acknowledgments
The anonymous reviewers provided valuable comments and suggestions for improving this
article.

References
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11, 625656.
Bar-Noy, A., & Naor, J. (1990). Sorting, minimal feedback sets, and Hamilton paths in
tournaments. SIAM Journal on Discrete Mathematics, 3 (1), 720.
Biggs, N. (1993). Algebraic Graph Theory. Cambridge Univ. Press. 2nd ed.
Bodirsky, M., & Grohe, M. (2008). Non-dichotomies in constraint satisfaction complexity.
In Proceedings of the 35th International Colloquium on Automata, Languages and
Programming (ICALP 2008), Reykjavik, Iceland, pp. 184196.
Brafman, R. I., & Domshlak, C. (2003). Structure and complexity in planning with unary
operators. Journal of Artificial Intelligence Research, 18, 315349.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, and when not. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI 2006),
Boston, MA, USA, pp. 809814. AAAI Press.
Chen, H., & Gimenez, O. (2010). Causal graphs and structurally restricted planning. Journal of Computer and Systems Science, 76 (7), 579592.
Domshlak, C., & Dinitz, Y. (2001a). Multi-agent off-line coordination: Structure and complexity. In Proceedings of the 6th European Conference on Planning (ECP01), Toledo,
Spain.
Domshlak, C., & Dinitz, Y. (2001b). Multi-agent off-line coordination: Structure and complexity. Tech. rep., Department of Computer Science, Ben-Gurion University. CS-0104.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Monographs in Computer Science. Springer, New York.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV of Texts in
Theoretical Computer Science. An EATCS Series. Springer, Berlin.
609

fiBackstrom & Jonsson

Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman, New York.
Gimenez, O., & Jonsson, A. (2008). The complexity of planning problems with simple
causal graphs. Journal of Artificial Intelligence Research, 31, 319351.
Gimenez, O., & Jonsson, A. (2009). Planning over chain causal graphs for variables with
domains of size 5 is NP-hard. Journal of Artificial Intelligence Research, 34, 675706.
Gimenez, O., & Jonsson, A. (2012). The influence of k-dependence on the complexity of
planning. Artificial Intelligence, 177-179, 2545.
Gupta, N., & Nau, D. S. (1992). On the complexity of blocks-world planning. Artificial
Intelligence, 56 (2-3), 223254.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 143 (2), 219262.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Proceedings
of the 14th International Conference on Automated Planning and Scheduling (ICAPS
2004), Whistler, BC, Canada, pp. 161170. AAAI Press.
Helmert, M. (2006a). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M. (2006b). New complexity results for classical planning benchmarks. In Proceedings of the 16th International Conference on Automated Planning and Scheduling
(ICAPS 2006), Cumbria, UK, pp. 5262. AAAI Press.
Impagliazzo, R., & Paturi, R. (2001). On the complexity of k-SAT. Journal of Computer
and System Science, 62 (2), 367375.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). Which problems have strongly exponential
complexity?. Journal of Computer and System Science, 63 (4), 512530.
Jonsson, A. (2009). The role of macros in tractable planning. Journal of Artificial Intelligence Research, 36, 471511.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (1-2), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence does not imply tractable
plan generation. Annals of Mathematics and Artificial Intelligence, 22 (3-4), 281296.
Karp, R. M., & Lipton, R. J. (1980). Some connections between nonuniform and uniform complexity classes. In Proceedings of the 12th ACM Symposium on Theory of
Computing (STOC80), Los Angeles, CA, USA, pp. 302309.
Katz, M., & Domshlak, C. (2007). Structural patterns of tractable sequentially-optimal
planning. In Proceedings of the 17th International Conference on Automated Planning
and Scheduling (ICAPS 2007), Providence, RI, USA, pp. 200207. AAAI Press.
Katz, M., & Domshlak, C. (2008). New islands of tractability of cost-optimal planning.
Journal of Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal of Artificial
Intelligence Research, 39, 51126.
610

fiA Refined View of Causal Graphs and Component Sizes

Katz, M., Hoffmann, J., & Domshlak, C. (2013). Who said we need to relax all variables?. In Proceedings of the 23rd International Conference on Automated Planning
and Scheduling (ICAPS 2013), Rome, Italy, 126134. AAAI Press.
Katz, M., & Keyder, E. (2012). Structural patterns beyond forks: Extending the complexity
boundaries of classical planning. In Proceedings of the 26th AAAI Conference on
Artificial Intelligence (AAAI 2012), Toronto, ON, Canada. AAAI Press.
Knoblock, C. A. (1994). Automatically generating abstractions for planning. Artificial
Intelligence, 68 (2), 243302.
Kuhn, D., Osthus, D., & Young, A. (2008). A note on complete subdivisions in digraphs of
large outdegree. Journal of Graph Theory, 57 (1), 16.
Ladner, R. E. (1975). On the structure of polynomial time reducibility. Journal of the
ACM, 22 (1), 155171.
Lovasz, L. (2005). Graph minor theory. Bulletin of the AMS, 43 (1), 7586.
Mohar, B. (2006). What is ... a graph minor. Notices of the AMS, 53 (3), 338339.
Redei, L. (1934). Ein kombinatorischer Satz. Acta Litteraria Szeged, 7, 3943.
Wehrle, M., & Helmert, M. (2009). The causal graph revisited for directed model checking.
In Proceedings of Static Analysis, the 16th International Symposium (SAS09), Los
Angeles, CA, USA, Vol. 5673 of LNCS, pp. 86101. Springer.
Williams, B., & Nayak, P. P. (1997). A reactive planner for a model-based executive.
In Proceedings of the 15th International Joint Conference on Artificial Intelligence
(IJCAI97), Nagoya, Japan, pp. 11781185.

611

fiJournal of Artificial Intelligence Research 47 (2013) 351-391

Submitted 03/13; published 06/13

Strong Equivalence of Qualitative Optimization Problems
Wolfgang Faber

faber@mat.unical.it

Department of Mathematics
University of Calabria
Via P. Bucci cubo 30B, 87036 Rende, Italy

Miroslaw Truszczynski

mirek@cs.uky.edu

Department of Computer Science
University of Kentucky
329 Rose Street, Lexington, KY 40506-00633, USA

Stefan Woltran

woltran@dbai.tuwien.ac.at

Institute of Information Systems
Vienna University of Technology
Favoritenstrae 911, 1040 Vienna, Austria

Abstract
We introduce the framework of qualitative optimization problems (or, simply, optimization problems) to represent preference theories. The formalism uses separate modules to
describe the space of outcomes to be compared (the generator ) and the preferences on outcomes (the selector ). We consider two types of optimization problems. They differ in the
way the generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics.
Under the latter interpretation of generators, optimization problems directly generalize
answer-set optimization programs proposed previously. We study strong equivalence of
optimization problems, which guarantees their interchangeability within any larger context. We characterize several versions of strong equivalence obtained by restricting the
class of optimization problems that can be used as extensions and establish the complexity
of associated reasoning tasks. Understanding strong equivalence is essential for modular
representation of optimization problems and rewriting techniques to simplify them without
changing their inherent properties.

1. Introduction
We introduce the framework of qualitative optimization problems in which, following the design of answer-set optimization (ASO) programs (Brewka, Niemela, & Truszczynski, 2003),
we use separate modules to describe the space of outcomes to be compared (the generator )
and the preferences on the outcomes (the selector ). In all optimization problems we consider, the selector module follows the syntax and the semantics of preference modules in
ASO programs, and the generator is given by a propositional theory. When this propositional theory is interpreted according to the standard propositional logic semantics, that is,
outcomes to be compared are classical models of the generator, we speak about classical optimization problems (CO problems, for short). When the generator theory is interpreted by
the semantics of equilibrium models (Pearce, 1997), we speak about answer-set optimization
problems (ASO problems, for short). We use this terminology, as equilibrium models are
c
2013
AI Access Foundation. All rights reserved.

fiFaber, Truszczynski, & Woltran

usually referred to as answer sets (Ferraris, 2005) for historical reasons. In ASO problems,
the answer sets of the generator are the outcomes used to determine the optimal outcomes.
Representing and reasoning about preferences in qualitative settings is an important
research area for knowledge representation and qualitative decision theory. The main objectives are to design expressive yet intuitive languages to model preferences, and to develop automated methods to reason about formal representations of preferences in these
languages. The literature on the subject of preferences is vast. We refer the reader to articles in the special issue of the Artificial Intelligence Magazine (Goldsmith & Junker, 2008)
and to a recent monograph by Kaci (2011) for a thorough discussion of the area and for
additional references.
Understanding when optimization problems are equivalent, in particular, when one can
be interchanged with another within any larger context, is fundamental to any preference
formalism. Speaking informally, optimization problems P and Q are interchangeable or
strongly equivalent when for every optimization problem R (context), P R and QR define
the same optimal models. Understanding when one optimization problem is equivalent to
another in this sense is essential for preference analysis, modular preference representation,
and rewriting techniques to simplify optimization problems into forms more amenable to
processing, without changing any of their inherent properties. Let us consider a multi-agent
setting, in which agents combine their preferences on some set of alternatives with the goal
of identifying optimal ones. Can one agent in the ensemble be replaced with another so that
the set of optimal alternatives is unaffected not only now, but also under any extension of
the ensemble in the future? Strong equivalence of agents optimization problems is precisely
what is needed to guarantee this full interchangeability property!
The notion of strong equivalence is of general interest, by no means restricted to preference formalisms. In some cases, most notably for classical logic, it coincides with equivalence,
the property of having the same models. However, if the semantics is not monotone, that is,
extending the theory may introduce new models not only eliminate some, strong equivalence
becomes a strictly stronger concept, and the one to adopt if theories being analyzed are to be
placed within a larger context. The nonmonotonicity of the semantics is the salient feature
of nonmonotonic logics (Marek & Truszczynski, 1993) and strong equivalence of theories in
nonmonotonic logics, especially logic programming with the answer-set semantics (Gelfond
& Lifschitz, 1991), was extensively studied in that setting (Lifschitz, Pearce, & Valverde,
2001; Turner, 2003; Eiter, Fink, & Woltran, 2007b). Preference formalisms also often
behave nonmonotonically as adding a new preference may cause a non-optimal outcome
(model) to become an optimal one. Thus, in preference formalisms, equivalence and strong
equivalence are typically different notions. Accordingly, strong equivalence was studied for
logic programs with rule preferences (Faber & Konczak, 2006), programs with ordered disjunction (Faber, Tompits, & Woltran, 2008) and programs with weak constraints (Eiter,
Faber, Fink, & Woltran, 2007a).
We extend the study of strong equivalence to the formalism of qualitative optimization
problems. The formalism is motivated by the design of answer-set optimization (ASO) programs of Brewka et al. (2003). It borrows two key features from ASO programs that make it
an attractive alternative to the preference modeling approaches based on logic programming
that we mentioned above. First, following ASO programs, optimization problems provide
a clear separation of hard constraints, which specify the space of feasible outcomes, and
352

fiStrong Equivalence of Qualitative Optimization Problems

preferences (soft constraints) that impose a preference ordering on feasible outcomes. Second, optimization problems adopt the syntax and the semantics of preference rules of ASO
programs that correspond closely to linguistic patterns of simple conditional preferences
used by humans.
The separation of preference modules from hard constraints facilitates eliciting and representing preferences. It is also important for characterizing strong equivalence. When a
clear separation is not present, like in logic programs with ordered disjunctions (Brewka,
Niemela, & Syrjanen, 2004), strong equivalence characterizations are cumbersome as they
have to account for complex and mostly implicit interactions between hard constraints
and preferences. For optimization problems, which impose the separation, we have onedimensional forms of strong equivalence, in which only hard constraints or only preferences
are added. These one-dimensional concepts are easier to study yet provide enough information to construct characterizations for the general case.
Main Contributions.

Our main contribution can be summarized as follows.

 We propose a general framework of qualitative optimization problems, extending in
several ways the formalism of ASO programs. We focus on two important instantiations of the framework, the classes of classical optimization (CO) problems and
answer-set optimization (ASO) problems. The latter one directly generalizes ASO
programs.
 We identify the problem of strong equivalence for theories in general preference formalisms. We point out that strong equivalence and equivalence do not coincide (in
general) in preference formalisms and that it is the concept of strong equivalence
that is fundamental for such issues as theory modularity, rewriting and simplification. Strong equivalence was studied earlier in the context of logic programs and logic
programs extended with preferences on rules or on atoms in the heads of rules (and
was similarly motivated). However, to the best of our knowledge, this is the first
paper that studies strong equivalence for a more typical preference formalism that
represents preferences in terms of preferred properties (modeled by formulas) and
independently of constraints defining outcomes to compare. As such, it is of more
relevance to the mainstream preference research than the previous studies.
 We characterize the concept of strong equivalence of optimization problems relative
to changing selector modules. The characterization is independent of the semantics
of generators and so, applies both to CO and ASP problems. We also characterize
strong equivalence relative to changing generators (with preferences fixed). In this
case, not surprisingly, the characterization depends on the semantics of generators.
However, we show that the dependence is quite uniform, and involves a characterization of strong equivalence of generators relative to their underlying semantics, when
they are considered on their own as propositional theories. Finally, we combine the
characterizations of the one-dimensional concepts of strong equivalence into a characterization of the general combined notion.
 We develop our results for the case when preferences are ranked. In practice, preferences are commonly ranked due to the hierarchical structure of preference providers.
353

fiFaber, Truszczynski, & Woltran

The general case we study allows for additions of preferences of ranks from a specified interval [i, j]. This covers the case when only some segment in the hierarchy of
preference providers is allowed to add preferences (top decision makers, middle management, low-level designers), as well as the case when there is no distinction between
the importance of preferences (the non-ranked case).
 We establish the complexity of deciding whether two optimization problems are strongly equivalent relative to changing selectors, generators, or both. Our results show that
these problems range from being co-NP- to P3 -complete.
Organization. In the following section, we introduce the concept of an optimization
problem and the necessary terminology, and define all equivalence problems we are interested
in here. We also discuss the relationship of optimization problems to further formalisms
from the literature, in particular to ASO programs. In Section 3 we provide our results for
the case where selectors may vary but no new hard constraints are allowed. Section 4 in
turn characterizes the strong equivalence notion, where preferences are unaffected but the
generator parts are subject to change. In Section 5 we finally show how the characterizations
obtained in the previous sections have to be combined in order to capture the general case
of strong equivalence. Our complexity analysis is presented in Section 6, followed by a
discussion of our results and considerations about future directions of research.
We present proof sketches and some of the simpler and not overly technical proofs in the
main text to facilitate understanding of the results at the intuitive level. Detailed proofs
can be found in the Appendix.
This article is a substantially extended version of an earlier published conference version
(Faber, Truszczynski, & Woltran, 2012).

2. Optimization Problems
In this section we provide the basic definitions of optimization problems in Section 2.1,
followed in Section 2.2 by definitions of the strong equivalence notions on optimization
problems that are studied in the remainder of the paper. Finally, in Section 2.3 we provide
a discussion on related formalisms.
2.1 Basic Definitions
A qualitative optimization problem (an optimization problem, from now on) is an ordered
pair P = (T, S), where T is called the generator and S the selector. The role of the
generator is to specify the family of outcomes to be compared. The role of the selector S
is to define a relation  on the set of outcomes and, consequently, define the notion of an
optimal outcome. The relation  induces relations > and : we define I > J if I  J and
J 6 I, and I  J if I  J and J  I. For an optimization problem P , we write P g and P s
to refer to its generator and selector, respectively.
Generators. As generators we use propositional theories in the language determined by
a fixed countable universe (or alphabet) U of propositional variables that form atomic
propositions, a Boolean constant , and Boolean connectives ,  and , and where we
define the constant >, and the connectives  and  in the usual way as > := ,  :=
354

fiStrong Equivalence of Qualitative Optimization Problems

  , and    := (  )(  ), respectively.1 Models of the generator, as defined
by the semantics used, represent outcomes of the corresponding optimization problem.
We consider two quite different semantics for generators: the classical propositional logic
semantics and the semantics of equilibrium models (Pearce, 1997). Thus, outcomes are
either models or equilibrium models, depending on the semantics chosen. The first semantics
is of interest due to the fundamental role and widespread use of classical propositional logic,
in particular, as a means to describe constraints. Equilibrium models generalize answer sets
of logic programs to the case of arbitrary propositional theories (Pearce, 1997; Ferraris, 2005)
and are often referred to as answer sets. The semantics of equilibrium models is important
due to the demonstrated effectiveness of logic programming with the semantics of answer
sets for knowledge representation applications. We use the terms equilibrium models and
answer sets interchangeably.
Throughout the paper, we represent interpretations as subsets of U, which contain
exactly those atomic propositions that are interpreted as true. We write I |=  to state that
an interpretation I  U is a (classical propositional) model of a formula . Furthermore,
we denote the set of classical models of a formula or theory T by Mod (T ).
Equilibrium models arise in the context of the propositional logic of here-and-there, or
the logic HT for short (Heyting, 1930). We briefly recall here definitions of concepts, as
well as properties of the logic HT that are directly relevant to our work. We refer to the
papers by Pearce (1997) and Ferraris (2005) for further details.
The logic HT is a logic located between the intuitionistic and the classical logics. Interpretations in the logic HT are pairs hI, Ji of standard propositional interpretations such
that I  J. We write hI, Ji |=HT  to denote that a formula  holds in an interpretation
hI, Ji in the logic HT. The relation |=HT is defined recursively as follows:
1. hI, Ji 6|=HT 
2. for an atom a, hI, Ji |=HT a precisely when a  I
3. hI, Ji |=HT    if hI, Ji |=HT  or hI, Ji |=HT 
4. hI, Ji |=HT    if hI, Ji |=HT  and hI, Ji |=HT 
5. hI, Ji |=HT    if J |=    (classical satisfiability), and hI, Ji 6|=HT  or
hI, Ji |=HT .
An equilibrium model or answer set of a propositional theory T is a standard interpretation I such that hI, Ii |=HT T and for every proper subset J of I, hJ, Ii 6|=HT T . Answer
sets of a propositional theory T are also classical models of T . The converse is not true in
general. We denote the set of all answer sets of a theory T by AS (T ), and the set of all
HT-models of T by ModHT (T ), that is, ModHT (T ) = {hI, Ji | I  J, hI, Ji |=HT T }.
For each of the semantics there are two natural concepts of equivalence. Two theories T1
and T2 are equivalent if they have the same models (classical or equilibrium, respectively).
They are strongly equivalent if for every theory S, T1  S and T2  S have the same models
(again, classical or equilibrium, respectively).
1. While the choice of primitive connectives is not common for the language of classical propositional logic,
it is standard for the of logic here-and-there which underlies the answer-set semantics.

355

fiFaber, Truszczynski, & Woltran

For classical semantics, strong equivalence and equivalence coincide. It is not so for
the semantics of equilibrium models. The result by Lifschitz et al. (2001) states that two
theories T1 and T2 are strongly equivalent for equilibrium models if and only if T1 and T2
are equivalent in the logic HT, that is, ModHT (T1 ) = ModHT (T2 ). We will now illustrate the
notions of HT-models and equilibrium models, and relate the latter to the classical ones.
Here and in all other examples, we consider only classical and HT-models over the alphabet
consisting of atoms explicitly mentioned in the theories discussed. That is sufficient to
determine all equilibrium models on the one hand (which happen to consist only of atoms
that are mentioned) and, on the other, to show that they differ from classical ones.
Example 1 Let us consider the theory Ta = {a  a}. The classical models of Ta (under the
restriction mentioned above) are  and {a}, so a being true and a being false are both possible
outcomes. The HT-models (again, under the same restriction) are h, i, h, {a}i, and
h{a}, {a}i. Hence, there is only one answer set (equilibrium model) . The other possible
candidate, {a}, is not an answer set. While h{a}, {a}i |=HT Ta holds, also h, {a}i |=HT Ta
does. Thus, intuitively, the theory does not contain any cause for a to hold.
Next, let us consider the theory Tb = {ab}. The classical models are {a}, {b} and {a, b},
and the HT-models are h{a}, {a}i, h{b}, {b}i, h{a}, {a, b}i, h{b}, {a, b}i and h{a, b}, {a, b}i.
The answer sets are therefore {a} and {b}, but not {a, b}. Again the intuition is that the
theory does not contain a cause for a and b to hold simultaneously.
Finally, let us consider the theory Tc = {(a  b)  (b  a)}. The classical models
are the same as for Tb , that is, Mod (Tc ) = {{a}, {b}, {a, b}}. We also have ModHT (Tc ) =
{h{a}, {a}i, h{b}, {b}i, h{a}, {a, b}i, h{b}, {a, b}i, h{a, b}, {a, b}i, h, {a, b}i} = ModHT (Tb )
{h, {a, b}i}. The answer sets are again the same as for Tb : AS (Tc ) = {{a}, {b}}.
We observe that Tb and Tc are equivalent for both the classical and equilibrium setting
(they have the same classical and equilibrium models). The former implies that they are
also strongly equivalent in the classical setting. However, they are not strongly equivalent in
the equilibrium setting because ModHT (Tb ) 6= ModHT (Tc ) (cf. the characterization of strong
equivalence under the equilibrium semantics by Lifschitz et al. (2001)). And indeed, for
S = {a  b, b  a}, we obtain AS (Tb  S) = {{a, b}}, while AS (Tc  S) = .
We recall that optimization problems under the classical interpretation of generators
are referred to as classical optimization problems or CO problems, and when we use the
answer-set semantics for generators, we speak about answer-set optimization problems or
ASO problems.
Selectors. We follow the definitions of preference modules in ASO programs (Brewka
et al., 2003), adjusting the terminology to our more general setting. A selector is a finite
set of ranked preference rules
j
1 >    > k  
(1)
where k and j are positive integers, and i , 1  i  k, and  are propositional formulas
over U. For a rule r of the form (1), the number j is the rank of r, denoted by rank (r),
hd (r) = {1 , . . . , k } is the head of r and  is the body (the condition) of r, bd (r). Moreover,
we write hd i (r) to refer to formula i .
If rank (r) = 1 for every preference rule r in a selector S, then S is a simple selector.
1
Otherwise, S is ranked. We often omit 1 from the notation  for simple selectors. For
356

fiStrong Equivalence of Qualitative Optimization Problems

a selector S, and i, j  {0, 1, 2, . . .}  {}, we define S[i,j] = {r  S | i  rank (r)  j}
(where we assume that for every integer k, k < ) and write [i, j] for the rank interval
{k | k an integer, i  k  j}. We extend this notation to optimization problems. For
P = (T, S) and a rank interval [i, j], we set P[i,j] = (T, S[i,j] ). For some rank intervals we
use shorthands, for example = i for [i, i], < i for [1, i  1],  i for [i, ], and similar.
For an interpretation I, a satisfaction
degree of a preference rule r is vI (r) = min{i | I |=
W
hd i (r)}, if I |= bd (r) and I |= hd (r); otherwise, the rule is irrelevant to I, and vI (r) = 1.
Intuitively, the lower the satisfaction degree the better the outcome. Thus, a preference
rule 1 >    > k   can informally be read as follows: irrelevant outcomes (those
not satisfying , or not satisfying any i ) and outcomes satisfying 1 are most preferred,
followed by outcomes satisfying 2 , and then outcomes satisfying 3 , etc. We note that
Brewka et al. (2003) represented the satisfaction degree of an irrelevant rule by a special
non-numeric degree, treated as being equivalent to 1. The difference is immaterial and the
two approaches are equivalent.
Selectors determine a preference relation on interpretations. Given interpretations I
and J and a simple selector S, I S J holds precisely when for all r  S, vI (r)  vJ (r).
Therefore, I >S J holds if and only if I S J and there exists r  S such that vI (r) < vJ (r);
I S J holds if and only if for every r  S, vI (r) = vJ (r).
Given a ranked selector S, we define I S J if for every preference rule r  S, vI (r) =
vJ (r), or if there is a rule r0  S such that the following three conditions hold:
1. vI (r0 ) < vJ (r0 )
2. for every r  S of the same rank as r0 , vI (r)  vJ (r)
3. for every r  S of smaller rank than r0 , vI (r) = vJ (r).
Moreover, I >S J if and only if there is a rule r0 for which the three conditions above hold,
and I S J if and only if for every r  S, vI (r) = vJ (r). Given an optimization problem
P = (T, S), we often write P for S (and similarly for > and ). Furthermore, for a set
V  2U and a relation  (like >, , or ) over 2U , we write V for the restriction of  to
V , that is, V = {(A, B)   | A, B  V }. The relationship between equalities of , >,
and  of two optimization problems is as follows.
Lemma 1 For all optimization problems P and Q, and every set V  2U , PV = Q
V
Q
P
implies >PV = >Q
V and V = V .
P
Proof. Suppose PV = Q
V and let I, J be interpretations such that I>V J. By definiQ
Q
Q
P
P
tion, IV J but J6V I. By assumption IV J and J6V I, implying I>V J. In case IPV J
Q
we have IPV J and JPV I. By assumption IQ
V J and JV I hold as well and we conclude
Q
Q
P
P
IQ
V J. The other direction (I>V J implies I>V J, and IV J implies IV J) is analogous. 2

Some aspects of ASO selectors require additional discussion. First, a preference rule
may be irrelevant to an outcome. This is the case when the outcome does not satisfy the
condition of a rule or, when it does, if it does not satisfy any formula in the head of the
rule. In such cases, we define the outcome to be most desirable with respect to the rule.
In making this choice, we followed the original definition (Brewka et al., 2003) (modulo a
357

fiFaber, Truszczynski, & Woltran

minor simplification mentioned earlier). Obviously, other choices could be considered, too.
For instance, we could define irrelevant outcomes to be the least desirable with respect to
the rule. We could also restrict attention only to selectors that do no permit irrelevance at
all (a preference rule does not allow irrelevance if it has no body and if the disjunction of
all options in the head is a tautology). This would eliminate the need to address that issue
altogether, however, at the price of a more constraining definintion of selector rules.
Ultimately, the question what is the right design choice here is of secondary importance
as the semantics of preference rules we adopted provides us the flexibility to represent other
possible definitions. In particular, we note that the semantics of the rule
1 > . . . > n  
is the same as that of the rule
1   > . . . >  n    .
In other words, the conditions (the rule bodies) are only a modeling device making preference
rules better correspond to conditional preferences expressed in natural language. They can
be compiled away. As for the second type of irrelevance, the formalism of selectors allows
the user to override the default we adopted. We can make our adopted design choice explicit,
that is, making outcomes not satisfying any of the options in the head explicitly the most
desirable. Intuitively, it is sufficient to rewrite the rule (without body, since bodies can be
removed, as shown earlier)
1 > . . . > n 
to
1  (1  . . .  n ) > . . . > n ,
or, equivalently,
1  (2  . . .  n ) > . . . > n 
while the following rewriting makes them least desirable:
1 > . . . > n  (1  . . .  n ) ,
or, equivalently,
1 > . . . > n  (1  . . .  n1 )  .
Another question concerns rules with only one option in the head. Intuitively, given
our semantics they should not be important, because their satisfaction degree is always 1.
Indeed, Corollary 12 later in the paper provides a formal result that confirms this statement.
Optimal (preferred) outcomes. For an optimization problem P , (P ) denotes the set
of all outcomes of P , that is, the set of all models (under the selected semantics) of the
generator of P . Thus, (P ) stands for all models of P in the framework of CO problems
and for all answer sets of P , when ASO problems are considered. A model I  (P ) is
optimal or preferred for P if there is no model J  (P ) such that J >P I. We denote the
set of all preferred models of P by (P ).
The following lemma asserts that if the preference relation of two optimization problems
is equal on their sets of outcomes, then the preferred models coincide. This result follows
358

fiStrong Equivalence of Qualitative Optimization Problems

immediately from the definitions and will be useful in the sequel. Its statement brings up a
subtle notational issue. Formally, a (strict) preorder is a pair (D, >), where D is a set (the
domain of the preorder) and > is a transitive acyclic binary relation on D (the preorder
relation). Two preorders are equal if they have the same domain and the same relation on
that domain. Typically, whenever the domain is understood, we refer to preorders just by
pointing to their relation symbols. Often, however, we write >D for the preorder relation
symbol to make the domain explicit in the notation. This is what we do in the statement
of the result below.
Lemma 2 Let P and Q be optimization problems with >P(P ) = >Q
(Q) . Then, (P ) = (Q).
Proof.
As observed above, the equality of preorders implies the equality of the domains. In our case, the equality >P(P ) = >Q
(Q) implies (P ) = (Q). Hence, for each
P
Q
I, J  (P ) = (Q), I > J iff I > J. The result follows now directly from the definition
of preferred outcomes. 2
We also observe that eliminating rules of large ranks can only make unpreferred outcomes preferred, and never make preferred outcomes unpreferred. We recall that, for a
given optimization problem P = (T, S), P<i = P[1,i1] = (T, S[1,i1] ) is the corresponding
optimization problem with all rules with rank i or higher removed.
Lemma 3 For every optimization problem P and every i  1, (P<i )  (P ).
Proof. Let us assume that I 
/ (P<i ). In case I 
/ (P<i ), we have I 
/ (P ), and I 
/ (P )
P
<i
follows. Otherwise, there is an interpretation J  (P<i ) such that J >
I. Thus, there
s , say of rank j, such that (i) v (r) < v (r); (ii) for every r 0  P s with the
is a rule r  P<i
J
I
<i
s with rank less then j, v (r 0 ) = v (r 0 ).
rank j, vJ (r0 )  vI (r0 ); and (iii) for every r0  P<i
J
I
We note that, since (P<i ) = (P ), J  (P ). Moreover, j < i and so the sets of rules
s coincide. Thus, J >P I follows and,
with ranks less than or equal to j in P s and P<i
consequently, I 
/ (P ). 2

2.2 Notions of Equivalence
We define the union of optimization problems as expected, that is, for P1 = (T1 , S1 ) and
P2 = (T2 , S2 ), we set P1  P2 = (T1  T2 , S1  S2 ). Two optimization problems P1 and
P2 are strongly equivalent with respect to a class R of optimization problems (referred
to as a class of contexts or simply contexts) if for every optimization problem R  R,
(P1  R) = (P2  R).
We consider three general classes of contexts. First and foremost, we are interested in
the class LU of all optimization problems over U. We also consider the families LgU and
LsU of all optimization problems of the form (T, ) and (, S), respectively. The first class
consists of optimization problems which, when added to any other problem, can affect the
set of feasible outcomes but cannot affect the preference relation. We call such optimization
problems generator problems. The second class consists of optimization problems which,
when added to any other problem, do not change the set of feasible outcomes but change
359

fiFaber, Truszczynski, & Woltran

(in general) the preference relation. We call such optimization problems selector problems.
These one-dimensional contexts provide essential insights into the general case. For these
two classes, we speak of strong gen-equivalence, denoted g , and strong sel-equivalence,
denoted s , respectively. For the general class LU of all optimization problems we simply
speak of strong equivalence, denoted sg .
We recall that the notion of strong equivalence is, by definition, underlying the replacement property. In fact, for an optimization problem P = (T, S) containing a subproblem
Q = (T 0 , S 0 ) (i.e. T 0  T and S 0  S) we can guarantee that Q can be replaced in P
by another subproblem R without changing the optimal outcomes, if Q sg R. Indeed, if
Q sg R holds, one can faithfully replace Q by R in any optimization problem (otherwise
we would have (Q  P 0 ) 6= (R  P 0 ) for some P 0  LU ).
Constraining ranks of rules in selectors gives rise to additional classes of contexts parameterized by rank intervals [i, j]:
s,[i,j]

1. LU

[i,j]

2. LU

= {(, S)  LsU | S = S[i,j] }

= {(T, S)  LU | S = S[i,j] }

The first class of contexts gives rise to strong sel-equivalence with respect to rules of
rank in [i, j], denoted by s,[i,j] . The second class of contexts yields the concept of strong
s,[i,j]
equivalence with respect to rules of rank in [i, j]. We denote it by g
. We call problems
[1,1]
=1
in the class LU = LU simple optimization problems.
2.3 Relation to Other Preference Formalisms
Optimization problems are most closely related to ASO programs (Brewka et al., 2003).
The formalism of optimization problems extends ASO programs in several ways. First,
the generators of optimization problems are arbitrary propositional theories. Under the
semantics of equilibrium models, the generators properly extend logic programs with the
answer-set semantics, which are used as generators in ASO programs. Second, the selectors
of optimization problems use arbitrary propositional formulas in the heads of preference
rules, as well as for conditions in their bodies, which again generalizes the selectors of
ASO programs. Finally, optimization problems explicitly allow for alternative semantics of
generators, a possibility mentioned but not pursued by Brewka et al. (2003).
As we already noted in the introduction there is vast literature on preference representation and reasoning (The special issue of Artificial Intelligence Magazine, Goldsmith &
Junker, 2008, and the monograph by Kaci, 2011, are two comprehensive sources of relevant
references. For a survey on preference approaches on top of nonmonotonic formalisms, see
Delgrande, Schaub, Tompits, & Wang, 2004). Discussing it goes beyond the scope of the
present paper, especially as the problem that we focus on here (strong equivalence) has
not been considered much in the preference research before, and there are essentially no
relevant earlier results except those we already mentioned in the introduction (Faber &
Konczak, 2006; Faber et al., 2008; Eiter et al., 2007a). Nevertheless, since our work uses
the preference formalism of ASO problems, an extension of the formalism of ASO programs
by Brewka et al. (2003) that has not received as much attention in the preference research
as some others, we will make a few comments on our choice.
360

fiStrong Equivalence of Qualitative Optimization Problems

First, ASO problems are explicit about the constraints that must not be violated (the
generator part) and preferences, that is, weaker constraints that only make some outcomes
more desirable than others (the selector part). In that ASO problems match well with practical settings, as typically both kinds of constraints are at play. For instance, in a product
configuration problem there are physical constraints limiting the space of available possibilities (not every type of engine can be put in a small family sedan, moon roof is not available
with the basic engine option, etc.), as well as user preferences that describe what the user
would like to have if possible. Preferential reasoning (optimization) in the presence of (hard)
constraints has received substantial attention. A representative approach in which CP-nets
(Boutilier, Brafman, Domshlak, Hoos, & Poole, 2003) are combined with constraints was
described in the paper by Boutilier, Brafman, Domshlak, Hoos, and Poole (2004).
The choice of propositional logic to represent constraints (the generator part in our formalism) is standard. However, in contrast with other approaches, in addition to the classical
semantics we also consider an appealing alternative, the semantics of answer sets. This is
important as the resulting formalism of answer-set programming (Marek & Truszczynski,
1999; Niemela, 1999) is steadily gaining on acceptance as a constraint language and is
supported by ever improving computational tools (Calimeri, Ianni, Krennwallner, & Ricca,
2012).
On the other hand, our choice of the formalism for the selector part is less obvious.
There are several reasons that motivated us. The first, and the one we already mentioned
earlier, is that preference rules have a natural reading agreeing well with the linguistic
patterns that humans use when formulating qualitative conditional preferences. Second,
as demonstrated in the original work where ASO selectors were introduced (Brewka et al.,
2003), they can be used to approximate preference relations that are defined by CP-nets
(Boutilier et al., 2003), one of the most broadly studied qualitative preference systems,
but has better computational properties. For instance, the dominance problem is in P
as opposed to being NP-hard or even PSPACE-complete for some generalized classes of
CP-nets (Goldsmith, Lang, Truszczynski, & Wilson, 2008).
Third, individual preference rules are closely related to one of the standard approaches
to representing preferences based on possibilistic logic. In that approach (we only give the
most basic details here, for a more comprehensive discussion we refer to Kaci, 2011, Ch.
3.3.3), a preference theory consists of formulas, each with a distinct rank (the assumption
that ranks are distinct is not limiting as formulas with repeating ranks can be conjuncted
into a single formula of that rank). The quality of an outcome is given by its score defined
as the minimum rank of a formula that the outcome does not satisfy (, if all formulas are
satisfied). The higher the score, the better the outcome. Let {1 , . . . , n } be a preference
theory, with the index i in i representing the rank of i . It is clear that the preference
semantics of that theory as described above is precisely captured by our preference rule
n > n1 > . . . > 1 > 1 
where i = 1  . . .  i , for i = 1, . . . , n. Thus, ASO problems subsume the preference
formalism based on possibilistic logic.
Finally, the selector part of ASO problems typically consists of several preference rules
and these rules may have different ranks. That allows us to model preferences coming from
different sources and having different importance. In such cases, the main issue is that
361

fiFaber, Truszczynski, & Woltran

of integrating these individual preferences into a single order. There is no single broadly
accepted way to do so. The approach used by our formalism boils down to the Pareto
principle, arguably the common core of all such integration principles. Accordingly, our
formalism allows conflicting rules in the selector (for instance, a > b  and b > a ) but
leaves such conflicts unresolved resulting in incomparability. As for the ranks, the lower the
rank, the more important the rule is. Rules of less importance are used to compare outcomes
only if the rules of more importance do not distinguish them. This way of handling ranks
is natural and shows up in many preference formalisms.
One prominent example in this context is prioritized (propositional) circumscription
(Lifschitz, 1985), where minimization (of certain atoms in models of a formula) is defined
with respect to classes of atoms of different priority. Formally, let T be a theory over
atoms A and (P1 , . . . , Pn , V, F ) be a partition on A. Then, a model M  Mod (T ) is called
(P1 , . . . , Pn , V, F )-minimal if there is no N  Mod (T ), such that (i) N  (P1      Pi1 ) =
M  (P1      Pi1 ) and N  Pi  M  Pi for some 1 Si  n, and (ii) N  F = M  F . The
intuition behind the definition is that atoms in P = i Pi are to be minimized, with the
assignments to V allowed to vary, while assignments to F are to be kept fixed. Atoms in P
are minimized such that those in P1 have the highest priority followed by those in P2 , etc.
The relation to ranks is quite obvious. One can show that the (P1 , . . . , Pn , V, F )-minimal
models of a theory T coincide with the preferred outcomes of the CO problem X = (T, S),
where the selector S is given by
1

1

S = {f > f  > | f  F }  {f > f  > | f  F }
i

 {p > p  > | p  Pi , 1  i  n}).
Indeed preference rules in the first two sets ensure that only interpretations with the same
fixed part are comparable, and preference rules in the last group precisely reflect the prioritized process of minimization of atoms in P1  . . .  Pn .
Let us just finally mention here that our formalism of optimization problems gives not
only a handle for classical prioritized circumscription, but also for circumscription put on
top of logic programs (this is not meaningless, as in the forms currently prevalent in answer
set programming, answer sets are not necessarily minimal models, see for instance Simons,
Niemela, & Soininen, 2002). To this end it suffices to apply the embedding using ASO
problems instead of CO problems.

3. Strong Sel-Equivalence
We start by analyzing the case of strong sel-equivalence that turns out to be the core case
for our study. Indeed, characterizations of strong sel-equivalence naturally imply characterizations for the general case thanks to the following simple observation.
Proposition 4 Let P and Q be optimization problems (either under classical or answer-set
s,[i,j]
semantics for the generators) and [i, j] a rank interval. Then P g
Q if and only if for
every generator R  LgU , P  R s,[i,j] Q  R.
s,[i,j]

Proof. () Let R  LgU . Since P g

s,[i,j]

Q, P  R g
362

Q  R and so, P  R s,[i,j] Q  R.

fiStrong Equivalence of Qualitative Optimization Problems

[i,j]

() Let R be any optimization problem in LU . We have P R = (P (Rg , ))(, Rs ) and
QR = (Q(Rg , ))(, Rs ). Moreover, by the assumption we have that P (Rg , ) s,[i,j]
Q  (Rg , ). Thus,
((P  (Rg , ))  (, Rs )) = ((Q  (Rg , ))  (, Rs )).
s,[i,j]

It follows that (P  R) = (Q  R) and, consequently, that P g

Q. 2

Furthermore, the set of outcomes of an optimization problem P is unaffected by changes
in the selector module. It follows that the choice of the semantics for generators does not
matter for characterizations of strong sel-equivalence. Thus, whenever in this section we
refer to the set of outcomes of an optimization problem P , we use the notation (P ), and
not the more specific one, Mod (P g ) or AS (P g ), that applies to CO and ASO problems,
respectively.
To formally state the subsequent results, we need one more auxiliary notation. For an
optimization problem P , we define diff P (I, J) to be the largest k such that I P<k J. If
for every k we have I P<k J, then we set diff P (I, J) = . It is clear that diff P (I, J) is
well-defined. Moreover, as I P<1 J, diff P (I, J)  1. The following lemma characterizes
the relation >P Q for ranked optimization problems P and Q.
Lemma 5 Let P and Q be optimization problems, and I, J be interpretations. Then,
I >P Q J holds if and only if one of the following conditions holds:
1. diff P (I, J) < diff Q (I, J) and I >P J;
2. diff P (I, J) > diff Q (I, J) and I >Q J;
3. diff P (I, J) = diff Q (I, J), I >P J and I >Q J.
Proof. The if direction is evident. To prove the only-if direction, we note that the
cases diff P (I, J) < diff Q (I, J) and diff P (I, J) > diff Q (I, J) are obvious, too. Thus, let us
assume diff P (I, J) = diff Q (I, J) = i. Clearly, i <  (otherwise, I P Q J, contrary to the
assumption). It follows that for every rule r  P s  Qs of rank less than i, vI (r) = vJ (r).
Next, for every r  P s  Qs of rank i, vI (r)  vJ (r). Finally, there are rules r  P s and
r0  Qs , each of rank i such that vI (r) 6= vJ (r) and vI (r0 ) 6= vJ (r0 ) (since diff P (I, J) = i
and diff Q (I, J) = i). It follows that vI (r) < vJ (r) and vI (r0 ) < vJ (r0 ). Thus, I >P J and
I >Q J, as needed. 2
Our first main result concerns strong sel-equivalence relative to selectors consisting of
preference rules of ranks in a rank interval [i, j]. Special cases for strong sel-equivalence will
follow as corollaries.
Considering strong sel-equivalence means that preference rules may be added to optimization problems. There are three main effects of doing so: outcomes that are equally good
may become strictly comparable, strict comparability may be turned into incomparability,
and the order of strict comparability may be reversed. To illustrate these phenomena, we
show how they may affect strong sel-equivalence using the forthcoming examples. Importantly, they lead us towards conditions that are necessary for strong sel-equivalence and
that motivate our characterization of that property that we formally state in Theorem 6.
363

fiFaber, Truszczynski, & Woltran

Example 2 Let P1 = (T1 , S1 ), where T1 is a theory generating exactly two outcomes {a}
and {b} (for example {a  b}) and S1 =  is empty. Clearly, (P1 ) = {{a}, {b}}, because
{a} and {b} are equally good with respect to P1 . It is possible to make them comparable
i
by adding new preference rules. For example, let R1 = (, {a > b }), where i  1. Now,
{a} >P1 R1 {b} and thus (P1  R1 ) = {{a}}. It is evident that for any pair of equally good
interpretations I, J one can find a context consisting only of preference rules that will make
I strictly preferred to J (each new rule should have I at least as preferred as J and one of
them should strictly prefer I to J), and that the precise ranks in such a context are not of
importance.
Example 3 Let P2 = (T1 , S2 ), where T1 is a theory generating exactly two outcomes {a}
i
and {b}, and S2 = {a > b } for some rank i  1. Clearly, {a} >P2 {b} and therefore
(P2 ) = {{a}}. It is possible to make {a} and {b} incomparable by adding an appropriate
i
context, for example R2 = (, {b > a }). We obtain {a} >
6 P2 R2 {b} and {b} >
6 P2 R2 {a},
thus (P2  R2 ) = {{a}, {b}}.
It is important to note that the rank of the context preference rule must be exactly equal
to the rank of the original preference rule in order to achieve this effect, otherwise one
preference rule would override the other. In general, for any pair of strictly comparable
interpretations I, J one can find an appropriate context that makes I and J incomparable,
but in contrast to Example 2, the context must make use of rules of particular ranks.
Example 4 Let P3 = (T1 , S3 ), where T1 is a theory admitting exactly two outcomes {a}
i
and {b}, and S3 = {a > b } for some rank i  2. Clearly, {a} >P3 {b} and therefore
(P3 ) = {{a}}. It is possible to reverse the comparability of {a} and {b} by adding an
1
appropriate context, for example R3 = (, {b > a }). We obtain {b} >P3 R3 {a}, thus
(P3  R3 ) = {{b}}.
It is important to note that, in order to achieve this effect, the context must contain
preference rules of lower ranks than the preference rules that originally ordered I and J,
as that original ordering has to be overridden. This also means that the technique is not
applicable for preference rules of rank 1. In general, for any pair of strictly comparable
interpretations I, J, where the comparison stems from preference rules of rank > 1, adding
any context consisting of preference rules of lower rank that reverse comparability of I and J
results in the reversed strict order. As in Example 3, the context must make use of particular
ranks.
These three effects may be exploited in order to construct examples of problems that
are not strongly sel-equivalent and suggest necessary conditions for strong sel-equivalence.
The first effect can turn preferred outcomes into non-preferred, while the second and third
can turn non-preferred outcomes into preferred ones. The second and third effects imply
conditions that are more specialized (context that needs rules of particular ranks).
2

3

Example 5 Consider P4 = (T1 , {a > b }) and Q4 = (T1 , {a > b }), where T1 is a
theory admitting exactly two outcomes {a} and {b}. We have {a} >P4 {b}, {a} >Q4 {b},
and (P4 ) = (Q4 ) = {{a}}. The two problems are therefore equivalent. However, there is
364

fiStrong Equivalence of Qualitative Optimization Problems

a discrepancy with respect to the ranks of the preference rules, which we can take advantage
of in order to show that the programs are not strongly sel-equivalent.
3
Let us consider the context R4 = (, {b > a }). This context exploits the second effect
mentioned above and makes {a} and {b} incomparable with respect to Q4 extended with
R4 ({a} 6>Q4 R4 {b} and {b} 6>Q4 R4 {a}) thus turning also {b} into a preferred outcome
(Q4  R4 ) = {{a}, {b}}. On the other hand, the new preference rule has no effect on P4 ,
as its rank is weaker than that of the only preference rule of P4 , hence (P4  R4 ) = {{a}},
and therefore P4 6s,[3,i] Q4 for all i  3.
Analyzing this example, we can observe that what the context R4 exploits is a difference
in the preferred outcomes when considering only preference rules of rank lower than 3.
Indeed, ((P4 )<3 ) = {{a}} and ((Q4 )<3 ) = {{a}, {b}}.
2

4

Example 6 Next, let us consider P5 = P4 = (T1 , {a > b }) and Q5 = (T1 , {a > b }),
where T1 is a theory admitting exactly two outcomes {a} and {b}. We have {a} >P5 {b},
{a} >Q5 {b}, and (P5 ) = (Q5 ) = {{a}}. Also here, we can observe that ((P5 )<3 ) =
{{a}} and ((Q5 )<3 ) = {{a}, {b}}. The only difference to Example 5 is that the preference
rule in Q5 is of rank 4.
Also here it is possible to construct a context that witnesses that P5 is not strongly sel3
3
equivalent to Q5 , using only preference rules of rank 3: R5 = (, {a > b , b > a }).
Here, we directly add conflicting preference rules that override the preference rule in Q5
and are overridden by the preference rule in P5 . So, also here we get (P5  R5 ) = {{a}}
and (Q5  R5 ) = {{a}, {b}}, and P5 6s,[3,i] Q5 for all i  3.
Examples 5 and 6 motivate condition (1) of Theorem 6. Moreover, it is also rather easy
to see that any counterexample to P s,[i,j] Q will involve only outcomes from (P<i ) =
(Q<i ), as no selector context in the rank interval [i, j] can make other outcomes preferred.
However, from a different point of view, condition (1) of Theorem 6 is also fairly weak, as
it does not cover some easy cases of strong sel-non-equivalence, as shown in the following
example.
2

2

Example 7 Let us define P6 = P4 = (T1 , {a > b }) and Q6 = (T1 , {b > a }), where T1
is a theory admitting exactly two outcomes {a} and {b}. We have {a} >P6 {b}, {b} >Q6 {a},
and (P6 ) = {{a}} 6= (Q6 ) = {{b}}. So P6 and Q6 are not even equivalent, hence also
P6 6s,[i,j] Q6 for any rank interval [i, j]. However, ((P6 )<2 ) = {{a}, {b}} and ((Q6 )<2 ) =
{{a}, {b}}. So condition (1) of Theorem 6 is satisfied for rank intervals [2, j].
Condition (2) of Theorem 6 covers cases like the one in Example 7. This example is
rather simple, as it does not even require a context in order to create a witness for strong
sel-non-equivalence, while in general one has to create a context in order to make certain
outcomes preferred. What remains to be considered are cases in which there is a discrepancy
stemming from preference rules inside the context rank interval.
2

3

Example 8 Let P7 = (T1 , {a > b }) and Q7 = (T1 , {a > b }), where T1 is a theory
admitting exactly two outcomes {a} and {b} (this is the pair of problems P4 and Q4 from
2
Example 5). Consider now the context R7 = (, {b > a }). Unlike R4 in Example 5, this
365

fiFaber, Truszczynski, & Woltran

rule has rank 2. The context makes {a} and {b} incomparable with respect to the extended
P7 ({a} 6>P7 R7 {b} and {b} 6>P7 R7 {a}), thus turning {b} into a preferred outcome, and
keeping {a} as such. Therefore, (P7  R7 ) = {{a}, {b}}. On the other hand, the new
preference rule overrides the one of Q7 , turning {b} into a preferred outcome and making
{a} non-preferred, (Q7  R7 ) = {{b}}. Therefore P7 6s,[2,i] Q7 for all i  2.
Unlike in Example 5, ((P7 )<2 ) = ((Q7 )<2 ) = {{a}, {b}}, so there is a different reason that allows for this counterexample. Here, we observe that diff P7 ({a}, {b}) = 2 6=
diff Q7 (a, b) = 3, which allows for adding an appropriate preference rule of rank 2. It is
important that we had to add a rule of one of the two differing ranks. Indeed, there is
no context comprising only rules of rank one that can serve as a counterexample to strong
sel-equivalence, and indeed P7 s,[1,1] Q7 .
This finally motivates condition (3) of Theorem 6: if there are two outcomes (as discussed
earlier, we can restrict ourselves to outcomes in (P<i ) = (Q<i ) for a context rank interval
[i, j]) which differ on ranks such that one of the ranks is inside the rank interval, we can use
constructions as in Example 8 in order to obtain a counterexample to strong sel-equivalence.
We can show that the three conditions of Theorem 6 indeed do characterize strong selequivalence.
Theorem 6 For all ranked optimization problems P and Q, and every rank interval [i, j],
P s,[i,j] Q if and only if the following conditions hold:
1. (P<i ) = (Q<i )
2. >P(P<i ) = >Q
(Q<i )
3. for every I, J  (P<i ) such that i < diff P (I, J) or i < diff Q (I, J), diff P (I, J) =
diff Q (I, J) or both diff P (I, J) > j and diff Q (I, J) > j.
The proof of this result is quite involved and requires several auxiliary properties. We
provide it in the appendix (together with proofs of our other main results).
Next, we discuss some special cases of the characterization in Theorem 6. First, we
consider the case i = 1, which allows for a simplification of Theorem 6.
Corollary 7 For all ranked optimization problems P and Q, and every rank interval [1, j],
P s,[1,j] Q if and only if the following conditions hold:
1. (P ) = (Q)
2. >P(P ) = >Q
(Q)
3. for every I, J  (P ), diff P (I, J) = diff Q (I, J) or both diff P (I, J) > j and diff Q (I, J) >
j.
Proof. Starting from Theorem 6, we note that the selectors of P<1 and Q<1 are empty and
hence (P<1 ) = (P ) and (Q<1 ) = (Q). Moreover, if the precondition i < diff P (I, J)
or i < diff Q (I, J) in condition (3) of Theorem 6 is not satisfied for i = 1 and a pair
I, J  (P ), then one of diff P (I, J) = 1 and diff Q (I, J) = 1 holds, and together with
366

fiStrong Equivalence of Qualitative Optimization Problems

diff P (I, J) = diff Q (I, J) the consequent is satisfied in that case as well, which allows for
omitting the precondition. 2
If in addition j = , we obtain the case of rank-unrestricted selector contexts, and
condition (3) can be simplified once more, since diff P (I, J) > j and diff Q (I, J) > j never
hold for j = .
Corollary 8 For all optimization problems P and Q, P s Q (equivalently, P s,1 Q or
P s,[1,] Q) if and only if the following conditions hold:
1. (P ) = (Q)
2. >P(P ) = >Q
(Q)
3. for every I, J  (P ), diff P (I, J) = diff Q (I, J).
Next, we note that if an optimization problem P is simple (all rules are of rank 1), then
diff P (I, J) > 1 if and only if diff P (I, J) = , which is equivalent to I P J. This observation leads to the following characterization of strong sel-equivalence of simple optimization
problems.
Corollary 9 For all simple optimization problems P and Q, the following statements are
equivalent:
(a) P s Q (equivalently, P s,[1,] Q)
(b) P s,=1 Q (equivalently, P s,[1,1] Q)
(c) (P ) = (Q) and P(P ) =Q
(Q) .
Proof. The implication (a)(b) is evident from the definitions.
(b)(c) From Corollary 7, with j = 1, we obtain (P ) = (Q). The condition P(P ) =Q
(Q)
follows from conditions (2) and (3) of that corollary. Indeed, let us consider I, J  (P )
such that I P J and distinguish two cases. If (i) diff P (I, J) = 1 then I >P J and by
condition (2) of Corollary 7, also I >Q J, implying I Q J. If (ii) diff P (I, J) > 1 then
by condition (3) of Corollary 7, diff Q (I, J) > 1. Since P, Q are simple, I Q J, and
consequently I Q J. By symmetry, we also have that I Q J implies I P J. Thus,
P(P ) =Q
(Q) .
Q
P
(c)(a) From (c) it follows by Lemma 1 that >P(P ) =>Q
(Q) and (P ) =(Q) . Thus,
conditions (1) and (2) of Corollary 8 follow. To prove condition (3), let us first assume
diff P (I, J) > 1 for I, J  (P ). It follows that diff P (I, J) =  and thus I P(P ) J. Since
Q
Q
P
Q
P(P ) =Q
(Q) , we get I (Q) J and thus diff (I, J) = . Hence diff (I, J) = diff (I, J).

For diff Q (I, J) > 1 we reason analogously. In the last remaining case, diff P (I, J) = 1 and
diff Q (I, J) = 1. Thus, we directly obtain diff P (I, J) = diff Q (I, J). By Corollary 8, P s Q
follows. 2

367

fiFaber, Truszczynski, & Woltran

Corollary 9 shows, in particular, that for simple problems there is no difference between
the relations s,1 and s,=1 . This property reflects the role of preference rules of rank 2
and higher. They allow us to break ties among optimal outcomes, as defined by preference
rules of rank 1. Thus, they can eliminate some of these outcomes from the family of optimal
ones, but they cannot introduce new optimal outcomes. Therefore, they do not affect strong
sel-equivalence of simple problems. This property has the following generalization to ranked
optimization problems.
Corollary 10 Let P and Q be ranked optimization problems and let k be the maximum
rank of a preference rule in P  Q. Then the relations s,k (equivalently, s,[k,] ) and
s,=k (equivalently, s,[k,k] ) coincide.
Proof. Clearly, P s,k Q implies P s,=k Q. Thus, it is enough to prove that if P s,=k Q
then P s,k Q. Using the characterization of Theorem 6, we observe that conditions (1)
and (2) for P s,=k Q and P s,k Q are the same. Since P s,=k Q, we have that for
every I, J  (P<k ) such that k < diff P (I, J) or k < diff Q (I, J), diff P (I, J) = diff Q (I, J)
or both diff P (I, J) > k and diff Q (I, J) > k. Let us consider I, J  (P<k ) such that
diff P (I, J) > k. It follows that diff Q (I, J) > k. Since k is the maximum rank of a preference rule in P or Q, diff P (I, J) =  and diff Q (I, J) = . Thus, diff P (I, J) = diff Q (I, J).
The case diff Q (I, J) > k is similar and we obtain that for every I, J  (P<k ) such that
k < diff P (I, J) or k < diff Q (I, J), diff P (I, J) = diff Q (I, J). That property implies condition (3) for P s,k Q. Thus, P s,k Q follows. 2
Our observation on the role of preference rules with ranks higher than ranks of rules in
P or Q also implies that P and Q are strongly sel-equivalent relative to selectors consisting
exclusively of such rules if and only if P and Q are equivalent (have the same optimal
outcomes), and if optimal outcomes that tie in P also tie in Q and conversely. Formally,
we have the following result.
Corollary 11 Let P and Q be ranked optimization problems and let k be the maximum
rank of a preference rule in P  Q. Then P s,k+1 Q if and only if (P ) = (Q) and
P(P ) =Q
(Q) .
Proof. Clearly, P<k+1 = P and Q<k+1 = Q and so, (P<k+1 ) = (P ) and (Q<k+1 ) =
(Q). Thus, the only-if part follows by Theorem 6 (condition (1) of that theorem reduces
to (P ) = (Q) and condition (3) implies P(P ) =Q
(Q) ). To prove the if part, we note
that condition (1) of Theorem 6 holds by the assumption. Moreover, the relations >P(P ) and
>Q
(Q) are empty and so, they coincide. Thus, condition (2) of Theorem 6 holds. Finally, if
I, J  (P ), and diff P (I, J) > k + 1, then diff P (I, J) =  and so, I P J. By the assumption, I Q J, that is, diff Q (I, J) =  = diff P (I, J). The case when diff Q (I, J) > k + 1 is
similar. Thus, condition (3) of Theorem 6 holds, too, and P s,k+1 Q follows. 2
Lastly, we give some simple examples illustrating how our results can be used to safely
modify or simplify optimization problems, that is rewrite one into another strongly selequivalent one.
368

fiStrong Equivalence of Qualitative Optimization Problems

Example 9 Let P = (T, S), where T = {a  b  c, (a  b), (a  c), (b  c)} and S = {a >
c , b > c }, and P 0 = (T, S 0 ), where S 0 = {a  b > c }. Regarding these problems
as CO problems, we have that (P ) = (P 0 ) = {{a}, {b}, {c}}. Moreover, it is evident that
0
P(P ) =P(P 0 ) . Thus, by Corollary 9, P and P 0 are strongly sel-equivalent. In other words,
we can faithfully replace rules a > c , b > c  in the selector of any optimization problem
with generator T by the single rule a  b > c .
For an example of a more general principle, we note that removing preference rules with
only one formula in the head yields a problem that is strongly sel-equivalent.
Corollary 12 Let P and Q be two CO or ASO problems such that P g = Qg and Qs is
obtained from P s by removing all preference rules with only one formula in the head (i.e.,
rules r for which |hd (r)| = 1). Then P and Q are strongly sel-equivalent.
Proof. Conditions (1)-(3) of Theorem 6 all follow from an observation that for every interpretation I and every preference rule r with |hd (r)| = 1, vI (r) = 1. 2

4. Strong Gen-Equivalence
We now focus on the case of strong gen-equivalence. The semantics of generators makes
a difference here but the difference concerns only the fact that under the two semantics
we consider, the concepts of strong equivalence are different. Other aspects of the characterizations are the same. Specifically, generators have to be strongly equivalent relative to
the selected semantics. Indeed, as the following example shows, if the generators are not
strongly equivalent, one can extend them uniformly so that after the extension one problem
has a single outcome, which is then trivially an optimal one, too, while the other one has
no outcomes and so, no optimal ones.
Example 10 Consider a CO problem P8 = (T8 , S8 ), where T8 = {a  b} and S8 = {a >
b }. There are two outcomes here, {a} and {b}, that is, (P8 ) = {{a}, {b}}. Let r be the
only preference rule in S8 . Clearly, v{a} (r) = 1 and v{b} (r) = 2. Thus, {a} >P8 {b} and so,
(P8 ) = {{a}}.
In addition, let Q8 = (T80 , S8 ) be a CO problem, where T80 = {a  b} and S8 is as
above. Then, (Q8 ) = {{a}} and, trivially, (Q8 ) = {{a}}. It follows that P8 and Q8
are equivalent, as they specify the same optimal outcomes. However, they are not strongly
gen-equivalent (and so, also not strongly equivalent). Indeed, let R8 = ({a}, ). Then
(P8  R8 ) = {{b}} and so, (P8  R8 ) = {{b}}. On the other hand, (Q8  R8 ) =  and,
therefore, (Q8  R8 ) = .
Moreover, the preference relation > defined by the selectors of the problems considered
must coincide.
Example 11 Let P9 = (T9 , S9 ) be a CO problem, where T9 = {a  b  c, (a  b), (a 
c), (b  c)} and S9 = {a > b , a > c }. We have (P9 ) = {{a}, {b}, {c}}, and
{a} >P9 {b}, {a} >P9 {c}, and {b} and {c} are incomparable. Thus, (P9 ) = {{a}}. Let
369

fiFaber, Truszczynski, & Woltran

now Q9 = (T9 , S90 ) be a CO problem, where S90 = {a > b > c }. Clearly, (Q9 ) = (P9 ) =
{{a}, {b}, {c}}. Moreover, {a} >Q9 {b} >Q9 {c}. Thus, (Q9 ) = {{a}} and so, P9 and Q9
are equivalent. However, they are not strongly (gen-)equivalent. Indeed, let R9 = ({a}, ).
Then, (P9  R9 ) = {{b}, {c}} but (P9  R9 ) = {{b}}.
The main insight here is that differences in the preference relation may be hidden
by preferred outcomes but, if they are present, they can be exposed by eliminating the
preferred outcomes that obscure them with the appropriately selected generator context.
All of these considerations apply to both the CO and ASO problems, and therefore we
formulate a single theorem that handles both types of problems.
Theorem 13 For all CO (ASO, respectively) problems P and Q, P g Q if and only if
P g and Qg are strongly equivalent (that is, Mod (P g ) = Mod (Q g ) for CO problems, and
ModHT (P g ) = ModHT (Qg ) for ASO problems) and >PMod(P g ) = >Q
Mod(Q g ) .
In view of Examples 10 and 11, the result is not unexpected. The two examples demonstrated that the conditions of the characterization cannot, in general, be weakened.
From Corollary 8 and Theorem 13, it follows that strong sel-equivalence of CO problems
is a stronger property than their strong gen-equivalence.
Corollary 14 For all CO problems P and Q, P s Q implies P g Q.
In general the implication in Corollary 14 cannot be reversed, as shown in the following
example.
Example 12 Let us consider problems P10 = (T10 , S10 ) and Q10 = (T10 , ), where T10 =
{a  b} and S10 = {a > b , b > a }. We have (P10 ) = (Q10 ) = {{a}, {b}}.
Moreover, {a} 6P10 {b} and {b} 6P10 {a}. Thus, (P10 ) = {{a}, {b}}. Since Qs10 =
, we also have (trivially) that {a} Q10 {b}. Thus, (Q10 ) = {{a}, {b}}, too, and the
problems P10 and Q10 are equivalent. They are not strongly sel-equivalent, though. Let
R10 = (, {a > b }). Then, P10  R10 = P10 and so, (P10  R10 ) = {{a}, {b}}. On the
other hand, {a} >Q10 R10 {b}. Thus, (Q10  R10 ) = {{a}}.
However, by virtue of Theorem 13, they are strongly gen-equivalent. Indeed, trivially
g
g
g
g
Mod (P10
) = Mod (Q10
) and, writing M for Mod (P10
) = Mod (Q10
), the relations >PM10 and
10
>Q
M are both empty and therefore equal.
The relation between strong sel-equivalence and strong gen-equivalence of ASO problem
is more complex. In general, neither property implies the other even if both problems P
and Q are assumed to be simple. It is so because P s Q if and only if AS(P g ) =
g
AS(Qg ) and PAS(P g ) =Q
AS(Qg ) (Corollary 9), and P g Q if and only of ModHT (P ) =
g
g
ModHT (Qg ) and >PMod(P g ) =>Q
Mod(Q g ) (Theorem 13). Now, AS(P ) = AS(Q ) (regular
equivalence of programs) does not imply ModHT (P g ) = ModHT (Qg ) (strong equivalence)
Q
P
and >PMod(P g ) =>Q
Mod(Q g ) does not imply AS(P g ) =AS(Qg ) .
We conclude the section with one more corollary concerning strong gen-equivalence of
problems with empty selectors.

370

fiStrong Equivalence of Qualitative Optimization Problems

Corollary 15 For all CO (ASO, respectively) problems P and Q such that P s = Qs = ,
P g Q if and only if P g and Qg are strongly equivalent under the respective semantics
(that is, Mod (P g ) = Mod (Q g ) for CO problems, and ModHT (P g ) = ModHT (Qg ) for ASO
problems).
The result is evident from the definitions. However, it is also an immediate consequence
of Theorem 13. Indeed, when optimization problems P and Q have empty selectors, the cong
g
dition >PMod(P g ) = >Q
Mod(Q g ) is equivalent to Mod (P ) = Mod (Q ), which is a consequence
of strong equivalence of P g and Qg . Thus, for problems with empty selectors the right
hand of the equivalence in the assertion of Theorem 13 reduces to just strong equivalence
of the generators.

5. Strong Equivalence  the Combined Case
Finally, we consider the relation sg , which results from considering contexts that combine
both generators and selectors. Since generators may vary here, as in the previous section,
the semantics of generators matters. But, as in the previous section, the difference boils
down to different characterizations of strong equivalence of generators.
We start with a result characterizing strong equivalence of CO and ASO problems
relative to combined contexts (both generators and selectors possibly non-empty) with
selectors consisting of rules of rank at least i and at most j, respectively.
Theorem 16 For all ranked CO (ASO, respectively) problems P and Q, and every rank
s,[i,j]
interval [i, j], P g
Q if and only if the following conditions hold:
1. P g and Qg are strongly equivalent (that is, Mod (P g ) = Mod (Q g ) for CO problems,
and ModHT (P g ) = ModHT (Qg ) for ASO problems)
2. >PMod(P g ) = >Q
Mod(Q g )
3. for every I, J  Mod (P g ) such that i < diff P (I, J) or i < diff Q (I, J), diff P (I, J) =
diff Q (I, J) or both diff P (I, J) > j and diff Q (I, J) > j
P

Q

<i
<i
4. >Mod(P
g ) = >Mod(Q g ) .

The corresponding characterizations for CO and ASO problems differ only in their respective conditions (1), which now reflect different conditions guaranteeing strong equivalence of generators under the classical and answer-set semantics. Moreover, the four conditions of Theorem 16 can be obtained by suitably combining and extending the conditions
of Theorem 6 and Theorem 13. First, as combined strong equivalence implies strong genequivalence, condition (1) is taken from Theorem 13. Second, we modify conditions (2)
and (3) from Theorem 6 replacing (P<i ) with Mod (P g ) (and accordingly (Q<i ) with
Mod (Qg )), as each classical model of P g can give rise to an optimal classical or equilibrium
one upon the addition of a context, an aspect also already visible in Theorem 13.
Finally, we have to add a new condition stating that the relations >P<i and >Q<i coincide
on the sets of models of P g and Qg . When generators are allowed to be extended, one can
make any two of their models to be the only outcomes after the extension. If the two
371

fiFaber, Truszczynski, & Woltran

outcomes, say I and J, are related differently by the corresponding strict relations induced
by rules with ranks less than i, say I >P<i J but not I >Q<i J, then I is optimal and J is
not optimal in the problem extending P no matter what preference rules of ranks from the
interval [i, j] we use (rules of rank i and higher have no effect on how I and J are ordered).
On the other hand, if I and J are incomparable by >Q<i , they remain incomparable after
Q is extended. If I and J are equally good with respect to rules of rank < i, they can be
rendered incomparable by means of preference rules of rank i. In each case, J is optimal
after Q is extended. Finally, if J >Q<i I, then J remains optimal no matter what preference
rules of ranks i and higher we add. It follows that if the two relations >P<i and >Q<i are
s,[i,j]
different, we cannot have P g
Q and the condition (4) is necessary.
As in the previous section, the case when the selectors of P and Q are empty reduces
to strong gen-equivalence of the generators.
Corollary 17 For all CO (ASO, respectively) problems P and Q such that P s = Qs =
s,[i,j]
, and for every rank interval [i, j], P g
Q if and only if P g and Qg are strongly
equivalent under the respective semantics (that is, Mod (P g ) = Mod (Q g ) for CO problems,
and ModHT (P g ) = ModHT (Qg ) for ASO problems).
The result is a simple consequence of Theorem 16. The condition (1) in that theorem
implies that Mod (P g ) = Mod (Q g ) and so, since the selectors of P and Q are empty, all the
remaining conditions become trivially true.
We conclude this section with observations concerning the relation sg for both CO and
ASO problems. The contexts relevant here may contain preference rules of arbitrary ranks.
We start with the case of CO problems, where the results are stronger. While they can be
derived from the general theorems above, we will present here arguments relying on results
from previous sections, which is possible since for CO problems equivalence and strong
equivalence of generators coincide.
We saw in the last section that for CO problems s is a strictly stronger relation than
g . In fact, for CO problems, s coincides with the general relation sg .
Theorem 18 For all CO problems P and Q, P sg Q if and only if P s Q.
Proof. The only-if implication is evident. To prove the converse implication, we will
use Proposition 4, which reduces checking for strong equivalence to checking for strong
sel-equivalence. Let R  LgU be a generator problem. Since P s Q, from Corollary 8 we
have Mod (P g ) = Mod (Qg ). Consequently, Mod ((P  R)g ) = Mod ((Q  R)g ). Writing M
for Mod (P g ) and M 0 for Mod ((P  R)g ) we have M 0  M . Thus, also by Corollary 8,
QR
>PMR
0 =>M 0 . Finally, condition (3) of Corollary 8 for P and Q implies condition (3) of
that corollary for P  R and Q  R (as R has no preference rules and M 0  M ). It follows,
again by Corollary 8, that P  R s Q  R. Thus, by Proposition 4, P sg Q. 2
In the case of simple CO problems and the ranked interval [1, 1], the argument above
can be repeated using instead of Corollary 8 the equivalence of (b) and (c) from Corollary
9. In this way, one can show that for simple CO problems, the relations s,=1
and s,=1
g
coincide. Thus, by Corollary 9 (the equivalence of (a) and (b)) and Theorem 18, for simple
CO problems all four relations sg , s,=1
, s,=1 , and s coincide and we obtain the following
g
result.
372

fiStrong Equivalence of Qualitative Optimization Problems

Corollary 19 For all simple CO problems P and Q, all properties P sg Q, P s,=1
Q,
g
P s,=1 Q and P s Q are equivalent.
For simple ASO problems we still have that sg and s,=1
coincide but in general these
g
notions are different from s and s,=1 (cf. the subtle difference in condition (c) compared
to Corollary 9).
Corollary 20 For all simple ASO problems P and Q, the following conditions are equivalent
s,[1,]

(a) P sg Q (equivalently, P g

Q)

s,[1,1]

(b) P s,=1
Q (equivalently, P g
g

Q)

(c) ModHT (P g )=ModHT (Qg ) and PMod(P g ) = Q
Mod(Qg ) .
Proof. The implication (a)(b) is evident.
Let us assume (b). By Theorem 16, we have ModHT (P g )=ModHT (Qg ). This identity
implies Mod (P g ) = Mod (Qg ). Let us assume that for some I, J  Mod (P g ), I PMod(P g ) J.
Q
If I >PMod(P g ) J then, by Theorem 16, I >Q
Mod(Qg ) J and so, I Mod(Qg ) J. Otherwise,

I P J and so, diff P (I, J) = . By Theorem 16, diff Q (I, J) > 1. Since Q is simple,
diff Q (I, J) = . Thus, I Q J and, also, I Q
Mod(Qg ) J. The converse implication follows
by symmetry. Thus, (c) holds.
Finally, we assume (c) and prove (a). To this end, we show that conditions (1)(4) of
Theorem 16 hold. Directly from the assumptions, we have that condition (1) holds. Condition (2) follows from Lemma 1. Moreover, we also have that Mod (P g ) = Mod (Qg ). To prove
condition (3), let us assume that I, J  Mod (P g ) and that diff P (I, J) > 1. Since P is simple,
I P J. Thus, I Q J and, consequently, diff P (I, J) =  = diff Q (I, J). Finally, condition
P<i
Q<i
g
g
(4), i.e. >Mod(P
g ) = >Mod(Q g ) , obviously holds in case i = 1 and Mod (P ) = Mod (Q ). 2

6. Complexity
In this section, we study the problems of deciding the various notions of strong equivalence.
Typically the comparisons between sets of outcomes in the characterizations determine the
respective complexity. We start with results concerning strong sel-equivalence.
Theorem 21 Given optimization problems P and Q, deciding P s Q is co-NP-complete
in case of CO problems and P2 -complete in case of ASO problems.
Proof. [Sketch, a detailed argument is provided in Appendix B.] For membership, we focus on
the complementary problem and consider pairs of interpretations I, J that violate at least
one of the conditions stated in Corollary 8. Clearly, if such a witness pair of interpretations
exists, there is also such a witness pair that is built only from atoms that occur in the
problems P and Q. Once such a pair is guessed, it can be verified in polynomial time (for
CO problems) or in polynomial time using an NP oracle (for ASO problems) that it indeed
373

fiFaber, Truszczynski, & Woltran

violates the conjunction of the three conditions from Corollary 8. The main observation is
that model checking is polynomial for the classical semantics, but co-NP-complete for the
equilibrium semantics (see Pearce, Tompits, & Woltran, 2009, Thm. 8).
Hardness follows from considering the equivalence problem for optimizations problems
with empty selectors, which is known to be co-NP-hard (for classical semantics) and P2 hard (for equilibrium semantics, see Pearce et al., 2009, Thm. 11). 2
For the ranked case, we observe an increase in complexity, which can be explained by
the characterization given in Theorem 6. Instead of outcome checking, this characterization
involves optimal outcome checking, which is more difficult (unless the polynomial hierarchy
collapses).
Theorem 22 Given optimization problems P and Q and a rank interval [i, j], deciding
P s,[i,j] Q is P2 -complete in case of CO problems and P3 -complete in case of ASO
problems.
Proof. [Sketch, a detailed argument is provided in Appendix B.] The membership part essentially follows the same arguments as the proof of Theorem 21, but here the problem of
deciding I  (P<i ) is in co-NP for CO problems and in P2 for ASO problems.
For the hardness part, we reduce the following problem to sel-equivalence of CO problems: Given two propositional theories S and T , decide whether they possess the same
minimal models. This problem is known to be P2 -complete (e.g., Eiter et al., 2007b,
Thm. 6.15), and the problem remains hard if S and T are in negation normal form (NNF)
given over the same alphabet. We adapt a construction used by Brewka et al., (2011).
Given a negation normal form theory T , we construct a CO problem PT by setting
PTg

= T [u/u0 ]  {u  u0 | u  U }, and

PTs

= {u0 > u | u  U },

where U denotes the set of atoms occurring in T , and T [u/u0 ] stands for the theory
resulting from replacing all u by u0 in T (we note that the ranks of rules in the selector
are 1).
The elements in (PT ) are in a one-to-one correspondence to the minimal models of
T . For theories S and T over U it follows that S and T have the same minimal models if
and only if (PS ) = (PT ). Since the problems PS and PT have the same selectors, that
latter condition is equivalent to PS s,2 PT (which can be shown directly or exploiting our
characterization of s,2 ).
Concerning the hardness part for ASO problems, we use the following problem: given
two open quantified Boolean formulas (QBFs) Y (X, Y ), Y (X, Y ), decide whether they
possess the same minimal models. This problem is P3 -hard (see Lemma 30 in Appendix
A). For (X, Y ), we construct P as follows:
Pg = {z  z 0 | z  X  Y } 
{(y  y 0 )  w, w  y, w  y 0 | y  Y } 
{[z/z 0 ]  w, w  w},
Ps = {x0 > x | x  X},
374

fiStrong Equivalence of Qualitative Optimization Problems

where [z/z 0 ] stands for the formula obtained by replacing all z by z 0 in (X, Y ) (again,
we stress that the ranks of rules in the selector are 1). The elements in (P ) are in a
one-to-one correspondence to the minimal models of Y (X, Y ). Now we can reason as
above and show that for  and  over X  Y , formulas Y (X, Y ) and Y (X, Y ) have
the same minimal models if and only if P s,2 P . 2
In Theorem 22 the rank interval [i, j] is given as input. When fixing the interval, the
hardness results still hold, provided that i > 1. In fact, the critical condition in Corollary 7
is (P<i ) = (Q<i ); for rank intervals [1, j], the selectors become empty and the condition
is reduced to (P ) = (Q), which is easier to decide.
Our characterizations imply that all the remaining problems are in co-NP. For strong
gen-equivalence, co-NP-hardness follows directly from Theorem 13 and co-NP-completeness
of deciding strong equivalence between two propositional theories (for both semantics).
Theorem 23 Given two CO (ASO, respectively) problems P and Q, deciding P g Q is
co-NP-complete.
Finally, for the combined case the hardness result follows from Theorem 16 and co-NPcompleteness of deciding strong equivalence of propositional theories.
Theorem 24 Given ranked CO (ASO, respectively) problems P and Q, and rank interval
s,[i,j]
[i, j], deciding P g
Q is co-NP-complete.
By construction, all hardness results hold already for simple optimization problems.

7. Discussion
We introduced the formalism of optimization problems, generalizing the principles of ASO
programs, in particular, the separation of hard and soft constraints (Brewka et al., 2003).
We focused on two important specializations of optimization problems: CO problems and
ASO problems. We studied various forms of strong equivalence for these classes of optimization problems, depending on what contexts are considered. Specifically, we considered
the following cases: new preference information is added, but the hard constraints remain unchanged (strong sel-equivalence); hard constraints are added but preferences remain
unchanged (strong gen-equivalence); both hard constraints and preferences can be added
(strong equivalence). To the best of our knowledge, this natural classification of equivalences in preference formalisms has not been studied yet. In certain cases some of these
notions coincide (Theorem 18) but this is no longer true when the underlying semantics is
changed or ranks in contexts are restricted.
In previous work, the notion of strong equivalence (both hard constraints and preferences
can be added) has been studied for logic programs with weak constraints by Eiter et al.,
(2007a) and logic programs with ordered disjunctions (LPODs) by Faber et al., (2008).
While for the former formalism, a separation of strong equivalence into different notions
 as suggested here for ASO problems  would be possible (it is instructive to compare
Eiter et al., 2007a, Lemma 23, to our results, e.g., Corollary 20), a similar separation for
strong equivalence is not straightforward for LPODs. The reason is the syntactic nature of
LPOD rules which act like hard constraints and preference rules at the same time. Faber et
375

fiFaber, Truszczynski, & Woltran

al., (2008) considered strong equivalence with respect to contexts that are logic programs
(which is similar to strong gen-equivalence) and the combined case of strong equivalence
(called strong equivalence for arbitrary contexts there), but they did not consider any
counterpart to the notion of strong sel-equivalence. In fact, it is even unclear whether in
every LPOD the generating and selecting modules can be cleanly separated.
In our paper, we established characterizations of all three types of strong equivalence.
They exhibit striking similarities. The characterizations of strong sel-equivalence for CO
and ASO problems in Theorem 6 are precisely the same, mirroring the fact that generators are not subject to change. Theorem 13 concerns strong gen-equivalence for CO and
ASO problems. In each case, the characterizations consist of two requirements: the strong
equivalence of generators, and the equality of the strict preference relations restricted to
the class of models of the generators. The only difference comes from the fact that strong
equivalence for classical and the equilibrium-model semantics have different characterizations. Theorem 16 which concerns the combined case of strong equivalence also does not
differentiate between CO and ASO problems other than implicitly (as before, the conditions
of strong equivalence are different for the two semantics). Moreover, the characterizations
provided by Theorem 16 arise in a rather systematic way from those given in Theorems 6
and 13. This being the case in each of the different semantics we used strongly suggests that
there are some abstract principles at play here. We are currently pursuing this direction,
conjecturing that this is an inherent feature of preference formalisms with separation of
logical and preferential constraints.
Coming back to LPODs, these comments suggest that identifying a split representation for that formalism might be of interest. It could lead to alternative characterizations of (combined) strong equivalence derived from the characterizations of the two
one-dimensional variants.
Next, we note that our results give rise to problem rewriting methods that transform
optimization problems into strongly equivalent ones. We provided two simple examples
illustrating that application of our results in Example 9 and Corollary 12. Similar examples
can be constructed for our results concerning strong gen-equivalence and (combined) strong
equivalence. A more systematic study of optimization problem rewriting rules that result
in strongly equivalent problems will be a subject of future work.
Finally, we established the complexity of deciding whether optimization problems are
strongly equivalent. Notably, in the general case of strong (combined) equivalence the problem is co-NP-complete for both CO and ASO problems. The same holds true for the strong
gen-equivalence problem. For the strong sel-equivalence problem, the situation is more
complex. When contexts of the form [1, j] or [1, ] are considered, the problem of deciding
strong sel-equivalence is co-NP-complete for CO problems and P2 -complete for ASO problems. If any rank interval is allowed as part of input or if the rank interval is fixed to [i, j],
with i  2, the problem gets computationally harder: in case of ASO problems, P3 -hard; in
case of CO problems P2 -hard. The difference between CO problems and ASO problems in
the case of strong sel-equivalence with respect to contexts consisting of preference rules with
ranks in the intervals [1, j] or [1, ] comes from the fact that the corresponding concepts
of strong sel-equivalence depend, in particular, on whether two theories are equivalent with
respect to models (CO problems) and with respect to equilibrium models (ASO problems).
These two types of equivalence have different complexities. The jump in the complexity for
376

fiStrong Equivalence of Qualitative Optimization Problems

strong sel-equivalence when arbitrary rank intervals [i, j] are allowed or if they are fixed so
that i  2 comes from the fact that in such cases, the concept depends on properties of the
class of outcomes that are optimal with respect to rules of ranks less than i, while in other
cases it depends on properties of the class of models. Decision problems concerning optimal
outcomes (such as: do two theories have the same optimal models) are harder than the
corresponding versions of the problems for models, explaining the jump. The results for
strong sel-equivalence also imply that ranked optimization problems cannot be efficiently
simulated by simple optimization problems.

Acknowledgments
We thank the reviewers for their useful and constructive comments. The first author was
supported by Regione Calabria and EU under POR Calabria FESR 2007-2013 within the
PIA project of DLVSYSTEM s.r.l., and by MIUR under the PRIN project LoDeN. The
second author was supported by the NSF grant IIS-0913459.

Appendix A. Useful Lemmas
We provide here several lemmas that we use later in the proofs of the results discussed in
the main body of the paper.
The first two lemmas are given without proofs, as they are easy consequences of results
by Ferraris (2005) and Ferraris and Lifschitz (2005).
Lemma 25 Let P be a theory, I be a classical model of P , and let [I] = {a   | a 
U \ I}  {a   | a  I}. Then, AS (P  [I]) = Mod (P   [I ]) = {I}.
Lemma 26 Let P be a theory, I, J two of its (classical) models such that I 6= J, and let
[I, J] = {a  b | a  I, b  J}
 {a  b | a  I, b  U \ J}
 {a  b | a  U \ I, b  J}
 {a  b | a  U \ I, b  U \ J}.
Then AS (P  [I, J]) = Mod (P   [I , J ])) = {I, J}.
Lemma 27 Let P be an optimization problem, I  (P<j ), where j  1, and let
j

j

Rj [I] = {a > >  | a  I}  {a > >  | a  U \ I)}.
Then
1. I  (P  Rj [I]);
2. for every J such that J 6= I and I Pj J, I >P Rj [I] J.
377

fiFaber, Truszczynski, & Woltran

Proof. When proving (1), to simplify the notation, we write R for Rj [I]. Since I  (P<j ),
I  (P ). Clearly, (P  R) = (P ) and so, I  (P  R). To show that I  (P  R),
let us consider an arbitrary interpretation J  (P  R) and assume that J >P R I. In
particular, J 6= I and so, diff R (I, J) = j. If diff P (I, J) < j, then diff P R (I, J) < j.
Consequently, J >(P R)<j I. Since all rules in R are of rank j, it follows that J >P<j I, a
contradiction with the fact that I  (P<j ). Thus, diff P (I, J)  j. Since diff R (I, J) = j,
we have diff P R (I, J) = j. Therefore, J >P R I implies J >R I, a contradiction again
(since, by definition of Rj [I] = R, I R J for each interpretation J). It follows that for
every J  (P  R), J 6>P R I, that is, I  (P  R).
The assertion (2) is evident, since by definition of Rj [I] = R, I >R J for each interpretation J 6= I. 2

Lemma 28 Let P be an optimization problem, I, J interpretations such that I, J  (P<j ),
be the union of the following sets of rules:
where j  1, and let Rj0 [I, J]  Ls,j
U
j

{a  b > >  | a, b  I  J}
j

{a  b > >  | a  I, b  U \ J}
j

{a  b > >  | a  U \ I, b  J}
j

{a  b > >  | a  U \ I, b  U \ J}.
Then all of the following hold:
1. for every r  Rj0 [I, J], vI (r) = vJ (r) = 1;
2. for every interpretation K 
/ {I, J}, there is a rule r  Rj0 [I, J] such that vK (r) = 2;
3. I >P J if and only if J 
/ (P  Rj0 [I, J]).
Proof. To simplify the notation, we write R0 for Rj0 [I, J].
The assertion (1) is evident. To prove the assertion (2), we note that the conjunction
of all formulas that appear as top options in the preference rules of R0 is equivalent to
^
^
^
^


{a | a  I}  {a | a  U \ I} 
{b | b  J}  {b | b  U \ J} .
That formula has only two models: I and J. Thus, for every other interpretation K, at
least one of the formulas that appears as top options in the preference rules of R0 is not
satisfied by K. For the corresponding preference rule r, vK (r) = 2.
Finally, to prove the assertion (3), let us assume that I >P J. Together with (1), it
0
implies that I >P R J. Thus, J 
/ (P  R0 ). To prove the converse implication, let us
0
assume that I 6>P J. Together with (1), it implies that I 6>P R J. Next, we note that if
0
diff P (J, K) < j, then since J  (P<j ), K 6>P R J. If diff P (J, K)  j, then property (2)
0
proved above implies that K 6>P R J. Since K is an arbitrary interpretation different from
0
I and J, and since I 6>P R J, J  (P  R0 ) follows. 2

378

fiStrong Equivalence of Qualitative Optimization Problems

Next, we note a property that allows us to infer the strong sel-equivalence of two problems treated as CO problems from the strong sel-equivalence of the same problems when
treated as ASO problems (and conversely). The property relies on the fact that changing
selectors only does not affect the class of outcomes. The proof is simple and we omit it.
Lemma 29 Let P and Q be optimization problems such that Mod (P g ) = AS (P g ) and
Mod (Q g ) = AS (Qg ), and [i, j] a rank interval. Then, P s,[i,j] Q when P and Q are viewed
as CO problems, if and only if P s,[i,j] Q when P and Q are viewed as ASO problems.
The final results in this section will be useful for the complexity results.
Lemma 30 Deciding whether open QBFs Y (X, Y ) and Y (X, Y ), where  and  are
in negation normal form, have the same minimal models is P3 -hard.
Proof. We show the result by a reduction from the P3 -hard problem of deciding satisfiability for QBFs of the form ZXY , where  is in negation normal form. Let  be a
QBF of such a form and consider the following formulas, where Z 0 = {z 0 | z  Z}, and u
and v are fresh atoms:
^
^

 =
(z  z 0 )  (
x  u)    (v  v)
zZ

 =

^
zZ

xX
0

(z  z )  (

^


x  v)    (u  u).

xX

It is clear that  and  are in negation normal form. The only difference between 
compared to  is that the latter uses u where the former uses v and vice versa, and the
only point of including the conjuncts v  v and u  u is to have occurrences of u and v
in both  and . We show that Y (U, Y ) and Y (U, Y ) have the same minimal models
(with open variables U = Z  Z 0  X  {u, v}) if and only if  is true.
We note that when considering models and minimal models of Y (U, Y ) and Y (U, Y )
we can move the quantifier Y so that it appears directly in front of . It is so because
there are no occurrences of atoms from Y in  and in  outside of .
It is also clear that if M contains neither u nor v, then M is a model of Y (U, Y ) if
and only if M is a model of Y (U, Y ). Consequently, for each such M it holds that M is
a minimal model of Y (U, Y ) if and only if M is a minimal model of Y (U, Y ).
Only-if direction: Let us assume  is false. Then, there exists an interpretation I  Z
of atoms in Z, such that for every interpretation J of atoms in X, Y  is false. Let us
consider Mu = I  (Z \ I)0  X  {u}. Clearly, Mu is a model of YV. If N is a model of
Y  and N  Mu , then I  (Z \ I)0  N because of the conjunct zZ (z  z 0 ). Thus,
N  Z = I. It follows that Y  is false when atoms in Z and X are interpreted by N and,
consequently, X  {u}  N . Thus, N = Mu , which implies that Mu is a minimal model of
Y . Essentially the same argument shows that Mv = I  (Z \ I)0  X  {v} is a minimal
model of Y . Since Mu 6= Mv , Y  and Y  have different minimal models.
If-direction: Let us assume that ZXY  is true. Let M be a minimal model of Y .
Clearly, v 
/ M (as M \ {v} is also a model of Y ). Let us assume that u  M . Let us
assume in addition that X \ M 6= . These assumptions imply that for the interpretations
379

fiFaber, Truszczynski, & Woltran

I = M Z of atoms in Z and J = M X of atoms in X, Y  is true. It follows that M \{u}
is a model of Y , a contradiction. Thus, X  M . Let I = M  Z. Since ZXY  is
true, there is an interpretation J  X of atoms in X such that Y  is true, with atoms
in Z interpreted by I and atoms in X interpreted by J. It follows that I  (Z \ I)0  J
is a model of Y . Since I  (Z \ I)0  J  M , this is a contradiction. It follows that
u
/ M . Consequently, by the comment above, M is a minimal model of . The converse
holds by the symmetry argument. Thus, the two formulas have the same minimal models. 2

Lemma 31 Given a ranked preference rule r, and an interpretation I, calculating vI (r)
can be done in polynomial time.
Proof. Initialize a variable i with 1. Check whether I 6|= bd (r) and if so, halt. Then, check
whether I |= hd i (r) and if so, halt; otherwise increment i and continue checking; if no more
options in the head of r exist, set i = 1. Each of the checks is a model checking task for a
propositional formula and hence in polynomial time. Upon halting, i is equal to vI (r). 2

Lemma 32 Given an optimization problem P and two interpretations I, J, calculating
diff P (I, J) can be done in polynomial time.
Proof. Initialize a variable x with , scan the rules in P s and for each ranked preference
rule r  P s , determine whether vI (r) 6= vJ (r) (in polynomial time due to Lemma 31). If so,
set x to rank (r) if rank (r) < x. After having processed all rules, x is equal to diff P (I, J). 2

Lemma 33 Given an optimization problem P , and two interpretations I, J, deciding whether
I >P J holds can be done in polynomial time.
Proof. First, sort the rules in P s by their ranks. Starting from the lowest rank upwards,
do the following for each rank i: Check for all rules of rank i whether vI (r) < vJ (r) or
vI (r)  vJ (r). If vI (r) < vJ (r) holds at least for one rule and vI (r)  vJ (r) for all other
rules of rank i, accept. If there are rules r and r0 of the rank i such that vI (r) < vJ (r) and
vI (r0 ) > vJ (r0 ), reject. If all ranks have been processed, reject. By Lemma 31, all steps are
doable in polynomial time. 2

Lemma 34 Given a classical optimization problem P and an interpretation I, deciding
whether I  (P ) is in co-NP.
Proof. We show that a witness J for the complementary problem (deciding whether
I 
/ (P )) can be verified in polynomial time. If J = I, verify in polynomial time that
I does not satisfy the propositional theory P g , which is well-known to be feasible in polynomial time. Otherwise, verify in polynomial time that J satisfies P g and that J >P I (both
in polynomial time, the latter by Lemma 33). 2

380

fiStrong Equivalence of Qualitative Optimization Problems

Lemma 35 Given an answer set optimization problem P and an interpretation I, deciding
whether I  (P ) is in P2 .
Proof. We show that a witness J for the complementary problem (deciding whether
I 
/ (P )) can be verified in polynomial time using an NP oracle. If J = I, verify that I
does not satisfy P g using the NP oracle. This is possible because answer-set checking is
co-NP-complete (Pearce et al., 2009, Thm. 8). Otherwise, verify using the NP oracle that J
satisfies the propositional theory P g and that J >P I (in polynomial time by Lemma 33). 2

Appendix B. Proofs
Theorem 6 For all ranked optimization problems P and Q, and every rank interval [i, j],
P s,[i,j] Q if and only if the following conditions hold:
1. (P<i ) = (Q<i )
2. >P(P<i ) = >Q
(Q<i )
3. for every I, J  (P<i ) such that i < diff P (I, J) or i < diff Q (I, J), diff P (I, J) =
diff Q (I, J) or both diff P (I, J) > j and diff Q (I, J) > j.
s,[i,j]

Proof. () Let R  LU
and let I  (P  R). By Lemma 3, I  ((P  R)<i ). Since
s,[i,j]
R  LU , R<i = (, ). Thus, I  (P<i ). By assumption, it follows that I  (Q<i ).
In particular, I  (Q<i ) and, as (Q<i ) = (Q), I  (Q). Since Rg = , we have
I  (Q  R). To show that I  (Q  R) we have to show that there is no J  (Q  R)
such that J >QR I. Let us assume to the contrary that such a J exists. By Lemma 5,
there are three possibilities.
First, we assume that diff Q (I, J) < diff R (I, J) and J >Q I. The latter property
implies diff Q (I, J)  i (otherwise, we would have J >Q<i I, contrary to I  (Q<i )).
In particular, we have I Q<i J and, since I  (Q<i ), it follows that J  (Q<i ). By
(1), J  (P<i ). Thus, by (2), J >P I. If diff Q (I, J)  j then diff R (I, J) > j and, as
s,[i,j]
R  LU , diff R (I, J) = . Since J >P I, J >P R I. Otherwise, i  diff Q (I, J) < j. If
i < diff Q (I, J) then, by (3), diff P (I, J) = diff Q (I, J). If i = diff Q (I, J), then again by (3),
diff P (I, J)  i. In either case, diff P (I, J) < diff R (I, J). Since J >P I, J >P R I.
s,[i,j]
Next, let us assume that diff Q (I, J) > diff R (I, J) and J >R I. Since R  LU ,
it follows that diff R (I, J)  i and so, diff Q (I, J) > i. We recall that I  (Q<i ).
Thus, J  (Q<i ) and, consequently, J  (P<i ). If diff Q (I, J)  j then, by (3),
diff P (I, J) = diff Q (I, J) and so, diff P (I, J) > diff R (I, J). If j < diff Q (I, J) then, also
by (3), j < diff P (I, J). Since diff Q (I, J) > diff R (I, J), diff R (I, J) <  and, consequently,
diff R (I, J)  j. Thus, diff P (I, J) > diff R (I, J) in this case, too. Since J >R I, J >P R I
follows.
Finally, let us assume that diff Q (I, J) = diff R (I, J), J >Q I and J >R I. Since
s,[i,j]
R  LU , diff R (I, J)  i. Thus, diff Q (I, J)  i and, since I  (Q<i ), J  (Q<i ). By
(1) we have J  (P<i ) and, by (2), J >P I. Consequently, J >P R I.
In all cases we obtained J >P R I, contrary to I  (P  R), a contradiction.
381

fiFaber, Truszczynski, & Woltran

() Let us assume that (P<i ) 6= (Q<i ). Without loss of generality, we can assume
that there is I  (P<i ) \ (Q<i ) and define R = (, Ri [I])  LUs,=i , where Ri [I] is as in
Lemma 27. By that lemma, I  (P  R). On the other hand, since I 
/ (Q<i ) and
s,=i Q, contrary to
R  Ls,=i
,
I

/
((Q

R)
).
By
Lemma
3,
I

/
(Q

R).
Thus,
P

6
<i
U
the assumption.
It follows that (P<i ) = (Q<i ), that is, that condition (1) holds. To prove condition
(2), let us consider interpretations I, J  (P<i ) such that I >P J. Let Ri0 [I, J] be the
selector defined in Lemma 28. Since I >P J, Lemma 28(3) implies that J 
/ (P  Ri0 [I, J]).
Consequently, J 
/ (Q  Ri0 [I, J]). By Lemma 28(3) again, it follows that I >Q J. By
Q
symmetry, I > J implies I >P J and so, condition (2) holds.
To prove condition (3), let us assume that there are interpretations I and J that satisfy
the assumptions but violate the corresponding conclusion. In what follows, we write p for
diff P (I, J) and q for diff Q (I, J). Thus, we have p > i or q > i, p 6= q, and p  j or
q  j. Without loss of generality, we can assume that p < q. It follows that p is finite and,
consequently, that diff P (I, J) <  and I 6= J. Moreover, i < q and p  j.
Let us assume first that p < i. We take the problem R = (, Ri [J]), where Ri [J] is as
specified in Lemma 27 and define P 0 = P  R and Q0 = Q  R. Since I, J  (P<i ) we
also have I, J  (Q<i ). By our assumptions, q > i. Thus, J Qi I and, in particular,
J Qi I. We recall that I 6= J. Consequently, by the assertion (2) of Lemma 27 we have
0
0 ) and diff P 0 (I, J) =
that J >Q I. Since all rules in R have ranks i, we have I, J  (P<i
0
0
diff P (I, J) < i. It follows that J 6>P I (otherwise, by diff P (I, J) < i we would have
0
J >P<i I). Let us define R0 = (, Ri0 [I, J]), where Ri0 [I, J] is as specified in Lemma 28. Since
0
J 6>P I, by the assertion (3) of that lemma, I  (P 0  R0 ). We have P 0  R0 = P  (R  R0 ).
Thus, I  (P  (R  R0 )) and, by P s,[i,j] Q, I  (Q  (R  R0 )) = (Q0  R0 ). By the
0
assertion (3) of Lemma 28, J 6>Q I, a contradiction.
Next, let p = i. Clearly, I 6>P J or J 6>P I. Without loss of generality, let us assume
that J 6>P I. Let R0 = (, Ri [I, J]), and let us define P 0 = P  R0 and Q0 = Q  R0 . Since
0 ). Moreover, from J 6>P I
all rules in R0 have ranks i, I, J  (P<i ) implies I, J  (P<i
0
it follows by Lemma 28(1) that J 6>P I. Let R = (, Ri [J]). All rules in R have rank i
0
0
and diff P (I, J) = diff P (I, J) = i. Thus, it follows that J 6>P R I. Moreover, for every
0
0
0 ). If diff P 0 (K, I)  i,
K
/ {I, J}, if diff P (K, I) < i, then K 6>P R I follows from I  (P<i
0
then K 6>P R I follows from Lemma 28(2). Thus, I  (P 0  R). On the other hand, we
0
recall that diff Q (I, J) = q > i. Thus, diff Q (I, J) > i, too (Lemma 28(1)). It follows that
0
0
J Qi I. Consequently, by Lemma 27(2), we have J >Q R I. Thus, I 
/ (Q0  R), a
contradiction.
It follows that p > i. To complete the proof of (3), we recall that p  j. Clearly, I 6>P J
or J 6>P I. Without loss of generality, let us assume that J 6>P I. Let R0 = (, Ri [I, J]), and
let us define P 0 = P  R0 and Q0 = Q  R0 . Let us assume that for some interpretation K 
/
0
P0
P
P
{I, J}, K > I. By Lemma 28(2), it follows that diff (I, K) < i. Thus, diff (I, K) < i,
0
a contradiction with I  (P<i ). Thus, for every interpretation K 
/ {I, J}, K 6>P I
0
and, by the same argument, K 6>P J. Consequently, for every interpretation K 
/ {I, J},
0
0
P<p
P<p
K 6>
I and K 6>
J. In addition, since I, J  (P<i ), by Lemma 28(1) we obtain
0
0
0 ). In addition, by Lemma 28(1),
that neither I >P<p J nor J >P<p I. Thus, I, J  (P<p
0
0
diff P (I, J) = diff P (I, J) = p and, since J 6>P I, J 6>P I.
382

fiStrong Equivalence of Qualitative Optimization Problems

0

0

Let R = (, Rp [J]). As (i) diff P (I, J) = p, (ii) J 6>P I, and (iii) all rules in R have
0
0
rank p, it follows that J 6>P R I. Moreover, for every K 
/ {I, J}, if diff P (K, I) < i, then
0
0
0
0 ). If diff P (K, I)  i, then K 6>P R I follows from
K 6>P R I follows from I  (P<i
Lemma 28(2) (and the definition of P 0 ). Thus, I  (P 0  R). On the other hand, we
0
recall that diff Q (I, J) = q > p. Thus, diff Q (I, J) > p, too (Lemma 28(1)). It follows that
0
0
/ (Q0  R), a
J Qp I. Consequently, by Lemma 27(2), we have J >Q R I. Thus, I 
s,[i,j]
0
0
0
contradiction (we recall that P 
Q, P = P  (R  R ), Q = Q  (R  R0 ), and, as
s,[i,j]
p  j, R  R0  LU ). 2
Theorem 13 For all CO (ASO, respectively) problems P and Q, P g Q if and only if
P g and Qg are strongly equivalent (that is, Mod (P g ) = Mod (Q g ) for CO problems, and
ModHT (P g ) = ModHT (Qg ) for ASO problems) and >PMod(P g ) = >Q
Mod(Q g ) .
Proof. () The first assumption implies strong equivalence of the generators P g and Qg
relative to the corresponding semantics (we recall that in the case of classical semantics,
strong and standard equivalence coincide). It follows that for every problem R  LgU ,
(P  R) = (Q  R). Moreover, for each semantics, (P  R)  Mod (P g  Rg )  Mod (P g )
and, similarly, (Q  R)  Mod (Qg  Rg )  Mod (Qg ). Since >PMod(P g ) = >Q
Mod(Q g ) , and
QR
R
By Lemma 2,
R  LgU does not change preferences, we have >P(P
g Rg ) = >(Qg Rg ) .
(P  R) = (Q  R).

() Let us assume that P g and Qg are not strongly equivalent. Then, there is a problem
R  LgU such that (P  R) 6= (Q  R). Without loss of generality, we can assume that for
some interpretation I, I  (P  R) \ (Q  R). Let us define a problem T  LgU by setting
T = ([I], ), where [I] is as defined in Lemma 25. By that lemma, (P  R  T ) = {I}
and (Q  R  T ) = . The former property implies that I is necessarily preferred, that is
I  (P RT ), and the latter one implies that (QRT ) = . This is a contradiction with
the assumption that P g Q. Thus, P g and Qg are strongly equivalent, that is, Mod (P g ) =
Mod (Qg ), in the case P and Q are CO problems, and ModHT (P g ) = ModHT (Qg ), in the
case P and Q are ASO problems.
Since ModHT (P g ) = ModHT (Qg ) implies Mod (P g ) = Mod (Qg ), the latter identity holds
in each of the two cases. Because of the equality, we will write M for both Mod (P g ) and
Mod (Qg ). It remains to show that >PM =>Q
M . Towards a contradiction, let us assume that
there are I, J  M that are in exactly one of these two relations; without loss of generality
we will assume that I >P J and I 6>Q J. The former identity implies, in particular, that
I 6= J. Let T = ([I, J], ), where [I, J] is a theory defined in Lemma 26. By that lemma,
(P  T ) = (Q  T ) = {I, J}. Clearly, J 
/ (P  T ) and J  (Q  T ), contrary to our
assumption that P g Q. 2
Theorem 16 For all ranked CO (ASO, respectively) problems P and Q, and every rank
s,[i,j]
interval [i, j], P g
Q if and only if the following conditions hold:
1. P g and Qg are strongly equivalent (that is, Mod (P g ) = Mod (Q g ) for CO problems,
and ModHT (P g ) = ModHT (Qg ) for ASO problems)
2. >PMod(P g ) = >Q
Mod(Q g )
383

fiFaber, Truszczynski, & Woltran

3. for every I, J  Mod (P g ) such that i < diff P (I, J) or i < diff Q (I, J), diff P (I, J) =
diff Q (I, J) or both diff P (I, J) > j and diff Q (I, J) > j
P

Q

<i
<i
4. >Mod(P
g ) = >Mod(Q g ) .

Proof. () By Proposition 4, it suffices to prove that for every R  LgU , P R s,[i,j] QR.
By (1), we have that (P  R) = (Q  R) (we recall that (P ) denotes the set of outcomes of an optimization problem P ; (P ) = Mod (P g ) in the case of CO problems, and
(P ) = AS (P g ) in the case of ASO problems). Moreover, for each type of problems, we also
have (P  R)  Mod (P g  R g )  Mod (P g ) and, similarly, (Q  R)  Mod (Q g  R g ) 
Mod (Q g ). Since all rules in R have rank at least i, by condition (4) it follows that
(P R)
(QR)
>(P R)<i =>(QR)<i . By Lemma 2, we have ((P  R)<i ) = ((Q  R)<i ) and so, condition
(1) of Theorem 6 holds for P  R and Q  R. Since ((P  R)<i )  (P  R)  Mod (P g ),
and since the corresponding inclusions hold for Q, too, conditions (2)(3) of this theorem
for P and Q imply conditions (2)(3) from Theorem 6 for P  R and Q  R. Thus, by
Theorem 6, P  R s,[i,j] Q  R.
s,[i,j]

() Let us assume that P g
Q. Then, P g Q follows and, by Theorem 13, implies the
appropriate version of condition (1). Since ModHT (P g ) = ModHT (Qg ) implies Mod (P g ) =
Mod (Q g ), for each of the two versions of the assertion we have Mod (P g ) = Mod (Q g ).
From now on in the proof, we write M for Mod (P g ) and, because of the equality, also for
Mod (Q g ).
Next, for interpretations I, J  M such that I 6= J, we define R = ([I, J], ), where
[I, J] is as in Lemma 26. Let us define P1 = P R and Q1 = QR. We have P1 s,[i,j] Q1 .
Moreover, by Lemma 26, we also have that (P1 ) = (Q1 ) = {I, J}.
To prove condition (4), let us assume that I >P<i J. It follows that I >P1 J (we recall
that R contains no preference rules). Since (P1 ) = {I, J}, J 
/ (P1 ) and I  (P1 ). By
the assumption, J 
/ (Q1 ). Since (Q1 ) = {I, J}, we have that I >Q1 J. In particular,
I  (Q1 ). If diff Q (I, J) < i then, since R has no preference rules, I >Q<i J. Thus, let us
assume that diff Q (I, J)  i and let us define R0 = (, Ri [J]), where Ri [J] is as in Lemma
27. Since (i) I >P<i J, (ii) R has no preference rules, and (iii) all preference rules in R0 have
0
rank i, it follows that I >P1 R J. The generator module in R0 is empty. It follows that
(P1  R0 ) = (Q1  R0 ) = {I, J}. Thus, J 
/ (P1  R0 ) and, consequently, J 
/ (Q1  R0 ).
Q
Q1
Since diff (I, J)  i, diff (I, J)  i. Moreover, I  (Q1 ) and so, also I  ((Q1 )<i ).
Thus, J  ((Q1 )<i ). By Lemma 27, J  (Q1  R0 ), a contradiction. The argument shows
that I >P<i J implies I >Q<i J. The converse implication follows by symmetry and so,
condition (4) holds.
To prove condition (2), let us assume that I >P J. If diff P (I, J) < i, then I >P<i J
and, by (4), I >Q<i J. Thus, I >Q J. Let us assume then that diff P (I, J)  i. Since
(P1 ) = {I, J} and since I >P J implies I >P1 J, J 
/ (P1 ). Thus, J 
/ (Q1 ). Since
(Q1 ) = {I, J}, I >Q1 J and so, I >Q J.
To prove condition (3), without loss of generality we assume that i < diff P (I, J). Thus,
we also have i < diff P1 (I, J) and that I, J  ((P1 )<i ), the latter follows from the properties that (P1 ) = {I, J} and that diff P1 (I, J) = diff P (I, J) > i. Since P1 s,[i,j] Q1 ,
condition (3) of Theorem 6 holds for P1 , Q1 , I and J, that is, diff P1 (I, J) = diff Q1 (I, J) or
both diff P1 (I, J) > j and diff Q1 (I, J) > j. Consequently, diff P (I, J) = diff Q (I, J) or both
384

fiStrong Equivalence of Qualitative Optimization Problems

diff P (I, J) > j and diff Q (I, J) > j, that is, condition (3) holds. 2
Theorem 21 Given optimization problems P and Q, deciding P s Q is co-NP-complete
in case of CO problems and P2 -complete in case of ASO problems.
Proof. For membership, we consider the complementary problem. By Corollary 8, it can
be specified as the problem to decide whether at least one of the conditions (1) - (3) below
holds:
1. (P ) 6= (Q), or equivalently, for some I, exactly one of the identities I  (P ) and
I  (Q) holds;
2. for some I, J  (P ), diff P (I, J) 6= diff Q (I, J);
3. >P(P ) 6= >Q
(Q) , or equivalently, for some I, J, we have I, J  (P )  (Q) and exactly
one of the properties I >P J and I >Q J holds.
We show that the problem is in NP for CO problems and in P2 for ASO problems. We
say that a pair of interpretations I, J is a witness for an instance of that problem to be a
YES instance if it demonstrates one of the conditions (1) - (3) to hold. It is easy to see
that if a witness exists, then there is a witness I, J such that I and J consist only of atoms
that occur in P and Q. Once such a witness is guessed, condition (1) can be verified in
polynomial time for CO problems (this is a model-checking problem for an interpretation
and a theory) and, for ASO problems, in polynomial time with the assist of two calls to
an NP oracle, since model checking for the equilibrium-model semantics is co-NP-complete
(Theorem 8, Pearce et al. (2009)). Since condition (2) can be verified in polynomial time by
Lemma 32, and condition (3) in polynomial time for CO problems and in polynomial time
with the assist of four calls to an NP oracle for ASO problems (Lemma 33 and the result
by Pearce et al., (2009) mentioned above), the membership part of the assertion follows.
For hardness, we observe that in case of problems with empty selectors, s coincides
with equivalence of propositional theories in case of CO problems, and with equivalence of
equilibrium theories in case of ASO problems. The former is well known to be co-NP-hard,
the latter is P2 -hard (Theorem 11, Pearce et al. (2009)). 2
Theorem 22 Given optimization problems P and Q and rank interval [i, j], deciding
P s,[i,j] Q is P2 -complete in case of CO problems and P3 -complete in case of ASO
problems.
Proof. To prove the membership part, we consider the complementary problem. By
Theorem 6, that problem consists of deciding whether it is the case that at least one of the
following conditions holds:
1. (P<i ) 6= (Q<i ), or equivalently, for some interpretation I, exactly one of the properties I  (P<i ) and I  (Q<i ) holds;
2. >P(P<i ) 6= >Q
(Q<i ) , or equivalently, for some interpretations I, J  (P<i )  (Q<i )
exactly one of the properties I >P J or I >Q J holds;
3. for some I, J  (P<i )  (Q<i ) such that i < diff P (I, J) or i < diff Q (I, J),
diff P (I, J) 6= diff Q (I, J) and it holds that diff P (I, J)  j or diff Q (I, J)  j.
385

fiFaber, Truszczynski, & Woltran

We call each pair of interpretations I, J that demonstrates that one of the conditions above
holds a witness. It is easy to see that if there is witness, there is also one that consists only
of atoms that occur in P and Q. We show that the complementary problem is in P2 for
CO problems and in P3 for ASO problems by showing that a witness once guessed can
be verified in polynomial time, using an NP oracle when dealing with a CO problem and a
P2 oracle when dealing with an ASO problem.
Consider a witness I, J of two interpretations. One can test whether I, J verifies
condition (1) (only I matters here) in polynomial time with two calls to an NP oracle for CO problems (Lemma 34), and with two calls to a P2 oracle for ASO problems
(Lemma 35). To verify condition (2), we need four calls to the respective oracles to test
that I, J  (P<i )  (Q<i ) and polynomial-time computation to test that exactly one of
the properties I >P J or I >Q J holds (Lemma 33). Similarly, for condition (3), we use
four calls to the respective oracles to test that I, J  (P<i )  (Q<i ) and polynomial-time
computation to test the condition involving diff P (I, J) and diff Q (I, J) (Lemma 32).
For the hardness part, we start with the case of CO problems. Therefore, we reduce the
following problem to strong sel-equivalence: given two propositional theories S and T , decide
whether they possess the same minimal models. This problem is known to be P2 -complete
(for instance, equivalence for positive disjunctive programs is known to P2 -complete, see
e.g., Eiter et al., 2007b, Thm. 6.15, which means testing whether two propositional formulas
of a particular class have the same minimal models). The problem remains hard if S and T
are in negation normal form over the same alphabet. Given a negation normal form theory
T we construct a CO problem PT such that the elements in (PT ) are in a one-to-one
correspondence to the minimal models of T . We adapt a construction used in (Brewka
et al., 2011). Specifically, we set
PTg = T [u/u0 ]  {u  u0 | u  U },
where U is the collection of atoms occurring in T , and T [u/u0 ] stands for replacing all u
by u0 in T , and
PTs = {u0 > u | u  U }.
Our first observation is that each outcome of PT must be of the form I  {y 0 | y  U \ I}
where I  U . For each interpretation I  U we write I + = I  {y 0 | y  U \ I}. It is clear
that I |= u if and only if I + |= u0 , and hence also I |= T if and only if I + |= T [u/u0 ].
Hence there is a one-to-one mapping between models M of T and outcomes M + of PT .
Now let us assume that M +  (PT ). Then M |= T , and for any N  M we we have
N 6|= T . Indeed if N |= T , then N + |= PTg and for rules r  PTs of the form u0 > u, where
u  M \ N , we obtain vN + (r) = 1 < 2 = vM + (r) and for all other rules r0 in PTs we have
vN + (r0 ) = 2 = vM + (r0 ). That implies N + >PT M + , contradicting M +  (PT ). Thus, M
is a minimal model of T .
Conversely, let us assume that M is a minimal model of T . Then M +  (PT ) and for
all N +  (PT ) we have that N + >PT M + does not hold, implying M +  (PT ). Indeed,
if N + >PT M + , then vN + (r) < vM + (r) for at least one r  PTs and vN + (r0 )  vM + (r0 ) for
all r0  PTs . The latter implies N  M and the former shows that N 6= M . Since N |= T it
contradicts the assumption that M is a minimal model of T . It follows that M +  (PT ).
We thus have that M +  (PT ) if and only if M is a minimal model of T . Moreover,
for S and T over U it follows that S and T have the same minimal models if and only
386

fiStrong Equivalence of Qualitative Optimization Problems

if PS s,2 PT . Indeed, for any R  LUs,2 it is easy to verify that (PS ) = (PS  R)
and (PT ) = (PT  R). That observation follows from the fact that there are no distinct
I, J  (PS ) such that I PS J and no distinct I, J  (PT ) such that I PT J. Thus,
rules in R which, being of weaker rank, can only break ties, do not affect the sets of optimal
outcomes.
Concerning hardness for ASO problems, we can use a similar idea. However, we shall
use the following problem: given two open QBFs Y (X, Y ), Y (X, Y ), decide whether
these two QBFs possess the same minimal models. By Lemma 30, this problem is P3 -hard
and, moreover, we can assume that  and  are in negation normal form. The reduction
then combines the idea from above with the reduction for general ASP consistency (Eiter
& Gottlob, 1995). More precisely, for each z  X  Y we introduce a new variable z 0 , and
then we construct P for a given (X, Y ) as follows:
Pg = {z  z 0 | z  X  Y } 
{(y  y 0 )  w, w  y, w  y 0 | y  Y } 
{[z/z 0 ]  w, w  w},
where [z/z 0 ] stands for replacing all z by z 0 , and w is a globally new atom. For the
selector we set
Ps = {x0 > x | x  X}.
Any equilibrium model M of Pg must contain w (otherwise w  w would be unsatisfied),
and it must also contain all of {y, y 0 | y  Y } (otherwise w  y, w  y 0 would be
unsatisfied); we write W for {y, y 0 | y  Y }  {w}  the set contained in each equilibrium
model. Moreover, each equilibrium model M must be of the form V  {z 0 | z  X \ V }  W .
Indeed, one of x and x0 must hold for each x  X to satisfy xx0 , but not both, as otherwise
hM \ {x}, M i |=HT Pg as well, contradicting the fact that M is an equilibrium model of Pg .
For each interpretation I  X we write I + = I  {x0 | x  X \ I}  W of Pg . One can show
that I is a model of Y (X, Y ) if and only if I + is an equilibrium model of Pg . Indeed, if I
satisfies Y (X, Y ), then hI + , I + i |=HT Pg and for all J  I + it holds that hJ, I + i 6|=HT Pg .
On the other hand, if I does not satisfy Y (X, Y ) then there exists J  Y such that
I  J 6|= (X, Y ). It follows that hI + \ ({y | y 
/ J}  {y 0 | y  J}  {w}), I + i |=HT Pg (the
key element of the argument here is that [z/z 0 ] contains no occurrences of negation, as 
is in negation normal form and so, I + \ ({y | y 
/ J}  {y 0 | y  J}  {w}) 6|= [z/z 0 ]; that
+
property allows one to show hI \({y | y 
/ J}{y 0 | y  J}{w}), I + i |=HT [z/z 0 ]  w).
+
Thus, I is not an equilibrium model of Pg .
Once the correspondence between models M of Y (X, Y ) and outcomes (equilibrium
models) M + of P is established, we can handle the issue of minimality as in the case of CO
problems. Let us assume first that M +  (P ). Then, as we just demonstrated, M satisfies
Y (X, Y ). Moreover, we have that for any N  M , N does not satisfy Y (X, Y ). Indeed
if N would satisfy Y (X, Y ), then we would have (i) N +  (P ), (ii) for rules r  Ps of
the form u0 > u, where u  M \ N , vN + (r) = 1 < 2 = vM + (r), and (iii) for all other rules
r0 in Ps , vN + (r0 ) = 2 = vM + (r0 ); these three properties would imply N + >P M + , contradicting M +  (P ). Conversely, let us assume that M is a minimal model of Y (X, Y ).
Then M +  (P ) and for all N +  (P ) we can show that N + >P M + does not hold
387

fiFaber, Truszczynski, & Woltran

(implying M +  (P )). To this end, we reason as follows. If N + >P M + would hold,
then vN + (r) < vM + (r) for at least one r  Ps , and vN + (r0 )  vM + (r0 ), for all r0  Ps .
This of course implies N  M and, since N satisfies Y (X, Y ) (we recall that N + is an
equilibrium model of Pg and so, N is a model of Y (X, Y )), it contradicts the assumption
that M is a minimal model of Y (X, Y ). We thus have that M +  (P ) if and only if M
is a minimal model of Y (X, Y ). From that fact, reasoning as in the case of CO problems,
we obtain that that Y (X, Y ) and Y (X, Y ) have the same minimal models if and only
if P s,2 P . 2
Theorem 23 Given two CO (ASO, respectively) problems P and Q, deciding P g Q is
co-NP-complete.
Proof. Deciding strong equivalence of propositional theories under the classical semantics
is co-NP-hard as in such case, strong equivalence and equivalence coincide. It is also coNP-hard under the equilibrium-model semantics (Lin, 2002). Thus, the hardness part of
the assertion follows from Corollary 15.
To prove membership, we consider the complementary problem. By Theorem 13, it
consists of deciding whether at least one of the conditions (1) and (2) below holds:
1. P g and Qg are not strongly equivalent or, equivalently, Mod (P g ) 6= Mod (Q g ), for CO
problems, and ModHT (P g ) 6= ModHT (Qg ), for ASO problems;
2. >PMod(P g ) 6= >Q
Mod(Q g ) or, equivalently, for some interpretations I, J that are models
g
of both P and Qg , exactly one of the properties I >P J and I >Q J holds.
Consequently, we consider a pair of interpretations I, J to be a witness for an instance
of that problem to be a YES instance if I is a model (for CO problems) or hI, Ji is an
HT-model (for ASO problems) of exactly one of the two theories P g and Qg , or if I, J are
models of both P g and Qg and exactly one of the properties I >P J and I >Q J holds.
It is easy to see that if a witness exists, then there is a witness I, J such that I and
J consist only of atoms that occur in P and Q. After such a witness is guessed, verifying
it (showing that condition (1) or (2) holds) can be done in polynomial time. It is well
known that checking whether I is a model (hI, Ji is an HT-model) of a propositional theory
can be done in polynomial time, and the same holds true for deciding I >P J and I >Q J
(Lemma 33). Thus, the complementary problem is in NP. 2
Theorem 24 Given ranked CO (ASO, respectively) problems P and Q, and rank interval
s,[i,j]
[i, j], deciding P g
Q is co-NP-complete.
Proof. For membership, we consider the complementary problem. By Theorem 16, it
consists of deciding whether at least one of the conditions (1) - (4) below holds:
1. P g and Qg are not strongly equivalent (that is, Mod (P g ) 6= Mod (Q g ) for CO problems,
ModHT (P g ) 6= ModHT (Qg ) for ASO problems);
2. for some interpretations I, J that are models of both Mod (P g ) and Mod (Q g ), exactly
one of properties I >PMod(P g ) J and I >Q
Mod(Q g ) J holds;
388

fiStrong Equivalence of Qualitative Optimization Problems

3. for some I, J  Mod (P g ) such that i < diff P (I, J) or i < diff Q (I, J), diff P (I, J) 6=
diff Q (I, J) and it holds that diff P (I, J)  j or diff Q (I, J)  j;
P

Q

<i
<i
4. >Mod(P
g ) 6= >Mod(Q g ) or, equivalently, for some interpretations I, J that are models of
both Mod (P g ) and Mod (Q g ), exactly one of properties I >P<i J and I >Q<i J holds.

We prove that the complementary problem is in NP. As in other proofs, we use for a
membership witness a pair of interpretations I, J that explicitly demonstrates that one of
the conditions (1) - (4) holds. As before, if such a witness exists, there is also one consisting
of atoms occurring in programs P and Q only. For each condition, given such a pair I, J
(restricted to atoms from P and Q), one can verify in polynomial time whether the condition
holds (for condition (1), we repeat the argument from the previous proof, for conditions (2)
and (4), we use Lemma 33, and for condition (3)  Lemma 32).
Hardness follows directly from co-NP-completeness of deciding strong equivalence between two propositional theories under either of the semantics we consider and from Corollary 17. 2

References
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2003). CP-nets: A tool
for representing and reasoning with conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research, 21, 135191.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). Preference-based
constrained optimization with CP-nets. Computational Intelligence, 20, 137157.
Brewka, G., Niemela, I., & Syrjanen, T. (2004). Logic programs with ordered disjunctions.
Computational Intelligence, 20 (2), 335357.
Brewka, G., Niemela, I., & Truszczynski, M. (2003). Answer set optimization. In Gottlob,
G., & Walsh, T. (Eds.), Proceedings of the 18th International Joint Conference on
Artificial Intelligence (IJCAI 2003), pp. 867872. Morgan Kaufmann.
Brewka, G., Niemela, I., & Truszczynski, M. (2011). Answer set optimization. Unpublished
manuscript.
Calimeri, F., Ianni, G., Krennwallner, T., & Ricca, F. (2012). The answer set programming
competition. AI Magazine, 33 (4), 114118.
Delgrande, J. P., Schaub, T., Tompits, H., & Wang, K. (2004). A classification and survey of preference handling approaches in nonmonotonic reasoning. Computational
Intelligence, 20 (2), 308334.
Eiter, T., Faber, W., Fink, M., & Woltran, S. (2007a). Complexity results for answer set
programming with bounded predicate arities and implications. Annals of Mathematics
and Artificial Intelligence, 51 (24), 123165.
Eiter, T., Fink, M., & Woltran, S. (2007b). Semantical characterizations and complexity of
equivalences in answer set programming. ACM Transactions on Computational Logic,
8 (3).
389

fiFaber, Truszczynski, & Woltran

Eiter, T., & Gottlob, G. (1995). On the computational cost of disjunctive logic programming:
Propositional case. Annals of Mathematics and Artificial Intelligence, 15 (3/4), 289
323.
Faber, W., Truszczynski, M., & Woltran, S. (2012). Strong equivalence of qualitative optimization problems. In Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings
of the 13th International Conference on Principles of Knowledge Representation and
Reasoning (KR 2012), pp. 188198. AAAI Press.
Faber, W., & Konczak, K. (2006). Strong order equivalence. Annals of Mathematics and
Artificial Intelligence, 47 (12), 4378.
Faber, W., Tompits, H., & Woltran, S. (2008). Notions of strong equivalence for logic
programs with ordered disjunction. In Brewka, G., & Lang, J. (Eds.), Proceedings
of the 11th International Conference on Principles of Knowledge Representation and
Reasoning (KR 2008), pp. 433443. AAAI Press.
Ferraris, P. (2005). Answer sets for propositional theories. In Baral, C., Greco, G., Leone,
N., & Terracina, G. (Eds.), Proceedings of the 8th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR 2005), Vol. 3662 of Lecture
Notes in Computer Science, pp. 119131. Springer.
Ferraris, P., & Lifschitz, V. (2005). Mathematical foundations of answer set programming.
In We Will Show Them! Essays in Honour of Dov Gabbay, pp. 615664. College
Publications.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive
databases. New Generation Computing, 9, 365385.
Goldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2008). The computational complexity of dominance and consistency in CP-nets. Journal of Artificial Intelligence
Research, 33, 403432.
Goldsmith, J., & Junker, U. (Eds.). (2008). Special Issue on Preferences, Vol. 29(4) of AI
Magazine.
Heyting, A. (1930). Die formalen Regeln der intuitionistischen Logik. Sitzungsberichte der
Preussischen Akademie der Wissenschaften, 1, 4256.
Kaci, S. (2011). Working with Preferences. Springer.
Lifschitz, V. (1985). Computing circumscription. In Joshi, A. K. (Ed.), Proceedings of
the 9th International Joint Conference on Artificial Intelligence (IJCAI 1985), pp.
121127. Morgan Kaufmann.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM
Transactions on Computational Logic, 2 (4), 526541.
Lin, F. (2002). Reducing strong equivalence of logic programs to entailment in classical
propositional logic. In Fensel, D., Giunchiglia, F., McGuiness, D. L., & Williams, M.A. (Eds.), Proceedings of the 8th International Conference on Principles of Knowledge
Representation and Reasoning (KR 2002), pp. 170176. Morgan Kaufmann.
Marek, V. W., & Truszczynski, M. (1993). Nonmonotonic Logic; Context-Dependent Reasoning. Springer, Berlin.
390

fiStrong Equivalence of Qualitative Optimization Problems

Marek, V., & Truszczynski, M. (1999). Stable models and an alternative logic programming
paradigm. In Apt, K., Marek, W., Truszczynski, M., & Warren, D. (Eds.), The Logic
Programming Paradigm: a 25-Year Perspective, pp. 375398. Springer, Berlin.
Niemela, I. (1999). Logic programming with stable model semantics as a constraint programming paradigm. Annals of Mathematics and Artificial Intelligence, 25 (3-4), 241273.
Pearce, D. (1997). A new logical characterisation of stable models and answer sets. In Dix,
J., Pereira, L. M., & Przymusinski, T. (Eds.), Proceedings of the 6th International
Workshop on Non-Monotonic Extensions of Logic Programming (NMELP 1996), Vol.
1216 of Lecture Notes in Computer Science, pp. 5770. Springer.
Pearce, D., Tompits, H., & Woltran, S. (2009). Characterising equilibrium logic and nested
logic programs: Reductions and complexity. Theory and Practice of Logic Programming, 9 (5), 565616.
Simons, P., Niemela, I., & Soininen, T. (2002). Extending and implementing the stable
model semantics. Artificial Intelligence, 138, 181234.
Turner, H. (2003). Strong equivalence made easy: Nested expressions and weight constraints.
Theory and Practice of Logic Programming, 3 (45), 609622.

391

fiJournal of Artificial Intelligence Research 47 (2013) 253279

Submitted 02/13; published 06/13

The Arcade Learning Environment:
An Evaluation Platform for General Agents
Marc G. Bellemare

mg17@cs.ualberta.ca

University of Alberta, Edmonton, Alberta, Canada

Yavar Naddaf

yavar@empiricalresults.ca

Empirical Results Inc., Vancouver,
British Columbia, Canada

Joel Veness
Michael Bowling

veness@cs.ualberta.ca
bowling@cs.ualberta.ca

University of Alberta, Edmonton, Alberta, Canada

Abstract
In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general,
domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600
game environments, each one different, interesting, and designed to be a challenge for
human players. ALE presents significant research challenges for reinforcement learning,
model learning, model-based planning, imitation learning, transfer learning, and intrinsic
motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and
benchmarking domain-independent agents designed using well-established AI techniques
for both reinforcement learning and planning. In doing so, we also propose an evaluation
methodology made possible by ALE, reporting empirical results on over 55 different games.
All of the software, including the benchmark agents, is publicly available.

1. Introduction
A longstanding goal of artificial intelligence is the development of algorithms capable of
general competency in a variety of tasks and domains without the need for domain-specific
tailoring. To this end, different theoretical frameworks have been proposed to formalize the
notion of big artificial intelligence (e.g., Russell, 1997; Hutter, 2005; Legg, 2008). Similar
ideas have been developed around the theme of lifelong learning: learning a reusable, highlevel understanding of the world from raw sensory data (Thrun & Mitchell, 1995; Pierce &
Kuipers, 1997; Stober & Kuipers, 2008; Sutton et al., 2011). The growing interest in competitions such as the General Game Playing competition (Genesereth, Love, & Pell, 2005),
Reinforcement Learning competition (Whiteson, Tanner, & White, 2010), and the International Planning competition (Coles et al., 2012) also suggests the artificial intelligence
communitys desire for the emergence of algorithms that provide general competency.
Designing generally competent agents raises the question of how to best evaluate them.
Empirically evaluating general competency on a handful of parametrized benchmark problems is, by definition, flawed. Such an evaluation is prone to method overfitting (Whiteson,
Tanner, Taylor, & Stone, 2011) and discounts the amount of expert effort necessary to
transfer the algorithm to new domains. Ideally, the algorithm should be compared across
c
2013
AI Access Foundation. All rights reserved.

fiBellemare, Naddaf, Veness, & Bowling

domains that are (i) varied enough to claim generality, (ii) each interesting enough to be
representative of settings that might be faced in practice, and (iii) each created by an
independent party to be free of experimenters bias.
In this article, we introduce the Arcade Learning Environment (ALE): a new challenge
problem, platform, and experimental methodology for empirically assessing agents designed
for general competency. ALE is a software framework for interfacing with emulated Atari
2600 game environments. The Atari 2600, a second generation game console, was originally
released in 1977 and remained massively popular for over a decade. Over 500 games were
developed for the Atari 2600, spanning a diverse range of genres such as shooters, beatem
ups, puzzle, sports, and action-adventure games; many game genres were pioneered on the
console. While modern game consoles involve visuals, controls, and a general complexity
that rivals the real world, Atari 2600 games are far simpler. In spite of this, they still pose
a variety of challenging and interesting situations for human players.
ALE is both an experimental methodology and a challenge problem for general AI competency. In machine learning, it is considered poor experimental practice to both train and
evaluate an algorithm on the same data set, as it can grossly over-estimate the algorithms
performance. The typical practice is instead to train on a training set then evaluate on a
disjoint test set. With the large number of available games in ALE, we propose that a similar methodology can be used to the same effect: an approachs domain representation and
parametrization should be first tuned on a small number of training games, before testing
the approach on unseen testing games. Ideally, agents designed in this fashion are evaluated on the testing games only once, with no possibility for subsequent modifications to the
algorithm. While general competency remains the long-term goal for artificial intelligence,
ALE proposes an achievable stepping stone: techniques for general competency across the
gamut of Atari 2600 games. We believe this represents a goal that is attainable in a short
time-frame yet formidable enough to require new technological breakthroughs.

2. Arcade Learning Environment
We begin by describing our main contribution, the Arcade Learning Environment (ALE).
ALE is a software framework designed to make it easy to develop agents that play arbitrary
Atari 2600 games.
2.1 The Atari 2600
The Atari 2600 is a home video game console developed in 1977 and sold for over a decade
(Montfort & Bogost, 2009). It popularized the use of general purpose CPUs in game console
hardware, with game code distributed through cartridges. Over 500 original games were
released for the console; homebrew games continue to be developed today, over thirty years
later. The consoles joystick, as well as some of the original games such as Adventure and
Pitfall!, are iconic symbols of early video games. Nearly all arcade games of the time 
Pac-Man and Space Invaders are two well-known examples  were ported to the console.
Despite the number and variety of games developed for the Atari 2600, the hardware is
relatively simple. It has a 1.19Mhz CPU and can be emulated much faster than real-time
on modern hardware. The cartridge ROM (typically 24kB) holds the game code, while the
console RAM itself only holds 128 bytes (1024 bits). A single game screen is 160 pixels wide
254

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Figure 1: Screenshots of Pitfall! and Space Invaders.
and 210 pixels high, with a 128-colour palette; 18 actions can be input to the game via a
digital joystick: three positions of the joystick for each axis, plus a single button. The Atari
2600 hardware limits the possible complexity of games, which we believe strikes the perfect
balance: a challenging platform offering conceivable near-term advancements in learning,
modelling, and planning.
2.2 Interface
ALE is built on top of Stella1 , an open-source Atari 2600 emulator. It allows the user to
interface with the Atari 2600 by receiving joystick motions, sending screen and/or RAM
information, and emulating the platform. ALE also provides a game-handling layer which
transforms each game into a standard reinforcement learning problem by identifying the
accumulated score and whether the game has ended. By default, each observation consists
of a single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels
high. The action space consists of the 18 discrete actions defined by the joystick controller.
The game-handling layer also specifies the minimal set of actions needed to play a particular
game, although none of the results in this paper make use of this information. When running
in real-time, the simulator generates 60 frames per second, and at full speed emulates up to
6000 frames per second. The reward at each time-step is defined on a game by game basis,
typically by taking the difference in score or points between frames. An episode begins on
the first frame after a reset command is issued, and terminates when the game ends. The
game-handling layer also offers the ability to end the episode after a predefined number of
frames2 . The user therefore has access to several dozen games through a single common
interface, and adding support for new games is relatively straightforward.
ALE further provides the functionality to save and restore the state of the emulator.
When issued a save-state command, ALE saves all the relevant data about the current
game, including the contents of the RAM, registers, and address counters. The restorestate command similarly resets the game to a previously saved state. This allows the use of
ALE as a generative model to study topics such as planning and model-based reinforcement
learning.
1. http://stella.sourceforge.net/
2. This functionality is needed for a small number of games to ensure that they always terminate. This
prevents situations such as in Tennis, where a degenerate agent could choose to play indefinitely by
refusing to serve.

255

fiBellemare, Naddaf, Veness, & Bowling

2.3 Source Code
ALE is released as free, open-source software under the terms of the GNU General Public
License. The latest version of the source code is publicly available at:
http://arcadelearningenvironment.org
The source code for the agents used in the benchmark experiments below is also available
on the publication page for this article on the same website. While ALE itself is written
in C++, a variety of interfaces are available that allow users to interact with ALE in
the programming language of their choice. Support for new games is easily added by
implementing a derived class representing the games particular reward and termination
functions.

3. Benchmark Results
Planning and reinforcement learning are two different AI problem formulations that can
naturally be investigated within the ALE framework. Our purpose in presenting benchmark
results for both of these formulations is two-fold. First, these results provide a baseline
performance for traditional techniques, establishing a point of comparison with future,
more advanced, approaches. Second, in describing these results we illustrate our proposed
methodology for doing empirical validation with ALE.
3.1 Reinforcement Learning
We begin by providing benchmark results using SARSA(), a traditional technique for
model-free reinforcement learning. Note that in the reinforcement learning setting, the
agent does not have access to a model of the game dynamics. At each time step, the
agent selects an action and receives a reward and an observation, and the agents aim is
to maximize its accumulated reward. In these experiments, we augmented the SARSA()
algorithm with linear function approximation, replacing traces, and -greedy exploration.
A detailed explanation of SARSA() and its extensions can be found in the work of Sutton
and Barto (1998).
3.1.1 Feature Construction
In our approach to the reinforcement learning setting, the most important design issue is
the choice of features to use with linear function approximation. We ran experiments using
five different sets of features, which we now briefly explain; a complete description of these
feature sets is given in Appendix A. Of these sets of features, BASS, DISCO and RAM
were originally introduced by Naddaf (2010), while the rest are novel.
Basic. The Basic method, derived from Naddafs BASS (2010), encodes the presence of
colours on the Atari 2600 screen. The Basic method first removes the image background
by storing the frequency of colours at each pixel location within a histogram. Each game
background is precomputed offline, using 18,000 observations collected from sample trajectories. The sample trajectories are generated by following a human-provided trajectory for
a random number of steps and subsequently selecting actions uniformly at random. The
256

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

screen is then divided into 16  14 tiles. Basic generates one binary feature for each of the
128 colours and each of the tiles, giving a total of 28,672 features.
BASS. The BASS method behaves identically to the Basic method save in two respects.
First, BASS augments the Basic feature set with pairwise combinations of its features.
Second, BASS uses a smaller, 8-colour encoding to ensure that the number of pairwise
combinations remains tractable.
DISCO. The DISCO method aims to detect objects within the Atari 2600 screen. To
do so, it first preprocesses 36,000 observations from sample trajectories generated as in the
Basic method. DISCO also performs the background subtraction steps as in Basic and
BASS. Extracted objects are then labelled into classes. During the actual training, DISCO
infers the class label of detected objects and encodes their position and velocity using tile
coding (Sutton & Barto, 1998).
LSH. The LSH method maps raw Atari 2600 screens into a small set of binary features
using Locally Sensitive Hashing (Gionis, Indyk, & Motwani, 1999). The screens are mapped
using random projections, such that visually similar screens are more likely to generate the
same features.
RAM. The RAM method works on an entirely different observation space than the other
four methods. Rather than receiving in Atari 2600 screen as an observation, it directly
observes the Atari 2600s 1024 bits of memory. Each bit of RAM is provided as a binary
feature together with the pairwise logical-AND of every pair of bits.
3.1.2 Evaluation Methodology
We first constructed two sets of games, one for training and the other for testing. We used
the training games for parameter tuning as well as design refinements, and the testing games
for the final evaluation of our methods. Our training set consisted of five games: Asterix,
Beam Rider, Freeway, Seaquest and Space Invaders. The parameter search involved
finding suitable values for the parameters to the SARSA() algorithm, i.e. the learning rate,
exploration rate, discount factor, and the decay rate . We also searched the space of feature
generation parameters, for example the abstraction level for the BASS agent. The results
of our parameter search are summarized in Appendix C. Our testing set was constructed
by choosing semi-randomly from the 381 games listed on Wikipedia3 at the time of writing.
Of these games, 123 games have their own Wikipedia page, have a single player mode, are
not adult-themed or prototypes, and can be emulated in ALE. From this list, 50 games
were chosen at random to form the test set.
Evaluation of each method on each game was performed as follows. An episode starts on
the frame that follows the reset command, and terminates when the end-of-game condition
is detected or after 5 minutes of real-time play (18,000 frames), whichever comes first.
During an episode, the agent acts every 5 frames, or equivalently 12 times per second of
gameplay. A reinforcement learning trial consists of 5,000 training episodes, followed by
500 evaluation episodes during which no learning takes place. The agents performance is
3. http://en.wikipedia.org/wiki/List of Atari 2600 games (July 12, 2012)

257

fiBellemare, Naddaf, Veness, & Bowling

Game
Asterix
Seaquest
Boxing
H.E.R.O.
Zaxxon

Basic
862
579
-3
6053
1392

BASS
860
665
16
6459
2069

DISCO
755
422
12
2720
70

LSH
987
509
10
3836
3365

RAM
943
594
44
3281
304

Random
288
108
-1
712
0

Const
650
160
-25
0
0

Perturb
338
451
-10
148
2

Human
620
156
-2
6087
820

Table 1: Reinforcement Learning results for selected games. Asterix and Seaquest are
part of the training set.

measured as the average score achieved during the evaluation episodes. For each game, we
report our methods average performance across 30 trials.
For purposes of comparison, we also provide performance measures for three simple
baseline agents  Random, Const and Perturb  as well as the performance of a non-expert
human player. The Random agent picks a random action on every frame. The Const agent
selects a single fixed action throughout an episode; our results reflect the highest score
achieved by any single action within each game. The Perturb agent selects a fixed action
with probability 0.95 and otherwise acts uniformly randomly; for each game, we report the
performance of the best policy of this type. Additionally, we provide human player results
that report the five-episode average score obtained by a beginner (who had never previously
played Atari 2600 games) playing selected games. Our aim is not to provide exhaustive or
accurate human-level benchmarks, which would be beyond the scope of this paper, but
rather to offer insight into the performance level achieved by our agents.
3.1.3 Results
A complete report of our reinforcement learning results is given in Appendix D. Table 1
shows a small subset of results from two training games and three test games. In 40 games
out of 55, learning agents perform better than the baseline agents. In some games, e.g.,
Double Dunk, Journey Escape and Tennis, the no-action baseline policy performs
the best by essentially refusing to play and thus incurring no negative reward. Within the
40 games for which learning occurs, the BASS method generally performs best. DISCO
performed particularly poorly compared to the other learning methods. The RAM-based
agent, surprisingly, did not outperform image-based methods, despite building its representation from raw game state. It appears the screen image carries structural information that
is not easily extracted from the RAM bits.
Our reinforcement learning results show that while some learning progress is already possible in Atari 2600 games, much more work remains to be done. Different methods perform
well on different games, and no single method performs well on all games. Some games are
particularly challenging. For example, platformers such as Montezumas Revenge seem
to require high-level planning far beyond what our current, domain-independent methods
provide. Tennis requires fairly elaborate behaviour before observing any positive reward,
but simple behaviour can avoid negative rewards. Our results also highlight the value of
ALE as an experimental methodology. For example, the DISCO approach performs rea258

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

sonably well on the training set, but suffers a dramatic reduction in performance when
applied to unseen games. This suggests the method is less robust than the other methods
we studied. After a quick glance at the full table of results in Appendix D, it is clear that
summarizing results across such varied domains needs further attention; we explore this
issue further in Section 4.
3.2 Planning
The Arcade Learning Environment can naturally be used to study planning techniques by
using the emulator itself as a generative model. Initially it may seem that allowing the
agent to plan into the future with a perfect model trivializes the problem. However, this
is not the case: the size of state space in Atari 2600 games prohibits exhaustive search.
Eighteen different actions are available at every frame; at 60 frames per second, looking
ahead one second requires 1860  1075 simulation steps. Furthermore, rewards are often
sparsely distributed, which causes significant horizon effects in many search algorithms.
3.2.1 Search Methods
We now provide benchmark ALE results for two traditional search methods. Each method
was applied online to select an action at every time step (every five frames) until the game
was over.
Breadth-first Search. Our first approach builds a search tree in a breadth-first fashion
until a node limit is reached. Once the tree is expanded, node values are updated recursively
from the bottom of the tree to the root. The agent then selects the action corresponding
to the branch with the highest discounted sum of rewards. Expanding the full search tree
requires a large number of simulation steps. For instance, selecting an action every 5 frames
and allowing a maximum of 100,000 simulation steps per frame, the agent can only look
ahead about a third of a second. In many games, this allows the agent to collect immediate
rewards and avoid death but little else. For example, in Seaquest the agent must collect
a swimmer and return to the surface before running out of air, which involves planning far
beyond one second.
UCT: Upper Confidence Bounds Applied to Trees. A preferable alternative to
exhaustively expanding the tree is to simulate deeper into the more promising branches.
To do this, we need to find a balance between expanding the higher-valued branches and
spending simulation steps on the lower-valued branches to get a better estimate of their
values. The UCT algorithm, developed by Kocsis and Szepesvari (2006), deals with the
exploration-exploitation dilemma by treating each node of a search tree as a multi-armed
bandit problem. UCT uses a variation of UCB1, a bandit algorithm, to choose which child
node to visit next. A common practice is to apply a t-step random simulation at the end
of each leaf node to obtain an estimate from a longer trajectory. By expanding the more
valuable branches of the tree and carrying out a random simulation at the leaf nodes, UCT
is known to perform well in many different settings (Browne et al., 2012).
Our UCT implementation was entirely standard, except for one optimization. Few Atari
games actually distinguish between all 18 actions at every time step. In Beam Rider, for
example, the down action does nothing, and pressing the button when a bullet has already
259

fiBellemare, Naddaf, Veness, & Bowling

Game
Asterix
Seaquest
Boxing
H.E.R.O.
Zaxxon

Full Tree
2136
288
100
1324
0

UCT
290700
5132
100
12860
22610

Best Learner
987
665
44
6459
3365

Best Baseline
650
451
-1
712
2

Table 2: Results for selected games. Asterix and Seaquest are part of the training set.
been shot has no effect. We exploit this fact as follows: after expanding the children of a
node in the search tree, we compare the resulting emulator states. Actions that result in
the same state are treated as duplicates and only one of the actions is considered in the
search tree. This reduces the branching factor, thus allowing deeper search. At every step,
we also reuse the part of our search tree corresponding to the selected action. Pseudocode
for our implementation of the UCT algorithm is given in Appendix B.
3.2.2 Experimental Setup
We designed and tuned our algorithms based on the same five training games used in
Section 3.1, and subsequently evaluated the methods on the fifty games of the testing set.
The training games were used to determine the length of the search horizon as well as the
constant controlling the amount of exploration at internal nodes of the tree. Each episode
was set to last up to 5 minutes of real-time play (18,000 frames), with actions selected every
5 frames, matching our settings in Section 3.1.2. On average, each action selection step
took on the order of 15 seconds. We also used the same discount factor as in Section 3.1.
We ran our algorithms for 10 episodes per game. Details of the algorithmic parameters can
be found in Appendix C.
3.2.3 Results
A complete report of our search results is given in Appendix D. Table 2 shows results on
a selected subset of games. For reference purposes, we also include the performance of the
best learning agent and the best baseline policy from Table 1. Together, our two search
methods performed better than both learning agents and the baseline policies on 49 of 55
games. In most cases, UCT performs significantly better than breadth-first search. Four of
the six games for which search methods do not perform best are games where rewards are
sparse and require long-term planning. These are Freeway, Private Eye, Montezumas
Revenge and Venture.

4. Evaluation Metrics for General Atari 2600 Agents
Applying algorithms to a large set of games as we did in Sections 3.1 and 3.2 presents
difficulties when interpreting the results. While the agents goal in all games is to maximize
its score, scores for two different games cannot be easily compared. Each game uses its own
scale for scores, and different game mechanics make some games harder to learn than others.
The challenges associated with comparing general agents has been previously highlighted
260

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

a) Random

30

b) Baseline

10

c) Inter-Algorithm

9
25
20

1

8

BASS
RAM

7

0.8

6

15

5

0.6

4
10

0.4

3
2

5

0.2

1
0

0

Seaq

uest

H.E.R

.O.

Boxin

g

0

Seaq

uest

H.E.R

.O.

Boxin

g

Seaq

uest

H.E.R

.O.

Boxin

g

Figure 2: Left to right: random-normalized, baseline and inter-algorithm scores.

by Whiteson et al. (2011). Although we can always report full performance tables, as we
did in Appendix D, some more compact summary statistics are also desirable. We now
introduce some simple metrics that help compare agents across a diverse set of domains,
such as our test set of Atari 2600 games.
4.1 Normalized Scores
Consider the scores sg,1 and sg,2 achieved by two algorithms in game g. Our goal here is
to explore methods that allow us to compare two sets of scores S1 = {sg1 ,1 , . . . , sgn ,1 } and
S2 = {sg1 ,2 , . . . , sgn ,2 }. The approach we take is to transform sg,i into a normalized score
zg,i with the aim of comparing normalized scores across games; in the ideal case, zg,i = zg0 ,i
implies that algorithm i performs as well on game g as on game g 0 . In order to compare
algorithms over a set of games, we aggregate normalized scores for each game and each
algorithm.
The most natural way to compare games with different scoring scales is to normalize
scores so that the numerical values become comparable. All of our normalization methods
are defined using the notion of a score range [rg,min , rg,max ] computed for each game. Given
such a score range, score sg,i is normalized by computing zg,i := (sg,i  rg,min ) / (rg,max 
rg,min ).
4.1.1 Normalization to a Reference Score
One straightforward method is to normalize to a score range defined by repeated runs of
a random agent across each game. Here, rg,max is the absolute value of the average score
achieved by the random agent, and rg,min = 0. Figure 2a depicts the random-normalized
scores achieved by BASS and RAM on three games. Two issues arise with this approach:
the scale of normalized scores may be excessively large and normalized scores are generally
not translation invariant. The issue of scale is best seen in a game such as Freeway, for
which the random agent achieves a score close to 0: scores achieved by learning agents, in
the 10-20 range, are normalized into thousands. By contrast, no learning agent achieves a
random-normalized score greater than 1 in Asteroids.
261

fiBellemare, Naddaf, Veness, & Bowling

4.1.2 Normalizing to a Baseline Set
Rather than normalizing to a single reference we may normalize to the score range implied
by a set of references. Let bg,1 , . . . , bg,k be a set of reference scores. A methods baseline
score is computed using the score range [mini{1,...,k} bg,i , maxi{1,...,k} bg,i ].
Given a sufficiently rich set of reference scores, baseline normalization allows us to reduce
the scores for most games to comparable quantities, and lets us know whether meaningful
performance was obtained. Figure 2b shows example baseline scores. The score range
for these scores corresponds to the scores achieved by 37 baseline agents (Section 3.1.2):
Random, Const (one policy per action), and Perturb (one policy per action).
A natural idea is to also include scores achieved by human players into the baseline set.
For example, one may include the score achieved by an expert as well as the score achieved
by a beginner. However, using human scores raises its own set of issues. For example,
humans often play games without seeking to maximize score; humans also benefit from
prior knowledge that is difficult to incorporate into domain-independent agents.
4.1.3 Inter-Algorithm Normalization
A third alternative is to normalize using the scores achieved by the algorithms themselves.
Given n algorithms, each achieving score sg,i on game g, we define the inter-algorithm
score using the score range [mini{1,...,n} sg,i , maxi{1,...,n} sg,i ]. By definition, zg,i  [0, 1].
A special case of this is when n=2, where zg,i  {0, 1} indicates which algorithm is better
than the other. Figure 2c shows example inter-algorithm scores; the relevant score ranges
are constructed from the performance of all five learning agents.
Because inter-algorithm scores are bounded, this type of normalization is an appealing
solution to compare the relative performance of different methods. Its main drawback is
that it gives no indication of the objective performance of the best algorithm. A good
example of this is Venture: the inter-algorithm score of 1.0 achieved by BASS does not
reflect the fact that none of our agents achieved a score remotely comparable to a humans
performance. The lack of objective reference in inter-algorithm normalization suggests that
it should be used to complement other scoring metrics.
4.2 Aggregating Scores
Once normalized scores are obtained for each game, the next step is to produce a measure
that reflects how well each agent performs across the set of games. As illustrated by Table
4, a large table of numbers does not easily permit comparison between algorithms. We now
describe three methods to aggregate normalized scores.
4.2.1 Average Score
The most straightforward method of aggregating normalized scores is to compute their
average. Without perfect score normalization, however, score averages tend to be heavily
influenced by games such as Zaxxon for which baseline scores are high. Averaging interalgorithm scores obviates this issue as all scores are bounded between 0 and 1. Figure 3
displays average baseline and inter-algorithm scores for our learning agents.
262

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Average Baseline Scores

35

1.4

30

Median Baseline Scores

5

0.2

RAM

LSH

0.4

BASS

LSH

0.6

DISCO

10

RAM

Basic

15

DISCO

0.8

BASS

1

20

Basic

1.2

25

0

0

0.7

Avg. Inter-Algorithm Scores

1

Median Inter-Algorithm Scores

0.6
0.8

0

RAM

LSH

DISCO

0.2

0.1

BASS

0.4

Basic

RAM

0.6

LSH

0.2

DISCO

0.3

Basic

0.4

BASS

0.5

0

Figure 3: Aggregate normalized scores for the five reinforcement learning agents.

4.2.2 Median Score
Median scores are generally more robust to outliers than average scores. The median is
obtained by sorting all normalized scores and selecting the middle element (the average of
the two middle elements is used if the number of scores is even). Figure 3 shows median
baseline and inter-algorithm scores for our learning agents. Comparing medians and averages in the baseline score (upper two graphs) illustrates exactly the outlier sensitivity of
the average score, where the LSH method appears dramatically superior due entirely to its
performance in Zaxxon.
4.2.3 Score Distribution
The score distribution aggregate is a natural generalization of the median score: it shows
the fraction of games on which an algorithm achieves a certain normalized score or better.
It is essentially a quantile plot or inverse empirical CDF. Unlike the average and median
scores, the score distribution accurately represents the performance of an agent irrespective
of how individual scores are distributed. Figure 4 shows baseline and inter-algorithm score
distributions. Score distributions allow us to compare different algorithms at a glance  if
one curve is above another, the corresponding method generally obtains higher scores.
Using the baseline score distribution, we can easily determine the proportion of games
for which methods perform better than the baseline policies (scores above 1). The interalgorithm score distribution, on the other hand, effectively conveys the relative performance
of each method. In particular, it allows us to conclude that BASS performs slightly better
than Basic and RAM, and that DISCO performs significantly worse than the other methods.
263

fiBellemare, Naddaf, Veness, & Bowling

1.0

1.0

Fraction of Games

Fraction of Games

BASS
BASS
0.5

RAM
Basic

LSH

DISCO

Basic
RAM
0.5

LSH
DISCO

0.0
2000

50

10

8

6

4

2

0.0
1.0

0

0.8

Baseline Score

0.6

0.4

0.2

0

Inter-Algorithm Score

Figure 4: Score distribution over all games.

Basic
BASS
DISCO
LSH
RAM

Basic

BASS

DISCO

LSH

RAM


3218
1339
1834
2522

1832

548
1736
2029

3913
485

3317
419

3418
3617
1733

3615

2225
2920
941
1536


Table 3: Paired tests over all games. Each entry shows the number of games for which
the performance of the first algorithm (left) is better (worse) than the second
algorithms.
4.3 Paired Tests
An alternate evaluation metric, especially useful when comparing only a few algorithms,
is to perform paired tests over the raw scores. For each game, we performed a two-tailed
Welshs t-test with 99% confidence intervals to determine whether one algorithms score was
statistically different than the others. Table 3 provides, for each pair of algorithms, the
number of games for which one algorithm performs statistically better or worse than the
other. Because of their ternary nature, paired tests tend to magnify small but significant
differences in scores.

5. Related Work
We now briefly survey recent research related to Atari 2600 games and some prior work on
the construction of empirical benchmarks for measuring general competency.
5.1 Atari Games
There has been some attention devoted to Atari 2600 game playing within the reinforcement learning community. For the most part, prior work has focused on the challenge of
finding good state features for this domain. Diuk, Cohen, and Littman (2008) applied their
DOORMAX algorithm to a restricted version of the game of Pitfall!. Their method extracts objects from the displayed image with game-specific object detection. These objects
are then converted into a first-order logic representation of the world, the Object-Oriented
264

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Markov Decision Process (OO-MDP). Their results show that DOORMAX can discover the
optimal behaviour for this OO-MDP within one episode. Wintermute (2010) proposed a
method that also extracts objects from the displayed image and embeds them into a logicbased architecture, SOAR. Their method uses a forward model of the scene to improve the
performance of the Q-Learning algorithm (Watkins & Dayan, 1992). They showed that by
using such a model, a reinforcement learning agent could learn to play a restricted version
of the game of Frogger. Cobo, Zang, Isbell, and Thomaz (2011) investigated automatic
feature discovery in the games of Pong and Frogger, using their own simulator. Their
proposed method takes advantage of human trajectories to identify state features that are
important for playing console games. Recently, Hausknecht, Khandelwal, Miikkulainen,
and Stone (2012) proposed HyperNEAT-GGP, an evolutionary approach for finding policies to play Atari 2600 games. Although HyperNEAT-GGP is presented as a general game
playing approach, it is currently difficult to assess its general performance as the reported
results were limited to only two games. Finally, some of the authors of this paper (Bellemare, Veness, & Bowling, 2012) recently presented a domain-independent feature generation
technique that attempts to focus its effort around the location of the player avatar. This
work used the evaluation methodology advocated here and is the only one to demonstrate
the technique across a large set of testing games.
5.2 Evaluation Frameworks for General Agents
Although the idea of using games to evaluate the performance of agents has a long history
in artificial intelligence, it is only more recently that an emphasis on generality has assumed
a more prominent role. Pell (1993) advocated the design of agents that, given an abstract
description of a game, could automatically play them. His work strongly influenced the
design of the now annual General Game Playing competition (Genesereth et al., 2005). Our
framework differs in that we do not assume to have access to a compact logical description
of the game semantics. Schaul, Togelius, and Schmidhuber (2011) also recently presented
an interesting proposal for using games to measure the general capabilities of an agent.
Whiteson et al. (2011) discuss a number of challenges in designing empirical tests to measure
general reinforcement learning performance; this work can be seen as attempting to address
their important concerns.
Starting in 2004 as a conference workshop, the Reinforcement Learning competition
(Whiteson et al., 2010) was held until 2009 (a new iteration of the competition has been
announced for 20134 ). Each year new domains are proposed, including standard RL benchmarks, Tetris, and Infinite Mario (Mohan & Laird, 2009). In a typical competition domain,
the agents state information is summarized through a series of high-level state variables
rather than direct sensory information. Infinite Mario, for example, provides the agent
with an object-oriented observation space. In the past, organizers have provided a special Polyathlon track in which agents must behave in a medley of continuous-observation,
discrete-action domains.
Another longstanding competition, the International Planning Competition (IPC)5 , has
been organized since 1998, and aims to produce new benchmarks, and to gather and dis4. http://www.rl-competition.org
5. http://ipc.icaps-conference.org

265

fiBellemare, Naddaf, Veness, & Bowling

seminate data about the current state-of-the-art (Coles et al., 2012). The IPC is composed
of different tracks corresponding to different types of planning problems, including factory
optimization, elevator control and agent coordination. For example, one of the problems
in the 2011 competition consists in coordinating a set of robots around a two-dimensional
gridworld so that every tile is painted with a specific colour. Domains are described using
either relational reinforcement learning, yielding parametrized Markov Decision Processes
(MDPs) and Partially Observable MDPs, or using logic predicates, e.g. in STRIPS notation.
One indication of how much these competitions value domain variety can be seen in the
time spent on finding a good specification language. The 2008-2009 RL competitions, for
example, used RL-Glue6 specifically for this purpose; the 2011 planning under uncertainty
track of the IPC similar employed the Relation Dynamic Influence Diagram Language.
While competitions seek to spur new research and evaluate existing algorithms through a
standardized set of benchmarks, they are not independently developed, in the sense that
the vast majority of domains are provided by the research community. Thus a typical
competition domain reflects existing research directions: Mountain Car and Acrobot remain staples of the RL competition. These competitions also focus their research effort on
domains that provide high-level state variables, for example the location of robots in the
floor-painting domain described above. By contrast, the Arcade Learning Environment and
the domain-independent setting force us to consider the question of perceptual grounding:
how to extract meaningful state information from raw game screens (or RAM information).
In turn, this emphasizes the design of algorithms that can be applied to sensor-rich domains
without significant expert knowledge.
There have also been a number of attempts to define formal agent performance metrics
based on algorithmic information theory. The first such attempts were due to HernandezOrallo and Minaya-Collado (1998) and to Dowe and Hajek (1998). More recently, the
approaches of Hernandez-Orallo and Dowe (2010) and of Legg and Veness (2011) appear
to have some potential. Although these frameworks are general and conceptually clean,
the key challenge remains how to specify sufficiently interesting classes of environments.
In our opinion, much more work is required before these approaches can claim to rival
the practicality of using a large set of existing human-designed environments for agent
evaluation.

6. Final Remarks
The Atari 2600 games were developed for humans and as such exhibit many idiosyncrasies
that make them both challenging and exciting. Consider, for example, the game Pong.
Pong has been studied in a variety of contexts as an interesting reinforcement learning
domain (Cobo et al., 2011; Stober & Kuipers, 2008; Monroy, Stanley, & Miikkulainen,
2006). The Atari 2600 Pong, however, is significantly more complex than Pong domains
developed for research. Games can easily last 10,000 time steps (compared to 2001000 in
other domains); observations are composed of 7-bit 160210 images (compared to 300200
black and white images in the work of Stober and Kuipers (2008), or 5-6 input features
elsewhere); observations are also more complex, containing the two players score and side
walls. In sheer size, the Atari 2600 Pong is thus a larger domain. Its dynamics are also
6. http://glue.rl-community.org

266

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

more complicated. In research implementations of Pong object motion is implemented
using first-order mechanics. However, in Atari 2600 Pong paddle control is nonlinear:
simple experimentation shows that fully predicting the players paddle requires knowledge
of the last 18 actions. As with many other Atari games, the player paddle also moves every
other frame, adding a degree of temporal aliasing to the domain.
While Atari 2600 Pong may appear unnecessarily contrived, it in fact reflects the unexpected complexity of the problems with which humans are faced. Most, if not all Atari
2600 games are subject to similar programming artifacts: in Space Invaders, for example,
the invaders velocity increases nonlinearly with the number of remaining invaders. In this
way the Atari 2600 platform provides AI researchers with something unique: clean, easilyemulated domains which nevertheless provide many of the challenges typically associated
with real-world applications.
Should technology advance so as to render general Atari 2600 game playing achievable,
our challenge problem can always be extended to use more recent video game platforms.
A natural progression, for example, would be to move on to the Commodore 64, then to
the Nintendo, and so forth towards current generation consoles. All of these consoles have
hundreds of released games, and older platforms have readily available emulators. With
the ultra-realism of current generation consoles, each console represents a natural stepping
stone toward general real-world competency. Our hope is that by using the methodology
advocated in this paper, we can work in a bottom-up fashion towards developing more
sophisticated AI technology while still maintaining empirical rigor.

7. Conclusion
This article has introduced the Arcade Learning Environment, a platform for evaluating
the development of general, domain-independent agents. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and designed to
be a challenge for human players. We illustrate the promise of ALE as a challenge problem
by benchmarking several domain-independent agents that use well-established reinforcement
learning and planning techniques. Our results suggest that general Atari game playing is a
challenging but not intractable problem domain with the potential to aid the development
and evaluation of general agents.

Acknowledgments
We would like to thank Marc Lanctot, Erik Talvitie, and Matthew Hausknecht for providing
suggestions on helping debug and improving the Arcade Learning Environment source code.
We would also like to thank our reviewers for their helpful feedback and enthusiasm about
the Atari 2600 as a research platform. The work presented here was supported by the
Alberta Innovates Technology Futures, the Alberta Innovates Centre for Machine Learning
at the University of Alberta, and the Natural Science and Engineering Research Council of
Canada. Invaluable computational resources were provided by Compute/Calcul Canada.

267

fiBellemare, Naddaf, Veness, & Bowling

Appendix A. Feature Set Construction
This section gives a detailed description of the five feature generation techniques from
Section 3.1.
A.1 Basic Abstraction of the ScreenShots (BASS)
The idea behind BASS is to directly encode colours present on the screen. This method is
motivated by three observations on the Atari 2600 hardware and games:
1. While the Atari 2600 hardware supports a screen resolution of 160210, game objects
are usually larger than a few pixels. Overall, important game events happen at a much
lower resolution.
2. Many Atari 2600 games have a static background, with a few important objects moving on the screen. While the screen matrix is densely populated, the actual interesting
features on the screen are often sparse.
3. While the hardware can show up to 128 colours in the NTSC mode, it is limited to
only 8 colours in the SECAM mode. Consequently, most games use a few number of
colours to distinguish important objects on the screen.
The game screen is first preprocessed by subtracting its background, detected using a simple
histogram method. BASS then encodes the presence of each of the eight SECAM palette
colours at a low resolution, as depicted in Figure 5. Intuitively, BASS seeks to capture
the presence of objects of certain colours at different screen locations. BASS also encodes
relations between objects by constructing all pairwise combinations of its encoded colour
features. In Asterix, for example, it is important to know if there is a green object (player
character) and a red object (collectable object) in its vicinity. Pairwise features allow us to
capture such object relations.

Figure 5: Left: Freeway in SECAM colours. Right: BASS colour encoding for the same
screen.

A.2 Basic
The Basic method generates the same set of features as BASS, but omits the pairwise
combinations. This allows us to study whether the additional features are beneficial or
268

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

harmful to learning. Because the Basic method has fewer features than BASS, it encodes
the presence of each of the 128 colours. In comparison to BASS, Basic therefore represents
colour more accurately, but cannot represent object interactions.
A.3 Detecting Instances of Classes of Objects (DISCO)
This feature generation method is based on detecting a set of classes representing game
entities and locating instances of these classes on the screen. DISCO is motivated by the
following additional observations on Atari 2600 games:
1. The game entities are often instances of a few classes of objects. For instance, as
Figure 6 shows, while there are many objects in a sample screen of the game Freeway,
all of these objects are instances of only two classes: Chicken and Car. Similarly, all
the objects on a sample screen of the game Seaquest are instances of one of these
six classes: Fish, Swimmer, Player Submarine, Enemy Submarine, Player Bullet, and
Enemy Bullet.
2. The interaction between two objects can often be generalized to all instances of their
respective classes. As an example, consider Car -Chicken object interactions in Freeway: learning that there is lower value associated with one Chicken instance hitting
a Car instance can be generalized to all instances of those two classes.

Figure 6: Left: Screenshot of the game Freeway. Although there are ten different cars,
they can all be considered as instances of a single class. Right: Screenshot of
the game Seaquest depicting four different object classes.
DISCO first performs a series of preprocessing steps to discover classes, during which
no value function learning is performed. When the agent subsequently learns to play the
game, DISCO generates features by detecting objects on the screen and classifying them.
The DISCO process is summarized by the following steps:
 Preprocessing:
 Background detection: The static background matrix is extracted using a
histogram method, as with BASS.
269

fiBellemare, Naddaf, Veness, & Bowling

Algorithm 1 Locally Sensitive Hashing (LSH) Feature Generation
Constants. M (hash table size), n (screen bit vector size)
l (number of random bit vectors), k (number of non-zero entries)
Initialization (once).
{v1 . . . vl }  generateRandomVectors(l, k, n)
{hash1 . . . hashl }  generateHashFunctions(l, M, n)
Input. A screen matrix I with elements Ixy  {0, . . . , 127}
LSH(I)
s  binarizeScreen(I) (s has length n)
Initialize   RlM = 0
for i = 1 . . . l do
h=0
for j = 1 . . . n do
h  h + I[sj =vij ] hashi [j] mod M
(hash the projection of s onto vi )
end for
[M (i  1) + h] = 1
(one binary feature per random bit vector)
end for
binarizeScreen(I)
Initialize s  Rn = 0
for y = 1 . . . h, x = 1 . . . w (h = 210, w = 160) do
s[x + y  h + Ixy ] = 1
end for
return s
generateRandomVectors(l, k, n)
Initialize v1 . . . vl  Rn = 0
for i = 1 . . . l do
Select x1 , x2 , . . . , xk distinct coordinates between 1 and n uniformly at random
vi [x1 ] = 1; vi [x2 ] = 1; . . . ; vi [xk ] = 1
end for
return {v1 , . . . vl }
generateHashFunctions(l, M, n) (hash functions are vectors of random coordinates)
Initialize hash1 . . . hashl  Rn = 0
for i = 1 . . . l, j = 1 . . . n do
hashi [j]  random(1, M ) (uniformly random coordinate between 1 and M)
end for
return {hash1 , . . . hashl }
Remark. With sparse vector operations, LSH has a O(lk + n) cost per step.

270

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Figure 7: Left: Screenshot of the game Seaquest. Right: Objects detected by DISCO
in the game Seaquest. Each colour represents a different class.

 Blob extraction: A list of moving blob (foreground) objects is detected in each
game screen.
 Class discovery: A set of classes is detected from the extracted blob objects.
 Class filtering: Classes that appear infrequently or are restricted to small region
of the screen are removed from the set.
 Class merging: Classes that have similar shapes are merged together.
 Feature generation:
 Class instance detection: At each time step, class instances are detected from
the current screen matrix.
 Feature vector generation: A feature vector is generated from the detected
instances by tile-coding their absolute position as well as the relative position
and velocity of every pair of instances from different classes. Multiple instances
of the same objects are combined additively.
Figure 7 shows discovered objects in a Seaquest frame. This image illustrates the difficulties in detecting objects: although DISCO correctly classifies the different fish as part
of the same class, it also detects a life icon and the oxygen bar as part of that class.
A.4 Locality Sensitive Hashing (LSH)
An alternative approach to BASS and DISCO is to use well-established feature generation
methods that are agnostic about the type of input they receive. Such methods include
polynomial bases (Schweitzer & Seidmann, 1985), sparse distributed memories (Kanerva,
1988) and locality sensitive hashing (LSH) (Gionis et al., 1999). In this paper we consider
the latter as a simple mean of reducing the large image space to a smaller, more manageable
set of features. The input  here, a game screen  is first mapped to a bit vector of size
7  210  160. The resulting vector is then hashed down into a smaller set of features.
LSH performs an additional random projection step to ensure that similar screens are more
likely to be binned together. The LSH generation method is detailed in Algorithm 1.
271

fiBellemare, Naddaf, Veness, & Bowling

A.5 RAM-based Feature Generation
Unlike the previous three methods, which generate feature vectors based on the game screen,
the RAM-based feature generation method relies on the contents of the console memory.
The Atari 2600 has only 128  8 = 1024 bits of random access memory7 , which must hold
the complete internal state of a game: location of game entities, timers, health indicators,
etc. The RAM is therefore a relatively compact representation of the game state, and in
contrast to the game screen, it is also Markovian. The purpose of our RAM-based agent
is to investigate whether features generated from the RAM affect performance differently
from features generated from game screens.
The first part of the generated feature vector simply includes the 1024 bits of RAM.
Atari 2600 game programmers often used these bits not as individual values, but as part
of 4-bit or 8-bit words. Linear function approximation on the individual bits can capture
the value of these multi-bit words. We are also interested in the relation between pairs of
values in memory. To capture these relations, the logical-AND of all possible bit pairs is
appended to the feature vector. Note that a linear function on the pairwise AN Ds can
capture products of both 4-bit and 8-bit words. This is because the product of two n-bit
words can be expressed as a weighted sum of the pairwise products of their bits.

7. Some games provided more RAM on the game cartridge: the Atari Super Chip, for example, offered an
additional 128 bytes of memory. The current approach only considers the main memory included in the
Atari 2600 console.

272

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Appendix B. UCT Pseudocode
Algorithm 2 UCT
Constants. m (search horizon), k (simulations per step)
Variables.  (search tree)
Input. s (current state)
UCT(s)
if  is empty or root() 6= s then
  empty search tree
root()  s
end if
repeat
sample(, m)
until visits(root()) = k
a  bestAction()
prune(, a)
(optional)
return a
sample(, m)
n  root()
while n is not a leaf, m > depth(n) do
if some action a was never taken in n then
(c, reward)  emulate(n, a)
(run model for one step)
immediate-return(c)  reward
child(n, a)  c
nc
(c is necessarily a leaf)
else
a  selectAction(n)
n  child(n, a)
end if
end while
R = rollout(n, m  depth(n))
update-value(n, R)
(propagate values back up)
bestAction()
return arg maxa [visits(child(root(), a))]
prune(, a)
root()  child(root(), a)

273

(action most frequently taken at root)

fiBellemare, Naddaf, Veness, & Bowling

Algorithm 3 UCT Routines
Constants.  : discount factor
selectAction(n)
for all c children of n do
q
V (c)  average-return(c) + log[visits(c)]
visits(n)
end for
return arg maxa V (child(n, a))
rollout(n, m)
R=0
(Initialize Monte-Carlo return to 0)
g=1
while m > 0 do
Select a according to some rollout policy (e.g. uniformly randomly)
(n, reward)  emulate(n, a)
R  R + g  reward
mm1
g g
end while
return R
update-value(n, R)
R  R + immediate-reward(n)
visits(n)
R
average-return(n)  average-return(n) visits(n)+1
+ visits(n)+1
visits(n)  visits(n) + 1
if n is not the root of , i.e. parent(n) 6= null then
update-value(parent(n),   R)
end if

274

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Appendix C. Experimental Parameters
General

All experiments
Reinforcement learning

Preprocessing

Background detection
Class discovery

Reinforcement
learning

All agents
BASS and
Basic

BASS only
Basic only
DISCO

RAM-based
LSH

Planning

UCT

Full-tree search

Maximum frames per episode
Frames per action
Training episodes per trial
Evaluation episodes per trial
Number of trials per result
Sample screens per game
Sample screens per game
Maximum number of classes
Maximum object velocity (pixels)
Minimum frequency of class appearance
Discount factor 
Exploration rate 
Learning rate 
Eligibility traces decay rate 
Grid width
Grid height
Number of different colours
Number of different colours
Learning rate 
Eligibility traces decay rate 
Tile coding, number of tilings
Tile coding, grid size
Learning rate 
Eligibility traces decay rate 
Learning rate 
Eligibility traces decay rate 
Number of random vectors l
Number of non-zero vector entries k
Per-vector hash table size M
Simulations per action
Maximum search depth (frames)
Exploration constant
Maximum frames emulated per action

275

18,000
5
5,000
500
30
18,000
36,000
10
8
20%
0.999
0.05
0.5
0.9
16
14
8
128
0.1
0.9
8
8
0.2
0.5
0.5
0.5
2000
1000
50
500
300
0.1
133,000

fiBellemare, Naddaf, Veness, & Bowling

Appendix D. Detailed Results
D.1 Reinforcement Learning
Game

Basic

BASS

DISCO

LSH

RAM

Random

Const

Perturb

Asterix
Beam Rider
Freeway
Seaquest
Space Invaders

862.3
929.4
11.3
579.0
203.6

859.8
872.7
16.4
664.8
250.1

754.6
563.0
12.8
421.9
239.1

987.3
793.6
15.4
508.5
222.2

943.0
729.8
19.1
593.7
226.5

288.1
434.7
0.0
107.9
156.1

650.0
996.0
21.0
160.0
245.0

337.8
754.8
22.5
451.1
270.5

Alien
Amidar
Assault
Asteroids
Atlantis
Bank Heist
Battle Zone
Berzerk
Bowling
Boxing
Breakout
Carnival
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Elevator Action
Enduro
Fishing Derby
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Journey Escape
Kangaroo
Krull
Kung-Fu Master
Montezumas Revenge
Ms. Pac-Man
Name This Game
Pooyan
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Skiing
Star Gunner
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

939.2
64.9
465.8
829.7
62687.0
98.8
15534.3
329.2
28.5
-2.8
3.3
2323.9
7725.5
1191.4
6303.1
520.5
-15.8
3025.2
111.8
-92.6
161.0
545.8
185.3
6053.1
-13.9
197.3
-8441.0
962.4
2823.3
16416.2
10.7
1537.2
1818.9
800.3
-19.2
81.9
613.5
1708.9
67.7
12.8
-1.1
850.2
-0.2
1728.2
40.7
3532.7
0.0
15046.8
1768.8
1392.0

893.4
103.4
378.4
800.3
25375.0
71.1
12750.8
491.3
43.9
15.5
5.2
1574.2
8803.8
1581.5
7455.6
318.5
-13.1
2377.6
129.1
-92.1
161.1
1288.3
251.1
6458.8
-14.8
202.8
-14730.7
1622.1
3371.5
19544.0
0.1
1691.8
2386.8
1018.9
-19.0
100.7
497.2
1438.0
65.2
10.1
-0.7
1069.5
-0.1
2299.5
52.6
3351.0
66.0
12574.2
1981.3
2069.1

623.6
67.9
371.7
744.5
20857.3
51.4
0.0
329.0
35.2
12.4
3.9
1646.3
6210.6
1349.0
4552.9
208.8
-23.2
4.6
0.0
-89.5
176.6
295.7
197.4
2719.8
-18.9
17.3
-9392.2
457.9
2350.9
3207.0
0.0
999.6
1951.0
402.7
-19.6
-23.0
326.3
0.0
21.4
9.3
-0.1
1002.2
-0.1
0.0
0.0
2473.4
0.0
10779.5
935.6
69.8

510.2
45.1
628.0
590.7
17593.9
64.6
14548.1
441.0
26.1
10.5
2.5
1147.2
6161.6
943.0
20453.7
355.8
-21.6
3220.6
95.8
-93.2
216.9
941.8
105.9
3835.8
-15.1
77.1
-13898.9
256.4
2798.1
8715.6
0.1
1070.8
2029.8
1225.3
-19.9
684.3
529.1
1904.3
42.0
10.8
-0.0
722.9
-0.1
2429.2
85.2
2475.1
0.0
9813.9
945.5
3365.1

726.4
71.4
383.6
907.3
19932.7
190.8
15819.7
501.3
29.3
44.0
4.0
765.4
7555.4
1397.8
23410.6
324.8
-20.3
507.9
112.3
-91.6
147.9
722.5
387.7
3281.1
-9.5
133.8
-8713.5
481.7
2901.3
10361.1
0.3
1021.1
2500.1
1210.9
-19.9
111.9
565.8
1309.9
41.0
28.7
0.0
769.3
-0.1
3741.2
114.3
3412.6
0.0
16871.3
1096.2
304.3

102.0
0.8
334.3
1526.7
33058.4
15.0
2920.0
233.8
24.6
-1.5
1.5
869.2
2805.1
698.2
2335.4
289.3
-15.6
1040.9
0.0
-93.8
70.3
243.7
205.4
712.0
-14.8
23.3
-18201.7
44.4
1880.1
488.2
0.3
163.3
2012.3
501.1
-20.9
-754.0
169.0
1608.6
36.2
1.6
0.0
638.1
-24.0
3458.8
23.1
131.6
0.0
20021.1
772.4
0.0

140.0
31.0
357.0
140.0
1500.0
0.0
13000.0
670.0
30.0
-25.0
3.0
0.0
16527.0
1000.0
0.0
130.0
0.0
0.0
9.0
-99.0
160.0
0.0
0.0
0.0
-1.0
0.0
0.0
200.0
0.0
0.0
0.0
210.0
3080.0
30.0
-21.0
0.0
150.0
1070.0
900.0
17.0
0.0
600.0
0.0
500.0
0.0
550.0
0.0
705.0
300.0
0.0

313.9
37.8
497.8
539.9
12089.1
13.5
5772.0
552.9
30.0
-10.1
2.9
485.4
8937.2
973.7
2235.0
776.2
-20.3
562.9
25.9
-97.2
175.2
286.8
106.0
147.5
-6.5
82.0
-10693.9
498.4
1690.1
578.4
0.0
505.5
1854.3
540.8
-20.8
1947.3
157.4
1455.5
857.9
11.3
0.0
509.8
-0.3
718.7
17.3
2962.9
0.0
9527.9
470.3
2.0

Times Best

6

17

1

8

8

2

9

4

Table 4: Reinforcement Learning results. The first five games constitute our training set.
See Section 3.1 for details.
.

276

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

D.2 Planning
Game

Full Tree

UCT

Best Learner

Best Baseline

Asterix
Beam Rider
Freeway
Seaquest
Space Invaders

2135.7
693.5
0.0
288.0
112.2

290700.0
6624.6
0.4
5132.4
2718.0

987.3
929.4
19.1
664.8
250.1

650.0
996.0
22.5
451.1
270.5

Alien
Amidar
Assault
Asteroids
Atlantis
Bank Heist
Battle Zone
Berzerk
Bowling
Boxing
Breakout
Carnival
Centipede
Chopper Command
Crazy Climber
Demon Attack
Double Dunk
Elevator Action
Enduro
Fishing Derby
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Journey Escape
Kangaroo
Krull
Kung-Fu Master
Montezumas Revenge
Ms. Pacman
Name This Game
Pooyan
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Skiing
Star Gunner
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard of Wor
Zaxxon

784.0
5.2
413.7
3127.4
30460.0
21.5
6312.5
195.0
25.5
100.0
1.1
950.0
125123.0
1827.3
37110.0
442.6
-18.5
730.0
0.6
-91.6
137.2
1019.0
395.0
1323.8
-9.2
25.0
1327.3
90.0
3089.2
12127.3
0.0
1708.5
5699.0
909.7
-20.7
57.9
132.8
2178.5
245.0
1.5
0.0
1345.0
-23.8
4063.6
64.1
746.0
0.0
55567.3
3309.1
0.0

7785.0
180.3
1512.2
4660.6
193858.0
497.8
70333.3
553.5
25.1
100.0
364.4
5132.0
110422.0
34018.8
98172.2
28158.8
24.0
18100.0
286.3
37.8
270.5
20560.0
2850.0
12859.5
39.4
330.0
7683.3
1990.0
5037.0
48854.5
0.0
22336.0
15410.0
17763.4
21.0
100.0
17343.4
4449.0
38725.0
50.4
-0.8
1207.1
2.8
63854.5
225.5
74473.6
0.0
254748.0
105500.0
22610.0

939.2
103.4
628.0
907.3
62687.0
190.8
15819.7
501.3
43.9
44.0
5.2
2323.9
8803.8
1581.5
23410.6
520.5
-13.1
3220.6
129.1
-89.5
216.9
1288.3
387.7
6458.8
-9.5
202.8
-8441.0
1622.1
3371.5
19544.0
10.7
1691.8
2500.1
1225.3
-19.0
684.3
613.5
1904.3
67.7
28.7
0.0
1069.5
-0.1
3741.2
114.3
3532.7
66.0
16871.3
1981.3
3365.1

313.9
37.8
497.8
1526.7
33058.4
15.0
13000.0
670.0
30.0
-1.5
3.0
869.2
16527.0
1000.0
2335.4
776.2
0.0
1040.9
25.9
-93.8
175.2
286.8
205.4
712.0
-1.0
82.0
0.0
498.4
1880.1
578.4
0.3
505.5
3080.0
540.8
-20.8
1947.3
169.0
1608.6
900.0
17.0
0.0
638.1
0.0
3458.8
23.1
2962.9
0.0
20021.1
772.4
2.0

Times Best

4

45

3

3

Table 5: Search results. The first five games constitute our training set. See Section 3.2 for
details.
.

277

fiBellemare, Naddaf, Veness, & Bowling

References
Bellemare, M., Veness, J., & Bowling, M. (2012). Investigating contingency awareness using
Atari 2600 games. In Proceedings of the the 26th Conference on Artificial Intelligence
(AAAI).
Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P.,
Tavener, S., Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of Monte Carlo
tree search methods. IEEE Transactions on Computational Intelligence and AI in
Games, 4 (1), 1 43.
Cobo, L. C., Zang, P., Isbell, C. L., & Thomaz, A. L. (2011). Automatic state abstraction
from demonstration. In Proceedings of the 22nd Second International Joint Conference
on Articial Intelligence (IJCAI).
Coles, A., Coles, A., Olaya, A., Jimenez, S., Lopez, C., Sanner, S., & Yoon, S. (2012). A
survey of the seventh international planning competition. AI Magazine, 33 (1), 8388.
Diuk, C., Cohen, A., & Littman, M. L. (2008). An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th International Conference on
Machine learning (ICML).
Dowe, D. L., & Hajek, A. R. (1998). A non-behavioural, computational extension to the
Turing Test. In Proceedings of the International Conference on Computational Intelligence and Multimedia Applications (ICCIMA).
Genesereth, M. R., Love, N., & Pell, B. (2005). General Game Playing: Overview of the
AAAI competition. AI Magazine, 26 (2), 6272.
Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via
hashing. In Proceedings of the International Conference on Very Large Databases.
Hausknecht, M., Khandelwal, P., Miikkulainen, R., & Stone, P. (2012). HyperNEAT-GGP:
A HyperNEAT-based Atari general game player. In Proceedings of the Genetic and
Evolutionary Computation Conference (GECCO).
Hernandez-Orallo, J., & Dowe, D. L. (2010). Measuring universal intelligence: Towards an
anytime intelligence test. Artificial Intelligence, 174 (18), 1508  1539.
Hernandez-Orallo, J., & Minaya-Collado, N. (1998). A formal definition of intelligence
based on an intensional variant of Kolmogorov complexity. In Proceedings of the
International Symposium of Engineering of Intelligent Systems (EIS).
Hutter, M. (2005). Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin.
Kanerva, P. (1988). Sparse Distributed Memory. The MIT Press.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. In Proceedings of
the 15th European Conference on Machine Learning (ECML).
Legg, S. (2008). Machine Super Intelligence. Ph.D. thesis, University of Lugano.
Legg, S., & Veness, J. (2011). An approximation of the universal intelligence measure. In
Proceedings of the Ray Solomonoff Memorial Conference.
278

fiThe Arcade Learning Environment: An Evaluation Platform for General Agents

Mohan, S., & Laird, J. E. (2009). Learning to play Mario. Tech. rep. CCA-TR-2009-03,
Center for Cognitive Architecture, University of Michigan.
Monroy, G. A., Stanley, K. O., & Miikkulainen, R. (2006). Coevolution of neural networks
using a layered pareto archive. In Proceedings of the 8th Genetic and Evolutionary
Computation Conference (GECCO).
Montfort, N., & Bogost, I. (2009). Racing the Beam: The Atari Video Computer System.
MIT Press.
Naddaf, Y. (2010). Game-Independent AI Agents for Playing Atari 2600 Console Games.
Masters thesis, University of Alberta.
Pell, B. (1993). Strategy Generation and Evaluation for Meta-Game Playing. Ph.D. thesis,
University of Cambridge.
Pierce, D., & Kuipers, B. (1997). Map learning with uninterpreted sensors and effectors.
Artificial Intelligence, 92 (1-2), 169227.
Russell, S. J. (1997). Rationality and intelligence. Artificial intelligence, 94 (1), 5777.
Schaul, T., Togelius, J., & Schmidhuber, J. (2011). Measuring intelligence through games.
CoRR, abs/1109.1314.
Schweitzer, P. J., & Seidmann, A. (1985). Generalized polynomial approximations in Markovian decision processes. Journal of mathematical analysis and applications, 110 (2),
568582.
Stober, J., & Kuipers, B. (2008). From pixels to policies: A bootstrapping agent. In
Proceedings of the 7th IEEE International Conference on Development and Learning
(ICDL).
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT
Press.
Sutton, R., Modayil, J., Delp, M., Degris, T., Pilarski, P., White, A., & Precup, D. (2011).
Horde: A scalable real-time architecture for learning knowledge from unsupervised
sensorimotor interaction. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagents Systems (AAMAS).
Thrun, S., & Mitchell, T. M. (1995). Lifelong robot learning. Robotics and Autonomous
Systems, 15 (1), 2546.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Whiteson, S., Tanner, B., Taylor, M. E., & Stone, P. (2011). Protecting against evaluation
overfitting in empirical reinforcement learning. In Proceedings of the IEEE Symposium
on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL).
Whiteson, S., Tanner, B., & White, A. (2010). The reinforcement learning competitions.
AI Magazine, 31 (2), 8194.
Wintermute, S. (2010). Using imagery to simplify perceptual abstraction in reinforcement
learning agents. In Proceedings of the the 24th Conference on Artificial Intelligence
(AAAI).

279

fiJournal of Artificial Intelligence Research 47 (2013) 475-519

Submitted 12/12; published 07/13

On the Computation of Fully Proportional Representation
Nadja Betzler

nadja.betzler@campus.tu-berlin.de

Institut fur Softwaretechnik und
Theoretische Informatik
TU Berlin

Arkadii Slinko

slinko@math.auckland.ac.nz

Department of Mathematics
University of Auckland

Johannes Uhlmann

johannes.uhlmann@campus.tu-berlin.de

Institut fur Softwaretechnik und
Theoretische Informatik
TU Berlin

Abstract
We investigate two systems of fully proportional representation suggested by Chamberlin & Courant and Monroe. Both systems assign a representative to each voter so that
the sum of misrepresentations is minimized. The winner determination problem for both
systems is known to be NP-hard, hence this work aims at investigating whether there are
variants of the proposed rules and/or specific electorates for which these problems can be
solved efficiently. As a variation of these rules, instead of minimizing the sum of misrepresentations, we considered minimizing the maximal misrepresentation introducing effectively
two new rules. In the general case these minimax versions of classical rules appeared to
be still NP-hard.
We investigated the parameterized complexity of winner determination of the two classical and two new rules with respect to several parameters. Here we have a mixture of
positive and negative results: e.g., we proved fixed-parameter tractability for the parameter the number of candidates but fixed-parameter intractability for the number of winners.
For single-peaked electorates our results are overwhelmingly positive: we provide polynomial-time algorithms for most of the considered problems. The only rule that remains
NP-hard for single-peaked electorates is the classical Monroe rule.

1. Introduction
There is an important conceptual difference in the purpose of single-winner and multiwinner elections. Single-winner social choice rules are used to make final decisions, e.g., to
elect a president or to choose a certain course of action. The multi-winner election rules
are used to elect an assembly whose members will be authorized to take final decisions on
behalf of the society. As a result the main property that multi-winner rules have to satisfy
is that the elected assembly represents the society adequately. This, in particular, means
that when a final decision is taken all opinions existing in the society are heard and taken
into consideration. As Black powerfully expressed it:
c
2013
AI Access Foundation. All rights reserved.

fiBetzler, Slinko, & Uhlmann

A scheme of proportional representation attempts to secure an assembly whose
membership will, so far as possible, be proportionate to the volume of the different shades of political opinion held throughout the country; the microcosm is
to be a true reflexion of the macrocosm (Black, 1958, p. 75).
And although any single-winner social choice rule can be easily extended to select an
assemblye.g., by choosing candidates with best scores or applying the rule repeatedly until
the required quantity of representatives is electedthis is a wrong approach to the problem
(Brams & Fishburn, 2002) (see also Lu & Boutilier, 2011 for some experimental evidence).
The reason is that the majoritarian logic which dominates the design of single-winner social
choice rules cannot provide for a balanced assembly membership.
The standard solution to the problem of electing an assembly has been the division of
the election into single-member districts with approximately equal population. Each district
elects one member of the assembly using a single-winner rule, normally the plurality. And
although one might question whether districting should be instead based on the total adult
population or on the number of registered voters, the current practice is well established and
entrenched by law in many countries, including the United States (Brams, 2008). However
the main problem with this approach is not the districting but the fact that it also fails to
give a representation to minorities; a minority may comprise 49% of the population and be
not represented in the assembly. On the positive side the districting method provides for a
high level of accountability: voters who know who is their representative, can address them
on particular issues and can even recall them, if they fail to represent them to a decent
standard.
Various voting systemse.g., Cumulative Voting, Single Non-Transferable vote, multiwinner variants of Single Transferable Vote, various party list systemshave been designed
to solve the problem of representation of minorities (Brams & Fishburn, 2002). However
none of them scored high on the accountability. It may even seem that we have a certain
trade-off here and we cannot have both representation of minorities and accountability.
However this is not the case. An important idea was suggested by Charles Dodgson (1884),
known also as Lewis Carroll, and considered in a different form by Black (1958). Then the
idea was further developed by Chamberlin and Courant (1983) and later by Monroe (1995).
The relative advantages of both methods from political science point of view have been
extensively discussed by Brams (2008). Dodgson asserted that a representation system
should find the coalitions in the election that would have formed if the voters had the
necessary time and information and allow each of the coalitions to elect their representative.
If this is adopted, then a sizable minority can form a coalition and be represented.
The realization of this idea required a new concept which is the concept of misrepresentation. It is assumed that voters form individual preferences over the candidates based on
their political ideology and their judgement about the abilities of candidates to participate
in deliberations and decision making consistent with how the individuals would wish to act
were they present (Chamberlin & Courant, 1983, p. 722). This was in a way a revolution.
Indeed, in the single-winner literature on voting rules it is widely accepted that
voters political preferences are more complex than their first choices alone. However,
in multi-winner voting literature fixation on first preferences led researchers to think
about proportional representation exclusively in terms of first preferences. In list systems
476

fiOn the Computation of Fully Proportional Representation

of proportional representation, parties are assigned a number of seats in parliament that
is proportional to the number of votes (first preferences) they received. The systems like
Single Non-Transferable Vote, Block Voting and Cumulative Voting also do not take second preferences in account (Levin & Nalebuff, 1995). Only the Single Transferable Vote
is a system of proportional representationin fact a family of voting methods according
to Tideman and Richardson (2000)that allows voters to express the order of preference
of candidates (Levin & Nalebuff, 1995). Voters rank the candidates in order of preference;
first preference votes are the first to be looked at, and the votes are then transferred, if
necessary, from candidates who have either been comfortably elected or who have done so
badly that they have been eliminated from the election.1
So, if a voter is represented by a candidate who is her first preference it is reasonable to
say that she is represented optimally or that the misrepresentation in this case is zero. In
general, if a voter is represented by a candidate who is her ith preference we may assume
that she is misrepresented to the degree si . Of course, it is reasonable to assume that
0 = s1  s2  . . .  sm . So in this case the rule for measuring the total misrepresentation
is fully defined by the vector s = (s1 , . . . , sm ), where m is the number of candidates.
Using the analogy with positional scoring rules for single-winner elections we may say that
this misrepresentation function is positional. In general, the problem of choosing a proper
misrepresentation function is far from being trivial. In the work of Levin and Nalebuff (1995,
p. 4) this difficulty is vividly illustrated: if the electorate is uniformly distributed on the
segment between 0 and 1, and we are to choose three representatives, should they be equally
spaced [0.25, 0.5, 0.75], or should they be selected so as to minimize the average distance
traveled to the nearest legislator [0.16, 0.5, 0.83]? In the broadest possible framework the
misrepresentation function may be even voter-dependent.
Staying with the classical positional misrepresentation functions for the time being suppose that every voter is assigned to a representative in some way. Measuring the total
misrepresentation for the whole society we may adopt either the Harsanyi (1995) approach
or the Rawlsian one (assuming that the utility of being represented by the ith best candidate is si , i.e., a nonpositive value). By Harsanyi we will have to measure the total
misrepresentation as
m
X
ni si ,
MH =
i=1

where ni is the number of voters represented by their ith most preferred candidate. According to Rawls (1999) welfare is maximized when the utility of those society members
that have the least is the greatest. This leads to the total misrepresentation function
MR =

max

i with ni >0

si .

Both Chamberlin and Courant (1983), and Monroe (1995) consider that the best set of
representatives must minimize the total misrepresentation which they both calculate using
the Borda vector of scores, that is, (0, 1, 2, . . . , m  1) and Harsanyis misrepresentation
1. In Northern Ireland this is the voting system used for elections to local councils, the Assembly, and the
European Parliament. It is used for all elections in the Irish Republic, Malta, and Australia (although
single-member constituencies are prevalent in Australia, apart from state level elections in Tasmania and
the ACT). Several other countries have recently debated adopting it.

477

fiBetzler, Slinko, & Uhlmann

function. Their methods are however different and the difference is very important. Chamberlin and Courant did not impose any restriction on the function that assigns candidates
to voters. This may potentially lead to a different number of voters represented by each
candidate. To remedy this Chamberlin and Courant suggested to use weighted voting in
the assembly where each elected candidate has the weight equal to the number of voters
they represent. Monroe rejected this approach and insisted on the principle one member
of assembly one vote. For this reason he insisted that the difference between the numbers
of voters assigned to any two representatives is at most one.
The reasons why Monroe rejected the Chamberlin and Courant approach are quite
substantial. Proportional allocation of weights is known to result in excessive voting powers
for the electorates of larger constituencies and there can be much debate on what is the right
way of allocating weights to representatives. An alternative to Chamberlin and Courants
method would be to give to representatives weights proportional to the square root of the
number of voters they represent. This is justified by the fact, that due to the square root
law of Penrose (1946), the a priori voting power (as defined by the Penrose-Banzhaf index)
of a member of a voting body is inversely proportional to the square root of its size. On the
basis of this theory, for example, Poland insisted that the EU allocate Council-of-Minister
votes according to the square root of each nations population (Slomczynski & Zyczkowski,
2006).
The computational problems which the Harsanyi approach entails are known to be
NP-hard (Lu & Boutilier, 2011; Procaccia, Rosenschein, & Zohar, 2008) for several classical misrepresentation functions. In this paper we try to achieve tractability in multiwinner
elections in three different ways. Firstly, we ask whether or not the problem of finding an optimal fully proportional representation becomes easier for these classical misrepresentation
functions if we adopt the Rawlsian approach for measurement of total misrepresentation.
The second goal is to find the parameterized complexity of the aforementioned problems
for some natural choices of parameters. The third is to develop efficient algorithms for
achieving an optimal fully proportional representation in single-peaked elections.
In the remainder of this section, we formally introduce the computational problems
considered in this paper, summarize the results in the extant literature, and describe our
approaches and results.
1.1 Computational Problems Considered
An election is a pair E = (C, V ) where C is a set of candidates (or alternatives) and V is an
ordered list of voters. Each voter is represented by her vote, which is a strict, linear order
over the set of candidates (also called this voters preference order ). We will refer to the
list V as a preference profile, and we denote the number of voters in V by n. The number
of alternatives will be denoted by m. If the order of voters is not important (the election
is anonymous), then V can be considered as a multiset2 of votes. In this paper we will
consider only anonymous elections.
By posv (c) we will denote the position of the alternative c in the ranking of voter v; the
top-ranked alternative has position 1, the second best has position 2, etc.
2. This is not a set since two different voters may have the same preference order.

478

fiOn the Computation of Fully Proportional Representation

Q

Definition 1. Given a profile V over C, a mapping r : V  C  +
0 will be called a
misrepresentation function if for any v  V and any two candidates c, c  C the condition
posv (c) < posv (c ) implies r(v, c)  r(v, c ).
This is to say that if c is preferred to c in vs ranking, then the misrepresentation of v,
when she is represented by c will be at least as large as her misrepresentation, when she
is represented by c. In the classical framework the misrepresentation of a candidate for a
voter is a function of the position of the candidate in the preference order of that voter
given by s = (s1 , . . . , sm ), i.e., the misrepresentation function in this case will be
r(v, c) = sposv (c) .
An important particular case is the Borda misrepresentation function defined by the vector
(0, 1, . . . , m  1).
In the approval voting framework, if a voter is represented by a candidate whom she
approves, her misrepresentation is considered to be zero, otherwise it is equal to one. This
function is called the approval misrepresentation function. This misrepresentation function does not have to be positional since different voters may approve different number of
candidates. Note that some misrepresentation functions, like Borda, can be derived from
the preference lists of the voters. In contrast, an approval misrepresentation function cannot be obtained from a preference list without further information about the threshold
that separates approved candidates from disapproved ones. In the general framework the
misrepresentation function may be arbitrary.
By w : V  C we denote the function that assigns voters to representatives (or the
other way around), i.e., under this assignment voter v is represented by candidate w(v).
The total misrepresentation of the election under w is then given by
X
r(v, w(v)) or max r(v, w(v))
vV

vV

in the Harsanyis classical and Rawls minimax versions, respectively. We say that a mapping w respects the M -criterion (or Monroe criterion) if |w(V )| = k and w assigns at least
n/k and at most n/k voters to every candidate from w(V ), where k is the total number
of representatives to be elected to the assembly. Note that in case of the M -criterion a set
of more than k winners might lead to a higher misrepresentation than a set of k winners.
For example, consider an election such that all voters favour the same candidate but the
set of winners that has to be elected is greater than one.
Based on the previous discussion, in this work we investigate the computational complexity of the following four combinatorial problems. The two classical ones described above
are named after Chamberlin and Courant (CC), for the case when a candidate can represent an arbitrary number of voters (and this number of voters will be his weight in the
elected assembly), and Monroe (M), for the case when every candidate represents roughly
the same number of voters (and each representative has one vote in the assembly). The two
previously unstudied versions which adopt the Rawlsian approach for measuring the total
misrepresentation are called the minimax versions of the classical ones.
CC-Multiwinner (CC-MW)
Given: A set C of candidates, a multiset V of voters, a misrepresentation
function r, a misrepresentation bound R  +
0 and a positive integer k.

Q

479

fiBetzler, Slinko, & Uhlmann


Task: Find a subset
P C  C of size k and an assignment of voters w such that

w(V ) = C and vV r(v, w(v))  R.

Minimax CC-Multiwinner (Minimax CC-MW)
Given: A set C of candidates, a multiset V of voters, a misrepresentation
function r, a misrepresentation bound R  +
0 and a positive integer k.

Q

C

Task: Find a subset
 C of size k and an assignment of voters w such that
w(V ) = C  and maxvV r(v, w(v))  R.
M-Multiwinner (M-MW)
Given: A set C of candidates, a multiset V of voters, a misrepresentation
function r, a misrepresentation bound R  +
0 and a positive integer k.

Q

C

Task: Find a subset
 C of size k and an assignment
of voters w, which
P

respects the M -criterion, w(V ) = C and such that vV r(v, w(v))  R.

Minimax M-Multiwinner (Minimax M-MW)
Given: A set C of candidates, a multiset V of voters, a misrepresentation
function r, a misrepresentation bound R  +
0 and a positive integer k.

Q

C

Task: Find a subset
 C of size k and an assignment of voters w, which
respects the M -criterion, w(V ) = C  and such that maxvV r(v, w(v))  R.
Note that finding an assignment of voters to a fixed set of k winners can be accomplished in polynomial time for all four problems by applying network flow algorithms (see
Section 3.2). Hence, in what follows we assume that k < m and k < n since otherwise all
four problems can be decided in polynomial time. We also note that all problems considered
are contained in NP since one can guess a set of k winners and a corresponding mapping
to the voters and check in polynomial time whether it satisfies the corresponding conditions.
The four problems above are stated for general misrepresentation functions (since some
of our algorithmic results hold even for this case) but the main focus of this work is on the
Borda and the approval ones.
1.2 Previous Computational Complexity Results
The study of the computational complexity of problems in the context of voting was initiated by Bartholdi III, Tovey, and Trick (1989) about 20 years ago but has became an active
area of research only recently (Conitzer, 2010; Faliszewski & Procaccia, 2010; Faliszewski,
Hemaspaandra, & Hemaspaandra, 2010; Faliszewski, Hemaspaandra, Hemaspaandra, &
Rothe, 2009b). While there is a large number of papers dealing with single-winner elections
or multi-winner elections whose final goal is still to choose a single winner after a tiebreaking, only few articles (Potthof & Brams, 1998; Procaccia et al., 2008; Lu & Boutilier,
2011) deal with the computational complexity of Multiwinner elections aimed at achieving
a proportional representation. In particular, these works contain NP-hardness proofs for
CC-Multiwinner and M-Multiwinner for approval misrepresentation function (Procaccia et al., 2008) and for CC-Multiwinner for Borda misrepresentation function (Lu &
Boutilier, 2011). Algorithmic approaches comprise Integer Linear Programming (Potthof &
480

fiOn the Computation of Fully Proportional Representation

Brams, 1998; Brams, 2008) for CC-MW and M-MW, approximation algorithms based on
greedy strategies (Lu & Boutilier, 2011) for CC-MW, and polynomial-time algorithms for
CC-MW and M-MW for instances where the number of candidates is constant (Procaccia
et al., 2008). In contrast, to the best of our knowledge, the computational complexity of
the minimax versions of the problems remained unstudied.
We are only aware of one further work explicitly studying computational complexity
issues in the context of multiwinner elections. Meir, Procaccia, Rosenschein, and Zohar
(2008) investigate the computational complexity of strategic voting for several multiwinner
elections for which a winner can be determined in polynomial time. The systems considered
do not lead to any kind of proportional representation.
1.3 Our Approach and Results for General Elections
Our first result is that the minimax versions of the classical Chamberlin-Courant and Monroe problems are also NP-complete. In other words, adopting the Rawlsian approach does
not make computation of the problems easier in general (but we will see that the situation changes completely for single-peaked elections where the minimax version becomes
easier indeed). Based on these negative results, this work aims at extending the previous
algorithmic approaches described above by an analysis whether or not there are settings
in which the problems become tractable. To this end, parameterized algorithmics is an
appropriate tool as it aims at identifying tractable special cases of NP-hard problems. The
cornerstone of this approach is the idea that the complexity of a problem is not only measured in the total size of an input instance I but also in an additional parameter p, usually
a nonnegative integer (but it can be a pair of integers or virtually anything). A problem is called fixed-parameter tractable if there is an algorithm solving every instance of it
in f (p) poly(|I|) time, where f is a computable function (Downey & Fellows, 1999; Flum
& Grohe, 2006; Niedermeier, 2006). For small values of p an algorithm with such running
time might represent an efficient algorithm for the NP-hard problem under consideration.
Parameterized complexity also provides a tool of parameterized reductions by which one
can show that a problem is presumably not fixed-parameter tractable. One of the most
important parameterized complexity classes for this purpose is W [2] (see Section 2 for more
details). We remark in passing that a parameterized complexity analysis has been employed for several other voting problems, (e.g., see Brandt, Brill, & Seedig, 2011; Betzler,
Guo, & Niedermeier, 2010; Betzler, Hemmann, & Niedermeier, 2009; Christian, Fellows,
Rosamond, & Slinko, 2007; Dorn & Schlotter, 2010; Elkind, Faliszewski, & Slinko, 2010b;
Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a and also Betzler, Bredereck,
Chen, & Niedermeier, 2012 for a survey).
In the context of multiwinner elections, a parameter that immediately attracts attention
is the number k of winners, which in many settings might be much smaller than the number
of candidates or the number of voters. Another reasonable parameter is the misrepresentation bound R since in an ideal (or fully personalizable Lu & Boutilier, 2011) situation R is
equal to zero, that is, every voter is represented by one of her most preferred candidates.
We provide a parameterized complexity analysis of all four considered problems for the
Borda and approval misrepresentation functions with respect to the parameters k and R.
481

fiBetzler, Slinko, & Uhlmann

Parameter

r

CC-MW

Minimax CC-MW

M-MW

Minimax M-MW

#winner k
#winner k

A
B

W[2]-hard ()
W[2]-hard ()

W[2]-hard ()
W[2]-hard ()

W[2]-hard ()
W[2]-hard ()

W[2]-hard ()
W[2]-hard ()

misr. R
misr. R

A
B

NP-h for R = 0 ()
XP ()

NP-h for R = 0 ()
NP-h for R  1 ()
P for R = 0 ()

NP-h for R = 0 ()
XP ()

NP-h for R = 0 ()
NP-h for R  1 ()
P for R = 0 ()

(R, k)
(R, k)

A
B

W[2]-hard ()
FPT ()

W[2]-hard ()
FPT ()

W[2]-hard ()
FPT ()

W[2]-hard ()
FPT ()

# cand.
# voters

U
U

FPT ()
FPT ()

FPT ()
FPT ()

FPT ()
FPT ()

FPT ()
FPT ()

Table 1: Parameterized complexity of the considered multiwinner problems for instances
where the misrepresentation function r is either approval (A), Borda (B) or unrestricted (U). Results are obtained as follows. : Theorem 1, : Theorem 2, : Theorem 3, : Theorem 4, : Theorem 5, : Theorem 6, : Theorem 7 : Proposition 1,
 Proposition 2.

In addition, we also investigate the composite parameter (R, k) consisting of the number of
winners and the misrepresentation bound.
An overview of the results is provided in Table 1. When the number of winners k is
a parameter, all considered problems turn out to be W[2]-hard. For the parameterization
by the total misrepresentation bound R the results are more varied. For the case R = 0,
for the approval misrepresentation function all four problems are NP-hard while they are
solvable in polynomial time for the Borda misrepresentation function. However, Minimax
CC-MW and Minimax M-MW become NP-hard for every R  1. In contrast, the summinimization variants CC-MW and M-MW for the Borda misrepresentation function are
solvable in polynomial time for constant R (the corresponding parameterized complexity
class is called XP). Note that the provided algorithm shows the containment in XP with
respect to R but not fixed-parameter tractability, this problem remains open. This inspired
our analysis of the composite parameter (R, k), covering scenarios in which there is a small
set of winners that can represent all voters with a small total misrepresentation. While
for the approval misrepresentation function, this still leads to parameterized intractability, for the Borda misrepresentation function, we show fixed-parameter tractability for all
considered problems. To complete the picture of a multivariate complexity analysis, we
additionally provide fixed-parameter tractability with respect to the parameters number
of voters and number of candidates.
1.4 Results for Single-Peaked Elections
Single-peakedness is one of the central notions in social choice and political science alike
(Black, 1958; Moulin, 1991; Tideman, 2006). The preferences of voters are single-peaked
when a single issue dominates their formation. This could be their ideological position on
the Left-Right or Liberal-Conservative spectra, level of taxation, immigration quota, etc.
Tideman compares single-peakedness with convexity of preferences and discusses when it is
482

fiOn the Computation of Fully Proportional Representation

CC-MW
2

O(nm ) 

Minimax CC-MW
O(nm) 

M-MW
3

3 3

O(n m k ) for approval 
NP-h for integer mis. func. 

Minimax M-MW
O(n3 m3 k3 ) 

Table 2: Overview of the computational complexity for singled-peaked elections. In the case
of polynomial-time solvability, the table provides the running times depending
on the number n of voters, the number m of candidates, and the number k of
winners. If not stated otherwise, the result holds for an arbitrary misrepresentation
function. : Theorem 8, : Proposition 4, : Theorem 10, : Proposition 5,
: Theorem 11.

reasonable to assume this. He refers to a data collection containing 87 ranked-ballot real-life
elections, which he has access to, and claims that most of them are single-peaked.
This single dominating issue is normally represented by an axis and each voter is characterized by a single point on this axis (see an example on Figure 1. The misrepresentation
function for a fixed voter is then a function of a single variable defined on that axis. The
single-peakedness of preferences implies that this function has exactly one local minimum.
We refer to Section 5 for a formal proof of this statement.
We note that for votes in the form of approval ballots as well as linear orders, singlepeakedness of the profile can be checked in linear time (Booth & Lueker, 1976; Escoffier,
Lang, & Ozturk, 2008) with the reconstruction of the order of the candidates on the axis.
In the case of single-peaked profiles some computational problems have turned out to
allow for more efficient solving strategies than in the general case (Brandt, Brill, Hemaspaandra, & Hemaspaandra, 2010; Conitzer, 2009). In particular, the study of the computational complexity of voting rules with NP-hard winner-determination problem shows
that for all Condorcet-consistent onesand these include Dodgson, Kemeny, and Young
rulesthe winner-determination problem becomes polynomial-time solvable if we restrict
ourselves to single-peaked profiles (Brandt et al., 2010). The obvious reason for this is that
single-peakedness eliminates the possibility of Condorcet cycles in the election profile.
It is not that obvious that single-peakedness must also simplify the winner-determination
problem for methods of proportional representation. However, it seems natural to investigate this possibility. Our results show that in many instances the winner-determination
problem for methods of proportional representation does indeed become easier too.
Our results are summarized in Table 2. For CC-MW and Minimax CC-MW the problems are solvable in polynomial time for an arbitrary misrepresentation function. More
specifically, for CC-MW we provide a dynamic programming algorithm running in O(nm2 )
time for n voters and m candidates, and Minimax CC-MW can be solved in O(nm) time
by a greedy algorithm. For the Monroe system and its variants, the results become more
diverse. While Minimax M-MW for the general misrepresentation function is still solvable
in polynomial time, M-MW is NP-hard. However, on the positive side, we can still show
polynomial-time solvability for M-MW for the approval misrepresentation function. Basically, the results are obtained as follows. For M-MW for the approval misrepresentation
483

fiBetzler, Slinko, & Uhlmann

function we establish a close connection to a one-dimensional rectangle stabbing problem with capacities. This allows to provide a dynamic programming algorithm based on a
decomposition property provided by Even, Levi, Rawitz, Schieber, Shahar, and Sviridenko
(2008). This result can be transferred to Minimax M-MW. The NP-hardness of M-MW
is established by a many-one reduction from a restricted version of the Exact 3-Cover
problem. The NP-hardness holds for an integer-valued misrepresentation function for which
the maximum misrepresentation value is still polynomial in the number of candidates. However, we need to allow situations in which a voter may be equally misrepresented by several
candidates. Hence, it is not clear how to transfer the corresponding many-one reduction
to M-MW for the Borda misrepresentation function. For this problem the computational
complexity is left open.
1.5 Organization of the Paper
The paper is organized as follows. In Section 2, we introduce the main concepts of parameterized complexity and some graph problems. Section 3 contains basic observations about
the relations of the four problems under consideration and fixed-parameter tractability results with respect to the number of voters and the number of candidates. The two main
contributions are proved in Section 4 and Section 5. In Section 4, we present our main parameterized complexity results as well as the NP-hardness results for the minimax versions.
In Section 5, the special case of single-peaked elections is handled. Finally, in Section 6 we
conclude with a discussion of the relevance of our results and some related problems and
settings.

2. Preliminaries
We briefly introduce the framework of parameterized complexity followed by some basic
graph problems that are employed in this paper. For basic notions regarding classical
complexity theory we refer to Garey and Johnson (1979).
2.1 Parameterized Complexity
The concept of parameterized complexity was pioneered by Downey and Fellows (1999). See
also the textbooks by Flum and Grohe (2006) and Niedermeier (2006). The fundamental
goal is to find out whether the seemingly unavoidable combinatorial explosion, occurring in
exact algorithms for NP-hard problems, can be confined to certain problem-specific parameters. The idea is that when such a parameter in a real-life application is restricted to small
values only, then an algorithm with a running time that is exponential exclusively with
respect to the parameter may be efficient and practical. We now provide formal definitions.
Definition 2. A parameterized problem is a language L     , where  is a finite
alphabet. The second component is called the parameter of the problem.
Basically, this means that an input to a parameterized problem is a pair (x, p), where x
can be considered as the main input and p is a parameter of the problem. We consider
parameters which are positive integers or composite parameters that are tuples of several
positive integers.
484

fiOn the Computation of Fully Proportional Representation

Definition 3. A parameterized problem L is fixed-parameter tractable if there is an algorithm that decides in f (p)  |x|O(1) time whether (x, p)  L, where f is an arbitrary
computable function that depends only on p. The complexity class of all fixed-parameter
tractable problems is called FPT.
Unfortunately, not all parameterized problems are fixed-parameter tractable. To this
end, Downey and Fellows (1999) developed a theory of parameterized intractability by
means of a completeness program with complexity classes. More specifically, they defined
the so-called W -hierarchy by using Boolean circuits. This hierarchy consists of the following
classes:
FPT  W[1]  W[2]  . . .  W[Sat]  W[P]  XP
(we refer the reader to the book by Downey & Fellows, 1999, for their precise description).
In particular, we stress that the concept of fixed-parameter tractability is different from
the notion of polynomial-time solvability for constant p since an algorithm running in
O(|x|p ) time does not imply fixed-parameter tractability. All problems that can be solved
in the running time O(|x|f (p) ) for a computable function f form the complexity class called
XP.
The containment W[1]  FPT would not imply P = NP as such. It would imply,
however, the failure of the Exponential Time Hypothesis (Impagliazzo, Paturi, & Zane,
2001). Hence, it is commonly believed that W[1]-hard problems are not fixed-parameter
tractable. To show the W[t]-hardness of a problem for some positive integer t, the following
reduction concept was introduced.
Definition 4. Let L, L     be two parameterized problems. We say that L reduces
to L by a parameterized reduction if there are two computable functions h1 :    and
h2 :  + and a function f :        such that for each (x, p)    

N Q

1. (x, p)  L  f (x, p)  L and f is computable in time |x|O(1)  h2 (|p|) and
2. if (x , p ) = f (x, p), then p = h1 (p).
Analogously to the case of NP-hardness, for any positive integer t, it suffices to give a
parameterized reduction from one W[t]-hard parameterized problem X to a parameterized
problem Y to show the W[t]-hardness of Y . For more details about parameterized complexity theory we refer to the textbooks (Downey & Fellows, 1999; Flum & Grohe, 2006;
Niedermeier, 2006).
In this work, we only provide results regarding the second level of (presumable) parameterized intractability captured by the complexity class W[2]. Several parameterized
reductions in this work are from the W[2]-complete Hitting Set (HS) problem: Given a
family F = {F1 , . . . , Fn } of sets over a universe U = {u1 , . . . , um } and an integer k  0,
decide whether there is a hitting set U   U of size at most k by which we understand a
set U  such that Fi  U  6=  for every 1  i  n. HS is NP-hard (Garey & Johnson, 1979)
and W[2]-hard with respect to parameter k (Downey & Fellows, 1999).
2.2 Graph Problems
Some of our algorithmic results employ algorithms for basic graph problems defined in the
following. An undirected graph is a pair G = (U, E), consisting of the set U of vertices
485

fiBetzler, Slinko, & Uhlmann

and the set E of edges, where an edge is an unordered pair (size-two set) of vertices. Two
vertices u, v  U are called adjacent if {u, v}  E. For an undirected graph G = (U, E) and
a vertex u  U , the neighborhood N (u) of u is the set of all vertices adjacent to u.
An undirected graph G = (U, E) is called bipartite if the vertex set U can be partitioned
into two nonintersecting subsets U1 and U2 such that E  {{u, v} | u  U1 and v  U2 }. A
matching is an edge set E   E such that e  e =  for every two distinct edges e, e  E  .
A maximum matching is a matching with maximum cardinality. In an undirected graph
where each edge {u, v} is associated
with a weight w({u, v}) a maximum-weight matching
P
is a matching E  such that {u,v}E  w({u, v}) is maximal.
A directed graph or a directed network is a pair G = (U, A), consisting of the set U
of vertices and the set A  U  U of directed edges (or arcs) where each directed edge
is an ordered pair of vertices. A flow network is a directed network G = (U, A) with
two distinguished vertices s  U (the source) and t  U (the sink or target) where each
arc (u, v)  A is associated with a nonnegative number c(u, v), called capacity. Roughly
speaking, a flow is a function f that assigns a real value f (u, v) with 0  f (u, v)  c(u, v)
to every arc (u, v)  A and satisfies the constraints that for every vertex v except for the
source and the sink the total flow into v equals the total flow out of v. See the textbook
of Cormen, Leiserson, Rivest, and Stein (2001) for details. A maximum flow is a flow such
that the total flow into the sink is maximal.
In this paper, we make use of the fact that a maximum-weight matching in a bipartite
graph as well as a maximum flow in general graphs can be computed in polynomial time
by standard graph algorithms (e.g., see Cormen et al., 2001).

3. Basic Results and Observations
In this section, we shed light on the combinatorial relations between the problems and
investigate the parameterized complexity of the considered problems with respect to the
parameters number of voters and number of candidates. These results will also be
employed in the following sections. In particular, some of the algorithms showing fixedparameter tractability will be used as subroutines in Section 4 to obtain fixed-parameter
tractability with respect to other parameterizations.
3.1 Relations between the Problems
Although all four problems come with different properties in general, in some special cases,
some of them coincide. One such example is the so-called fully personalizable setting (Lu
& Boutilier, 2011), that is, the case when the misrepresentation bound R is zero and hence
every voter has to be represented by one of her best alternatives (i.e., one of those for which
the misrepresentation is zero). Clearly, asking for a set of winners and an assignment for
which the sum of misrepresentations is zero is equivalent to asking for a set of winners and
an assignment for which the maximum misrepresentation value is zero. This leads to the
following observation.
Observation 1. For R = 0, Minimax M-Multiwinner coincides with M-Multiwinner
and Minimax CC-Multiwinner coincides with CC-Multiwinner.
486

fiOn the Computation of Fully Proportional Representation

Moreover, for the two minimax versions of the problems, it only matters whether a
particular misrepresentation value exceeds the threshold R or not. Hence, an instance of
a minimax version with an arbitrary misrepresentation function r can be reduced to an
equivalent instance of the same problem with the approval misrepresentation function r 
as follows. For every voter v and every candidate c, set r  (v, c) = 1 if r(v, c) > R, and
r  (v, c) = 0 if r(v, c)  R and, finally, set R := 0.
Observation 2. For a Minimax M-/CC-Multiwinner instance (C, V, r, R, k) with misrepresentation function r, there is an instance (C, V, r  , 0, k) with the approval misrepresentation function r  such that the new instance is a yes-instance if and only if the original
instance is a yes-instance.
As a direct consequence, for the minimax versions every algorithm for the approval misrepresentation function also applies to instances with general misrepresentation function.
Moreover, hardness results for an arbitrary misrepresentation function transfer to the approval misrepresentation function. Combining Observations 1 and 2, we conclude that an
algorithm for M-MW (CC-MW) for instances with R = 0 also solves the corresponding
minimax version for general misrepresentation function.
Finally, observe that a hardness result established for the approval misrepresentation
function can be directly transferred to the minimax version of the same problem if the
misrepresentation function is such that a voter is allowed to give an arbitrary number of
candidates a misrepresentation value of at most R. Note that this does not hold for the
Borda misrepresentation function where every voter v must specify exactly R+1 candidates
that can represent v with misrepresentation at most R.
3.2 The Numbers of Voters and Candidates as Parameters
We argue that all four problems considered are fixed-parameter tractable with respect to
the number of candidates as well as with respect to the number of voters. Our algorithms
are based on brute-force search combined with maximum flow and matching techniques.
First, we consider the parameterization by the number of voters. Then, we focus on the
parameterization by the number of candidates.
3.2.1 The Number of Voters as Parameter
We show that all considered multiwinner problems are fixed-parameter tractable when parameterized by the number n of voters. The basic idea is that an assignment of candidates
to voters induces a partition of the set of voters such that all voters in any set of this
partition are represented by the same candidate. Given a partition of the set of voters, the
best set of candidates for this partition can be found by the computation of a matching in
a bipartite auxiliary graph. Since we may try all O(kn )  O(nn ) partitions of the set of
voters into k sets the resulting algorithm shows fixed-parameter tractability.
Proposition 1. (Minimax) CC-Multiwinner and (Minimax) M-Multiwinner can
be solved in nn  poly(n, m) time for an instance with n voters and m candidates.
Proof. First, we present a solution strategy for Minimax CC-MW. To find a set of k
winners, try all O(kn ) partitions of the set of voters into k subsets. In the case of a yesinstance of Minimax CC-MW, there must be a partition V1 , . . . , Vk of the multiset of voters
487

fiBetzler, Slinko, & Uhlmann

V as follows. For every Vi , all voters of Vi are assigned to the same candidate c of an optimal
set of k winners and no other voter is assigned to c. Hence, for every partition, it remains to
select k candidates, one candidate ci for every subset Vi , such that by assigning the voters
in Vi to ci the misrepresentation of any voter is at most R. For Minimax CC-MW the
set of candidates can be determined by computing a maximum-cardinality matching in the
following bipartite graph. One part of the graph represents the set of candidates and the
other part the set {V1 , . . . , Vk }. Moreover, there is an edge between a vertex representing a
candidate c and a vertex representing a subset Vi if and only if r(v, c)  R for all v  Vi . It is
straightforward to verify that all voters can be represented with maximal misrepresentation
bound R if and only if there is a maximum-cardinality matching of size k (all vertices
representing the subsets are matched) in the constructed graph.
Regarding the running time, the computation of a maximum-weight matching in a
bipartite graph with nv vertices and ne edges can be accomplished in O(nv (ne + nv  log nv ))
time (Fredman & Tarjan, 1987). Since the number of edges and vertices in the constructed
bipartite graph are polynomial in the number of candidates and k  n, the claimed running
time follows.
Next, we focus on CC-MW. Again, we try all partitions of the voters into k subsets.
For every such partition, we compute a maximum-weight matching in the following edgeweighted bipartite graph. One part consists of vertices corresponding to candidates and
the other part of vertices corresponding to the subsets of the partition V1 , . . . , Vk of the
multiset of voters.
PMoreover, there is an edge between every vertex c and every vertex Vi
with weight T  vVi r(v, c), where T is a positive integer that is large enough to ensure
that all weights are positive. The crucial observation is that in a maximum-weight matching
every vertex from V1 , . . . , Vk is matched since the edge weights are positive (here we assume
that k  m). Hence, the computation of a maximum-weight matching yields a set of k
candidates representing the subsets of voters as good as possible. More specifically,
let W denote the weight of the maximum-weight matching. Then, kT  W is the total
misrepresentation under the corresponding assignment.
Finally, observe that for the two problems where the assignment of the voters to the
winners must fulfill the M -criterion we can proceed in the same way with the single exception
that we need only to consider partitions such that every subset contains at least n/k and
at most n/k voters. The running time bound follows in complete analogy to Minimax
CC-MW as discussed above.
3.2.2 The Number of Candidates as Parameter
For a fixed number of candidates all four considered multiwinner problems can be solved
efficiently. For (Minimax) CC-Multiwinner parameterizedby the number m of candim
dates fixed-parameter tractability is trivial: We can test all m
k  2 subsets of candidates
and report a set of candidates with minimum total misrepresentation. To this end, one
assigns every voter v to the candidate of the considered subset that represents v in the
best possible way and then directly obtains the sum of misrepresentations or maximum
misrepresentation.
Clearly, such an assignment of the voters does not have to fulfill the M -criterion. However, for (Minimax) M-Multiwinner, one can apply network flow algorithms to find an
488

fiOn the Computation of Fully Proportional Representation

optimal assignment of the voters to a size-k subset C  of the set of candidates (see the
Preliminaries in Subsection 2.2 for basic definitions regarding network flows).
For Minimax M-Multiwinner, construct a directed network with a vertex for every
candidate from C  , one vertex for every voter, a source, and a sink vertex. There is an
arc with capacity n/k and lower bound of n/k from the source to every candidatevertex3 . Moreover, there is a capacity-one arc from a candidate-vertex to a votervertex if and only if the corresponding candidate can represent the corresponding voter
with misrepresentation at most R. Finally, there is an arc with capacity one from every
voter-vertex to the sink vertex. It is straightforward to verify that there is a network
flow of size n if and only if there is an assignment from the voters to C  that satisfies the
M -criterion and every voter is represented with misrepresentation at most R.
For M-Multiwinner, the construction given for the minimax version can be further
extended. In particular, it follows from Theorem 2 by Procaccia et al. (2008), that finding
an M -criterion fulfilling assignment from V to C  with minimum total misrepresentation
can be accomplished in polynomial time by the computation of a transportation problem
or, equivalently, by the computation of a minimum-weight maximum flow.
Proposition 2. (Minimax) CC-Multiwinner and (Minimax) M-Multiwinner can
be solved in O(2m  nm) and O(2m  poly(n, m)) time, respectively, for instances with m
candidates.

4. The Number of Winners and the Misrepresentation Bound as
Parameters
In this section, we show that all four problems for the approval and the Borda misrepresentation functions are W[2]-hard with respect to the number of winners. For both misrepresentation functions we provide one parameterized reduction that works for all four
problems. We further investigate the misrepresentation bound R as parameter. While for
the approval misrepresentation function NP-hardness for R = 0 follows directly from the
parameterized reduction with respect to the number of winners, for the Borda misrepresentation function, this parameter needs to be investigated separately. We show that CC-MW
and M-MW are in XP with respect to R, that is, they are solvable in polynomial time
when R is constant. However, the corresponding algorithm does not show fixed-parameter
tractability with respect to R. The question whether or not our result can be extended
to fixed-parameter tractability with respect to R is left open. We present, however, some
fixed-parameter tractability results with respect to the composite parameter (R, k) at the
end of this section. An overview of the results can be found in Table 1.
4.1 The Approval Misrepresentation Function
We provide a reduction from the W[2]-complete Hitting Set problem to establish W[2]hardness for all four problems. Before doing so, we discuss some related results. In the conference paper (Procaccia, Rosenschein, & Zohar, 2007) it was stated that the NP-hardness
for CC-Multiwinner and M-Multiwinner follows from a reduction from Max k-Cover
3. The problem variant with lower bounds (demands) can be solved in polynomial time by a simple reduction
to the normal flow problem (Ahuja, Magnanti, & Orlin, 1993, Section 6.7).

489

fiBetzler, Slinko, & Uhlmann

(omitting the problem definition and the construction) but in the subsequent journal paper (Procaccia et al., 2008) the reduction was given from Exact 3-Cover. Although this
is sufficient to show NP-hardness, a reduction from Exact 3-Cover does not imply W[2]hardness. The reduction given here is conceptually similar but requires some additional
voters to deal with the fact that the sets of a Hitting Set instance might come with
different/unbounded size.
Theorem 1. For the approval misrepresentation function, (Minimax) CC-Multiwinner
and (Minimax) M-Multiwinner are W[2]-hard with respect to the number k of winners
even if R = 0. Minimax CC-Multiwinner and Minimax M-Multiwinner are NPcomplete.
Proof. First, we show W[2]-hardness for M-Multiwinner. Then, we argue that the presented reduction works for the other three problems as well.
Given an instance of Hitting Set (F = {F1 , . . . , Fn }, U = {u1 , . . . , um }, k), build an
instance of M-Multiwinner with set C of candidates as follows. There is a candidate ci 
C for every element ui  U . The multiset of voters is VF  D, where VF := {vF | F  F}
is the multiset of voters indexed by F and |D| = n(k  1) is a set of dummy voters.
Furthermore, for every F  F and every ui  U , let r(vF , ci ) := 0 if ui  F and r(vF , ci ) :=
1, otherwise. Finally, for every d  D and every ui  U , set r(d, ci ) := 0. This completes
the construction. For the correctness we show the following.
Claim. There is a hitting set of size k for F if and only if there is a winner set
of size k for M-Multiwinner that represents all voters with total misrepresentation R = 0.
: Let U  denote a size-k hitting set for F and C  := {ci | ui  U  }. We show that one can
build a mapping w : V  C  that respects the M -criterion and with total misrepresentation
zero. First, for every F  F, set w(vF ) := ci for an arbitrary chosen element ui  F  U  .
Clearly, r(vF , ci ) = 0. So far, the n voters from VF are assigned to the candidates from C 
and it remains to assign the n(k1) voters from D. Since each candidate in C  can represent
each dummy voter in D with misrepresentation zero, we can easily extend this assignment
so that each candidate from C  is assigned to exactly n voters.
:PLet C   C denote a size-k winner set and let w be a mapping from V to C  such
that vV r(v, w(v)) = 0. Since a voter vF  VF can only be represented with cost zero by
a candidate ci if ui  F , the set U  := {ui | ci  C  } is a size-k hitting set for F.
This completes the proof for M-Multiwinner. It is straightforward to verify that
the same construction yields a parameterized reduction for CC-Multiwinner. Finally,
the W[2]-hardness for the minimax versions follows directly by Observation 1 since the
reduction works for R = 0. Moreover, NP-hardness directly follows since the reduction can
clearly be carried out in polynomial time and containment in NP is obvious.
4.2 The Borda Misrepresentation Function
We refine the reduction from the previous subsection to show that also for the Borda misrepresentation function the considered problems are W[2]-hard with respect to the number k of
490

fiOn the Computation of Fully Proportional Representation

winners as the parameter. However, in contrast to the case of the approval misrepresentation function the reduction does not hold for the case that R = 0. Hence, we investigate the
parameter total misrepresentation R as well as the composite parameter (R, k) subsequently.
4.2.1 The Number of Winners as Parameter
For the Borda misrepresentation function, we also provide a many-one reduction from
Hitting Set for M-Multiwinner and then argue that the presented reduction works
for the other three problems as well. For CC-Multiwinner W[2]-hardness also directly
follows from the NP-hardness reduction (also from Hitting Set) provided by Lu and
Boutilier (2011, Thm. 8).4 Our reduction, however, can deal with the M-criterion and can
be adopted to the minimax versions of the two rules; in particular, using some padding of
candidates and voters to deal with the M-criterion.
Theorem 2. (Minimax) CC-Multiwinner and (Minimax) M-Multiwinner are W[2]hard with respect to the number k of winners for the Borda misrepresentation function.
Minimax CC-Multiwinner and Minimax M-Multiwinner are NP-complete.
Proof. First, we show W[2]-hardness for M-Multiwinner by a parameterized reduction
from Hitting Set. Given an HS-instance (F = {F1 , . . . , Fn }, U = {u1 , . . . , um }, k) build an
instance of M-Multiwinner as follows. Let z := nmk. The set C of candidates is CU  B,
where CU := {cu | u  U } and B := {b1i , . . . , bzi | 1  i  nk}. Moreover, the multiset of
voters is VF  D, where VF := {vi | Fi  F} and D := {d1 , . . . , dn(k1) }. For each voter his
misrepresentation function is given by his preference list.5
Each of the n set voters vi  VF has the following preference list:
{cu | u  Fi } > b1i > . . . > bzi > {cu | u  U \ Fi } > {b1j , . . . , bzj | 1  j  nk, j 6= i}.
Finally, for each i  {1, . . . , n(k  1)}, the voter di from D has the following preference
list:
c1 > c2 > . . . > cm > b1n+i > . . . > bzn+i > {b1j , . . . , bzj | 1  j  nk, j 6= n + i}.
This completes the construction. For the correctness we show the following.
Claim. There is a size-k hitting set for F if and only if there is a size-k winner
set for M-Multiwinner that represents all voters with total misrepresentation
at most z = nmk.
: Let U  denote a size-k hitting set for F and C  := {cu | u  U  }. We build a
mapping w : V  C  as follows. First, for every Fi  F, set w(vi ) := cu for an arbitrarily
chosen element u  Fi U  . Clearly, r(vi , cu )  m since the elements in Fi top the preference
list of vi and |Fi |  m. So far, the n voters from VF are assigned to the candidates from C  .
4. The proof is provided in the extended version that appeared at the Third International Workshop on
Computational Social Choice (COMSOC-10) under the title Budgeted Social Choice: A Framework for
Multiple Recommendations in Consensus Decision Making, see Thm. 6.
5. To improve the readability, we also use sets of candidates in the description of preference lists. Such a
set can be fixed in an arbitrary order.

491

fiBetzler, Slinko, & Uhlmann

Since each candidate in C  can represent each dummy voter in D with misrepresentation at
most m, one can extend the mapping so that exactly n voters are assigned to each cu  C 
with misrepresentation at most m for each voter. Thus, the total misrepresentation of this
assignment is at most nm + nm(k  1) = nmk.
:PLet C   C denote a size-k winner set and w be a mapping from V to C  such
that vV r(v, w(v))  mnk. First, we show that C  can contain no candidate bji from B.
Every candidate from B can represent at most one voter with misrepresentation value better
than z. More specifically, if 1  i  n, then bji can represent only the voter vi with this
quality of representation and if, n < i  nk, then bji can present only the voter di . Since
every candidate from C  must be assigned to exactly n voters and the misrepresentation
bound is z, we conclude that C   B = .
It remains to show that U  := {u  U | cu  C  } is a hitting set for F. A voter vi 
VF can be represented by a candidate cu  CU with misrepresentation at most z only
if u  Fi since all candidates cu with u  U \ Fi occur in the preference list of vi after the
candidates b1i , . . . , bzi . Thus, U  is a hitting set for F of size k.
This completes the proof for M-Multiwinner. The same construction yields a parameterized reduction for CC-Multiwinner based on the same claim. The direction from
left to right follows in complete analogy. For the other direction, the only difference is
that here a solution set C  of the CC-Multiwinner instance might contain a candidate
from B. However, if there is such a candidate bji , then it can represent at most one voter
(that is, vi ) within the required misrepresentation bound and hence can be replaced by a
candidate cu  Fi that represents the corresponding voter even better.
For the proof of Minimax M-Multiwinner and Minimax CC-Multiwinner, it follows directly from the arguments above that there is a size-k hitting set for F if and only
if there is a set of winners for Minimax M/CC-Multiwinner consisting of k candidates
that represent all voters with maximum misrepresentation at most R := m  1. Hence,
W[2]-hardness as well as NP-hardness follow.
4.2.2 Parameter Misrepresentation Bound
Recall that for the approval misrepresentation function, all four problems are NP-hard even
in the fully personalized setting, that is, when R = 0. In contrast, for CC-MW and MMW for the Borda misrepresentation function, we provide polynomial-time algorithms for
every constant R while showing that the minimax versions are NP-hard for R  1 and
polynomial-time solvable for R = 0. First, by a simple exhaustive search strategy, one
obtains the following.
Theorem 3. For the Borda misrepresentation function, CC-Multiwinner and M-Multiwinner are solvable in polynomial time when the misrepresentation bound R is constant.
Proof. In every solution, at most R voters can be represented with misrepresentation greater
than 0. Thus, for constant values of R, one can try all O(|V |R ) subsets of at most R voters
to find a subset V   V of voters that are not represented with misrepresentation value
zero by an optimal winner set. For each such subset V  , for each voter of v  V  , one
further tries all possible misrepresentation values from 1 to R, that is, one tries O(RR )
possibilities for each V  . For each such possibility, the misrepresentation value of each
492

fiOn the Computation of Fully Proportional Representation

voter is determined. Since for Borda there is exactly one candidate that can represent
a voter with a specific value, this also implies a corresponding mapping of V  to a set
of candidates. Every remaining voter is assigned to the candidate which represents him
with misrepresentation value zero. In the case of CC-Multiwinner, it remains to check
whether at most k candidates have become representatives and whether the corresponding
set of candidates can represent all voters with total misrepresentation at most R. In the case
of M-Multiwinner one additionally needs to check whether the corresponding assignment
satisfies the M -criterion. It follows that in both cases an optimal set of k winners can be
computed in O((|V |  R)R  |V ||C|) time.
Note that Theorem 3 does not imply fixed-parameter tractability with respect to R,
which remains open in this work. However, we provide fixed-parameter tractability results
with respect to the composite parameter (R, k) at the end of this section. Now, we contrast
the results for CC-MW and M-MW by showing that the minimax versions become provably
more difficult. More specifically, we show the following.
Theorem 4. For the Borda misrepresentation function, minimax CC-Multiwinner and
minimax M-Multiwinner are solvable in polynomial time if the total misrepresentation
bound R = 0 and are NP-hard for every R  1.
Proof. For R = 0 polynomial-time solvability follows directly from the fact that every voter v
must be assigned to a candidate c with r(v, c) = 0 and for the Borda misrepresentation
function there is only one such candidate. Hence, one only needs to check if there are less
than k such candidates and, for minimax M-Multiwinner whether the corresponding
assignment satisfies the M-criterion.
Now, we show NP-hardness for R = 1 by a reduction from a special case of Hitting
Set. More specifically, Hitting Set is NP-hard even if every set consists of two elements
and every element appears in at most three sets (Garey, Johnson, & Stockmeyer, 1974,
Thm. 2.4).6
Given such a restricted HS-instance (F = {F1 , . . . , Fn }, U = {u1 , . . . , um }, k), build
an election as follows. Identify every set from F with a voter and identify every element
from U with a candidate. Moreover, define the following misrepresentation function. For
each F = {u, v}  F, let the misrepresentation of voter F be zero for the candidate u and
one for the candidate v, and the remaining misrepresentation values are assigned arbitrarily
to the remaining candidates. Then, the following claim is easy to see.
Claim: There is a hitting set of size k if and only if there is a set of k winners
such that the misrepresentation for each voter is at most 1.
This shows the theorem for Minimax CC-MW and R = 1. For Minimax M-MW,
one can use the following observation showing NP-hardness for an even more restricted
setting. It follows directly from the Hitting Set instances constructed in the NP-hardness
proof (Garey et al., 1974, Thm. 2.4) that for a yes-instance there is always a hitting set
such that every element hits either two or three sets from F. More specifically, in case
of a yes-instance there is a hitting set U   U such that every u  U  can be assigned
6. The problem is Vertex Cover on cubic graphs.

493

fiBetzler, Slinko, & Uhlmann

1
2
3
4
5
6
7
8
9
10
11
12
13

Branch (V  , R , C  ) :
if R < 0 or |C  | > k then
return no;
P
if wV  (mindC  r(w, d))  R then
return yes;
Consider an arbitrary v  V  ;
V  := V  \ {v};
for each c  C with r(v, c)  R do
R := R  r(v, c) ;
C  := C   {c};
V  := V  \ {w  V  | r(w, c) = 0};
if Branch (V  , R , C  ) then
return yes ;

14
15
16

end
return no;

Algorithm 1: Branching strategy for CC-Multiwinner for Borda misrepresentation
functions showing fixed-parameter tractability with respect to the composite parameter (R, k). Initially, the algorithm is invoked with the arguments (V, R, ). Moreover, C
and k are provided as global variables.
either to two or three sets from F and every set is hit by exactly one element. Such a
hitting set then one-to-one-corresponds to a winner set fulfilling the M-criterion and hence
the theorem also follows for Minimax M-MW and R = 1. For every R > 1, NP-hardness
can be proved by similar arguments. Basically, extend the previous construction as follows.
For every voter, add R  1 new candidates that are placed at the first R  1 positions for
this voter and at a position higher than R for every other voter. Since these new candidates
clearly cannot be part of any Minimax M-MW solution with misrepresentation bound R,
one can argue analogously for this case.
4.2.3 Composite Parameter Number of Winners and Misrepresentation
Bound
In this paragraph, we focus on the scenario that one has a small set of winners that can
represent all the voters with small total misrepresentation. This is modeled by the composite
parameter (R, k), where k is the number of winners and R is the total misrepresentation.
We show that for the Borda misrepresentation function, all four considered problems are
fixed-parameter tractable.
Theorem 5. For the Borda misrepresentation function, CC-Multiwinner and Minimax CC-Multiwinner are fixed-parameter tractable with respect to the composite parameter (R, k), where k denotes the number of winners and R the misrepresentation bound.
Proof. First, we provide a branching strategy for Minimax CC-MW. To find a size-k
solution we proceed as follows. For an arbitrary voter v  V , branch according to all
494

fiOn the Computation of Fully Proportional Representation

candidates c with r(v, c)  R. For each possibility, create a subinstance by deleting each
voter w with r(w, c)  R from V and recursively solve the corresponding subinstance
for k  1. Finally, report whether for at least one subinstance a solution of size k  1
has been found. The recursion stops either if k < 0 (reporting no) or if all voters are
represented (reporting yes).
The correctness of the corresponding algorithm is obvious since a voter v must be represented by a candidate c with r(v, c)  R. Regarding the running time, one branches
into R + 1 possibilities for every considered voter and decreases the value of k by one for
every subinstance. Hence, the algorithm investigates at most (R + 1)k possibilities.
We show how to extend this branching strategy to work for CC-MW. The branching
recursion is displayed in Algorithm 1 and is invoked with the arguments (V, R, ). Note
that C and k are provided as global variables.
The correctness of Algorithm 1 can be seen as follows. The algorithm first checks
whether the misrepresentation bound or the solution size are exceeded (Line 2). Second,
the algorithm checks whether the current candidate set is already a winner set, that is,
whether all voters are represented and the assignment is within the misrepresentation bound
(Line 4). Otherwise, for an arbitrarily chosen voter v (Line 6), the algorithm tries all
possible ways of representation without exceeding the misrepresentation bound (Line 8).
For each possibility, it decreases R by the value needed for the representation of v by the
corresponding candidate (Line 9). If this possibility implies that a new candidate is added
to the current solution, then we can clearly assign all voters that are optimally represented
by this candidate to it and hence delete the corresponding voters (Line 11). Finally, we
recursively invoke the Branch procedure for the corresponding subinstance (Line 12). Since
all possibilities to represent v are considered at least one possibility must lead to a solution
(if one exists).
Regarding the running time, we show that before each recursive call (Line 12) the
algorithm decreases R or increases |C  | (or both). In the initial call one has C  =  and
hence |C  | is increased by one (Line 10). For every further call, the only case in which |C  |
is not increased is that the considered candidate c is already in the current solution set C  .
In this case, one cannot have r(v, c) = 0 since then v would have been deleted from V  at
the point when c has been added to C  . Hence, r(v, c) > 0 and R is decreased (Line 9).
Since the recursion ends when R < 0 or |C  | > k (Line 2), it follows that the recursion
depth is at most R + k. Moreover, in each recursive call, one branches according to R + 1
possible candidates (Line 8). Hence, the algorithm can be executed in (R+1)R+k poly(n, m)
time.
We remark that the results from Theorem 5 also hold for any instance with misrepresentation functions with nonnegative integer values and |{c  C | r(v, c)  R}|  R + 1 for
every voter v  V . Moreover, fixed-parameter tractability already follows when |{c  C |
r(v, c)  R}|  f (k, R) for any computable function f . In contrast, the branching strategy
for CC-MW from the Theorem 5 cannot be directly transferred to M-MW since due to the
M-criterion one cannot assign a voter to a selected candidate even if it is her best alternative. This means that in an analogous approach the parameter could not be reduced
and, hence the size of the search tree could not be bounded. To show fixed-parameter
495

fiBetzler, Slinko, & Uhlmann

tractability for M-MW we apply a different approach that employ structural observations
based on the M-criterion.
Consider an instance (C, V, r, R, k) of M-MW. Let a zero-candidate be a candidate c  C
with r(v, c) = 0 for at least one voter v  V .
Lemma 1. In a yes-instance of M-Multiwinner with the Borda misrepresentation function, there can be at most R + k zero-candidates.
Proof. We apply a proof by contradiction. Assume that there are more than R + k zerocandidates and there is a size-k winner set representing all voters with total misrepresentation at most R. For the Borda misrepresentation function, for every voter v there is exactly
one candidate c with r(v, c) = 0. If c is not part of the winner set, then v contributes by at
least one to the total misrepresentation since r(v, c )  1 for every c  C \ {c}. Since there
are more than R + k zero-candidates, more than R of them are not part of a size-k solution.
For each of them there is a corresponding voter which is represented by an other candidate
of the solution with misrepresentation at least one. Hence, the total misrepresentation is
more than R; a contradiction.
To make use of the bounded number of zero-candidates, we provide another observation
that exploits the M-criterion of a solution.
Lemma 2. Consider an M-Multiwinner instance with the Borda misrepresentation function. If the number n of voters is greater than (R + 1)k, then every size-k set of winners
consists of zero-candidates.
Proof. Assume on the contrary that there are more than (R + 1)k voters and a candidate c
in the solution set does not represent any of the voters with misrepresentation value zero.
Due to the M-criterion and since there are more than (R + 1)k voters, c must represent at
least ((R + 1)k)/k = R + 1 voters with misrepresentation value at least one, respectively.
Since the bound for the total misrepresentation is R, c cannot be part of a solution.
Based on the two previous lemmas, we show the following.
Theorem 6. For the Borda misrepresentation function, the M-Multiwinner problem is
fixed-parameter tractable with respect to the composite parameter (R, k) where k denotes the
number of winners and R the misrepresentation bound.
Proof. An algorithm can be described by distinguishing two cases: n  (R + 1)k and
n > (R + 1)k. If the former, then fixed-parameter tractability follows from Proposition 1 (showing fixed-parameter tractability w.r.t. the number of voters). If the latter,
in a yes-instance, by Lemma 1 there are at most R + k zero-candidates and by Lemma 2
the solution has to consist of zero-candidates. After removing all but the zero-candidates,
fixed-parameter tractability follows from Proposition 2 (showing fixed-parameter tractability w.r.t. the number of candidates).
Regarding the running time, the first case leads to a running time of ((R + 1)k)(R+1)k 
poly(n, m) while the second case can be accomplished in 2R+k  poly(n, m) time. Hence, the
theorem follows.
496

fiOn the Computation of Fully Proportional Representation

Finally, we show fixed-parameter tractability with respect to (R, k) for Minimax MMW with the Borda misrepresentation function. The corresponding algorithm is based on
the same case distinction as the algorithm for M-MW (Theorem 7). While the basic idea of
bounding the number of zero-candidates cannot be transferred from M-MW to Minimax
M-MW, the following algorithm for Minimax M-MW works also for M-MW but leads to
a worse running time bound for the case that the number of voters exceeds (R + 1)k. More
specifically, for the case n > (R + 1)k, the exponential running time part is 4(R+1)k instead
of 2R+k .
Theorem 7. For the Borda misrepresentation function, Minimax M-Multiwinner is
fixed-parameter tractable with respect to (R, k).
Proof. Consider a Minimax M-MW instance (C, V, r, R, k) with r being a Borda misrepresentation function. Because the case R = 0 is trivial for Minimax M-MW with the Borda
misrepresentation function, we assume that R  1 in the following.
In the case that n  (R + 1)k, fixed-parameter tractability follows from Proposition 1
(analogously to the proof of Theorem 6). Hence, we consider the case that n > (R + 1)k.
Let C := {c1 , . . . , cm } and Ei := {v  V | r(v, ci )  R} for every ci  C. Moreover,
let C  := {ci  C : |Ei |  n/k}. We show that, first, every solution has to consist
of candidates from C  and, second, |C  |  2(R + 1)k. Then, after removing all but the
candidates in C  , fixed-parameter tractability follows from Proposition 2.
First, due to the M-criterion at least n/k voters are assigned to every winning candidate in a solution and hence a candidate ci with |Ei | < n/k cannot be part of a winner
set.
Second, assume toward a contradiction that |C  | > 2(R + 1)k. Note that for Borda
misrepresentation functions every voter occurs in at most R + 1 sets from E1 , . . . , Em .
Moreover, since |Ei |  n/k for every ci  C 
n > 2(R + 1)k  n/k  1/(R + 1) > 2k(n/k  1) = 2n  2k.
Since in the considered case n > (R + 1)k  2k, this is a contradiction.
Finally, based on Proposition 2, for the case that n > (R + 1)k, one obtains a running
time bound of 4(R+1)k  poly(n, m).

5. Single-Peaked Elections
As discussed in the introduction (Subsection 1.4), single-peakedness is a central notion in
political science reflecting elections where a single issue dominates preferences of all voters.
Let us now formally define this property.
Definition 5. Let V be a profile over a set of candidates C, and let  be a linear order over
C (the societal axis). We say that an order v  V is compatible with  if for all c, d, e  C
such that either c  d  e or e  d  c it holds that
posv (c) < posv (d) = posv (d) < posv (e).

(1)

(We remind the reader that positions are counted from the top so that, if a is higher in a
linear order than b, then the position of a is lower.) We say that V is single-peaked with
497

fiBetzler, Slinko, & Uhlmann

3

+

2
1



0


c1





3



+



2



+

c2

c3



1

+

0

c4

+
+
+



+





c1

c2

c3



c4

Figure 1: An election consists of three voters with the following preferences: c1 > c2 >
c3 > c4 , c2 > c3 > c4 > c1 , and c3 > c2 > c1 > c4 . Its single-peakedness is
witnessed by the societal order c1  c2  c3  c4 . The diagram on the left-hand
side shows, for every voter, the Borda score that each alternative gets from this
voter marked by the solid line, dashed line and dotted line, respectively. Note
that every preference order has one local maximum. If the voters express their
Borda misrepresentations values instead, then one obtains the diagram on the
right. Here, the misrepresentation function for an arbitrary fixed voter has one
local minimum.

respect to  if for each i = 1, . . . , n the order vi is compatible with . A profile V is called
single-peaked if there exists a linear order  over C such that V is single-peaked with respect
to ; we will say that  witnesses the single-peakedness of V and refer to  as societal
order.
Proposition 3. Let V be a single-peaked profile over a set of candidates C witnessed by
the societal order . Let r be a misrepresentation function for V . Then for every triple
{ci , cj , ck }  C with ci  cj  ck or ck  cj  ci and for every v  V
r(v, ci ) < r(v, cj ) = r(v, cj )  r(v, ck ).
Proof. By the definition of misrepresentation function (see Definition 1) r(v, ci ) < r(v, cj )
implies posv (ci ) < posv (cj ). Now the result follows by (1) and again Definition 1.
In this section, we investigate the computational complexity of determining proportional
representations using Chamberlin and Courant and Monroe methods together with their
variants, when the input profile is single-peaked. As discussed in the introduction, when
the input profile is single-peaked all voters can be viewed as located on a certain axis where
their location is their bliss point. Their most preferred candidate will be either the one
closest on the right or the one closest on the left. Without loss of generality we may assume
that for each voter her bliss point is the location of one of the candidates (who is then most
preferred by her).
498

fiOn the Computation of Fully Proportional Representation

It is clear that the misrepresentation function r(v, c) for a single-peaked profile must
satisfy the following. If we fix the voter v and change c from one end of the societal axis to
the other the value r(v, c) should decrease monotonically to vs most preferred candidate
at the bliss point and then increase again monotonically for all candidates beyond the bliss
point. That is, for each voter the function expressing the voters misrepresentation by
candidates is single-troughed (that is, has exactly one local minimum) with respect to the
order that witnesses single-peakedness of the profile.
Before describing our results, we briefly outline the typical shapes of some prominent
misrepresentation functions in single-peaked settings. The Borda misrepresentation function is strictly ascending when moving away from the local minimum in both directions.
Moreover, if from the candidate preceding the candidate at the local minimum the misrepresentation function drops by d > 0 points, then, for the next d  1 candidates on the other
side of the local minimum, the misrepresentation function must ascend in size-one steps.
In contrast, for the approval misrepresentation function, there is exactly one interval of
consecutive candidates on the societal axis for whom the misrepresentation is zero while for
all remaining candidates outside the interval the misrepresentation is one. For the minimax
variants one obtains a similar structure, in the sense that there can be only one interval in
which a particular voter can be represented without exceeding the given misrepresentation
bound. Note that there is some remote similarity here between the last two cases and
preferences over intervals in the aggregating range values model introduced by Farfel and
Conitzer (2011).
In the remainder of this section, we provide the following results summarized on Table 2. We show that CC-Multiwinner, Minimax CC-Multiwinner, and Minimax MMultiwinner for single-peaked elections can be solved in polynomial time for an arbitrary
misrepresentation function (Theorem 8, Proposition 4, and Proposition 5, respectively). In
contrast to the three aforementioned problems, we present a reduction from an NP-hard version of the Exact 3-Cover problem which shows that M-Multiwinner is NP-hard even
when restricted to single-peaked profiles (Theorem 11). However, for the approval misrepresentation function, we still obtain polynomial-time solvability for M-Multiwinner and
single-peaked input profiles (Theorem 10). We leave open the computational complexity
for M-Multiwinner for the Borda misrepresentation function in the single-peaked case.
5.1 (Minimax) CC-Multiwinner
We show that on single-peaked input profiles CC-Multiwinner and Minimax CC-Multiwinner are polynomial-time solvable for an arbitrary misrepresentation function. We first
provide a dynamic programming algorithm for the case of CC-Multiwinner. Second, we
argue that Minimax CC-Multiwinner can be solved optimally by a greedy algorithm.
5.1.1 A Dynamic Programming Procedure for CC-Multiwinner
For CC-Multiwinner the polynomial-time solvability is established by presenting a dynamic programming algorithm leading to the following.
Theorem 8. For a single-peaked input profile and an arbitrary misrepresentation function
CC-Multiwinner can be solved in O(nm2 ) time.
499

fiBetzler, Slinko, & Uhlmann

1

Function SinglePeaked-CC-MW(V , C, r, k) Input: A multiset of
voters V := {v1 , . . . , vn }, a set of candidates C := {c1 , . . . , cm }, a
misrepresentation function r, and a positive integer k. The voters have
single-peaked preferences according to the societal order , where
c1  c2  . . .  cm .
Output: The minimum total misrepresentation for k winners.

begin
for i = 1, . . . ,P
m do
4
z(i, 1) := vV r(v, ci );
5
end
6
for p = 1, . . . , m do
7
for i = p + 1,P
. . . , m do
8
d(p, i) := vV max{0, r(v, cp )  r(v, ci )};
9
end
10
end
11
for i = 2, . . . , m do
12
for j = 2, . . . , min(k, i) do
13
z(i, j) := minp{j1,...,i1} (z(p, j  1)  d(p, i));
14
end
15
end
16
return mini{k,...,m} (z(i, k));
17 end
Algorithm 2: Dynamic programming algorithm for CC-Multiwinner for single-peaked
input profiles.
2

3

Proof. Throughout the proof we assume that the voters have single-peaked preferences
according to the societal order , where c1  c2  . . .  cm . For a set C   C, the
minimum total misrepresentation is defined as
s(C  ) =

X

vV

min {r(v, c )}.

c C 

We define a dynamic programming table z, containing an entry z(i, j) for all 1  i  m
and all 1  j  min(i, k). Informally speaking, the entry z(i, j) gives the minimum total
misrepresentation for a set of j winners from {c1 , . . . , ci } including ci .
The dynamic programming procedure SinglePeaked-CC-MW is provided in Algorithm 2.
We show that it solves CC-Multiwinner in the claimed running time. Regarding the
correctness, we will show that after the execution of SinglePeaked-CC-MW the following
equation is satisfied

z(i, j) = min s(C  ) | C   {c1 , . . . , ci }  |C  | = j  ci  C  .

(2)

Then, the minimum total misrepresentation of an optimal size-k winner set is clearly given
by mini{k,...,m} z(i, k) (see Line 16).
500

fiOn the Computation of Fully Proportional Representation

The proof of Equation 2 follows by induction on j. First, we argue that the entries z(i, 1)
satisfy Equation (2), yielding the base for induction. To this end, observe that if there is
only one candidate ci in the winner
P set, then all voters must be assigned to ci , yielding the
misrepresentation sum s({ci }) = vV r(v, ci ), see Line 4.
Next, we show that an entry z(i, j) with j > 1 (as computed in Line 13) complies with
Equation (2) provided that z(p, j 1) does for all 1  p < i. Consider a set C   {c1 , . . . , ci }
with ci  C  and |C  | = j such that s(C  ) is minimum among all such sets. We argue
that z(i, j) = s(C  ). Let p < i such that cp  C  and c 6 C  for all p <  < i.
This implies that p  j  1. The crucial observation is as follows. If for a voter v it
holds that r(v, ci ) < r(v, cp ), then the single-peakedness implies that r(v, cq )  r(v, cp ) >
r(v, ci ) for all q < p. Hence, if we consider a set C  of j  1 candidates from {c1 , . . . , cp }
with cp  C  , then we can assume that the value r(v, cp ) is the contribution of voter v to
the total misrepresentation s(C  ). Hence, by adding ci to C  there is an improvement of
r(v, cp )  r(v, ci ) for each voter v with r(v, cp ) > r(v, ci ). For every remaining voter v, it
holds that r(v, ci )  r(v, cp ) and hence onePcannot improve the representation by assigning
him to ci . It follows that s(C  ) = s(C  )  vV max{0, r(v, cp )  r(v, ci )} = s(C  )  d(p, i)
and by the induction assumption we have z(p, j  1) = s(C  ). Finally, since the algorithm
tries all possible choices of p (see Line 13) we have that z(i, j) = s(C  ).
It is straightforward to verify that the running time of Algorithm 2 is O(nm2 ).
5.1.2 A Greedy Algorithm for Minimax CC-Multiwinner
For single-peaked input profiles, the minimax version of CC-Multiwinner can be solved
by a simple greedy algorithm. The basic idea is to iterate over the candidates according to
the societal order and to put into the solution the first candidate for whom there is a voter
that cannot be represented by the previously selected candidates. The correctness is based
on the observation that for each candidate there is a representation range (or interval) of
consecutive candidates in which the voter must be represented. Thus, we can choose the
latest possible candidate that can represent the voter with the representation range that
ends first since this candidate is at least as good as every previous candidate.
In other words, the basic combinatorial problem is to cover or stab a given set of intervals
(corresponding to the representation ranges of the voters) by k points (corresponding to
the candidates). This stabbing problem in turn corresponds to a clique cover problem in
interval graphs and can be solved in linear time (Golumbic, 1980). In the next section, we
investigate the relationships between the considered voting problems and special (rectangle)
stabbing problems in more detail. Here, we conclude with the following.
Proposition 4. For a single-peaked input profile and an arbitrary misrepresentation function Minimax-CC-Multiwinner can be solved in O(nm) time.
5.2 (Minimax) M-Multiwinner
We focus on the case when the assignment of the candidates to the winner set satisfies the
M-criterion, that is, it is required that each winner represents about the same number of
candidates. This additional constraint makes the winner determination more involved. Indeed, we can show that for an integer-valued misrepresentation function M-Multiwinner
501

fiBetzler, Slinko, & Uhlmann

is NP-hard even if the input profile is single-peaked. On the positive side, we show that MMultiwinner for single-peaked input profiles and the approval misrepresentation function
and Minimax M-Multiwinner for arbitrary misrepresentation functions are polynomialtime solvable. However, the solving strategies (that are also based on dynamic programming) are more intricate than for (Minimax) CC-Multiwinner. For proving polynomialtime solvability we establish a close relationship to the so-called 1-dimensional Rectangle Stabbing (Even et al., 2008). We start with the polynomial-time algorithms followed
by the NP-hardness proof. The computational complexity for M-Multiwinner for the
Borda misrepresentation function for single-peaked input profiles is left open.
5.2.1 M-Multiwinner for Approval and Minimax M-Multiwinner
We use the notation of Even et al. (2008) whenever possible. The input consists of a set U of
horizontal intervals and a set S of vertical lines with capacity c(S)  {0, . . . , |U |} for every
line S  S. Informally, the task is to cover (or stab) all intervals by a minimum number of
vertical lines from S, where each line S covers at most c(S) intervals (a vertical line covers
a horizontal interval iff they intersect). Since a line S  S can cover at most c(S) intervals,
one has to specify which interval is assigned to which line in the solution. Let U (S) denote
the set of intervals from U intersecting with S  S. An assignment is a function A : S  2U ,
where A(S)  U (S).
A set S   S is a cover if there is an assignment A with |A(S)|  c(S)
S
for all S  S  and SS  A(S) = U .
One-Dimensional Rectangle Stabbing with Hard Constraints (Hard1-RS):
Input: A set U of horizontal intervals and a set S of vertical lines with capacities c(S)  {0, . . . , |U |} for every line S  S.
Task: Find a minimum-cardinality cover S   S (and the corresponding assignment).

Now, consider a single-peaked instance of M-Multiwinner with R = 0 in which every
winner represents exactly the same number of voters, that is, n mod k = 0 for n voters
and k winners. In this case, the problem can be reduced to Hard-1-RS as follows. For
every candidate there is a vertical line according to its position on the societal axis. Since
each voter v must be represented by a candidate c with r(v, c) = 0 and all the candidates
with r(v, c) = 0 are ordered consecutively on the societal axis, we can represent each voter
by a horizontal interval reaching from the leftmost candidate c with r(v, c) = 0 to the
rightmost such candidate. Finally, each vertical line is associated with a capacity of n/k,
which is a whole number. Clearly, there is a solution for the M-Multiwinner instance
with R = 0 if and only if there is a size-k cover for the constructed instance of Hard-1-RS.
Note that for the case, when n mod k 6= 0, this transformation cannot be applied since
one does not know whether a candidate line has capacity n/k or n/k in an optimal
solution.
Even et al. (2008) presented a dynamic programming algorithm for Hard-1-RS with
running time O(|U |2  |S|2  (|U | + |S|)). Since the transformation described above can be
easily accomplished in linear time, one directly obtains the following.

502

fiOn the Computation of Fully Proportional Representation

Corollary 1. An instance of M-Multiwinner with a single-peaked profile such that n mod
k = 0, and R = 0 (and an arbitrary misrepresentation function) can be solved in O(n2 m2 (n+
m)) time.
We show that for single-peaked input profiles, M-Multiwinner for the approval misrepresentation function (and arbitrary misrepresentation bound R) can be solved in polynomial time. To this end, we show that these instances can be reduced to a version of
one-dimensional rectangle stabbing where the goal is to stab a maximum number of horizontal intervals with k vertical lines. More specifically, we introduce the following problem
which to the best of our knowledge has not been studied before.
Maximum Balanced One-Dimensional Rectangle Stabbing (Max-Bal1-RS):
Input: A multi-set U = {u1 , . . . , un } of horizontal intervals, a set S = {S1 , . . . , Sm }
of vertical lines, and a positive integer k.
Task: Find a size-k set S   S and an assignment A such that each of the
following statements hold.
S
 | SS  A(S)| is maximum,
 for every S  S  , |A(S)|  n/k, and
 |{S  S  : |A(S)| = n/k}|  n mod k, where n mod k is the remainder
on division of n by k.
The last two restrictions in the problem description can be considered as saying that we
have kc = n mod k lines with capacity n/k and kf = k  kc lines with capacity n/k, not
specifying which line has which capacity, while for Hard-1-RS there is a specific capacity
for every line. The other difference between Max-Bal-1-RS and Hard-1-RS is that in the
latter all intervals must be covered by a minimum number of lines whereas in the former
the goal is to cover a maximum number of intervals with k lines.
We show that the dynamic programming algorithm of Even et al. (2008) for Hard-1-RS
can be adapted to work for Max-Bal-1-RS. To this end, we employ the same decomposition
property (stated in Observation 3 below) but the dynamic programming table and the
algorithm will be different.
We introduce the following notation to state the dynamic programming. For an interval u  U , let l(u) denote the left endpoint of u and r(u) denote the right endpoint of u
(that is, l(u)  r(u)). Let x(S) denote the coordinate of line S  S and S(x) denote the
vertical line associated with a coordinate x. For two integers s and t let [s, t] denote the set
of all integers i with s  i  t.
For ease of presentation, we assume that the input has the following normalized form
that can easily be established. First, we assume that all endpoints of the intervals and the
coordinates of all lines are integers. Second, we assume that {x(S) | S  S} = [1, m]. Third,
we assume that the endpoints of all intervals are from [1, m]. In what follows, we do not
distinguish between a line S  S and its coordinate x(S), that is, we identify the lines in S by
the elements of [1, m] (and vice versa). Finally, we assume that the intervals u1 , u2 , . . . , un
are ordered so that l(u1 )  l(u2 )  . . .  l(un ) (we fix one such ordering).
The algorithm makes use of the fact that there is always an optimal solution that satisfies
the leftmost interval first property defined as follows (Even et al., 2008).
503

fiBetzler, Slinko, & Uhlmann

Definition 6. Let S   S denote a size-k set of lines and let A denote an assignment. We
say that (S  , A) has the leftmost interval first property if the following holds. Let S  S 
and let ui  A(S). For every S   S  with l(ui )  S  < S and for every uj with uj  A(S  ),
either j < i or r(uj ) < S.
Note that the leftmost interval first property is defined with respect to the fixed ordering
of the intervals. Any solution can be transformed into an equivalent one satisfying the
leftmost interval first property by simply swapping the assignments of conflicting interval
pairs, (see, e.g., Even et al., 2008). Hence, there is always an optimal solution satisfying the
leftmost interval first property and one can apply a dynamic programming procedure based
on the following decomposition, analogously to the work of Even et al. (2008, Section 2).
Observation 3. Let (S  , A) be an optimal solution of Max-Bal-1-RS that satisfies the
leftmost interval first property. For any range [x1 , x2 ]  [1, m], let ui  U be the earliest
interval among the intervals covered by a line from [x1 , x2 ] (that is, for any uj covered by
lines in this range we have j > i). If ui is covered by line S  [x1 , x2 ] \ {x1 }, then the right
endpoint of all intervals covered by lines in the range [x1 , S  1] are to the left of S.
Basically, Observation 3 is used in the algorithm in the following way. Consider the
range [x1 , m] and assume that x1 is the leftmost line of the considered solution. Moreover,
assume that ui is the earliest interval that is covered by a line S from [x1 , m]. Then, every
interval uj with j < i will not be covered by the solution, every interval u with  > i
and r(u ) < S can only be covered by lines from [x1 , S  1], and every interval ur with
S  r(ur ) can only be covered by lines from [S, m]. This implies a decomposition of the
instance into two subinstances. The left instance contains the intervals ug with g > i
and r(ug ) < S and the right instance contains the intervals ud with S  r(ud )  m.
Theorem 9. Maximum Balanced One-Dimensional Rectangle Stabbing can be
solved in O(m3 n3 k3 ) time.
Proof. We use the following definitions to state the dynamic programming algorithm. Let
kc := n mod k and kf := k  kc . For ui  U and for any two coordinates x1  x2 such
that r(ui )  [x1 , x2 ], let
U (ui , x1 , x2 ) := {uj  U | j  i  r(uj )  [x1 , x2 ]}.
Note that ui  U (ui , x1 , x2 ) and U = U (u1 , 1, m).
The algorithm maintains a dynamic programming table with an entry
(ui , x1 , x2 , kc , kf , b)  N
defined for every ui  U , for any two coordinates x1  x2 such that r(ui )  [x1 , x2 ]
and x1  ui , for each 0  kc  kc and for each 0  kf  kf with kf + kc  k  1
and |[x1 , x2 ]|  kc + kf + 1, and for each 1  b  n/k. Informally, the table entry contains
the maximum number of intervals from U (ui , x1 , x2 ) that can be covered by kc + kf + 1 lines
from [x1 , x2 ] under the assumption that x1 is contained in the solution and covers at most b
intervals, ui is covered by a line from [x1 , x2 ], and at most kc solution lines different from
x1 are assigned to n/k intervals. (Formally, the kc + kf + 1 solution lines must satisfy the
conditions (C1) to (C6).)
504

fiOn the Computation of Fully Proportional Representation

Next, we define subsets of intervals needed for the decomposition into left and right
subinstances. For ui  U , two coordinates 1  x1  x2  m such that r(ui )  [x1 , x2 ],
and x  [x1 , x2 ] let

,
if x = x1
Ul (ui , x, x1 , x2 ) :=
{uj  U | j > i  r(uj )  [x1 , x  1]}, otherwise
and
Ur (ui , x, x1 , x2 ) := U (ui , x1 , x2 ) \ (Ul (ui , x, x1 , x2 )  {ui }).
Algorithm. We state the algorithm which will be explained when discussing the correctness
below. Basically, the algorithm works in three phases. In a first phase, the dynamic
programming table is initialized as follows. For each ui  U , for every two coordinates x1 
x2 such that r(ui )  [x1 , x2 ], x1  ui , and for every integer b  [1, n/k], let
(ui , x1 , x2 , 0, 0, b) := min(b, |{u  U (ui , x1 , x2 ) : x1  u}|).

(3)

In a second phase, the table is updated. The update of a table entry (ui , x1 , x2 , kc , kf , b)
is provided by Algorithm 3, where the order in which the update is invoked is determined
by Algorithm 4. In a third phase, the algorithm outputs the maximum value over all ui  U
and all x1  [1, . . . , m] with x1  ui and |[x1 , m]|  k of
(
(ui , x1 , m, kc  1, kf , n/k)
(4)
max
(ui , x1 , m, kc , kf  1, n/k).
Correctness. We show that in every stage of the dynamic programming an entry contains
the value of a best assignment of a partial solution for the subinstance with interval set
U (ui , x1 , x2 ) such that six conditions (C1) to (C6) hold. More specifically, we argue that
for every ui  U , for any two coordinates x1  x2 such that r(ui )  [x1 , x2 ] and x1  ui , for
each 0  kc  kc and for each 0  kf  kf with kf + kc  k  1 and |[x1 , x2 ]|  kc + kf + 1,
and for each 1  b  n/k
[
(ui , x1 , x2 , kc , kf , b) = max |
A(S  )|
S  S 

over all sets S   [x1 , x2 ] and assignments A : S   2U (ui ,x1,x2 ) with
 there is an S  S  with ui  A(S),

(C1)

 |S  | = kc + kf + 1 ,

(C2)

 x1  S  ,

(C3)

 |A(x1 )|  b,

(C4)

 S  S  |A(S  )|  n/k, and

(C5)

 |{S   S  \ {x1 } : |A(S  )| = n/k}|  kc .

(C6)

505

fiBetzler, Slinko, & Uhlmann

1
2
3
4
5
6
7
8
9
10
11
12
13

Function Update(ui , x1 , x2 , kc , kf , b) begin
M := 0;
if b > 1 then
for every uj  U (ui , x1 , x2 ) \ {ui } with x1  uj do
M := max{M, (uj , x1 , x2 , kc , kf , b  1)};
end
else
for every x = x1 + 1 to x2 with |[x , x2 ]|  kc + kf do
for every uj  U (ui , x1 , x2 ) \ {ui } with x  uj do
if kc > 0 then
M := max{M, (uj , x , x2 , kc  1, kf , n/k)};
if kf > 0 then
M := max{M, (uj , x , x2 , kc , kf  1, n/k)};

14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

end
end
for every x = x1 + 1 to r(ui ) with x  ui do
for all kcl  0 and kcr  0 with kcl + kcr = kc do
for all kfl  0 and kfr  0 with kfl + kfr = kf do
if |[x1 , x  1]|  kcl + kfl + 1 and |[x, x2 ]|  kcr + kfr then
Ml , Mr := 0;
for every uj  Ul (ui , x, x1 , x2 ) with x1  uj do
Ml := max{Ml , (uj , x1 , x  1, kcl , kfl , b)};
end
for every uj  Ur (ui , x, x1 , x2 ) with x  uj do
if kcr > 0 then
Mr := max{Mr , (uj , x, x2 , kcr  1, kfr , n/k  1)};
if kfr > 0 then
Mr := max{Mr , (uj , x, x2 , kcr , kfr  1, n/k  1)};

30

end
32
M := max{M, Ml + Mr };
33
end
34
end
35
end
36
end
37
(ui , x1 , x2 , kc , kf , b) := M + 1;
38 end
Algorithm 3: Update step employed by the dynamic programming algorithm for MaxBal-1-RS presented in the proof of Theorem 9.
31

506

fiOn the Computation of Fully Proportional Representation

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Main :
for all [x1 , x2 ]  [1, m] in increasing order of x2  x1 do
for kc = 0, . . . , kc do
for kf = 0, . . . , kf do
if |[x1 , x2 ]|  kc + kf + 1 and 1  kc + kf  k  1 then
for b = 1, . . . , n/k do
for ui  U with r(ui )  [x1 , x2 ] and x1  ui do
(ui , x1 , x2 , kc , kf , b) := Update(ui , x1 , x2 , kc , kf , b);
end
end
end
end
end
end
Algorithm 4: Main loop after the initialization.

For every entry computed in the initialization step (Equation 3), the algorithm stores the
maximum value of a partial solution S  = {x1 } (satisfies (C2) and (C3)), that covers ui
(C1) (which is possible since x1  ui and b  1), and that satisfies |A(x1 )|  b (C4). Clearly,
(C5) and (C6) hold as well.
Regarding the update step (see Algorithm 3), let us assume that the values of all entries
that are accessed in the update are correct and well-defined (discussed below). To ensure (C1) for an entry (ui , x1 , x2 , kc , kf , b), interval ui must be covered by one of the lines
of [x1 , x2 ]. We argue that all such possibilities that hold the conditions C1 - C6 are considered systematically and the current maximum value is stored in the variable M . Before
going into further details, we observe that Algorithm 3 adds one to the overall maximum
value (Line 37) to take into account that ui has been newly covered. This is correct
because there is at least one possibility to cover ui in the considered interval, namely, x1
can always cover u1 since x1  ui (Line 7 of Algorithm 4) and b  1 (Line 6 of Algorithm 4).
Lines 3 to 16 of Algorithm 3 consider the possibility that x1 is used to cover ui . Here,
two possibilities are distinguished. The first investigated possibility (Line 3 to Line 7 of
Algorithm 3) is that ui is covered by x1 and x1 can cover at least one more interval, that
is, b > 1. In this case, we can compute the optimal value based on the value for the
subinstance not containing ui and in which x1 is in the solution but can be assigned to
one interval less. To this end, b is decreased by one (that is, (C4) holds) and all possible
intervals in U (ui , x1 , x2 ) \ {ui } (Line 4) are checked to be the leftmost covered interval in
a corresponding subsolution. Moreover, since kf and kc remain the same and we assume
that conditions (C2), (C5), and (C6) hold for (uj , x1 , x2 , kc , kf , b  1), they also hold for
(ui , x1 , x2 , kc , kf , b).
The second investigated possibility (Lines 7 to 16 of Algorithm 3) is that ui is covered
by x1 and x1 can cover at most one interval, that is, b = 1. This case can be traced back
to the same subinstance without ui and x1 . To access the corresponding possibilities in the
dynamic programming table, Algorithm 3 tries all possible lines to be new solution lines
(Line 8) and leftmost intervals to be covered in the new subinstance (Line 9). Then, it
507

fiBetzler, Slinko, & Uhlmann

chooses the maximum of assigning a capacity of n/k (Line 11) or n/k (Line 13) to
x . Note that since 1  kc + kf (Line 5 of Algorithm 4) at least one of these two cases must
be possible (if there is at least one further interval to be covered, that is, U (ui , x1 , x2 ) \
{ui } =
6 ). Assume that we have a solution corresponding to a table entry (uj , x , x2 , kc 
1, kf , n/k)} (Line 11) or (uj , x , x2 , kc , kf  1, n/k)} (Line 13), respectively, and hence,
fulfilling the conditions (C1) to (C6) for the corresponding subinstance. Then, clearly
adding x1 to this solution and assigning ui to x1 gives a solution fulfilling all constraints
for (ui , x1 , x2 , kc , kf , b) for b = 1.
The following loop of Algorithm 3 (Lines 17 to 37) tries all possibilities to cover ui
by a line x 6= x1 . If x 6= x1 , according to Observation 3, the instance can be divided
into two subinstances. All combinations of sizes of the subsolutions are tested by iterating
over kcl , kcr , kfl , and kfr (Lines 18 and 19). In Lines 22 to 23 Algorithm 3 computes an
optimal solution for the left subinstance and in Lines 25 to 29 it computes a solution
for the right subinstance obtained after assigning ui to x. The decomposition into the
subinstances defined by the interval sets Ur (ui , x, x1 , x2 ) and Ul (ui , x, x1 , x2 ) follows directly
from Observation 3. Moreover, since x1 is part of the left subinstance with unchanged
capacity bound b, conditions (C1), (C3) and (C4) hold. It remains to show that (C2),
(C5) and (C6) hold, that is, in addition to x1 a considered subsolution consists of kc lines
that are assigned to at most n/k intervals and kf lines that are assigned to at most n/k
intervals. In any considered possibility, only x is newly specified as a solution line and
according to the decomposition (see Observation 3 and the definitions of Ul , Ur ) it is part of
the right subinstance. In Lines 27 and 29, Algorithm 3 chooses the maximum between the
possibilities that x can be assigned to at most n/k or n/k intervals, respectively, and
adapts the values or kcr and kfr accordingly when accessing the corresponding table entries.
Hence, all conditions hold.
Now, consider the output of the overall algorithm, see Equation (4). The first maximum
function iterates over all intervals ui and possible leftmost solution lines and hence will find a
pair ui and x1 as follows. The interval ui is the leftmost interval that is covered by a solution
with leftmost interval first property and x1 is the leftmost line of the considered solution.
Then, the second maximum function chooses the maximum of the two cases that x1 is
assigned to at most n/k or n/k intervals, respectively. Since
for the
S (C1) to (C6) hold

corresponding entries, the algorithm outputs the maximum | SS  A(S)| overall S  [1, m]
and corresponding assignments A fulfilling the further constraints of the definition of MaxBal-1RS.
It remains to show that the algorithm only accesses well-defined entries, that is, the
accessed entries have been computed before. This is ensured by iterating over the dynamic programming table as described in Algorithm 4. Regarding the computation of
(uj , x1 , x2 , kc , kf , b  1) in Line 5 of Algorithm 3, all parameters values except b are the
same as in the current entry. In Line 6 Algorithm 4 iterates over b in increasing order
and, hence, (uj , x1 , x2 , kc , kf , b  1) has been computed before it is accessed. Moreover,
the condition b > 1 in Line 3 of Algorithm 3 ensures that (uj , x1 , x2 , kc , kf , b  1) is well
defined. In Lines 11 and 13 of Algorithm 3, the accessed range [x , x2 ] is smaller than the
range from x1 to x2 . Since the algorithm iterates over ranges according to increasing size
(Line 2 in Algorithm 4) and the other parameter values have also been considered in former
iteration loops, the entry has been computed before. In Line 23 of Algorithm 3 the accessed
508

fiOn the Computation of Fully Proportional Representation

entry has a range from x1 to x  1 < x2 in Lines 27, and 29 the considered range [x, x2 ]
is also strictly smaller than the range [x1 , x2 ]. Again, according to Line 2 in Algorithm 4),
the entries have been computed before.
Running time. Regarding the running time, the update can be accomplished in O(mk2 n)
time (see Algorithm 3) by iterating over at most m coordinates of the lines in S (Line 17),
less than k2 cases in Lines 18 and 19, and less than n intervals in the inner loops in
Lines 5, 22, and 25, respectively. The overall loop (Algorithm 4) gives an additional factor
of O(m2 n2 k): By an appropriate implementation it can be accomplished by iterating over
less than m2 coordinate ranges (Line 2), less than k2 possibilities in Lines 4 and 3, n/k 
(n + 1)/k values of b (Line 6), and at most n intervals in Line 7. This yields a running
time bound of O(m2 k(n + 1)n) = O(m2 n2 k). Hence, the overall running time is bounded
by O(n3 k3 m3 ).
An instance with a single-peaked input profile of M-Multiwinner for the approval
misrepresentation function reduces to Max-Bal-1-RS by the same transformation as described in Section 5.2.1. We have a vertical line for every candidate, and a horizontal interval
for each voter v reaching from the leftmost candidate with r(v, c) = 0 and to the rightmost
such candidate. The crucial observation is, that minimizing the total misrepresentation for
M-Multiwinner is equivalent to maximizing the number of voters that are represented by
candidates with misrepresentation zero and, hence, to maximizing the number of covered
intervals in the Max-Bal-1-RS instance. Altogether, we arrive at the main result of this
section.
Theorem 10. M-Multiwinner for the approval misrepresentation function and singlepeaked input profiles can be decided in O(n3 m3 k3 ) time.
Recall that an instance of Minimax M-Multiwinner with R > 0 can be reduced to
an instance of M-Multiwinner with R = 0 and approval misrepresentation function by
setting for each voter v and each candidate c the misrepresentation value to 0, if r(v, c)  R,
and to 1 otherwise (see Observations 1 and 2). Altogether, we arrive at the following.
Proposition 5. An instance of Minimax M-Multiwinner with a single-peaked profile
(and an arbitrary misrepresentation function) can be solved in O(n3 m3 k3 ) time.
5.2.2 NP-Hardness of M-Multiwinner for a Single-Peaked Election
Contrasting the polynomial-time solvability results for the other three considered problems, we show that there is an integer-valued misrepresentation function such that MMultiwinner is NP-complete even restricted to instances with a single-peaked input profile. More specifically, we show that M-Multiwinner is NP-hard for single-peaked input
profiles and integer-valued misrepresentation functions such that the maximum misrepresentation value of a voter is bounded from above by a polynomial in the number of candidates.
Note that for establishing the NP-hardness we have to allow that a voter can assign the
same misrepresentation value to several candidates.
The NP-hardness follows by a reduction from a restricted variant of Exact 3-Cover.
Restricted Exact 3-Cover (rX3C)
Input: A family S := {S1 , . . . , Sm } of sets over elements E := {e1 , . . . , en } such
509

fiBetzler, Slinko, & Uhlmann

that every set from S has size 3 and every element of E occurs in exactly three
sets.

Question: Is there a subset
S S  S such that every element of E occurs in

exactly one set of S and SS  S = E?

Such a set S  is called an exact 3-cover of E. Since in all yes-instances n is a multiple of
3, in what follows we assume that n is divisible by 3. The NP-hardness of rX3C follows
from an NP-hardness reduction for the case that every element occurs in at most three
subsets (Garey & Johnson, 1979) and a construction to extend this NP-hardness result to
the case that every element occurs in exactly three subsets (Gonzalez, 1985).
Theorem 11. M-Multiwinner is NP-hard for single-peaked input profiles and an integervalued misrepresentation function even if the maximum misrepresentation value of every
voter is polynomial in the number of candidates (and every winner represents exactly three
voters).
Proof. We use the following notation. Consider an rX3C instance (S, E). For an element
e  E that occurs in the three subsets Si , Sj , and Sk with i < j < k, we say that the first
occurrence of e is in Si , the second occurrence is in Sj , and the third occurrence is in Sk .
For an rX3C instance (S, E), define an M-MW instance as follows. The set of candidates
is
C := E  {sj | Sj  S}
and the multiset of voters is
V := {vix | ei  E and x  {1, 2, 3}}  {fi | ei  E}.
That is, there is a candidate for each element and each subset and there are four voters for
each element. Next, we specify the misrepresentation functions of the voters:
for
for
for
for
for

i  {1, . . . , n}
i  {1, . . . , n}, c  C \ {ei }
i  {1, . . . , n}, x  {1, 2, 3}, 1  z  i
i  {1, . . . , n}, x  {1, 2, 3}, z > i
1  j  m, x  {1, 2, 3}, if the xth occurrence of ei is in Sj
else

r(fi , ei ) := 0
r(fi , c) := 2n2 + 1
r(vix , ez ) := i + z  1
r(vix , ez ) := 2n2 + 1
r(vix , sj ) := 0
r(vix , sj ) := 1

Finally, set the misrepresentation bound to R := 2n2 and let the number of winners be
k := n/3 + n. Before showing the correctness of the reduction, we discuss three crucial
properties of the construction.
First, we verify that the profile is single-peaked witnessed by the societal order
s1      sm  e1      en .
For every voter fi single-peakedness is obvious since his misrepresentation is 0 for one
candidate and 2n2 + 1 for every other candidate. For every vix , within the candidate set E,
the misrepresentation function decreases monotonously when we move from en to e1 along
the societal axis: For z > i, this is obvious since the misrepresentation remains constant at
the value 2n2 +1 and for z  i the misrepresentation value is i+z 1 and hence the function
510

fiOn the Computation of Fully Proportional Representation

clearly assumes smaller values for decreasing values of z. This settles the single-peakedness
for the range from e1 to en . To see the overall single-peakedness, first note that for e1
the misrepresentation of every vix is at least 1. Then, since the misrepresentation is 1 for
all but one of the candidates from {s1 , . . . , sm } and 0 for the remaining candidates, the
single-peakedness for every vix follows.
Second, since there are 4n voters and k = (4n)/3, exactly three voters have to be
assigned to every winning candidate of a solution.
Third, we show that the four voters that can be best represented by candidate ei are
fi , vi1 , vi2 , vi3 . More specifically, we show the following.
Observation 4. For every ei and x  {1, 2, 3}, r(vix , ei ) < r(y, ei ) for every y  V \({fi }
{vi1 }  {vi2 }  {vi3 }).
To see the correctness, observe that for every fixed a  {1, . . . , n}, r(vax , ea ) = 2a  1
and for every x  {1, 2, 3}


 for 1  b < a, r(vbx , ea ) = 2n2 + 1 > 2a  1 ,


 for a < b  n, r(vbx , ea ) = a + b  1 > 2a  1, and
 for b 6= a, r(fb , ea ) = 2n2  1 > 2a  1.
Now, we show the following.
Claim: There is an exact 3-cover for (S, E) if and only if there is a set of
k = 4n/3 candidates that can represent all voters with total misrepresentation
R = 2n2 such that exactly three voters are assigned to one candidate.
 Given an exact 3-cover S   S, we show that the set {sj | Sj  S  }  E of candidates
is a winning set as required by the claim. The corresponding mapping is as follows.
 For every 1  i  n, the voter fi is assigned to the candidate ei .
 For 1  i  n and x  {1, 2, 3}, if ei occurs for the xth time in Sj  S  , then vix is
assigned to sj , else vix is assigned to ei .
Since in the exact 3-cover every element is covered exactly once, it follows that every voter
is assigned to exactly one candidate and every winning candidate represents three voters.
More specifically, for the three voters vi1 , vi2 , and vi3 corresponding to the three occurrences
of the element ei , one of them is represented by the candidate corresponding to the solution
set in which ei occurs and the two other voters by the candidate ei (the third candidate
represented by ei is fi ). It remains to compute the total misrepresentation of this solution.
Due to the definition, every candidate sj represents all three voters with misrepresentation
0. Moreover, every candidate ei represents fi with misrepresentation 0 and two voters from
{vi1 , vi2 , vi3 } with misrepresentation r(vix , ei ) = i + i  1 = 2i  1 for x  {1, 2, 3}. Hence, the
total misrepresentation is
n
X

2(2i  1) = 2n(n + 1)  2n = 2n2 .

i=1

511

(5)

fiBetzler, Slinko, & Uhlmann

 Consider a size-k set C   C of winners that represent all voters with total misrepresentation R = 2n2 . Since for every voter fi , the only candidate that can represent fi with
misrepresentation at most R is ei , it follows that E  C  . Recall that due to the M-criterion,
every candidate must represent exactly three voters. Thus, every candidate ei  E must
represent two further voters (besides fi ). Clearly, a lower bound for the total misrepresentation is achieved in the case when we assign to every ei  E two further voters which can be
represented by ei at least as good as any other voters. Due to Observation 4, two such voters
are from {vi1 , vi2 , vi3 }. Moreover, according to Equation 5 the corresponding lower bound for
the total misrepresentation matches the total misrepresentation R = 2n2 . Since assigning
ei to any voter other than {vi1 , vi2 , vi3 } would lead to a strictly higher misrepresentation (Observation 4), this implies that ei is assigned to exactly two voters from {vi1 , vi2 , vi3 }. Finally,
for every 1  i  n, there remains one voter vix that must be represented by a candidate
from C  \ E with misrepresentation zero. Since |C  \ E| = n/3 and a candidate sj can only
represent a voter vix with misrepresentation 0 if the element ei occurs in Sj , i.e., the sets
corresponding to the candidates in C  \ E must form an exact 3-cover.

6. Conclusion and Outlook
We start with summarizing the relevance of the results of this work. This will be followed
by a discussion of closely related problems and models that might be investigated in future
research. We conclude with several questions that directly follow from our results.
6.1 Relevance of Results
The computation of a set of candidates that fully proportionally represent the society has
applications in many relevant settings. The main problem with the suggested approaches
in the extant literature is that the corresponding combinatorial problems are NP-hard, that
is, they cannot be solved efficiently in general. This raises the question whether these
approaches despite the theoretically proven advantages (see, e.g., a detailed discussion of
those in Brams, 2008) are useless in practice.
One approach is of course to try to escape high complexity by modifying the concept
while keeping it still meaningful. In this regard we tried to change the way the total
misrepresentation is calculated taking the minimax (or Rawlsian) approach. This appeared
not to help in the general caseall problems remain computationally hardhowever, it
partially helped for single-peaked elections: while the classical Monroe scheme remains
NP-hard, its minimax version can be solved in polynomial time.
In general, there are several ways to deal with NP-hard problems. For example, NPhardness is based on the worst-case analysis and hence one might be able to develop algorithms that work efficiently for most instances. However, although unlikely, it still might
happen that the outcome of an election leads to a hard instance. Then, this would lead to
the situation of political impasse with unpredictable consequences.
Another common approach to tackle NP-hard problems is to invoke approximation algorithms. While for some scenarios like in the context of resource allocation with sharable
items a nearly optimal solution might be sufficient and approximation algorithms are meaningful (Lu & Boutilier, 2011; Skowron, Faliszewski, & Slinko, 2012); for other scenarios, like
political elections, the use of approximation algorithms is hard to imagine. A voting rule is
512

fiOn the Computation of Fully Proportional Representation

a constitutional matter: whatever it is, it must be adhered to. Under the current legislation
any candidate or a party can ask for a recount and, if an approximation to a voting rule
was used, they may require using another approximation which they can argue give a better
representation. It is not hard to imagine prolonged court proceedings on such matters.
Based on the previous discussion, it seems clearly desirable to identify well-specified
settings for which an optimal solution can be computed efficiently. This will extend the
applicability of the fully proportional representation to such settings. In this regard, we
conducted an investigation in two different directions. The first was the class of settings in
which some parameters are small (parameterized complexity analysis). The second approach
was to restrict attention to single-peaked domains.
Regarding the parameterized complexity of the four studied problems, most of our results are negative (see Table 1). In particular, for the natural and well-motivated parameter
the number of winners, the corresponding problems turned out to be W[2]-complete. If,
however, in addition there is a winner set that can represent all voters with a small total
misrepresentation, three of the four problems become tractable for the Borda misrepresentation function. Moreover, the fixed-parameter tractability results with respect to the
number of voters and the number of candidates, respectively, are useful for such restricted
settings.
Regarding single-peaked elections, almost all of our results are positive and come with
polynomial-time algorithms (see Table 2). A possible critique of this approach is to claim
that single-peakedness is in a way an idealized model which is not robust enough. A smallest
honest mistake of a voter in filling her ballot may result in election becoming not singlepeaked. Also there may be a secondary issue in the election that is also important for some
voters which may lead to the election being almost single-peaked but not exactly singlepeaked. In this regard it would be interesting to investigate how difficult is to find a singlepeaked profile closest to the given one. For this one might employ techniques of the socalled distance rationalizability approach (Baigent, 1987; Meskanen & Nurmi, 2008; Elkind,
Faliszewski, & Slinko, 2010a; Elkind et al., 2010b). It is thus not surprising that near singlepeakedness is now starting to be an active area of research (Faliszewski, Hemaspaandra,
& Hemaspaandra, 2011; Erdelyi, Lackner, & Pfandler, 2012). Since our algorithms show
polynomial-time solvability for the important basic case of single-peakedness, they might
be a basis for developing efficient algorithms for such extended settings.
Summarizing, our work contributes to the important topic of making fully proportional
representation ideas practical and complements the analysis of this method by Potthof and
Brams (1998), Procaccia et al. (2008) and Lu and Boutilier (2011).
6.2 Related Problems and Scenarios
Before concluding the work with several open questions, we, first, describe some relations
of the considered problems to facility location problems and, second, describe a reasonable
alternative multi-winner model. Both topics might also lead to interesting questions for
future research.
513

fiBetzler, Slinko, & Uhlmann

6.2.1 Relations to Facility Location.
A basic scenario for this problem is that a company needs to choose a set of facility locations
to serve a set of customers with as little cost to them as possible. Fellows and Fernau (2011)
investigated the parameterized complexity of a variant of this problem that is closely related
to CC-Multiwinner. Basically, the facility locations can be considered as the set of
candidates, the customers as the multiset of voters and the goal is to find a set of facility
locations to serve these customers. The only difference is the cost function: In addition
to a term that resembles the misrepresentation for every voter (customer), every facility
location comes with a certain cost that is required to install the facility.
Similar to our study, Fellows and Fernau (2011) studied the parameter number k of
winners/selected facilities locations and the total cost. For the parameter k, W[2]-hardness
for CC-Multiwinner follows from the reduction given for the facility location problem.
Regarding the parameter total cost, the results of the two papers are not directly comparable. This is due to the fact that the facility location problem stipulates that there
is a minimum cost of 1 for serving a customer even at the best facility location (which
would be an analogue of the condition r(v, c)  1 for the misrepresentation function r).
In this case, the considered problem is fixed-parameter tractable with respect to the total
cost. This might not come as a surprise since here the total cost/misrepresentation is at
least the number of voters and fixed-parameter tractability with respect to this parameter
holds for all four voting problems that we considered (Proposition 2). In contrast, with
the condition r(v, c)  0 all considered problems are at least W[2]-hard with respect to the
total misrepresentation/cost (see Table 1).
The close connection between facility location and multi-winner problems clearly seems
to deserve more attention in future work. We remark that analogues of several problems
considered in this work might also make sense in the context of the facility location problem.
For example, the Monroe model might apply for sets of facilities such that every facility can
serve about the same number of customers. Moreover, the single-peaked scenario translates,
for example, to the setting that all potential facility locations are along one main street and
each resident ranks the cost of using the facility according to the distance from that facility
to the place of his residence.
6.2.2 Multiset of Candidates Model.
There may be a compromise solution between the two systems of Chamberlin and Courant
and Monroe. We may still divide voters into equal or almost equal groups but we may assign
the same representative to more than one group of voters. Say, if there are n voters and k
representatives are to be elected we may split voters into groups of sizes n/k and n/k+1
but allow the same candidate to represent more than one group. Mathematically this would
result in selecting not a set of representatives of cardinality k but a multiset of the same
cardinality. The classic Monroe (1995) example which considers subscription of newspapers
for the common room is in fact a better fit for the multiset model. Indeed, if demand, say
for Financial Times, is strong several copies of this newspaper can be subscribed to. We
will still need to use weighted voting in the assembly but in this case all weights will be
integers.
514

fiOn the Computation of Fully Proportional Representation

To illustrate the difference let us consider six people electing a representative assembly
of three. Suppose our candidates must come from the set A = {a, b, c, d} and the preferences
of voters are as follows:
4
a
b
c
d

2
c
b
a
d

A set variant of Monroe scheme will give us the set of representatives {a, b, c} while from
the multiset point of view it is more natural to have a multiset {a2 , c} as the answer which
could be interpreted to mean that two votes given to a and one to c. The multiset point of
view seems more natural here, indeed, b does not seem to represent anybody nicely. So the
misrepresentation will be nonzero in the set version and zero in the multiset one.
As far as we know the computational complexity for the computation of a winner in the
multiset model is unstudied so far. On a first glance, it seems conceivable that the computational complexity for the multiset model lies between the complexity for CC-Multiwinner
and M-Multiwinner. This leads to interesting questions such as whether a set of winners
according to the multiset model can be computed in polynomial time when the electorate
is single peaked.
6.3 Open Questions
Several questions arise from this work.
 For CC- and M-Multiwinner for the Borda misrepresentation function we provided algorithms showing polynomial-time solvability for a constant misrepresentation
bound R. Are these problems fixed-parameter tractable with respect to R?
 Is Minimax M-Multiwinner for the Borda misrepresentation function fixed-parameter
tractable with respect to the composite parameter (R, k)?
 For M-Multiwinner for single-peaked elections we have shown NP-hardness for
integer-valued misrepresentation functions. Is the problem fixed-parameter tractable
with respect to the number of winners k or/and with respect to the misrepresentation
bound R?
 Is M-Multiwinner for the Borda misrepresentation function polynomial-time solvable for single-peaked instances?
 Can the results for single-peaked elections be extended to generalized single-peakedness
(e.g., as defined by Nehring & Puppe, 2007) or to almost single-peaked profiles (in
some sense)? This might be of particular interest if the problem of finding the closest single-peaked profile to a given one would turn out to be polynomial-time solvable
(for some distance on the set of profiles).
515

fiBetzler, Slinko, & Uhlmann

7. Acknowledgments
Most of the work was done while Nadja Betzler stayed for a research visit at the University of
Auckland. Her visit was funded by a fellowship by the Deutscher Akademischer Austausch
Dienst (DAAD). Johannes Uhlmann was partially supported by the DFG, project PAWS
NI-931/10.
We are very grateful to our referees whose careful and thoughtful reading has significantly
improved this paper. Moreover, we like to thank Steven Brams for his interest in this work
and valuable discussions and Rolf Niedermeier for his useful comments and support.

References
Ahuja, R. K., Magnanti, T. L., & Orlin, J. B. (1993). Network Flows: Theory, Algorithms,
and Applications. Prentice Hall.
Baigent, N. (1987). Metric rationalisation of social choice functions according to principles
of social choice. Mathematical Social Sciences, 13 (1), 5965.
Bartholdi III, J. J., Tovey, C. A., & Trick, M. A. (1989). The computational difficulty of
manipulating an election. Social Choice and Welfare, 6, 227241.
Betzler, N., Bredereck, R., Chen, J., & Niedermeier, R. (2012). Studies in computational
aspects of voting - a parameterized complexity perspective. In The Multivariate Algorithmic Revolution and Beyond, Vol. 7370 of Lecture Notes in Computer Science,
pp. 318363. Springer.
Betzler, N., Guo, J., & Niedermeier, R. (2010). Parameterized computational complexity
of Dodgson and Young elections. Information and Computation, 208 (2), 165177.
Betzler, N., Hemmann, S., & Niedermeier, R. (2009). A multivariate complexity analysis
of determining possible winners given incomplete votes. In Proceedings of the 21st
International Joint Conference on Artificial Intelligence (IJCAI), pp. 5358.
Black, D. (1958). The Theory of Committees and Elections. Cambridge University Press.
Booth, E., & Lueker, G. (1976). Testing for the consecutive ones property, interval graphs,
and graph planarity using PQ-trees algorithms. Journal of Computer and System
Sciences, 13, 335379.
Brams, S. (2008). Mathematics and Democracy. Princeton University Press.
Brams, S., & Fishburn, P. (2002). Voting procedures. In Arrow, K., Sen, A. K., & Suzumura,
K. (Eds.), Handbook of Social Choice and Welfare, Vol. 1, pp. 173236. Elsevier.
Brandt, F., Brill, M., & Seedig, H. G. (2011). On the fixed-parameter tractability of
composition-consistent tournament solutions. In Proceedings of the 22nd International
Joint Conference on Artificial Intelligence (IJCAI), pp. 8590. AAAI Press.
Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. A. (2010). Bypassing combinatorial protections: Polynomial-time algorithms for single-peaked electorates. In
Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI), pp. 715
722.
516

fiOn the Computation of Fully Proportional Representation

Chamberlin, J. R., & Courant, P. N. (1983). Representative deliberations and representative
decisions: Proportional representation and the Borda rule. American Political Science
Review, 77 (3), 718733.
Christian, R., Fellows, M. R., Rosamond, F. A., & Slinko, A. (2007). On complexity of
lobbying in multiple referenda. Review of Economic Design, 11 (3), 217224.
Conitzer, V. (2009). Eliciting single-peaked preferences using comparison queries. Journal
of Artificial Intelligence Research, 35, 161191.
Conitzer, V. (2010). Making decisions based on the preferences of multiple agents. Communications of the ACM, 53 (3), 8494.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms
(2nd edition). MIT Press.
Dodgson, C. (1884). The Principles of Parliamentary Representation. Harrison, London.
Dorn, B., & Schlotter, I. (2010). Multivariate complexity analysis of swap bribery. In
Proceedings of the 5th International Symposium on Parameterized and Exact Computation (IPEC), Vol. 6478 of LNCS, pp. 107122. Springer.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Springer.
Elkind, E., Faliszewski, P., & Slinko, A. (2010a). Cloning in elections. In Proceedings of the
24th AAAI Conference on Artificial Intelligence (AAAI), pp. 768773.
Elkind, E., Faliszewski, P., & Slinko, A. (2010b). On the role of distances in defining voting
rules. In Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS), pp. 375382.
Erdelyi, G., Lackner, M., & Pfandler, A. (2012). The complexity of nearly single-peaked
consistency. In Proceedings of the 4th International Workshop on Computational Social Choice (COMSOC 2012), pp. 179190.
Escoffier, B., Lang, J., & Ozturk, M. (2008). Single-peaked consistency and its complexity.
In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI), pp.
366370. IOS Press.
Even, G., Levi, R., Rawitz, D., Schieber, B., Shahar, S., & Sviridenko, M. (2008). Algorithms for capacitated rectangle stabbing and lot sizing with joint set-up costs. ACM
Transactions on Algorithms, 4 (3), 34:134:17.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2010). Using complexity to
protext elections. Communications of the ACM, 53 (1), 7482.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2009a). Llull and
Copeland voting computationally resist bribery and constructive control. Journal of
Artificial Intelligence Research, 35, 275341.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2009b). A richer
understanding of the complexity of election systems. In Ravi, S., & Shukla, S.
(Eds.), Fundamental Problems in Computing: Essays in Honor of Professor Daniel
J. Rosenkrantz, chap. 14, pp. 375406. Springer.
517

fiBetzler, Slinko, & Uhlmann

Faliszewski, P., & Procaccia, A. (2010). AIs war on manipulation: Are we winning?. AI
Magazine, 31 (4), 5364.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2011). The complexity of
manipulative attacks in nearly single-peaked electorates. In Proceedings of the 13th
Conference on Theoretical Aspects of Rationality and Knowledge, TARK XIII, pp.
228237, New York, NY, USA. ACM.
Farfel, J., & Conitzer, V. (2011). Aggregating value ranges: Preference elicitation and
truthfulness. Journal of Autonomous Agents and Multi-Agent Systems, 22 (1), 127
150.
Fellows, M. R., & Fernau, H. (2011). Facility location problems: A parameterized view.
Discrete Applied Mathematics, 159 (11), 11181130.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer.
Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps and their uses in improved network
optimization algorithms. Journal of the ACM, 34 (3), 596615.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman.
Garey, M. R., Johnson, D. S., & Stockmeyer, L. (1974). Some simplified NP-complete
problems. In Proceedings of the sixth annual ACM symposium on Theory of computing
(STOC), pp. 4763. ACM.
Golumbic, M. (1980). Algorithmic Graph Theory and Perfect Graphs. Academic Press.
Gonzalez, T. F. (1985). Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38, 293306.
Harsanyi, J. (1995). Can the maximin principle serve as a basis for morality? a critique of
John Rawlss theory. American Political Science Review, 69 (2), 594606.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). Which problems have strongly exponential
complexity?. Journal of Computer and System Sciences, 63 (4), 512530.
Levin, J., & Nalebuff, B. (1995). An introduction to vote-counting schemes. Journal of
Economic Perspectives, 9 (1), 326.
Lu, T., & Boutilier, C. (2011). Budgeted social choice: From consensus to personalized
decision making. In Proceedings of the Twenty-second International Joint Conference
on Artificial Intelligence (IJCAI-11), pp. 280286. Also presented at COMSOC-10.
Meir, R., Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2008). The complexity of strategic behavior in multi-winner elections. Journal of Artificial Intelligence Research, 33,
149178.
Meskanen, T., & Nurmi, H. (2008). Closeness counts in social choice. In Braham, M., &
Steffen, F. (Eds.), Power, Freedom, and Voting, pp. 289306. Springer.
Monroe, B. L. (1995). Fully proportial representation. American Political Science Review,
89 (4), 925940.
Moulin, H. (1991). Axioms of Cooperative Decision Making. Cambridge University Press.
518

fiOn the Computation of Fully Proportional Representation

Nehring, K., & Puppe, C. (2007). The structure of strategy-proof social choice  Part I: General characterization and possibility results on median spaces. Journal of Economic
Theory, 135 (1), 269305.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford University Press.
Penrose, L. (1946). The elementary statistics of majority voting. Journal of the Royal
Statistical Society, 109, 5357.
Potthof, R. F., & Brams, S. J. (1998). Proportional representation: Broadening the options.
Journal of Theoretical Politics, 10 (2), 147178.
Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2007). Multi-winner elections: Complexity of manipulation, control and winner-determination. In Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 14761481.
Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2008). On the complexity of achieving
proportional representation. Social Choice and Welfare, 30, 353362.
Rawls, J. (1999). A Theory of Justice. Cambridge: Harvard University Press, 1971, Revised
Edition (Cambridge: Harvard University Press).
Skowron, P., Faliszewski, P., & Slinko, A. M. (2012). Proportional representation as resource
allocation: Approximability results. CoRR, abs/1208.1661.
Slomczynski, W., & Zyczkowski, K. (2006). Penrose voting system and optimal quota. Acta
Physica Polonica, B 37 (11), 31333143.
Tideman, N., & Richardson, R. (2000). Better voting methods through technology: The
refinement-manageability trade-off in the single transferable vote. Public Choice,
103 (1-2), 1334.
Tideman, N. (2006). Collective Decisions and Voting. Ashgate Publishing Ltd.

519

fiJournal of Artificial Intelligence Research 47 (2013) 1-34

Submitted 10/2012; published 05/2013

A Feature Subset Selection Algorithm Automatic
Recommendation Method
Guangtao Wang
Qinbao Song
Heli Sun
Xueying Zhang

gt.wang@stu.xjtu.edu.cn
qbsong@mail.xjtu.edu.cn
hlsun@mail.xjtu.edu.cn
zhangxueying.725@stu.xjtu.edu.cn

Department of Computer Science & Technology,
Xian Jiaotong University, 710049, China

Baowen Xu
Yuming Zhou

bwxu@nju.edu.cn
zhouyuming@nju.edu.cn

Department of Computer Science & Technology,
Nanjing University, China

Abstract
Many feature subset selection (FSS) algorithms have been proposed, but not all of them
are appropriate for a given feature selection problem. At the same time, so far there is
rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus,
FSS algorithm automatic recommendation is very important and practically useful. In
this paper, a meta learning based FSS algorithm automatic recommendation method is
presented. The proposed method first identifies the data sets that are most similar to the
one at hand by the k -nearest neighbor classification algorithm, and the distances among
these data sets are calculated based on the commonly-used data set characteristics. Then,
it ranks all the candidate FSS algorithms according to their performance on these similar
data sets, and chooses the algorithms with best performance as the appropriate ones.
The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric
that takes into account not only the classification accuracy over the selected features, but
also the runtime of feature selection and the number of selected features. The proposed
recommendation method is extensively tested on 115 real world data sets with 22 wellknown and frequently-used different FSS algorithms for five representative classifiers. The
results show the effectiveness of our proposed FSS algorithm recommendation method.

1. Introduction
Feature subset selection (FSS) plays an important role in the fields of data mining and
machine learning. A good FSS algorithm can effectively remove irrelevant and redundant
features and take into account feature interaction. This not only leads up to an insight
understanding of the data, but also improves the performance of a learner by enhancing the
generalization capacity and the interpretability of the learning model (Pudil, Novovicova,
Somol, & Vrnata, 1998a; Pudil, Novovicova, Somol, & Vrnata, 1998b; Molina, Belanche,
& Nebot, 2002; Guyon & Elisseeff, 2003; Saeys, Inza, & Larranaga, 2007; Liu & Yu, 2005;
Liu, Motoda, Setiono, & Zhao, 2010).
c
2013
AI Access Foundation. All rights reserved.

fiWang, Song, Sun, Zhang, Xu & Zhou

Although a large number of FSS algorithms have been proposed, there is no single
algorithm which performs uniformly well on all feature selection problems. Experiments
(Hall, 1999; Zhao & Liu, 2007) have confirmed that there could exist significant differences
of performance (e.g., classification accuracy) among different FSS algorithms over a given
data set. That means for a given data set, some FSS algorithms outperform others.
This raises a practical and very important question: which FSS algorithms should be
picked up for a given data set? The common solution is to apply all candidate FSS algorithms to the given data set, and choose one with the best performance by the crossvalidation strategy. However, this solution is quite time-consuming especially for highdimensional data (Brodley, 1993).
For the purpose of addressing this problem in a more efficient way, in this paper, an FSS
algorithm automatic recommendation method is proposed. The assumption underlying our
proposed method is that the performance of an FSS algorithm on a data set is related to
the characteristics of the data set. The rationality of this assumption can be demonstrated
as follows:
1) Generally, when a new FSS algorithm is proposed, its performance needs to be extensively evaluated at least on real world data sets. However, the published FSS algorithms
are rarely tested on the identical group of data sets (Hall, 1999; Zhao & Liu, 2007; Yu &
Liu, 2003; Dash & Liu, 2003; Kononenko, 1994). That is, for any two algorithms, they
are usually tested on the different data. This implies that the performance of an FSS
algorithm biases to some data sets.
2) At the same time, the famous NFL (No Free Lunch) (Wolpert, 2001) theory tells us
that, for a particular data set, different algorithms have different data-conditioned performance, and the performance differences vary with data sets.
The above evidences imply that there is a relationship between the performance of an
FSS algorithm and the characteristics of data sets. In this paper, we intend to explore this
relationship and utilize it to automatically recommend appropriate FSS algorithm(s) for
a given data set. The recommendation process can be viewed as a specific application of
meta-learning (Vilalta & Drissi, 2002; Brazdil, Carrier, Soares, & Vilalta, 2008) that has
been used to recommend algorithms for classification problems (Ali & Smith, 2006; King,
Feng, & Sutherland, 1995; Brazdil, Soares, & Da Costa, 2003; Kalousis, Gama, & Hilario,
2004; Smith-Miles, 2008; Song, Wang, & Wang, 2012).
To model this relationship, there are three fundamental issues to be considered: i) which
features (often are referred to as meta-features) are used to characterize a data set; ii) how
to evaluate the performance of an FSS algorithm and identify the applicable one(s) for a
given data set; iii) how to recommend FSS algorithm(s) for a new data set.
In this paper, the meta-features, which are frequently used in meta-learning (Vilalta &
Drissi, 2002; Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Castiello, Castellano,
& Fanelli, 2005), are employed to characterize data sets. At the same time, a multi-criteria
metric, which takes into account not only the classification accuracy of a classifier with an
FSS algorithm but also the runtime of feature selection and the number of selected features,
is used to evaluate the performance of the FSS algorithm. Meanwhile, a k -NN (k -Nearest
Neighbor) based method is proposed to recommend FSS algorithm(s) for a new data set.
2

fiSubset Selection Algorithm Automatic Recommendation

Our proposed FSS algorithm recommendation method has been extensively tested on
115 real world data sets with 22 well-known and frequently-used different FSS algorithms
for five representative classifiers. The results show the effectiveness of our proposed recommendation method.
The rest of this paper is organized as follows. Section 2 introduces the preliminaries.
Section 3 describes our proposed FSS algorithm recommendation method. Section 4 provides the experimental results. Section 5 conducts the sensitivity analysis of the number
of the nearest data sets on the recommendation results. Finally, section 6 summarizes the
work and draws some conclusions.

2. Preliminaries
In this section, we first describe the meta-features used to characterize data sets. Then,
we introduce the multi-criteria evaluation metric used to measure the performance of FSS
algorithms.
2.1 Meta-features of Data Sets
Our proposed FSS algorithm recommendation method is based on the relationship between
the performance of FSS algorithms and the meta-features of data sets.
The recommendation can be viewed as a data mining problem, where the performance
of FSS algorithms and the meta-features are the target function and the input variables,
respectively. Due to the ubiquity of Garbage In, Garbage Out (Lee, Lu, Ling, & Ko, 1999)
in the field of data mining, the selection of the meta-features is crucial for our proposed
FSS recommendation method.
The meta-features are measures that are extracted from data sets and can be used to
uniformly characterize different data sets, where the underlying properties are reflected. The
meta-features should be not only conveniently and efficiently calculated, but also related to
the performance of machine learning algorithms (Castiello et al., 2005).
There has been 15 years of research to study and improve on the meta-features proposed
in the StatLog project (Michie, Spiegelhalter, & Taylor, 1994). A number of meta-features
have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005;
Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer,
1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship
between the characteristics of data sets and the performance (e.g., classification accuracy) of
learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al.,
2004; Smith-Miles, 2008). As these meta-features do characterize data sets themselves, and
have no connection with learning algorithms and their types, so we use them to model the
relationship between data sets and FSS algorithms.
The most commonly used meta-features are established focusing on the following three
aspects of a data set: i) general properties, ii) statistic-based properties, and iii) informationtheoretic based properties (Castiello et al., 2005). Table 11 shows the details.
1. In order to compute the information-theoretic features, for data sets with continuous-valued features, if
needed, the well-known MDL (Minimum Description Length) method with Fayyad & Irani criterion was
used to discretize the continuous values.

3

fiWang, Song, Sun, Zhang, Xu & Zhou

Category
General properties

Statistical based properties

Information-theoretic properties

Notation
I
F
T
D
(X, Y )
Skew(X)
Kurt(X)
H(C)norm
H(X)norm
M I(C, X)
M I(C, X)max
ENattr
N Sratio

Measure description
Number of instances
Number of features
Number of target concept values
Data set dimensionality, D = I/F
Mean absolute linear correlation coefficient of all possible pairs of features
Mean skewness
Mean kurtosis
Normalized class entropy
Mean normalized feature entropy
Mean mutual information of class and attribute
Maximum mutual information of class and attribute
Equivalent number of features, ENattr = H(C)/M I(C, X)
Noise-signal ratio, N Sratio = (H(X)  M I(C, X))/M I(C, X)

Table 1: Meta-features used to characterize a data set
2.2 Multi-criteria Metric for FSS Algorithm Evaluation
In this section, first, the classical metrics evaluating the performance of FSS algorithm are
introduced. Then, by analyzing the user requirements in practice application, based on
these metrics, a new and user-oriented multi-criteria metric is proposed for FSS algorithm
evaluation by combining these metrics together.
2.2.1 Classical Performance Metrics
When evaluating the performance of an FSS algorithm, the following three metrics are
extensively used in feature selection literature: i) classification accuracy , ii) runtime of
feature selection, and iii) number of selected features.
1) The classification accuracy (acc) of a classifier with the selected features can be used to
measure how well the selected features describe a classification problem. This is because
for a given data set, different feature subsets generally result in different classification
accuracies. Thus, it is reasonable that the feature subset with higher classification accuracy has stronger capability of depicting the classification problem. The classification
accuracy also reflects the ability of an FSS algorithm in identifying the salient features
for learning.
2) The runtime (t) of feature selection measures the efficiency of an FSS algorithm for
picking up the useful features. It is also viewed as a metric to measure the cost of feature
selection. The longer the runtime, the higher the expenditure of feature selection.
3) The number of selected features (n) measures the simplicity of the feature selection
results. If the classification accuracies with two FSS algorithms are similar, we usually
favor the algorithm with fewer features.
Feature subset selection aims to improve the performance of learning algorithms which
usually is measured with classification accuracy. The FSS algorithms with higher classification accuracy are in favor. However, this does not mean that the runtime and the
number of selected features could be ignored. This can be explained by the following two
considerations:
1) Suppose there are two different FSS algorithms Ai and Aj , and a given data set D. If
the classification accuracy with Ai on D is slightly greater than that with Aj , but the
4

fiSubset Selection Algorithm Automatic Recommendation

runtime of Ai and the number of features selected by Ai are much greater than those of
Aj , then Aj is often chosen.
2) Usually, we do not prefer to use the algorithms with higher accuracy but longer runtime,
so is those with lower accuracy but shorter runtime. Therefore, we need a tradeoff between classification accuracy and the runtime of feature selection/the number of selected
features. For example, in real-time systems, it is impossible to choose the algorithm with
high time-consumption even if its classification accuracy is high.
Therefore, it is necessary to allow users making a user-oriented performance evaluation
for different FSS algorithms. For this purpose, it is needed to address the problem of how
to integrate classification accuracy with the runtime of feature selection and the number of
selected features to obtain a unified metric. In this paper, we resort to the multi-criteria
metric to explore this problem. The underlying reason lies that the multi-criteria metric
has been successfully used to evaluate data mining algorithms by considering the positive
properties (e.g. classification accuracy) and the negative ones (e.g. runtime and number of
selected features) simultaneously (Nakhaeizadeh & Schnabl, 1997, 1998).
When comparing two algorithms, besides the metrics used to evaluate their performance,
the ratio of the metric values can also be used. For example, suppose A1 and A2 are two
different FSS algorithms, if A1 is better than A2 in terms of classification accuracy, i.e.,
acc1 > acc2 2 , then ratio acc1 /acc2 > 1 can be used to show A1 is better than A2 as well.
On the contrary, for the negative metrics runtime of feature selection and number of of the
selected features, the corresponding ratio < 1 means a better algorithm.
Actually, a multi-criteria metric adjusted ratio of ratios (ARR) (Brazdil et al., 2003),
which combines classification accuracy and runtime together as a unified metric, has been
proposed to evaluate the performance of a learning algorithm. We extend ARR by integrating it with the runtime of feature selection and the number of selected features, so a new
multi-criteria metric EARR (extened ARR) is proposed. In the following discussion, we will
show that the new metric EARR is more inclusive, very flexible, and easy to understand.
2.2.2 Multi-Criteria Metric EARR
Let DSet = {D1 , D2 ,    , DN } be a set of N data sets, and ASet = {A1 , A2 ,    , AM } be a
set of M FSS algorithms. Suppose accji is the classification accuracy of a classifier with FSS
algorithm Ai on data set Dj (1  i  M , 1  j  N ), and tji and nji denote the runtime
and the number of selected features of FSS algorithm Ai on data set Dj , respectively. Then
the EARR of Ai to Aj over Dk is defined as
k
EARRD
Ai ,Aj =

accki /acckj
1 +   log (tki /tkj ) +   log (nki /nkj )

(1  i 6= j  M, 1  k  N ),

(1)

where  and  are the user-predefined parameters which denote the relative importance of
the runtime of feature selection and the number of selected features, respectively.
The computation of the metric EARR is based on the ratios of the classical FSS algorithm performance metrics, the classification accuracy, the runtime of feature selection and
2. Where acc1 and acc2 are the corresponding classification accuracies of algorithms A1 and A2 , respectively.

5

fiWang, Song, Sun, Zhang, Xu & Zhou

the number of selected features. From the definition we can know that EARR evaluates an
FSS algorithm by comparing it with another algorithm. This is reasonable since it is more
objective to assert an algorithm is good or not by comparing it with another one instead of
just focusing on its own performance. For example, suppose there is a classifier with 70%
classification accuracy on a data set, we will get confused on whether the classifier is good
or not. However, if we compare it with another classifier that can obtain 90% classification
accuracy on the same data set, then we can definitely say that the first classifier is not good
compared with the second one.
Noted that, in practice, the runtime difference between two different FSS algorithms
usually can be quite great. Meanwhile, for high-dimensional data sets, the difference of the
number of selected features for two different FSS algorithms can be great as well. Thus,
the ratio of runtime and the ratio of the number of selected features usually have much
more wide ranges than that of the classification accuracy. If the simple ratio of runtime and
the simple ratio of the number of selected features are employed, they would dominate the
value of EARR, and the ratio of the classification accuracy will be drowned. In order to
avoid this situation, the common logarithm (i.e., the logarithm with base 10) of the ratio
of runtime and the common logarithm of the ratio of the number of selected features are
employed.
The parameters  and  represent the amount of classification accuracy that a user is
willing to trade for a 10 times speedup/reduction on the runtime of feature selection/the
number of selected features, respectively. This allows users to choose the algorithms with
shorter runtime and less features but acceptable accuracy. This can be illustrated by the
following example. Suppose that accki = (1 +  + )  acckj , the runtime of algorithm Ai on
a given data set is 10 times of that of Aj (i.e., tki = 10  tkj ), and the number of selected
features of algorithm Ai is 10 times of that of Aj (i.e., nki = 10  nkj ). Then, according to
Dk
1
3
k
Eq. (1), EARR D
Ai ,Aj = 1, and EARR Aj ,Ai = 1(+)2 > 1. In this case, Aj outperforms
Ai . If a user prefers fast algorithms with less features, Aj will be his/her choice.
k
The value of EARR varies around 1. The value of EARR D
Ai ,Aj is greater than (or equal
k
to, or smaller than) that of EARR D
Aj ,Ai indicates that Ai is better than (or equal to, or
worse than) Aj .
Eq. (1) can be directly used to evaluate the performance of two different FSS algorithms.
When comparing multiple FSS algorithms, the performance of any algorithm Ai  ASet on
a given data set D can be evaluated by the metric EARR D
Ai defined as follows:

EARRD
Ai =

1
M 1

M
X

EARRD
Ai ,Aj .

(2)

j=1j6=i

This equation shows that the EARR of an FSS algorithm Ai on D is the arithmetic
mean of the EARRD
Ai ,Aj of Ai to other algorithm Aj on D. That is, the performance of any
FSS algorithm Ai  ASet is evaluated based on the comparisons with other algorithms in
{ASet {Ai }}. The larger the value of EARR, the better the corresponding FSS algorithm
on the given data set D.
3. Since log (x/y) =  log (y/x) and ( + )2 > 0

6

fiSubset Selection Algorithm Automatic Recommendation

3. FSS Algorithm Recommendation Method
In this section, we first give the framework of the FSS algorithm recommendation. Then,
we introduce the nearest neighbor based recommendation method in detail.
3.1 Framework
Based on the assumption that there is a relationship between the performance of an FSS
algorithm on a given data set and the data set characteristics (aka meta-features), our
proposed recommendation method firstly constructs a meta-knowledge database consisting
of data set meta-features and FSS algorithm performance. After that, with the help of
the meta-knowledge database, a k-NN based method is used to model this relationship and
recommend appropriate FSS algorithms for a new data set.
Therefore, our proposed recommendation method consists of two parts: Meta-knowledge
Database Construction and FSS Algorithm Recommendation. Fig. 1 shows the details.
Meta-knowledge database construction
Feature selection
algorithms

Historical
data sets

Performance metric
aquirement
Meta-features
extraction

Performance metrics

Meta-features

FSS algorithm recommendation

New data set
Recommended
FSS algorithms

Metaknowledge
database
Performance
metrics

Meta-features

Meta-features
extraction

Meta-features

Nearest data sets
identification

Top r algorithms
recommendation

Ranks

FSS algorithms
ranking

Nearest
data sets

Metric
collection

Performance
metrics

Figure 1: Framework of feature subset selection algorithm recommendation
1) Meta-Knowledge Database Construction
As mentioned previously, the meta-knowledge database consists of the meta-features of
a set of historical data sets and the performance of a group of FSS algorithms on them.
It is the foundation of our proposed recommendation method, and the effectiveness of the
recommendations depends heavily on this database.
The meta-knowledge database is constructed by the following three steps. Firstly, the
meta-features in Table 1 are extracted from each historical data set by the module Metafeatures extraction. Then, each candidate FSS algorithm is applied on each historical data
set. The classification accuracy, the runtime of feature selection and the number of selected
features are recorded, and the corresponding value of the performance metric EARR is
calculated. This is accomplished by the module Performance metric calculation. Finally,
for each data set, a tuple, which is composed of the meta-features and the values of the
performance metric EARR for all the candidate FSS algorithms, is obtained and added into
the knowledge database.
2) FSS Algorithm Recommendation
7

fiWang, Song, Sun, Zhang, Xu & Zhou

Based on the introduction of the first part Meta-knowledge Database Construction
we presented above, the learning target of the meta-knowledge data is a set of EARR values
instead of an appropriate FSS algorithm. In this case, it has been demonstrated that the
researchers usually resort to the instance-based or k -NN (nearest neighbors) methods or
their variations (Brazdil et al., 2003, 2008) for algorithm recommendation. Thus, a k -NN
based FSS algorithm recommendation procedure is proposed.
When recommending FSS algorithms for a new data set, firstly, the meta-features of
this data set are extracted. Then, the distance between the new data set and each historical
data set is calculated according to the meta-features. After that, its k nearest data sets
are identified, and the EARR values of the candidate FSS algorithms on these k data sets
are retrieved from the meta-knowledge database. Finally, all the candidate FSS algorithms
are ranked according to these EARR values, where the algorithm with the highest EARR
achieves the top rank, the one with the second highest EARR gets second rank, and so
forth, and the top r algorithms are recommended.
3.2 Recommendation Method
To recommend appropriate FSS algorithms for a new data set Dnew based on its k nearest
data sets, there are two foundational issues to be solved: i) how to identify its k nearest
data sets, and ii) how to recommend appropriate FSS algorithms based on these k data
sets.
1) k nearest data sets identification
The k nearest data sets of Dnew are identified by calculating the distance between Dnew
and each historical data set based on their meta-features. The smaller the distance, the
more similar the corresponding data set to Dnew .
In order to effectively calculate the distance between two data sets, the L1 norm distance
(Atkeson, Moore, & Schaal, 1997) is adopted since it is easy to understand and calculate,
and its ability in measuring the similarity between two data sets has been demonstrated by
Brazdil et al. (2003).
Let Fi = <fi,1 , fi,2 ,    , fi,h > be the meta-features of data set Di , where fi,p is the
value of pth feature of Fi and h is the length of the meta-features. The L1 norm distance
between data sets Di and Dj can be formulated as
dist(Di , Dj ) = kFi  Fj k1 =

h
X

|fi,p  fj,p |.

(3)

p=1

It is worth noting that the ranges of different meta-features are quite different. For example,
of the meta-features introduced in Table 1, the value of normalized class entropy varies from
0 to 1, while the number of instances can be millions. Thus, if these meta-features with
different ranges are directly used to calculate the distance between two data sets, the metafeatures with large range would dominate the distance, and the meta-features with small
range will be ignored. In order to avoid this problem, the 0-1 standardized method (Eq.
(4)) is employed to make all the meta-features have the same range [0, 1].
fi,p  min (f,p )
,
max (f,p )  min (f,p )
8

(4)

fiSubset Selection Algorithm Automatic Recommendation

where fi,p (1  p  h) is the value of the pth meta-feature of data set Di , min (f,p ) and
max (f,p ) denote the minimum and maximum values of the pth meta-feature over historical
data sets, respectively.
2) FSS algorithm recommendation
Once getting the k nearest data sets of Dnew , the performance of the candidate FSS
algorithms on Dnew is evaluated according to those on the k nearest data sets. Then, the
algorithms with the best performance are recommended.
D
Let Dknn = {D1 , D2 ,    , Dk } be the k nearest data sets of Dnew and EARR Aij be the
performance metric of the FSS algorithm Ai on data set Dj  Dknn (1  j  k). Then the
performance of Ai on the new data set Dnew can be evaluated by
knn
EARRD
Ai

=

k
X

j 

D
EARRAij ,

where j = dj

1

k
X
dt 1 , dj = dist(Dnew , Dj ).
/

(5)

t=1

j=1

Eq. (5) indicates that the performance of the FSS algorithm Ai on Dnew is evaluated by
its performance on the Dknn of Dnew . For a data set Dj  Dknn , the smaller the distance
dj between itself and Dnew , the more similar the two data sets. This means for two data
sets Dp and Dq , if dp < dq then the data set Dp is more similar to Dnew , so the EARR
of Ai on Dp is more important for evaluating the performance of Ai on Dnew . Thus, the
weighted average, which takes into account the relative importance of each data set in Dknn
rather than treating each data set equally, is employed. Moreover, in the domain of machine
learning, the reciprocal of the distance usually is used to measure the similarity. So the
k
P
j = dj 1 / dt 1 is used as the weight of the EARR of Ai on Dj  Dknn .
t=1

According to the EARR of each candidate FSS algorithm in ASet on Dnew , a rank of
these candidate FSS algorithms can be obtained. The greater the EARR, the higher the
rank. Then, the top r (e.g., r = 3 in this paper) FSS algorithms are picked up as the
appropriate ones for the new data set Dnew .
Procedure FSSAlgorithmRecommendation shows the pseudo-code of the recommendation.
Time complexity. The recommendation procedure consists of two parts. In the first part
(lines 1-3), the k nearest data sets of the given new data set D are identified. Firstly,
the meta-features F of D are extracted by function MetaFeatureExtraction(). Then, the
k-nearest historical data sets are identified by function NeighborRecognition() based on
the distance between F and the meta-features Fi of each historical data set Di . Suppose
that P is the number of instances and Q is the number of features in the given data set
D, the time complexity of function MetaFeatureExtraction() is O(P + Q). For function
NeighborRecognition(), the time complexity is O(n) which depends on the number of the
historical data sets n. Consequently, the time complexity of the first part is O(P +Q)+O(n).
In the second part (lines 4-8), the r FSS algorithms are recommended for the data set
D. Since the weights and EARRs of the k nearest data sets can be obtained directly, the
time complexity of these two steps is O(1). The time complexity for estimating and ranking
the EARRs of the algorithms in ASet is O(k  m) + O(m  log(m)), where k is the preassigned
number of the nearest data sets and m is the number of the candidate FSS algorithms.
9

fiWang, Song, Sun, Zhang, Xu & Zhou

Procedure FSSAlgorithmRecommendation
Inputs :
D - a new given data set;
DSet - {D1 , D2 ,    , Dn }, historical data sets;
ASet - {A1 , A2 ,    , Am }, candidate FSS algorithms;
MetaDataBase - {<Fi , EARRsi >|1  i  n} where Fi and EARRs i are the
meta-features and the EARRs of ASet on Di , respectively;
k - the predefined number of the nearest neighbors;
r - the number of recommended FSS algorithms.
Output: RecAlgs - Recommended FSS algorithms for D
//Part 1: Recognition of the k nearest data sets for D
1 F = MetaFeatureExtraction (D);//Extract meta-features from D
2 MetaFeatureSet = {F1 , F2 ,    , Fn };//Meta-feature of each data set in DSet
3 Neighbors = NeighborRecognition (k, F, MetaFeatureSet);
//Part 2: Appropriate FSS algorithm recommendation
4 WeightSet = calculate the weight for each data set in Neighbors //See Eq. (5)
5 EARRSet = the corresponding EARRs for each data set in Neighbors from MetaDataBase;
6 Estimate the EARR of each FSS algorithm  ASet on D according to WeightSet and EARRSet

by Eq. (5) and rank the algorithms in ASet based on these EARRs;
7 RecAlgs = top r FSS algorithms;
8 return RecAlgs;

To sum up, the time complexity of the recommendation procedure is O(P + Q) + O(n) +
O(km)+O(mlog(m)). In practice, for a data set D that needs to conduct feature selection,
the number of the instances P and/or the number of the features Q in D are much greater
than the number of the nearest data sets k and the number of the candidate FSS algorithms
m, so the major time consumption of this recommendation procedure is determined by the
first part.

4. Experimental Results and Analysis
In this section, we experimentally evaluate the proposed feature subset selection (FSS)
algorithm recommendation method by recommending algorithms over the benchmark data
sets.
4.1 Benchmark Data Sets
115 extensively-used real world data sets, which come from different areas such as Computer,
Image, Life, Biology, Physical and Text 4 , are employed in our experiment. The sizes of
these data sets vary from 10 to 24863 instances, and the numbers of their features are
between 5 and 27680.
The statistical summary of these data sets is shown in Table 2 in terms of the number
of instances (denoted as I), the number of features (denoted as F) and the number of target
concepts (denoted as T).
4. These data sets are available from http://archive.ics.uci.edu/ml/datasets.html, http://
featureselection.asu.edu/datasets.php, http://sci2s.ugr.es/keel/datasets.php, http://www.
upo.es/eps/bigs/datasets.html, and http://tunedit.org/repo/Data, respectively.

10

fiSubset Selection Algorithm Automatic Recommendation

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58

Data Name
ada agnostic
ada prior
anneal
anneal ORIG
AR10P 130 674
arrhythmia
audiology
autos
balance-scale
breast-cancer
breast-w
bridges version1
bridges version2
car
CLL-SUB-111 111 2856
cmc
colic
colic.ORIG
colon
credit-a
credit-g
cylinder-bands
dermatology
diabetes
ECML90x27679
ecoli
Embryonaldataset C
eucalyptus
flags
GCM Test
gina agnostic
gina prior
gina prior2
glass
grub-damage
heart-c
heart-h
heart-statlog
hepatitis
hypothyroid
ionosphere
iris
kdd ipums la 97-small
kdd ipums la 98-small
kdd ipums la 99-small
kdd JapaneseVowels test
kdd JapaneseVowels train
kdd synthetic control
kr-vs-kp
labor
Leukemia
Leukemia 3c
leukemia test 34x7129
leukemia train 38x7129
lung-cancer
lymph
Lymphoma45x4026+2classes
Lymphoma96x4026+10classes

I
4562
4562
898
898
130
452
226
205
625
286
699
107
107
1728
111
1473
368
368
62
690
1000
540
366
768
90
336
60
24863
194
46
3468
3468
3468
214
155
303
294
270
155
3772
351
150
7019
7485
8844
5687
4274
600
3196
57
72
72
34
38
32
148
45
96

F
49
15
39
39
675
280
70
26
5
10
10
13
13
7
2857
10
23
28
2001
16
21
40
35
9
27680
8
7130
249
30
16064
971
785
785
10
9
14
14
14
20
30
35
5
61
61
61
15
15
62
37
17
7130
7130
7130
7130
57
19
4027
4027

T
2
2
6
6
10
16
24
7
3
2
2
6
6
4
3
3
2
2
2
2
2
2
6
2
43
8
2
12
8
14
2
2
10
7
4
5
5
2
2
4
2
3
9
10
9
9
9
6
2
2
2
3
2
2
3
4
2
11

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115

Data Name
Lymphoma96x4026+9classes
mfeat-fourier
mfeat-morphological
mfeat-pixel
mfeat-zernike
molecular-biology promoters
monks-problems-1 test
monks-problems-1 train
monks-problems-2 test
monks-problems-2 train
monks-problems-3 test
monks-problems-3 train
mushroom
oh0.wc
oh10.wc
oh15.wc
oh5.wc
pasture
pendigits
PIE10P 210 1520
postoperative-patient-data
primary-tumor
segment
shuttle-landing-control
sick
SMK-CAN-187 187 1815
solar-flare 1
solar-flare 2
sonar
soybean
spectf test
spectf train
spectrometer
spect test
spect train
splice
sponge
squash-stored
squash-unstored
sylva agnostic
sylva prior
TOX-171 171 1538
tr11.wc
tr12.wc
tr23.wc
tr31.wc
tr41.wc
tr45.wc
trains
vehicle
vote
vowel
wap.wc
waveform-5000
white-clover
wine
zoo

I
96
2000
2000
2000
2000
106
432
124
432
169
432
122
8124
1003
1050
913
918
36
10992
210
90
339
2310
15
3772
187
323
1066
208
683
269
80
531
187
80
3190
76
52
52
14395
14395
171
414
313
204
927
878
690
10
846
435
990
1560
5000
63
178
101

F
4027
77
7
241
48
59
7
7
7
7
7
7
23
3183
3239
3101
3013
23
17
1521
9
18
20
7
30
1816
13
13
61
36
45
45
103
23
23
62
46
25
24
217
109
1538
6430
5805
5833
10129
7455
8262
33
19
17
14
8461
41
32
14
18

T
9
10
10
10
10
2
2
2
2
2
2
2
2
10
10
10
10
3
10
10
3
22
7
2
2
2
2
3
2
19
2
2
48
2
2
3
3
3
3
2
2
4
9
8
6
7
10
10
2
4
2
11
20
3
4
3
7

Table 2: Statistical summary of the 115 data sets

4.2 Experimental Setup
In order to evaluate the performance of the proposed FSS algorithm recommendation
method, further verify whether the proposed method is potentially useful in practice, and
confirm the reproducibility of our experiments, we set the experimental study as follows.
4.2.1 FSS Algorithms
FSS algorithms can be grouped into two broad categories: Wrapper and Filter (Molina et al.,
2002; Kohavi & John, 1997). The Wrapper method uses the error rate of the classification
algorithm as the evaluation function to measure a feature subset, while the evaluation
function of the Filter method is independent of the classification algorithm. The accuracy
of the Wrapper method is usually high; however, the generality of the result is limited,
and the computational complexity is high. In comparison, Filter method is of generality,
and the computational complexity is low. Due to the fact that the Wrapper method is
computationally expensive (Dash & Liu, 1997), the Filter method is usually a good choice
11

fiWang, Song, Sun, Zhang, Xu & Zhou

when the number of features is very large. Thus, we focus on the Filter method in our
experiment.
A number of Filter based FSS algorithms have been proposed to handle feature selection
problems. These algorithms can be significantly distinguished by i) the search method used
to generate the feature subset being evaluated, and ii) the evaluation measures used to assess
the feature subset (Liu & Yu, 2005; de Souza, 2004; Dash & Liu, 1997; Pudil, Novovicova,
& Kittler, 1994).
In order to guarantee the generality of our experimental results, twelve well-known or
the latest search methods and four representative evaluation measures are employed. The
brief introduction of these search methods and evaluation measures is as follows.
1) Search methods
i) Sequential forward search (SFS): Starting from the empty set, sequentially add the
feature which results in the highest value of objective function into the current
feature subset.
ii) Sequential backward search (SBS): Starting from the full set, sequentially eliminate
the feature which results in smallest or no decrease in the value of objective function
from the current feature subset.
iii) Bi-direction search (BiS): A parallel implementation of SFS and SBS. It searches
the feature subset space in two directions.
iv) Genetic search (GS): A randomized search method which performs using a simple
genetic algorithm (Goldberg, 1989). The genetic algorithm finds the feature subset
to maximize special output function using techniques inspired by natural evolution.
v) Linear search (LS): An extension of BestFirst search (Gutlein, Frank, Hall, & Karwath, 2009) which searches the space of feature subsets by greedy hill-climbing
augmented with a backtracking facility.
vi) Rank search (RS) (Battiti, 1994): It uses a feature evaluator (such as gain ratio)
to rank all the features. After a feature evaluator is specified, a forward selection
search is used to generate a ranking list.
vii) Scatter search (SS) (Garcia Lopez, Garcia Torres, Melian Batista, Moreno Perez,
& Moreno-Vega, 2006): This method performs a scatter search through the feature
subset space. It starts with a population of many significant and diverse feature
subsets, and stops when the assessment criteria is higher than a given threshold or
does not have improvement any longer.
viii) Stepwise search (SWS) (Kohavi & John, 1997): A variation of the forward search.
At each step in the search process, after a new feature is added, a test is performed
to check if some features can be eliminated without significant reduction in the
output function.
ix) Tabu search (TS) (Hedar, Wang, & Fukushima, 2008): It is proposed for combinatorial optimization problems. It is an adaptive memory and responsive exploration
by combining a local search process with anti-cycling memory-based rules to avoid
trapping in local optimal solutions.
12

fiSubset Selection Algorithm Automatic Recommendation

x) Interactive search (Zhao & Liu, 2007): It traverses the feature subset space for
maximizing the target function while taking consideration the interaction among
features.
xi) FCBF search (Yu & Liu, 2003): It evaluates features via the relevance and redundancy analysis, and uses the analysis results as guideline to choose features.
xii) Ranker (Kononenko, 1994; Kira & Rendell, 1992; Liu & Setiono, 1995): It evaluates
each feature individually and ranks the features by the values of their evaluation
metrics.
2) Evaluation measures
i) Consistency (Liu & Setiono, 1996; Zhao & Liu, 2007): This kind of measure evaluates the worth of a feature subset by the level of consistency in the target concept
when the instances are projected onto the feature subset. The consistency of any
feature subset can never be lower than that of the full feature set.
ii) Dependency (Hall, 1999; Yu & Liu, 2003): This kind of measure evaluates the worth
of a subset of features by considering the individual predictive ability of each feature
along with the degree of redundancy among these features. The FSS methods based
on this kind of measure assume that good feature subsets contain features closely
correlated with the target concept, but uncorrelated with each other.
iii) Distance (Kira & Rendell, 1992; Kononenko, 1994): This kind of measure is proposed based on the assumption that the distance of instances from different target
concepts is greater than that from same target concepts.
iv) Probabilistic significance (Zhou & Dillon, 1988; Liu & Setiono, 1995): This measure
evaluates the worth of a feature by calculating the probabilistic significance as a
two-way function, i.e., the association between feature and the target concept. A
good feature should have significant association with the target concept.
We should pay attention to that, besides the above four evaluation measures, there is
another basic kind of measure: information-based measure (Liu & Yu, 2005; de Souza, 2004;
Dash & Liu, 1997), which is not contemplated in the experiments. The reason is demonstrated as follow. The information-based measure is usually in conjunction with ranker
search method. Thus, the FSS algorithms based on this kind of measure usually provide a
rank list of the features instead of telling us which features are relevant to the learning target. In this case, we should preassign particular thresholds for these FSS algorithms to pick
up the relevant features. However, there is not any effective method to set the thresholds
or any acknowledged default threshold for these FSS algorithms. Moreover, it is unfair to
conclude that these information measure based FSS algorithms with any assigned threshold
are not appropriate when comparing to the other FSS algorithms. Therefore, this kind of
FSS algorithm is not employed in our experiments.
Based on the search methods and the evaluation measures introduced above, 22 different
FSS algorithms are obtained. Table 3 shows the brief introduction of these FSS algorithms.
Where all these algorithms are available in the data mining toolkit Weka5 (Hall, Frank,
5. http://www.cs.waikato.ac.nz/ml/weka/

13

fiWang, Song, Sun, Zhang, Xu & Zhou

Holmes, Pfahringer, Reutemann, & Witten, 2009), and the search method INTERACT is
implemented based on Weka and its source codes are available online6 .
ID
1
2
3
4
5
6
7
8
9
10
11

Search Method
BestFirst + Sequential Forward Search
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Tabu Search
Interactive Search
BestFirst + Sequential Forward Search

Evaluation Measure
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Consistency

Notation
CFS-SFS
CFS-SBS
CFS-BiS
CFS-GS
CFS-LS
CFS-RS
CFS-SS
CFS-SWS
CFS-TS
INTERACT-D
Cons-SFS

ID
12
13
14
15
16
17
18
19
20
21
22

Search Method
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Interactive Search
FCBFsearch
Ranker
Ranker

Evaluation Measure
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Dependency
Distance
Probabilistic Significance

Notation
Cons-SBS
Cons-BiS
Cons-GS
Cons-LS
Cons-RS
Cons-SS
Cons-SWS
INTERACT-C
FCBF
Relief-F
Signific

Table 3: Introduction of the 22 FSS algorithms
It is noted that some of these algorithms require particular settings of certain parameters. For the purpose of allowing other researchers to confirm our results, we introduce the
parameter settings of these FSS algorithms. Such as, for FSS algorithms INTERACT-D
and INTERACT-C, there is a parameter, c-contribution threshold, used to identify the
irrelevant features. We set this threshold as 0.0001 suggested by Zhao and Liu (2007). For
FSS algorithm FCBF, we set the relevance threshold to be the SU (Symmetric Uncertainty) value of the bN/ log N cth ranked feature suggested by Yu and Liu (2003). For FSS
algorithm Relief-F, we set the significance threshold to 0.01 used by Robnik-Sikonja and
Kononenko (2003). For FSS algorithm Signific, there is a threshold, statistical significance level , used to identify the irrelevant features. We set  as the commonly-used value
0.01 in our experiment. The other FSS algorithms are conducted in the Weka environment
with the default setting(s).
4.2.2 Classification Algorithms
Since the actual relevant features of real world data sets are usually not known in advance, it
is impracticable to directly evaluate an FSS algorithm by the selected features. Classification
accuracy is an extensively used metric for evaluating the performance of FSS algorithms,
and also plays an important role in our proposed performance metric EARR for assessing
different FSS algorithms.
However, different classification algorithms have different biases. An FSS algorithm may
be more suitable for some classification algorithms than others (de Souza, 2004). This fact
affects the performance evaluation of FSS algorithms.
With this in mind, in order to demonstrate that our proposed FSS algorithm recommendation method is not limited to any particular classification algorithm, five representative
classification algorithms based on different hypotheses are employed. They are bayes-based
Naive Bayes (John & Langley, 1995) and Bayes Network (Friedman, Geiger, & Goldszmidt,
1997), information gain-based C4.5 (Quinlan, 1993), rule-based PART (Frank & Witten,
1998), and instance-based IB1 (Aha, Kibler, & Albert, 1991), respectively.
Although Naive Bayes and Bayes Net are both bayes-based classification algorithms,
they are quite different from each other since Naive Bayes is proposed based on the hypoth6. http://www.public.asu.edu/huanliu/INTERACT/INTERACTsoftware.html

14

fiSubset Selection Algorithm Automatic Recommendation

esis that the features are conditional independent (John & Langley, 1995), while Bayes Net
takes into account the feature interaction (Friedman et al., 1997).
4.2.3 Measures to Evaluate the Recommendation Method
FSS algorithm recommendation is an application of meta-learning. So far as we know,
there are no unified measures to evaluate the performance of the meta-learning methods.
In order to assess our proposed FSS algorithm recommendation method, two measures,
recommendation hit ratio and recommendation performance ratio, are defined.
Let D be a given data set and Arec be an FSS algorithm recommended by the recommendation method for D, these two measures can be introduced as follows.
1) Recommendation hit ratio
An intuitive evaluation criterion is whether the recommended FSS algorithm Arec meets
users requirements. That is, whether Arec is the optimal FSS algorithm for D, or the performance of Arec on D has no significant difference with that of the optimal FSS algorithm.
Suppose Aopt represents the optimal FSS algorithm for D, and ASetopt denotes the FSS
algorithm set in which each algorithm has no significant difference with Aopt (of course it
includes Aopt as well). Then, a measure named recommendation hit can be defined to assess
whether the recommended algorithm Arec is effective on D.
Definition 1 (Recommendation hit). If an FSS algorithm Arec is recommended to a data
set D, then the recommendation hit Hit(Arec , D) is defined as
(
1, if Arec  ASetopt
Hit(Arec , D) =
.
(6)
0, otherwise
Where Hit(Arec , D) = 1 means the recommendation is effective since the recommended
FSS algorithm Arec is one of the algorithms in ASetopt for D, while hit(Arec , D) = 0 indicates
the recommended FSS algorithm Arec is not a member of ASetopt , i.e., Arec is significantly
worse than the optimal FSS algorithm Aopt on D, thus the recommendation is bad.
From Definition 1 we know that the recommendation hit Hit(Arec , D) is used to evaluate
the recommendation method for a single data set. Thus, it is extended as recommendation
hit ratio to evaluate the recommendation for a set of data sets, and is defined as follows.
Definition 2 Recommendation hit ratio
G

1 X
Hit Ratio(Arec ) =
Hit(Arec , Di ).
G

(7)

i=1

Where G is the number of the historical data sets, e.g., G = 115 in our experiment.
Definition 2 represents the percentage of data sets on which the appropriate FSS algorithms are effectively recommended by our recommendation method. The larger this value,
the better the recommendation method.
2) Recommendation performance ratio
The recommendation hit ratio reveals that whether or not an appropriate FSS algorithm
is recommended for a given data set, but it cannot tell us the margin of the recommended
algorithm to the best one. Thus, a new measure, the recommendation performance ratio
for a recommendation, is defined.
15

fiWang, Song, Sun, Zhang, Xu & Zhou

Definition 3 (Recommendation performance ratio). Let EARRrec and EARRopt be the
performance of the recommended FSS algorithm Arec and the optimal FSS algorithm on D,
respectively. Then, the recommendation performance ratio (RPR) for Arec is defined as
RPR(Arec , D) = EARRrec /EAARopt .

(8)

In this definition, the best performance EARR opt is employed as a benchmark. Without
the benchmark, it is hard to determine the recommended algorithms are good or not.
For example, suppose the EARR of Arec on D is 0.59. If EARR opt = 0.61, then the
recommendation is effective since EARR of Arec is very close to EARR opt . However, the
recommendation is poor if EARR opt = 0.91.
RPR is the ratio of EARR of a recommended FSS algorithm to that of the optimal one.
It measures how close the recommended FSS algorithm to the optimal one, and reveals the
relative performance of the recommended FSS algorithm. Its value varies from 0 to 1, and
the larger the value of RPR, the closer the performance of the recommended FSS algorithm
to that of the optimal one. The recommended algorithm is the optimal one if and only if
RPR = 1.
4.2.4 Values of the Parameters  and 
In this paper, a multi-criteria metric EARR is proposed to evaluate the performance of an
FSS algorithm. For the proposed metric EARR, two parameters  and  are established
for users to express their requirements on algorithm performance.
In the experiment, when presenting the results, two representative value pairs of parameters  and  are used as follows:
1)  = 0 and  = 0. This setting represents the situation where the classification
accuracy is most important. The higher the classification accuracy over the selected features,
the better the corresponding FSS algorithms.
2)  6= 0 and  6= 0. This setting represents the situation where the user can tolerate
an accuracy attenuation and favor the FSS algorithms with shorter runtime and fewer
selected features. In the experiment, both  and  are set to 10% that is quite different
from  =  = 0. This allows us can explore the impact of these two parameters on the
recommendation results.
Moreover, in order to explore how parameters  and  affect the recommended FSS
algorithms in terms of classification accuracy, runtime and the number of selected features,
different parameters settings are provided. Specifically, the values of  and  vary from 0
to 10% with an increase of 1%.
4.3 Experimental Process
In order to make sure the soundness of our experimental conclusion and guarantee the
experiments reported being reproducible, in this part, we introduce the four crucial processes
used in our experiments. They are i) meta-knowledge database construction, ii) optimal
FSS algorithm set identification for a given data set, iii) Recommendation method validation
and iv) sensitivity analysis of the number of the nearest data sets on recommendations.
1) Meta-knowledge database construction
16

fiSubset Selection Algorithm Automatic Recommendation

Procedure PerformanceEvaluation
Inputs : data = a given data set, i.e, one of the 115 data sets;
learner = a given classification algorithm, i.e., one of {Naive Bayes, C4.5, PART,
IB1 or Bayes Network};
FSSAlgSet = {FSSAlg 1 , FSSAlg 2 ,    , FSSAlg 22 }, the set of the 22 FSS
algorithms;
Output: EARRset = {EARR 1 , EARR 2 ,    , EARR 22 }, the EARRs of the 22 FSS
algorithms on data;
1 M = 5; FOLDS = 10;
2 for i = 1 to 22 do
3
EARR i = 0;
4 for i = 1 to M do
5
randomized order from data;
6
generate FOLDS bins from data;
7
for j = 1 to FOLDS do
8
TestData = bin[j ];
9
TrainData = data- TestData;
10
numberList = Null , runtimeList = Null , accuracyList = Null ;
11
for k = 1 to 22 do
12
(Subset, runtime) = apply FSSAlg k on TrainData;
13
number = |Subset |;
14
RedTestData = reduce TestData according to selected Subset;
15
RedTrainData = reduce TrainData according to selected Subset;
16
classifier = learner (RedTrainData);
17
accuracy = apply classifier to RedTestData;
18
numberList [k ] = number , runtimeList [k ] = runtime, accuracyList [k ] =

accuracy;
19
20
21

for k = 1 to 22 do
EARR = EARRCompution(accuracyList, runtimeList, numberList, k );
//Compute EARR of FSSAlg k on jth bin of pass i according Eqs. (1) and (2)
EARR k = EARR k + EARR;

22 for i  1 to 22 do
23
EARR i = EARR i /(M FOLDS );
24 return EARRset;

For each data set Di (1  i  115), we i) extract its meta-features Fi ; ii) calculate the
EARRs for the 22 candidate FSS algorithms with the stratified 510-fold cross-validation
strategy (Kohavi, 1995), and iii) combine the meta-features Fi and the EARR of each FSS
algorithm together to form a tuple, which is finally added to the meta-knowledge database.
Since the extraction of meta-features and the combination of the meta-features and the
EARRs are straightforward, we just present the calculation of EARRs, procedure PerformanceEvaluation shows the details.
2) Optimal FSS algorithm set identification
The optimal FSS algorithm set for a given data set Di consists of the optimal FSS algorithm for this data set and those algorithms that have no significant performance difference
with the optimal one on Di .
The optimal FSS algorithm set for a given data set Di is obtained via a non-parametric
Friedman test (1937) followed by a Holm procedure test (1988) on the performance, which
17

fiWang, Song, Sun, Zhang, Xu & Zhou

is estimated by the 510 cross validation strategy, of the 22 FSS algorithms. If the result
of the Friedman test shows that there is no significant performance difference among the
22 FSS algorithms, these 22 FSS algorithms are added to the optimal FSS algorithm set.
Otherwise, the FSS algorithm with the highest performance is viewed as the optimal one
and added to the optimal FSS algorithm set. Then, the Holm procedure test is performed
to identify the algorithms from the rest 21 FSS algorithms. The algorithms that have no
significant performance differences with the optimal one are added into the optimal FSS
algorithm set.
The reason why the non-parametric test is employed lies in that it is difficult for the
performance values to follow the normal distribution and satisfy variance homogeneous
condition.
Note that the optimal FSS algorithm sets for different settings of parameters  and  are
different, since the values of these two parameters directly affect the required performance
values.
3) Recommendation method validation
The leave-one-out strategy is used to empirically evaluate our proposed FSS algorithm recommendation method as follows: for each data set Di (1  i  115) that
is viewed as the test data, i) identify its k nearest data sets from the training data =
{D1 ,    , Di1 , Di+1 ,    , D115 } excluding Di ; ii) calculate the performance of the 22 candidate FSS algorithms according to Eq. (5) based on the k nearest data sets where the value
of k is determined by the standard cross-validation strategy, and recommend the top three
to Di ; and iii) evaluate the recommendations by the measures introduced in section 4.2.3.
4) Sensitivity analysis of the number of the nearest data sets on recommendations
In order to explore the effect of the number of the nearest data sets on the recommendations and provide users an empirical method to choose its value, for each data set, all
the possible numbers of the nearest data sets are tested. That is, when identifying the k
nearest data sets for a given data set, k is set from 1 to the number of the historical data
sets minus 1 (e.g., 114 in this experiment).
4.4 Results and Analysis
In this section, we present the recommendation results in terms of recommended FSS algorithms, hit ratio and performance ratio , respectively. Due to the space limit of the
paper, we do not list all the recommendations, but instead present the results under two
significantly different pairs of  and , i.e., ( = 0,  = 0) and ( = 10%,  = 10%).
Afterward, we also provide the experimental results of the influence of the user-oriented
parameters  and  on recommendations in terms of classification accuracy, runtime, and
the number of selected features, respectively.
4.4.1 Recommended Algorithms and Hit Ratio
Figs. 2, 3, 4, 5 and 6 show the first recommended FSS algorithms for the 115 data sets
when the classification algorithms Naive Bayes, C4.5, PART, IB1 and Bayes Network are
used, respectively.
18

fiSubset Selection Algorithm Automatic Recommendation

In each figure, there are two sub-figures corresponding to the recommendation results
for ( = 0,  = 0) and ( = 10%,  = 10%), respectively. In each sub-figure,  and 
denote the correctly and incorrectly recommended algorithms, respectively.

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 2: FSS algorithms recommended for the 115 data sets when Naive Bayes is used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 3: FSS algorithms recommended for the 115 data sets when C4.5 is used
From these figures, we observe that:
1) For all the five classifiers, the proposed method can effectively recommend appropriate
FSS algorithms for most of the 115 data sets.
In the case of ( = 0,  = 0), the number of data sets, whose appropriate FSS
algorithms are correctly recommended, is 109 out of 115 for Naive Bayes, 111 out of 115
for C4.5, 109 out of 115 for PART, 108 out of 115 for IB1, and 109 out of 115 for Bayes
19

fiWang, Song, Sun, Zhang, Xu & Zhou

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 4: FSS algorithms recommended for the 115 data sets when PART is used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 5: FSS algorithms recommended for the 115 data sets when IB1 is used

Network, respectively. This states that the recommendation method is effective when
classification accuracy is most important.
In the case of ( = 10%,  = 10%), the number of data sets, whose appropriate FSS
algorithms are correctly recommended, is 104 out of 115 for Naive Bayes, 109 out of 115
for C4.5, 110 out of 115 for PART, 106 out of 115 for IB1, and 104 out of 115 for Bayes
Network, respectively. This indicates that the recommendation method also works well
when tradeoff is required among classification accuracy, runtime, and the number of
selected features.
2) The distribution of the recommended FSS algorithms for the 115 data sets is different for
different parameters settings. The distribution is relatively uniform for ( = 0,  = 0),
20

fiSubset Selection Algorithm Automatic Recommendation

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 6: FSS algorithms recommended for the 115 data sets when Bayes Network is used

while it is seriously biased to some algorithm (e.g., the 22th FSS algorithm) for ( =
10%,  = 10%).
This phenomenon is similar for all the five classifiers. This can be explained as follows.
The FSS algorithms with the best classification accuracy distribute on the 115 data sets
uniformly. Thus, in the case of ( = 0,  = 0) where users favor accurate classifiers, the
distribution of the recommended FSS algorithms is relatively uniform as well. However,
there exist some FSS algorithms that run faster (e.g., the 22th algorithm Signific)
or select fewer features (e.g., the 8th algorithm CFS-SWS, the 18th algorithm ConsSWS, and the 22th algorithm Signific) on most of the 115 data sets. For this reason, in
the case of ( = 10%,  = 10%) where users prefer the FSS algorithms with less runtime
and fewer features, the distribution of the FSS algorithms with the best performance on
the 115 data sets is biased to some algorithms, so is the recommended FSS algorithms.
3) The 22th FSS algorithm performs well on about 85 out of 115 data sets for all classifiers
when ( = 10%,  = 10%). It seems that this FSS algorithm is a generally wellperformed FSS algorithm that can be adopted by all FSS tasks and there is no need
for FSS algorithm recommendation. Unfortunately, this is not the case. The 22th FSS
algorithm is still failing to perform well over about a quarter of the 115 data sets.
Yet, our recommendation method can distinguish these data sets and further effectively
recommend appropriate FSS algorithms for them. This indicates our recommendation
method is still necessary in this case.
Compared with ( = 0,  = 0), we can know that this case is due to the larger  and
 values and can be explained as follows. For all the 22 FSS algorithms, although the
classification accuracies of a classifier over the features selected by them are different,
the differences are usually bounded. Meanwhile, from Eq. (1) we know that when /
is set to be greater than the bound value, the value of EARR will be dominated by
the runtime/the number of selected features. This means that if  or  is set to be
a relatively large value, the algorithm with a lower time complexity or the algorithm
that chooses smaller number of features will be recommended, and the classification
21

fiWang, Song, Sun, Zhang, Xu & Zhou

accuracy over the selected features will be ignored. However, as we know, one of the
most important targets of feature selection is to improve the performance of learning
algorithms. So it is unreasonable to ignore the classification accuracy and just focus on
the speed and the simplicity of an FSS algorithm.
Thus, in real applications, the values of  and  should be set under the limit of classification accuracies. Generally, the / should be bounded by (accmax  accmin )/accmin ,
where accmax and accmin denote the maximum and the minimum classification accuracies, respectively.
Parameter setting
 = 0,  = 0

 = 10%,  = 10%


Recommendation
Alg1st
Alg2nd
Alg3rd
Top 3
Alg1st
Alg2nd
Alg3rd
Top 3

Naive Bayes
94.78
83.48
74.78
99.13
90.43
71.30
38.26
99.13

C4.5
96.52
79.13
80.87
98.26
94.78
69.57
45.22
100.0

PART
94.78
92.17
84.35
99.13
94.78
70.43
42.61
100.0

IB1
93.91
77.39
75.65
99.13
92.17
64.35
43.48
100.0

Bayes Network
94.78
83.48
73.91
98.26
90.43
71.30
36.52
99.13

Algx denotes only the x -th algorithm in the ranking list is recommended while Top 3 means the top three
algorithms are recommended.

Table 4: Hit ratio (%) of the recommendations for the five classifiers under different settings
of (, )
Table 4 shows the hit ratio of the recommended FSS algorithms for the five classifiers.
From it we can observe that:
1) If a single FSS algorithm is recommended, the hit ratio of the first recommended algorithm Alg1st is the highest, its value is up to 96.52% and at least is 90.43% for all the
five classifiers. Thus, Alg1st should be the first choice.
2) If the top three algorithms are recommended, the hit ratio is up to 100% and at least
is 98.62%. That indicates that the confidence of the top three algorithms including an
appropriate one is very high. This is the reason why only the top three algorithms
are recommended. Moreover, the proposed recommendation method has reduced the
number of candidate algorithms to three, users can further pick up the one that fits
his/her specific requirement from them.
4.4.2 Recommendation Performance Ratio
Figs. 7 and 8 show the recommendation performance ratio RPR of the first recommended
FSS algorithm for the five classifiers with ( = 0,  = 0) and ( = 10%,  = 10%),
respectively. From these two figures we can observe that, for most data sets and the two
settings of  and , the RPRs of the recommended FSS algorithms are greater than 95%
and some of them are up to 100% no matter which classifier is used. This indicates that the
FSS algorithms recommended by our proposed method are very close to the optimal one.
Table 5 shows the average RPRs over the 115 data sets for the five classifiers under
different settings of (, ). In this table, for each classifier, columns Rec and Def
shows the RPR value corresponding to the recommended FSS algorithms and default FSS
algorithms, respectively. Where the default FSS algorithm is the most frequent best one on
the 115 data sets under the classifier.
22

fiSubset Selection Algorithm Automatic Recommendation

RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 7: RPR of the 1 st recommended FSS algorithm with ( = 0,  = 0) for the five
classifiers
RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 8: RPR of the 1 st recommended FSS algorithm with ( = 10%,  = 10%) for the
five classifiers

23

fiWang, Song, Sun, Zhang, Xu & Zhou

From it we observe that the average RPRs range from 97.32% to 98.8% for ( = 0,
 = 0), and from 97.82% to 98.99% for ( = 10%,  = 10%), respectively. Moreover,
the average RPR of the recommended FSS algorithms surpasses that of the default FSS
algorithms for all the five different classifiers. This means our proposed recommendation
method works very well and greatly fits users performance requirement.
Parameter setting
 = 0,  = 0
 = 10%,  = 10%

Naive bayes
Rec
Def
98.24 96.42
98.69 91.95

C4.5
Rec
Def
98.80
98.18
97.82
92.40

PART
Rec
Def
97.61 94.79
97.89 92.40

IB1
Rec
Def
97.32
96.43
98.11
92.35

Bayes Network
Rec
Def
98.37
96.63
98.99
92.43

Table 5: Average RPR (%) over 115 data sets for the five classifiers

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58


NB
0.0443
0.0227
0.0118
0.0079
0.0244
0.019
0.0091
0.0111
0.011
0.0091
0.0086
0.0062
0.0068
0.0077
0.0616
0.0099
0.0074
0.0083
0.0102
0.007
0.008
0.0103
0.008
0.0066
0.0088
0.0061
0.0097
0.0083
0.007
0.0273
0.2236
0.2602
0.3691
0.008
0.0084
0.0103
0.0065
0.0084
0.0098
0.0278
0.007
0.0138
0.1219
0.1453
0.1937
0.0225
0.0149
0.0101
0.0228
0.0069
0.0195
0.0202
0.0128
0.0128
0.0075
0.0084
0.0146
0.0432

C4.5
0.0425
0.0131
0.0124
0.0092
0.0253
0.019
0.0093
0.0062
0.0076
0.0087
0.007
0.0068
0.0085
0.0087
0.0582
0.0083
0.0126
0.0077
0.0078
0.0071
0.01
0.0079
0.0083
0.0064
0.0158
0.0068
0.0078
0.0069
0.0132
0.0272
0.2231
0.2616
0.3722
0.0103
0.0068
0.0068
0.0088
0.007
0.0352
0.0243
0.0099
0.006
0.1228
0.1427
0.1955
0.0232
0.0142
0.0125
0.0245
0.0075
0.0201
0.0207
0.0135
0.013
0.0073
0.0073
0.008
0.0464

PART
0.0499
0.0147
0.0123
0.0082
0.0257
0.0221
0.0093
0.0076
0.0072
0.0084
0.0072
0.0065
0.0064
0.0086
0.0575
0.0081
0.0076
0.0128
0.0105
0.007
0.009
0.011
0.0079
0.0067
0.0101
0.0075
0.0113
0.008
0.0093
0.0291
0.2236
0.2605
0.3689
0.0083
0.0065
0.0066
0.0086
0.0084
0.0069
0.0245
0.011
0.0106
0.1214
0.144
0.1972
0.0242
0.0125
0.0101
0.0191
0.0084
0.0196
0.0165
0.0116
0.0197
0.0069
0.007
0.0095
0.0449

IB1
0.0423
0.0137
0.0117
0.0114
0.0246
0.0192
0.0114
0.0066
0.0074
0.0138
0.0075
0.0063
0.0093
0.0084
0.058
0.0082
0.0084
0.0117
0.0067
0.0069
0.0075
0.0135
0.0093
0.0069
0.0082
0.0083
0.0117
0.0082
0.0072
0.0273
0.2239
0.262
0.3689
0.0113
0.0064
0.0066
0.0059
0.0093
0.0067
0.0246
0.007
0.0116
0.1231
0.145
0.194
0.0219
0.0139
0.0092
0.0198
0.0068
0.0209
0.0192
0.0156
0.0138
0.0065
0.006
0.0082
0.0436

BNet
0.0464
0.0129
0.0116
0.0096
0.0241
0.0208
0.0113
0.0088
0.0076
0.0073
0.0083
0.011
0.0096
0.0077
0.0586
0.009
0.0085
0.007
0.0091
0.0081
0.0079
0.0088
0.0072
0.0093
0.0094
0.0077
0.0123
0.0074
0.007
0.0272
0.2255
0.2596
0.3714
0.007
0.009
0.0069
0.0061
0.0085
0.0077
0.0249
0.0086
0.0063
0.1241
0.1434
0.1935
0.0228
0.015
0.0104
0.024
0.009
0.0197
0.0165
0.0158
0.0134
0.0068
0.0069
0.0103
0.0445

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
Average

NB
0.0446
0.0361
0.0087
0.0845
0.0225
0.0108
0.0117
0.0089
0.0092
0.0082
0.0061
0.0069
0.0611
0.203
0.1854
0.1195
0.1246
0.0147
0.0576
0.0685
0.0081
0.0086
0.0203
0.0095
0.0244
0.0683
0.0074
0.0084
0.0066
0.0096
0.0095
0.006
0.0158
0.0068
0.0097
0.0365
0.0108
0.0058
0.0082
0.4402
0.4591
0.0504
0.1012
0.04
0.0633
0.9103
0.6484
0.5864
0.0067
0.0091
0.0082
0.0088
0.7746
0.0267
0.0082
0.0106
0.0086
0.0658

C4.5
0.0439
0.0351
0.0096
0.0843
0.0177
0.0077
0.0105
0.0075
0.0067
0.0063
0.0058
0.0077
0.0496
0.1993
0.1689
0.1112
0.1208
0.0064
0.0526
0.0704
0.006
0.0085
0.0144
0.0109
0.0244
0.0671
0.0069
0.0077
0.0101
0.0091
0.0095
0.0069
0.0152
0.0073
0.0076
0.0396
0.0081
0.0062
0.0078
0.4429
0.4532
0.0496
0.095
0.0393
0.0618
0.9096
0.6448
0.5884
0.0056
0.0075
0.0074
0.0131
0.7759
0.0255
0.0075
0.0095
0.0079
0.0651

PART
0.0443
0.0407
0.0073
0.0835
0.0211
0.0065
0.0074
0.0058
0.0079
0.0071
0.006
0.0079
0.051
0.1976
0.1631
0.1105
0.1217
0.0068
0.0509
0.0641
0.0082
0.0087
0.0141
0.0054
0.0276
0.0674
0.0086
0.0411
0.0071
0.0124
0.0078
0.0085
0.0164
0.0066
0.008
0.0378
0.0064
0.0062
0.0064
0.444
0.4557
0.0502
0.0954
0.0424
0.0644
0.9091
0.6443
0.5861
0.0065
0.0102
0.0087
0.0131
0.7736
0.025
0.0066
0.0095
0.0073
0.0652

IB1
0.0477
0.034
0.0078
0.0853
0.02
0.0076
0.0068
0.008
0.0065
0.0085
0.0067
0.0097
0.0485
0.1994
0.1652
0.1103
0.1209
0.0067
0.0519
0.066
0.008
0.0072
0.0118
0.0091
0.0257
0.0692
0.0086
0.0074
0.0069
0.014
0.0124
0.0066
0.0183
0.0081
0.0065
0.0374
0.0094
0.0061
0.0068
0.4401
0.4551
0.0539
0.0966
0.0416
0.0635
0.9142
0.6464
0.5851
0.0075
0.0131
0.0064
0.0093
0.7739
0.0266
0.0063
0.0055
0.0062
0.0649

BNet
0.0427
0.0354
0.0085
0.0859
0.0194
0.0098
0.0071
0.0079
0.0064
0.0116
0.0067
0.007
0.0514
0.1983
0.1638
0.1096
0.1226
0.006
0.0533
0.0646
0.013
0.0089
0.0184
0.0082
0.0248
0.0664
0.0071
0.0132
0.0077
0.0113
0.0092
0.0079
0.0165
0.0061
0.008
0.0375
0.0077
0.0064
0.0083
0.4413
0.4545
0.0526
0.0968
0.0394
0.0625
0.9122
0.6461
0.5854
0.0055
0.0103
0.0095
0.0078
0.7746
0.0282
0.0135
0.0073
0.0064
0.0650

NB and BNet denote Naive Bayes and Bayes Network, respectively.

Table 6: Recommendation time over 115 data sets for the five classifiers (in second )

24

fiSubset Selection Algorithm Automatic Recommendation

4.4.3 Recommendation Time
When recommending FSS algorithms for a feature selection problem, the recommendation
time is contributed by meta-features extraction, k nearest data sets identification, and the
candidate algorithm ranking according to their performance on the k data sets.
Of these three recommendation time contributors, only the candidate algorithm ranking
is related with the parameters  and  of the performance metric EARR.
However, the computation of performance EARR is the same whatever the values of 
and  are. This means recommendation time is independent of the specific settings of 
and . Thus, we just present the recommendation time with ( = 0,  = 0), and Table 6
shows the details.
From Table 6 we observe that for a given data set, the recommendation time differences
for the five classifiers are small. The reason is that the recommendation time is mainly
contributed by the extraction of meta-features, which has no relation with classifiers. This
is consistent with the time complexity analysis in Section 3.2. We also observe that for most
of the data sets, the recommendation time is less than 0.1 second, and its average value on
the 115 data sets is around 0.65 second for each of the five classifiers. This is much faster
than the conventional cross validation method.
4.4.4 Impact of the Parameters  and 
Figs. 9, 10, 11, 12 and 13 show the impact of the settings of  and  on the classification
accuracy, the runtime of feature selection, the number of selected features, the Hit Ratio
and the RPR value, respectively.
NaiveBayes

C4.5

PART

Average accuracy

0.845

Average accuracy

IB1

Bayes Network

0.845

0.84
0.835
0.83
0.825
0.82
0.815
0.81

0.84
0.835
0.83
0.825
0.82
0.815
0.81
0.805

0.805
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 9: Classification accuracies of the five classifiers with the recommended FSS algorithms under different values of  and 
Fig. 9 shows the classification accuracies of the five classifiers under the different values
of  and . From it we observe that, with the increase of either  or , the classification
accuracies of the five classifiers with the recommended FSS algorithms decrease. This is
because the increase of  or  indicates that users much prefer faster FSS algorithms or the
FSS algorithms that can get less features. Thus, the proportion of classification accuracy
in performance is decreased. This means the ranks of the FSS algorithms that run faster
and/or get less features are improved and the corresponding FSS algorithms are finally
selected.
25

fiWang, Song, Sun, Zhang, Xu & Zhou

C4.5

PART

IB1

Bayes Network

2400

Average runtime (ms)

Average runtime (ms)

NaiveBayes
2400
2200
2000
1800
1600
1400
1200
1000
800
600
400
200

2200
2000
1800
1600
1400
1200
1000
800
600
400

0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 10: Runtime of the FSS algorithms recommended to the five classifiers under different
values of  and 
Fig. 10 shows the runtime of the FSS algorithms that recommended to the five classifiers
under the different values of  and  for the five classifiers. From it we observe that:
1) With the increase of , the average runtime of the recommended FSS algorithms for
each classifier decreases. Note a larger value of  means users favor faster FSS algorithms. Thus, this indicates that users performance requirement is met since faster FSS
algorithms were recommended.
2) With the increase of , the average runtime of the recommended FSS algorithms increases
as well. This is because in our proposed recommendation method, the appropriate
FSS algorithms for a given data set are recommended based on its nearest data sets.
Moreover, in the experiment, for more than half (i.e., 69) of the 115 data sets, there is
a negative correlation between the number of selected features and the runtime of the
22 FSS algorithms. Thus, the more data sets with this kind of negative correlation, the
more possible the nearest neighbors of a given data set have the negative correlation.
Therefore, a larger  means longer runtime. Another possible reason is that a larger
value of  means users favor the FSS algorithms that choose fewer features, and in order
to get fewer features, the FSS algorithms need to consume relatively more time.
C4.5

PART

Average number of features

Average number of features

NaiveBayes

100
90
80
70
60
50
40
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

IB1

Bayes Network

120
100
80
60
40
20
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 11: Number of features selected by the FSS algorithms that recommended to the
five classifiers under different values of  and 
Fig. 11 shows the number of features selected by the FSS algorithms that recommended
to the five classifiers under different values of  and . From it we observe that:
26

fiSubset Selection Algorithm Automatic Recommendation

1) With the increase of , the average number of selected features increases as well. This
is because in our proposed recommendation method, the appropriate FSS algorithms
for a given data set are recommended based on its nearest data sets. Moreover, in the
experiment, for more than half (i.e., 69) of the 115 data sets, there is a negative correlation between the number of selected features and the runtime of the 22 FSS algorithms.
Thus, the more data sets with this kind of negative correlation, the more possible the
nearest neighbors of a given data set have the negative correlation. Therefore, a larger
 means more features. Another possible reason is that a larger value of  means users
favor faster FSS algorithms. It is possible that shorter computation time can be obtained
via filter out less features so more features are remained.
Note that there is an exception. That is, the average number of selected features for
C4.5 decreases when the value of  is small. However, the decrement comes up in a quite
small range of  (i.e., < 0.005).
2) With the increase of , the average number of features selected by the recommended
FSS algorithm decreases. Note a larger value of  means users favor the FSS algorithms
that can get fewer features. Thus, this indicates that users requirement is met since the
FSS algorithms that can get fewer features were recommended.
NaiveBayes

C4.5

PART

Average Hit Ratio (%)

Average Hit Ratio (%)

IB1

Bayes Network

100

100
99
98
97
96
95
94
93
92

99
98
97
96
95
94
93
92

91
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 12: Average Hit Ratio of the FSS algorithms that recommended to the five classifiers
under different values of  and 
C4.5

PART
100

99.5

99.5

Average RPR (%)

Average RPR (%)

NaiveBayes
100

99
98.5
98
97.5

IB1

Bayes Network

99
98.5
98
97.5
97

97
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 13: Average RPR of the FSS algorithms that recommended to the five classifiers
under different values of  and 
Figs. 12 and 13 show the average hit ratio and RPR of the recommended FSS algorithms
under different values of  and  for the five classifiers.
27

fiWang, Song, Sun, Zhang, Xu & Zhou

From them we observe that, the average hit ratio falls in the intervals [91.74%, 100%]
under  and [92.56%, 99.13%]) under . The average RPR varies in the intervals [97.69%,
98.82%] under  and [97.68%, 98.73%] under . With the change of the  and , the hit
ratio and RPR of the recommended FSS algorithms vary as well. However, the change
intervals fall in a relative small interval and the lower bound stands at a fairly high level.
The minimum average hit ratio is up to 91.74% and the minimum average RPR is up
to 97.68%. This indicates that the proposed FSS algorithm recommendation method has
general application and works well for different settings of  and .

5. Sensitivity Analysis of the Number of Nearest Data Sets on
Recommendation Results
In this section, we analyze how the number of the nearest data sets affects the recommendation performance. Based on the experimental results, we provide some guidelines for
selecting the appropriate number of nearest data sets in practice.
5.1 Experimental Method
Generally, different numbers of the nearest data sets (i.e., k) will result in different recommendations. Thus, when recommending FSS algorithms to a feature selection problem, an
appropriate k value is very important.
The k value that results in higher recommendation performance is preferred. However,
the recommendation performance difference under two different k values sometimes might
be random and not significant. Thus, in order to identify an appropriate k value from
alternatives, we should first determine whether or not the performance differences among
them are statistically significant. Non-parametric statistical test, Friedman test followed by
Holm procedure test as suggested by Demsar (2006), can be used for this purpose.
In the experiment, we conducted FSS algorithm recommendation with all possible k
values (i.e., from 1 to 114) over the 115 data sets. When identifying the appropriate k
values, the non-parametric statistical test is conducted as follows.
Firstly, the Friedman test is performed over the 114 recommendation performance at
the significance level 0.05. Its null hypothesis is that the 114 k values perform equivalently
well in the proposed recommendation method over the 115 data sets.
If the Friedman test rejects the null hypothesis, that is, there exists significant difference
among these 114 k values, then we choose one under which the recommendation has the best
performance as the reference. After that, the Holm procedure test is performed to find out
the k values under which the recommendation performance has no significant difference with
that of the reference. The identified k values including the reference are the appropriate
numbers of the nearest data sets.
5.2 Results Analysis
Fig. 14 shows how the number of the nearest data sets (i.e., k) affects the performance
of the recommendation method under different settings of  and , where  denotes the
k under which the recommendation performance is significantly worse than others at the
significance level of 0.05. From it we observe that:
28

fiSubset Selection Algorithm Automatic Recommendation

Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number of neighbors

1
0.995

RPR

0.99
0.985
0.98
0.975
0.97
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number of nearest data sets

(a)  = 0,  = 0
Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number of neighbors

1

RPR

0.995
0.99
0.985
0.98
0.975
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number of nearest data sets

(b)  = 10%,  = 10%

Figure 14: Number of the nearest data sets vs. RPR
1) When  =  = 0 (Fig. 14(a)), for each of the five classifiers, the RPR varies with
different k values. Specifically, the RPR is fluctuant when k is smaller than 20, while it
is relatively flat in the middle part, and it decreases when k is larger than 79 except for
C4.5. However, the increment of C4.5 is very small (< 0.002). This might be due to that
C4.5 picks up useful features to build the tree by itself, so the impact of other feature
selection methods is less. Moreover, the difference among accuracies of C4.5 on most
data sets is relatively small, while the performance metric EARR that used to evaluate
different FSS algorithms depends only on classification accuracy when  =  = 0. Thus,
the RPR of C4.5 is relatively stable for the different values of k.
2) In the case of  =  = 10% (Fig. 14(b)), the variation of RPR is different from that of
 =  = 0. For each of the five classifiers, the RPR first decreases with fluctuations, then
increases, and finally decreases slowly and steadily. This could be due to that, when the
parameters  and  are set to be a relatively large value (such as 10% in our experiment),
the runtime of ( or the number of features selected by) an FSS algorithm will play a
more important role in evaluating the performance of the FSS algorithm. Thus, for a
given data set, the FSS algorithms with lower time complexity (or the smaller number of
selected features) will be more possibly higher ranked and have larger RPR. Therefore,
with the increasing of k, these algorithms are more possibly recommended. Meanwhile,
for most data sets, these algorithms are either the real appropriate algorithms or with
larger RPR, so the RPR averaged over all data sets is relatively stable with the increasing
of k.
29

fiWang, Song, Sun, Zhang, Xu & Zhou

3) Comparing the cases of  =  = 0 and  =  = 10%, we found that  appears when
k < 21 for the former and k < 29 for the latter, while it emerges again when k > 76 for
the former. This means we cannot choose the k values falling into these ranges. At the
same time, we also found that the peak values of RPR for  =  = 10% appear in the
range of [32, 54], which is also one of the peak value ranges for  =  = 0 except C4.5.
This means if we set k to 28% to 47% of the number of data sets, better recommendation
performance can be obtained.

6. Conclusion
In this paper, we have presented an FSS algorithm recommendation method with the aim
to support the automatic selection of appropriate FSS algorithms for a new feature selection
problem from a number of candidates.
The proposed recommendation method consists of meta-knowledge database construction and algorithm recommendation. The former obtains the meta-features and the performance of all the candidate FSS algorithms, while the latter models the relationship between
the meta-features and the FSS algorithm performance based on a k -NN method and recommends appropriate algorithms for a feature selection problem with the built up model.
We have thoroughly tested the recommendation method with 115 real world data sets, 22
different FSS algorithms, and five representative classification algorithms under two typical
users performance requirements. The experimental results show that our recommendation
method is effective.
We have also conducted a sensitivity analysis to explore how the number of the nearest
data sets (k) impacts the FSS algorithm recommendation, and suggest to set k as the 28%
to 47% of the number of the historical data sets.
In this paper, we have utilized the well-known and commonly-used meta-features to
characterize different data sets. Which meta-features are informative? and Are there
any other more informative meta-features? are still open questions. To our knowledge,
there still does not exist any effective method to answer these questions. Thus, for future
work, we plan to explore further that how to measure the information of the meta-features
and whether there are some more informative meta-features that can lead to further improvements for FSS algorithm recommendation.

Acknowledgements
This work is supported by the National Natural Science Foundation of China under grant
61070006.

References
Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms. Machine learning, 6 (1), 3766.
Ali, S., & Smith, K. A. (2006). On learning algorithm selection for classification. Applied
Soft Computing, 6 (2), 119138.
30

fiSubset Selection Algorithm Automatic Recommendation

Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning. Artificial
intelligence review, 11 (1), 1173.
Battiti, R. (1994). Using mutual information for selecting features in supervised neural net
learning. IEEE Transactions on Neural Networks, 5 (4), 537550.
Brazdil, P., Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications to data
mining. Springer.
Brazdil, P. B., Soares, C., & Da Costa, J. P. (2003). Ranking learning algorithms: Using IBL
and meta-learning on accuracy and time results. Machine Learning, 50 (3), 251277.
Brodley, C. E. (1993). Addressing the selective superiority problem: Automatic algorithm/model class selection. In Proceedings of the Tenth International Conference
on Machine Learning, pp. 1724. Citeseer.
Castiello, C., Castellano, G., & Fanelli, A. (2005). Meta-data: Characterization of input
features for meta-learning. Modeling Decisions for Artificial Intelligence, 457468.
Dash, M., & Liu, H. (1997). Feature selection for classification. Intelligent data analysis,
1 (3), 131156.
Dash, M., & Liu, H. (2003). Consistency-based search in feature selection. Artificial Intelligence, 151 (1-2), 155176.
de Souza, J. T. (2004). Feature selection with a general hybrid algorithm. Ph.D. thesis,
University of Ottawa.
Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 130.
Engels, R., & Theusinger, C. (1998). Using a data metric for preprocessing advice for data
mining applications..
Frank, E., & Witten, I. H. (1998). Generating accurate rule sets without global optimization.
In Proceedings of the 25th international conference on Machine learning, pp. 144151.
Morgan Kaufmann, San Francisco, CA.
Friedman, M. (1937). The use of ranks to avoid the assumption of normality implicit in
the analysis of variance. Journal of the American Statistical Association, 32 (200),
675701.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
learning, 29 (2), 131163.
Gama, J., & Brazdil, P. (1995). Characterization of classification algorithms. Progress in
Artificial Intelligence, 189200.
Garcia Lopez, F., Garcia Torres, M., Melian Batista, B., Moreno Perez, J. A., & MorenoVega, J. M. (2006). Solving feature subset selection problem by a parallel scatter
search. European Journal of Operational Research, 169 (2), 477489.
Goldberg, D. E. (1989). Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley Professional.
31

fiWang, Song, Sun, Zhang, Xu & Zhou

Gutlein, M., Frank, E., Hall, M., & Karwath, A. (2009). Large-scale attribute selection
using wrappers. In Proceedings of IEEE Symposium on Computational Intelligence
and Data Mining, pp. 332339. IEEE.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. The
Journal of Machine Learning Research, 3, 11571182.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The
weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11 (1),
1018.
Hall, M. A. (1999). Correlation-based Feature Selection for Machine Learning. Ph.D. thesis,
The University of Waikato.
Hedar, A. R., Wang, J., & Fukushima, M. (2008). Tabu search for attribute reduction
in rough set theory. Soft Computing-A Fusion of Foundations, Methodologies and
Applications, 12 (9), 909918.
Hommel, G. (1988). A stagewise rejective multiple test procedure based on a modified
bonferroni test. Biometrika, 75 (2), 383386.
John, G. H., & Langley, P. (1995). Estimating continuous distributions in Bayesian classifiers. In Proceedings of the eleventh conference on uncertainty in artificial intelligence,
Vol. 1, pp. 338345. Citeseer.
Kalousis, A., Gama, J., & Hilario, M. (2004). On data and algorithms: Understanding
inductive performance. Machine Learning, 54 (3), 275312.
King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: comparison of classification algorithms on large real-world problems. Applied Artificial Intelligence, 9 (3), 289333.
Kira, K., & Rendell, L. (1992). A practical approach to feature selection. In Proceedings of the ninth international workshop on Machine learning, pp. 249256. Morgan
Kaufmann Publishers Inc.
Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and
model selection. In International joint Conference on artificial intelligence, Vol. 14,
pp. 11371145. Citeseer.
Kohavi, R., & John, G. (1997). Wrappers for feature subset selection. Artificial intelligence,
97 (1), 273324.
Kononenko, I. (1994). Estimating attributes: analysis and extensions of RELIEF. In Proceedings of the European conference on machine learning on Machine Learning, pp.
171182. Springer-Verlag New York.
Lee, M., Lu, H., Ling, T., & Ko, Y. (1999). Cleansing data for mining and warehousing.
In Proceedings of the 10th International Conference on Database and Expert Systems
Applications, pp. 751760. Springer.
Lindner, G., & Studer, R. (1999). AST: Support for algorithm selection with a CBR approach. Principles of Data Mining and Knowledge Discovery, 418423.
Liu, H., Motoda, H., Setiono, R., & Zhao, Z. (2010). Feature Selection: An Ever Evolving
Frontier in Data Mining. In The Fourth Workshop on Feature Selection in Data
Mining, pp. 314. Citeseer.
32

fiSubset Selection Algorithm Automatic Recommendation

Liu, H., & Setiono, R. (1995). Chi2: Feature selection and discretization of numeric attributes. In Proceedings of the Seventh International Conference on Tools with Artificial Intelligence, pp. 388391. IEEE.
Liu, H., & Setiono, R. (1996). A probabilistic approach to feature selection-a filter solution..
pp. 319327. Citeseer.
Liu, H., & Yu, L. (2005). Toward integrating feature selection algorithms for classification
and clustering. IEEE Transactions on Knowledge and Data Engineering, 17 (4), 491
502.
Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (1994).
statistical classification..

Machine learning, neural and

Molina, L. C., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: A survey and
experimental evaluation. In Proceedings of IEEE International Conference on Data
Mining, pp. 306313. IEEE.
Nakhaeizadeh, G., & Schnabl, A. (1997). Development of multi-criteria metrics for evaluation of data mining algorithms. In Proceedings of the 3rd International Conference
on Knowledge Discovery and Data mining, pp. 3742.
Nakhaeizadeh, G., & Schnabl, A. (1998). Towards the personalization of algorithms evaluation in data mining. In Proceedings of the 4th International Conference on Knowledge
Discovery and Data mining, pp. 289293.
Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods in feature selection.
Pattern recognition letters, 15 (11), 11191125.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998a). Conceptual base of feature
selection consulting system. Kybernetika, 34 (4), 451460.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998b). Feature selection expertuser
oriented approach. Advances in Pattern Recognition, 573582.
Quinlan, J. R. (1993). C4.5: programs for machine learning. Morgan Kaufmann.
Robnik-Sikonja, M., & Kononenko, I. (2003). Theoretical and empirical analysis of relieff
and rrelieff. Machine learning, 53 (1), 2369.
Saeys, Y., Inza, I., & Larranaga, P. (2007). A review of feature selection techniques in
bioinformatics. Bioinformatics, 23 (19), 25072517.
Smith-Miles, K. A. (2008). Cross-disciplinary perspectives on meta-learning for algorithm
selection. ACM Computing Surveys, 41 (1), 125.
Sohn, S. Y. (1999). Meta analysis of classification algorithms for pattern recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 21 (11), 11371144.
Song, Q. B., Wang, G. T., & Wang, C. (2012). Automatic recommendation of classification
algorithms based on data set characteristics. Pattern Recognition, 45 (7), 26722689.
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18 (2), 7795.
33

fiWang, Song, Sun, Zhang, Xu & Zhou

Wolpert, D. H. (2001). The supervised learning no-free-lunch theorems. In Proceedings
of 6th Online World Conference on Soft Computing in Industrial Applications, pp.
2542. Citeseer.
Yu, L., & Liu, H. (2003). Feature selection for high-dimensional data: A fast correlationbased filter solution. In Proceedings of The Twentieth International Conference on
Machine Leaning, Vol. 20, pp. 856863.
Zhao, Z., & Liu, H. (2007). Searching for interacting features. In Proceedings of the 20th
International Joint Conference on Artifical Intelligence, pp. 11561161. Morgan Kaufmann Publishers Inc.
Zhou, X., & Dillon, T. (1988). A heuristic-statistical feature selection criterion for inductive machine learning in the real world. In Proceedings of the IEEE International
Conference on Systems, Man, and Cybernetics, Vol. 1, pp. 548552. IEEE.

34

fiJournal of Artificial Intelligence Research 47 (2013) 741-808

Submitted 01/13; published 08/13

Acyclicity Notions for Existential Rules and
Their Application to Query Answering in Ontologies
Bernardo Cuenca Grau
Ian Horrocks
Markus Krotzsch
Clemens Kupke
Despoina Magka
Boris Motik
Zhe Wang

bernardo.cuenca.grau@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk
markus.kroetzsch@cs.ox.ac.uk
clemens.kupke@cs.ox.ac.uk
despoina.magka@cs.ox.ac.uk
boris.motik@cs.ox.ac.uk
zhe.wang@cs.ox.ac.uk

Department of Computer Science, University of Oxford
Parks Road, Oxford OX1 3QD, United Kingdom

Abstract
Answering conjunctive queries (CQs) over a set of facts extended with existential rules
is a prominent problem in knowledge representation and databases. This problem can be
solved using the chase algorithm, which extends the given set of facts with fresh facts in
order to satisfy the rules. If the chase terminates, then CQs can be evaluated directly in
the resulting set of facts. The chase, however, does not terminate necessarily, and checking
whether the chase terminates on a given set of rules and facts is undecidable. Numerous
acyclicity notions were proposed as sufficient conditions for chase termination. In this
paper, we present two new acyclicity notions called model-faithful acyclicity (MFA) and
model-summarising acyclicity (MSA). Furthermore, we investigate the landscape of the
known acyclicity notions and establish a complete taxonomy of all notions known to us.
Finally, we show that MFA and MSA generalise most of these notions.
Existential rules are closely related to the Horn fragments of the OWL 2 ontology
language; furthermore, several prominent OWL 2 reasoners implement CQ answering by
using the chase to materialise all relevant facts. In order to avoid termination problems,
many of these systems handle only the OWL 2 RL profile of OWL 2; furthermore, some
systems go beyond OWL 2 RL, but without any termination guarantees. In this paper we
also investigate whether various acyclicity notions can provide a principled and practical
solution to these problems. On the theoretical side, we show that query answering for
acyclic ontologies is of lower complexity than for general ontologies. On the practical
side, we show that many of the commonly used OWL 2 ontologies are MSA, and that the
number of facts obtained by materialisation is not too large. Our results thus suggest that
principled development of materialisation-based OWL 2 reasoners is practically feasible.

1. Introduction
Existential rules are first-order implications between conjunctions of function-free atoms
that may contain existentially quantified variables in the implications consequent (Baget,
Leclere, Mugnier, & Salvat, 2011a; Cal, Gottlob, Lukasiewicz, Marnette, & Pieris, 2010a).
Such rules are used in a variety of ways in databases, knowledge representation, and logic
programming. In database theory, existential rules are known as tuple-generating dependencies (Abiteboul, Hull, & Vianu, 1995) and are used to capture a wide range of schema

c
2013
AI Access Foundation. All rights reserved.

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

constraints. Furthermore, they are also used as declarative data transformation rules in
data exchangethe process of transforming a database structured according to a source
schema into a database structured according to a target schema (Fagin, Kolaitis, Miller, &
Popa, 2005). Existential rules also provide the foundation for several prominent knowledge
representation formalisms, such as Datalog (Cal, Gottlob, & Pieris, 2010b; Cal et al.,
2010a), and they are also closely related to logic programs with function symbols in the
head. Practical applications of existential rules range from bioinformatics (Mungall, 2009)
to modelling complex structures of chemical compounds (Magka, Motik, & Horrocks, 2012;
Hastings, Magka, Batchelor, Duan, Stevens, Ennis, & Steinbeck, 2012).
Answering conjunctive queries (CQs) over a set of facts extended with existential rules is
a fundamental, yet undecidable (Beeri & Vardi, 1981) reasoning problem for existential rules.
The problem can be characterised using chase (Johnson & Klug, 1984; Maier, Mendelzon,
& Sagiv, 1979)a technique closely related to the hypertableau calculus (Motik, Shearer, &
Horrocks, 2009b; Baumgartner, Furbach, & Niemela, 1996). In a forward-chaining manner,
the chase extends the original set of facts with facts that can be derived using the rules.
The result of the chase is a universal model, in the sense that an arbitrary CQ over the
original facts and rules can be answered by evaluating the query in this model.
1.1 Chase Termination and Acyclicity Notions
Rules with existentially quantified variables in the headso-called generating rulesrequire
the introduction of fresh individuals. Cyclic applications of generating rules may prevent
the chase from terminating, and in fact determining whether chase terminates on a set of
rules and facts is undecidable (Deutsch, Nash, & Remmel, 2008). However, several decidable
classes of existential rules have been identified, and the existing proposals can be classified
into two main groups. In the first group, rules are restricted such that their possibly infinite
universal models can be represented using finitary means. This group includes rules with
universal models of bounded treewidth (Baget et al., 2011a), guarded rules (Cal et al.,
2010a), and sticky rules (Cal, Gottlob, & Pieris, 2011). In the second group, one uses a
sufficient (but not necessary) acyclicity notion that ensures chase termination.
Roughly speaking, acyclicity notions analyse the information flow between rules to ensure that no cyclic applications of generating rules are possible. Weak acyclicity (WA)
(Fagin et al., 2005) was one of the first such notions, and it was extended to notions such as
safety (Meier, Schmidt, & Lausen, 2009), stratification (Deutsch et al., 2008), acyclicity of
a graph of rule dependencies (aGRD) (Baget, Mugnier, & Thomazo, 2011b), joint acyclicity (JA) (Krotzsch & Rudolph, 2011), and super-weak acyclicity (SWA) (Marnette, 2009).
Syntactic acyclicity criteria have also been investigated in the context of logic programs
with function symbols in the rule heads, where the goal is to recognise logic programs with
finite stable models. Several such notions have been implemented in state of the art logic
programming engines, such as omega-restrictedness (Syrjanen, 2001) from the Smodels system (Syrjanen & Niemela, 2001), lambda-restrictedness from the ASP grounder GrinGo
(Gebser, Schaub, & Thiele, 2007), argument-restrictedness (Lierler & Lifschitz, 2009) from
the DLV system (Leone, Pfeifer, Faber, Eiter, Gottlob, Perri, & Scarcello, 2006), and many
others (Calimeri, Cozza, Ianni, & Leone, 2008; Greco, Spezzano, & Trubitsyna, 2012; De
Schreye & Decorte, 1994).

742

fiAcyclicity Notions for Existential Rules

1.2 Applications of Acyclicity Notions
Acyclicity notions are interesting for several reasons. First, unlike guarded rules, acyclic
rules can axiomatise structures of arbitrary shapes, as long as these structures are bounded
in size. Second, the result of the chase for acyclic rules can be stored and manipulated as
if it were a database; this is important, for example, in data exchange, where the goal is to
materialise the transformed database.
In this paper, we further argue that acyclicity notions are also relevant to description logics (DLs)knowledge representation formalisms underpinning the OWL 2 ontology
language (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). CQ
answering over DL ontologies is a key reasoning service in many DL applications, and the
problem was studied for numerous different DLs (Calvanese, De Giacomo, Lembo, Lenzerini,
& Rosati, 2007; Krotzsch, Rudolph, & Hitzler, 2007; Glimm, Horrocks, Lutz, & Sattler,
2008; Ortiz, Calvanese, & Eiter, 2008; Lutz, Toman, & Wolter, 2009; Perez-Urbina, Motik,
& Horrocks, 2009; Rudolph & Glimm, 2010; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev, 2011). Answering CQs over ontologies, however, is quite technical and often of
high computational complexity. Therefore, practical OWL 2 reasoners frequently solve this
problem using materialisationa reasoning technique in which the relevant consequences
of the ontology are precomputed using chase, allowing queries to be directly evaluated in
the materialised set of facts. Examples of materialisation-based systems include Oracles
Semantic Data Store (Wu, Eadon, Das, Chong, Kolovski, Annamalai, & Srinivasan, 2008),
Sesame (Broekstra, Kampman, & van Harmelen, 2002), OWLIM (Kiryakov, Ognyanov, &
Manov, 2005), Jena (Carroll, Dickinson, Dollin, Reynolds, Seaborne, & Wilkinson, 2004),
and DLE-Jena (Meditskos & Bassiliades, 2008). Such reasoning is possible if (i) the ontology
is Horn (Hustadt, Motik, & Sattler, 2005) and thus does not require disjunctive reasoning,
and (ii) the chase is guaranteed to terminate. To satisfy the second assumption, reasoners
often consider only axioms in the OWL 2 RL profile (Motik, Cuenca Grau, Horrocks, Wu,
Fokoue, & Lutz, 2009a); this systematically excludes generating rules and thus trivially
ensures chase termination, but it also makes the approach incomplete. Generating rules are
partially supported in systems such as OWLim (Bishop & Bojanov, 2011) and Jena, but
such support is typically ad hoc and provides no completeness and/or termination guarantees. Acyclicity notions can be used to address these issues: if an ontology is Horn and
acyclic, a complete materialisation can be computed without the risk of non-termination.
1.3 Our Contributions
Motivated by the practical importance of chase termination, in this paper we present two
new acyclicity notions: model-faithful acyclicity (MFA) and model-summarising acyclicity
(MSA). Roughly speaking, these acyclicity notions use a particular model of the rules to
analyse the implications between existential quantifiers, which is why we call them model
based. In particular, MFA uses the actual canonical model induced by the facts and
the rules, which makes the notion very general. We prove that checking whether a set of
existential rules is MFA is 2ExpTime-complete, and it becomes ExpTime-complete if the
predicates in the rules are of bounded arity. Due to the high complexity, MFA may be
unsuitable for practical application. Thus, we introduce MSA, which can be understood as
MFA in which the analysis is performed over models that summarise (or overestimate) the
743

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

actual models. Checking MSA of existential rules can be realised via checking entailment of
ground atoms in datalog programs. We use this close connection between MSA and datalog
to prove that checking MSA is ExpTime-complete for general existential rules, and that it
becomes coNP-complete if the arity of rule predicates is bounded.
We next conduct a detailed investigation of the landscape of known acyclicity notions,
augmented with MFA and MSA. For the class of logic programs that correspond to existential rules with skolemised existential quantifiers, we show that MSA and MFA strictly
subsume existing acyclicity notions known from logic programming. We also show that
MSA is strictly more general than SWAone of the most general acyclicity notions known
in database theory. Furthermore, we investigate the relationship between the known notions
and thus complete the picture with respect to their relative expressiveness.
Both MSA and MFA can be applied to general existential rules without equality. Equality can be incorporated via singularisationa technique proposed by Marnette (2009) that
transforms the rules to encode the effects of equality. Singularisation is orthogonal to
acyclicity: after computing the transformed rules, one can use MFA, MSA, or in fact any
notion to check whether the result is acyclic; if so, the chase of the signularised rules terminates, and the chase result can be used in a particular way to answer arbitrary CQs.
Unfortunately, singularisation is nondeterministic: some ways of transforming the rules
may produce acyclic rule sets, but not all ways are guaranteed to do so. In this paper, we
refine singularisation to obtain practically useful upper and lower bounds for acyclicity. We
also show that, when used with JA, our lower bound actually coincides with WA.
We next turn our attention to theoretical and practical issues of using acyclicity for
materialisation-based CQ answering over ontologies. On the theoretical side, we show that
checking MFA and MSA of Horn-SROIF ontologies is ExpTime- and PTime-complete,
respectively, and that answering CQs over acyclic Horn-SROIF ontologies is ExpTimecomplete as well. Furthermore, we show that, for Horn-SHIF ontologies, the complexity
of checking MFA and of answering CQs drops to PSpace. Answering CQs is ExpTimecomplete for general (i.e., not acyclic) Horn-SHIF ontologies (Eiter, Gottlob, Ortiz, &
Simkus, 2008; Ortiz, Rudolph, & Simkus, 2011), so acyclicity makes this problem easier.
Furthermore, Horn ontologies can be extended with arbitrary SWRL rules (Horrocks &
Patel-Schneider, 2004) without affecting decidability or worst-case complexity, provided
that the union of the ontology and SWRL rules is acyclic; this is in contrast to the general
case, where SWRL extensions of DLs easily lead to undecidability.
On the practical side, we explore the limits of reasoning with acyclic OWL 2 ontologies
via materialisation. We checked MFA, MSA, and JA for 336 Horn ontologies; furthermore,
to estimate the impact of materialisation, we compared the size of the materialisation
with the number of facts in the original ontologies. Our experiments revealed that many
ontologies are MSA, and that some complex ones are MSA but not JA; furthermore, the
universal models obtained via materialisation are typically not too large. Thus, our results
suggest that principled, materialisation-based reasoning for ontologies beyond the OWL 2
RL profile may be practically feasible.
This is an extended version of a paper by Cuenca Grau, Horrocks, Krotzsch, Kupke,
Magka, Motik, and Wang (2012) published at KR 2012.

744

fiAcyclicity Notions for Existential Rules

2. Preliminaries
In this section we introduce definitions and notation used in the rest of our paper.
2.1 First-Order Logic
We use the standard notions of constants, function symbols, and predicate symbols, where
 is the equality predicate, > is universal truth, and  is universal falsehood. Each function
or predicate symbol is associated with a nonnegative integer arity. Variables, terms, substitutions, atoms, first-order formulae, sentences, interpretations (i.e., structures), and models
are defined as usual. By a slight abuse of notation, we often identify a conjunction with
the set of its conjuncts. Furthermore, we often abbreviate a vector of terms t1 , . . . , tn as ~t;
we define |~t| = n; and we often identify ~t with the set of indexed terms {t1 , . . . , tn }. With
(~x) we stress that ~x = x1 , . . . , xn are the free variables of a formula , and with  we
denote the result of applying a substitution  to . A term, atom, or formula is ground if it
does not contain variables; a fact is a ground atom. The depth dep(t) of a term t is defined
as 0 if t is a constant or a variable, and dep(t) = 1 + maxni=1 dep(ti ) if t = f (t1 , . . . , tn ). A
term t0 is a subterm of a term t if t0 = t or t = f (~s) and t0 is a subterm of some si  ~s; if
additionally t0 6= t, then t0 is a proper subterm of t. A term s is contained in an atom P (~t)
if s  ~t, and s occurs in P (~t) if s is a subterm of some term ti  ~t; thus, if s is contained
in P (~t), then s also occurs in P (~t), but the converse may not hold. A term s is contained
(resp. occurs) in a set of atoms I if s is contained (resp. occurs) in some atom in I.
In first-order logic, the equality predicate  is commonly assumed to have a predefined
interpretationthat is, every first-order interpretation is required to interpret  as the
smallest reflexive relation over the domain. Satisfaction of a sentence  in an interpretation
I where  is interpreted in this way is written I |= , and entailment of a sentence  from
a sentence  is written  |= . Unless otherwise stated, we use this standard interpretation
of equality throughout this paper.
Equality, however, can also be treated as an ordinary predicate with an explicit axiomatisation. Let  be an arbitrary set of function-free first-order formulae. Then,  =  if 
does not occur in ; otherwise,  contains formulae (1)(3) and an instance of formula (4)
for each n-ary predicate P occurring in  different from , and for each 1  i  n. Note
that all variables in all of these formulae are (implicitly) universally quantified.
xx

(1)

x1  x2  x2  x1

(2)

x1  x2  x2  x3  x1  x3

(3)

P (x1 , . . . , xi , . . . , xn )  xi 

x0i



P (x1 , . . . , x0i , . . . , xn )

(4)

If  is treated as an ordinary predicate, satisfaction of a formula  in a model I is written
I |= , and entailment of a formula  from formula  is written  |= . Please note that,
according to our definitions, I |=  can hold even if interpretation I interprets predicate 
in an arbitrary way; in contrast, I |=  can hold only if interpretation I interprets predicate
 as the identity relation on the models domain. The consequences of  w.r.t. |= and of
   w.r.t. |= coincidethat is, for each first-order sentence  constructed using the
symbols from , we have  |=  if and only if    |= .
745

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

2.2 Rules and Queries
An instance is a finite set of function-free facts. An existential rule (or just rule) is a
function-free sentence of the form
~x~z.[(~x, ~z)  ~y .(~x, ~y )]

(5)

where (~x, ~z) and (~x, ~y ) are conjunctions of atoms, and tuples of variables ~x, ~y , and ~z are
pairwise disjoint. Formula  is the body and formula  is the head of the rule. For brevity,
quantifiers ~x~z are often omitted. For convenience, we sometimes identify a rule body or
head with the set of the respective conjuncts. A datalog rule is a rule where ~y is empty.
A rule is equality-free if it does not contain the equality predicate . A term s occurs in
an existential rule if s occurs in a head or body atom of the rule, and these definitions
are extended to a set of rules in the obvious way; existential rules do not contain function
symbols, so an analogous notion of s being contained in a rule coincides with this one. Two
variables are directly connected in a rule if they occur together in a body atom of the rule;
furthermore, connected is the transitive closure of directly connected ; finally, a rule of the
form (5) is connected if all pairs of variables w, w0  ~x  ~z are connected in the rule.
A conjunctive query (CQ) is a formula of the form Q(~x) = ~y .(~x, ~y ), where (~x, ~y ) is
a conjunction of atoms; the query is Boolean if ~x is empty. A substitution  mapping ~x
to constants is an answer to Q(~x) w.r.t. a set of rules  and instance I if   I |= Q(~x).
Answering CQs is the core reasoning problem in many applications of existential rules.
When answering a conjunctive query Q(~x) over a set of rules  and an instance I, in
the rest of this paper we implicitly assume that Q(~x) and I contain only the predicates
from . This simplifies the presentation since it allows us to define various transformations
of  without having to take into account possible predicates that occur in Q(~x) or I only.
This assumption is w.l.o.g., as we can always extend  with tautological rules of the form
P (~x)  P (~x) for each predicate P occurring in Q(~x) or I but not in .
Furthermore, we assume that  does not occur in the body of any rule in  or in the
query Q(~x). This is w.l.o.g. since we can eliminate each atom of the form x  t in a rule body
and further replace x with t in the rest of the rule; furthermore, to eliminate body atoms of
the form a  b with a and b constants, we can introduce a fresh predicate Oa , add a new rule
 Oa (a), replace each body atom a  b with conjunction Oa (x)  x  b in which x is a fresh
variable, and finally eliminate atom x  b as before. Similarly, we do not provide an explicit
support for the inequality predicate 6. Inequality in rule heads can be simulated using an
ordinary predicate: each atom of the form s 6 t occurring in a rule head can be replaced with
NotEqual(s, t), where NotEqual is a fresh ordinary predicate that is explicitly axiomatised as
irreflexive; note that, if  is handled as a regular predicate explicitly axiomatised by rules
(1)(4), then the replacement axioms (4) must be instantiated for P = NotEqual as well. In
contrast, atoms involving the inequality predicate occurring in rule bodies generally require
disjunctive reasoning, which is not supported by existential rules.
Finally, we assume that conjunctions (~x, ~z) and (~x, ~y ) in each rule of the form (5) are
both not empty. We also assume that > and  are treated as ordinary unary predicates,
and that the semantics of > is captured explicitly in  by instantiating the following rule
for each n-ary predicate P occurring in :
P (x1 , . . . , xn )  >(x1 )  . . .  >(xn )
746

(6)

fiAcyclicity Notions for Existential Rules

These assumptions ensure that I   is always satisfiable, but that   I |= y.(y) if and
only if I   is unsatisfiable w.r.t. the conventional treatment of > and . By allowing
body atoms of the form >(x), without loss of generality we can require each existential
rule to be safe (i.e., that each universally quantified variable occurring in a head atom also
occurs in a body atom of the rule), which greatly simplifies many of our definitions.
In database theory, satisfaction and entailment are often considered only w.r.t. finite
interpretations under the unique name assumption (UNA); the latter ensures that distinct
constants are interpreted as distinct elements. In contrast, such assumptions are not customary in ontology-based KR. In this paper, we do not assume UNA, as UNA can be
axiomatised explicitly if needed using the inequality predicate (or a simulation thereof).
Furthermore, in this paper we investigate theories that are satisfiable in finite models (i.e.,
for which the chase is finite); thus, the difference between finite and infinite satisfiability is
immaterial to our results.
We frequently use skolemisation to interpret rules in Herbrand interpretations, which
are defined as possibly infinite sets of ground atoms. In particular, for each rule r of the
form (5) and each variable yi  ~y , let fri be a function symbol globally unique for r and yi of
arity |~x|; furthermore, let sk be the substitution such that sk (yi ) = fri (~x) for each yi  ~y .
Then, the skolemisation sk(r) of r is the following rule:
(~x, ~z)  (~x, ~y )sk

(7)

The skolemisation sk() of a set of rules  is obtained by skolemising each rule in .
Skolemisation does not affect the answers to CQsthat is, for each conjunctive query Q(~x)
formed from only the predicates in , each instance I, and each substitution , we have
  I |= Q(~x) if and only if sk()    I |= Q(~x).
2.3 The Skolem Chase
Answering CQs can be characterised using chase, and in this paper we use the skolem chase
variant (Marnette, 2009). Let r =    be a skolemised rule and let I be a set of ground
atoms. A set of ground atoms S is a consequence of r on I if substitution  exists mapping
the variables in r to the terms occurring in I such that   I and S  . The result
of applying r to I, written
S r(I), is the union of all consequences of r on I. For  a set of
skolemised rules, (I) = r r(I). Let I be a finite set of ground atoms, let  be a set
of rules, let 0 = sk()   , and let 0f and 0n be the subsets of 0 containing rules with
and without function symbols, respectively. The chase sequence for I and  is a sequence
of sets of facts I0 , I1 , . . . where I0 = I and, for each i > 0, set Ii is defined as follows:
 if 0n (Ii1 ) 6 Ii1 , then Ii = Ii1  0n (Ii1 ),
 otherwise Ii = Ii1  0f (Ii1 ).
S
The chase of I and  is defined as I = i Ii ; note that I can be infinite. The chase can
be used as a database for answering CQs: a substitution  is an answer to Q over  and I
if and only if I |= Q. The chase of I and  terminates if i  0 exists such that Ii = Ij
for each j  i; the chase of  terminates universally if the chase of I and  terminates for
each I. If the skolem chase of I and  terminates, then both the nonoblivious chase (Fagin
et al., 2005) and the core chase (Deutsch et al., 2008) of I and  terminate as well.
747

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

The critical instance I for a set of rules  contains all facts that can be constructed
using all predicates occurring in , all constants occurring in the body of a rule in , and
one special fresh constant . The skolem chase for I and  terminates if and only if the
skolem chase of  terminates universally (Marnette, 2009).
2.4 Acyclicity Notions
Checking whether the skolem chase terminates on a given instance is undecidable, and
checking universal skolem chase termination is conjectured to be undecidable as well. Consequently, various sufficient acyclicity notions have been proposed in the literature. Formally, an acyclicity notion X is a class of finite sets of rules; such a definition allows us
to talk about (proper) containment between acyclicity notions. We sometimes write  is
X, by which we mean   X. We next introduce weak and joint acyclicity: the former is
one of the first such notions considered in the literature; and as we show in Section 3, the
latter notion is relatively powerful, yet still easy to understand. We use these two notions
throughout the paper to present examples and state various technical claims. In Section 3
we present the definitions of many other acyclicity notions known in the literature.
In the following, let  be a set of rules where no variable occurs in more than one rule. A
position is an expression of the form P |i where P is an n-ary predicate and i is an integer with
1  i  n. Given a rule r of the form (5) and a variable w occurring in r, the set PosB (w) of
body positions of w contains each position P |i such that P (t1 , . . . , tn )  (~x, ~z) and ti = w
for some vector ~t of terms. The set PosH (w) of head positions is defined analogously, but
w.r.t. the head atoms of r. Note that, since each variable occurs in at most one rule in
, sets PosB (w) and PosH (w) are (indirectly) associated with the rule that contains w. In
the rest of this paper, whenever we use notation such as PosH (w) or PosB (w), we silently
assume that no variable occurs in more than one rule and so the notation is unambiguous.
This is clearly w.l.o.g. as one can always arbitrarily rename variables in different rules.
Weak acyclicity (WA) (Fagin et al., 2005) can be applied to existential rules that contain
the equality predicate. The WA dependency graph WA() for  contains positions as
vertices; furthermore, for each rule r   of the form (5), each variable x  ~x, each position
P |i  PosB (x), and each variable y  ~y , graph WA() contains
 a regular edge from P |i to each Q|j  PosH (x) such that Q 6=  and,
 a special edge from P |i to each Q|j  PosH (y) such that Q 6= .
Set  is WA if WA() does not contain a cycle that involves a special edge. Equality atoms
are effectively ignored by WA.
Joint acyclicity (JA) (Krotzsch & Rudolph, 2011) generalises WA, but it is applicable
only to equality-free rules. For an existentially quantified variable y in , let Move(y) be
the smallest set of positions such that
 PosH (y)  Move(y), and
 for each existential rule r   and each universally quantified variable x occurring in
r, if PosB (x)  Move(y), then PosH (x)  Move(y).

748

fiAcyclicity Notions for Existential Rules

The JA dependency graph JA() of  is defined as follows. The vertices of JA() are
the existentially quantified variables occurring in . Given arbitrary two such variables y1
and y2 , the JA dependency graph JA() contains an edge from y1 to y2 whenever the rule
that contains y2 also contains a universally quantified variable x such that PosH (x) 6=  and
PosB (x)  Move(y1 ). Set  is JA if JA() does not contain a cycle.
2.5 Rule Normalisation
Existential rules can often be transformed into other existential rules by replacing parts
of the rule head or body with atoms involving fresh predicates. Such a transformation is
called normalisation, and is often used as a preprocessing step to bring the rules into a
suitable form. For example, Horn OWL 2 axioms can be translated into existential rules
by using the well known transformations of first-order logic, and the latter can then be
normalised to a form we describe in Section 6. In this section we introduce a definition of
rule normalisation that captures all similar methods known to us.
Let r be a rule of the form (8), where 1 , 2 , 1 , and 2 are conjunctions of atoms
satisfying ~x1  ~x2 = ~x3  ~x4 , ~z2  ~z3 = , and ~y2  ~y3 = .
1 (~x1 , ~z1 , ~z2 )  2 (~x2 , ~z1 , ~z3 )  ~y1 , ~y2 , ~y3 .[1 (~x3 , ~y1 , ~y2 )  2 (~x4 , ~y1 , ~y3 )]

(8)

A normalisation step replaces a conjunction in either the head or the body of the rule with
an atom involving a fresh predicate. More precisely, a head normalisation step replaces
1 (~x3 , ~y1 , ~y2 ) with atom Q(~x3 , ~y1 ) where Q is a fresh predicate, thus replacing r with rule
(9), and it adds rule (10).
1 (~x1 , ~z1 , ~z2 )  2 (~x2 , ~z1 , ~z3 )  ~y1 , ~y3 .[Q(~x3 , ~y1 )  2 (~x4 , ~y1 , ~y3 )]
Q(~x3 , ~y1 )  ~y2 .1 (~x3 , ~y1 , ~y2 )

(9)
(10)

Alternatively, a body normalisation step replaces 1 (~x1 , ~z1 , ~z2 ) with atom Q(~x1 , ~z1 ) where
Q is a fresh predicate, thus replacing r with rule (11), and it adds rule (12).
Q(~x1 , ~z1 )  2 (~x2 , ~z1 , ~z3 )  ~y1 , ~y2 , ~y3 .[1 (~x3 , ~y1 , ~y2 )  2 (~x4 , ~y1 , ~y3 )]
1 (~x1 , ~z1 , ~z2 )  Q(~x1 , ~z1 )

(11)
(12)

Given a set of existential rules , normalisation steps are often applied to  iteratively. If
the predicate Q introduced in each step is always fresh, we call such normalisation without
structure sharing. In contrast, normalisation with structure sharing allows the predicate
Q to be reused across different normalisation steps. For example, once a predicate Q is
introduced in a head normalisation step to replace 1 (~x1 , ~z1 , ~z2 ), then a conjunction of the
form 1 (~x01 , ~z10 , ~z20 ) where ~x01 , ~z10 , ~z20 are renamings of ~x1 , ~z1 , ~z2 can be replaced with Q(~x03 , ~y10 )
without introducing the corresponding rule (10). An analogous optimisation can be used
in a body normalisation step.
Let 0 be a set of rules obtained via normalisation (with or without structure sharing)
from . It is well known that 0 is a conservative extension of . Consequently, for each
instance I and each BCQ Q that does not use the freshly introduced predicates, we have
  I |= Q if and only if 0  I |= Q.

749

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

3. Novel Acyclicity Notions
Weak acyclicity has considerably influenced the field of data exchange in databases, but it
is a rather strict notion and so it may not be sufficient in many applications of existential
rules. Joint acyclicity significantly relaxes weak acyclicity and was developed mainly for
rule based knowledge representation applications.
In Section 3.1 we show that even joint acyclicityone of the most general acyclicity
notions developed so fardoes not capture rules corresponding to axioms commonly found
in ontologies for which the chase terminates universally. To address this important limitation, we propose in Section 3.2 model-faithful acyclicity (MFA)a novel, very general,
notion that can be used to successfully ensure chase termination for many ontologies used
in practice. The computational cost of checking MFA is, however, rather high; hence, in
Section 3.3 we introduce model-summarising acyclicity (MSA)a more strict notion that
is easier to check and produces the same results as MFA on most existing ontologies.
3.1 Limitations of Existing Acyclicity Notions
To motivate our new acyclicity notions, we first present an example that shows how known
acyclicity notions, such as JA, are not satisfied by rules that are equivalent to very simple
axioms that abound in OWL ontologies.
Example 1. Let  be the set of rules (13)(17).
A(x1 )  y1 .R(x1 , y1 )  B(y1 )

r1 =
r2 =

R(x2 , z1 )  B(z1 )  A(x2 )

(13)
(14)

r3 =

B(x3 )  y2 .R(x3 , y2 )  C(y2 )

(15)

r4 =

C(x4 )  D(x4 )

(16)

r5 =

R(x5 , z2 )  D(z2 )  B(x5 )

(17)

Rules r1 and r2 correspond to the description logic axiom A  R.B, rule r3 corresponds to
axiom B v R.C, rule r4 corresponds to axiom C v D, and rule r5 corresponds to axiom
R.D v B. Such axioms are very common in OWL ontologies.
By the definition of JA from Section 2, we have Move(y1 ) = {R|2 , B|1 , R|1 , A|1 }. Thus,
the JA dependency graph contains an edge from y1 to itself, so the set of axioms  is not
JA. In contrast, the following table shows the chase sequence for I and .
A()

R(, f ())

R(f (), g(f ()))

B()

B(f ())

C(g(f ()))

C()

R(, g())

D(g())

D()

C(g())

D(g(f ()))

R(, )
Rule r2 is not applicable to R(f (), g(f ())) since I3 does not contain the fact B(g(f ()))
necessary to match the atom B(z1 ) from the rule. Thus, the chase terminates.

All existing acyclicity notions essentially try to estimate whether an application of a
rule can produce facts that can (possibly by applying chase to other rules) repeatedly
750

fiAcyclicity Notions for Existential Rules

trigger the same rule in an infinite manner. The key difference between various notions is
how rule applicability is determined. In particular, JA considers each variable in a rule in
isolation and does not check satisfaction of all body atoms at once; for example, rule (14)
is not applicable to the facts generated by rule (15), but this can be determined only by
considering variables x2 and z1 in rule (14) simultaneously. These notions thus overestimate
rule applicability and, as a result, they can fail to detect chase termination.
3.2 Model-Faithful Acyclicity (MFA)
Our main intuition for addressing this problem is that more precise chase termination
guarantees can be obtained by tracking rule applicability more faithfully. A simple solution
is to be completely precise about rule applicability: one can run the skolem chase and then
use sufficient checks to identify cyclic computations. Since no sufficient, necessary, and
computable test can be given for the latter, we must adopt a practical approach. For
example, we can raise the alarm and stop the process if the chase derives a cyclic term
f (~t), where f occurs in ~t. This idea can be further refined; for example, one could stop only
if f occurs nested in a term some fixed number of times. The choice of the appropriate test
thus depends on an application; however, as our experiments show, checking only for one
level of nesting suffices in many cases. In particular, no term f (~t) with f occurring in ~t is
generated in the chase of the set of rules  from Example 1.
Definition 2. A term t is cyclic if a function symbol f exists such that some term f (~s) is
a subterm of t, and some term f (~u) is a proper subterm of f (~s).
Our notion of acyclicity is declarative: the given set of rules  is transformed into a new
set of rules 0 that tracks rule dependencies using fresh predicates; then,  is identified as
being acyclic if 0 does not entail a special nullary predicate C. Since acyclicity is defined
via entailment, it can be decided using any theorem proving procedure for existential rules
that is sound and complete. Acyclicity guarantees termination of the skolem chase, which
also guarantees termination of nonoblivious chase and core chase. We call our notion modelfaithful acyclicity because it estimates rule application precisely, by examining the actual
structure of the universal model of .
Definition 3. For each rule r = (~x, ~z)  ~y .(~x, ~y ) and each variable yi  ~y , let Fir be a
fresh unary predicate unique for r and yi ; furthermore, let S and D be fresh binary predicates,
and let C be a fresh nullary predicate. Then, MFA(r) is the following rule:



^
^
Fir (yi ) 
(~x, ~z)  ~y . (~x, ~y ) 
S(xj , yi )
yi ~
y

xj ~
x

For a set  of rules, MFA() is the smallest set that contains MFA(r) for each rule r  ,
rules (18)(19), and rule (20) instantiated for each Fir corresponding to some r  :

Fir (x1 )

S(x1 , x2 )  D(x1 , x2 )

(18)

D(x1 , x2 )  S(x2 , x3 )  D(x1 , x3 )

(19)

Fir (x2 )

(20)

 D(x1 , x2 ) 

751

C

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

The set  is model-faithful acyclic (MFA) w.r.t. an instance I if I  MFA() 6|= C; furthermore,  is universally MFA1 if  is MFA w.r.t. I .
Example 4. Let  be the set of rules from Example 1. Then, MFA(r1 ) and MFA(r3 ) are
given by (21) and (22), respectively; since r1 and r3 contain a single existentially quantified variable each, we omit the superscripts in Fr1 and Fr3 for the sake of clarity. Thus,
MFA() consists of rules (14), (16), and (17), rules (21)(22), rules (18)(19), and rule
(20) instantiated for Fr1 and Fr3 .
A(x1 )  y1 .R(x1 , y1 )  B(y1 )  Fr1 (y1 )  S(x1 , y1 )

(21)

B(x3 )  y2 .R(x3 , y2 )  C(y2 )  Fr3 (y2 )  S(x3 , y2 )

(22)

It is straightforward to see that the chase of I and MFA() consists of the facts presented
in Example 1, augmented with the following facts:
S(, f ())

D(, f ())

D(f (), g(f ()))

S(, g())

D(, g())

D(, g(f ()))

Fr1 (f ())

S(f (), g(f ()))

Fr3 (g())

Fr3 (g(f ()))

The chase of I and MFA() does not contain C, which implies that I  MFA() 6|= C. As
a result,  is universally MFA.

MFA is formulated as a semantic, rather than a syntactic notion, and is thus mainly
independent from algorithmic details: entailment I  MFA() 6|= C can be checked using an
arbitrary sound and complete first-order calculus. In Section 4 we discuss the relationship
between MFA and existing notions, and we show that MFA generalises most of them.
The following proposition shows that MFA characterises the derivations of the skolem
chase in which no cyclic terms occur.

Proposition 5. A set  of rules is not MFA w.r.t. an instance I if and only if IMFA()
contains a cyclic term.

Proof. Let 0 = MFA(), and let I0 0 , I1 0 , . . . be the chase sequence for I and 0 . Moreover,
let fri be the function symbol used to skolemise the i-th existentially quantified variable in
rule r, as defined in Section 2.2. We next prove that the following claims hold for all terms
t and t0 occurring in Ik 0 , each rule r, each integer i, and each integer k, as well as k = .
1. Term t is of the form fri (~u) if and only if Fir (t)  Ik 0 .
2. Term t is of the form fri (~u) and t0  ~u if and only if S(t0 , t)  Ik 0 .
0

3. If t0 is a proper subterm of t, then D(t0 , t)  Ik+2
0 ; furthermore, D(t , t)  I0 if and
0
only if t is a proper subterm of t.

1. In the rest of this paper we often omit universally; furthermore, when used as an acyclicity notion,
MFA means universally MFA.

752

fiAcyclicity Notions for Existential Rules

(Claims 1 and 2, direction ) The proof is by induction on k. Set I0 0 does not contain
functional terms, and so it clearly satisfies both claims. For the induction step, assume
that both claims hold for Ik1
and consider Ik 0 . Since Ik1
 Ik 0 , both claims clearly hold
0
0
k1
for each term t that occurs in I0 . Consider an arbitrary term t of the form fri (~u) that
0
does not occur in Ik1
u. Clearly, t is introduced into Ik 0 by an
0 , and an arbitrary term t  ~
application of the skolemisation of MFA(r) for some rule r  . Since the head of MFA(r)
contains atoms Fir (yi ) and S(xj , yi ) for each xj  ~x, we have Fir (t)  Ik 0 and S(t0 , t)  Ik 0
for each t0 S~u, and so we have Fir (t)  I0 and S(t0 , t)  I0 for each t0  ~u as well. Finally,
since I0 = k Ik 0 , these claims clearly hold for k = .
(Claims 1 and 2, direction ) Predicate S and each predicate Fir occur in 0 only in head
atoms of the form Fir (yi ) and S(xj , yi ); hence, the skolemised rules contain these predicates
only in head atoms of the form Fir (fri (~x)) and S(xj , fri (~x)), which clearly implies our claim.
(Claim 3, the first part for k 6= ) The proof is by induction on k. The base case holds
vacuously since I0 0 does not contain functional terms. Assume now that the claim holds
for some k  1, and consider an arbitrary term t = fri (~u) occurring in Ik 0 such that t0 is
a subterm of some ti  ~u. By Claim 2, we have S(ti , t)  Ik 0 ; furthermore, ti occurs in
k+1
0
Ik1
0 , so by the induction assumption we have D(t , ti )  I0 . Finally, the rules without
functional terms are applied before the rules with functional terms; hence, by rule (19) we
have D(t0 , t)  Ik+2
0 , as required.
(Claim 3, the second part) The proper subterm relation is transitive, and rules (18)
and (19) effectively define D as the transitive closure of S, which clearly implies this claim.
Assume now that I0 contains a cyclic term t. Then, some term t1 = fri (~s) is a subterm
of t and some term t2 = fri (~u) is a proper subterm of t1 . By Claims 1 and 3, then we have
{Fir (t2 ), D(t2 , t1 ), Fir (t1 )}  I0 . But then, since 0 contains rule (20), we have C  I0 , so
 is not MFA. For the converse claim, assume that  is not MFA w.r.t. an instance I.
Then, by Definition 3 we have that I  MFA() |= C. Since the special nullary predicate C
occurs only on the right-hand side of rule (20), there exist terms t1 and t2 , a rule r  ,
and a predicate Fir such that {Fir (t1 ), D(t1 , t2 ), Fir (t2 )}  I0 . Since Fir (t1 ) and Fir (t2 ) are
contained in I0 , Claim 1 implies that t1 and t2 are of the form t1 = fri (u~1 ) and t2 = fri (u~2 ),
respectively. Finally, D(t1 , t2 )  I0 and Claim 3 imply that t1 is a proper subterm of t2 , so
I0 contains a cyclic term.
This characterisation implies termination of skolem chase of MFA rules  in 2ExpTime.
In particular, a term t derived by the skolem chase of 0 = MFA() cannot be cyclic by
Proposition 5; such t can then be seen as a tree with branching factor bounded by the
maximum arity of a function symbol in sk(0 ) and with depth bounded by the number
of function symbols in sk(0 ). The chase can thus generate at most a doubly exponential
number of different terms and atoms. The 2ExpTime bound already holds if the rules are
WA (Cal et al., 2010b), so CQ answering for MFA rules is not harder than for WA rules.
Proposition 6. If a set of rules  is MFA w.r.t. an instance I, then the skolem chase for
I and  terminates in double exponential time.
Proof. Let 0 = MFA(), let c, f , and p be the number of constants, function symbols,
and predicate symbols, respectively, occurring in sk(0 ), let ` be the maximum arity of a
function symbol, and let a be the maximum arity of a predicate symbol in sk(0 ). Consider
753

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

now an arbitrary term t occurring in I0 ; clearly, t can be seen as a tree with branching
factor ` containing constants in the leaf nodes and function symbols in the internal nodes;
furthermore, since t is not cyclic, dep(t)  f , the number of leaves is bounded by `f , and the
total number of nodes is bounded by f  `f . Each node is assigned a constant or a function
f
symbol, so the number of different terms occurring in I0 is bounded by  = (c + f )f ` ,
and the number of different atoms in I0 is bounded by p  a , which is clearly doubly
exponential in  and I. Consequently, the size of I0 is at most doubly exponential in 
and I. Furthermore, for an arbitrary set of facts I 0 and rule r, the set r(I 0 ) can be computed
by examining all mappings of the variables in r to the terms occurring in I 0 , which requires
exponential time in the size of r and polynomial time in the size of I 0 . Consequently, I0 can
be computed in time that is double exponential in I and . Finally, it is straightforward
to see that I  I0 , so I can be computed in double exponential time as well.
By Proposition 6, answering a BCQ over MFA rules is in 2ExpTime. We next prove
that checking MFA w.r.t. a specific instance I is also in 2ExpTime, and that checking universal MFA is 2ExpTime-hard. This provides tight complexity bounds for both problems.
Towards this goal, we first establish in Lemma 7 a relationship between answering certain
kinds of queries over certain kinds of rules and checking whether a related set of rules is
universally MFA; we use this relationship in several hardness proofs in the rest of this paper.
Then, in Theorem 8 we present our main complexity result.
Lemma 7. Let  be a set of weakly acyclic, constant-free, equality-free, and connected rules
with predicates of nonzero arity, let A and B be unary predicates, let R be a fresh binary
predicate, let a be a constant, and let  be  extended with rule (23).
R(z, x)  B(x)  y.[R(x, y)  A(y)]

(23)

Then, we have {A(a)}   6|= B(a) if and only if  is universally MFA.
Proof. Let I = {A(a)}, and let I0 , I1 , . . . be the chase sequence for I and . Furthermore,
let 0 = MFA(), let J = I , let J0 0 , J1 0 , . . . be the chase sequence for J and 0 , and let
f be the function symbol used to skolemise the existential quantifier in rule (23). Set  is
constant-free, so a is the only constant occurring in each set Ii .
We next show that the facts in Jj 0 are of a certain form. To this end, for each `  0,
let t` = f (. . . f () . . .) where the function symbol f is repeated ` times (by this definition,
we have t0 = ); also, each term or fact obtained from t` by zero or more applications of
predicates or function symbols not in {f, D, S, C, R} is of level `. By induction on the chase
sequence for J and 0 , we next prove that the sequence satisfies the following property ():
for each fact F  Jj 0 , some integer ` exists such that F is of the form R(, )
or R(t` , t`+1 ), or the predicate of F is contained in {D, S, C}, or F is an `-level
fact and the predicate of F is not contained in {D, S, C, R}.
Set J0 0 = J clearly satisfies property () since each fact in it is clearly of level 0. Now assume
that Jj 0 satisfies property () for some j, and consider an application of a rule r  0 . If r
corresponds to rule (18), (19), (20), or (23), then the result of the rule application clearly
satisfies property (). Otherwise, r is safe and no body atom contains a predicate in
754

fiAcyclicity Notions for Existential Rules

{D, S, C, R}; by induction assumption, then some atom is matched to a fact of some level `;
the body atoms of r are connected, so all body atoms are matched to facts of the same level;
finally, the head atoms of r contain function symbols different from f , but no constants or
predicates of zero arity, so each fact derived by an atom in the head of r either contains
predicate S or is of level `.
We next show that the chase sequences for I and , and for J and 0 are related by the
following property ():
for each fact F 0 of level 1 and the fact F obtained by replacing each t1 in F 0
with a, we have F  Ii for some i if and only if F 0  Jj 0 \ J for some j.
The proof of () is straightforward: J contains R(, ) and B(), so J1 0 contains R(, f ())
and A(f ()); moreover, due to (), term t1 plays in the chase sequence for J and 0 the
same role as constant a in the chase sequence for I and , so the rule applications to facts
of level 1 in the former chase sequence correspond one-to-one with rule applications in the
latter chase sequence. We omit the formal details for the sake brevity.
Now assume that {A(a)}   |= B(a). Then, B(a)  Ii holds for some i. By property
(), then integer j exists such that B(f ())  Jj 0 . But then, due to rule (23), some `  j
exists such that A(f (f ()))  J` 0 . By Proposition 5, then  is not universally MFA.
Conversely, assume that {A(a)}   6|= B(a). Since  is weakly acyclic and equalityfree,  is super-weakly acyclic (Marnette, 2009); as we will show in Section 4 (see Theorem
19),  is then MFA as well. Now consider an arbitrary integer j and fact F  Jj 0 . If F
is of level 0 or 1, since  is MFA, fact F does not contain a cyclic term. Furthermore,
B(a) 6 I so, by property (), fact F is not of the form B(f ()); thus, rule (23) does not
fire to introduce facts of level greater than 1. Consequently, F does not contain a cyclic
term, and so, by Proposition 5, the set  is universally MFA.
Theorem 8. Given a set of rules , deciding whether  is MFA w.r.t. an instance I is in
2ExpTime, and deciding whether  is universally MFA is 2ExpTime-hard. Both results
hold even if the arity of predicates in  is bounded.
Proof. (Membership) Let 0 = MFA(), let I0 0 , I1 0 , . . . be the chase sequence for I and 0 ,
and let , p, and a be as stated in the proof of Proposition 6. The number of different
atoms that can be constructed from  terms is bounded by k = p  a ; note that this is
double exponential even if a is bounded. Let k 0 = k + 4; we next show that whether  is
0
0
MFA w.r.t. I can be decided by constructing Ik 0 and then checking whether C  Ik 0 . As in
the proof of Proposition 6, the latter can be done in double exponential time.
0
If Ik 0 = Ik 0 , then I0 = Ik 0 , so  is not MFA if and only if C  Ik 0 . Otherwise, we
0
have Ik 0 ( Ik 0 ; but then, Ik+1
clearly contains at least one cyclic term t = fri (~t) such that
0
t0 = fri (~s) is a subterm of some ti  ~t. Since Ik+1
satisfies Claims 13 from the proof of
0
k+3
Proposition 5, we have D(ti , t)  I0 ; by rule (20) and the fact that rules without functional
0
terms are applied before rules with functional terms, we have C  Ik 0 ; thus, C  I0 , so 
is not MFA by Proposition 5.
(Hardness) We prove the claim by a reduction from the problem of checking I   |= Q,
where  is a weakly acyclic set of equality-free rules and with predicates of bounded arity,

755

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

I is an instance, and Q = ~y .(~y ) is a Boolean conjunctive query. Cal et al. (2010b) show
that, for such I, , and Q, deciding I   |= Q is 2ExpTime-complete. We next transform
I, , and Q so that we can apply Lemma 7, which proves our claim.
Let 1 =   {(~y )  B} where B is a fresh predicate of zero arity; clearly, I   |= Q
holds if and only if I  1 |= B holds.
Let 2 and I2 be obtained by eliminating constants from the rules in 1 ; that is, we
initially set I2 = I and then, for each rule r   and each constant c occurring in r, we
replace all occurrences in c with a fresh variable wc , add an atom Oc (wc ) to the body of r
where Oc is a fresh predicate uniquely associated with c, and add a fact Oc (c) to I2 . It is
straightforward to see that I  1 |= B if and only if I2  2 |= B.
Finally, to transform 2 and I2 into 3 , we define some notation. Let P be a fresh n+1ary predicate unique for each n-ary predicate PV
, and let w be a fresh variable not occurring
in 2 . For a conjunction of atoms , let  = P (~t) P (~t, w). Rule (24) is obtained from
I2 as specified below, where A is a fresh unary predicate, each constant c occurring in I2 is
associated with a distinct, fresh variable vc , and ~vc is the vector of all such variables:
^
A(w)  ~vc .
P (vc1 , . . . , vck , w)
(24)
P (c1 ,...,ck )I2

Finally, the set 3 contains rule (24) and a rule
(~x, ~z, w)  ~y .(~x, ~y , w)

for each rule

(~x, ~z)  ~y .(~x, ~y ) in 2 .

(25)

Clearly, all predicates in 3 are of nonzero arity; all rules in 3 are constant-free and
connected; and A occurs only in the body of rule (24) and 2 is WA, so 3 is WA as well.
Finally, let I3 = {A(a)} where a is a fresh constant; by induction on the chase sequences for
2 and I2 , and 3 and I3 , it is straightforward to show that, for each integer i and each fact
P (c1 , . . . , ck ), we have P (c1 , . . . , ck )  (I2 )i2 if and only if P (fc1 (a), . . . , fck (a), a)  (I3 )i+1
3 ,
where fc1 , . . . , fck are the skolem functions used to skolemise vc1 , . . . , vck in rule (24). Thus,
I2  2 |= B if and only if I3  3 |= B(a), which by Lemma 7 implies our claim.
The results of Theorem 8 are somewhat discouraging: known acyclicity notions can
typically be checked in PTime or in NP. We consider MFA to be an upper bound of
practically useful acyclicity notions. We see two possibilities for improving these results. In
Section 3.3 we introduce an approximation of MFA that is easier to check; our experiments
(see Section 7) show that this notion often coincides with MFA in practice. Furthermore,
we show next that the complexity is lower for rules of the following shape.
Definition 9. A rule r of the form (5) is an -1 rule if ~y is empty or ~x contains at most
one variable.
As we discuss in the following sections, -1 rules capture (extensions of) Horn DLs. We
next show that BCQ answering and MFA checking for -1 rules is easier than for general
rules. Intuitively, if  is MFA and contains only -1 rules, then all functional terms in
sk(MFA()) are unary and hence the number of different terms and atoms derivable by
chase becomes exponentially bounded, as shown by the following theorem.

756

fiAcyclicity Notions for Existential Rules

Theorem 10. Let  be a set of -1 rules, and let I be an instance. Checking whether  is
MFA w.r.t. I is in ExpTime, and checking whether  is universally MFA is ExpTime-hard.
Moreover, if  is MFA w.r.t. I, then answering a BCQ over  and I is ExpTime-complete.
Proof. We defer the proof of both hardness claims to Section 6, which deals with an even
smaller class of rules that correspond to Horn description logic ontologies. In particular,
we prove hardness of BCQ answering in Lemma 59, and hardness of checking whether  is
MFA w.r.t. I in Lemma 60. In the rest of this proof, we show both membership results.
Let 0 = MFA(); let c be the number of constants in an instance; and let f be the
number of function symbols in the rules. Since  contains only -1 rules, 0 also contains
only -1 rules; consequently, all functional terms in sk(0 ) are of arity 1. Hence, each
noncyclic term can be understood as a sequence of at most f function symbols, so the total
number of different noncyclic terms is bounded by  = c  (f + 1)f . The total number of
atoms is bounded by p  a , where p is the number of predicates and a is the maximum
arity of a predicate in 0 . Note that this is exponential even if a is fixed. As in the proof
of Proposition 6, we can now show that either the chase for 0 and I terminates or a cyclic
term is derived in exponential time, which proves that the complexity of checking whether
 is MFA w.r.t. I is in ExpTime.
Finally, since I  I0 , if  is MFA, then I can be computed in exponential time, so
a BCQ over  and I can be answered in ExpTime.
3.3 Model-Summarising Acyclicity (MSA)
The high cost of checking MFA of  arises because the arity of function symbols in sk() is
unbounded and the depth of cyclic terms can be linear in . To obtain an acyclicity notion
that is easier to check, we must coarsen the structure used for cycle analysis. We thus next
introduce model-summarising acyclicity, which summarises the models of  by reusing the
same constant to satisfy an existential quantifier, instead of introducing deep terms.
Definition 11. Let S, D, and Fir be as specified in Definition 3; furthermore, for each rule
r = (~x, ~z)  ~y .(~x, ~y ) and each variable yi  ~y , let cir be a fresh constant unique for r
and yi . Then, MSA(r) is the following rule, where MSA is the substitution that maps each
variable yi  ~y to cir :


^
^
Fir (yi )MSA 
(~x, ~z)  (~x, ~y )MSA 
S(xj , yi )MSA 
yi ~
y

xj ~
x

For a set  of rules, MSA() is the smallest set that contains MSA(r) for each rule
r  , rules (18)(19), and rule (20) instantiated for each predicate Fir . Set  is modelsummarising acyclic (MSA) w.r.t. an instance I if I  MSA() 6|= C; furthermore,  is
universally MSA if  is MSA w.r.t. I .
Example 12. Consider again the set of rules  from Example 1. MSA(r1 ) and MSA(r3 )
are given by rules (26) and (27), respectively; since r1 and r3 contain a single existentially
quantified variable each, we omit the superscripts in Fr1 , Fr3 , cr1 , and cr3 for the sake of

757

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

clarity. Thus, MSA() consists of rules (14), (16), and (17), rules (26)(27), rules (18)
(19), and rule (20) instantiated for Fr1 and Fr3 .
A(x1 )  R(x1 , cr1 )  B(cr1 )  Fr1 (cr1 )  S(x1 , cr1 )

(26)

B(x3 )  R(x3 , cr3 )  C(cr3 )  Fr3 (cr3 )  S(x3 , cr3 )

(27)

The following table shows the chase sequence for I and MSA().
A()

R(, cr1 )

R(cr1 , cr3 )

B()

R(, cr3 )

D(cr3 )

C()

B(cr1 )

S(cr1 , cr3 )

D()

C(cr3 )

D(, cr3 )

R(, )

S(, cr1 )

D(, cr1 )

D(cr1 , cr3 )

S(, cr3 )
Fr1 (cr1 )
Fr3 (cr3 )
The result of the chase does not contain C, and so  is universally MSA.



Note that MSA() is equivalent to a set of datalog rules: the only minor difference is
that the rules in MSA() can contain several head atoms, but such rules can clearly be
transformed into equivalent datalog rules. Thus, MSA can be checked using a datalog reasoner. This connection with datalog and the complexity results by Dantsin, Eiter, Gottlob,
and Voronkov (2001) for checking entailment of a ground atom in a datalog program provide
us with the upper complexity bound for checking MSA in Theorem 13. The complexity of
datalog reasoning is O(r  nv ) where r is the number of rules, v is the maximum number of
variables in a rule, and n is the size of the set of facts that the rules are applied to; thus,
checking MSA should be feasible if the rules in  are short and so v is small.
Theorem 13. For  a set of rules, deciding whether  is MSA w.r.t. an instance I is in
ExpTime, and deciding whether  is universally MSA is ExpTime-hard. The two problems
are in coNP and coNP-hard, respectively, if the arity of the predicates in  is bounded.
Proof. (Membership) Let 0 = MSA(), and note that  is MSA w.r.t. I if and only if
I  0 6|= C if and only if C 6 I0 . The total number of atoms occurring in I0 is p  ca , where
p is the number of predicates, c is the number of constants, and a is the maximum arity of
the predicates in 0 ; this number is clearly exponential if a is not bounded. The rest of the
proof is the same as in Theorem 8.
Assume now that a is bounded; then the number of ground atoms in I0 becomes polynomial. Furthermore, by the definition of the chase, C  I0 if and only if there exist a sequence of rules r1 , . . . , rn of the form ri = i  i and a sequence of substitutions 1 , . . . , n
such that i  I  {j j | j < i}  I0 for each 1  i  n and n n = C. Clearly, we can
assume that n  p  ca , which is polynomial. Thus, we can guess the two sequences in nondeterministic polynomial time, and we can check the required property in polynomial time.
Thus, I  0 |= C can be checked in nondeterministic polynomial time, so checking whether
 is MSA w.r.t. I is in coNP.
758

fiAcyclicity Notions for Existential Rules

(Hardness) Let  be a set of datalog rules, let I be an instance, and let Q be ground
atom. Checking whether I   |= Q is ExpTime-complete in general (Dantsin et al., 2001).
Furthermore, the problem is NP-hard if the arity of predicates is bounded: a rule in  can
encode an arbitrary Boolean conjunctive query with atoms of bounded arity but arbitrarily
many variables, for which answering is well known to be NP-hard.
Let 4 and I4 be obtained from  as in the proof of Theorem 8; then, I   |= Q if and
only if I4  4 |= B(a), and the set of rules  obtained from 4 as specified in Lemma 7 is
universally MFA if and only if I4  4 6|= B(a). Finally, the only existential variable in 
occurs in a rule of the form (23), so it is straightforward to see that  is universally MFA
if and only if T 0 is universally MSA.
Before concluding this section, we present Theorem 14 and Example 15, which together
show that MFA is strictly more general than MSA.
Theorem 14. If a set of rules  is MSA (w.r.t. an instance I), then  is MFA (w.r.t. I)
as well.
Proof. Let 1 = MFA() and let 2 = MSA(). Furthermore, let h be the mapping of
ground terms to constants defined such that h(t) = cir if t is of the form fri (. . .), and h(t) = t
if t is a constant; for an atom A = P (t1 , . . . , tn ), let h(A) = P (h(t1 ), . . . , h(tn )); and for an
instance I, let h(I) = {h(A) | A  I}. Finally, let I0 1 , I1 1 , . . . be the chase sequence for I
and 1 , and let I0 2 , I1 2 , . . . be the chase sequence for I and 2 . Note that sk(2 ) = 2
differs from sk(1 ) only in that the former contains the constant cir in place of each functional term fri (~x). Please note that, although our definition of the chase applies rules with
function symbols after rules without function symbols, one can clearly construct the chase
of the function-free set of rules 2 using any order of rule applications, including the one
corresponding to the order of rule applications in the chase of 1 . Assuming this slight
modification, one can show by a straightforward induction on i that h(Ii 1 )  Ii 2 for each
i; this implies h(I1 )  I2 . Consequently, C 6 I2 clearly implies C 6 I1 ; hence, if  is
MSA, then  is MFA as well, as required.
Example 15. Let  be the set of rules (28)(31).
r1 =

A(x)  y.R(x, y)  B(y)

(28)

r2 =

B(x)  y.S(x, y)  T (y, x)

(29)

r3 =

A(z)  S(z, x)  C(x)

(30)

r4 =

C(z)  T (z, x)  A(x)

(31)

It is straightforward to check that  is universally MFA, but not universally MSA.



3.4 Acyclicity Notions and Normalisation
As mentioned in Section 2.5, existential rules are often normalised into a particular form;
however, this cannot destroy acyclicity: if a set of rules  is MFA, then each set of rules
obtained from  by normalisation is MFA as well. This claim involves certain technical
assumptions about the treatment of equality, which is why we postpone a formal proof
of this statement until Section 5. Next, however, we show that normalisation can have a
positive effect on acyclicity.
759

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Example 16. Let  be the set containing only the following rule:
A(x)  y.[B(x)  A(y)]

(32)

As specified in Section 2.2, this rule is skolemised as follows, which causes that the skolem
chase of  and instance I = {A(a)} does not terminate.
A(x)  B(x)  A(f (x))

(33)

Note, however, that atoms B(x) and A(y) in the head of the rule do not share variables,
so we can normalise this rule as follows, where Q is a fresh predicate of zero arity:
A(x)  B(x)  Q

(34)

Q  y.A(y)

(35)

It is straightforward to check that this normalised set of rules is MFA; in fact, the normalised
set of rules is even JA. Intuitively, normalisation, as defined in Section 2.5, ensures that
each functional symbol introduced during normalisation depends on as few variables in the
rule as possible.

Normalisation, however, can have a negative effect on universal termination, as shown
by the following example.
Example 17. Let  be the set containing only the following rule:
C(z)  R(z, x)  B(x)  y1 y2 .[R(x, y1 )  R(y1 , y2 )  B(y2 )]

(36)

One can readily check that  is universally MFA. Now let 0 be the following set of rules,
which is obtained by replacing conjunction R(y1 , y2 )  B(y2 ) in the rule head with Q(y1 ):
C(z)  R(z, x)  B(x)  y1 .[R(x, y1 )  Q(y1 )]
Q(y1 )  y2 .[R(y1 , y2 )  B(y2 )]

(37)
(38)

Let f1 and f2 be function symbols used to skolemise the existential quantifier in rule (37)
and (38), respectively. Since Q()  I 0 , the chase of 0 and I 0 derives R(, f2 ()) and
B(f2 ()); but then, these facts, C(), and rule (37) derive Q(f1 (f2 ())), after which rule
(38) derives R(f1 (f2 ()), f2 (f1 (f2 ()))) and B(f2 (f1 (f2 ()))). The chase of 0 and I 0 thus
contains a cyclic term, so 0 is not universally MFA.
Intuitively, this problem occurs because the critical instance I 0 for 0 also instantiates
the predicate Q introduced during normalisation. Such predicates, however, cannot occur in
arbitrary input instances, so we can use the critical instance for . Since Q() 6 I , the
skolem chase of 0 and I does not derive a cyclic term, from which we can conclude that
the skolem chase of 0 terminates on each instance I that contains facts constructed using
only the predicates occurring in .


4. Relationship with Known Acyclicity Notions
Many acyclicity notions have been proposed in the literature, but the relationships between
them have been only partially investigated. We next investigate the relationship between
MFA, MSA, and the acyclicity notions known to us, and we produce a detailed picture of
their relative expressiveness. We show that MFA and MSA generalise most of these notions.
760

fiAcyclicity Notions for Existential Rules

4.1 Acyclicity in Databases
Acyclicity notions have been considered in databases in data integration and data exchange
scenarios. Weak acyclicity (Fagin et al., 2005) was one of the first such notions, and it has
spurred on the research into more sophisticated notions for ensuring chase termination.
4.1.1 Super-Weak Acyclicity
Marnette (2009) proposed super-weak acyclicity (SWA), which generalises weak acyclicity
provided that the rules are equality-free. We next recapitulate the definition of SWA, and
then we show that MSA and MFA are strictly more general than SWA.
Definition 18. Let  be a set of existential rules in which no variable occurs in more than
one rule, and let sk be the substitution used to skolemise the rules in .2 A place is a pair
hA, ii where A is an n-ary atom occurring in a rule in  and 1  i  n. A set of places P 0
covers a set of places P if, for each place hA, ii  P , a place hA0 , i0 i  P 0 and substitutions
 and  0 exist such that A = A0  0 and i = i0 . Given a variable w occurring in a rule
r =   ~y ., sets of places In(w), Out(w), and Move(w) are defined as follows:
 set In(w) contains each place hR(~t), ii such that R(~t)   and ti = w;
 set Out(w) contains each place hR(~t)sk , ii such that R(~t)   and ti = w; and
 set Move(w) is the smallest set of places such that
 Out(w)  Move(w) and
 Out(w0 )  Move(w) for each variable w0 that is universally quantified in some
rule in  such that Move(w) covers In(w0 ).
The SWA dependency graph SWA() of  contains a vertex for each rule of , and an
edge from a rule r   to a rule r0   if a variable x0 occurring in both the body and the
head of r0 and an existentially quantified variable y occurring in the head of r exists such
that Move(y) covers In(x0 ). Set  is super-weakly acyclic (SWA) if SWA() is acyclic.
Marnette (2009) uses a slightly different definition: the notation for places is the same
as our notation for positions; a variable may occur in more than one rule so sets In(w),
Out(w), and Move(w) are defined w.r.t. a rule and a variable; and a rule trigger relation is
used instead of the SWA dependency graph. For simplicity, Definition 18 introduces SWA
in the same style as JA; however, both definitions capture the same class of rules.
The following theorem shows that MSA is more general than SWA. Furthermore, in
Example 12 we argued that the set of rules  from Example 1 is MSA, and one can readily
check that  is not SWA. Consequently, MSA is strictly more general than SWA.
Theorem 19. If a set of rules  is SWA, then  is universally MSA.
2. Substitution sk is unique for each rule in Section 2.2; however, since each variable in  occurs in at most
one rule, w.l.o.g. we can take sk as the substitution used to skolemise all the rules in .

761

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Proof. Let 0 = MSA(), let I 0 , I 1 , . . . be the chase sequence for I and 0 , and let I  be the
chase of I and 0 . Furthermore, let  be the function that maps constants to themselves and
that maps ground functional terms as (fri (~t)) = cir , where fri and cir were introduced in Section 2.2 and Definition 11, respectively. Finally, let (P (t1 , . . . , tn )) = P ((t1 ), . . . , (tn )).
We next prove the following property (): for each rule r  , each existentially quantified variable yi occurring in r, each P (~t)  I  where P 6 {S, D, C}, and each tj  ~t such that
tj = cir , a substitution  and a place hA, ji  Move(yi ) exist such that P (~t) = (A ). The
proof is by induction on the length of the chase. Since I 0 = I does not contain a constant
of the form cir , property () holds vacuously for I 0 . Assume now that property () holds for
some I k1 , and consider an arbitrary rule r  , an existentially quantified variable yi in r,
a fact P (~t)  I k \ I k1 with P 6 {S, D, C}, and a term tj  ~t such that tj = cir . Fact P (~t) is
derived in I k from the head atom H of some rule r1  MSA(). Let  be the substitution
used in the rule application; clearly, we have H = P (~t). Furthermore, let r2   be the rule
such that r1 = MSA(r2 ), let r3 = sk(r2 ), and let H 3 be the head atom of r3 that corresponds
to H; clearly, we have (H 3 ) = P (~t). Now if H has cir in position j, then r = r1 since r1
is the only rule that contains cir ; thus, hH 3 , ji  Out(yi )  Move(yi ), so property () holds.
Otherwise, H contains at position j a universally quantified variable x such that (x) = cir .
Let B1 , . . . , Bn be the body atoms of r1 that contain x; clearly, {B1 , . . . , Bn }  I k1 .
All these atoms satisfy the induction assumption, so for each Bm  {B1 , . . . , Bn } and each
0 , `i  Move(y ) and substitu` such that Bm contains variable x at position `, a place hBm
i
0  m ). Let  0 be the substitution obtained from  by
tion  m exist such that Bm  = (Bm
setting  0 (w) =  m (w) for each variable w for which  m (w) is a functional term; clearly,
0  m . But then, Move(y ) covers In(x); hence, by the definition of Move, we have
Bm  0 = Bm
i
that hH 3 , ji  Move(yi ), so property () holds.
0
We additionally prove the following property (): if S(cir , cir0 )  I  for some i and i0 ,
then SWA() contains an edge from r to r0 . Consider an arbitrary such fact, let yi be the
existentially quantified variable of r corresponding to cir , and let k be the smallest integer
0
0
0
such that S(cir , cir0 )  I k . Clearly, S(cir , cir0 ) is derived in I k from the head atom S(x, cir0 )
of rule r0 . Let  be the substitution used in the rule application; thus, (x) = cir . Let
B1 , . . . , Bn be the body atoms of r that contain x; clearly, we have {B1 , . . . , Bn }  I k1 .
All these atoms satisfy property (), so for each Bm  {B1 , . . . , Bn } and each ` such that
0 , `i  Move(y ) and substitution  m exist
Bm contains variable x at position `, a place hBm
i
0
m
such that Bm  = (Bm  ). But then, as in the previous paragraph we have that Move(yi )
covers In(x), so SWA() contains an edge from r to r0 .
Assume now that  is not MSA, so C  I  ; then {Fir (t), D(t, t0 ), Fir (t0 )}  I  holds for
some Fir due to rules (20). But then, since predicate Fir occurs in 0 only in an atom Fir (cir ),
we have t = t0 = cir . Finally, since D is axiomatised in 0 as the transitive closure of S,
clearly SWA() contains a path from r to itself, and so  is not SWA.
The rule set in Example 1 is MSA but not SWA. Furthermore, it is known that SWA is
more general than JA, and the two notions differ only if at least one rule contains a body
atom in which at least one variable occurs more than once (Krotzsch & Rudolph, 2013).
The following example shows that SWA is strictly more general than JA.
Example 20. Let  be the set of the following rules:
r1 =

A(x1 )  y.R(x1 , y)  R(y, x1 )  R(x1 , x1 )
762

(39)

fiAcyclicity Notions for Existential Rules

r2 =

R(x2 , x2 )  B(x2 )

(40)

r3 =

B(x3 )  A(x3 )

(41)

One can readily verify that  is SWA, but not JA.



Theorem 19 holds even if  contains the equality predicate, but provided that the
axiomatisation of equality (cf. Section 2) is taken as part of the input. On such rule sets,
however, SWA, JA, MSA, and MFA are not strictly more general than WA. We discuss the
underlying problems, as well as possible solutions, in Section 5.
4.1.2 Acyclicity by Rewriting
Spezzano and Greco (2010) proposed an acyclicity notion called Adn-WA. Roughly speaking, one first rewrites a set of rules  into another set of rules 0 by adorning the positions
in the predicates that can contain infinitely many terms during the chase; then, one checks
whether 0 is WA. The rewriting algorithm is rather involved, so we do not recapitulate its
definition; instead, we discuss it by means of an example. Spezzano and Greco used this
example to show that Adn-WA is not subsumed by SWA, but the same example also shows
that Adn-WA is not subsumed by MFA either.
Example 21. Let  be the set containing the following rules:
A(x)  y.R(x, y)
B(z)  R(z, x)  A(x)

(42)
(43)

The transformation by Spezzano and Greco (2010) produces a set 0 that consists of
three groups of rules. The first group contains rules (44)(47).
Ab (x)  y.Rbf (x, y)
b

bb

b

(44)

B (z)  R (z, x)  A (x)

(45)

B b (z)  Rbf (z, x)  Af (x)

(46)

f

(47)

ff

A (x)  y.R (x, y)

For each n-ary predicate P , the transformation introduces predicates of the form P m , where
m is an adornmenta string of length n of letters b or f . Intuitively, if m contains letter
b at position i, then during the chase construction the i-th position of P m can contain only
constants occurring in an instance. Rules (44)(47) were derived as follows. Rule (44)
is obtained from rule (42) by marking all positions of variable x with b, which effectively
creates a variant of the rule whose body is applicable only to constants. Variable y in the
head of rule (44) occurs under an existential quantifier, so the corresponding position is
marked with f . Rule (45) is obtained from rule (43) in an analogous way. But then, since
facts introduced by rule (44) can trigger an application of rule (43), the latter rule is marked
as rule (46); predicate Af in the head of rule (46) reflects the fact that variable x in the rule
body is instantiated by atom Rbf (z, x). Finally, facts derived by rule (46) can trigger an
application of rule (42), so the latter rule is instantiated as (47). At this point the algorithm
terminates: since no rule was instantiated with a marking B f in the head, it is not possible
to use predicate Rff to mark the body of rule (43) in a consistent way.
763

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

The second group consists of rules (48)(50), which populate the adorned predicates
with the contents of an instance.
R(x1 , x2 )  Rbb (x1 , x2 )

(48)

A(x)  Ab (x)

(49)

b

B(x)  B (x)

(50)

The third group consists of rules (51)(56), which gather the content of each adorned
predicate P m into a fresh output predicates P .
Rbb (x1 , x2 )  R(x1 , x2 )

(51)

Rbf (x1 , x2 )  R(x1 , x2 )

(52)

ff

R (x1 , x2 )  R(x1 , x2 )
b

(53)

A (x)  A(x)

(54)

Af (x)  A(x)

(55)

b

B (x)  B(x)

(56)

It is straightforward to check that  is not MFA. In contrast, 0 is WA; furthermore,
Spezzano and Greco (2010) show that, for each instance I and each vector of ground terms
~t, we have P (~t)  I0 if and only if P (~t)  I . Since 0 is WA, I0 is finite, and, by the
previously mentioned property, I is finite as well.

The following example shows that MFA is not subsumed by Adn-WA, which indicates
that MFA and Adn-WA are incomparable.
Example 22. Let  be the set containing the following rules:
A(x)  y.R(x, y)  B(y)

r1 =

S(z, x)  B(x)  y.S(x, y)

r2 =

(57)
(58)

The rules in the first group of the set 0 obtained by the transformation are shown below;
we do not show the rules in the second and the third group for the sake of brevity.
Ab (x)  y.Rbf (x, y)  B f (y)
bb

b

bf

(59)

S (z, x)  B (x)  y.S (x, y)

(60)

S bf (z, x)  B f (x)  y.S ff (x, y)

(61)

ff

f

ff

S (z, x)  B (x)  y.S (x, y)

(62)

The last rule ensures that the WA dependency graph for 0 contains a special edge from
position S ff |2 to itself; thus, 0 is not WA, and therefore  is not Adn-WA. In contrast,
one can readily verify that  is MFA.

Spezzano and Greco (2010) also proposed several optimisations of this transformation,
the discussion of which is out of scope of this paper. All of them can be seen as unfolding
the rules in  up to a certain number of chase steps. This seems close to an idea by Baget
764

fiAcyclicity Notions for Existential Rules

et al. (2011b), who propose to run the chase for some fixed number of steps before checking
for potential cycles. A similar effect could be obtained by extending the notion of MFA to
check for terms that contain a function symbol nested some fixed number of times.
Finally, note that the transformation by Spezzano and Greco (2010) is independent
from the notion used to check the acyclicity of the transformed rule set; hence, given an
arbitrary acyclicity notion X, one can define Adn-X in the obvious way. Given arbitrary
notions X and Y such that X  Y , it is obvious that Adn-X  Adn-Y ; consequently, we
have Adn-X 6 MFA for each X such that WA  X. In contrast, however, it not obvious
whether the inclusion between Adn-X and Adn-Y is strict whenever the inclusion between
X and Y is strict, or whether MFA is contained in Adn-X for some X with WA  X.
Finally, we conjecture that X  Adn-X holds for an arbitrary notion X, but we do not
have a formal proof of this conjecture. Due to the complex nature of the rewriting, we
refrain from further analysis of these relationships.
4.1.3 Monitor Graph
Meier et al. (2009) propose an idea that is similar in spirit to MFA. The idea is to track
each chase step in an additional data structure called the monitor graph. If the chase is
infinite, then the monitor graph contains cycles of arbitrary length; conversely, if one can
show that the monitor graph does not contain a cycle of some fixed length, then the chase is
guaranteed to terminate. While this idea is closely related to MFA, note that the definition
of MFA is semantic; hence, one can use an arbitrary theorem proving technique to check
whether MFA() |= C. In contrast, the notion of a monitor graph is specifically tied to the
nonoblivious chase. It is well known that the result of the nonoblivious chase depends on
the order in which the rules applied; consequently, a set of rules can be identified as cyclic
or acyclic depending on the selected rule application strategy. Because of this dependence,
it is difficult to compare the monitor graph approach with other acyclicity notions.
4.2 Acyclicity in Knowledge Representation
Existential rules can capture knowledge representation formalisms such as Horn fragments
of description logics (see Section 6), conceptual graphs (Baget, 2004; Baget et al., 2011a),
and datalog rules (Cal et al., 2010a), and so acyclicity notions allow for materialisationbased query answering over knowledge bases. In this context, Baget (2004) and Baget et al.
(2011a) proposed the notion of acyclic graph rule dependencies (aGRD). Intuitively, aGRD
introduces a rule dependency relation  for which r1  r2 means that an application of
rule r1 on an instance I can subsequently trigger an application of rule r2 . If the relation
 is acyclic, then no rule can trigger itself so the skolem chase terminates on an arbitrary
instance. This can be formalised as follows.
Definition 23. The rule dependency relation      on a set of rules  is defined
as follows. Let r1 = 1  ~y1 .1 and r2 = 2  ~y2 .2 be arbitrary rules in , and let
sk(r1 ) = 1  10 and sk(r2 ) = 2  20 . Then, r1  r2 if and only if there exist an instance
I, a substitution 1 for all variables in sk(r1 ), and a substitution 2 for all variables in sk(r2 )
such that 1 1  I, 2 2 6 I, 2 2  I  10 1 , and 20 2 6 I  10 1 . Set  has an acyclic
graph of rules dependencies (aGRD) if the relation  on  is acyclic.

765

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Definition 23 differs from the original definition by Baget (2004) in several ways. First,
Baget uses fresh nulls to capture the effect of existential quantifiers, whereas Definition 23
uses skolem functions; however, this does not change the resulting relation  in any way.
Second, Baget does not require 20 2 6 I  10 1 . This condition intuitively ensures that
an application of r1 to I enables r2 to derive something new; analogous optimisations were
proposed by Deutsch et al. (2008) and Greco et al. (2012). It should be clear that Definition
23 is stronger than the one by Baget. To unify the notions used in various parts of this
paper, we included this optimisation into Definition 23; however, we nevertheless call the
resulting stronger notion aGRD.
The following example shows that aGRD, even in its weaker form as originally proposed
by Baget (2004), is not contained in SWA.
Example 24. Let  be the set consisting of the following rule:
A(z1 , x, z2 )  B(z2 )  y1 y2 .A(x, y1 , y2 )

r=

(63)

To see that r  r does not hold, consider the skolemisation r0 of r:
A(z1 , x, z2 )  B(z2 )  A(x, f1 (x), f2 (x))

sk(r) =

(64)

Now let I be an arbitrary instance, and let 1 and 2 be arbitrary substitutions such that
{A(z1 , x, z2 )1 , B(z2 )1 }  I and {A(z1 , x, z2 )2 , B(z2 )2 } 6 I. Since instance I contains
only constants, atom A(x, f1 (x), f2 (x))1 is of the form A(a, f1 (a), f2 (a)); but then, for
{A(z1 , x, z2 )2 , B(z2 )2 }  I  {A(a, f1 (a), f2 (a))} to hold, it must be that 2 (z2 ) = f2 (a);
thus, B(z2 )2 = B(f2 (a)) should be contained in I, which is impossible since I is an instance
and thus does not contain functional terms. Note that the additional condition by Greco et al.
(2012) plays no role here. Thus, we have r 6 r, so  is aGRD even in the weaker form by
Baget (2004). However, one can easily check that  is not SWA.

However, aGRD seems to be a rather weak notion: as the following example shows, even
a set of rules without existential quantifiers can be cyclic according to this criterion.
Example 25. Let  be the set consisting of the following rules:
r1 =

A(x)  B(x)

(65)

r2 =

B(x)  C(x)

(66)

r3 =

C(x)  A(x)

(67)

To see that r1  r2 , let I = {A(a)}, let  = {x 7 a}, and note that A(x)  I, B(x) 6 I,
B(x)  I  {B(x)}, and C(x) 6 I  {B(x)}. Analogously, by taking I = {B(a)} we get
r2  r3 , and by taking I = {C(a)} we get r3  r1 . Consequently,  is not aGRD. However,
 is obviously WA since it does not contain existentially quantified variables.

Baget et al. (2011a) suggested that rule dependencies become more powerful if they are
combined with an arbitrary acyclicity notion X. Intuitively, the main idea is to use  to
partition a set of rules into strongly connected components, and then check whether each
component is X; we call this notion X  . This idea can be formalised as follows.
766

fiAcyclicity Notions for Existential Rules

Definition 26. Let  be a set of existential rules, and let  be the rule dependency relation
on . Relation  is extended to arbitrary sets C   and C 0   such that C  C 0 if
0
and only if rules r  C and r0  C 0 exist such that
Sn r  r . A dependency partition of 
is a sequence of sets 1 , . . . , n such that  = i=1 i , each i is a strongly connected
component of , and j 6 i for all i and j such that 1  i < j  n.
Let X be an arbitrary acyclicity notion. Then,   X  if a dependency partition
1 , . . . , n of  exists such that, for each 1  i  n, we have i  X, or i consists of
a single rule ri such that ri 6 ri .
If  is aGRD, then each strongly connected component i contains a single rule ri such
that ri 6 ri . Now if Definition 26 did not consider the special case where i consists of a
single rule that does not depend on itself, then SWA would not extend aGRD; for example,
the rule in Example 24 would not be in SWA . The extra condition in Definition 26 thus
ensures that aGRD is contained in X  regardless of the choice of X, and that aGRD can
be understood as  the acyclicity notion obtained by extending the empty notion (i.e.,
the notion under which no rule set is acyclic) with rule dependencies.
We next present two simple results. Proposition 27 precludes inclusions between certain
acyclicity notions and will thus help us establish proper inclusions between many acyclicity
notions. Furthermore, Proposition 28 shows that combining an acyclicity notion contained
in SWA with rule dependencies creates a strictly stronger acyclicity notion; note that this
holds even for the weaker form of rule dependencies originally proposed by Baget (2004).
Proposition 27. Let X and Y be acyclicity notions such that X  Y . Then, X   Y  .
Furthermore, if there exists a set   Y \ X whose rule dependency relation has a cycle
containing all the rules from , then Y 6 X  , Y  6 X  , and X  ( Y  .
Proof. Relationship X   Y  is immediate from Definition 26. Assume now that there
exists a set of rules   Y \ X whose rule dependency relation has a cycle containing all the
rules from . By Definition 26,  6 X implies  6 X  , and   Y implies   Y  . But
then, we clearly have Y 6 X  and Y  6 X  , and the latter clearly implies X  ( Y  .
Proposition 28. For each acyclicity notion X such that X  SWA, we have X ( X  and
aGRD 6 X.
Proof. Set  from Example 24 is in aGRD and thus in X  ; however,  is not in SWA and
hence not in X either.
MSA also does not contain aGRD; however, unlike for SWA, our claim depends on the
optimisation in Definition 23. An analysis of the relationship between MSA and the version
of rule dependencies originally proposed by Baget (2004) is out of scope of this paper.
Example 29. Let  be the set consisting of the following rules:
r1 =

R(x1 , x1 )  U (x1 , z)  U (x2 , z)  R(x1 , x2 )

(68)

r2 =

R(z, x)  y.T (x, y)

(69)

r3 =

T (z, x)  y.U (x, y)

(70)

767

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

It is obvious that r1  r2 , r1 6 r3 , r2 6 r1 , r2 6 r2 , r2  r3 , r3 6 r2 , and r3 6 r3 . We next
argue that r1 6 r1 and r3 6 r1 , which implies that  is aGRD.
To see that r1 6 r1 , assume that an application of r1 to an instance I produces an atom
of the form R(a, b); due to atom R(x1 , x1 ) in the body of r1 , we have R(a, a)  I. Now let
I 0 = I  {R(a, b)}; since R(a, a)  I, the rule application derives something new only if
a 6= b. Now assume that a substitution 2 exists that makes r1 applicable to I 0 but not to I;
this rule application must use the fact R(a, b), which implies that R(x1 , x1 )2 = R(a, b);
however, this is impossible since a 6= b. Consequently, we have r1 6 r1 , and this holds even
for the version of rule dependencies by Baget (2004).
Furthermore, to see that r3 6 r1 , assume that r3 is applicable to an instance I, and that
the rule application derives a fact of the form U (a, f (a)). Now let I 0 = I  {U (a, f (a))},
and assume that a substitution 2 exists that makes r1 applicable to I 0 but not to I; this
rule application must use the fact U (a, f (a)), which implies that 2 (x1 ) = 2 (x2 ) = a and
2 (z) = f (a). Furthermore, rule r1 is applicable only if R(a, a)  I; but then, the rule application does not derive something new since R(x1 , x2 )2 = R(a, a). Consequently, we have
r3 6 r1 ; however, unlike in the previous paragraph, this claim depends on the optimisation
in Definition 23.
Consider now the chase of I and MSA() as shown below (facts involving the predicates
D, Fr2 , and Fr3 are omitted for clarity). The chase result contains C, so  is not in MSA,
and thus aGRD 6 MSA; as a corollary, we also get MSA ( MSA .
R(, )

T (, cr2 )

U (cr2 , cr3 )

T (, )

U (, cr3 )

S(cr2 , cr3 )

U (, )

S(, cr2 )

R(, cr2 )

T (cr2 , cr2 )

C

S(cr2 , cr2 )

S(, cr3 )
Note that R(, cr2 ) is derived from R(, ), U (, cr3 ), and U (cr2 , cr3 ), where the latter two
facts are obtained from distinct instantiations of MSA(r3 ). Rule dependencies, however,
analyse rule applicability w.r.t. sk(r3 ), which is closer to the actual skolem chase.

In contrast to this result, in Theorem 32 we will show that extending MFA with rule
dependencies does not create a stronger notion: MFA coincides with MFA, which implies
that X   MFA for each notion X such that X  MFA. Towards this goal, we show in
Lemma 30 that independent rule sets can be evaluated independently, and in Lemma 31
that a single rule that does not depend on itself can be applied only once.
Lemma 30. Let 1 and 2 be sets of existential rules such that 2 6 1 , and let F be a
set of ground facts not containing a function symbol in sk(2 ). Then, F1 2 = (F1 )
2 .
Proof. Let F0 = F1 ; let F0 , F1 , . . . be the chase sequence for F0 and 2 where, for convenience, we assume each Fi to be obtained from Fi1 by a single rule application (this
assumption is clearly w.l.o.g.); and let F 0 = (F0 )
2 . By the definition of the skolem chase,
0

we clearly have F  F1 2 . Furthermore, assume that F1 2 6 F 0 ; then, a skolemised
rule r1  sk(1 ) of the form r1 = 1 (~x1 )  1 (~x1 ) exists such that F 0 ( r1 (F 0 ). Fix the
smallest i such that Fi ( r1 (Fi ) (we clearly have i > 0), and let 1 be the substitution used
in the application of r1 . Furthermore, let r2  sk(2 ) be the skolemised rule of the form
768

fiAcyclicity Notions for Existential Rules

r2 = 2 (~x2 )  2 (~x2 ) that is used to derive Fi from Fi1 , and let 2 be the substitution
used in the application of r2 . Now consider an arbitrary term f (~x2 ) in the head of r2 and
assume that f (~x2 )2 occurs in Fi1 ; since the function symbol f is private to r2 , the head
of r2 must have been already instantiated for 2 ; but then, 2 2  Fi1 , which contradicts
our assumption that 2 2  Fi \ Fi1 . Thus, we have the following property (?):
for each term f (~x2 ) occurring in the head of r2 , ground term f (~x2 )2 does not
occur in Fi1 .
Finally, let  be a function that maps each ground term in Fi1 to a fresh distinct constant;
let I = (Fi1 ); let 20 be the substitution defined by 20 (w) = (2 (w)) for each variable w
in r2 ; and let 10 be the substitution defined as follows for each variable w in r1 :
 10 (w) = f ((~t)) if 1 (w) = f (~t) for f a function symbol private to r2 ; and
 10 (w) = (1 (w)) otherwise.
We clearly have 2 20  I and 2 20 6 I; furthermore, by (?), we also have 1 10  I  2 20
and 1 10 6 I  2 20 . Moreover, 1 10 6 I follows from our assumption that i is the smallest
integer such that Fi ( r1 (Fi ). But then, by Definition 23, we have r2  r1 and, consequently,
2  1 as well, which is a contradiction.
Lemma 31. Let  = {r} be a singleton rule set such that r 6 r, and let F be a set of facts
not containing a function symbol in sk(). Then, F = (F ).
Proof. Let F0 = F , and let F0 , F1 , . . . be sets of facts such that each S
Fi+1 is the union of
Fi with the result
of a distinct
single application of r to F0 ; clearly, i Fi = (F0 ). Now
S
S
assume that i Fi ( ( i Fi ); then analogously to the proof of Lemma 30, one can show
that r  r, which is a contradiction; we omit the details for the sake of brevity.
Theorem 32. Let  be an arbitrary set of rules and let I be an arbitrary instance. If  is
MFA w.r.t. I, then  is also MFA w.r.t. I.
Proof. Assume that  is in MFA ; let I be an arbitrary instance; let 1 , . . . , Sn be a
dependency partition of ; let 0 =  and I0 = I; and, for each 1  i  n, let i = i`=1 `
and Ii = (Ii1 )
i . By the definition of dependency partitions, we have that i 6 i1 holds
for each 1  i  n. We next show that, for each 0  i  n, the following two properties hold:
(a) Ii = (I0 )
i , and
(b) Ii does not contain a cyclic term.
Set I0 does not contain functional terms and hence it trivially satisfies (a) and (b). Now
consider arbitrary 0 < i < n such that Ii1 satisfies (a) and (b). By the induction assump

tion, Lemma 30, i 6 i1 , and i = i  i1 , we have that (I0 )
i = ((I0 )i1 )i ; thus,
Ii satisfies (a). To see that Ii satisfies (b), note that no function symbol used to skolemise
the rules in i is used to skolemise the rules in i1 ; we call this property (?). Now there
are two ways to compute Ii .
 Assume that i = {ri } such that ri 6 ri . By Lemma 31, we have Ii = ri (Ii1 ); but
then, Ii does not contain a cyclic term due to (?).
769

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

 If i is MFA, then Ii does not contain a cyclic term due to (?) and Proposition 5.
 = I  and that I does not contain a cyclic
From the above claim we have that In = I
n

n
term; but then,  is MFA w.r.t. I by Proposition 5.

Combinations of rule dependencies with acyclicity notions have also been considered in
databases: Deutsch et al. (2008) proposed a notion of stratification, and Meier et al. (2009)
further developed this idea and proposed a notion of c-stratification. Roughly speaking, each
such notion checks whether all strongly connected components of a certain rule dependency
graph are WA. The rule dependency notions, however, were developed for the nonoblivious
chase and are thus different from Definition 23, as illustrated by the following rule:
r=

R(z, x)  y.R(x, y)  R(y, y)

(71)

The skolem chase on the critical instance for r is infinite, and r  r by Definition 23. In
contrast, rule r does not pose problems for the nonoblivious chase. In particular, assume
that the rule is matched to an atom R(t1 , t2 ), and that it derives R(t2 , t3 ) and R(t3 , t3 ).
Then, rule r is not applicable to R(t2 , t3 ) or R(t3 , t3 ) since in either case the head atom
is satisfied; hence, the rule dependency graphs by Deutsch et al. and Meier et al. are both
empty. These results can be summarised as follows: if a rule set  satisfies the notion by
Deutsch et al., then for each instance I there exists a finite nonoblivious chase sequence;
furthermore, if  satisfies the notion by Meier et al., then for each instance I all chase
sequences (regardless of the rule application strategy) are finite. Meier (2010) discusses in
detail the subtle differences between these notions. Since these notions consider a different
chase variant, we do not discuss them any further in this paper.
4.3 Acyclicity and Logic Programming
Acyclicity notions have also been considered in the context of disjunctive logic programs
with function symbols under the answer set semantics, with the goal of ensuring that a given
program has finitely many answer sets, all of which are finite. All of these notions must deal
with disjunction and nonmonotonic negation, which is one of the main differences to the
notions considered thus far. All notions from logic programming, however, are applicable to
rules without disjunction and nonmonotonic negation, in which case they ensure termination
of the skolem chase. Therefore, in this section we compare such specialisations of the
acyclicity notions from logic programming with aGRD, WA, JA, SWA, MSA, and MFA.
We simplify all definitions so that they apply only to skolemised existential rulesthat is, we
do not present parts of definitions that handle disjunctions in the head and nonmonotonic
negation and function symbols in the body.
4.3.1 Finite Domain Notion
Calimeri et al. (2008) proposed a finite domain (FD) notion. We next recapitulate this
definition, but we do so in the style of Greco et al. (2012), which will come useful in Section
4.3.3 when we introduce -acyclicity. Both approaches use an argument graph to determine
possible ways for propagating ground terms between positions during chase. The definition
of the argument graph is the same as that of the WA dependency graph (see Section 2.4),

770

fiAcyclicity Notions for Existential Rules

but without the distinction between regular and special edges. To simplify the presentation,
we consistently use the WA dependency graph instead of the argument graph.
Definition 33. Let  be a set of rules. A position P |i is -recursive with a position Q|j
if the WA dependency graph WA() contains a cycle (consisting of regular and/or special
edges) going through P |i and Q|j . The set PosFD () of finite domain positions of  is the
largest set of positions in  such that, for each position P |i  PosFD (), each rule r   of
the form r = (~x, ~z)  ~y .(~x, ~y ), and each head atom of r of the form P (~t), the following
conditions are satisfied:
 if the i-th component of ~t is a variable x  ~x, then PosB (x)  PosFD () 6= ; and
 if the i-th component of ~t is a variable y  ~y , then, for each variable x  ~x, some
position Q|j  PosB (x)  PosFD () exists that is not -recursive with P |i .
Set  is FD if PosFD () coincides with the set of all positions in .
Note that the notion of -recursive positions introduced above is symmetric: if P |i is
-recursive with Q|j , then Q|j is also -recursive with P |i . Furthermore, note that Calimeri
et al. (2008) defined FD as follows:
A set of rules  is FD if, for each rule r = (~x, ~z)  ~y .(~x, ~y ) in , each atom
Q(~t) in the head of r, each j-th term of ~t that is an existential variable y, and
each variable x  ~x, there exists a position P |i  PosB (x) such that Q|j is not
-recursive with P |i .
Conditions in the above definition clearly correspond to the conditions in Definition 33; but
then, since PosFD () was defined as the maximal set satisfying these conditions, the two
definitions of FD coincide.
We next show that WA is strictly contained in FD. To this end, we first prove that WA
is contained in FD, and then we present an example showing that the inclusion is strict.
Proposition 34. If a set of rules  is WA, then  is FD.
Proof. Let  be a set of rules that is not FD. Then, there exist a rule r  , an atom Q(~t)
in the head of r, a j-th term of ~t equal to an existential variable y, and a variable x  ~x such
that each position P |i  PosB (x) is -recursive with Q|j . The set PosB (x) is not empty (~x
contains precisely those variables occurring both in the body and the head of the rule),
so choose an arbitrary position P |i  PosB (x). The WA dependency graph WA() then
contains a special edge from P |i to Q|j . Furthermore, since Q|j is -recursive with P |i ,
graph WA() contains a cycle going through P |i and Q|j . Thus, WA() clearly contains a
cycle containing a special edge, so  is not WA.
Example 35. Let  be the set containing rules (72) and (73).
r1 =

R(z, x)  A(x)  y.S(x, y)
S(x1 , x2 )  R(x1 , x2 )

r2 =

771

(72)
(73)

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Set  is not WA since the WA dependency graph contains a special edge from R|2 to S|2 and
a regular edge from S|2 to R|2 . However,  is FD because position S|2 is not -recursive
with A|1  PosB (x). Together with Proposition 34, we can conclude that WA ( FD.
In addition, we have r1  r2 and r2  r1 . In Section 4.3.2 we will prove that FD  JA;
hence, FD ( FD , WA ( FD , and FD 6 WA from Propositions 27, 28, and 34.

4.3.2 Argument-Restricted Rule Sets
Lierler and Lifschitz (2009) proposed the notion of argument-restricted rule sets, whose
definition we summarise next.
Definition 36. An argument ranking for a set of rules  is a function  that assigns a
nonnegative integer to each position in  such that the following conditions are satisfied for
each rule r  , each universally quantified variable x in r, and each existentially quantified
variable y in r:
1. for each P |i  PosH (x), some Q|j  PosB (x) exists such that (P |i )  (Q|j ); and
2. for each P |i  PosH (y), some Q|j  PosB (x) exists such that (P |i ) > (Q|j ).
Set  is argument restricted (AR) if an argument ranking for  exists.
An argument-restricted set of rules has a finite skolem chase on an arbitrary instance:
by a straightforward induction on the chase sequence, one can show that dep(ti )  (P |i )
for each ground fact P (t1 , . . . , tn ) derived by the chase and each 1  i  n.
We next show that JA is strictly more general than AR. Towards this goal, we first
prove an auxiliary lemma that establishes a relationship between the set Move from the
definition of JA and an argument ranking; next, we use this lemma to prove that AR  JA;
and finally we present an example that shows this inclusion to be proper.
Lemma 37. Let  be a set of rules, let  be an argument ranking for , let y be an
existentially quantified variable in , and let Move(y) be the set of positions used in the
definition of JA. For each position P |i  Move(y), some position Q|j  PosH (y) exists such
that (P |i )  (Q|j ) holds.
Proof. Let y be an existentially quantified variable occurring in some rule r  , and
consider an arbitrary position P |i  Move(y). We prove the claim by induction on the
definition of Move(y). The base case when P |i  PosH (y) is trivial. Assume now that
P |i  PosH (x) for some variable x occurring in a rule r0  , and that PosB (x)  Move(y),
so P |i needs to be added to Move(y). By the definition of an argument ranking and since
P |i  PosH (x), position P 0 |`  PosB (x) exists such that (P |i )  (P 0 |` ). But then, since
P 0 |`  PosB (x)  Move(y), by the induction hypothesis we have that position Q|j  PosH (y)
exists such that (P 0 |` )  (Q|j ). Thus, (P |i )  (Q|j ) holds, as required.
Theorem 38. If a set of rules  is AR, then  is JA.
Proof. Assume that  is AR, let  be an argument ranking for , and let JA() be the JA
dependency graph for . We next prove the following claim: for each edge in JA() from
a variable y1 to a variable y2 , and for each position Q|j  PosH (y2 ), there exists a position
772

fiAcyclicity Notions for Existential Rules

P |i  PosH (y1 ) such that (P |i ) < (Q|j ). Consider an arbitrary edge from y1 to y2 in
JA() and an arbitrary position Q|j  PosH (y2 ). By the definition of the JA dependency
graph, then the rule r that contains y2 also contains a universally quantified variable x such
that x occurs in the head of r and PosB (x)  Move(y1 ). Since  is an argument ranking for
, some position P 0 |`  PosB (x) exists such that (P 0 |` ) < (Q|j ). Since P 0 |`  Move(y1 ),
by Lemma 37 position P |i  PosH (y1 ) exists such that (P |i )  (P 0 |` ). Thus, we have
(P |i ) < (Q|j ), and so our claim holds. But then, this claim clearly implies that the JA
dependency graph JA() is acyclic, and therefore  is JA.
Example 39. Let  be the set consisting of the following rules:
r1 =

R(z1 , x1 )  y1 .S(x1 , y1 )

(74)

r2 =

R(z2 , x2 )  y2 .S(y2 , x2 )

(75)

r3 =

S(x3 , x4 )  T (x3 , x4 )

(76)

r4 =

T (x5 , x6 )  T (x6 , x5 )  R(x5 , x6 )

(77)

Let  be an argument ranking for . Then, (R|2 ) < (S|2 ) due to (74); (R|2 ) < (S|1 )
due to (75); (S|1 )  (T |1 ) and (S|2 )  (T |2 ) due to (76); and (T |2 )  (R|2 ) or
(T |1 )  (R|2 ) due to (77). Together, these observations are contradictory, so such  cannot exist and  is not AR. In contrast, Move(y1 ) = {S|2 , T |2 } and Move(y2 ) = {S|1 , T |1 },
and so  is JA.
In addition, we have r1  r3 , r2  r3 , r3  r4 , r4  r1 , and r4  r2 ; hence, we have
AR ( AR , AR ( JA , and JA 6 AR from Theorem 38 and Propositions 27 and 28. 
Lierler and Lifschitz (2009, Thm. 4) proved that AR is strictly more general than FD.
We next present an example that shows FD ( AR, but that also settles the relationships
between FD and AR .
Example 40. Let  be the set consisting of the following rules:
A(x)  y.R(x, y)

r1 =

R(x1 , x2 )  S(x1 , x2 )

r2 =
r3 =

S(z, x)  B(x)  A(x)

(78)
(79)
(80)

The WA dependency graph for  contains a special edge from A|1 to R|2 , as well as regular
edges from R|2 to S|2 and from S|2 to A|1 ; thus, R|2 is -recursive with A|1 . Consequently,
rule (78) cannot satisfy the conditions in Definition 33, so we have R|2 6 PosFD (), and
thus  is not FD. In contrast,  is AR, as evidenced by the following argument ranking:
 = {A|1 7 0, B|1 7 0, R|1 7 0, R|2 7 1, S|1 7 0, S|2 7 1}
In addition, we have r1  r2 , r2  r3 , and r3  r1 ; hence, FD ( FD , FD ( AR ,
and AR 6 FD from Propositions 27 and 28.

Finally, we note that -restricted programs by Gebser et al. (2007) and -restricted
programs by Syrjanen (2001) are both included in FD and AR; thus, when restricted to
skolemised existential rules, these notions are also included in JA.
773

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

4.3.3 -Acyclicity
Greco et al. (2012) recently proposed the notion of -acylicity for logic programs with
function symbols. The original definition of -acyclicity is rather complex, so we next
present a simplified version of -acyclicity that is applicable to existential rules. To unify
the naming style for the notions in this paper, we often write -acyclicity as A.
Greco et al. (2012) introduce a notion of an activation graph, which tracks whether a rule
can trigger another rule. This notion is closely related to the notion of rule dependencies
from Definition 23, but with the requirement that I is an arbitrary finite set of ground facts
(possibly containing functional terms). To understand why the latter is needed in logic
programming, consider the following logic program:
r1 =

A(x)  B(x)  A(f (x))

(81)

r2 =

A(x)  B(x)  B(f (x))

(82)

If we restrict the set I in Definition 23 to be an instance, then r1 6 r2 and r2 6 r1 ; however,
the skolem chase of r1 , r2 , and facts A(a) and B(a) is infinite. Intuitively, r1 and r2 contain
the same function symbol f , so to determine whether an application of r1 can trigger an
application of r2 , we must allow the set I in Definition 23 to contain facts such as B(f (a)).
In our setting, however, function symbols are introduced by skolemisation and are thus
private to each rule, which allows us to restrict the set I in Definition 23 to facts without
functional terms. Thus, in the rest of this section, we simply reuse the rule dependency
relation  from Definition 23, which gives us a slightly stronger version of A for existential
rules than the one proposed by Greco et al. (2012).
Furthermore, Greco et al. (2012) handle logic programming rules with functional terms
in the body. Such rules, however, are not considered in this paper, which allows us to omit
the definition of a labelled argument graph and simplify the notion of a propagation graph
to a subset of the WA dependency graph.
We are now ready to present a simplified version of -acyclicity that is applicable to
existential rules.
Definition 41. Let  be a set of rules. The rule dependency relation  is taken from
Definition 23, and the set of finite domain positions PosFD () is taken from Definition 33.
The set of safe positions of , written PosS (), is the least set of the positions of 
such that PosFD ()  PosS (), and P |i  PosS () if and only if, for each rule r  , at
least one of the following conditions is satisfied:
 if P occurs in the head of r, then  does not contain a cycle going through r, or
 for each atom P (~t) in the head of sk(r) and each variable x that occurs in i-th component of ~t, we have PosB (x)  PosS () 6= .
A position is affected if it is not safe. The propagation graph PG() for  has the
affected positions of  as vertices, and the edges of PG() are defined as in weak acyclicity,
but restricted to affected positions. The set  is -acyclic (A) if PG() does not contain
a cycle that involves a special edge.

774

fiAcyclicity Notions for Existential Rules

In order to relate A to the notions considered thus far, we first establish some containment relationships. It is obvious from Definition 41 that FD  A: if all positions in  are
finite domain, then they are also safe and so the propagation graph is empty. Furthermore,
the set of rules in Example 40 is actually A (all positions are safe), but not FD; hence, by
Proposition 27, we have that FD ( A, FD ( A , and A 6 FD . Next, Proposition 42
observes that aGRD is contained in A, and Theorem 43 shows that, perhaps somewhat
surprisingly, A is contained in AR .
Proposition 42. If a set of rules  is aGRD, then  is A.
Proof. If the rule dependency relation  on  is acyclic, then by the first safety condition
in Definition 41 all positions in  are safe; but then, PG() is empty, and so  is A.
Theorem 43. If a set of rules  is A , then  is AR .
Proof. The claim clearly follows from the following property: if the rule dependency relation
 for  has just one strongly connected component and  is A, then  is AR. Thus, assume
that each rule r   occurs on a cycle of . We next construct a mapping  that assigns a
nonnegative integer to each position in , and then we show that  is an argument ranking
for . In the rest of this proof, we write p1
p2 if WA() (see Section 2.4) contains a path
(consisting of regular and/or special edges) from position p1 to position p2 .
Due to our assumption on , the first item in Definition 41 never applies. Furthermore,
let  be the function that maps a set S of positions into another set of positions as follows:
(S) = S  {P |i | PosB (x)  S 6=  for each r  , each atom P (~t) in the head of sk(r),
and each variable x occurring in the i-th component of ~t}
S
Let 0 (S) = S, k (S) = (k1 (S)) for each k > 0, and  (S) = k (S). From Definition 41 it is obvious that PosS () =  (PosFD ()).
We next define the mapping . In the rest of this proof, let Y be the set containing
each position p  PosFD () for which an existentially quantified variable y in  exists such
that p  PosH (y). Furthermore, we use a convention that max  = 0.
 For each position p  PosFD (), we define (p) as follows:
 0
|{p  Y | p0
p and p0 6= p}| + 1
(p) =
|{p0  Y | p0
p}|

if p  Y
if p 
6 Y

 For each position p  PosS () \ PosFD (), we define (p) as follows:
h
i
(p) = min{k | p  k (PosFD ())} + [max{(q) | q  PosFD ()}]
 For each position p in  with p 6 PosS (), we define (p) as follows, where m(p) is
the maximum number of special edges occurring in PG() on a path ending at p:
(p) = m(p) + 1 + [max{(q) | q  PosS ()}]
Since  is A , PG() does not contain a cycle involving a special edge, so m(p) is
always a nonnegative integer and (p) is correctly defined.
775

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

We next show that  is an argument rankingthat is, that it satisfies all conditions
of Definition 36. To this end, consider an arbitrary rule r  , an arbitrary existentially
quantified variable y in r, an arbitrary universally quantified variable x in r, and an arbitrary
position P |i  PosH (y); we have the following cases.
 P |i  PosFD (). By Definition 33, position Q|j  PosB (x)  PosFD () exists that is
not -recursive with P |i . Thus, we have P |i 6 Q|j ; furthermore, Q|j
P |i by the
definition of WA(). Together, the latter two properties imply the following:
{p0  Y | p0

Q|j and p0 6= Q|j }  {p0  Y | p0

P |i and p0 6= P |i }

If Q|j  Y , this inclusion is strict since Q|j is contained in the set on the righthand side, but not in the set on the left-hand side; thus, (Q|j ) < (P |i ) holds, as
required. If Q|j 6 Y , then (Q|j ) < (P |i ) holds since the definition of  ensures that
(P |i )  (Q|j ) is at least 1.
 P |i  PosS () \ PosFD (). Let k be the smallest number with P |i  k (PosFD ()).
By the definition of , there exists a position Q|j  PosB (x)  k1 (PosFD ()), so
k  1  (Q|j ) by the definition of . Thus, (P |i ) > (Q|j ) holds, as required.
 P |i 6 PosS (). The first possibility is that some position Q|j  PosB (x)  PosS ()
exists; but then, by the definition of , we have (Q|j ) < (P |i ), as required. The
second possibility is that there exists some affected position Q|j  PosB (x); but then,
Q|j has at least one less incoming special edge in PG() than P |i ; thus, we also have
(Q|j ) < (P |i ), as required.
To complete the proof, we must also consider an arbitrary position P |i  PosH (x); however,
the cases are analogous as above, so we omit them for the sake of brevity.
To place A precisely in the landscape of acyclicity notions, we present three examples
that disprove relevant containment relationships. Greco et al. (2012) stated that AR is
strictly contained in A, but we were unable to find a formal proof of that statement;
in fact, Example 44 shows that this is not the case, and that actually A ( AR holds.
Moreover, Example 45 shows that A 6 MSA. Finally, Example 46 shows that WA 6 A.
Example 44. Let  be the set consisting of the following rules:
A(x)  y.R(x, y)

r1 =

R(x1 , x2 )  S(x1 , x2 )

r2 =
r3 =

S(z, x)  B(x)  A(x)

(83)
(84)
(85)

r4 =

R(z, x)  T (x, x)

(86)

r5 =

T (x, z)  R(x, x)

(87)

r6 =

T (z1 , x)  R(z2 , x)  y.T (x, y)

(88)

One can readily verify that the following mapping of positions to nonnegative integers is an
argument ranking for :
 = {A|1 7 0, B|1 7 0, R|1 7 1, R|2 7 1, S|1 7 1, S|2 7 1, T |1 7 1, T |2 7 2}
776

fiAcyclicity Notions for Existential Rules

We next argue that  is not A. First, the rule dependency relation in  holds (at least)
between the pairs of rules shown below. Thus, each rule in  occurs in  on a cycle, and
so  is the only strongly connected component of .
r1  r2

r2  r3

r3  r1

r1  r4

r4  r5

r5  r2

r5  r6

r6  r5

Second, the WA dependency graph for  contains a special edge from A|1 to R|2 due to
rule r1 , a regular edge from R|2 to S|2 due to rule r2 , and a regular edge from S|2 to A|1 due
to rule r3 ; consequently, R|2 is -recursive with A|1 ; but then, rule r1 does not satisfy
the conditions in Definition 33, and so R|2 6 PosFD (). Furthermore, due to rule r4 , we
have T |1 6 PosFD () and T |2 6 PosFD () as well. Finally, R|1 6 PosFD () due to rule r5 .
Consequently, the set of finite domain positions is given by PosFD () = {A|1 , B|1 , S|1 }.
Third, we argue that PosS () = PosFD (). In particular, there is no need to extend
PosS () with R|2 : position R|2 occurs in the head of rule r5 , but since T |2 is not a finite
domain position and r5 occurs on a cycle of , neither condition from Definition 41 holds.
Analogously, positions T |1 and T |2 do not need to be added to PosS () either.
Fourth, since positions R|2 , T |1 , and T |2 are all affected, the propagation graph PG()
contains a special edge from T |2 to itself due to rule r6 . Consequently,  is not A.
Finally, since  is the only strongly connected component of , this example also shows
that AR 6 A and AR 6 A ; but then, by Theorem 43, we have A ( AR .

Example 45. Let  be the set of rules from Example 29. As explained in the example, 
is aGRD, but not MSA and thus also not JA, AR, or FD. By Proposition 42,  is A,
which implies A 6 MSA, and thus A 6 SWA, A 6 JA, A 6 AR, and A 6 FD.

Example 46. Let  be the set consisting of the following rules:
r1 =

R(x1 , x1 )  y1 y2 .[A(x1 )  S(y1 , x1 )  S(x1 , y2 )]

(89)

r2 =

A(x2 )  B(x2 )

(90)

r3 =

B(x3 )  R(x3 , x3 )

(91)

r4 =

S(x4 , x4 )  y3 y4 .[C(x4 )  R(y3 , x4 )  R(x4 , y4 )]

(92)

r5 =

C(x5 )  D(x5 )

(93)

r6 =

D(x6 )  S(x6 , x6 )

(94)

Note that r1 6 r4 and r4 6 r1 , so the rule dependency relation  in  has two strongly
connected components: the first one consists of r1 , r2 , and r3 , and the second one consists
of r4 , r5 , and r6 . Moreover, each strongly connected component is WA, so  is WA .
In contrast, each position in  is -recursive with itself, so PosFD () = . Moreover,
each position in  occurs in the head of a rule that (i) appears in  in a cycle and (ii) does
not satisfy the second safety condition in Definition 41; hence, PosS () = , and all positions are affected. But then, PG() = WA(), and so  is not A.

It may seem counterintuitive that AR 6 A, but A ( AR . Intuitively, the notion
of safe positions from Definition 41 uses the rule dependency relation, which allows us to
construct an example that is in A but not in AR. In A , this extra condition is always
applied to rules that occur in  on a cycle; thus, the notion of safe positions collapses to a
notion weaker than AR, which in turn allows A to be subsumed by AR .
777

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

MFA = MFA
MSA

MSA

SWA

SWA

JA

JA

AR

AR
A

Adn-WA

A

FD

FD

WA

WA
aGRD

Figure 1: The Landscape of Acyclicity Notions
4.4 The Landscape of Acyclicity Notions
To obtain a complete picture of the relative expressiveness of the acyclicity notions considered in this paper, we make the following observations.
 The rule set from Example 15 is MFA but not MSA, and one can readily verify that
r1  r2  r3  r4  r1 ; but then, MSA ( MFA = MFA by Proposition 27.
 The rule set from Example 20 is SWA but not JA, and one can readily verify that
r1  r2  r3  r1 ; but then, JA ( SWA and SWA 6 JA by Proposition 27.
 The rule set from Example 1 is MSA but not SWA, and one can readily verify that
r1  r3  r4  r5  r2  r1 ; but then, SWA ( MSA and MSA 6 SWA by Proposition 27.
 The rule set from Example 22 is aGRD: we have r1 6 r1 , r1 6 r2 , r2 6 r1 , and r2 6 r2 .
Thus, aGRD 6 Adn-WA.
 The rule set from Example 22 is FD: we can assume all positions in the rule set to
be finite domain without violating conditions of Definition 33. Thus, FD 6 Adn-WA.
The landscape of the acyclicity notions considered in this paper is shown in Figure 1.
All inclusions between notions shown in the figure are strict: if a notion X is reachable
from a notion Y via one or more (directed) arcs, then X is strictly more general than Y .
Furthermore, all inclusions are also complete: if a notion X is not reachable from a notion
Y via one or more (directed) arcs, then X does not contain Y .
778

fiAcyclicity Notions for Existential Rules

5. Handling Equality via Singularisation
Most acyclicity notions presented so far provide no special provision for the equality predicate. If a set of rules  contains the equality predicate, one can always axiomatise equality
explicitly and then check acyclicity. More precisely, the acyclicity of    (under any
notion introduced thus far) guarantees termination of the skolem chase of . Furthermore,
note that MFA and MSA are defined as entailment checks in first-order logic with equality,
which effectively incorporates the rules of equality into these checks even if rules (1)(4) are
not explicitly given; however, the effect of such a definition is the same.
While handling equality explicitly may be simple, such an approach does not ensure
termination of the skolem chase in many practically relevant cases. In particular, the
following example shows that the equalities between terms tend to proliferate during skolem
chase, which can lead to non-termination.
Example 47. Consider the set of rules  containing rules (95)(96).
A(x)  B(x)  y.[R(x, y)  B(y)]
R(z, x1 )  R(z, x2 )  x1  x2

(95)
(96)

The skolem chase of I and  derives the following infinite set of facts:
R(, f ())
B(f ())   f () A(f ())
R(f (), f (f ))) B(f (f ()))
...
Thus,  is not universally MFA by Proposition 5, and by Theorem 14 it is not universally
MSA either.

It is worth noticing that in the presence of equality WA is no longer subsumed by MFA
and hence both notions become incomparable. As explained in Section 2.4, WA can be
applied to rules containing the equality predicate (and without an explicit axiomatisation
of equality). Under such a treatment, the rules in Example 47 are WA. This, however, does
not contradict the results from Section 4: WA does not require an explicit axiomatisation of
equality because it ensures termination of nonoblivious chasean optimised chase variant
that expands existential quantifiers only if necessary and that handles equality by replacing
equal terms with canonical representatives. In contrast, the results in Section 4 ensure
termination of the skolem chase; since this chase variant uses an explicit axiomatisation of
equality, all of our results hold only for equality-free rules (or, equivalently, for the rules
containing an explicit axiomatisation of equality). The rules in Example 47 are not WA if
equality is axiomatised explicitly, which explains the apparent mismatch with Section 4.
In order to use the skolem chase with rule sets such as the ones in Example 47, Marnette
(2009) proposed the singularisation technique. Roughly speaking, singularisation replaces
the equality predicate  with a fresh binary predicate Eq to clarify that the two are to be
treated differently; furthermore, it axiomatises Eq as reflexive, symmetric, and transitive,
but it does not introduce replacement rules analogous to (4); finally, it modifies the rules in
 to take the lack of the replacement rules into account. The chase of the transformed rule
set is not a model of , but it can be used to answer queries over  in a particular welldefined way. The modification of , however, is nondeterministic: there are many ways to
modify  and, while some may ensure termination of the skolem chase, not all are required
to do so. We next recapitulate the definition of singularisation by Marnette (2009).
779

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Definition 48. A marking Mr of a rule r of the form (5) is a mapping that assigns to each
variable w  ~x  ~z a single occurrence of w in ; the marked occurrence of w in a rule is
written w . All other occurrences of w are unmarked, and all occurrences of constants are
unmarked as well. For  a set of rules, a marking M of  contains exactly one marking Mr
for each r  . Let Eq be a fresh binary predicate not occurring in . The singularisation
of  under M is the set Sing(, M ) that contain rules
 Eq(x, x)

(97)

Eq(x1 , x2 )  Eq(x2 , x1 )

(98)

Eq(x1 , x2 )  Eq(x2 , x3 )  Eq(x1 , x3 )

(99)

and, for each rule r  , the rule obtained from r by replacing each atom s  t with atom
Eq(s, t), and by replacing each unmarked occurrence of a term t in a body atom with a fresh
variable z 0 and then adding atom Eq(t, z 0 ) to the rule body.
Note that Sing(, M ) is unique up to the renaming of the fresh variables. Furthermore,
note that rule (97) can be transformed into a safe rule as explained in Section 2.2. Finally,
note that Sing(, M ) is equality-free (since  and Eq are different predicates); therefore, no
specific treatment of equality is needed when computing its chase or checking its acyclicity.
Example 49. Singularisation of the marked rule (100) produces rule (101).
A(x )  B(x)  R(x, z  )  C(x)

(100)

A(x)  B(x1 )  R(x2 , z)  Eq(x, x1 )  Eq(x, x2 )  C(x)

(101)

Note that singularisation should be applied globally to all rules, including the ones that do
not contain the equality predicate.

The properties of singularisation can be summarised as follows. Let  be a set of rules,
let I be an instance, and let M be a marking for . Furthermore, let 0 = Sing(, M ),
and let I 0 = I0 be the chase of I and 0 . Finally, note that predicate Eq is interpreted
in I 0 as an equivalence relation, so let  be a function that maps each term t occurring in
I 0 to an arbitrarily chosen representative from the equivalence class of t. The first-order
interpretation (I 0 ) is defined as follows, where rng() is the range of the mapping , the
0
0
set 4(I ) is the universe of (I 0 ), and (P )(I ) is the interpretation of a predicate P :
0

4(I ) = rng()
0
(P )(I ) = {h(t1 ), . . . , (tn )i | P (t1 , . . . , tn )  I} for each P different from Eq
0
0
(Eq)(I ) = {hx, xi | x  4(I ) }
Note that (I 0 ) interprets  as true equalitythat is, each term t is interpreted in (I 0 )
as a representative of the equivalence class that contains t; hence, (I 0 ) is not a Herbrand
interpretation. Marnette (2010) showed that, for an arbitrary , interpretation (I 0 ) is
a universal model of  and Ithat is, (I 0 ) can be homomorphically embedded into an
arbitrary model of  and I. Thus, (I) can be used for query answering: for a Boolean
conjunctive query Q, we have I   |= Q if and only if (I 0 ) |= Q.

780

fiAcyclicity Notions for Existential Rules

This result can be reformulated as follows. Let , I, M , and I 0 be as specified above,
and let us assume that Q is of the form Q = ~y .(~y ). Furthermore, let r be the following
rule, and let M 0 be an arbitrary marking of r:
r=

(~y )  H

(102)

Then, the above characterisation of singularisation implies that
I   |= Q if and only if
I  Sing(  {r}, M  M 0 ) |= H if and only if
I 0  Sing({r}, M 0 ) |= H.
Hence, we can answer Q w.r.t.  and I by evaluating Sing({r}, M 0 ) in the chase of I and
Sing(, M ). It is straightforward to generalise this approach to non-Boolean queries.
The absence of replacement rules (4) often allows the skolem chase to terminate on
Sing(, M ), but this may depend on the selected marking.
Example 50. Rule (95) from Example 47 admits the following two markings:
A(x )  B(x)  y.[R(x, y)  B(y)]


A(x)  B(x )  y.[R(x, y)  B(y)]

(103)
(104)

The skolem chase does not universally terminate for the singularisation obtained from (104)
and (96). In contrast, the singularisation obtained from (103) and (96) is JA.

Definition 51. For X  {MFA, MSA, JA}, acyclicity notion X  (resp. X  ) contains each
finite set of rules  such that Sing(, M )  X for some (resp. each) marking M of .
Clearly, X   X  for each X  {MFA, MSA, JA}, and Example 50 shows this inclusion
to be proper. We next show that JA actually coincides with WA.
Theorem 52. For  an arbitrary finite set of rules,  is JA if and only if  is WA.
Proof. (JA  WA) We prove the contrapositive, so let  be an arbitrary set of rules that is
not WA; w.l.o.g. we assume that each variable in  occurs in at most one rule. We consider
each edge from p to q in the WA dependency graph WA() to be a triple e = hp, q, ti, where
t =  if the edge is regular and t =  if the edge is special. By the definition of WA, for
each such e, a rule r   and universally quantified variable x occurring in the head and
the body of r exist such that p  PosB (x), so let xe be one such arbitrarily chosen but fixed
variable; furthermore, if edge e is special, then an existentially quantified variable y exists
such that q  PosH (y), so let ye be one such arbitrarily chosen but fixed variable.
A cycle in WA() is a sequence of edges e1 , . . . , en of the form ei = hpi , qi , ti i such that
qi = pi+1 for each 1  i < n and and qn = p1 . Such a cycle is dangerous if an edge ek exists
that is special; and such a cycle is simple if xei 6= xej for all 1  i < j  n.3
Now let 0 = e1 , . . . , en be an arbitrary dangerous cycle in WA(). If 0 is not simple,
we show how to transform 0 to a shorter dangerous cycle. Towards this goal, assume that
0 contains edges ei = hpi , qi , ti i and ej = hpj , qj , tj i such that 1  i < j  n and xei = xej ;
hence, some rule r   contains body atoms in which xei occurs at positions pi and pj .
Furthermore, let ek be an arbitrarily chosen, but fixed special edge in 0 ; such ek exists
since 0 is dangerous. We have the following possibilities.
3. Note that a cycle of length one is always simple.

781

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

 If i  k < j, let 00 = e, ei+1 , . . . , ej1 where e = hpj , qi , ti i. If ei is regular, then xei
occurs in a head atom of r at position qi ; furthermore, if ei is special, then some head
atom of r contains an existentially quantified variable at position qi . Either way, e is
an edge of WA(), so 00 is a cycle in WA(). Furthermore, e is special if k = i, and
00 contains ek otherwise; hence, 00 is dangerous.
 Otherwise, let 00 = e1 , . . . , ei1 , e, ej+1 , . . . , en where e = hpi , qj , tj i. Edges e and ej
are of the same type, so e is an edge of WA() and 00 is a cycle in WA(). Furthermore, e is special if k = j, and 00 contains ek otherwise; hence, 00 is dangerous.
In both cases, 00 contains at least one edge less than 0 . Thus, we can iteratively transform
an arbitrary dangerous cycle in WA() to a simple dangerous cycle .
Now let M be a marking for  that marks each variable w occurring in the body of a
rule r   as follows.
 If an edge e = hp, q, ti in  exists such that w = xe , then M marks an occurrence of
w in r at position p (if there are multiple such occurrences, one is chosen arbitrarily).
Since  is simple, edge e is unique, and so M is correctly defined.
 Otherwise, M marks an arbitrarily chosen occurrence of w in r.
Let 0 = Sing(, M ), and let JA(0 ) be the JA dependency graph for 0 . To show that
JA(0 ) contains a cycle, we first prove the following property (?).
For each subpath e1 , . . . , ek of  where edge e1 is special and each edge ei with
1 < i  k is regular, we have {qi , Eq|1 }  Move(ye1 ) for each 1  i  k.4
Since 0 contains rule (97), we clearly have Eq|1  Move(ye1 ). We next prove (?) by induction on k. For the base case k = 1, we have q1  Move(ye1 ) by the definition of JA. For
the induction step, assume that the claim holds for all subpaths of length k, and consider
a subpath e1 , . . . , ek , ek+1 . By the induction assumption and the fact that qk = pk+1 , we
have pk+1  Move(ye1 ). Furthermore, variable xek+1 occurs in the body and the head atom
of some rule r  0 at positions pk+1 and qk+1 , respectively. Finally, by the definition of M
and the properties of singularisation, we have that PosB (xek+1 ) contains pk+1 and possibly
Eq|1 . But then, by the definition of JA, we have qk+1  Move(ye1 ), as required.
To complete the proof, consider now an arbitrary subpath e1 , . . . , e` of  where edges
e1 and e` are special and each edge ei with 1 < i < k is regular. By (?) and the fact that
q`1 = p` , we have {p` , Eq|1 }  Move(ye1 ). Furthermore, as in the previous paragraph,
PosB (xe` ) contains p` and possibly Eq|1 ; but then, JA(0 ) contains an edge from ye1 to ye` .
Since  is a cycle, JA(0 ) clearly contains a cycle, so 0 is not JA, as required.
(JA  WA) Assume that  6 JA , so there exists a marking M for  such that
= Sing(, M ) is not JA. We assume that  does not contain an existentially quantified variable that occurs in an equality atom; this is w.l.o.g. as we can always replace each
equality atom y  t with an atom R(x, t) and add a rule R(x1 , x2 )  x1  x2 for R a fresh
binary predicate, and such a transformation clearly does not affect the membership of the

0

4. The notion of a subpath is defined in the obvious way; however, please note that, although  is defined
as a sequence of edges, subpaths of  can wrap around this sequence as  is a cycle.

782

fiAcyclicity Notions for Existential Rules

rule set in JA and WA. Now consider an arbitrary existentially quantified variable y, and
arbitrary positions p  PosH (y) and q  Move(y) that do not involve Eq (both sets are w.r.t.
0 ); by induction on the construction of Move(v), one can prove that WA() then contains a
sequence of regular edges from p to q. The proof is straightforward, and we omit the details
for the sake of brevity. Similarly, consider an arbitrary edge from y1 to y2 in JA(0 ), and
arbitrary positions p  PosH (y1 ) and q  PosH (y2 ) that do not involve Eq; by the definition
of JA, a variable x occurring in the rule of y2 and a position s not involving Eq exist such
that s  Move(y1 ) and s  PosB (x). But then WA() contains a path consisting of regular
edges from p to s, as well as a special edge from s to q. Since JA(0 ) is cyclic, WA()
clearly contains a cycle involving a special edge.
Checking all possible markings may be infeasible: the number of candidates is exponential in the total number of variables that occur more than once in a rule body. Theorem 52
shows that JA can be decided using WA. For the other cases, the following simple observation shows how to reduce the number of markings.
Definition 53. A variable x is relevant for a rule r   if x occurs more than once in the
body of r, and the head of r contains an atom P (~t) such that x  ~t and P is not .
Proposition 54. Let M and M 0 be markings for  such that, for each rule r  , the
markings for r in M and M 0 coincide on each relevant variable in r. Then, for each
instance I, the result of the skolem chase for I and Sing(, M ) coincides with the result of
the skolem chase for I and Sing(, M 0 ); furthermore, Sing(, M ) is JA/MSA/MFA if and
only if Sing(, M 0 ) is JA/MSA/MFA.
Proof. Consider an arbitrary rule r  . If a variable x occurs only in the body of r, then
marking various occurrences of x in r clearly produces rules equivalent up to the renaming of
variables. Furthermore, assume that a variable x occurs in the head of r only in an equality
atom of the form x  t, and that the markings of x differ. Then, the rules obtained from r by
singularisation will all have the same body (up to the renaming of variables); furthermore,
the bodies contain atoms Eq(xi , x), and the rule heads are of the form Eq(x, t). Since
Sing(, M ) and Sing(, M 0 ) contain rules (97)(99), the skolem chase for I and Sing(, M )
clearly derives the same ground atoms as the skolem chase for I and Sing(, M 0 ).
Despite this optimisation, the number of markings to check can still be exponential in the
size of , so we next describe a useful approximation. Let M be a maximal set of markings
for  such that, for all M1 , M2  M, each rule r  , and each variable x that is not relevant
in r, the markings of x in r under M1 and M2 coincide. Intuitively, such M contains all
possible markings of the relevant variables, but the markings of all other variables coincide.
By
S Proposition 54 it is clear
S that, given two such sets M1 and M2 , the skolem chase of
I; thus, let
M M1 Sing(, M ) and
M M2 Sing(, M ) coincides for an arbitrary instance
S
M be one arbitrarily chosen such set of markings. Also, let Sing  () = M M Sing(, M ),
let MFA be the class containing each rule set  such that Sing  ()  MFA, and let MSA
and JA be defined analogously. As the following proposition shows, Sing  () provides a
lower bound on acyclicity that can be obtained via singularisation.

783

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Proposition 55. For each X  {MFA, MSA, JA}, we have that X   X  . Furthermore,
the size of Sing  () is exponential in the maximal number of relevant variables in a rule in
, and it is linear in the number of rules in .
Proof. The first claim follows from the fact that all considered notions of acyclicity are
monotone in the sense that every subset of an acyclic rule set is also acyclic. The second
claim follows from the fact that, if a rule r exists that contains k relevant variables and
each variable occurs m times in r, then M contains mk different markings for r.
This result is interesting when dealing with rules that are obtained from DLs, where
each rule has at most one relevant variable: on such rule sets, the size of Sing  () is linear
in the size of . For the general case, the complexity of acyclicity checking does not increase
despite the exponential increase in the number of rules.
Theorem 56. Deciding whether  is in MFA (MFA , MFA ) is 2ExpTime-complete.
Deciding whether  is in MSA (MSA , MSA ) is ExpTime-complete.
Proof. If  contains no equality, it is easy to see that  is in MFA (MFA , MFA ) if and
only if it is in MFA. The same can be observed for MSA. Thus, hardness follows from
Theorems 8 and 13.
For membership, we first consider the cases of MFA , MFA , MSA , and MSA . Each
of these properties can be decided by considering all of the at most exponentially many
markings. Since Sing(, M ) is linear in the size of , the property can be checked for
each marking for MFA in 2ExpTime (cf. Theorem 8) and for MSA in ExpTime (cf. Theorem 13). This yields the required bound since an exponential factor is not significant for
the considered complexity classes.
For MFA and MSA , membership follows by observing that the membership of MFA
and MSA in 2ExpTime and ExpTime, respectively, is obtained from the double/single
exponential bound on the number of ground facts that potentially need to be derived in
order to decide the required property. While Sing  () is exponentially larger than , the
maximal number of relevant ground facts is still the same since no new predicates or constant
symbols are introduced. The increased number of rules leads to an exponential increase of
the time to check applicability of all rules in each of the doubly/singly exponentially many
steps, but this exponential factor does not affect membership of the decision problem in
2ExpTime/ExpTime.
We finish this section by examining the interaction between rule normalisation and
singularisation. Note that normalisation reduces the number of variables in a rule, which
at least at first sight suggests that normalisation could prevent one from finding a marking
that ensures acyclicity of the singularised rules. We next show that this cannot happen if
normalisation is used without structure sharing: if the original set of rules is MFA w.r.t.
some set of markings, then the transformed set of rules is MFA w.r.t. a set of markings as
well. Furthermore, we show that this does not hold if normalisation is used with structure
sharing; hence, normalisation should be applied with care when used with singularisation.
Theorem 57. Let  be a set of existential rules, let 0 be obtained from  by applying a
single normalisation step without structure sharing, and let I be an instance. Then each
784

fiAcyclicity Notions for Existential Rules

marking M of  for which Sing(, M ) is MFA w.r.t. I can be extended to a marking M 0 of
0 such that Sing(0 , M 0 ) is MFA w.r.t. I.
Proof. Let M be a marking of  such that Sing(, M ) is MFA, let r   be the rule of the
form (8) to which the normalisation step is applied, and let 0 be the set of rules obtained
from  after the application of a normalisation step to r. We next prove that the claim
holds for both a head and a body normalisation step.
(Head Normalisation) Assume that the set of rules 0 is obtained by replacing a rule
r   with rules r1 and r2 of the following forms, where ~x = ~x3  ~x4 :
r=
r1 =
r2 =

(~x, ~z)  ~y1 , ~y2 , ~y3 .[1 (~x3 , ~y1 , ~y2 )  2 (~x4 , ~y1 , ~y3 )]
(~x, ~z)  ~y1 , ~y3 .[Q(~x3 , ~y1 )  2 (~x4 , ~y1 , ~y3 )]
Q(~x3 , ~y1 )  ~y2 .1 (~x3 , ~y1 , ~y2 )

Let M 0 be a marking that coincides with M on all rules different from r, that marks r1
in the same way as M marks r, and that marks r2 in the only possible way (note that
the body of this rule does not contain repeated occurrences of variables); furthermore, let
 = Sing(, M ) and  = Sing(0 , M 0 ). We assume that rule r is skolemised by replacing
each variable y  ~y1 with g1y (~x), each variable y  ~y2 with g2y (~x), and each variable y  ~y3
with g3y (~x); rule r1 is skolemised as r; and rule r2 is skolemised by replacing each variable
y  ~y2 with hy (~x3 , ~y1 ). Thus, the skolemised and singularised rules have the following form;
formula 0 is a singularisation of , and all freshly introduced variables are contained in ~z1 :
0 (~x, ~z1 )  1 (~x3 , ~g1 (~x), ~g2 (~x))  2 (~x4 , ~g1 (~x), ~g3 (~x))
0 (~x, ~z1 )  Q(~x3 , ~g1 (~x))  2 (~x4 , ~g1 (~x), ~g3 (~x))
Q(~x3 , ~y1 )  1 (~x3 , ~y1 , ~h(~x3 , ~y1 ))
Finally, we inductively define a partial mapping  from terms to terms as follows:
 (c) = c for each constant c,
 (f (~t)) = f ((~t)) for each function symbol f not of the form hy or g1y and all terms ~t
such that (~t) is defined, and
 (hy (~s, ~g1 (~s, ~t))) = g2y ((~s), (~t)) for each function symbols of the form hy , the corresponding symbol g2y , and all terms ~s and ~t such that (~s) and (~t) are defined.
 where A is a predicate
We next show the following property (?): for each A(~t)  I
occurring in  (i.e., A was not introduced by the normalisation step), (t) is defined and
0 , I 1 , . . . for I and . The
A((~t))  I . The proof is by induction on the chase sequence I

base case holds trivially. Furthermore, since  and  coincide on all rules apart from r,
r1 , and r2 , the proof of the claim is trivial for each conclusion of a rule different from r1
i+1
i by a
or r2 . For the remaining cases, we can assume w.l.o.g. that I
is obtained from I
single application of r1 of substitution  and an application of r2 to the result; thus, the
rules together derive the following facts:

Q(~x3 , ~g1 (~x))
~
1 (~x3 , ~g1 (~x), h(~x3 , ~g1 (~x)))
2 (~x4 , ~g1 (~x), ~g3 (~x))
785

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

By the induction assumption, 0 ((~x), (~z))  I , and so I contains the following
facts:
1 ((~x3 ), ~g1 ((~x)), ~g2 ((~x)))
2 ((~x4 ), ~g1 ((~x)), ~g3 ((~x)))
Clearly, each term hy (~x3 , ~g1 (~x)) is of the form hy (~s, ~g1 (~s, ~t)), so the mapping  is defined
on the term. Furthermore, by the definition of , it is clear that property (?) holds.
 always in (sub)terms
The proof of (?) also reveals that functions symbols hy occur in I
 . This
of the form hy (~s, ~g1 (~s, ~t)), and that (u) is defined for each term u occurring in I
observation and the following property of  clearly imply the claim of this theorem: if u is
a cyclic term and (u) is defined, then (u) is cyclic as well. To prove the latter, it suffices
to consider the following two cases.
 Assume that u is cyclic due to the repetition of a function symbol f not of the form
hy . Thus, u contains a subterm of the form f (~s), and some si  ~s contains a subterm
of the form f (~t). By the definition of , then (u) contains a term of the form f ((~s)),
and some s0i  (~s) contains a subterm of the form f ((~t)). Clearly, (u) is cyclic.
 Assume that u is cyclic due to the repetition of a function symbol of the form hy . By
the above observation, then u contains a subterm of the form hy (~s, ~g1 (~s, ~t)), and some
si  ~s  ~t contains a subterm of the form hy (~v , ~g1 (~v , w)).
~
By the definition of , then
y
~
(u) contains a subterm of the form g2 ((~s), (t)), and some s0i  (~s)  (~t) contains
~
Clearly, u is cyclic.
a subterm of the form g2y ((~v ), (w)).
(Body Normalisation) Assume that the set of rules 0 is obtained by replacing a rule
r   with rules r1 and r2 of the following forms, where ~x = ~x1  ~x2  ~x3 , and ~x1 , ~x2 , ~x3 ,
~z1 , ~z2 , and ~z3 are all pairwise disjoint:
r=
r1 =
r2 =

1 (~x1 , ~x2 , ~z1 , ~z2 )  2 (~x1 , ~x3 , ~z1 , ~z3 )  ~y .(~x, ~y )
1 (~x1 , ~x2 , ~z1 , ~z2 )  Q(~x1 , ~x2 , ~z1 )
Q(~x1 , ~x2 , ~z1 )  2 (~x1 , ~x3 , ~z1 , ~z3 )  ~y .(~x, ~y )

For each marked variable v, let ~uv be the variables used to replace v in singularisation. Then,
the singularised rule r can be represented as follows, where for clarity we do not show the
free variables of various formulae, 01 and 02 do not contain atoms with predicate Eq, and
1 and 2 are the conjunctions of atoms with predicate Eq obtained by renaming unmarked
occurrences of the variables in 1 (~x1 , ~x2 , ~z1 , ~z2 ) and 2 (~x1 , ~x3 , ~z1 , ~z3 ), respectively:
01  02  1  2  ~y .(~x, ~y )
Now let M 0 be a marking that coincides with M on all rules different from r, and that, for
each marked occurrence of a variable w  ~x1  ~x2  ~z1 in r, marks r1 and r2 as follows.
 If the marked occurrence of w appears in 1 (~x1 , ~x2 , ~z1 , ~z2 ), then the corresponding
occurrence of w is marked in r1 ; in addition, if w  ~x1  ~x2  ~z1 , then the occurrence
of w in atom Q(~x1 , ~x2 , ~z1 ) is marked in r2 .

786

fiAcyclicity Notions for Existential Rules

 If the marked occurrence of w appears in 2 (~x1 , ~x3 , ~z1 , ~z3 ), then the corresponding
occurrence of w is marked in r2 ; in addition, if w  ~x1  ~x2  ~z1 , then an arbitrary
occurrence of w is marked in r1 .
Since there is no structure sharing,  does not contain r1 , so the above definition is wellformed. The singularisation of r1 and r2 under M 0 can be represented as follows:
001  001  Q(~x1 , ~x2 , ~z1 )
Q(~x01 , ~x2 , ~z10 )  02  02  ~y .(~x, ~y )
By the definition of M 0 , it should be clear that 001  001 is isomorphic to a subset of 01  01 .

Based on this observation, it is now routine to prove that, if A(~t)  ISing(,M
) and A is

~
different from the newly introduced predicate Q, then A(t)  ISing(0 ,M 0 ) , which clearly
implies our claim.
In contrast to Theorem 57, the following example shows that normalisation with structure sharing can prevent one from finding a marking that makes the normalised rules acyclic.
This example shows that normalisation must be used with care in applications that use singularisation to deal with equality.
Example 58. Let  be the following set of rules marked by a marking M shown below.
A(x)  T (x , z)  B(z  )  y.[R(x, y)  A(y)]




A(x )  T (x, z)  C(z )  y1 y2 .[S(x, y1 )  T (y1 , y2 )]


R(z , x1 )  R(z, x2 )
S(z  , x1 )  S(z, x2 )
T (z  , x1 )  T (z, x2 )

(105)
(106)

 x1  x2

(107)

 x1  x2

(108)

 x1  x2

(109)

One can show that Sing(, M ) is MFA w.r.t. the instance I given below.
I = {A(a), R(a, a), T (a, b), B(b), A(a0 ), S(a0 , a0 ), T (a0 , b0 ), C(b0 )}
Furthermore, let M1 be a marking identical to M but which marks A(x ) in rule (105), and
let M2 be a marking identical to M but which marks T (x , z) in rule (106). One can show
that neither Sing(, M1 ) nor Sing(, M2 ) is MFA w.r.t. I.
Now let 0 be obtained from  by applying normalisation with structure sharing to rules
(105) and (106); thus, rules (105) and (106) are replaced with the following rules:
Q(x, z)  B(z)  y.[R(x, y)  A(y)]

(110)

Q(x, z)  C(z)  y1 y2 .[S(x, y1 )  T (y1 , y2 )]

(111)

A(x)  T (x, z)  Q(x, z)

(112)

Note that conjunction A(x)  T (x, z) occurs in 0 only in rule (112); therefore, variable x
in this conjunction can be marked in only one way. This, however, has the same effect as
choosing M1 or M2 for : no possible marking M 0 will make Sing(0 , M 0 ) MFA w.r.t. I.
Intuitively, normalisation with structure sharing reduces the space of available markings,
due to which it may be impossible to find a marking that makes the rules acyclic.

787

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

6. Applying Acyclicity to Horn Description Logics
In this section we apply various acyclicity notions to reasoning problems in description
logic (DL) ontologies. Description logics are knowledge representation formalisms that underpin the Web Ontology Language (OWL). DL ontologies are constructed from atomic
concepts (i.e., unary predicates), atomic roles (i.e., binary predicates), and individuals (i.e.,
constants). Special atomic concepts > and  denote universal truth and falsehood, respectively. For each atomic role R, expression R is an inverse role; furthermore, a role is
an atomic or an inverse role. DLs provide a rich set of constructors for building concepts
(first-order formulae with one free variable) from atomic concepts and roles. A description
logic TBox is a set of axioms, which correspond to first-order sentences. In this paper
we consider only Horn description logics, in which TBoxes can be translated into existential rules. Furthermore, in this paper we will consider only normalised TBoxes, in which
concepts do not occur nested in other concepts. The latter assumption is without loss of
generality as each Horn description logic TBox can be normalised in linear time, and the
normalised ontology is a model-conservative extension of the original one.
In this paper we consider several logics all of which are fragments of the description logic
Horn-SROIF, which provides the formal underpinning for a prominent subset of OWL.
A normalised Horn-SROIF TBox T consists of axioms shown on the left-hand side of
Table 1; in the table, A, B, and C are atomic concepts (including possibly > and ), R, S,
and T are (not necessarily atomic) roles, and a is an individual. To guarantee decidability
of reasoning, T must satisfy certain global conditions (Kutz, Horrocks, & Sattler, 2006),
which we omit for the sake of brevity. Roughly speaking, only so-called simple roles are
allowed to occur in axioms of Type 2, and axioms of Type 6 must be regular according to
a particular condition that allows such axioms to be represented using a nondeterministic
finite automaton. We also consider the following fragments of Horn-SROIF.
 Horn-SRI TBoxes are not allowed to contain axioms of Type 2 or 7.
 Horn-SHIF TBoxes are not allowed to contain axioms of Type 7, and all axioms of
Type 6 satisfy R = S = T . Note that all Horn-SHIF TBoxes are regular.
 Horn-SHI TBoxes inherit the restrictions from Horn-SHIF and are further not allowed to contain axioms of type 2.
To simplify the presentation, we do not consider general at-least number restrictions
that is, concepts of the form  n R.A with n > 1. The translation of such concepts into rules
would require an explicit inequality predicate. As explained in Section 2.2, the inequality
predicate can be simulated using an ordinary predicate, and so the extension of our results
to general at-least number restrictions is straightforward.
In the rest of this paper we allow inverse roles to occur in atoms, so we take an atom
of the form R (t1 , t2 ) with R an atomic role as an abbreviation for R(t2 , t1 ). Then, each
Horn-SROIF axiom corresponds to an existential rule as shown in Table 1. As explained
in Section 2.2, we treat > and  as ordinary unary predicates where > is explicitly axiomatised. Thus, we can take a substitution  to be an answer to a CQ Q(~x) w.r.t. a T
and I if T  I |= Q(~x) or I  T |= y.(y); the latter condition takes into account that an
unsatisfiable theory entails all possible formulae. Due to this close correspondence between
788

fiAcyclicity Notions for Existential Rules

1.
A v R.B
2.
A v  1 R.B
3. A u B v C
4.
A v R.B
5.
RvS
6. R  S v T
7.
A v {a}

A(x)  y.[R(x, y)  B(y)]
A(z)  R(z, x1 )  B(x1 )  R(z, x2 )  B(x2 )  x1  x2
A(x)  B(x)  C(x)
A(z)  R(z, x)  B(x)
R(x1 , x2 )  S(x1 , x2 )
R(x1 , z)  S(z, x2 )  T (x1 , x2 )
A(x)  x  a

Table 1: Axioms of normalised Horn-SROIF ontologies and corresponding rules
description logic axioms and existential rules, in the rest of this paper we identify a TBox
T with the corresponding set of rules.
The complexity of answering Boolean conjunctive queries over general (i.e., not acyclic)
DL TBoxes is 2ExpTime- and ExpTime-complete for Horn-SROIF (Ortiz et al., 2011)
and Horn-SHIF (Eiter et al., 2008), respectively. In the rest of this section we investigate
the complexity of this problem on acyclic ontologies, as well as the complexity of acyclicity
checking. In particular, in Section 6.1 we consider the case when the TBox is expressed
in Horn-SROIF, for which we show that both BCQ answering and MFA checking are
ExpTime-complete. Then, in Section 6.2 we consider Horn-SHIF TBoxes, for which we
show that the complexity of these problems drops to PSpace.
6.1 Acyclic Horn-SROIF TBoxes
We start by showing that BCQ answering for WA Horn-SRI TBoxes is ExpTime-hard.
Intuitively, this is due to the axioms of Type 6, which can be used to axiomatise existence
of non-tree-like structures. Although regularity ensures that axioms of Type 6 can be
represented by a nondeterministic finite automaton, this automaton can be exponential; as
a consequence, axioms of Type 6 can axiomatise exponential non-tree-like structures, which
is the main source of complexity.
Lemma 59. Let T be a WA Horn-SRI TBox, let I be an instance, and let F be a fact.
Then, checking whether I  T |= F is ExpTime-hard.
Proof. Let M = (S, Q, , Q0 , Qa ) be a deterministic Turing machine, where S is a finite set
of symbols, Q is a finite set of states,  : Q  S  Q  S  {, } is a transition function,
Q0  Q is the initial state, and Qa the accepting state. Furthermore, assume that an integer
k
k exists such that M halts on each input of length n in time 2n . Given an arbitrary input
Si1 , . . . , Sin , we construct an MFA set of Horn-SRI rules T and an instance I such that
I  T |= Qa (a) if and only if M accepts the input. To simplify the presentation, we will use
a slightly more general rule syntax than what is allowed by Table 1; however, all such rules
can be brought into the required form by renaming parts of the rules with fresh predicates.
Let ` = nk ; since k is a constant, ` is polynomial in n. Our construction uses a unary
predicate for each symbol and state; for simplicity, we do not distinguish between the
predicate and the symbol/state. In addition, the construction also uses binary predicates
Li , Ri , Ti , Ui , Di , Hi , and Vi for 1  i  `, unary predicates Ai and Bi for 0  i  `, and
unary predicates O1 , . . . , On+1 , N1 , and N2 . Instance I contains only the fact A0 (a). We
789

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

next present the rules of T . The set T will contain only Horn rules without empty heads,
so it will be satisfiable in a minimal Herbrand model. For readability, we divide T into
groups of rules and prove for each group various facts about this minimal Herbrand model
of T  I, which is shown schematically in Figure 2. The construction of T proceeds along
the following lines.
 The first, the second, and the third group of rules construct the exponential grid shown
at the bottom of Figure 2, whose edges are labelled with H` and V` . Each sequence
of V` -edges will be used to encode the contents of the tape of the Turing machine at
some point in time; furthermore, precisely one vertex in each such sequence will be
labelled with a state, thus representing the position of the head. In contrast, H` -edges
will connect different points in time and will be used to encode the transitions of the
Turing machine.
 The fourth group labels the right-most V` -chain with Si1 , . . . , Sin , St , St , . . . , St , St ,
where St represents the empty tape symbol.
 The fifth and the sixth groups ensure that the symbols on the tape that are not
modified by a move of a Turing machine are propagated between time points.
 The seventh and the eighth group encode the transitions of the Turing machine.
 The ninth group propagates the acceptance condition to the top of the figure by
labelling the individual a with the accepting state Qa .
We next present the rules of T in detail.
The first group of rules in T contains rules (113)(115) for each 0 < i  `, and rule (116)
for each 1 < i  `.
Ai1 (x)  y.[Li (x, y)  Ai (y)]

(113)

Ai1 (x)  y.[Ri (x, y)  Ai (y)]

(114)

0

0

(115)

0

0

(116)

Ri (z, x)  Li (z, x )  Ti (x, x )
0

0

Li (z, x)  Ti1 (z, z )  Ri (z , x )  Ti (x, x )

On I, these rules axiomatise existence of a triangular structure in the top part of Figure 2
containing Ti links.
The second group of rules in T contains rule (117), rules (118)(120) for each 0 < i  `,
and rule (121) for each 1 < i  `.
A` (x)  B0 (x)

(117)

Bi1 (x)  y.[Ui (x, y)  Bi (y)]

(118)

Bi1 (x)  y.[Di (x, y)  Bi (y)]

(119)

0

0

(120)

0

0

(121)

Ui (z, x)  Di (z, x )  Vi (x, x )
0

0

Di (z, x)  Vi1 (z, z )  Ui (z , x )  Vi (x, x )

These rules axiomatise existence of triangular structures in the bottom part of Figure 2
containing Vi links.
790

fiAcyclicity Notions for Existential Rules

L1

R1
T1

L2

Legend:

R2

T2

T2

T2

Li

Ui

Ri

Di

Ti
Hi
T`

T`

H0

H0
D1

Vi

U1

H1

V 1 U2
D2

V2
V2
V2

H2
H`

H`

O1 , Q0
V`

H`

V`

H`

Figure 2: Grid Model of T
The third group of rules in T contains rule (122), and rules (123) and (124) for each
0 < i  `.
T` (x, x0 )  H0 (x, x0 )

(122)

0

0

0

0

(123)

0

0

0

0

(124)

Ui (z, x)  Hi1 (z, z )  Ui (z , x )  Hi (x, x )
Di (z, x)  Hi1 (z, z )  Di (z , x )  Hi (x, x )

These rules axiomatise existence of Hi links, which with Vi links form a grid of size 2i  2i
shown in Figure 2.
In the rest of this proof, for variables w0 and w` , we use R` (w0 , w` ) as an abbreviation for
R1 (w0 , w1 )  . . .  R` (w`1 , w` ), where each wi with 0 < i < m is a variable not occurring
outside the conjunction. Furthermore, we analogously use U ` (w0 , w` ) as an abbreviation
for U1 (w0 , w1 )  . . .  U` (w`1 , w` ).
The fourth group of rules in T contains rule (125), rules (126) and (127) for each
1  j  n, and rules (128)(129), where St is the empty tape symbol. Remember that
Si1 , . . . , Sin encodes the input to M.
A0 (z)  R` (z, z 0 )  U ` (z 0 , x)  O1 (x)  Q0 (x)

(125)

Oj (z)  V` (z, x)  Oj+1 (x)

(126)

Oj (x)  Sij

(127)

On+1 (z)  V` (z, x)  On+1 (x)

(128)

On+1 (x)  St (x)

(129)

791

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Rule (125) labels the grid origin and sets the initial state as shown in Figure 2. Rules
(126) ensure that the n subsequent nodes are labelled with O2 , . . . , On+1 , and rule (128)
propagates On+1 to the rest of the V` -chain. Finally, rules (127) and (129) ensure that
nodes labelled with Oj are also labelled with Sij , and that nodes labeled with On+1 are
labeled with St . Thus, this group of rules in T ensures that the right-most V` -chain in the
grid contains the initial state of the tape of M.
The fifth group of rules in T contains rules (130)(131) for each state Qk  Q, and rules
(132)(133). These rules essentially ensure that all nodes before and after a node labelled
with some state Qk  Q are labeled with N1 and N2 , respectively, thus indicating that the
head is not above the node.
Qk (z)  V` (x, z)  N1 (x)

(130)

Qk (z)  V` (z, x)  N2 (x)

(131)

N1 (z)  V` (x, z)  N1 (x)

(132)

N2 (z)  V` (z, x)  N2 (x)

(133)

The sixth group of rules in T contains rules (134)(135) instantiated for each symbol
Sk  S; these rules ensure that the contents of the tape is copied between successive time
points for all points in the grid not containing the head.
N1 (z)  Sk (z)  H` (z, x)  Sk (x)

(134)

N2 (z)  Sk (z)  H` (z, x)  Sk (x)

(135)

The seventh group of rules in T contains rules (136)(137) instantiated for each symbol
Sk  S and each state Qk  Q such that (Qk , Sk ) = (Qk0 , Sk0 , ). These rules encode
moves of M where the head moves left.
Qk (z)  Sk (z)  H` (z, x)  Sk0 (x)
0

0

Qk (z)  Sk (z)  H` (z, z )  V` (x, z )  Qk0 (x)

(136)
(137)

The eighth group of rules in T contains rules (138)(139) instantiated for each symbol
Sk  S and each state Qk  Q such that (Qk , Sk ) = (Qk0 , Sk0 , ). These rules encode
moves of M where the head moves right.
Qk (z)  Sk (z)  H` (z, x)  Sk0 (x)
0

0

Qk (z)  Sk (z)  H` (z, z )  V` (z , x)  Qk0 (x)

(138)
(139)

The ninth group of rules in T contains rules (140)(143) for each 1  i  `; these rules
simply ensure that acceptance is propagated back to the root of the upper tree.
Qa (z)  Ui (x, z)  Qa (x)

(140)

Qa (z)  Di (x, z)  Qa (x)

(141)

Qa (z)  Li (x, z)  Qa (x)

(142)

Qa (z)  Ri (x, z)  Qa (x)

(143)

792

fiAcyclicity Notions for Existential Rules

The above discussion shows that labelling of the nodes in the grid shown in Figure 2
simulates the execution of M on input Si1 , . . . , Sin , where the contents of the tape at some
time instant is represented by a V` -chain, and H` -links connect tape cells at successive
time instants. Thus, I  T |= Qa (a) if and only if M accepts Si1 , . . . , Sin in time 2` . It is
straightforward to see that T is WA, so the claim of this theorem holds.
The proof of Lemma 59 can be adapted to obtain the lower bound for checking MFA of
Horn-SRI rules.
Lemma 60. Checking whether a Horn-SRI TBox is universally MFA is ExpTime-hard.
Proof. Let M be an arbitrary deterministic Turing machine and let Si1 , . . . , Sin be an input
k
string on which M terminates in time 2n . For such M and Si1 , . . . , Sin , let T be as in
the proof of Lemma 59. TBox T is WA, it contains only constant-free, equality-free, and
connected rules, and no predicate in T is of zero arity; hence, by Lemma 7, a Horn-SRI
TBox T 0 exists such that M accepts Si1 , . . . , Sin if and only if T 0 is not universally MFA.
Note that Lemmas 59 and 60 apply to Horn-SRI and thus do not rely on a particular
treatment of equality. We can deal with the equality predicate in Horn-SROIF TBoxes
using singularisation as described in Section 5, which leads us to the following result.
Theorem 61. Let T be a Horn-SROIF TBox, let M be a marking of T , let I be an
instance, and let Q be a BCQ. Then, checking whether Sing(T , M ) is MFA w.r.t. I is
ExpTime-complete. Furthermore, if Sing(T , M ) is MFA w.r.t. I, then checking whether
I  T |= Q holds is ExpTime-complete as well.
Proof. Note that all rules in Table 1 are -1 rules. Since all rules in Sing(T , M ) are -1
rules as well, Theorem 10 gives us an ExpTime upper bound for both of our problems. The
matching lower bounds follow from Lemmas 59 and 60 (note that every Horn-SRI TBox
is also a Horn-SROIF TBox) and the fact that their proofs do not use predicate .
In fact, Theorem 10 provides us with even stronger complexity bounds. In particular,
even if T does not satisfy all the required global conditions, and even if T is extended with
SWRL rules (Horrocks, Patel-Schneider, Bechhofer, & Tsarkov, 2005), the rules in T are
all still -1 rules. Thus, one can decide whether such T is MFA (universally or w.r.t. an
instance) in ExpTime, and if that is the case, one can answer BCQs in ExpTime as well.
Consequently, ontology-based applications can freely use the expressivity beyond what is
currently available in OWL without an increase in the complexity of reasoning, assuming
that the resulting TBox is acyclic.
We conclude this section by observing that MSA provides us with a tractable notion
for Horn-SROIF rules. Intuitively, all rules in MSA(T ) have a bounded number of variables and all predicates in MSA(T ) are of bounded arity, which eliminates all sources of
intractability in datalog reasoning. We prove the matching lower bound in Section 6.2 for
the more specific case of Horn-SHIF ontologies.
Theorem 62. Let T be Horn-SROIF TBox, let M be a marking, and let I be an instance.
Then, checking whether Sing(T , M ) is MSA w.r.t. I is in PTime.

793

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Proof. As one can see in Table 1, the rules in T all contain a bounded number of variables and atoms in the body, and so the number of variables in the body of each rule in
MSA(Sing(T , M )) is bounded as well. Furthermore, the datalog program MSA(Sing(T , M ))
contains predicates of bounded arity, so its chase w.r.t. I is polynomial in size. Thus, the
chase of I and MSA(T ) can be computed in polynomial time, which implies our claim.
6.2 Acyclic Horn-SHIF TBoxes
The exponential lower bound of Lemmas 59 and 60 critically depend on axioms of Type 6,
which can be used to encode exponential structures; furthermore, a combination of inverse
roles and axioms of Types 2 and 7 (i.e., of inverse roles, number restrictions, and nominals)
is also well known to be problematical (Horrocks & Sattler, 2007). In practice, however,
TBoxes are often expressed in Horn-SHIF, which disallows such axioms in TBoxes. We
next show that, for such TBoxes, the complexity of both problems drops to PSpace.
We first prove PSpace-hardness for both problems. Note that the PSpace-hardness
proof of concept satisfiability checking by Baader, Calvanese, McGuinness, Nardi, and PatelSchneider (2007) is not applicable to Horn ontologies since it uses disjunctive concepts.
Nonetheless, PSpace-hardness can be proved by a reduction from checking QBF validity.
Lemma 63. Let T be a WA Horn-SHI TBox, let I be an instance, and let F be a fact.
Then, checking whether I  T |= F is PSpace-hard.
Proof. Let  = Q1 x1 . . . Qn xn .C1  . . .  Ck be an arbitrary quantified Boolean formula defined over variables x1 , . . . , xn , where each Qi  {, }, 1  i  n is a quantifier, and each
Cj , 1  j  k is a clause of the form Cj = Lj,1  Lj,2  Lj,3 . Checking validity of  is the
canonical PSpace-hard problem.
In the rest of this proof, for a binary predicate P and variables w0 and wm , we use
P m (w0 , wm ) as an abbreviation for P (w0 , w1 )  . . .  P (wm1 , wm ), where each wi with
0 < i < m is a variable not occurring outside the conjunction. Let T be the Horn-SHI
TBox containing rules (144)(147) for each 1  i  n, rule (148) for each clause Cj and each
literal Lj,m = x` occurring in Cj , rule (149) for each clause Cj and each literal Lj,m = x`
occurring in Cj , rule (150), rule (151) for each 1  i  n such that Qi = , and rule (152)
for each 1  i  n such that Qi = .
Ai1 (x)  y.[Xi+ (x, y)  Ai (y)]

(144)

y.[Xi (x, y)  Ai (y)]
Xi+ (x, x0 )  P (x, x0 )
Xi (x, x0 )  P (x, x0 )

(145)
(146)

(z, x)  An (x)  Cj (x)

(148)

 P n` (z, x)  An (x)  Cj (x)

(149)

C1 (x)  . . .  Ck (x)  Fn (x)

(150)

P (x, z)  Fi (z)  Fi1 (x)

(151)

Ai1 (x) 

X`+ (z 0 , z)
X` (z 0 , z)

Xi+ (x, z)

P

 Fi (z) 

n`

Xi (x, z 0 )

794

0

 Fi (z )  Fi1 (x)

(147)

(152)

fiAcyclicity Notions for Existential Rules

Strictly speaking, rules (148), (149), (150), and (152) are not Horn-SHI rules, but they can
be transformed into Horn-SHI rules by replacing parts of their bodies with fresh concepts.
It is straightforward to see that T is WA.
Let I = {A0 (a)}, and let IT be the chase of I and T . Due to rules (144)(145), IT
contains a binary tree of depth n in which each leaf node is reachable from a via a path
that, for each 1  i  n, contains either Xi+ or Xi . If we interpret the presence of Xi+ and
Xi as assigning variable xi to t and f, respectively, then each leaf node corresponds to one
possible assignment of x1 , . . . , xn . Rules (148) and (149) then clearly label each leaf node
with the clauses that are true in the node, and rule (150) labels each leaf node with Fn for
which all clauses are true. Finally, rules (151) and (152) label each interior node of the tree
with Fi1 according to the semantics of the appropriate quantifier of . Clearly,  is valid
if and only if I  T |= F0 (a), which implies our claim.
We next turn our attention to the upper bounds on the complexity of answering a
BCQ over an MFA TBox, and checking whether a TBox is MFA. While in Section 6.1 we
considered a TBox T singularised according to some marking M , in this section we assume
that equality in T is handled by means of an explicit axiomatisation T . As we explain
next, this is because singularised rules are not local, which makes a PSpace membership
proof quite difficult. For example, consider the following singularised rule:
A(x)  x  x0  B(x0 )  C(x)

(153)

Atoms A(x) and B(x0 ) in the rule do not share variables and therefore need not be matched
locally in the chase of Sing(T , M ) and I; furthermore, the chase can be exponential in size,
so it is not trivial to see how it can be explored using polynomial space. Nevertheless, we
conjecture that it is possible to extend our proof to singularised rules as well; however, the
details involved seem quite technical, without explaining much about the nature of BCQ
answering under equality. Therefore, we leave this problem open and restrict ourselves to
the technically simpler case when equality in T is encoded explicitly using T .
We next show that answering a BCQ Q over an MFA Horn-SHIF TBox T and an
instance I can be performed in polynomial space. The proof uses the well-known tracing
technique of inspecting a model of T  I using polynomial space. The key aspect of this
result, however, is dealing with the transitive roles in the query, which allow the query to
be embedded non-locally into the chase of T and I. Note, however, that we can guess an
embedding of Q into the result of IT using nondeterministic polynomial time; furthermore,
since IT is a minimal Herbrand model of T (i.e., since T is Horn), we can check the
entailment of each mapped atom of Q separately, and in doing so we can use the wellknown encoding by Demri and de Nivelle (2005) to handle transitive roles.
Theorem 64. Let T be Horn-SHIF TBox, let I be an instance such that T is MFA w.r.t.
I, and let Q be a BCQ. Then, checking whether I  T |= Q is PSpace-complete.
Proof. Hardness follows from Lemma 63. We next present a nondeterministic polynomial
space algorithm that decides I  T |= Q; by Savitchs Theorem, this algorithm can be transformed into a deterministic polynomial space algorithm, which proves our claim.
Assume that BCQ Q is of the form Q = ~y .B1  . . .  Bn . Furthermore, let  = sk(T ).
Since  is just a regular atomic concept, I  T is always satisfiable in the chase IT of I
795

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

and T . Furthermore, I  T |= Q if and only if a substitution  from the variables in ~y to
the terms in IT exists such that Bi   IT for each 1  i  n; the latter clearly holds if and
only if I   |= Bi . As shown in the proof of Theorem 10, each term in IT is of the form
g1 (. . . g` (a) . . .), where ` is less than or equal to the number of function symbols in . Thus,
the first step in deciding I  T |= Q is to examine all possible  and then check I   |= Bi 
for each 1  i  n; this can clearly be done using a deterministic Turing machine that uses
polynomial space to store , provided that each individual check I   |= Bi  can also be
decided in polynomial space.
If Bi  is of the form C(t), then let 0 = , and let D = C. Alternatively, if Bi  is of
the form R(t0 , t), then let 0 be  extended with the following rules, where D and E are
fresh concepts not occurring in  and I:
 E(t0 )

(154)

E(z)  R(z, x)  D(x)

(155)

It is straightforward to see that I   |= R(t0 , t) if and only if I  0 |= D(t). Let 00 be
obtained from 0 by deleting each rule in 0 of the form
R(x1 , z)  R(z, x2 )  R(x1 , x2 )

(156)

and, for each role R occurring in such a rule, replacing each rule of the form
A(z)  R(z, x)  B(x)

(157)

with the following rules, where QA,R,B is a fresh concept unique for A, R, and B:
A(z)  R(z, x)  QA,R,B (x)

(158)

QA,R,B (z)  R(z, x)  QA,R,B (x)

(159)

QA,R,B (x)  B(x)

(160)

This transformation corresponds to the well-known elimination of transitivity by Demri and
de Nivelle (2005), so I  0 |= D(t) if and only if I  00 |= D(t); the proof of this claim is
straightforward and we omit it for the sake of brevity.
Let  be 00 extended with the equality axioms (2) and (4). Since  does not occur
in the body of the rules in 00 , we have that I  00 6|= D(t) if and only if I   6|= D(t).
Let I be the chase for I and ; then I   6|= D(t) if and only if D(t) 6 I . Note that
 contains rules of Types 15 from Table 1, rules (2) and (4), and possibly rules of the
form  E(t1 ). These facts can be used to show that each assertion in I is of one of the
following forms, where a and b are constants, t is a constant or a term that contains only
unary function symbols, f and g are unary function symbols, C is an atomic concept, and
R is an atomic role:
 C(t),
 R(a, b), R(a, f (b)), R(f (b), a), R(t, f (t)), R(f (t), t), or
 t  f (g(t)), f (t)  g(t), a  b, a  f (b), or an equality symmetric to these ones.
796

fiAcyclicity Notions for Existential Rules

The proof is by induction on the length of the chase sequence for I and , and the claim
follows straightforwardly from the I form of rules of Types 15. Motik et al. (2009b)
prove an analogous claim for a more general description logic, and their proof carries over
to the above setting with only syntactic changes.
We say that x is the central variable in a rule of Type 1 or 3, and that z is the central
variable in a rule of Type 2 or 4. W.l.o.g. we assume that the body of a rule of Type 5 does
not contain inverse roles; then, x1 is the central variable of a rule of Type 5. Finally, in the
equality replacement rules (4), the central variable is the variable being replaced.
Clearly, D(t) 6 I if and only if a Herbrand interpretation J exists in which all assertions are of the form mentioned above, such that I  J, J |= , and D(t) 6 J. We next
show how to check the existence of such J using a nondeterministic Turing machine that
runs in polynomial space.
Let f1 , . . . , fm be all function symbols occurring in . We first guess a Herbrand interpretation J 0 over the constants of I satisfying I  J 0 , and we check whether all rules in 
not of Type 1 are satisfied in J 0 . If that is the case, we consider each constant c in J0 and
call the following procedure for s = c and i = 1:
1. If i = m + 1 return true.
2. Guess a Herbrand interpretation J i such that each assertion in J i is of a form as
specified earlier and involves at least one term among f1 (s), . . . , fm (s).
3. If D(t)  J i , return false.
4. Check whether the equality symmetry rule (4) is satisfied in J i ; if not, return false.
5. Check whether J i  J i1  . . .  J 0 satisfies each rule in  if the central variable of
the rule is mapped to s; if this is not the case for each rule, return false.
6. For each 1  k  m, recursively call this procedure for fk (s) and i + 1; if one of this
calls returns false, return false as well.
7. Return true.
Assume that this procedure returns true for each constant c, and let J be the union of all J i
considered in the process. It is straightforward to see that I  J and D(t) 6 J; furthermore,
J |=  holds since the satisfaction of each rule r   in J can be ascertained locally, by
inspecting the vicinity of the ground term that is mapped to the central variable of r.
Furthermore, the recursion depth of our algorithm is m and at each recursion level we need
to keep a polynomially sized interpretation J i , so our algorithm can be implemented using
a nondeterministic Turing machine that uses polynomial space.
Theorem 65. Let T be Horn-SHIF TBox, and let I be an instance. Then, deciding
whether T is MFA w.r.t. I is in PSpace, and deciding whether T is universally MFA is
PSpace-hard.
Proof. (Membership) Rules in MFA(T ) are almost Horn-SHIF rules: rule (19) can be
made a Horn-SHIF rule by replacing S in the body with D (which clearly does not affect
the consequences of the rule), and the fact that rule (20) contains a nullary atom in the
797

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

head is immaterial. Thus, the claim can be proved by a straightforward adaptation of
the membership proof of Theorem 64. The main difference in the algorithm is that, with n
function symbols, we need to examine the models to depth n+1; however, such an algorithm
still uses polynomial space.
(Hardness) Let  be an arbitrary QBF, and let T be as in the hardness proof of
Lemma 63. TBox T is WA; it contains only constant-free, equality-free, and connected
rules; and it does not contain a predicate of zero arity. Hence, by Lemma 7, a Horn-SHI
TBox T 0 exists such that  is valid if and only if T 0 is not universally MFA.
We finish this section by proving that checking whether a set of Horn-SHI rules is
universally MSA is PTime-hard; in this way, we also obtain a matching lower bound for
theorem Theorem 62 from Section 6.1.
Theorem 66. Checking whether a Horn-SHI TBox T is universally MSA is PTime-hard.
Proof. Let N be a set of Horn propositional clauses of the form v1  . . .  vn  vn+1 and
let v be a propositional variable; deciding N |= v is well known to be PTime-hard. Let
Vi be a concept uniquely associated with each propositional variable vi ; let A be a fresh
concept; and let T be the TBox obtained by transforming each propositional clause in N
of the above form into rule (161).
A(x)  V1 (x)  . . .  Vn (x)  Vn+1 (x)

(161)

Finally, let I = {A(a)}. Clearly, N |= v holds if and only if I  T |= V (a) holds. TBox T is
WA, it contains only constant-free, equality-free, and connected rules, and no predicate in
T is of zero arity; hence, by Lemma 7, a Horn-SHI TBox T 0 exists such that N 6|= v holds
if and only if T 0 is universally MFA. Finally, the only existential variable in T 0 occurs in a
rule of the form (23), so it is straightforward to see that T 0 is universally MFA if and only
if T 0 is universally MSA.

7. Experiments
To estimate the extent to which various acyclicity notions can be used in practice, we conducted two sets of experiments. First, we implemented MFA, MSA, JA, and WA checkers,
and we used them to check acyclicity of a large corpus of Horn ontologies. Our goal was
to see how many ontologies are acyclic and could thus be used with (suitably extended)
materialisation-based OWL reasoners. Second, we computed the materialisation of the
acyclic Horn ontologies and compared the number of facts before and after materialisation.
The goal of these tests was to see whether materialisation-based reasoning with acyclic
ontologies is practically feasible.
Tests were performed on a Windows R2 Server with two Intel Xeon 3.06GHz processors.
We used a repository of 336 OWL ontologies whose TBox axioms can be transformed into
existential rules where at least one rule contains an existential quantifier in the head. These
ontologies include a large subset of the Gardiner ontology corpus (Gardiner, Tsarkov, &
Horrocks, 2006), the LUBM ontology, several Phenoscape ontologies, and a number of
ontologies from two versions of the Open Biomedical Ontology (OBO) corpus. Please note
798

fiAcyclicity Notions for Existential Rules

that no test ontology has been obtained from conceptual models (e.g., the ER models or
UML diagrams): due to the specific modelling patterns used in conceptual modelling, such
ontologies are less likely to be acyclic. Each test ontology can be accessed online from our
ontology repository by means of a unique ID.5 Each ID identifies one self-contained OWL
ontology frozen in time with all of its imports resolved at the time the ontology was added
to the repository; furthermore, any possible future version of the ontology will be assigned a
fresh ID. These measures should ensure that our experiments can be independently repeated
at any point in the future.
7.1 Acyclicity Tests
We implemented all acyclicity checks by adapting the HermiT reasoner.6 HermiT was
used to transform an ontology into DL-clausesformulae quite close to existential rules.
In the result, at-least number restrictions in head atoms were replaced with existential
quantification, atoms involving datatypes were eliminated, and the DL-clauses with no
head atoms were removed: datatypes and empty heads can cause inconsistencies, but they
cannot prevent the skolem chase from terminating.
Each set of rules  obtained by the above preprocessing steps was considered in combination with each acyclicity notion X  {WA, JA, MSA, MFA} as follows. If  did not contain
the equality predicate, we simply checked whether   X. If  contained the equality predicate, we checked whether   X  , and we also checked whether 0  X for 0   the set
of all rules of  that do not contain the equality predicate; these tests provided us with a
lower and an upper bound for acyclicity, respectively. Each acyclicity test was performed
by modifying  (or 0 ) as required by X and then running HermiT to check for a particular
logical entailment on the critical instance.
Our tests revealed MFA and MSA to be indistinguishable for all 336 test ontologies; that
is, all MFA ontologies were found to be MSA as well (the converse holds per Theorem 14).
A total of 213 (63.4%) ontologies were found to be MSA, including 43 of the 49 (87.8%)
ontologies from the Gardiner corpus, 164 of the 208 (78.8%) OBO ontologies, and the
LUBM ontology. In contrast, the GALEN ontology and its variants, the GO ontology
and its extensions, and the 55 Phenoscape ontologies were found not to be MFA. These
results are summarised in Table 2. Given the large number of ontologies tested, it would
be impractical to present the results for each ontology individually. Instead, the ontologies
are grouped by number of generating rules (G-rules), which are the rules containing an
existential quantifier; for each group, Table 2 shows the total number of ontologies, as well
as the numbers of ontologies found to be MSA, JA, and WA. Of the 123 ontologies that
are not MFA, seven ontologies are in ELHr , so CQ answering over these ontologies can be
realised using the combined approaches by Lutz et al. (2009) and Kontchakov et al. (2011).
The five older versions of OBO ontologies (IDs 00359, 00374, 00376, 00382, and 00486)
are MSA, whereas their newer versions (IDs 00360, 00375, 00377, 00383, and 00487) are
not MFA. In contrast, two older versions of OBO ontologies are not MFA (IDs 00432 and
00574), but their newer versions (IDs 00433 and 00575) are MSA.
5. URL http://www.cs.ox.ac.uk/isg/ontologies/UID/xxxxx.owl can be used to download an ontology
that has been assigned ID xxxxx.
6. http://www.hermit-reasoner.com/

799

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

G-rules
< 100
1001K
1K5K
5K12K
12K160K

ontologies without equality
Total MSA JA
WA
44
42
42
41
69
62
62
41
38
31
30
24
28
23
16
14
18
7
6
5

ontologies with equality
Total MSA JA WA
48
39
39
39
41
1
0
0
17
2
1
1
10
5
0
0
23
1
0
0

Table 2: Results of acyclicity tests
Finally, we found 15 large OBO ontologies (including different versions of the same
ontologies) that are MSA but not JA. Thus, MSA seems to be particularly useful on
complex ontologies since it analyses implications between existentially quantified variables
more precisely than the previously known notions. Table 3 shows for each of these ontologies
the number of generating rules (G-rules), whether the ontology uses the equality predicate
(Eq), the ontology expressivity in the description logic family of languages (DL), and the
number of classes (C), properties (P), and axioms (A) that the ontology contains. Different
versions of the same ontology are distinguished in the table as old and new. Two further
ontologies (IDs 00762 and 00766) containing the equality predicate are MSA , but their
status regarding joint acyclicity is unknown: they are JA when the rules involving the
equality predicate are deleted, but are not JA .
7.2 Materialisation Tests
To estimate the practicability of materialisation in acyclic ontologies, we measured the
maximal depth of function symbol nesting in terms generated by materialisation on critical
instances. This measure, which we call ontology depth, is of interest as it provides us
with a bound on the size of the materialisation. Out of the 213 MSA ontologies, our test
succeeded on 207 of them (tests were aborted if they did not finish in three hours). On the
latter ontologies, depth was distributed as follows:
 123 (59.4%) ontologies have depth less than 5;
 30 (14.5%) ontologies have depth between 5 and 9;
 47 (22.7%) ontologies have depth between 10 to 19;
 5 (2.4%) ontologies have depth between 20 and 49; and
 2 (1.0%) ontologies have depth between 50 to 70.
These results leads us to believe that many (but clearly not all) ontologies have manageable
depths, which should allow for successful materialisation-based query answering.
We also computed the materialisation for several acyclic ontologies. As our implementation is prototypical, our primary goal was not to evaluate the performance of computing
the materialisation, but rather to estimate the blowup in the number of facts. We clearly do

800

fiAcyclicity Notions for Existential Rules

Ontology ID G-rules Eq
DL
C
P
A
biological process xp cell.imports-local.owl
00371
7464
yes SHIF 17296 178
117925
biological process xp cellular component.imports-local.owl (old)
00374
8270
yes SHIF 18673 186
126796
biological process xp multi organism process.imports-local.owl (old)
00382
8378
no EL++ 27900 18
295396
biological process xp plant anatomy.imports-local.owl (old)
00386
7559
yes SHIF 19146 193
122062
biological process xp plant anatomy.imports-local.owl (new)
00387
12025 yes SRIF 27412 215
213956
bp xp cell.imports-local.owl
00398
7419
yes SHIF 17296 177
117881
bp xp cellular component.imports-local.owl
00400
7999
yes SHIF 18676 175
126540
cellular component xp go.imports-local.owl (old)
00415
7752
no EL++ 27890
8
210765
cellular component xp go.imports-local.owl (new)
00416
12269
no EL++ 37254
9
334762
fypo.owl
00476
1834
no EL++
1677
22
8027
go xp regulation.imports-local.owl (old)
00486
7777
no EL++ 27891
5
295138
go xp regulation.owl (old)
00488
7777
no EL++ 27883
5
214080
go xp regulation.owl (new)
00489
9507
no EL++ 30170
6
238200
molecular function xp regulators.imports-local.owl (old)
00536
6762
no EL++ 25521
5
198170
molecular function xp regulators.imports-local.owl (new)
00537
11089
no EL++ 34135
8
316057
Table 3: MSA but not JA ontologies
not expect this blowup to depend linearly on size of the input number of facts; however, our
results should provide us with a rough estimate of the performance of materialisation-based
reasoning in practice. Most of our test ontologies, however, do not contain many facts:
ontologies are often constructed as general vocabularies, while facts are often applicationspecific and are thus not publicly available. To overcome this problem, we conducted two
kinds of experiments.
First, we computed the materialisation of two ontologies that contain facts: LUBM with
one university (ID 00347), and the kmi-basic-portal ontology (ID 00078). The TBox of
LUBM has eight generating rules and depth one, and there are 100, 543 facts before ma801

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Depth

#

<5
59
1070

123
30
54

time
max avg
38
0.7
71
7
9396 1807

gen. size
max avg
65
2
122
30
1286 175

mat. size
max avg
89
8
132
38
1297 189

Table 4: Materialisation times (in seconds) and sizes
terialisation. Materialisation took only 2 seconds, and it produced 231, 200 new facts, of
which 97, 860 were added by the generating rules. The kmi-basic-portal ontology has ten
generating rules and depth two, and there were 198 facts before materialisation. Materialisation took only 0.03 seconds, and it produced 744 new facts, of which 145 were added by
the generating rules.
Second, for each of the ontologies identified as MSA, we instantiated each class and
each property with fresh individuals. We then computed the materialisation and measured
the generated size (the number of facts introduced by the generating rules divided by the
number of facts before materialisation), the materialisation size (the number of facts after
materialisation divided by the number of facts before materialisation), and the time needed
to compute the materialisation. Since most generating rules in these ontologies have singleton body atoms (i.e., they are of the form A(x)  R.C(x)), these measures should provide
a reasonable estimate of the increase in the number of facts during materialisation. Table 4
summarises the results of our tests for the 207 ontologies on which the test succeeded. Ontologies are grouped by their depth, and each group shows the number of ontologies (#),
and the maximal and average materialisation time, generated size, and materialisation size.
Thus, materialisation seems practically feasible for many ontologies: for 123 ontologies
with depth less than 5, materialisation increases the ontology size by a factor of 8. This
suggests that principled, materialisation-based reasoning for ontologies beyond the OWL 2
RL profile may be feasible, especially for ontologies with relatively small depths.

8. Conclusions
In this paper, we investigated acyclicity notionssufficient conditions that ensure termination for skolem chase on existential rules. We proposed two novel notions, called MFA and
MSA, for which we determined tight complexity bounds for membership checking, as well
as for conjunctive query answering over acyclic existential rules.
We also conducted a thorough investigation of the acyclicity notions known in the literature, and we produced a complete taxonomy of their relative expressiveness. Our results
show that MFA and MSA generalise most of the previously considered notions.
We next investigated ways to ensure acyclicity of existential rules that contain the
equality predicate. To this end, we presented several optimisations of the singularisation
technique by Marnette (2009). Our optimisations can often reduce the number of acyclicity
checks needed, thus making the singularisation technique more suitable for practical use.
Finally, we studied the problem of answering conjunctive queries over acyclic DL ontologies. On the theoretical side, we showed that acyclicity can make this problem computation802

fiAcyclicity Notions for Existential Rules

ally easier; furthermore, provided that the result is acyclic, one can extend Horn ontologies
with arbitrary SWRL rules without affecting decidability and the worst-case complexity
of query answering. On the practical side, we investigated the extent to which acyclicity
notions enable principled extensions of materialisation-based ontology reasoners with support for existential quantification. Our tests show that many ontologies commonly used
in practice are acyclic, and that the blowup in the number of facts due to materialisation
is manageable. This suggests that principled extensions of materialisation-based ontology
reasoners are practically feasible and useful.
An interesting topic for future work is to see whether our acyclicity notions can be used
in a more general logic programming setting. We see several main sources of technical
difficulties towards this goal. First, general logic programs can contain functional terms in
body atoms. Such terms can cancel out function symbols introduced by head atoms, and
it is not clear how to take this into account in an acyclicity test. Second, logic programs can
contain atoms under nonmonotonic negation, which are likely to need special treatment;
Magka, Krotzsch, and Horrocks (2013) recently made a first step in that direction. Third, it
might be desirable to modularise the ways in which these different concerns are handled and
thus arbitrarily combine the approaches for handling function symbols in the body and/or
the head with the approaches for dealing with nonmonotonic negation.

Acknowledgments
This work was supported by the Royal Society, the Seventh Framework Program (FP7)
of the European Commission under Grant Agreement 318338, Optique, and the EPSRC
projects ExODA, Score!, and MaSI3 .

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison Wesley.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2007). The Description Logic Handbook: Theory, Implementation and Applications
(2nd edition). Cambridge University Press.
Baget, J.-F. (2004). Improving the Forward Chaining Algorithm for Conceptual Graphs
Rules. In Dubois, D., Welty, C. A., & Williams, M.-A. (Eds.), Proc. of the 9th Int.
Conf. on Principles of Knowledge Representation and Reasoning (KR2004), pp. 407
414, Whistler, BC, Canada. AAAI Press.
Baget, J.-F., Leclere, M., Mugnier, M.-L., & Salvat, E. (2011a). On rules with existential
variables: Walking the decidability line. Artificial Intelligence, 175 (910), 16201654.
Baget, J.-F., Mugnier, M.-L., & Thomazo, M. (2011b). Towards Farsighted Dependencies
for Existential Rules. In Rudolph, S., & Gutierrez, C. (Eds.), Proc. of the 5th Int.
Conf. on Web Reasoning and Rule Systems (RR 2011), Vol. 6902 of LNCS, pp. 3045,
Galway, Ireland. Springer.

803

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Baumgartner, P., Furbach, U., & Niemela, I. (1996). Hyper Tableaux. In Proc. of the
European Workshop on Logics in Artificial Intelligence (JELIA 96), No. 1126 in
LNAI, pp. 117, Evora, Portugal. Springer.
Beeri, C., & Vardi, M. Y. (1981). The Implication Problem for Data Dependencies. In
Even, S., & Kariv, O. (Eds.), Proc. of the 8th Colloquium on Automata, Languages
and Programming (ICALP 1981), Vol. 115 of LNCS, pp. 7385, Acre (Akko), Israel.
Springer.
Bishop, B., & Bojanov, S. (2011). Implementing OWL 2 RL and OWL 2 QL rule-sets for
OWLIM. In Dumontier, M., & Courtot, M. (Eds.), Proc. of the OWL: Expreiences
and Directions Workshop (OWLED 2011), Vol. 796 of CEUR WS Proceedings, San
Francisco, UCA, USA.
Broekstra, J., Kampman, A., & van Harmelen, F. (2002). Sesame: A Generic Architecture
for Storing and Querying RDF and RDF Schema. In Horrocks, I., & Hendler, J. A.
(Eds.), Proc. of the 1st Int. Semantic Web Conf. (ISWC 2002), Vol. 2342 of LNCS,
pp. 5468, Sardinia, Italy. Springer.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010a). Datalog : A
Family of Logical Knowledge Representation and Query Languages for New Applications. In Proc. of the 25th IEEE Symposium on Logic in Computer Science (LICS
2010), pp. 228242, Edinburgh, United Kingdom. IEEE Computer Society.
Cal, A., Gottlob, G., & Pieris, A. (2010b). Query Answering under Non-guarded Rules
in Datalog . In Hitzler, P., & Lukasiewicz, T. (Eds.), Proc. of the 4th Int. Conf. on
Web Reasoning and Rule Systems (RR 2010), Vol. 6333 of LNCS, pp. 117, Bressanone/Brixen, Italy. Springer.
Cal, A., Gottlob, G., & Pieris, A. (2011). New Expressive Languages for Ontological Query
Answering. In Burgard, W., & Roth, D. (Eds.), Proc. of the 25th National Conference
on Artificial Intelligence (AAAI 2011), pp. 15411546, San Francisco, CA, USA. AAAI
Press.
Calimeri, F., Cozza, S., Ianni, G., & Leone, N. (2008). Computable Functions in ASP:
Theory and Implementation. In Garcia de la Banda, M., & Pontelli, E. (Eds.), Proc.
of the 24th Int. Conf. on Logic Programming (ICLP 2008), Vol. 5366 of LNCS, pp.
407424, Udine, Italy. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
Reasoning and Efficient Query Answering in Description Logics: The DL-Lite Family.
Journal of Automated Reasoning, 9 (3), 385429.
Carroll, J. J., Dickinson, I., Dollin, C., Reynolds, D., Seaborne, A., & Wilkinson, K. (2004).
Jena: Implementing the Semantic Web Recommendations. In Feldman, S. I., Uretsky,
M., Najork, M., & Wills, C. E. (Eds.), Proc. of the 13th Int. Conf. on World Wide
Web (WWW 2004)Alternate Track, pp. 7483, New York, NY, USA. ACM.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2012). Acyclicity Conditions and their Application to Query Answering in Description Logics. In Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proc. of the 13th Int.

804

fiAcyclicity Notions for Existential Rules

Conf. on the Principles of Knowledge Representation and Reasoning (KR 2012), pp.
243253, Rome, Italy.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U.
(2008). OWL 2: The next step for OWL. Journal of Web Semantics: Science, Services
and Agents on the World Wide Web, 6 (4), 309322.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity and expressive
power of logic programming. ACM Computing Surveys, 33 (3), 374425.
De Schreye, D., & Decorte, S. (1994). Termination of Logic Programs: The Never-Ending
Story. Journal of Logic Programming, 1920, 199260.
Demri, S., & de Nivelle, H. (2005). Deciding Regular Grammar Logics with Converse
Through First-Order Logic. Journal of Logic, Language and Information, 14 (3), 289
329.
Deutsch, A., Nash, A., & Remmel, J. B. (2008). The chase revisited. In Lenzerini, M., &
Lembo, D. (Eds.), Proc. of the 27th ACM SIGMOD-SIGACT-SIGART Symposium on
Principles of Database Systems (PODS 2008), pp. 149158, Vancouver, BC, Canada.
Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query Answering in the Description
Logic Horn-SHIQ. In Holldobler, S., Lutz, C., & Wansing, H. (Eds.), Proc. of the
11th European Conference on Logics in Artificial Intelligence (JELIA 2008), Vol. 5293
of LNCS, pp. 166179, Dresden, Germany. Springer.
Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: semantics and
query answering. Theoretical Computer Science, 336 (1), 89124.
Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework for an Automated Comparison
of Description Logic Reasoners. In Cruz, I. F., Decker, S., Allemang, D., Preist, C.,
Schwabe, D., Mika, P., Uschold, M., & Aroyo, L. (Eds.), Proc. of the 5th Int. Semantic
Web Conference (ISWC 2006), Vol. 4273 of LNCS, pp. 654667, Athens, GA, USA.
Springer.
Gebser, M., Schaub, T., & Thiele, S. (2007). GrinGo: A New Grounder for Answer Set
Programming. In Baral, C., Brewka, G., & Schlipf, J. S. (Eds.), Proc. of the 9th
Int. Conf. on Logic Programming and Nonmonotonic Reasoning (LPNMR 2007), Vol.
4483 of LNCS, pp. 266271, Tempe, AZ, USA.
Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2008). Conjunctive Query Answering for
the Description Logic SHIQ. Journal of Artificial Intelligence Research, 31, 151198.
Greco, S., Spezzano, F., & Trubitsyna, I. (2012). On the Termination of Logic Programs
with Function Symbols. In Dovier, A., & Santos Costa, V. (Eds.), Proc. of the 8th Int.
Conf. on Logic Programming (ICLP 2012), Vol. 17 of Leibniz International Proceedings
in Informatics, pp. 323333, Budapest, Hungary.
Hastings, J., Magka, D., Batchelor, C., Duan, L., Stevens, R., Ennis, M., & Steinbeck, C.
(2012). Structure-based classification and ontology in chemistry. Journal of Cheminformatics, 4 (8).
Horrocks, I., & Patel-Schneider, P. F. (2004). A Proposal for an OWL Rules Language. In
Proc. of the 13th Int. World Wide Web Conference (WWW 2004), pp. 723731, New
York, NY, USA. ACM Press.
805

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Horrocks, I., & Sattler, U. (2007). A Tableau Decision Procedure for SHOIQ. Journal of
Automated Reasoning, 39 (3), 249276.
Horrocks, I., Patel-Schneider, P. F., Bechhofer, S., & Tsarkov, D. (2005). Owl rules: A
proposal and prototype implementation. J. Web Sem., 3 (1), 2340.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity of Reasoning in Very Expressive Description Logics. In Proc. of the 19th Int. Joint Conf. on Artificial Intelligence
(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.
Johnson, D. S., & Klug, A. C. (1984). Testing Containment of Conjunctive Queries under
Functional and Inclusion Dependencies. Journal of Computer and System Sciences,
28 (1), 167189.
Kiryakov, A., Ognyanov, D., & Manov, D. (2005). OWLIM  A Pragmatic Semantic Repository for OWL. In Dean, M., Guo, Y., Jun, W., Kaschek, R., Krishnaswamy, S., Pan,
Z., & Sheng, Q. Z. (Eds.), Proc. of the Int. Workshop on Web Information Systems
Engineering (WISE 2005), Vol. 3807 of LNCS, pp. 182192, New York, NY, USA.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). The Combined Approach to Ontology-Based Data Access. In Walsh, T. (Ed.), Proc. of the 22nd
Int. Joint Conf. on Artificial Intelligence (IJCAI 2011), pp. 26562661, Barcelona,
Spain.
Krotzsch, M., & Rudolph, S. (2011). Extending Decidable Existential Rules by Joining
Acyclicity and Guardedness. In Walsh, T. (Ed.), Proc. of the 22nd Int. Joint Conf.
on Artificial Intelligence (IJCAI 2011), pp. 963968, Barcelona, Spain.
Krotzsch, M., & Rudolph, S. (2013). On the Relationship of Joint Acyclicity and SuperWeak Acyclicity. Tech. rep. 3037, Institute AIFB, Karlsruhe Institute of Technology.
Available online at http://www.aifb.kit.edu/web/Techreport3013.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Conjunctive Queries for a Tractable Fragment of OWL 1.1. In Aberer, K., Choi, K.-S., Noy, N. F., Allemang, D., Lee, K.-I.,
Nixon, L. J. B., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G.,
& Cudre-Mauroux, P. (Eds.), Proc. of the 6th Int. Semantic Web Conference (ISWC
2007), Vol. 4825 of LNCS, pp. 310323, Busan, Korea. Springer.
Kutz, O., Horrocks, I., & Sattler, U. (2006). The Even More Irresistible SROIQ. In
Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proc. of the 10th Int. Conf. on the
Principles of Knowledge Representation and Reasoning (KR 2006), pp. 6878, Lake
District, UK. AAAI Press.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
The DLV system for knowledge representation and reasoning. ACM Transactions on
Computational Logic, 7 (3), 499562.
Lierler, Y., & Lifschitz, V. (2009). One More Decidable Class of Finitely Ground Programs. In Hill, P. M., & Warren, D. S. (Eds.), Proc. of the 25th Int. Conf. on Logic
Programming (ICLP 2009), Vol. 5649 of LNCS, pp. 489493, Pasadena, CA, USA.
Springer.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive Query Answering in the Description
Logic EL Using a Relational Database System. In Boutilier, C. (Ed.), Proc. of the 21st
806

fiAcyclicity Notions for Existential Rules

Int. Joint Conf. on Artificial Intelligence (IJCAI 2009), pp. 20702075, Pasadena, CA,
USA.
Magka, D., Krotzsch, M., & Horrocks, I. (2013). Computing Stable Models for Nonmonotonic Existential Rules. In Proc. of the 23rd Int. Joint Conf. on Artificial Intelligence
(IJCAI 2013). AAAI Press/IJCAI. To appear.
Magka, D., Motik, B., & Horrocks, I. (2012). Modelling Structured Domains Using Description Graphs and Logic Programming. In Simperl, E., Cimiano, P., Polleres, A.,
Corcho, O., & Presutti, V. (Eds.), Proc. of the 9th Extended Semantic Web Conference
(ESWC 2012), Vol. 7295 of LNCS, pp. 330344, Heraklion, Greece. Springer.
Maier, D., Mendelzon, A. O., & Sagiv, Y. (1979). Testing Implications of Data Dependencies. ACM Transactions on Database Systems, 4 (4), 455469.
Marnette, B. (2009). Generalized schema-mappings: from termination to tractability. In
Paredaens, J., & Su, J. (Eds.), Proc. of the 28th ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems (PODS 2009), pp. 1322, Providence,
RI, USA.
Marnette, B. (2010). Tractable Schema Mappings Under Oblivious Termination. Ph.D.
thesis, University of Oxford, Oxford, UK.
Meditskos, G., & Bassiliades, N. (2008). Combining a DL Reasoner and a Rule Engine for
Improving Entailment-Based OWL Reasoning. In Sheth, A. P., Staab, S., Dean, M.,
Paolucci, M., Maynard, D., Finin, T. W., & Thirunarayan, K. (Eds.), International
Semantic Web Conference, Vol. 5318 of LNCS, pp. 277292, Karlsruhe, Germany.
Meier, M. (2010). On the Termination of the Chase Algorithm. Ph.D. thesis, Universitat
Freiburg.
Meier, M., Schmidt, M., & Lausen, G. (2009). On Chase Termination Beyond Stratification.
Proceedings of the VLDB Endowment, 2 (1), 970981.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009a). OWL 2
Web Ontology Language: Profiles, W3C Recommendation.
http://www.w3.org/TR/owl2-profiles/.
Motik, B., Shearer, R., & Horrocks, I. (2009b). Hypertableau Reasoning for Description
Logics. Journal of Artificial Intelligence Research, 36, 165228.
Mungall, C. (2009). Experiences Using Logic Programming in Bioinformatics. In Hill,
P. M., & Warren, D. S. (Eds.), Proc.of the 25th Int. Conf. on Logic Programming
(ICLP 2009), Vol. 5649 of LNCS, pp. 121, Pasadena, CA, USA. Springer.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data Complexity of Query Answering in
Expressive Description Logics via Tableaux. Journal of Automated Reasoning, 41 (1),
6198.
Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query Answering in the Horn Fragments of
the Description Logics SHOIQ and SROIQ. In Walsh, T. (Ed.), Proc. of the 22nd
Int. Joint Conf. on Artificial Intelligence (IJCAI 2011), pp. 10391044, Barcelona,
Spain.

807

fiCuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang

Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). Tractable Query Answering and Rewriting under Description Logic Constraints. Journal of Applied Logic, 8 (2), 151232.
Rudolph, S., & Glimm, B. (2010). Nominals, Inverses, Counting, and Conjunctive Queries
or: Why Infinity is your Friend!. Journal of Artificial Intelligence Research, 39, 429
481.
Spezzano, F., & Greco, S. (2010). Chase Termination: A Constraints Rewriting Approach.
Proceedings of the VLDB Endownment, 3 (1), 93104.
Syrjanen, T. (2001). Omega-Restricted Logic Programs. In Eiter, T., Faber, W., &
Truszczynski, M. (Eds.), Proc. of the 6th Int. Conference on Logic Programming and
Nonmonotonic Reasoning (LPNMR 2001), Vol. 2173 of LNCS, pp. 267279, Vienna,
Austria. Springer.
Syrjanen, T., & Niemela, I. (2001). The Smodels System. In Eiter, T., Faber, W., &
Truszczynski, M. (Eds.), Proc. of the 6th Int. Conf. on Logic Programming and Nonmonotonic Reasoning (LPNMR 2001), Vol. 2173 of LNAI, pp. 434438, Vienna, Austria. Springer.
Wu, Z., Eadon, G., Das, S., Chong, E. I., Kolovski, V., Annamalai, M., & Srinivasan, J.
(2008). Implementing an Inference Engine for RDFS/OWL Constructs and UserDefined Rules in Oracle. In Alonso, G., Blakeley, J. A., & Chen, A. L. P. (Eds.), Proc.
of the 24th Int. Conf. on Data Engineering (ICDE 2008), pp. 12391248, Cancun,
Mexico. IEEE.

808

fiJournal of Artificial Intelligence Research 47 (2013) 613-647

Submitted 1/13; published 7/13

Asymmetric Distributed Constraint Optimization Problems
Tal Grinshpoun
Alon Grubshtein

grinshpo@cs.bgu.ac.il
alongrub@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University of the Negev
Beer-Sheva, Israel

Roie Zivan

zivanr@bgu.ac.il

Department of Industrial Engineering and Management
Ben-Gurion University of the Negev
Beer-Sheva, Israel

Arnon Netzer
Amnon Meisels

netzerar@cs.bgu.ac.il
am@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University of the Negev
Beer-Sheva, Israel

Abstract
Distributed Constraint Optimization (DCOP) is a powerful framework for representing
and solving distributed combinatorial problems, where the variables of the problem are
owned by different agents. Many multi-agent problems include constraints that produce
different gains (or costs) for the participating agents. Asymmetric gains of constrained
agents cannot be naturally represented by the standard DCOP model.
The present paper proposes a general framework for Asymmetric DCOPs (ADCOPs).
In ADCOPs different agents may have different valuations for constraints that they are
involved in. The new framework bridges the gap between multi-agent problems which tend
to have asymmetric structure and the standard symmetric DCOP model. The benefits of
the proposed model over previous attempts to generalize the DCOP model are discussed
and evaluated.
Innovative algorithms that apply to the special properties of the proposed ADCOP
model are presented in detail. These include complete algorithms that have a substantial
advantage in terms of runtime and network load over existing algorithms (for standard
DCOPs) which use alternative representations. Moreover, standard incomplete algorithms
(i.e., local search algorithms) are inapplicable to the existing DCOP representations of
asymmetric constraints and when they are applied to the new ADCOP framework they
often fail to converge to a local optimum and yield poor results. The local search algorithms
proposed in the present paper converge to high quality solutions. The experimental evidence
that is presented reveals that the proposed local search algorithms for ADCOPs achieve
high quality solutions while preserving a high level of privacy.

1. Introduction
The universe is asymmetric and I am persuaded that life, as it is known to us,
is a direct result of the asymmetry of the universe or of its indirect consequences. The universe is asymmetric.  Louis Pasteur
c
2013
AI Access Foundation. All rights reserved.

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Multi-agent systems (MAS) often include a combinatorial problem which is distributed
among the agents. Examples of multi-agent combinatorial problems are the Meetings
Scheduling problem (Modi & Veloso, 2004; Gershman, Grubshtein, Meisels, Rokach, &
Zivan, 2008), Sensor nets (Zhang, Xing, Wang, & Wittenburg, 2005; Zivan, Glinton, &
Sycara, 2009), and Vehicle Routing (Leaute & Faltings, 2011). The natural representation
of such problems in terms of agent-owned variables and in terms of agent-specified values for
combinations of assignments (whether costs or utilities), has encouraged the study of Distributed Constraint Optimization Problems (DCOPs). DCOPs are a powerful framework
for formulating and solving MAS combinatorial problems. In the last decade algorithmic
search techniques for DCOPs were intensively studied (Yeoh, Felner, & Koenig, 2010; Petcu
& Faltings, 2006; Mailler & Lesser, 2004; Gershman, Meisels, & Zivan, 2009). Since DCOPs
are NP-hard, many recent studies consider incomplete algorithms (Maheswaran, Pearce, &
Tambe, 2006; Zhang et al., 2005; Pearce & Tambe, 2007; Zivan, 2008; Rogers, Farinelli,
Stranders, & Jennings, 2011).
The strong relation between MAS and DCOPs was identified and discussed in previous
studies (e.g., Maheswaran et al., 2006; Chapman, Rogers, & Jennings, 2008). Chapman
et al. (2008) examine the analogy between the DCOP formulation and a class of games
known as Potential Games. The importance of this analogy lies in the fact that every
finite potential game possesses at least one pure strategy Nash Equilibrium (NE) (Monderer
& Shapley, 1996).
In the world of game theory, a pure strategy NE is a stable profile of actions corresponding to the set of all participants, in which any unilateral change of action by a single
participant will not yield a better personal gain for the participant. In the DCOP formulation, the above definition coincides with special solutions known as local optima (minima
or maxima) (Yokoo, 2000; Zhang et al., 2005). These solutions are sets of assignments to
variables made by all agents, in which a single change of assignment by an agent will only
reduce the global gain.
The source of this correspondence between NEs and local optima stems from the constraint structure of DCOPs. Each constraint C over variables of k agents is defined as a
mapping from the domains of the variables to a single (positive) real value:
C : Di1  Di2     Dik  R+
The above definition of a constraint implies that the cost (gain) of a constraint is the same
for all participating agents. When an agent lowers its cost or gain from a constraint, all
of its constrained peers share a similar decrement in cost from that constraint. Thus, it
is clear that a change of an assignment can reduce personal gains if and only if it reduces
global gains as well.
In many real life situations constrained agents value differently the results of decisions
on constrained issues even if they consider the same constraints. In fact, this is the natural
scenario in a typical MAS situation as in the following examples. In the meeting scheduling problem, agents which attend the same meeting may derive different utilities from it.
Moreover, their preferences and constraints regarding its time and location are expected
to be different. Another example for a distributed application which is asymmetric is the
smart grid (Ramchurn, Vytelingum, Rogers, & Jennings, 2011), where the cost users pay
for electricity may be higher in heavy load hours, yet the increase of price is endured with
614

fiAsymmetric Distributed Constraint Optimization Problems

respect to the agents use and is not evenly spread among users. Supply chain management (Burke, Brown, Dogru, & Lowe, 2007) among multiple consumers might also include
asymmetric constraints. Different consumers can have different levels of urgency regarding
the time of supply, therefore the latency costs they endure are different.
The above observation calls for a generalization of the standard DCOP model (Modi,
Shen, Tambe, & Yokoo, 2005; Meisels, 2007). Such a generalized model will enable the
representation of asymmetric gains for agents involved in a constraint. As a result, it will
be applicable to asymmetric MAS scenarios.
Former studies proposed to capture asymmetric gains among constrained agents by introducing additional variables for each agent. The additional variables are duplicates of the
variables of constrained agents. Each agent holds duplications for every variable its own
variables are constrained with. By imposing hard equality constraints between variables
and their duplications the model allows each agent to account for its own constraints (Maheswaran, Tambe, Bowring, Pearce, & Varakantham, 2004b; Petcu, 2007). The complete
scheme of duplicating all variables of constrained agents and of using rigid constraints to
enforce equality of assignments with other agents was termed Private Events as Variables
(PEAV).
When considering complete search, the PEAV formulation indeed offers a solvable representation to an asymmetric DCOP that allows the use of algorithms designed to solve
symmetric DCOPs. The main consequence of using this model is the increment in the
problem size, which (as is demonstrated in the experimental evaluation of the present paper) for an NP-hard problem such as DCOP, has a devastating effect on performance.
The situation becomes more complicated when considering incomplete methods. Specifically, PEAV does not enable the use of standard local search algorithms for solving asymmetric problems. The present paper demonstrates that every allocation which satisfies the
hard equality constraints in PEAV is a local optimum which cannot be escaped by local
search algorithms.
The present study proposes Asymmetric DCOPs (ADCOPs), a model for representing
asymmetric combinatorial multi-agent problems. ADCOPs naturally accommodate constraints where the participating agents have different gains or costs. It allows each agent to
hold its own evaluation of different outcomes with respect to each constraint it is involved
in.
The shortcomings of existing DCOP algorithms for solving problems represented by the
proposed model triggers an intensive algorithmic study:
1. for complete search we propose algorithms that can solve an asymmetric problem without the need to expand the number of variables of the problem as in the PEAV model.
The advantages in performance of the proposed algorithms over state-of-the-art complete algorithms that use the PEAV representation are demonstrated empirically.
2. for incomplete (local) search we propose algorithms that are able to converge to high
quality solutions when solving asymmetric problems, in contrast to existing DCOP local search algorithms. A proof that guarantees this convergence is provided. The proposed algorithms require some exchange of the problems information among agents.
Thus, the algorithms are evaluated not only in terms of solution quality, but in terms
615

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

of privacy as well. The empirical results of the present paper demonstrate that the
privacy loss of the proposed algorithms is minor.
The rest of this paper is organized as follows. The proposed Asymmetric DCOP model
is presented in Section 2 along with several alternatives for representing asymmetric constraints within the standard DCOP model. Section 3 introduces new complete search algorithms that are designated to efficiently and correctly solve asymmetric problems. Section 4
focuses on local search. It demonstrates the incompatibility of existing local search algorithms for solving asymmetric problems and proposes several novel asymmetric local search
algorithms. Section 5 includes an extensive experimental evaluation of both complete search
algorithms and local search algorithms. The results of the experimental evaluation are quite
conclusive. The paper is summarized and conclusions are drawn in Section 6.

2. Asymmetric Distributed Constraint Optimization
First, the standard DCOP model is presented followed by its proposed generalization that
captures asymmetric constraints. Next, some alternatives for representing asymmetric constraints by the standard DCOP model are described.
2.1 Distributed Constraint Optimization Problems (DCOPs)
A DCOP is a tuple hA, X , D, Ri. A is a finite set of agents A1 , A2 , ..., An . X is a finite
set of variables X1 , X2 , ..., Xm . Each variable is held by a single agent (an agent may hold
more than one variable). D is a set of domains D1 , D2 , ..., Dm . Each domain Di contains a
finite set of values which can be assigned to variable Xi . R is a set of relations (constraints).
Each constraint C  R defines a non-negative cost for every possible value combination of
a set of variables, and is of the form:
C : Di1  Di2  . . .  Dik  R+

(1)

A binary constraint refers to exactly two variables and is of the form Cij : Di  Dj  R+ . A
binary DCOP is a DCOP in which all constraints are binary. A value assignment is a pair
including a variable, and a value from that variables domain. A partial assignment (PA) is
a set of value assignments, in which each variable appears at most once. A full assignment
or a solution is a partial assignment that includes all the variables (vars(P A) = X ). An
optimal solution is a full assignment of aggregated minimal cost.
In maximization problems, for each constraint we have utilities instead of costs and a
solution is a full assignment of maximal aggregated utility. In the rest of this paper, unless
stated differently, the problems discussed are minimization problems.
2.2 Asymmetric DCOPs (ADCOPs)
ADCOPs generalize DCOPs in the following manner: instead of assuming equal payoffs
for constrained agents, the ADCOP constraint explicitly defines the exact payoff for each
participant. That is, domain values are mapped to a tuple of costs, one for each constrained
agent.
More formally, an ADCOP is defined by the following tuple hA, X , D, Ri, where A, X ,
and D are defined in exactly the same manner as in DCOPs. Each constraint C  R of an
616

fiAsymmetric Distributed Constraint Optimization Problems

A2
A1 \

x

y

a

3, 4

6, 1

b

7, 2

5, 8

x

1

x

2

Figure 1: A two agents interaction. The left-hand side presents the agents costs incurred by
the interaction  Agent A1 s possible value assignments are either a or b and the
costs they endure are depicted in the left value in each cell. Agent A2 s possible
value assignments are x or y (costs on the right side). The right-hand side of the
figure provides a graphical presentation the resulting ADCOP network.

asymmetric DCOP defines a set of non-negative costs for every possible value combination
of a set of variables, and takes the following form:
C : Di1  Di2     Dik  Rk+

(2)

This definition of an asymmetric constraint is natural for general MAS problems, and
requires little manipulation when formulating these as ADCOPs. It applies when each agent
holds exactly one variable among the k variables involved in the constraint C. We note that
when several variables involved in a constraint are held by the same agent, the length of the
vector of costs representing the constraint is equal to the number of agents involved and
not the number of variables.
Consider the simple example of two interacting agents presented in Figure 1. In this
problem each agent pays either the left- or right-hand side of the values depicted in the
bi-matrix of Figure 1. Agent A1 controls variable x1 and may assign either a or b to it and
Agent A2 controls variable x2 and may assign the values x or y. The constraint between
the two interacting agents maps assignment pairs to cost pairs. For example, if agent A1
assigns the value b and agent A2 assigns the value x, A1 s cost will be 7 and A2 s cost will
be 2. We often refer to the cost pairs as two sides of a constraint. The right-hand side of
the figure illustrates the ADCOP formulation of this problem in terms of agents and their
variables (where the single constraint is presented in full in the bi-matrix on the left).
It should be noted that although the above ADCOP model bears great resemblance
to the graphical games model (Kearns, Littman, & Singh, 2001) these two models are
fundamentally different. While game-theoretic agents are self-interested entities, ADCOP
agents are cooperative by nature and always follow the protocol (algorithm)  even at the
risk of personal degradation of gain.
2.3 Alternatives for Representing Asymmetric Constraints
Extending the Distributed Constraint Reasoning (DCR) (Meisels, 2007) paradigm to encompass asymmetric payoffs is of great interest when considering real world problems. Typical
problems must often take into account the individual state of each agent (remaining battery life, user preferences, etc) which rarely coincide. Several alternatives for representing
asymmetric constraints by DCOPs were discussed in former papers.
617

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

2.3.1 Disclosure of Constraint Payoffs
The simplest way to solve MAS problems with asymmetric payoffs by DCOPs is through
the disclosure of constraint payoffs. That is, aggregate values of all agents taking a joint action. However, constraint disclosure reveals private information (preferences) (Greenstadt,
Pearce, & Tambe, 2006; Maheswaran, Pearce, Varakantham, Bowring, & Tambe, 2005;
Yokoo, K.Suzuki, & Hirayama, 2002) and many times needs to be avoided.
2.3.2 Use of Unary Constraints
A possible technique for representing preferences and affecting different payoffs is through
the introduction of unary constraints. Constraints are added to each variable participating
in a constraint and the additional costs generate asymmetry.
Proposition 1 There exists an asymmetric DCOP that cannot be expressed by a symmetric
DCOP with the addition of unary constraints of the form UAi () = .
Proof: Consider for example the interaction depicted in Figure 1. The most general solution
is to add an unary constraint for each variable held by the agents. The cost of the binary
constraint, B(, ), and unary constraint of each variable UA (), UB () must be consistent
with the cost incurred on each agent, and can be described by the following set of linear
equations:
UA1 (a) + B(a, x) = 3
UA1 (a) + B(a, y) = 6
..
.
UA2 (y) + B(b, y) = 8
This set of equations that is derived from the problem of Figure 1 has no solution. 

Corollary 2 Symmetric DCOPs with the addition of unary constraints are strictly less
expressive than asymmetric DCOPs.
Proof: It remains to show that every problem with unary and symmetric binary constraints can be represented as an asymmetric DCOP. Unary constraints are an integral part
of the asymmetric DCOP model, while symmetric binary constraints are inherently less expressive than asymmetric ones. Consequently, every symmetric DCOP (with added unary
constraints) can be represented as an asymmetric DCOP. 
The unary constraints approach is thus shown to be insufficient for representing asymmetric constraints. More precisely, this approach fails to properly capture cases in which the
personal valuation of a state by an agent is dependent upon assignments by other agents.
618

fiAsymmetric Distributed Constraint Optimization Problems

A2
A1 \

hx, a i

hy, a i

hx, b i

hy, b i

ha, x i

7

54

55

111

ha, y  i

60

7

108

64

hb, x i

61

108

9

65

hb, y  i

109

56

57

13

x

1
1

x

1
2

x

2
2

x

2
1

Figure 2: The same interaction of Figure 1 formulated as a PEAV-DCOP. The left-hand
side presents the value of each possible end state. The right-hand side provides a
graphical representation of the resulting PEAV-DCOP network.

2.3.3 Private Events As Variables (PEAV)
The PEAV model (Maheswaran et al., 2004b) successfully captures asymmetric payoffs
within standard DCOPs. The incurred cost of the PEAV model on an agent involves one
mirror variable per each of its neighbors in the constraint network (i.e., for each constrained
variable held by another agent). Consistency with the neighbors state variables is imposed
by a set of hard equality constraints. One way to represent hard constraints is to assign a
cost that is calculated for each specific problem (Maheswaran et al., 2004b).
The resulting representation of an asymmetric MAS problem in a PEAV-DCOP is much
larger in terms of variables and constraints than an ADCOP. Figure 2 describes the same
interaction of Figure 1 formulated according to PEAV (note that this is a minimization
problem). In this example x11 and x22 are the original variables and x12 and x21 are the mirror
variables generated by the PEAV formulation. The upper bound value used as a hard
constraint in this example is 50.
The cost of each end state is depicted in the table on the left-hand side of the figure.
For example, the top-left cell represents the assignments x11 = a, x22 = x, and consistent
values of the mirror variables. Consequently, the cost (7) is exactly the sum of costs of both
agents in the original MAS problem (Figure 1).
The PEAV representation of this problem must accommodate the mirror variables, and
combinations of values previously not considered must also be taken into account. For
example, the top-right cell represents the assignments x11 = a, x22 = y, but here the mirror
variables are not consistent with the original variables (x12 = x, x21 = b). The cost (111)
includes the sum of costs of both agents in the original MAS problem, where each agent
considers the value of the mirror variable as the real value of the other agent (3 and 8). To
this value, two upper bound values (50 each) were added to express the inconsistency of
the mirror variables. Consequently, the PEAV matrix is 4  4, including 16 payoff values 
a quadratic increase in the size of the search space.

3. Asymmetric Distributed Complete Search
Although DCOPs are NP-hard, a large number of former DCOP research was dedicated to
complete search algorithms (Modi et al., 2005; Yeoh et al., 2010; Petcu & Faltings, 2006;
619

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

A2
A1

x

y

A3
A1

i

j

a

3, 4

6, 1

a

3, 2

4, 1

b

7, 2

5, 8

b

1, 1

3, 2

A1

A2

A3

Figure 3: Example problem.
Mailler & Lesser, 2004; Gershman et al., 2009). Unfortunately, most existing complete
DCOP algorithms cannot find the optimal solution when solving ADCOPs. We elaborate
on this issue at the beginning of this section and then introduce several novel complete
ADCOP algorithms.
3.1 Complete Search with Asymmetric Constraints
One may attempt to solve ADCOPs by simply using existing complete DCOP algorithms.
The following scenario demonstrates the shortcomings of such an attempt. Consider the
SyncBB algorithm (Hirayama & Yokoo, 1997). SyncBB was chosen for its simplicity, however, this demonstration is relevant for most existing DCOP algorithms. In SyncBB a partial
assignment is generated sequentially on a Current Partial Assignment (CPA) message (cf.,
Meisels, 2007). When an agent receives the CPA, it assigns its variable so that the overall
cost of the partial assignment is minimal. Running a Branch & Bound algorithm implies
that whenever the cost of a partial assignment becomes larger than the upper bound, the
agent which holds the CPA backtracks to the agent before it.
For ADCOPs the above standard process is not correct. The full cost of a partial
assignment must include the constraints held by all the agents which participate in it. In
other words, once an agent Ai assigns its variables it can only compute the cost of the CPA
based on its own evaluation, while other agents with higher priority (i.e., ordered before it)
are holding constraints between the agents newly assigned variable and their own. Consider
the example problem in Figure 3. When attempting to solve this problem using SyncBB,
only the parts of the constraints held by the lower priority agents in each constraint are
evaluated. Thus, the resulting solution is hA1 = a, A2 = y, A3 = ji with a cost of 2. The
constraints held by higher priority agents (A1 in this example) are not evaluated. The real
cost of this solution when considering both sides of the constraints is 12. The actual solution
is hA1 = b, A2 = x, A3 = ii with a cost of 11.
Following the above example, it is clear that most existing complete DCOP algorithms
are no longer correct when the ADCOP model is used. A complete ADCOP algorithm
must allow all agents participating in a constraint to evaluate the related assignments. An
exception may be the OptAPO algorithm (Mailler & Lesser, 2006), since it does not perform
distributed search in order to resolve conflicts (Grinshpoun & Meisels, 2008). Thus, when
620

fiAsymmetric Distributed Constraint Optimization Problems

(mediator) agents perform search they are able to generate a centralized (symmetric)
problem and solve it.
3.2 ADCOP Complete Search Algorithms
Solving an asymmetric problem requires that both parts of each binary constraint are
evaluated and aggregated in the assignments cost. Similarly to the DCSP case (Brito,
Meisels, Meseguer, & Zivan, 2009) considering both sides of the constraints can be generally
performed in two ways. One strategy is to solve the problem in two phases. In the first
phase, a full assignment is reached while considering only one side of each constraint 
this phase can be performed using a symmetric DCOP algorithm. In the second phase,
the full cost of the assignment is verified by checking the complementary part of all the
involved constraints. After the second phase is complete and new bounds were set, the first
phase is resumed in a search for a better solution until the entire search space is covered.
Another strategy is to systematically check both sides of the constraints before reaching
a full assignment, forming a one-phase strategy. The present paper refers to checking the
reversed-order part of the constraints as back-checking. Back-checking can be performed
either synchronously or asynchronously.
We next present several innovative complete algorithms designed for ADCOPs. We
begin by introducing two asymmetric versions for the SyncBB algorithm. The first version,
SyncABB-2ph, follows the two-phase strategy, while the other version follows the one-phase
strategy. We also present the Asynchronous Two-Way Bounding algorithm (ATWB), which
is an asymmetric version for the AFB algorithm (Gershman et al., 2009). ATWB also follows
the one-phase strategy and naturally performs asynchronous back-checking.
3.2.1 Synchronous Asymmetric Branch & Bound  2-phase (SyncABB-2ph)
The SyncABB-2ph algorithm is a combination of the SyncBB algorithm with the two-phase
strategy. In phase I the algorithm works exactly as SyncBB, where each agent counts
the costs of its constraints with lower-indexed agents. Phase I is completed when a full
assignment is reached. In standard (symmetric) SyncBB operating on a symmetric DCOP,
this means that a new best solution and a new bound has been found. For ADCOPs backchecking is needed in order to verify that the reversed order parts of the constraints do not
increment the cost beyond the bound.
Algorithm 1 SyncABB-2ph: phase II
when received hCPA BACK MSG, CPA, costi do
1: f  cost of constraints with higher-indexed agents (Ai+1 ...An )
2: if cost + f  B do
3:
send hCPA MSG, CPAi to An
4: else if Ai 6= A1 do
5:
send hCPA BACK MSG, CPA, cost + f i to Ai1
6: else
7:
B  cost + f
8:
broadcast hNEW SOLUTION, CPA, Bi
9:
send hCPA MSG, CPAi to An
621

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

A1

CPA

CPA

A1=b
A2=y
A3=j

A1=b
A2=y
A3=i

A2

A3

A1

A2

Cost=8

Cost=9

(a)

A1

(b)

CPA

CPA

A1=b
A2=y
A3=i

A1=b
A2=y
A3=i

A2

A3

A3

A1

Cost=9

A2

A3

Cost=15

(c)

(d)

Figure 4: SyncABB-2ph example  limited pruning.
In phase II the last agent (An ) sends to its preceding agent (An1 ) a CPA BACK MSG
message. This message includes the CPA and its one-side cost that was gathered in phase
I. Each agent that receives the CPA BACK MSG message (Algorithm 1) performs backchecking by computing the cost f of its constraints with upper-indexed agents (line 1). If
the addition of f to the cost of the CPA (the lower bound) reaches the bound B (line 2),
there is no point to continue the back-checking phase since a value assignment must be
replaced. To ensure the completeness of the algorithm, regardless of which agent identified
that the cost exceeded the global bound (the lowest cost for a full assignment found so far),
the CPA must be returned to the last agent An (line 3). If the global bound has not been
reached, the back-checking continues to the preceding agent (lines 4-5) until it reaches A1 .
When the total cost of the CPA is below the existing global bound, agent A1 updates the
bound B (line 7) and informs all the agents of the new best solution and new bound (line
8). Next, phase I is resumed by sending the CPA back to the last agent (line 9).
During the solving process parts of the search space are expected to be pruned. In
fact, the amount of pruned search space faithfully reflects the effectiveness of a Branch &
Bound algorithm. Lines 2 and 3 in Algorithm 1 state that when the global bound is reached
during the back-checking phase, the CPA is returned to the last agent. Returning the CPA
to the last agent means no pruning. Consequently, during the run of the SyncABB-2ph
algorithm pruning can only be achieved in the first phase. This limits the pruning that
can be performed by the algorithm since in the first phase only a subset of the constraints
are considered. This shortcoming of the algorithm is illustrated in Figure 4. When the
value of A2 is assigned (stage a), the CPA is hA1 = b, A2 = yi. A global look at the CPA
(that considers both sides of the constraint) reveals that its cost is 13, which is higher than
the current bound (11 at this stage of the search). Nevertheless, at the first phase of the
algorithm only one side of the constraint is evaluated, resulting in a cost of 8. Thus, the
622

fiAsymmetric Distributed Constraint Optimization Problems

CPA advances and agent A3 also assigns a value (stage b). The bound is only passed during
the second phase (stage d), and so the CPA is returned back to agent A3 , which in turn
will assign the value A3 = j.
The demonstrated pruning problem may very well result in poor performance of the
SyncABB-2ph algorithm. Indeed, our experimental evaluation supported this conjecture,
and so the two-phase approach was abandoned.
3.2.2 Synchronous Asymmetric Branch & Bound  1-phase (SyncABB)
The SyncABB algorithm is a combination of the SyncBB algorithm with the one-phase
strategy. After each step of the algorithm, when an agent adds an assignment to the CPA
and updates the cost with one side of the constraint, the CPA is sent back to the agents that
have already assigned their variables to update the lower bound by adding the costs of all
backwards directed constraints (back-checking). This is done by replacing the CPA MSG
message sent after each value assignment to the next agent (as in SyncBB and SyncABB2ph) with a CPA BACK MSG message to the preceding agent.
Algorithm 2 SyncABB: back-checking
when received hCPA BACK MSG, CPA, costi do
1: j  CP A.lastId
2: f  cost of constraint with agent Aj
3: if cost + f  B do
4:
send hCPA MSG, CPAi to Aj
5: else if Ai 6= A1 do
6:
send hCPA BACK MSG, CPA, cost + f i to Ai1
7: else if Aj = An do
8:
B  cost + f
9:
broadcast hNEW SOLUTION, CPA, Bi
10:
send hCPA MSG, CPAi to An
11: else
12:
CP A.cost  cost + f
13:
send hCPA MSG, CPAi to Aj+1
The handling of a CPA BACK MSG in SyncABB is presented in Algorithm 2. First,
it is important to know the identity j of the initiator of the back-checking (in SyncABB-2ph
it was always n). Consequently, agent An (in SyncABB-2ph) is now replaced by Aj (lines
2,4). Additionally, when the back-checking is complete (reaches A1 ) and Aj is not the last
agent (line 11), the algorithm simply sends the CPA to the next assigning agent Aj+1 (line
13).
Figure 5 illustrates how the CPA is moved between the agents in the SyncABB algorithm. The given example shows the run of the algorithm at the beginning of the search
process for the problem depicted in Figure 3.
The main motivation for this one-phase version is that when the global bound is reached,
the CPA can be returned to the initiator of the back-checking (line 4), which in many cases
will not be An . This can lead to effective pruning of the search space in comparison to
the two-phase strategy. This behavior of the algorithm is illustrated in Figure 6. When
623

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

CPA

CPA

A1=a
A2=x
A3=i

A1=a
A2=x
A3=i

A1

A2

A1

A3

A2

A3

Cost=4

Cost=0

(b)

(a)
CPA

CPA

A1=a
A2=x
A3=i

A1=a
A2=x
A3=i

A1

A2

A3

A1

A2

Cost=7

Cost=9

(c)

A1

A3

(d)

CPA

CPA

A1=a
A2=x
A3=i

A1=a
A2=x
A3=i

A2

A3

A1

Cost=9

A2

A3

Cost=12

(e)

(f)
Figure 5: SyncABB-1ph example.

A1

CPA

CPA

A1=b
A2=y
A3=j

A1=b
A2=y
A3=j

A2

A3

A1

Cost=8

A2

A3

Cost=13

(a)

(b)

Figure 6: SyncABB-1ph example  effective pruning.

the value of A2 is assigned (stage a), the CPA includes hA1 = b, A2 = yi. After the backchecking is performed (stage b) the cost becomes 13 and so the bound (11 at this stage of
the search) is reached. The CPA is passed at that point back to agent A2 . Consequently,
some of the search space is pruned (complete assignments hA1 = b, A2 = y, A3 = ii and
hA1 = b, A2 = y, A3 = ji).
624

fiAsymmetric Distributed Constraint Optimization Problems

Proposition 3 SyncABB is sound and complete.
Proof: The soundness of SyncABB is immediate since a CPA is sent forward by an assigning agent only after the new assignment was evaluated by all agents whose assignment is
included in the CPA. In other words, a CPA is sent forward only after all costs generated
by the addition of the new value assignment were added to the lower bound on the CPA
(Algorithm 2, lines 12-13). This includes the complete assignment in a reported solution
(lines 8-10).
The completeness of SyncABB follows from the exhaustive search structure. Only partial assignments whose cost exceeds the global bound are not extended and therefore the
existence of a solution with a lower cost than the solution found by the algorithm is ruled
out.
Termination also follows from the exhaustive structure of the Branch & Bound algorithm in which no partial assignment can be explored twice. 

3.2.3 Asymmetric Two-Way Bounding (ATWB)
To achieve a larger degree of asynchronicity, one can build upon an existing and efficient
asynchronous DCOP algorithm, AFB (Gershman et al., 2009). AFB was found to perform faster than competing complete algorithms such as DPOP and ADOPT. In the AFB
algorithm agents assign a CPA sequentially as in SyncBB, but following each assignment
the assigning agent triggers asynchronous checks of bounds by sending copies of the CPA
via BOUND CPA messages to agents which have not yet assigned their variables. The
agents that receive copies of the CPA calculate the lower bound on the cost of the constraints they hold with the assignments on the CPA. The lower bound is sent back to the
assigning agent via an ESTIMATE message. The assigning agent aggregates the bounds
in the ESTIMATE messages it receives into an updated lower bound and if it exceeds the
current upper bound, the agent initiates a backtrack.
The AFB algorithm can be adjusted to accommodate both forward bounding and backward bounding and thus, can be adjusted for solving ADCOPs. In other words, instead of
sending each assigned CPA back to the first assigning agent sequentially as in SyncABB,
copies of the CPA can be sent backwards to agents whose assignments are included in the
CPA. Agents that receive a copy of the CPA compute their estimate and send it back
(forward) to the assigning agent just as in standard AFB. We refer to this version as the
Asynchronous Two-Way Bounding algorithm (ATWB).
The ATWB algorithm follows the pseudo-code of AFB (Gershman et al., 2009) with
several modifications. BOUND CPA messages are now sent both forward and backwards
whenever a value is assigned (procedure assign CPA). Additionally, the last agent An
cannot declare a new solution until it receives all the estimates from the backward bounding.
Thus, the handling of ESTIMATE messages must be revised and its new version is given
in Algorithm 3.
When an estimate message is received the agent checks whether the new estimate reaches
the global bound (line 2). If this is the case, then a new value is assigned by the current
agent (line 3). In case this is the last agent and all the backward estimates have arrived
(line 4), the agent can declare a new solution (lines 5-6) and assign a new value (line 7).
625

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Algorithm 3 ATWB  receive estimate
when received (ESTIMATE, estimate)
1: save estimate
2: if (CPA.cost + all saved estimates) B do
3:
assign CPA()
4: else if CPA is a full assignment and all estimates arrived do
5:
B  CPA.cost + all saved estimates
6:
broadcast (NEW SOLUTION, CPA, B)
7:
assign CPA()

In the case of forward bounding, an agent still does not know which assignment it will
select and therefore its estimate is a lower bound on the cost of the constraints considering
all values in its variables domain. In contrast, when an agent Ai receives a BOUND CPA
message from an agent Aj ordered after it (backward bounding), it can accurately compute
the cost of the constraints of its own assignment with all other assignments on the CPA.
A precomputed h2 (v, j) function (per value v, per agent Aj which holds the current CPA)
that gives a lower bound on the cost of constraints with agents that are after Aj in the
order (Ak |k > j) can be added to the estimation computation. The h2 (v, j) function can
also be used in forward bounding as a lower bound for the back-checking of value v with all
the agents between Ai and Aj (Ak |j < k < i). The h2 function is additive, since it refers
to the back-checking of yet unassigned variables.
Proposition 4 ATWB is sound and complete.
Proof: The soundness of ATWB is established by the fact that a new solution is stored
and later reported only after all estimates arrive at the last agent (Algorithm 3, lines 4-6).
At that point, all constraints have been evaluated by all involved agents. Note that the
estimates received by the last agent include all (backward) constraints. The possibility that
some estimate was not yet received by some agent (due to delay of messages) does not
compromise the algorithms soundness. In case the delayed estimate would have triggered
a need to backtrack, the estimate sent by the same agent to the last agent would be at least
as high and therefore would trigger a backtrack as well.
Similarly to SyncABB, the completeness and termination of ATWB follow from the
exhaustive structure of the Branch & Bound algorithm. 

4. Asymmetric Distributed Local Search
Distributed local search techniques for solving DCOPs have gained popularity in recent
years. Although local search algorithms are inherently incomplete, i.e. they do not guarantee to report the optimal solution, they offer a practical solution for significantly larger
problems. Adding asymmetry to the problems makes a complete solving process even more
difficult, a fact that enhances the suitability of local search for solving ADCOPs.
We next present an overview of standard DCOP local search algorithms followed by a
discussion of their applicability to the asymmetric case. We then introduce several novel
local search algorithms designed for solving ADCOPs.
626

fiAsymmetric Distributed Constraint Optimization Problems

4.1 Local Search
The general design of local search algorithms for DCOPs is synchronous. In each step of
the algorithm an agent sends its assignment to all its neighbors in the constraint network
and receives the assignments of all its neighbors. We hereby present in detail two leading
algorithms that apply to this general framework  the Distributed Stochastic Algorithm
(DSA) (Zhang et al., 2005) and the Max Gain Message (MGM) algorithm (Maheswaran,
Pearce, & Tambe, 2004a).1
In the initial step of the DSA algorithm agents randomly pick some value assignment for
their variable. Next, agents perform a sequence of steps until some termination condition is
met. In each step, each agent sends its value assignment to its neighbors in the constraints
graph and receives the assignments of its neighbors. The present paper follows the general
definition of a DCOP which does not include a synchronization mechanism. If such a
mechanism exists, agents in DSA can send value messages only in steps in which they
change their value assignments. After collecting the assignments of all its neighbors, each
agent decides whether to keep its value assignment or to change it, by using a stochastic
strategy (see Zhang et al., 2005 for details on the possible strategies and the difference in
the resulting performance). A sketch of DSA is presented in Algorithm 4.
Algorithm 4 Standard DSA
1: value  ChooseRandomValue()
2: while no termination condition is met do
3:
send value to neighbors
4:
collect neighbors values
5:
if ReplacementDecision() do
6:
select and assign the next value
The MGM algorithm is a strapped down version of the DBA algorithm (Yokoo, 2000;
Zhang et al., 2005). In every synchronous step, each agent sends its current value assignment
to its neighbors and collects their current value assignments. After receiving the assignments
of all its neighbors, the agent computes the maximal improvement (i.e., reduction in cost)
to its local state that can be achieved by replacing its assignment and sends this proposed
reduction to its neighbors. After collecting the proposed reductions from its neighbors, each
agent changes its assignment only if its proposed reduction is greater than the reductions
proposed by all of its neighbors. In more advanced versions of MGM, agents group together
in order to propose a common improvement and thus avoid local minima to which a smaller
group would have converged. Algorithm 5 includes a sketch of the standard MGM algorithm.
After selecting a random value for its variable (line 1), the agent enters a loop where each
iteration is a step of the algorithm. After sending its value assignment to its neighbors and
collecting their value assignments (lines 3-4), the agent calculates its best weight reduction
and sends it to its neighbors (lines 5-6). After receiving the possible weight reductions of
all of its neighbors the agent decides whether to replace its assignment and upon a positive
decision reassigns its variable (lines 7-10).
1. Our description considers an improvement to be a decrease in the number of violated constraints (as in
Max-CSPs).

627

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Algorithm 5 Standard MGM
1: value  ChooseRandomValue()
2: while no termination condition is met do
3:
send value to neighbors
4:
collect neighbors values
5:
LR  BestPossibleLocalReduction()
6:
send LR to neighbors
7:
collect neighbors LRs
8:
if LR > 0 do
9:
if LR > LRs of neighbors (ties broken using indexes) do
10:
value  the value that gives LR

A different incomplete approach for solving DCOPs is implemented in the Max-Sum
algorithm (Farinelli, Rogers, Petcu, & Jennings, 2008). The Max-Sum algorithm operates
on a factor graph which is a bipartite graph in which the nodes represent variables and
constraints.2 Each node representing a variable of the original DCOP is connected to all
function-nodes that represent constraints which it is involved in. Similarly, a function-node
is connected to all variable-nodes that represent variables in the original DCOP which are
included in the constraint it represents. Agents in Max-Sum perform the roles of different
nodes in the factor graph. We will assume that each agent takes the role of the variablenodes which represent her own variables and for each function-node, one of the agents whos
variable is involved in the constraint it represents, performs its role. Variable-nodes and
function-nodes are considered as agents in Max-Sum, i.e., they can send messages, read
messages, and perform computation.
The content of messages sent by function-nodes is different than the content of messages
sent by variable-nodes. A message sent from a variable-node to a function-node includes
for each of its possible value assignments, the sum of costs/utilities for this value it received
from all other function neighbors. A message sent from a function-node to a variable-node
includes for each possible value assignment of the variable the best (minimal in a minimization problem, maximal in a maximization problem) cost/utility that can be achieved
from any combination of assignments to the variables involved in the function not including
costs/utilities reported by the destination variable. At the end of the run each variable
selects the value assignment that received the best sum of costs/utilities included in the
messages which were received most recently from its neighboring function-nodes.
4.2 Local Search with Asymmetric Constraints
Let us start the discussion of local search algorithms for ADCOPs by demonstrating the
shortcomings of existing local search methods.
Consider again the problem described in Figure 1. Assuming each agent is only aware of
the left (agent A1 ) or right (A2 ) value in the matrix, standard DCOP local search algorithms
such as DSA and MGM can be applied to this problem. In DSA, for example, agents only
2. We preserve the terminology of Farinelli et al. (2008) and call constraint representing nodes in the factor
graph function-nodes.

628

fiAsymmetric Distributed Constraint Optimization Problems

consider their personal gain, or improvement, and as a result change values according to
their local state. A similar situation exists with respect to MGM. However, the maximum
change that is reported by agents running MGM does not necessarily imply an improvement
to neighbors as well.
The asymmetric structure of constraints alters the algorithms behavior. For example,
while DSA and MGM converge to local optima on standard DCOPs, this is not true for
ADCOPs. In local search, agents continuously attempt to change their value assignment if
an improving value assignment exists. When no such value assignment is found by any of
the agents the state of the system as a whole is said to be stable. This state is not necessarily
a local optimum when asymmetric payoffs are considered. A change of an assignment by
an agent may increase its own local cost, but due to asymmetry this change can also result
in an overall lower cost to the system as a whole! On the other hand, such stable solutions
comply with the definition of Nash Equilibria (NE)  no unilateral change by any single
agent can improve its state. For similar reasons, MGM in ADCOPs looses its important
monotonicity property (Maheswaran et al., 2004a). Agents sending their maximal possible
improvement to the current state to their neighbors can actually consider a change that
would cause a deterioration of the state of their neighbors and of the global state.
Nash Equilibria do not necessarily coincide with the optimum of a global objective
function. In the well known example of the prisoners dilemma, when maximizing the gain
of participants, the globally worst solution is the only NE. It is important to note that
NEs do not exist in every asymmetric problem and even in the presence of a NE, it is
possible that neither DSA nor MGM will converge to it. Thus, the convergence prediction
for DCOPs made by Chapman et al. (2008) does not apply in the case of ADCOPs.
One may attempt to run existing local search algorithms on the derived PEAV-DCOP of
the asymmetric cost problem (e.g., Figure 2). However, the PEAV formulation significantly
reduces the usefulness of standard local search algorithms.
PEAV includes hard equality constraints for any pair of variables which are a variable
in the original DCOP and its duplication. Consider a global assignment to the problem
that does not violate any of these constraints. Any attempt of an agent to replace a value
assignment to one of its variables will result in a violation of a hard constraint. Thus, the
representation of any assignment for the original DCOP in the PEAV representation forms
a local optimum.
Another way to describe this phenomenon is by pointing out that the PEAV formulation
generates new local optima, and thus, implicitly, new NEs. The new local optima can be
easily observed on the main diagonal of the cost matrix of Figure 2. When considering
and analyzing the personal costs of each agent from this interaction one sees that these
correspond to NEs. This implies that the PEAV formulation of a given problem produces
new stable points (local optima)! In the case of the example in Figure 2, four new NEs are
generated where originally there were none.
The PEAV formulation does not solve the problems of any of the standard local search
algorithms. In the case of DSA, an agent only considers the current assignments of its
neighbors. In the case of MGM, every change to a variable that would generate an inequality
would not be considered as a maximal reduction.
629

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Algorithm 6 ACLS
1: value  ChooseRandomValue()
2: while no termination condition is met do
3:
send value to neighbors
4:
collect neighbors values
5:
IMP SET  LocalReductions()
6:
P V  RandomSelectProposedValue(IMP SET )
7:
send P V to neighbors
8:
collect neighbors P V s
9:
foreach neighbor An do
10:
send constraint cost with An s P V
11:
collect all constraint costs
12:
cost  SumOfAllConstraintsCosts()  C
13:
if cost < currentState do
14:
assign with probability p: value  P V

4.3 ADCOP Local Search Algorithms
The aforementioned shortcomings of standard local search algorithms on both the ADCOP
model and the PEAV formulation call for the development of new local search algorithms
specifically designed for ADCOPs. Such algorithms that attempt to incorporate information from the agents local neighborhood and utilize it to locate high quality solutions are
presented next.
4.3.1 Asymmetric Coordinated Local Search (ACLS)
The ACLS algorithm presented in Algorithm 6, attempts to combine information from each
agents surrounding in order to produce a global evaluation.
It proceeds in synchronous steps and continues running (after a random initial assignment) until a termination condition is met. At each step, an agent running ACLS begins
by sending its current assignment to its neighbors and collecting assignments from them
(lines 3-4). It then collects all assignments that can improve its local state (line 5). Based
on this improving set a proposed assignment P V is randomly picked according to the distribution of gains from each proposal (line 6). This proposal is sent to all neighbors and
the neighbors proposals are collected (lines 7,8). An agent receiving a proposal responds
with the value of its side of the constraint resulting from its current assignment and the
proposed assignment (lines 9-10). When all such impact messages arrive, the agent assesses
the potential gain or loss from the assignment (lines 11-12). ACLS agents use a special
coordination value, C, representing the amount of cooperation with their neighborhood.
That is, when this constant is zero, all impact messages are ignored and ACLS produces
results similar to those of DSA (albeit with a high overhead of network load and privacy
degradation). An agent running ACLS concludes each round by committing to a change
with probability p (lines 13-14). The use of the probability parameter p here is similar to
the use of p in DSA (Zhang et al., 2005). It prevents many concurrent changes in the same
neighborhood that may cause thrashing.
630

fiAsymmetric Distributed Constraint Optimization Problems

Algorithm 7 MCS-MGM
1: value  ChooseRandomValue()
2: while no termination condition is met do
3:
send value to neighbors
4:
collect neighbors values
5:
foreach neighbor An do
6:
  increase due to An s new value
7:
if  > An s last known LR do
8:
send constraint cost with An s new value
9:
change constraint cost with An s new value to 0
10:
collect neighbors constraint updates
11:
update constraint with each of the neighbors
12:
LR  BestPossibleLocalReduction()
13:
send LR to neighbors
14:
collect neighbors LRs
15:
if LR > 0 do
16:
if LR > LRs of neighbors (ties broken using indexes) do
17:
value  the value that gives LR
4.3.2 Minimal Constraint Sharing MGM (MCS-MGM)
Similarly to ACLS, the MCS-MGM algorithm presented in Algorithm 7 also attempts to
employ knowledge of its local neighborhood to achieve a better gain to its surroundings.
The MCS-MGM algorithm also proceeds in synchronous steps and terminates according
to a predefined condition. Each step consists of three different interaction phases. An
agent begins by exchanging assignments with its neighbors (lines 3-4). It then evaluates
the impact of its neighbors assignment change on its own local state. If the neighbors
assignment change degrades the current state by more than that neighbors last known best
local reduction, the constraint is passed on to the neighbor. That is, the agent sends to
its neighbor its side of the constraint with the neighbors new value, and assigns a cost of
zero instead (lines 5-9). The updated constraints are gathered and the local sub-problem
is slightly modified (lines 10-11). Using the new information, the agent seeks the best local
reduction and sends this information to its peers (lines 12-13). As in MGM, the agents
declaring the highest local reductions change their values (lines 14-17).
4.3.3 Guaranteed Convergence Asymmetric MGM (GCA-MGM)
A small adjustment to MCS-MGM can guarantee its convergence to local optima (note that
it converges to local optima and not to a NE). Line 7 is replaced by:
7:

if  > 0 do

We call the resulting algorithm Guaranteed Convergence Asymmetric MGM (GCAMGM). The guaranteed convergence comes with a price. GCA-MGM is expected to preserve
less privacy than MCS-MGM since it has a weaker condition for exchanging constraints
among agents (see Section 5).
631

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Proposition 5 GCA-MGM is guaranteed to converge to a local optimum in a finite number
of steps.
Proof: Assume that GCA-MGM does not converge. Consequently, agents are repeatedly
changing their value assignments. Each change that causes an increase for some agent triggers a constraint exchange and therefore the next time this assignment change is performed,
it will not cause an increase (i.e., cannot occur more than once). Thus, the number of increases in cost is bounded by the number of constraints, which is finite. After all possible
increments have caused an exchange of constraints, the convergence is guaranteed as for
standard MGM (Maheswaran et al., 2004b; Pearce & Tambe, 2007). 
Our experiments (Section 5) demonstrate that although MCS-MGM does not guarantee
convergence, both versions converge very rapidly. This rapid convergence has a strong
impact on privacy loss during the search process (Section 5.2).

5. Experimental Evaluation
The experimental evaluation is divided into two parts. In the first part, the performance
of the complete ADCOP algorithms that were presented in Section 3 is compared to the
performance of standard state-of-the-art complete DCOP algorithms when solving asymmetric problems that are represented by the PEAV formulation. Two standard measures
of performance for complete algorithms are used  runtime and network load.
In the second part of the experimental evaluation, the proposed ADCOP local search
algorithms are compared with state-of-the-art local search algorithms for solving DCOPs.
The focus of the evaluation of incomplete algorithms is on the quality of the solution they
produce in a given limited run time. As will be demonstrated by the experimental evaluation, the proposed ADCOP local search algorithms converge to a high quality solution, in
contrast to standard incomplete methods which do not converge.
The privacy loss incurred by running each of the ADCOP algorithms (both complete
and local search) is also of great interest. Following Greenstadt et al. (2006) and Brito et al.
(2009), the privacy loss is measured in terms of entropy.
Several domains of evaluation are used. The first domain, random asymmetric MaxDisCSP, is used to evaluate both complete and local search algorithms. Max-DisCSP is a
subclass of DCOP in which all constraint costs are equal to one (Modi et al., 2005). MaxCSPs are commonly used in experimental evaluations of constraint optimization problems
(COPs) (Larrosa & Schiex, 2004) and of Distributed COPs (Gershman et al., 2009). MaxDisCSPs are classified by the number of agents n (assuming each holds exactly one variable),
the size of the domains k, the probability of a constraint among any pair of variables p1 ,
and the probability for the occurrence of a violation (a non-zero cost) among two value
assignments p2 . In our formulation we consider the constraint tightness p2 as the average
fraction of forbidden value pairs, as viewed by each agent involved in a given constraint.
This implies that some pairs allowed by one of the involved agents are disallowed by the
other, and vice versa. As a result, the fraction of cost-inflicting pairs is greater than the p2
value. We refer to this fraction as p2ef f . Its expected value is:
p2ef f = 1  (1  p2 )2
632

(3)

fiAsymmetric Distributed Constraint Optimization Problems

The second evaluation domain is that of random graphical games (Kearns et al., 2001;
Nisan, Roughgarden, Tardos, & Vazirani, 2007; Maheswaran et al., 2004a). In these problems, each constraint between two agents represents a local randomly generated game. In
these local interactions, each constrained agent is assigned a cost for each joint action (value
assignment pair) of the two constrained agents. The goal of the agents is to reach a globally
minimal cost assignment. In the evaluation of ADCOP local search algorithms, general-form
graphical games, as well as scale-free networks, were used. More details on these domains
are given in Section 5.2.
In order to evaluate the runtime performance of the algorithms, we measure NonConcurrent Logical Operations (NCLOs) (cf., Netzer, Grubshtein, & Meisels, 2012). This
measure, which is based on the notion of atomic operations (Gershman, Zivan, Grinshpoun, Grubshtein, & Meisels, 2008), is a generalization of NCCCs (Zivan & Meisels, 2006).
NCLOs enable the comparison of runtime performance with algorithms that their basic
operation is not necessarily a Constraint Check, such as ODPOP (Petcu & Faltings, 2006).
5.1 Evaluation of ADCOP Complete Search Algorithms
The evaluation of the ADCOP complete search algorithms consists of three experiment
settings  two settings of random asymmetric Max-DisCSPs, followed by a graphical games
setting. 50 random instances were generated for each set of experiments and the results
were averaged over these 50 instances.
In the first set of experiments, random asymmetric Max-DisCSPs with 10 agents (n =
10), 10 values (k = 10), constraint density p1 = 0.4, and varying constraint tightness
0.1  p2  0.9 were generated. Figures 7 and 8 present the runtime and network load
results of running the proposed ADCOP algorithms. Figure 7 presents the results for
the mean number of NCLOs. In these algorithms, we measure Constraint Checks, i.e.,
NCCCs. The results show that SyncABB outperforms ATWB for most of the problems.
This suggests that asynchronicity actually impairs performance in ADCOPs, in contrast
to the symmetric case of DCOPs. Verifying the bound by sending the CPA backwards
sequentially before resuming the search is more efficient than continuing with the search
while bounds are checked asynchronously. As problems become tighter, the effectiveness of
two-way bounding increases. For very tight problems (p2 = 0.9) the performance of ATWB
is very close to that of SyncABB.
The results for the total number of sent messages are presented in Figure 8. As expected
the network load of ATWB is higher than that of SyncABB due to the overhead of the added
BOUND CPA and ESTIMATE messages.
In the second set of experiments, asymmetric Max-DisCSPs with 6 agents (n = 6), 6
values (k = 6), constraint density p1 = 0.5, and varying constraint tightness 0.1  p2  0.9
were randomly generated. This value of p1 (0.5) was chosen to ensure that most of the
generated constraint graphs are connected, which is important for the faithful evaluation
of algorithms that employ a pseudo-tree (BnB-ADOPT and ODPOP). Applying the PEAV
formulation, an equivalent set of symmetric problems was also generated, enabling the
comparison of performance to existing DCOP algorithms. The symmetric formulation was
much larger in terms of variables and the number of constraints than the corresponding
ADCOPs forcing the use of relatively small problems.
633

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

10,000,000,000

SyncABB

1,000,000,000

ATWB

NCLOs

100,000,000
10,000,000
1,000,000
100,000
10,000
1,000
100
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Problem tightness (P2)

Figure 7: Mean NCLOs of complete algorithms  asymmetric Max-DisCSPs (10 agents).
10,000,000,000

SyncABB

Sent messages

1,000,000,000

ATWB

100,000,000
10,000,000
1,000,000
100,000
10,000
1,000
100
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Problem tightness (P2)

Figure 8: Sent messages in complete algorithms  asymmetric Max-DisCSPs (10 agents).
In this setup, the runtime and network load of six algorithms were compared. These
included the proposed ADCOP algorithms SyncABB and ATWB, as well as several state-ofthe-art DCOP algorithms (solving the symmetric problems)  SyncBB (Hirayama & Yokoo,
1997), AFB (Gershman et al., 2009), BnB-ADOPT (Yeoh et al., 2010), and ODPOP (Petcu
& Faltings, 2006).
Figures 9 and 10 present the results of the second setup. Unlike the other algorithms,
the main computational operation in ODPOP is the comparison of combinations of assignments sent to each computing agent by its offspring in the pseudo tree (Petcu & Faltings,
2006). Figure 9 presents the runtime in terms of NCLOs of all six algorithms. The ADCOP algorithms show lower runtime by several orders of magnitude compared to SyncBB,
AFB, and ODPOP. ODPOP ran out of heap memory (heap set to 2GB) in relatively tight
problems (p2  0.5). This is not surprising, since the memory that ODPOP requires is
exponential in the induced width of the constraint graph (Petcu & Faltings, 2006).
The PEAV formulation results in very sparse problems. The PEAV representation of
an asymmetric problem with 10 variables and p1 = 0.4 includes 46 variables and its density
is p1 = 0.07. Consequently, BnB-ADOPT, which is very efficient in solving sparse problems, does well and displays runtime performance comparable to the ADCOP algorithms.
634

fiAsymmetric Distributed Constraint Optimization Problems

1,000,000,000

SyncABB

100,000,000
ATWB

NCLOs

10,000,000

SyncBB
(PEAV)
AFB
(PEAV)
BnB-ADOPT
(PEAV)
ODPOP
(PEAV)

1,000,000
100,000
10,000
1,000
100
10
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Problem tightness (P2)

Figure 9: Mean NCLOs of complete algorithms  asymmetric Max-DisCSPs (6 agents).
1,000,000,000

SyncABB

Sent messages

100,000,000

ATWB

10,000,000
SyncBB
(PEAV)
AFB
(PEAV)
BnB-ADOPT
(PEAV)
ODPOP
(PEAV)

1,000,000
100,000
10,000
1,000
100
10
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Problem tightness (P2)

Figure 10: Sent messages in complete algorithms  asymmetric Max-DisCSPs (6 agents).
However, as demonstrated in Figure 10, it incurs a high network load. In fact, none of
the algorithms solving symmetric DCOPs (PEAV) managed to complete the first set of
experiments (with 10 agents) due to their high runtime and/or network load.
In the third set of experiments, graphical games with 6 agents (n = 6), 6 values (k = 6),
and varying node degree were randomly generated. The average node degrees used spanned
from 2.5 to 5 (which indicates a complete graph). Each constraint matrix included 50%
zeroes, while the rest of the assignment pair values had random values in the range [0..9].
This setting is particulary interesting as it shows how varying density (node degree) effects
the performamce of the algorithms.
Figures 11 and 12 present the results of the third setup in linear scale. The results
indicate that increased density (node degree) has minor effect on the performance of ADCOP
algorithms. On the other hand, as the density of the problems increases, the performance
of BnB-ADOPT is drastically impaired. As suspected, the number of sent messages is
especially high in BnB-ADOPT. The other DCOP algorithms (using the PEAV formulation)
were not able to complete the graphical games set of experiments.
The average privacy loss incurred by running each of the complete ADCOP algorithms
in the first problem setup is presented in Figure 13. Following Brito et al. (2009), the
635

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

NCLOs

500000

SyncABB

400000

ATWB

300000

BnB-ADOPT
(PEAV)

200000

100000
0
2.5

3

3.5

4

4.5

5

Node degree

Figure 11: Mean NCLOs of complete algorithms  graphical games.

Sent messages

1000000

SyncABB

800000

ATWB

600000

BnB-ADOPT
(PEAV)

400000
200000
0

2.5

3

3.5

4

4.5

5

Node degree

Figure 12: Sent messages in complete algorithms  graphical games.
100%

Privacy loss

SyncABB
ATWB

80%

60%
40%
20%
0%

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Problem tightness (P2)
Figure 13: Privacy loss of complete algorithms  asymmetric Max-DisCSPs (10 agents).
privacy results were calculated by comparing the percentage of lost entropy at the end of
the computation to the initial entropy. The results clearly indicate that ATWB has a higher
636

fiAsymmetric Distributed Constraint Optimization Problems

10,000,000,000

SyncABB

1,000,000,000

ATWB

NCLOs

100,000,000

SyncBB

10,000,000

AFB

1,000,000
100,000

10,000
1,000

100
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Problem tightness (P2)

Figure 14: Mean NCLOs of complete algorithms  asymmetric Max-DisCSPs (10 agents).
10,000,000,000

SyncABB

Sent messages

1,000,000,000

ATWB

100,000,000

SyncBB

10,000,000

AFB

1,000,000
100,000
10,000

1,000
100
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Problem tightness (P2)

Figure 15: Sent messages in complete algorithms  asymmetric Max-DisCSPs (10 agents).
degree of average privacy preservation when compared to SyncABB. This is not surprising
when one considers that in ATWB information is revealed only in a single direction, while
SyncABB decreases entropy by agents in both lower and higher priority.
An additional method for solving problems with asymmetric constraints is to aggregate
both sides of all constraints and simply run a symmetric DCOP algorithm (Section 2.3.1).
The clear downside of this alternative is that all private constraint information is disclosed
a-priory. It is interesting to investigate how the attempt to keep constraint information
private (by using ADCOPs) affects performance. The following figures present experiments
in which the performance of ADCOP algorithms, SyncABB and ATWB, is compared to the
performance of their symmetric variants when using constraints disclosure. In Figures 14
and 15 the four algorithms were compared when running on the first problem setup. In the
comparison between SyncABB and symmetric SyncBB one can observe that SyncABB performs only about 30%-40% more NCLOs than SyncBB, while the network load of SyncABB
is just bellow one order of magnitude higher than that of SyncBB. These results indicate
that the impact on performance when using ADCOPs for preserving portions of private
information is reasonable. Different outcomes follow the comparison between ATWB and
symmetric AFB, as the number of NCLOs in ATWB is almost two orders of magnitude
637

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

120,000
SyncABB

NCLOs

100,000

ATWB

80,000

SyncBB

60,000

AFB

40,000
20,000
0
2.5

3

3.5

4

4.5

5

Node degree

Figure 16: Mean NCLOs of complete algorithms  graphical games.
50,000

Sent messages

SyncABB
40,000

ATWB
SyncBB

30,000

AFB
20,000
10,000
0
2.5

3

3.5

4

4.5

5

Node degree

Figure 17: Sent messages in complete algorithms  graphical games.
larger than that of AFB in tight problems. The gap is even greater when considering
network load.
In the graphical games setup, where simple branch and bound is more effective (due to
the variance in costs), the performance impairment of the proposed ADCOP algorithms is
even further refined. Figure 16 shows that symmetric SyncBB outperforms SyncABB in
terms of runtime by just 25% in the higher density problems. The gap between ATWB and
symmetric AFB is also relatively low, as ATWB is about 5 times faster than AFB. Moreover,
the variance in costs renders that SyncABB performs less NCLOs than symmetric AFB in
the higher density problems. The gaps in network load are slightly larger, as is shown in
Figure 17.
While ADCOP algorithms prevent a-priori loss of all private information, the results
given in Figure 13 reveal that for relatively hard problems, a major part of the private
information is revealed after all. In an additional experiment, the extent of privacy loss
was limited by stopping the search process whenever the amount of private information
that some agent gained has passed a predefined threshold. Figures 18 and 19 present the
outcomes of this limitation in terms of solution quality (distance in cost from the optimal
solution) for problems from the first setup with p2 = 0.3 and p2 = 0.5, respectively. These
638

fiAsymmetric Distributed Constraint Optimization Problems

6

SyncABB

Solution cost

5

ATWB

4
3
2
1
0
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%

Maximal gain of private data

Solution cost

Figure 18: Solution quality when limiting the maximal gain of private information in complete algorithms  asymmetric Max-DisCSPs (10 agents, p1 = 0.4, p2 = 0.3).
10
9
8
7
6
5
4
3
2
1
0

SyncABB
ATWB

10% 20% 30% 40% 50% 60% 70% 80% 90% 100%

Maximal gain of private data
Figure 19: Solution quality when limiting the maximal gain of private information in complete algorithms  asymmetric Max-DisCSPs (10 agents, p1 = 0.4, p2 = 0.5).

specific p2 values were chosen, since p2 = 0.3 represents rather tight but still satisfiable
problems, while p2 = 0.5 represents harder problems that are mostly unsatisfiable. For
p2 = 0.3 the solution cost drastically reduces as the threshold is raised and with a maximal
private information gain of 60% the algorithms reach the optimal solution. The average
privacy loss in this case is considerably lower and is around 10% in both algorithms (see
Figure 13). For p2 = 0.5 some agent usually gains all the private information in order to
reach the optimal solution, but as can be seen in Figure 13, the average privacy loss of all
agents is much lower.
It is interesting that for most threshold values the maximal gain of private information
in ATWB is higher than in SyncABB, while it was shown that the average privacy loss of
639

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

ATWB is lower (Figure 13). The reason for this phenomenon is that in ATWB agents only
gain private information of their predecessors, so commonly the last agents in the order
gain more information, while the first agent gains none. The result is that on average an
agent running ATWB looses less privacy than an agent running SyncABB, but in SyncABB
the privacy loss is better distributed among the agents. This last experiment illustrates the
clear tradeoff between privacy loss and solution quality when running ADCOP algorithms
with a privacy gain threshold.
5.2 Evaluation of ADCOP Local Search Algorithms
The introduced local search algorithms are evaluated over three domains  asymmetric MaxDisCSPs, general-form graphical games, and scale-free networks. For each type of problem,
500 different problem instances with 200 agents and a domain size of 10 values per agent
were generated. The presented results are the average over the 500 solutions obtained by the
algorithms for these instances. Asymmetric Max-DisCSPs were generated with an average
of 10 neighbors per agent (density parameter p1 = 0.05) and tightness parameter p2 = 0.7.
The rest of the details are as described above for the asymmetric Max-DisCSPs generated to
evaluate complete algorithms. In the general-form graphical games agents had an average of
5 neighbors each, i.e., agents were connected by an asymmetric constraint with a probability
value p = 0.025 (Erdos-Renyi graphs) (Erdos & Renyi, 1960). In the scale-free networks
domain, graphs were constructed by following the Barabasi-Albert model (Jackson, 2008).
In the two latter setups the constraint costs were selected in the range [0..100]. Costs were
separately selected in a non uniform manner  the cost for an agent was 0 with probability
0.35 and uniformly selected in the range [1..100] with probability 0.65. This structure
ensures that an improving assignment change to one agent can increase the cost incurred
on its neighbors (cf. Equation 3).
The scale-free networks were built using the Barabasi-Albert model. An initial set of
10 agents was randomly selected and connected. At each iteration of the Barabasi-Albert
procedure an agent was added and connected to 4 other agents with a probability that is
proportional to the number of links that the existing agents already have.
A large number of algorithms were examined. These include DSA, MGM, MGM-2, MaxSum, ACLS, MCS-MGM, and GCA-MGM. These algorithms were executed for a maximum
of 200 cycles, where a cycle includes all actions between two consecutive value messages
sent by the same agent (Max-Sum cycles include both messages from function-nodes to
variable-nodes and vise versa).
Figure 20 presents the average solution quality for asymmetric Max-DisCSPs for each
of the algorithms as a function of cycles. MCS-MGM produced the highest quality results
and after 200 cycles the best algorithms were MCS-MGM, MGM-2, ACLS, and GCAMGM, where the performance of the two latter algorithms was similar. All three ADCOP
algorithms demonstrated very fast convergence. The highest costs (e.g., worst solutions)
were reported by Max-Sum, which did not explore much of the search space and its reported
incurred cost was significantly higher than that reported by other algorithms. Consequently,
the results of Max-Sum were left out of the plots to enable a better view of the overall
performance of the remaining algorithms (and a correct scale). Surprisingly, DSA produced
solutions of significantly lower quality than MGM. This is in contrast to its performance
640

fiAsymmetric Distributed Constraint Optimization Problems

Solution cost

180

MCS-MGM

160

GCA-MGM

140

ACLS
MGM2

120
MGM

100

DSA

80
0

50

100

150

200

Cycles

Figure 20: Solution quality of local search algorithms  asymmetric Max-DisCSPs
12000
MCS-MGM

Solution cost

10000

GCA-MGM

8000

ACLS

6000

MGM2
MGM

4000

DSA

2000
0

50

100

150

200

Cycles

Figure 21: Solution quality of local search algorithms  Erdos-Renyi graphs
when solving symmetric problems, where DSA is known to produce higher quality solutions
than MGM.
Figures 21 and 22 present similar results for the average solution quality in graphical
games and scale-free networks, respectively. It is notable that MCS-MGM dominates in all
problem scenarios. GCA-MGM produces better results than MGM-2 in graphical games,
while their results are of similar quality in scale-free networks. DSA and Max-Sum produced
low quality results in these problem scenarios as well.
It is worth noting that in MGM-2, an agent optimizing for itself and another agent
can cause an increase in the valuation of the proposed alternative state for neighboring
agents of both. As a result, agents optimizing for different pairs can generate loops of
assignment changes just as described for MGM. Thus, the increase in the size of the group
of agents considered by the optimizing agent is not sufficient to ensure convergence. A
similar phenomenon, where MGM-2 can eventually fail to provide higher quality solutions
than even MGM, was reported in the presence of uncertainty (Taylor, Jain, Tandon, &
Tambe, 2009).
In the problem scenarios evaluated, ACLS produced results of lower quality than MCSMGM and GCA-MGM (except for the similar results in asymmetric Max-DisCSPs). How641

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

1200

MCS-MGM

Solution cost

1000

GCA-MGM

800

ACLS

600

MGM2

400

MGM
DSA

200
0

50

100

150

200

Cycles

Figure 22: Solution quality of local search algorithms  scale-free networks

Privacy loss

100%
80%

MCS-MGM

60%

GCA-MGM

40%

ACLS

20%

MGM2

0%
0

50

100

150

200

Cycles

Figure 23: Privacy loss of local search algorithms  asymmetric Max-DisCSPs
ever, it is also observable that ACLS is the fastest algorithm to converge in all three problem
scenarios.
The above results indicate that the cooperation inherent to the three proposed algorithms, ACLS, MCS-MGM, and GCA-MGM, renders these algorithms better suite the
asymmetric case than most other algorithms. However, despite the lower costs attributed
with their cooperation, the nature of the agents requires some revelation of private information. Thus, it is important to asses the privacy loss resulting from the coordination of
agents, in contrast to standard local search (1-opt) algorithms which preserve a high level of
privacy (Greenstadt, 2008). To measure the overall loss of privacy in our system of agents,
one needs to aggregate the number of revealed constraint parts by each agent (Greenstadt
et al., 2006; Greenstadt, 2008).
In ACLS, a fraction of the constraint is revealed in line 10 (Algorithm 6), while MCSMGM and GCA-MGM reveal constraint information in lines 8 and 9 (Algorithm 7). Another
algorithm that attempts to coordinate joint moves is MGM-2 (Maheswaran et al., 2004a),
in which offerer agents propose several improving assignments along with their costs to
one of their peers, which respond with the lowest improving cost incurred on them. Thus,
MGM-2 agents reveal a much larger fraction of the constraint in every interaction.
642

fiAsymmetric Distributed Constraint Optimization Problems

100%
MCS-MGM

Privacy loss

80%

GCA-MGM

60%

ACLS

40%

MGM2

20%
0%
0

50

100

150

200

Cycles

Figure 24: Privacy loss of local search algorithms  Erdos-Renyi graphs

Privacy loss

100%
80%

MCS-MGM

60%

GCA-MGM

ACLS

40%

MGM2

20%
0%

0

50

100

150

200

Cycles

Figure 25: Privacy loss of local search algorithms  scale-free networks
Figures 23, 24, and 25 present the privacy loss measurements. Agents running MGM-2
reveal most of their problem structure, while the other algorithms maintain a substantially
higher degree of constraint privacy. In all three problem scenarios it is apparent that the
privacy loss of the proposed ADCOP algorithms is negligible compared to the privacy loss
of MGM-2. Furthermore, the privacy of MGM-2 decreases throughout the 200 cycles of the
algorithm (although the privacy loss function has a notable concave structure), while ACLS,
MCS-MGM, and GCA-MGM lose a very small amount of privacy in the first few iterations,
and do not endure additional privacy loss. These results indicate that quick convergence to
a solution may have a substantial impact on privacy loss.

6. Conclusions
Many problems that are distributed by nature include agents which have different valuations of the possible states of the world. For distributed constraint optimization problems
the constrained agents may have different costs assigned to valued constraints. The present
paper proposes the Asymmetric Distributed Constraint Optimization Problems (ADCOP)
model, which captures the inherent asymmetry of distributed problems in a natural and ef643

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

ficient way. The proposed ADCOP model represents private gains without revealing private
information a-priori. Instead, agents reveal only the information which is necessary during
the distributed search for a solution. This is in contrast to alternative DCOP formulations
that either centralize constraints (resulting in privacy loss and possibly heavy network load),
or change the problem into a more complex structure (PEAV).
The algorithmic impact of introducing the new framework was discussed, as well as the
applicability of existing DCOP algorithms. Several novel algorithms were proposed  some
for complete search and others for local search.
When considering complete search, the proposed complete ADCOP algorithms eliminate the need to extend the problem by using the PEAV model. Furthermore, they behave
well for the whole range of problem difficulty. Two of the complete ADCOP algorithms
showed superior performance (runtime as well as network load) when compared to the leading (symmetric) DCOP algorithms that use the PEAV representation. Another alternative
is to aggregate both sides of all constraints and simply run a symmetric DCOP algorithm.
However, this method leads to an a-priori disclosure of all private constraint information,
while only achieving moderately better run-time performance than the respective ADCOP
algorithms. The results indicate that the synchronous algorithm (SyncABB) outperforms
the asynchronous algorithm (ATWB) in most cases. SyncABB usually performs less NCLOs, sends significantly less messages, and leads to a better distribution of privacy loss
between the agents. In contrast, the average privacy loss in ATWB is lower.
In the proposed ADCOP local search algorithms the agents cooperate and perform
search in their local neighborhood, instead of maximizing their own gain. A proof that
one of the algorithms, GCA-MGM, is guaranteed to converge to a local optimum, was
presented. The PEAV representation cannot be used in combination with existing local
search algorithms, since any assignment which does not violate hard constraints is a local
optimum that PEAV generates. However, existing local search algorithms can be used
in combination with the proposed ADCOP model. Nevertheless, an empirical evaluation
demonstrated that the new ADCOP algorithms consistently find higher quality solutions,
and do so with a high degree of privacy preservation. It turns out that their fast convergence
strongly limits the amount of privacy lost.

References
Brito, I., Meisels, A., Meseguer, P., & Zivan, R. (2009). Distributed constraint satisfaction
with partially known constraints. Constraints, 14 (2), 199234.
Burke, D. A., Brown, K. N., Dogru, M., & Lowe, B. (2007). Supply chain coordination
through distributed constraint optimization. In Proc. CP Workshop on Distributed
Constraint Reasoning (DCR-07).
Chapman, A. C., Rogers, A., & Jennings, N. R. (2008). A parameterisation of algorithms for
distributed constraint optimisation via potential games. In Proc. AAMAS Workshop
on Distributed Constraint Reasoning (DCR-08).
Erdos, P., & Renyi, A. (1960). On the evolution of random graphs. In Publication of the
Mathematical Institute of the Hungarian Academy of Sciences, pp. 1761.
644

fiAsymmetric Distributed Constraint Optimization Problems

Farinelli, A., Rogers, A., Petcu, A., & Jennings, N. R. (2008). Decentralised coordination
of low-power embedded devices using the max-sum algorithm. In Proc. AAMAS-08,
pp. 639646.
Gershman, A., Grubshtein, A., Meisels, A., Rokach, L., & Zivan, R. (2008). Scheduling
meetings by agents. In Proc. PATAT-08.
Gershman, A., Meisels, A., & Zivan, R. (2009). Asynchronous forward bounding. Journal
of Artificial Intelligence Research (JAIR), 34, 2546.
Gershman, A., Zivan, R., Grinshpoun, T., Grubshtein, A., & Meisels, A. (2008). Measuring distributed constraint optimization algorithms. In Proc. AAMAS Workshop on
Distributed Constraint Reasoning (DCR-08).
Greenstadt, R. (2008). An analysis of privacy loss in k-optimal algorithms. In Proc. AAMAS
Workshop on Distributed Constraint Reasoning (DCR-08).
Greenstadt, R., Pearce, J., & Tambe, M. (2006). Analysis of privacy loss in distributed
constraint optimization. In Proc. AAAI-06, pp. 647653.
Grinshpoun, T., & Meisels, A. (2008). Completeness and performance of the APO algorithm.
Journal of Artificial Intelligence Research (JAIR), 33, 223258.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem. In
Proc. CP-97, pp. 222236.
Jackson, M. O. (2008). Social and Economic Networks. Princeton University Press.
Kearns, M. J., Littman, M. L., & Singh, S. P. (2001). Graphical models for game theory.
In Proc. UAI-01, pp. 253260.
Larrosa, J., & Schiex, T. (2004). Solving weighted CSP by maintaining arc consistency.
Artificial Intelligence, 159, 126.
Leaute, T., & Faltings, B. (2011). Distributed constraint optimization under stochastic
uncertainty. In Proc. AAAI-11, pp. 6873.
Maheswaran, R. T., Pearce, J. P., & Tambe, M. (2004a). Distributed algorithms for DCOP:
A graphical-game-based approach. In Proc. Parallel and Distributed Computing Systems (PDCS-04), pp. 432439.
Maheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004b).
Taking DCOP to the real world: Efficient complete solutions for distributed multievent scheduling. In Proc. AAMAS-04, pp. 310317.
Maheswaran, R. T., Pearce, J. P., & Tambe, M. (2006). A family of graphical-game-based
algorithms for distributed constraint optimization problems. In Coordination of LargeScale Multiagent Systems, pp. 127146. Springer-Verlag.
Maheswaran, R. T., Pearce, J. P., Varakantham, P., Bowring, E., & Tambe, M. (2005).
Valuations of possible states (VPS): a quantitative framework for analysis of privacy
loss among collaborative personal assistant agents. In Proc. AAMAS-05, pp. 1030
1037.
Mailler, R., & Lesser, V. R. (2004). Solving distributed constraint optimization problems
using cooperative mediation. In Proc. AAMAS-04, pp. 438445.
645

fiGrinshpoun, Grubshtein, Zivan, Netzer, & Meisels

Mailler, R., & Lesser, V. (2006). Asynchronous Partial Overlay: A New Algorithm for Solving Distributed Constraint Satisfaction Problems. Journal of Artificial Intelligence
Research (JAIR), 25, 529576.
Meisels, A. (2007). Distributed Search by Constrained Agents: Algorithms, Performance,
Communication. Springer Verlag.
Modi, J., & Veloso, M. (2004). Multiagent meeting scheduling with rescheduling. In Proc.
CP Workshop on Distributed Constraint Reasoning (DCR-04).
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). ADOPT: asynchronous distributed
constraints optimizationwith quality guarantees. Artificial Intelligence, 161, 149180.
Monderer, D., & Shapley, L. S. (1996). Potential games. Games and Economic Behavior,
14, 124143.
Netzer, A., Grubshtein, A., & Meisels, A. (2012). Concurrent forward bounding for distributed constraint optimization problems. Artificial Intelligence, 193, 186216.
Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. V. (2007). Algorithmic Game
Theory. Cambridge University Press.
Pearce, J. P., & Tambe, M. (2007). Quality guarantees on k-optimal solutions for distributed
constraint optimization problems. In Proc. IJCAI-07, pp. 14461451.
Petcu, A., & Faltings, B. (2006). ODPOP: An algorithm for open/distributed constraint
optimization. In Proc. AAAI-06, pp. 703708.
Petcu, A. (2007). A class of algorithms for distributed constraint optimization. Ph.D. thesis,
Ecole Polytechnique Fdrale de Lausanne (EPFL), Switzerland.
Ramchurn, S. D., Vytelingum, P., Rogers, A., & Jennings, N. (2011). Agent-based control
for decentralized demand side management in the smart grid. In Proc. AAMAS-11,
pp. 512.
Rogers, A., Farinelli, A., Stranders, R., & Jennings, N. R. (2011). Bounded approximate
decentralised coordination via the max-sum algorithm. Artificial Intelligence, 175 (2),
730759.
Taylor, M. E., Jain, M., Tandon, P., & Tambe, M. (2009). Using DCOPs to balance exploration and exploitation in time-critical domains. In Proc. IJCAI Workshop on
Distributed Constraint Reasoning (DCR-09).
Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: An asynchronous branch-andbound DCOP algorithm. Journal of Artificial Intelligence Research (JAIR), 38, 85
133.
Yokoo, M. (2000). Algorithms for distributed constraint satisfaction problems: A review.
Autonomous Agents and Multi-Agent Systems, 3 (2), 185207.
Yokoo, M., K.Suzuki, & Hirayama, K. (2002). Secure distributed constraints satisfaction:
Reaching agreement without revealing private information. In Proc. CP-02, pp. 387
401.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2005). Distributed stochastic search
and distributed breakout: properties, comparishon and applications to constraints
optimization problems in sensor networks. Artificial Intelligence, 161 (1-2), 5588.
646

fiAsymmetric Distributed Constraint Optimization Problems

Zivan, R. (2008). Anytime local search for distributed constraint optimization. In Proc.
AAAI-08, pp. 14491452.
Zivan, R., & Meisels, A. (2006). Message delay and DisCSP search algorithms. Annals of
Mathematics and Artificial Intelligence, 46 (4), 415439.
Zivan, R., Glinton, R., & Sycara, K. P. (2009). Distributed constraint optimization for large
teams of mobile sensing agents. In Proc. IAT-09, pp. 347354.

647

fiJournal of Artificial Intelligence Research 47 (2013) 809-851

Submitted 04/13; published 08/13

A Decidable Extension of SROIQ with Complex Role
Chains and Unions
Milenko Mosurovic

milenko@ac.me

Faculty of Natural Sciences and Mathematics,
University of Montenegro, Montenegro.

Nenad Krdzavac

nenadkr@tesla.rcub.bg.ac.rs

Faculty of Organizational Sciences,
University of Belgrade, Serbia.

Henson Graves

henson.graves@hotmail.com

Algos Associates, 2829 West Cantey Street,
Fort Worth, TX 76109 U.S.

Michael Zakharyaschev

michael@dcs.bbk.ac.uk

Department of Computer Science and Information Systems,
Birkbeck, University of London, U.K.

Abstract
We design a decidable extension of the description logic SROIQ underlying the Web
Ontology Language OWL 2. The new logic, called SR+ OIQ, supports a controlled use
of role axioms whose right-hand side may contain role chains or role unions. We give a
tableau algorithm for checking concept satisfiability with respect to SR+ OIQ ontologies
and prove its soundness, completeness and termination.

1. Introduction
The ever growing number and scope of application areas puts constant pressure on the
designers of ontology languages. Thus, the first version of the Web Ontology Language
OWL, which became a formal W3C recommendation in 2004, contained the description logic
(DL, for short) SHOIN that allowed the use of the basic DL ALC together with inverse
and transitive roles, role hierarchies, nominals and unqualified cardinality restrictions. Its
second reincarnation OWL 2, adopted in 2009, is based on a more powerful formalism,
SROIQ, which extends SHOIN with such features as complex role chains, asymmetric,
reflexive and disjoint roles, and qualified cardinality restrictions (Horrocks & Sattler, 2004;
Horrocks, Kutz, & Sattler, 2006; Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider,
& Sattler, 2008).
The addition of role inclusions that involve role chains was motivated by multiple use
cases in the life sciences domain which require means to describe interactions between
locative properties and various kinds of part-whole properties (Cuenca Grau et al., 2008).
For example, the role inclusion axiom
hasLocation  isPartOf v hasLocation
states that if an object x is located in y, and if y is part of z, then x is also located in
z (Rector, 2002). However, having resolved the issue of role chains in the left-hand side of
c
2013
AI Access Foundation. All rights reserved.

fiMosurovic, Krdzavac, Henson & Zakharyaschev

role inclusion axioms, as in the example above, SROIQ and OWL 2 fall short of providing
means to represent such chains and/or unions of roles on the right-hand side, which are often
required for modelling structured objects, in particular, in the emerging area of ontological
product modelling and collaborative design (Bock, Zha, Suh, & Lee, 2010).
Consider, for example, the product model of cars by Bock (2004) and Krdzavac and
Bock (2008), part of which is shown in the UML-like diagram below:
A
Car

B
Wheel

Engine

powers

powers
hasEngine
hasWheel

hasWheel
Wheel

Engine

Oilpump
hasEngine

C

Car
hasHub
Hub

hasOilpump

Crankshaft

powers

Gear
powers

powers

hasCrankshaft

hasGenerator

Generator

Hub

powers

Crankshaft
powers

Figure 1: A product model of a car (Krdzavac & Bock, 2008).
The fragment in Fig. 1 (A) involves two statements:
hasEngine  hasCrankshaft  powers v hasWheel  hasHub

(1)

says that whatever is powered by a crankshaft in an engine of a car is a hub in a wheel of
the same car and, conversely,
hasWheel  hasHub v hasEngine  hasCrankshaft  powers

(2)

states that a hub in a wheel of a car is powered by a crankshaft in an engine of that car.
The fragment in Fig. 1 (B) means that an engine in a car can power wheels, the generator
and the oil pump, which can be represented by the axiom
hasEngine  powers v hasWheel t hasGenerator t hasOilPump.

(3)

Finally, Fig. 1 (C) is supposed to mean that the role powers is transitive:
powers  powers v powers.

(4)

Role inclusion axioms of the form (1), (2), (4) were a feature of the original KL-ONE
terminological language (Brachman & Schmolze, 1985), where they were called role-valuemaps and could be applied to certain individuals. Role inclusions with disjunctions on the
right-hand side also arise in the context of spatial reasoning with description logics (Wessel,
2001, 2002), where they are used to represent compositions of the RCC8-relations such as
PO  TPP  PO  TPP  NTPP (in English: if a region x partially overlaps a region y
810

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

and y is a tangential proper part of a region z, then either x partially overlaps z, or x is a
tangential proper part of z, or x is a non-tangential proper part of z).
Role inclusions with a complex right-hand side are not allowed by the syntax of SROIQ
and OWL 2, which makes adequate representation of models such as in Fig. 1 problematic.
Indeed, in these languages, we cannot exclude situations when, for example, car1 is related
to hub1 via hasEngine  hasCrankshaft  powers and, at the same time, hub1 is part of car2.
Axiom (1) asserts the existence of an individual that is a wheel in car1 and has hub1.
The main issue with axioms such as (1) is that they are similar to rewrite rules in
semi-Thue systems, the word problem for which is known to be undecidable. One of the
simplest examples was given by Tseitin (1956) who showed that the associative calculus
(Thue system) with the axioms
ac = ca, ad = da, bc = cb, bd = db, edb = be, eca = ae, abac = abacc
is undecidable. Schmidt-Schau (1989) used the undecidability of the word problem to show
that the logic underlying KL-ONE is undecidable. Baader (2003) proved (by a reduction
of semi-Thue systems) that the tractable description logic EL becomes undecidable when
extended with role inclusions containing role chains on the right-hand side. On the other
hand, he observed that role inclusions with a single role on the right-hand side do not
increase the complexity of EL. Horrocks and Sattler (2004) proved that the extension
of SHIQ with axioms of the form R  S v R and S  R v R is undecidable; however,
decidability can be regained by requiring that such axioms do not involve cycles. Axioms
of the form (3) also lead to undecidable logics: Wessel (2001, 2002) showed (by reduction
of PCP) that the extension of ALC with role axioms of the form S  T v R1 t    t Rn is
undecidable.
Similar problems have been investigated by the modal logic community. In modal logic,
axioms of the form
i1 . . . in p  j1 . . . jm p,
(5)
known as modal reduction principles, have always attracted attention and still present a
great challenge (for example, it is open whether the extension of the basic modal logic K with
either of the axioms p  p or p  p is decidable). Axioms of the form (5)
give rise to grammars generated by the production rules i1 . . .in  j1 . . .jm , and the modal
logics axiomatised by such axioms are called grammar logics (del Cerro & Penttonen, 1988).
It was shown by Demri (2001) and Baldoni (1998) that if this grammar is regular, then the
corresponding modal logic is decidable in ExpTime; on the other hand, linear (contextfree) grammar logics can be undecidable. It follows, in particular, that the satsifiability
problem for ALC knowledge bases extended with role inclusions R1 . . . Rn v S1 . . . Sk is also
ExpTime-complete provided that the grammar generated by the rules S1 . . . Sk  R1 . . . Rn
is regular (Demri, 2001, Section 5.3).
In this paper, we design a decidable extension SR+ OIQ of the description logic SROIQ
that supports a controlled use of role inclusion axioms with a complex right-hand side such
as in the examples above. Thus, we can use role inclusion axioms with a chain or union of
roles on the right-hand side, and we can also express equality of two role chains or unions
such as in (1) and (2). To ensure decidability, we impose certain regularity conditions on the
role axioms in a given ontology that generalise the syntactic restrictions of Horrocks et al.
811

fiMosurovic, Krdzavac, Henson & Zakharyaschev

(2006) and Kazakov (2010). These conditions are checked in polynomial time and employed,
as a pre-processing step, to build finite automata for some roles in the ontology. Intuitively,
the automaton for a role R recognises role chains that are subsumed by R according to the
ontology and passes the concept C to the end of the chain whenever its beginning belongs
to R.C.
Our decision algorithm builds on the tableau technique developed by Horrocks et al.
(2006) and uses some ideas of Halpern and Moses (1992, pp. 34-35) in order to pass sets
of concepts along role chains required by role inclusions with a complex right-hand side
such as (1)(3). If there are no such axioms, our tableau algorithm behaves precisely as
the tableau algorithm for SROIQ; otherwise it may suffer multiple exponential blowups
(depending on the number of role inclusions with a complex right-hand side).
An alternative approach to modelling complex structures with description logics was
suggested by Motik, Cuenca Grau, Horrocks, and Sattler (2009). Their decidable formalism
is based on description graphs that can encode axioms of the form (1), but not in the
presence of transitivity (4) (in which case the language generated by the role chain in the
left-hand side of (1) is infinite and cannot be represented by a finite graph). To ensure
decidability, Motik et al. (2009) impose acyclicity conditions on the description graphs and
do not allow the same role to appear in the description graph and the DL ontology. For
example, we cannot straightforwardly combine a description graph encoding the model in
Fig. 1 with a vehicle tax ontology containing axioms such as
Car u hasEngine.LargeEngine v vehicleTax.HigherTax.

(6)

In SR+ OIQ, the addition of (6) to (1)(4) does not cause a problem.
The structure of the paper is as follows. We define the syntax and semantics of the
description logic SR+ OIQ in the next two sections. In particular, Section 3 defines and
gives the intuition behind the regularity conditions imposed by SR+ OIQ on role axioms.
The aim of Section 4 is to illustrate by a number of examples the new challenges in the
tableau construction we are facing when dealing with SR+ OIQ compared to the case of
SROIQ. We use these examples to motivate and explain the new ideas, notions and techniques that are required for our tableau-based decision algorithm for SR+ OIQ. Tableaux
for SR+ OIQ are defined formally in Appendix A. In Appendix B, we give a tableau algorithm for SR+ OIQ and prove that it is sound, complete and always terminates. We
discuss the obtained results and open problems in Section 5.

2. Description Logic SR+ OIQ
We begin by formally defining the syntax and semantics of the description logic SR+ OIQ.
The alphabet of SR+ OIQ consists of three countably infinite and disjoint sets NC , NR and
NI of concept names, role names and individual names, respectively. We also distinguish
some proper subset NN $ NC , whose members are called nominals. This alphabet is
interpreted in structures, or interpretations, of the form I = (I , I ), where I 6=  is the
domain of interpretation, and I is an interpretation function that assigns to every A  NC
a subset AI  I , with AI being a singleton set if A  NN ; to every R  NR a binary
relation RI  I  I ; and to every a  NI an element aI  I . Following the OWL 2
812

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

standards, we do not adopt the unique name assumption and allow aI = bI for distinct
a, b  NI .
We now introduce the role and concept constructs that are available in SR+ OIQ. For
each role name R  NR , the inverse R of R is interpreted by the relation
(R )I = {(y, x)  I  I | (x, y)  RI }.
We call role names and their inverses basic roles, set NR = NR  {R | R  NR } and write
rn(R) = rn(R ) = R, for R  NR . We define a SR+ OIQ-role as a chain R1 . . . Rn or a
union R1 t    t Rn of basic roles Ri , and interpret these new constructs by taking
(R1 . . . Rn )I = R1I      RnI ,
(R1 t    t Rn )I = R1I      RnI ,
where  denotes the composition of binary relations. Define a function inv () on role chains
by taking inv (R1 . . . Rn ) = inv (Rn ) . . . inv (R1 ), where inv (R) = R and inv (R ) = R, for
R  NR .
In the set NR of role names, we distinguish some proper subset NS and call its members
and their inverses simple roles; those basic roles that are not simple will be called nonsimple. Simple and non-simple roles will have to satisfy different constraints in concepts
and role inclusion axioms to be defined below.
SR+ OIQ-concepts, C, are defined by the following grammar, where A  NC , R is a
basic role, S a simple role, and n a positive integer (given in binary):
C

::= A

|

R.C


|

|

>

R.C

|
|

C

|

 nS.C

C1 u C2
|

 nS.C

|

C1 t C2
|

|

S.Self .

The interpretation of these concepts is defined as follows, where ]X is the cardinality of X:
>I = I ,

I = ,

(C)I = I \ C I ,

(C1 u C2 )I = C1I  C2I ,

(C1 t C2 )I = C1I  C2I ,

(R.C)I = {x  I | y  C I (x, y)  RI },
(R.C)I = {x  I | y ((x, y)  RI  y  C I )},
(S.Self )I = {x  I | (x, x)  S I },
( n S.C)I = {x  I | ]{y | (x, y)  S I  y  C I }  n},
( n S.C)I = {x  I | ]{y | (x, y)  S I  y  C I }  n}.
A SR+ OIQ-knowledge base (KB, for short) consists of a TBox, an RBox and an ABox.
A TBox, T , is a finite set of concept inclusions (CIs), which are expressions of the form
C1 v C2 . Such a CI is satisfied in I if C1I  C2I , in which case we write I |= C1 v C2 . An
ABox, A, is a finite set of assertions of the form
a : C,

(a, b) : R,

(a, b) : S,
813

a 6= b,

fiMosurovic, Krdzavac, Henson & Zakharyaschev

where a and b are individual names, R a basic role, S a simple role, and C a concept. The
satisfaction relation for such ABox assertions is given by
I |= a : C

iff

aI  C I ,

I |= (a, b) : R

iff

(aI , bI )  RI ,

I |= (a, b) : S

iff

(aI , bI ) 
/ SI ,

I |= a 6= b iff

aI 6= bI .

An RBox, R, is a finite set of disjointness constraints and role axioms. A disjointness
constraint Dis(S1 , S2 ) is imposed on simple roles S1 , S2 ; it is satisfied in I if S1I  S2I = .
A role axiom (RA) can be of the following six types, where S, S 0 are simple roles; Q0 , Q,
Q1 , . . . , Qm non-simple roles; and R, R1 , . . . , Rm are arbitrary basic roles:
(A) S v S 0 , QQ v Q, Q v Q,
(B) R1 . . . Rm v Q, QR1 . . . Rm v Q, R1 . . . Rm Q v Q, for m  1,
(C) R v QR1 . . . Rm , for m  1,
(D) R v Q1 t    t Qm , for m > 1,
(E) Q0 = QR1 . . . Rm , for m  1,
(F) Q = Q1 t    t Qm , for m > 1.
RAs of the form (A)(D) are called role inclusions (RIs), while those of the form (E) and
(F) role equalities (REs). An RBox R may contain any set of role axioms satisfying the
regularity conditions to be defined and discussed in the next section.
Note that, although RAs in SR+ OIQ are only restricted to the form (A)(F), they
can encode more general role inclusions of the form (provided that they meet the regularity
conditions to be defined below)
(R11 . . . Rn1 1 ) t    t (R1m . . . Rnmm ) v (R1m+1 . . . Rnm+1
) t    t (R1k . . . Rnk k ).
m+1

(7)

(In particular, one can easily write an RBox capturing all the RAs (1)(4) from the introduction.) A detailed discussion of what can actually be represented by SR+ OIQ RBoxes
will also be given in the next section.
If %i is a chain or union of roles, i = 1, 2, then %1 v %2 (or %1 = %2 ) is satisfied in I if
I
%1  %I2 (respectively, %I1 = %I2 ). We say that the KB K = (T , R, A) is satisfiable if there
exists an interpretation I satisfying all the members of T , R and A. In this case we write
I |= K and call I a model of K.
Our main reasoning problem in this paper is concept satisfiability with respect to KBs:
given a SR+ OIQ concept C and a KB K, decide whether there is a model I of K such
that C I 6= . All other standard reasoning problems such as subsumption, KB satisfiability
or instance checking are known to be reducible to concept satisfiability with respect to
KBs. Moreover, concept satisfiability with respect to arbitrary KBs can be reduced to
concept satisfiability with respect to KBs of the form (, R, ) (with empty TBoxes and
ABoxes); (see Horrocks et al., 2006, Thm. 9).
814

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

For a concept C, we denote by nom(C) the set of all nominals that occur in C, and by
role(C) the set of all basic roles R such that either R or inv (R) occurs in C; role(C, K) and
role(C, R) contain those basic roles and their inverses that occur in C or K/R.

3. Regular RBoxes
As mentioned in the introduction, unrestricted RAs can easily simulate all kinds of undecidable problems. In this section, we define regular RBoxes that are allowed in SR+ OIQ.
For SROIQ RAsthat is, RAs of the form (A) and (B)our restrictions are the same
as those used by Kazakov (2010). As suggested by the term regular, we are going to use
the regularity restrictions to construct finite automata for roles R that recognise role chains
subsumed by R in the RBox in question.
Suppose R is a set of RAs. To define the regularity conditions (to be given in Definition 3), we require the following binary relation 0 on the set of role names occur in
R:
 rn(Ri ) 0 rn(Q), i = 1, . . . , m, for RIs of type (B),
 rn(R) 0 rn(Q), for RIs of type (C),
 rn(R) 0 rn(Qi ), i = 1, . . . , m, for RIs of type (D),
 rn(Ri ) 0 rn(Q0 ), i = 1, . . . , m, for REs of type (E).
Denote by R the transitive and reflexive closure of 0 . We write R1 'R R2 if both
R1 R R2 and R2 R R1 , and R1 R R2 if R1 R R2 and R2 R R1 . By the depth dR (R)
of R in R we understand the largest n for which there exists a chain R1 R R2 R    R
Rn R R.
We represent R as the union R = RA  RB  RC  RD  RE  RF , where RX contains
those RAs from R that are of the form (X), X  {A, B, C, D, E, F }. We also write RA,B
for RA  RB , etc.
For an RI r = (% v R)  RA,B and role chains %0 and %00 , we write %0 vr %00 if either
0
% = %01 %%02 and %00 = %01 R%02 , or %0 = %01 inv (%)%02 and %00 = %01 inv (R)%02 , for some %01 and %02 .
We write %0 vR %00 if %0 vr %00 , for some r  RA,B , and denote by vR the reflexive and
transitive closure of vR . It follows immediately from the definitions of R and vR that we
have:
Lemma 1 If % = %0 R0 %00 and % vR R, then rn(R0 ) R rn(R).
Following Kazakov (2010), we say that an RI (% v R0 )  RA,B is stratified in R if, for
every R 'R R0 with % = %1 R%2 , there exists R1 such that %1 R vR R1 and R1 %2 vR R0 . We
call RA,B stratified if every RI % v R with % vR R is stratified in R.
For every role R in R, we define the following language LR (R) of role chains regarded
as words over basic roles:
LR (R) = {% | % vR R}.
Theorem 2 (Kazakov, 2010) Suppose R is an RBox with stratified RA,B . Then the
language LR (R) is regular, for every role R in R. Moreover, one can construct a nondeterministic finite automaton recognising LR (R) the number of transitions in which does
not exceed O(|R|2dR (R) ).
815

fiMosurovic, Krdzavac, Henson & Zakharyaschev

We are now in a position to define regular RBoxes.
Definition 3 An RBox R is called regular if the following conditions are satisfied:
(c1) RA,B is stratified;
(c2) rn(R) R rn(Q), for RIs of type (C);
(c3) rn(R) R rn(Qi ), i = 1, . . . , m, for RIs of type (D);
(c4) rn(Ri ) R rn(Q0 ), i = 1, . . . , m, for RAs of type (E);
(c5) there exists a quasi-order 1R  R for which
 rn(Q0 ) 1R rn(Q), for each RA of type (E);
 rn(Q) 1R rn(Qi ), i = 1, . . . , m, for each RA of type (F);
(c6) there exists a quasi-order 2R  R for which
 rn(Q) 2R rn(Q0 ), for each RA of type (E);
 rn(Qi ) 2R rn(Q), i = 1, . . . , m, for each RA of type (F);
(c7) there do not exist RAs r and r 0 such that one of the following conditions holds:
 r 0 = (Q0 = Q0 R1 . . . Rm0 ), r = (Q = Q1 t    t Qm ), rn(Q0 ) = rn(Q) and
rn(Q0 ) = rn(Qj ), for some j, 1  j  m;
0 ), r = (Q0 = Q R . . . R ), rn(Q0 ) = rn(Q0 ) and
 r 0 = (Q00 = Q0 R10 . . . Rm
0
1 1
m
1
0
1
rn(Q0 ) = rn(Q1 );

 r 0 = (Q0 = Q01 t    t Q0m0 ), r = (Q = Q1 t    t Qm ), rn(Q0 ) = rn(Q) and
rn(Q0i ) = rn(Qj ), for some i, j, 1  i  m0 , 1  j  m.
In the remainder of this section, we discuss the regularity conditions (c1)(c7) and illustrate them by concrete examples. Note first that condition (c1) is required to ensure
decidability of SROIQ; as mentioned in the introduction, dropping it immediately leads
to undecidability (Demri, 2001; Horrocks & Sattler, 2004). To understand (c2), consider
the following:
Example 4 Let R = {RQ v Q0 , Q0 v QR}. The former RI is of type (B), while the latter
one is of type (C). Clearly, Q0 v QR does not satisfy (c2), and so the RBox is not regular.
To see why this situation is dangerous, we observe that R |= RQ v QR. Now, if the TBox
generates infinite chains of Q- and R -arrows starting from the same point, then the RI
RQ v QR would generate the N  N-grid shown on the left-hand side of the picture below:
816

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

Q

Q
Q1

R
Q

R
Q

R
Q

Q

Q Q

Q

Q1

R
Q

Q0

R

R
Q

R
Q

R

Q
R

R


Q

Q

Q

R

R

Q
R

It is routine then to reduce the undecidable N  N-tiling problem to KB satisfiability.
On the other hand, the RBox R0 = {R v QRQ } is regular (rn(R) R0 rn(Q)).
However, it cannot generate a proper N  N-grid (as shown on the right-hand side of the
picture above). To be able to encode the N  N-tiling problem, we require additional RIs
such as Q Q v Q1 and Q Q1 Q v Q1 . But then the resulting RBox will not satisfy
condition (c1).
Condition (c3) is similar to (c2); that its omission leads to undecidability was shown
by Wessel (2002). To illustrate (c4), we give one more example.
Example 5 Consider the RBox R = {Q0 Q v Q1 , Q0 = Q Q1 }. Clearly, it does not satisfy
(c4), but with this condition omitted, we only have Q0 R Q1 and Q R Q1 . Now observe
that the dangerous RI Q Q1 Q v Q1 from Example 4 is a consequence of R.
Since the REs (E) and (F) imply Q0 v QR1 . . . Rm and Q v Q1 t    t Qm of types (C)
and (D), condition (c5) is similar to (c2) and (c3). For (c6), consider the following:
Example 6 The RBox R = {Q1 Q2 v Q2 , Q3 Q4 v Q3 , Q4 = Q2 S} is regular. However,
R1 = R  {Q1 = Q3 S 0 } is not regular because rn(Q1 ) R1 rn(Q2 ), rn(Q4 ) R1 rn(Q3 ),
rn(S) R1 rn(Q4 ), rn(S 0 ) R1 rn(Q1 ), and so (c1)(c5) and (c7) are satisfied, while
(c6) is not. Now, R1 implies Q3 S 0 Q2 v Q2 and Q3 Q2 S v Q3 , from which we obtain
Q3 Q2 SS 0 Q2 v Q2 . The RBox containing this RI generates a language that is not regular.
Finally, we require condition (c7) in view of the following:
Example 7 The RAs Q0 = QR and Q0 = Q t Q1 clearly imply Q v QR. As we saw in
Example 4, in the presence of the RI RQ v Q, this would lead to undecidability. Condition
(c7) does not allow RBoxes of this sort to be counted as regular.
As was already noted, we restrict SR+ OIQ RBoxes to RAs of types (A)(F) mainly
in order to simplify notation and proofs; see (7). Every RI R1 . . . Rn v P1 . . . Pm is
equivalent to the RI inv (Rn ) . . . inv (R1 ) v inv (Pm ) . . . inv (P1 ). In particular, the RI
inv (R) v inv (Qm ) . . . inv (Q1 )inv (Q) is equivalent to the RI R v QQ1 . . . Qm of type (C),
and so we can use the former in SR+ OIQ RBoxes provided that rn(R)  rn(Q). Every
0 can be replaced with the RIs R . . . R v P . . . P T and
RI R1 . . . Rn v P1 . . . Pk P10 . . . Pm
1
n
1
k
0
0
T v P1 . . . Pm , for a fresh role name T , without affecting the satisfiability of the KB. In
817

fiMosurovic, Krdzavac, Henson & Zakharyaschev

particular, if rn(Ri )  rn(Q), then we can represent R1 . . . Rn v P1 . . . Pk QPk+1 . . . Pm
by means of three SR+ OIQ RIs: R1 . . . Rn v R, inv (R) v inv (T )inv (Pk ) . . . inv (P1 ) and
T v QPk+1 . . . Pm , for fresh role names R and T . Instead of R1 . . . Rn v P1 t    t Pm we
use R1 . . . Rn v R and R v P1 t    t Pm , for a fresh role name R. The same can be done
for role equality axioms.
The reflexivity constraint Ref (R) (saying that RI is reflexive) can be expressed by means
of the RI S v R and CI > v S.Self , where S is a fresh simple role.

Example 8 The RI (1) from the introduction is represented in SR+ OIQ by two RIs:
hasEngine  hasCrankshaft  powers v Q,

(8)

Q v hasWheel  hasHub,

(9)

where Q is a fresh non-simple role name. One might suggest that (9) could be replaced with
the RI Q  hasHub v hasWheel. However, this is not the case: the interpretation given
below satisfies the former but not the latter (obviously, Q  hasHub v hasWheel does not
imply (9)).

Q

Q

hasWheel

hasWheel
hasHub

hasHub

Example 9 Consider the (regular) RBox R = {R v Q1 R1 , Q1 v Q2 P, P = Q3 R} and the
ABox A = {(x0 , x1 ) : P }. Any model of R and A contains a sequence of (not necessarily
distinct) points x0 , x1 , x2 , . . . arranged according to the patter shown in the picture below:
x4

x0
Q3
P
x1

x2

Q2

Q3
P

R

Q1
R1

x7
x5

P

R

x3

Q2


Q1
R1

x6

When applying the tableau algorithm to R and A (to be introduced in the remainder of
the paper), we construct the same model, but represent it as a tree-shaped structure by
omitting the Q1 -, Q2 - and Q3 -arrows, which can always be restored. (In general, we always
omit the first role on the right-hand side of an axiom of type (C) and (E), and all roles
on the right-hand side of an axiom of type (D) and (F).) This is illustrated in the picture
below.
818

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

x0

x0

P

P

x1

x1

Q3

R1
x3
P
R

R1
x6

R1

R
x2

P
R

R1
x4

x2

Q1



x5

R

x3

x6

Q1



Q2

Q3

x4

x5

4. SR+ OIQ Tableaux by Examples
We prove decidability of SR+ OIQ using a tableau-based algorithm, which is a generalisation of the algorithm given by Horrocks et al. (2006). We assume that the reader is familiar
with the tableau technique for standard DLs such as ALCI (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Our aim in this section is to explain, using concrete
examples, both the problems one encounters when constructing tableaux for SR+ OIQ and
the way to resolve these problems suggested in the paper. Having worked through the
examples, the reader will have grasped the general idea of the tableaux for SR+ OIQ.
We assume that all concepts are in negation normal form (NNF). In particular, when we
write C, for a concept C, we actually mean the NNF of C. Denote by con(C) the smallest
set that contains C and is closed under sub-concepts and . For a KB K = (T , R, A), we
denote by con(K) the union of con(C), for all concepts C occurring in K. For a basic role
R and   con(K), we set |R = {C | R.C  }.
4.1 RIs with Role Chains on the Right-Hand Side
Example 10 Consider first the KB K = (T , R, A), where
A = {a : A},

R = {R v QP },

T = {A v R.>, A v Q.B, A v Q.C}.

We start the construction of a tableau for K by applying the standard tableau rules for ALC.
Thus, we create a root node x0 (corresponding to the only ABox individual a) and label it
with `(x0 ) = {A, R.>, Q.B, Q.C}, indicating thereby (some of) the concepts that should
contain a according to K. In view of R.>  `(x0 ), we then create an R-successor x1 of x0 .
The interpretation, corresponding to the resulting tableau and shown on the left-hand side
of the picture below, is clearly a model of T and A, but not of R.
`(x0 )
x0

`(x0 )
R

x1

x0

x1

R
Q

P
x2

To satisfy R, we need a Q-successor x2 of x0 , which has x1 as its P -successor. However,
the resulting interpretation, shown on the right-hand side of the picture above, is not a
819

fiMosurovic, Krdzavac, Henson & Zakharyaschev

tree. To keep the tableau tree-shaped, we would prefer to create x2 as a P -successor of
x1 without drawing the Q-arrow from x0 to x2 explicitly. To trigger the creation of x2
and to ensure that a Q-arrow can always be inserted between x0 and x2 , we add to each
label `(xi ) a new quasi-concept of the form R.P  .`(xi )|Q , which encodes R v QP . The
intended meaning of this quasi-concept is as expected: every R-successor of xi must have a
P  -successor whose label contains all the concepts in `(xi )|Q = {C | Q.C  `(xi )}. (Note
that tableau nodes are not part of the syntax for quasi-concepts. The quasi-concepts, in
fact, extend the syntax with the expressions such as R.S, where S is a set of ordinary
concepts.) If we agree to extend the standard tableau rules for R and P  to such quasiconcepts, then we only need one new tableau rule (which will be generalised later on in the
paper):
(r1) if (R v QP )  R and R.P  .`(x)|Q 
/ `(x), then set `(x) := `(x){R.P  .`(x)|Q }.
Now, returning to our example, we apply (r1) to `(x0 ), `(x1 ) =  and obtain:

	
`(x0 ) := A, R.>, Q.B, Q.C, R.P  .{B, C} ,

	
`(x1 ) := R.P  ., P  .{B, C} .
We then create a P -successor x2 of x1 with `(x2 ) = {B, C, R.P  .}, as in the picture
below, and stop with a complete and clash-free tableau, which gives a model of K if we
insert the missing Q-arrow between x0 and x2 .
`(x0 )
x0

`(x1 )
R

x1

`(x2 )
P

x2

Note that inserting the missing Q-arrow in the example above becomes more problematic
if we extend T with the CI B v Q .A because then we shall have to add A to `(x0 ),
and obtain a clash. However, we cannot do this without constructing that arrow explicitly.
To cope with this problem, together with `(x0 )|Q , we can also pass to `(x2 ) the set

`Q (x0 ) of those concepts C  `(x0 ) that can potentially occur in Q .C  `(x2 ), namely,
the set `(x0 )  con(K)|Q . We can store this set in some special memory of x2 in order
to compare it with `(x2 )|Q : if `(x2 )|Q 6 `
Q (x0 ), then we report a clash. However, this
does not solve our problem yet. To see why, consider the extension of T with B v Q .C
(rather than B v Q .A). As C does not belong to `(x0 ), we would have to report a clash,
though an addition of C to `(x0 ) would not lead to a contradiction. A solution we suggest
for such situations is to make sure that, for every concept D in {C | Q .C  con(K)} and
{Q.C | Q.C  con(K)}, either D  `(x0 ) or D  `(x0 ).
To formalise the idea above as tableau rules, we require some new notation. We allow
quasi-concepts of the form `Q (x) = (tr , t , t ), where tr = Q, t = `(x)|Q and t =
`(x)  con(K)|Q ; we also denote the first component of this triple by `rQ (x), the second by
`Q (x), and the third by `
Q (x). The special memory associated with node x will be denoted
by m(x); we assume that originally it is empty. We require the following tableau rules,
which supersede the former (r1):
(r1) if (R v QP )  R, rule (r3) is not applicable, and R.P  .`Q (x) 
/ `(x), then set
`(x) := `(x)  {R.P  .`Q (x)};
820

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

(r2) if P  .t  `(x), for t = (tr , t , t ), and x has no P  -neighbour1 y with t  `(y) and
t  m(y), then we create a new P  -successor y of x and set `(y) = t and m(y) = {t};
(r3) if (R v QP )  R and there is D  {Q.C | Q.C  con(K)}  {C | Q .C  con(K)}
with {D, D}  `(x) = , then we set `(x) := `(x)  {E}, for some E  {D, D};
(clash) if (tr , t , t )  m(x) and `(x)|inv (tr ) 6 t , then report a clash.
Example 11 To illustrate, consider the KB K = (T , R, A), where
A = {a : A},

R = {R v QP },

T = {A v R.>, A v Q.B, B v Q .C, C v Q.D}.

We obtain the following complete and clash-free tableau for K:
`(x0 ) = {A, R.>, Q.B},

(by v)

`(x0 ) := `(x0 )  {Q.D, C},

	
`(x0 ) := `(x0 )  R.P  .`Q (x0 ) , `Q (x0 ) = {B, D}, `
Q (x0 ) = {C},

(by r3)
(by r1)

create x1 with x0 Rx1 , `(x1 ) = {Q.B, Q.D, C},
(by R, r3)

	
`(x1 ) := `(x1 )  R.P  .`Q (x1 ), P  .`Q (x0 ) , `Q (x1 ) = {B, D}, `
Q (x1 ) = ,
(by r1, R)
create x2 with x1 P  x2 , m(x2 ) = {`Q (x0 )} and `(x2 ) = `Q (x0 ) = {B, D},

	
`(x2 ) := `(x2 )  Q .C, Q.B, Q.D, C, R.P  .`Q (x2 )) .

(by r2)
(by v, r3, r1)

There is no clash because `(x2 )|inv (Q)  `
Q (x0 ).
4.2 RIs with Role Unions on the Right-Hand Side
Our next example illustrates tableaux for RIs with unions on the right-hand side.
Example 12 Consider the KB K = (T , R, A) with
A = {a : A},

R = {R v Q t T },

T = {A v R .C, A v R .D, C v Q.A, C v Q.B, C v T.A,
D v T.A, D v T.B, D v Q.B}.
By applying the standard rules, we obtain the tableau shown in the picture below:
`(x1 ) = {C, Q.A, Q.B, T.A}

`(x2 ) = {D, T.A, T.B, Q.B}

x1

x2
R



R



x0
`(x0 ) = {A, R .C, R .D}
1. Intuitively, a neighbour is a successor or a predecessor of a given node. A formal definition of this notion
will be given in Section B.

821

fiMosurovic, Krdzavac, Henson & Zakharyaschev

Now, to satisfy R, we have to draw either a Q- or a T -arrow from x1 to x0 , and also from
x2 to x0 . As before, we do not do this explicitly. To ensure that such arrows can always
be drawn, we add to each `(xi ) a quasi-concept of the form R.(`(xi )|Q  `(xi )|T ), where
as before `(xi )|P = {C | P.C  `(xi )}. The meaning of this quasi-concept should be
self-evident. Thus, we extend the `(xi ) to:

	
`(x0 ) := `(x0 )  R.(  ) ,

	
`(x1 ) := `(x1 )  R.({A, B}  {A}) ,

	
`(x2 ) := `(x2 )  R.({B}  {A, B}) .
But then we have to add either A, B or A to `(x0 ) in view of the quasi-concept in `(x1 ),
and also either B or A, B in view of the quasi-concept in `(x2 ). The only clash-free way
of doing this is to extend `(x0 ) with A, B. Clearly, we can draw a Q-arrow from x1 to x0
and a T -arrow from x2 to x0 .
We can now formulate tableau rules for handling role unions in RIs, taking into account
quasi-concepts with triples considered above:
(r4) if (R v QtT )  R and there is D  {P.C | P.C  con(K)}{C | P  .C  con(K)},
for P  {Q, T } with {D, D}  `(x) = , then we set `(x) := `(x)  {E}, for some
E  {D, D};
(r5) if (R v Q t T )  R, rule (r4) is not applicable, and R.(`Q (x)  `T (x)) 
/ `(x), then
we set `(x) := `(x)  {R.(`Q (x)  `T (x))};
(r6) if (t1  t2 )  `(x), for ti = (tri , ti , t
i ), i = 1, 2, and there is no j  {1, 2} such that
tj  `(x) and tj  m(x), then take some j  {1, 2} and set `(x) := `(x)  tj and
m(x) := m(x)  {tj }.
4.3 RIs with Role Chains on the Left-Hand Side
The technique illustrated in the examples above works perfectly well for RIs with a single
role in the left-hand side. To cope with more complex RIs, we follow Horrocks and Sattler
(2004) and Horrocks et al. (2006) and encode every R  role(K) in a regular RBox R
by means of a nondeterministic finite automaton (NFA) AR = (SAR , role(K), sR , AR , aR ),
where SAR is a finite set of states, role(K) is the input alphabet, sR  SAR is the initial
state of AR , AR : SAR  role(K)  2SAR is the transition function and aR  SAR is the
accepting state. If there are no REs in R, then AR accepts precisely those role chains that
belong to the language LR (R); in other words L(AR ) = LR (R).
In the tableau construction, whenever R.C  `(x), we extend `(x) with the quasiconcept AsR .C, where s is the initial state of AR . Next, if ApR .C  `(x), y is a T -neighbour
of x and q  AR (p, T ), then we extend `(y) with AqR .C. Finally, if AaR .C  `(y), where a
is an accepting state of AR , we extend `(y) with C. To define tableau rules more formally,
we first confine attention to a single RI of the form r = (R v QP ).
We start by defining sets of quasi-concepts that are allowed for RBoxes containing r.
Denote by qc the set of all quasi-concepts of the form ApR .C such that R.C  con(K)
822

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

and p is a state of AR . For a set   qc and a basic role T , we now set:
|T = {AqR .C | ApR .C   and q  AR (p, T )},
qc (r) = {ApT .C | ApT .C  qc and there exists q

qc(r) = ApR .P  .(tr , t , t ) | p a state of AR , tr

(10)
 AT (p, Q)}  qc|Q ,


= Q, t 

qc|Q ,



t 

(11)

qc|Q

	

.

(12)

It will be convenient to think of the labels `(x) in tableaux as consisting of two disjoint
parts `(x) = c(x)  a(x), with c(x) containing standard concepts and a(x) quasi-concepts;
that is: c(x)  con(K) and
a(x)  qc  qc(r)  {P  .t | ApR .P  .t  qc(r)}  {C | C  qc (r)}.
We now allow quasi-concepts of the form aQ (x) = (Q, a(x)|Q , a(x)  qc|Q ); we denote the
first component of this triple by arQ (x), the second by aQ (x), and the third by a
Q (x).
Using the new notation, we rewrite (r1)(r3) as follows:
(r1) if r  R and there exists C  qc (r) with {C, C}  a(x) = , then we set a(x) :=
a(x)  {D}, for some D  {C, C};
(r2) if R.C  c(x) and AsR .C 6 a(x), where s is the initial state of AR , then we set
a(x) := a(x)  {AsR .C};
(r3) if r  R, rule (r1) is not applicable for r and AsR .P  .aQ (x) 6 a(x), where s is the
initial state of AR , then we set a(x) := a(x)  {AsR .P  .aQ (x)};
(r4) if ApR .C  a(x), q  AR (p, T ), y is a T -neighbour of x and AqR .C 6 a(y), then we set
a(y) := a(y)  {AqR .C};
(r5) if AaR .C  a(x), a an accepting state, and C 
/ c(x), then we set c(x) := c(x)  {C};
(r6) if AaR .P  .t  a(x), where a is an accepting state, and P  .t 
/ a(x), then we set

a(x) := a(x)  {P .t};
(r7) if P  .t  a(x), for t = (tr , t , t ), and x has no P  -neighbour y with t  a(y) and
t  m(y), then we create a new P  -successor y of x and set a(y) = t and m(y) = {t}.
The clash rule remains the same as before, with a in place of `. Note that the rule (r7) can
be replaced with two rules such that one creates a new node y and sets a(y) = {t}, while
the other rule sets a(y) := a(y)  t . In this case we do not need m(y). We illustrate the
new terminology and tableau rules by revisiting Example 11.
Example 11 (cont.) Consider again the KB K = (T , R, A) with
A = {a : A},

R = {R v QP },

T = {A v R.>, A v Q.B, B v Q .C, C v Q.D}.

In the tableau below, AQ and AR are NFAs with L(AQ ) = {Q}, L(AR ) = {R}, each having
two states: initial s and accepting a.
c(x0 ) = {A, R.>, Q.B}, a(x0 ) = {AsQ .B},
823

(by v, r2)

fiMosurovic, Krdzavac, Henson & Zakharyaschev


	
a(x0 ) := a(x0 )  AsQ .D, AaQ .C , c(x0 ) := c(x0 )  {C},

	
a(x0 ) := a(x0 )  AsR .P  .aQ (x0 ) , where
a
aQ (x0 ) = {AaQ .D, AaQ .B}, a
Q (x0 ) = {AQ .C},

(by r1, r5)

(by r3)

create x1 with x0 Rx1 , c(x1 ) = , a(x1 ) = {AsQ .D, AsQ .B, AaQ .C},

(by R, r1)

a(x1 ) := a(x1 )  {AsR .P  .aQ (x1 )}, aQ (x1 ) = (Q, {AaQ .D, AaQ .B}, {AaQ .C}), (by r3)

	
a(x1 ) := a(x1 )  AaR .P  .aQ (x0 ), P  .aQ (x0 ) ,
(by r4, r6)
create x2 with m(x2 ) = {aQ (x0 )}, x1 P  x2 , c(x2 ) = , a(x2 ) = {AaQ .D, AaQ .B}, (by r7)

	
c(x2 ) := {B, D, Q .C}, a(x2 ) := a(x2 )  AsQ .C ,
(by r5, v, r2)

	
a(x2 ) := a(x2 )  AsQ .D, AsQ .B, AaQ .C, AsR .P  .(Q, {AaQ .D, AaQ .B}, ) ,
(by r1, r3)
As a(x2 )|inv (Q) = {AaQ .C}  a
Q (x0 ), the resulting tableau is complete and clash-free.
4.4 Interaction of RIs with Role Chains on the Right-Hand Side
Example 13 Consider the KB K = (T , R, A) with
A = {a : A},

R = {R v QP, Q v Q1 P1 },

T = {A v R.>, A v Q.B, B v Q .C, C v Q.D, C v Q1 .B}.
Here we have two RIs of the form (C), with Q occurring on the right-hand side of R v QP
and in the left-hand side of Q v Q1 P1 . We expect the tableau algorithm to construct a
model of K as shown in the picture below:
Q1
Q

x0

x1

R

x2

P

P1

x3



A, Q.B, C, Q.D, Q1 .B

B

B, Q .C, D

However, if we apply the available rules, we can only produce the following tableau:
AsR .P  .aQ (x0 ), AsQ .P1 .aQ1 (x0 )

P  .aQ (x0 )

x0

x1

R

AaQ .D, AaQ .B
P

x2

where aQ (x0 ) = (Q, {AaQ .D, AaQ .B}, {AaQ .C}) and aQ1 (x0 ) = (Q1 , {AaQ1 .B}, ). As
there is no explicit Q-arrow between x0 and x2 , we cannot apply (r4) to obtain the quasiconcept AaQ .P1 .aQ1 (x0 ), and so, by (r6), P1 .aQ1 (x0 ) in x2 , which would trigger the
construction of a P1 -arrow from x2 to x3 . To overcome this problem, we will use the quasiconcept encoding the RI Q v Q1 P1 in the construction of the quasi-concept for R v QP .
More precisely, we add AaQ .P1 .aQ1 (x0 ) to aQ (x0 ), thus obtaining
aQ (x0 ) = {AaQ .D, AaQ .B, AaQ .P1 .aQ1 (x0 )}.
824

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

As AsR .P  .aQ (x0 ) is in a(x0 ), we apply (r4) and obtain AaR .P  .aQ (x0 ), and so, by
(r6), also P  .aQ (x0 ) in a(x1 ). We then construct x2 with a(x2 ) containing three quasiconcepts AaQ .D, AaQ .B and AaQ .P1 .aQ1 (x0 ), the last of which requires the existence of
a P1 -successor x3 .
In order to formalise the previous idea, we introduce a dependency relation C. Given
RIs R1 v Q1 P1 and R2 v Q2 P2 , we write (R1 v Q1 P1 ) C (R2 v Q2 P2 ) if there are states
p, q of AR1 such that either q  AR1 (p, Q2 ) or q  AR1 (p, Q
2 ). In particular, we have
(Q v Q1 P1 ) C (R v QP ). As R is regular, it is not hard to see that the relation C is
acyclic. Indeed, it follows from the definition of C, Lemma 1, Definition 3 and AR1 that
rn(Q2 ) R rn(Q1 ) (since rn(Q2 ) R rn(R1 ) and rn(R1 ) R rn(Q1 )).
Now, by induction on C we define sets qc(r) for RIs r = (R v QP ). For the C-minimal
r, qc(r) is defined by (12). Then, assuming that qc(r 0 ) is defined for every r 0 C r with
r = (R v QP ), we set
qc(r) = {ApR .P  .t | p state in AR },
S
S
where t = (tr , t , t ), tr = Q, t  qc|Q  r0 Cr qc(r 0 )|Q , t  qc|Q  r0 Cr qc(r 0 )|Q
and, for r 0 = (R0 v Q0 P 0 ) and (r 0 )  qc(r 0 ),
(r 0 )|T = {AqR0 .C | ApR0 .C  (r 0 ) and q  AR0 (p, T )}.
We also set
qc (r) = {ApT .C | there exists q  AT (p, Q), ApT .C  qc 
{AqT .C

| q  AT (p, Q ),

[

qc(r 0 )} 

r 0 Cr
ApT .C

 qc 

[

qc(r 0 )}.

r 0 Cr

Returning to our example, we see that r 1 C r, for r 1 = (Q v Q1 P1 ), r = (R v QP ),
and so qc(r 1 ) remains as it was before, while qc(r) is becoming larger and, in particular,
contains {ApR .P  .(tr1 , t1 , t
1 ) | p  {s, a}}, where
a
t1  {AaQ .D, AaQ .B, AaQ .P1 .(Q1 , {AaQ1 .B}, ), AaQ .P1 .(Q1 , , )}, t
1  {AQ .C}.

For example, qc(r) contains the quasi-concept
AsR .P  .(Q, {AaQ .D, AaQ .B, AaQ .P1 .(Q1 , {AaQ1 .B}, )}, {AaQ .C}).
The construction of a tableau for K in Example 13, using the newly defined sets qc(r), is
routine and left to the reader.
The dependency relation C between RIs in RBoxes will become more complex in the
presence of unions of roles.
825

fiMosurovic, Krdzavac, Henson & Zakharyaschev

4.5 Role Equalities
Example 14 Consider the RBox R with two RAs: ST v R of type (B) and R = QP of
type (E). Clearly, R = QP can be replaced by the RIs R v QP and QP v R, but the
resulting RBox {ST v R, R v QP, QP v R} will not be regular. Let us observe now that
both RBoxes
R0 = {ST v R, QP v R}

and R00 = {ST v R, R v QP }

are regular. Denote by AR the NFA for R determined by R0 , and by A1 the NFA for R
given by R00 (see the picture below).
p
AR

T

S
s

start

p

a

R
Q

s

start

A1

T

S
R

a

P
q

Let us see now whether we can use any of these automata in the rules (r2) and (r3) on
page 823 for the role R. Consider the KB K = (T , R, A), where R is as above and
A = {a : A},

T = {A v R.>, A v Q.B, A v Q.C, A v R.D}.

First we try AR . By applying the tableau rules we obtain the following:
A, R.>, R.D, Q.B, Q.C, AsR .D, AsQ .B, AsQ .C

AaR .D, D

x0

x1

R

Now we have to apply (r3) to the RI R v QP and add the quasi-concept AsR .P  .aQ (x0 )
to a(x0 ), where aQ (x0 ) = (Q, a(x0 )|Q , a(x0 )  qc|Q ). Because of the Q-transition in AR ,
we must then have AqR .P  .aQ (x0 )  a(x0 )|Q , which is impossible because a(x0 )|Q cannot
be an element of itself.
Alternatively, we can use A1 for R. This gives us
R.D, As1 .P  .aQ (x0 ), As1 .D
x0

AaQ .C, AaQ .B

P  .aQ (x0 ), Aa1 .D, D
P

x1

R

x2

with aQ (x0 ) = (Q, {AaQ .C, AaQ .B}, ), which defines a model of K when we add the missing
Q-arrow from x0 to x2 .
Now, we replace the CI A v R.> in K with A v Q.P.> and use A1 for R. In this
case, we obtain the following tableau:
R.D, As1 .P  .aQ (x0 ), As1 .D
x0

x1

Q

826

P

x2

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

To produce a satisfying interpretation I, we have to add an R-arrow from x0 to x2 . However,
this cannot be done for free (as in Example 10) because x2 
/ DI . An alternative would
be to use AR and R0 instead of R (because we do not have to apply (r3) to R v QP ). We
then obtain the following tableau:
AqR .D

R.D, AsR .D
x0

Q

x1

AaR .D, D
P

x2

The addition of an R-arrow from x0 to x2 gives an interpretation I such that I |= QP v R
and x2  DI .
To sum up: the rule (r2) requires the NFA AR , while (r3) requires A1 . So rule (r2) on
page 823 remains the same and we rewrite rule (r1) and (r3) to include role equalities as
follows:
(r1) if r  R, for r = (R v QP ) or r = (R = QP ), and there exists C  qc (r) with
{C, C}  a(x) = , then we set a(x) := a(x)  {D}, for some D  {C, C};
(r3) if r  R, for r = (R v QP ) or r = (R = QP ), rule (r1) is not applicable for r
and As1 .P  .aQ (x) 6 a(x), where s is the initial state of A1 , then we set a(x) :=
a(x)  {As1 .P  .aQ (x)}.
Note that in the case r = (R v QP ) NFA A1 is same as AR and in the case r = (R = QP )
NFA A1 is different from AR as described above.

5. Main Result and Discussion
The examples of the previous section provide the basic ingredients that can be added to
SROIQ tableaux of Horrocks et al. (2006) and Horrocks and Sattler (2007) in order to
obtain sound and complete tableaux for SR+ OIQ. We present all the technical details
and definitions in Appendix A. A corresponding sound, complete and terminating tableau
algorithm is given in Appendix B. Thus, we obtain the following:
Theorem 15 Concept satisfiability with respect to SR+ OIQ KBs is decidable.
It is to be noted that the decision algorithm in Appendix B is a (quite sophisticated)
extension of the standard tableau procedure for SROIQ; if the input RBox does not
contain RAs of the form (C)(F) then our tableau algorithm behaves exactly as the SROIQ
procedure. To simplify presentation and avoid a number of technical details, we decided
not to optimise our tableau algorithm in this paper. In fact, there is plenty of room for
optimisations; for example, one can work on a more careful choice of quasi-concepts as well
as utilise the approach of Motik, Shearer, and Horrocks (2009).
The exact complexity of concept satisfiability with respect to SR+ OIQ KBs is still
unknown. If the RBox contains one RA r 1 of the form (C)(F), our algorithm will have
to construct the set qc(r 1 ) of quasi-concepts, which contains subsets of the previously
constructed sets of quasi-concepts qc(r 0 ), and so may suffer an exponential blow-up. Furthermore, the algorithm may suffer one more exponential blow-up every time we add an
extra RA of the form (C)(F), and thereby extend the C-chains of RAs, because again the
827

fiMosurovic, Krdzavac, Henson & Zakharyaschev

set of quasi-concepts may become exponentially larger. To investigate the complexity of
full SR+ OIQ, it may be useful to consider first its various sub-languages. For example,
we conjecture that ALCI-concept satisfiability with respect to regular RBoxes that only
contain axioms of type (C) and the roles rn(Ri ), i = 1, . . . , m, do not appear in left-hand
side of RIs, is PSpace-complete. SI-concept satisfiability with respect to RBoxes which
contain only one axiom of the form R v QP , where rn(R), rn(Q), rn(P ) are different role
names that are not transitive, is also PSpace-complete.
The step from SROIQ to SR+ OIQ is, to some extent, similar to the step from SHOIQ
to SROIQ: as SROIQ extends SHOIQ with role inclusion axioms containing role chains
in the left-hand side, SR+ OIQ extends SROIQ with role inclusion axioms containing role
chains or unions in the right-hand side. Attempts to extend various DLs with such role
inclusions have been made since 1985 (Brachman & Schmolze, 1985; Baader, 2003; Wessel,
2001, 2002); however, all of them resulted in undecidable formalisms. Similar problems
were investigated in modal logic, where it was shown that regular grammar logics are decidable (Demri, 2001). Our regularity condition for RAs axioms generalises the restrictions
of Horrocks et al. (2006) and Kazakov (2010). (However, a closer inspection of how our
results are related to grammar modal logics is needed.) Simanck (2012) showed that complex RIs in SROIQ can be encoded using SHOIQ axioms. It would be of interest to find
out whether a similar reduction is possible in the case of SR+ OIQ.
One of the aims of introducing complex role inclusion axioms in DLs is to model complex
structured objects. Suppose, for example, that we have to represent the cycle shown on the
left-hand side of the picture below:
x0

x0
Q

R1
x1

x4

x1

R2

R4

R2

x2

R3

Q

R1

x3

x4

Q1

x2

Q2
R3

R4
x3

In SROIQ, we can only use the RI axiom R1 R2 R3 R4 v Q, which produces the required
cycle only if there is a chain of the form R1 R2 R3 R4 . Using description graphs from the
work of Motik et al. (2009), we can express the existence of the cycle above as a whole. In
SR+ OIQ, we can model this situation by the following regular RBox, where Q1 and Q2
are fresh role names:
R1 v QR4 R3 R2 ,

R2 v Q1 R1 ,

 
Q
1 v QR4 R3 ,

R3 v Q2 R4 ,


Q
2 v Q R1 R2 ,

R4 v Q R1 R2 R3

(see the picture above). The RBox produces the required cycle if there is at least one Ri ,
for i = 1, 2, 3, 4, in a model. In this connection, it would be of interest to consider the
extension of SHOIQ with RI axioms of the form (A) and (C).
828

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

Appendix A. SR+ OIQ Tableaux
As observed by Horrocks et al. (2006, Thm. 9), without loss of generality we can define
tableaux for SR+ OIQ KBs with empty TBoxes and ABoxes. Let R be a regular RBox
and C0 a SR+ OIQ concept. We assume that RC,D,E,F = {r i | i = 1, . . . , l}, where, for
some k1 , k, l1 such that 1  k1  k  l1  l,
r i = (Ri v Qi Pi1 . . . Pimi ),

for i = 1, . . . , k1 ,

r i = (Ri = Qi Pi1 . . . Pimi ),

for i = k1 + 1, . . . , k,

r i = (Ri = Ti1 t    t Timi ),

for i = k + 1, . . . , l1 ,

r i = (Ri v Ti1 t    t Timi ),

for i = l1 + 1, . . . , l.

For every R  role(C0 , R), we construct, as a preprocessing step, an NFA AR and special
NFAs Ai , for i = 1, . . . , l, as described below. Recall that L(A) denotes the language
recognised by A. If p is a state in A, then Ap is the NFA obtained from A by making p the
(only) initial state of A.
Define an RBox
R0 = RA,B  {Qi Pi1 . . . Pimi v Ri | i = k1 + 1, . . . , k} 
{Tij v Ri | i = k + 1, . . . , l1 , j = 1, . . . , mi },
which only contains axioms of types (A) and (B). Since R is regular and in view of conditions
(c1), (c4) and (c6) in Definition 3, the RBox R0 is stratified. By Theorem 2, we use R0 to
construct, for any R  role(C0 , R), an NFA AR = (SAR , role(C0 , R), sR , AR , aR ) such that
L(AR ) = LR0 (R).
We also define RBoxes
Ri = R0 \ {Qi Pi1 . . . Pimi v Ri },
i

0

R = R \ {Tij v Ri | j = 1, . . . , mi },

i = k1 + 1, . . . , k,
i = k + 1, . . . , l1 ,

and construct NFAs Ai such that L(Ai ) = LRi (Ri ), i = k1 + 1, . . . , l1 . For i = 1, . . . , k1 and
i = l1 + 1, . . . , l, we simply set Ai = ARi .
Now, we are going to define formally the set qc(C0 , R). The elements of qc(C0 , R) are
called quasi-concepts (for C0 w.r.t. R); we use them to define labels for tableau nodes. In
the definition of qc(C0 , R), we require a dependency relation C on RC,D,E,F .
For each role name Q  {rn(Qi ), rn(Ti1 ), . . . , rn(Timi ) | 1  i  l}, let AutIn(Q) be the
set of those i  {1, . . . , l} for which there are states p and q of Ai such that q  Ai (p, Q) or
q  Ai (p, Q ). We define C on RC,D,E,F by taking r i C r j if
 1  j  k and i  AutIn(rn(Qj )), or
 k < j  l and there is h  {1, . . . , mj } such that i  AutIn(rn(Tjh )).
The following lemma shows that the transitive closure of C is acyclic:
Lemma 16 (i) If r i C r j then r j C r i does not hold.
(ii) If r i1 C r i2 and r i2 C r i3 , then r i3 C r i1 does not hold.
829

fiMosurovic, Krdzavac, Henson & Zakharyaschev

Proof. Observe first that if i  AutIn(Q) then there is a Q- or Q -transition of Ai , and so
we have rn(Q) 1R rn(Ri ). If i  k then rn(Ri ) 1R rn(Qi ), and so rn(Q) 1R rn(Qi ). If
i > k then rn(Ri ) 1R rn(Tih ), and so rn(Q) 1R rn(Tih ), for all h  {1, . . . , mi }.
(i) Let r i C r j . Four cases are possible.
Case 1: i, j  k. Then i  AutIn(rn(Qj )), and so rn(Qj ) 1R rn(Qi ). Similarly, if we
had r j C r i , then rn(Qi ) 1R rn(Qj ), which is impossible.
Case 2: j  k and i > k. Then i  AutIn(rn(Qj )), and so rn(Qj ) 1R rn(Tih ), for all
h  {1, . . . , mi }. If r j C r i then there is Tih0 , 1  h0  mi , such that j  AutIn(rn(Tih0 )).
Hence, rn(Tih0 ) 1R rn(Qj ), which is a contradiction.
Case 3: i  k and j > k. This is a mirror image of case 2.
Case 4: i, j > k. Then there is Tjh0 , 1  h0  mj , such that i  AutIn(rn(Tjh0 )), and so
rn(Tjh0 ) 1R rn(Tie ), for all e  {1, . . . , mi }. Similarly, if we had r j C r i , then there is Tie0 ,
1  e0  mi , such that rn(Tie0 ) 1R rn(Tjh ), for all h  {1, . . . , mj }, which is impossible.
The proof of (ii) is similar and left to the reader.
q
We will require the following notation. Let
qc = {ApR .C | R.C  con(C0 ) and p a state of AR }.
For a set   qc and a basic role P , we set
|P = {AqR .C | ApR .C   and q  AR (p, P )}.
Sometimes it will be convenient for us to write qc(r 0 ) in place of qc and assume that r 0 Cr i ,
for all i, 1  i  l. Now, assuming that qc(r j ) is defined for every r j C r i where 0  j  l
and 1  i  l, we define qc(r i ) to be the set of all Aqi .C such that
 q is a state of Ai ;

 r  
 for i  k, C = Pim
.    Pi1
.(t0 , t0 , t0 ) and tr0 = Qi ;
i
W i r  
r
 for i > k, C = m
h=1 (th , th , th ) and th = Tih ;
S
 th  rj Cri qc(r j )|tr ;
h



t
h



S

r j Cr i

qc(r j )|inv (tr ) .
h

For (r i )  qc(r i ) and a basic role P , let
(r i )|P = {Aqi .C | Api .C  (r i ) and q  Ai (p, P )}.
Finally, we set
qc(C0 , R) =

l
[

qc(r i ),

i=0

and, for   qc(C0 , R) and a basic role P ,
|P

=

l
[

(  qc(r j ))|P

and

| = {Aq .C | Ap .C   and q  A (p, )},

j=0

830

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

where  is the empty role chain. For   qc(C0 , R) and 1  i  l, let
(
 r  
P  i . . . Pi1
.(t0 , t0 , t0 ), tr0 = Qi , if i  k,
(r i , ) = Wmim

r 
r
i
if i > k,
h=1 (th , th , th ), th = Tih ,

(13)


s
where th = |tr , t
h =   qc(C0 , R)|inv (trh ) , 0  h  mi . Clearly, Ai .((r i , ))  qc(r i ).
h
Intuitively, if  is the label of a node u, that is,  = a(u), then (r i , ) is the quasi-concept
encoding the RA r i in the node u.

Example 17 Let R = {r 1 , r 2 }, where r 1 = (R1 v Q1 P1 P2 ) and r 2 = (R2 v T1 t T2 ).
The NFAs for the roles in R have two states: initial s and accepting a. Suppose
 = {AsQ1 .C1 , AaQ .C2 , AsT1 .C3 , AsT2 .C4 , AsT2 .C5 , AaT  .C6 }.
1

1

Then
(r 1 , ) = P2 .P1 .(Q1 , {AaQ1 .C1 }, {AaQ .C2 }),
1

(r 2 , ) =

(T1 , {AaT1 .C3 }, {AaT  .C6 })
1

 (T2 , {AaT2 .C4 , AaT2 .C5 }, ).

Remark 18 If P is a symmetric role (i.e., (P  v P )  R), then each occurrence of P and
inv (P ) is treated as rn(P ). For example, {P.D, P  .C}|P  = {D, C}.
We are now in a position to define SR+ OIQ tableaux. Note that the most essential
difference compared with the tableaux for SROIQ (Horrocks et al., 2006) are the rules
(p19), (p21) and (p22).
A tableau for C0 w.r.t. R is a structure of the form T = (S, c, a, E), where S is non-empty
set, c : S  2con(C0 ) , a : S  2qc(C0 ,R) , E : role(C0 , R)  2SS such that the following
conditions hold:
(p1) C0  c(u0 ) for some u0  S,
(p2) if C  c(u) then C 
/ c(u), where C is either a concept name or R.Self ,
(p3) >  c(u) and  
/ c(u) for any u,
(p4) if R.Self  c(u) then (u, u)  E(R),
(p5) if R.Self  c(u) then (u, u) 6 E(R),
(p6) if (C1 u C2 )  c(u) then C1  c(u) and C2  c(u),
(p7) if (C1 t C2 )  c(u) then C1  c(u) or C2  c(u),
(p8) if R.C  c(u) then there is some v  S with (u, v)  E(R) and C  c(v),
(p9) (u, v)  E(R) iff (v, u)  E(inv (R)),
(p10) if ( nS.C)  c(u) then ]{v  S | (u, v)  E(S) and C  c(v)}  n,
831

fiMosurovic, Krdzavac, Henson & Zakharyaschev

(p11) if ( nS.C)  c(u) then ]{v  S | (u, v)  E(S) and C  c(v)}  n,
(p12) if ( nS.C)  c(u) and (u, v)  E(S), then C  c(v) or C  c(v),
(p13) if o  c(u)  c(v), for some o  nom(C0 ), then v = u,
(p14) for each o  nom(C0 ), there is some vo  S with o  c(vo ),
(p15) if Dis(R, S)  R then E(R)  E(S) = ,
(p16) if (u, v)  E(R) and R v S, then (u, v)  E(S),2
(p17) if R.C  c(u) then AsR .C  a(u), where s is the initial state of AR ,
(p18) if AaR .C  a(u), where a is an accepting state, then C  c(u),
(p19) Asi .C  a(u), where s is the initial state of Ai and C = (r i , a(u)), for all u  S
and 1  i  l,
(p20) if (u, v)  E(R) then a(u)|R  a(v),
(p21) if Aai .C  a(u), where i  k, C = inv (Pimi ).    inv (Pi1 ).(tr , t , t ) and a is an
accepting state, then there are v0 , v1 , . . . , vmi = u such that (vj , vj1 )  E(inv (Pij )),
for 1  j  mi , t  a(v0 ) and a(v0 )|inv (tr )  t ,
W i r  
(p22) if Aai .C  a(u), where a is an accepting state, i > k and C = m
h=1 (th , th , th ), then
there is j  {1, . . . , mi } such that tj  a(u) and a(u)|inv (tr )  t
,
j
j

(p23) a(u)|  a(u).
Let T = (S, c, a, E) be a tableau, R a basic role and u, v  S. If a(u)|R  a(v) and
a(v)|inv (R)  a(u), then we write ar(R, u, v). If there is an R-arrow from u to v then
ar(R, u, v) holds; see Proposition 20 (i). On the other hand, the meaning of ar(R, u, v) is
that we can always insert an R-arrow (for a non-simple role R) from u to v without violating
any of the tableau conditions.
Lemma 19 A concept C0 is satisfiable w.r.t. a SR+ OIQ RBox R if and only if there
exists a tableau for C0 w.r.t. R.
Proof. () Let T = (S, c, a, E) be a tableau for C0 w.r.t. R. Define an interpretation
I = (I , I ) by taking I = S, C I = {u | C  c(u)}, for a concept name C  con(C0 ).
For a role name R, we define E(R) (by induction on 1R ) and RI in the following way. For
a role name R, we define E(R) and RI by induction on 1R in the following way. For 1R minimal R, we set E(R) = E(R). We extend E() with E(inv (R)) = {(u, v)|(v, u)  E(R)}
2. Here v is the transitive closure of v.

832

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

and E(S1 . . . Sn ) = E(S1 )      E(Sn ). Suppose now that E(S) is defined for all S 1R R.
Then we set, where wi = Pi1 . . . Pimi and E(wi ) = E(Pi1 )      E(Pimi ),
E(R) = E(R) 
[

{(u, v) | ar(R, u, v) & %  L(Ai ) z ((u, z)  E(%)  (v, z)  E(wi ))} 

{i|R=Qi }

[

{(u, v) | ar(R, u, v) & %  L(Ai ) z ((v, z)  E(%)  (u, z)  E(wi ))} 

{i|R=inv (Qi )}

[

{(u, v) | ar(R, u, v) & %  L(Ai ) (u, v)  E(%)} 

{i|j R=Tij }

[

{(u, v) | ar(R, u, v) & %  L(Ai ) (v, u)  E(%)},

{i|j R=inv (Tij )}

RI

= {(u0 , un ) | u1 , . . . , un1 ((ui , ui+1 )  E(Si+1 )  S1 S2 . . . Sn  L(AR ))}.

We need E(R) to adjust E(R) by taking account of the omitted R-arrows for RIs of the form
(C)(F) as we do not use these RIs in the construction of AR . The picture below illustrates
such a situation for a role Q and two RIs QQ v Q and R v QP (E(Q)  E(Q)  QI ).
QI
E(Q)

u0

E(Q)

u1

RI

u2

(P  )I

u3

We have to show that I is a model of C0 and R. To this end, we require the following:
Proposition 20 (i) If (u, v)  E(R) then ar(R, u, v).
(ii) If (u, v)  E(R) then ar(R, u, v).
(iii) If %  L(A), (u, v)  E(%) and As .C  a(u) then Aa .C  a(v).
(iv) If (u, v)  RI and AsR .C  a(u) then AaR .C  a(v).
Proof. (i) Follows from (p20) and (p9). More precisely, if (u, v)  E(R) then, by (p9),
(v, u)  E(inv (R)). By (p20), (u, v)  E(R) implies a(u)|R  a(v), while (v, u)  E(inv (R))
implies a(v)|inv (R)  a(u). Thus, we obtain ar(R, u, v).
(ii) Follows from (i) and the definition of E(R).
(iii) Let % = S1 . . . Sn . Since (u, v)  E(%), we have u = u0 , . . . , un = v with (ui1 , ui ) 
E(Si ), for i = 1, . . . , n. On other hand, since S1 . . . Sn  L(A), there are s = p0 , . . . , pn = a
such that pi  A (pi1 , Si ). We have Ap0 .C  a(u0 ). If Api .C  a(ui ), i < n, then (ii) and
pi+1  A (pi , Si+1 ), (ui , ui+1 )  E(Si+1 ) give Api+1 .C  a(ui+1 ). So Aa .C  a(v).
(iv) Follows from (iii) and the definition of RI .
q
We show now that I is a model of R by considering all types of constraints.
Dis(S1 , S2 ): Then the Si are simple roles, SiI = E(Si ), and so, by (p15), S1I  S2I = .
833

fiMosurovic, Krdzavac, Henson & Zakharyaschev

S1 v S2 : The Si are simple roles and SiI = E(Si ). Thus, if (u, v)  S1I then (u, v)  E(S1 )
and, by (p16), (u, v)  E(S2 ); hence (u, v)  S2I .
S1 . . . Sn v R: We have S1 . . . Sn  L(AR ). If (u, v)  (S1 . . . Sn )I then there are u =
u0 , . . . , un = v such that (ui1 , ui )  (Si )I , for i = 1, . . . , n. By the definition of
(Si )I , there are ui1 = ui0 , . . . , uini = ui with (uij1 , uij )  E(Sji ), for 1  j  ni , and
S1i S2i . . . Sni i  L(ASi ). Therefore, S11 . . . Sn11 S12 . . . Snnn  L(AR ) and (u, v)  RI .
RR v R, RS1 . . . Sn v R and S1 . . . Sn R v R are considered analogously.
R v R: As mentioned earlier, each occurrence of R is treated as R. It follows that
(u, v)  E(R) if and only if (v, u)  E(R), and (u, v)  E(R) if and only if (v, u)  E(R).
In addition, (u, v)  RI if and only if (v, u)  RI . Indeed, let (u, v)  RI . Then, by
the definition of RI , there exist u = u0 , u1 , . . . , un = v with (ui , ui+1 )  E(Si+1 ) and
S1 S2 . . . Sn  L(AR ). Now we have (ui+1 , ui )  E(inv (Si+1 )) and, by the construction
of AR , inv (Sn ) . . . inv (S1 )  L(AR ), and so (v, u)  RI .
Ri v Qi Pi1 . . . Pimi : Let (u, v)  RiI . Then, by (p19), we have Asi .C  a(u), where s is the
initial state of Ai and C = (r i , a(u)) = inv (Pimi ).    inv (Pi1 ).(Qi , a(u)|Qi , a(u) 
qc(C0 , R)|inv (Qi ) ). By Proposition 20, Aai .C  a(v), where a is an accepting state.
Now, by (p21), there are v0 , v1 , . . . , vmi = v such that (vj , vj1 )  E(inv (Pij )),
a(u)|Qi  a(v0 ) and a(v0 )|inv (Qi )  a(u)  qc(C0 , R)|inv (Qi )  a(u), that is, (v0 , v) 
(Pi1 . . . Pimi )I and ar(Qi , u, v0 ). Hence, (u, v0 )  QIi and (u, v)  (Qi Pi1 . . . Pimi )I .
Ri v Ti1 t    t Timi : Let (u, v)  RiI . Then, by
(p19), we have Asi .C  a(u), where s is the
Wm
i
(Tih , a(u)|Tih , a(u)  qc(C0 , R)|inv (Tih ) ).
initial state of Ai and C = (r i , a(u)) = h=1
a
By Proposition 20, Ai .C  a(v), where a is an accepting state. Now, by (p22), there is
j  {1, . . . , mi } such that a(u)|Tij  a(v) and a(v)|inv (Tij )  a(u)qc(C0 , R)|inv (Tij ) 
a(u), i.e., ar(Tij , u, v). Hence, (u, v)  TijI and (u, v)  (Ti1 t    t Timi )I .
Ri = Qi Pi1 . . . Pimi : Let (u, v)  RiI . Then there exists a role chain %  L(ARi ) such that
(u, v)  E(%). If %  L(Ai ) then (u, v)  (Qi Pi1 . . . Pimi )I , and the proof is same as for
Ri v Qi Pi1 . . . Pimi . So suppose % 
/ L(Ai ) is shortest possible. Since % vR0 Ri , there
is a sequence % vr1 %1 vr2    vrn %n vrn+1 Ri , where r j  R0 , for 1  j  n + 1, and
at least one r j is not in Ri . If r j 6 Ri , for j < n + 1, then we can find a shorter %, and
so r n+1 = (Qi Pi1 . . . Pimi v Ri ). Therefore, % = %00 %01 . . . %0mi is such that %00 vR0 Qi
and %0j vR0 Pij , for 1  j  mi . Thus, (u, v)  (Qi Pi1 . . . Pimi )I .
Let now (u, v)  (Qi Pi1 . . . Pimi )I . Then there exists v0 such that (u, v0 )  QIi and
(v0 , v)  (Pi1 . . . Pimi )I . In the case (u, v0 )  E(Qi )\E(Qi ) then by construction of
E(Qi ) we have (%  L(Ai ))(z) (u, z)  E(%). If v = z then we have (u, v)  RiI .
Otherwise the proof is same as for Qi Pi1 . . . Pimi v Ri .
Ri = Ti1 t    t Timi : Similar to the previous case.
To prove that I satisfies C0 , we show that
C  c(u) implies u  C I , for each u  S and each C  con(C0 ).
834

(14)

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

Together with (p1), this will imply u0  (C0 )I . We prove (14) by induction on the construction of concepts. If C is a concept name then (14) follows from the definition. For 
and >, it follows from (p3), and for C1 u C2 , C1 t C2 , R.C,  qS.C, and S.Self , from
(p6), (p7), (p8), (p11) and (p4). The case of C follows from (p2) and (p5) and case of
 qS.C follows from (p10) and (p12). Consider now the (only interesting) case C  R.D.
Let R.D  c(u) and (u, v)  RI . By (p17) we have AsR .D  a(u), where s is the initial
state. Therefore, by Proposition 20, we have AaR .D  a(v) and a is an accepting state.
Now, by (p18), D  c(v); by IH, v  DI , and thus u  (R.D)I .
For o  nom(C0 ), by (p14), there is vo with o  c(vo ), and so vo  oI . If u  oI then
o  c(u), and so, by (p13), u = vo . Thus, oI is a singleton set.
() Suppose I = (I , I ) is a model of C0 and R. We define T = (S, c, a, E) by taking
S = I ,

E(R) = RI ,

c(u) = {C  con(C0 ) | u  C I }  {>}

and define a(u) as follows. First, we define by induction on C auxiliary sets a0 (u, r), where
r is an RI of the from (C)(F). Recalling that r 0 C r i for all i, 1  i  l, we set
a0 (u, r 0 ) = {AsR .C | s is the initial state, R.C  con(C0 ) and u  (R.C)I } 
{AqR .C  qc | for all S1 S2 . . . Sn  L(AqR ), u  (S1 .S2 .    Sn .C)I
and if   L(AqR ) then u  C I }.
Then, assuming that a0 (u, r 0 ) is defined for every r 0 C ri , we set
a0 (u, r i ) = {Aqi .C | v  I w  (role(C0 , R)) (w, q)  prefix L(Ai ),
(v, u)  (w)I , C = (r i ,

[

a0 (v, r 0 ))},

r 0 Cri

where (w)I = S1I . . . SnI , for w = S1 . . . Sn , and
prefix L(Ai ) = {(w, q) | q a state in Ai , w0  L(Aqi ) ww0  L(Ai )}.
S
Note that {Asi .C | s the initial state, C = (r i , r0 Cri a0 (u, r 0 )}  a0 (u, r i ).
Finally, we set
l
[
a(u) =
a0 (u, r j ).
j=0

We now prove that T is a tableau for C0 w.r.t. R. Properties (p1)(p16) follow immediately from the definitions of c and E, while (p17)(p19) follow from the definitions
of c(u) and a(u). For (p20), suppose (u, v)  E(R), Ap .C  a(u) and q  A (p, R).
Then Ap .C  a0 (u, r i ), for some i. If i > 0 then A = Ai and, by S
the definition of
a0 (u, r i ), there are u0  I and w  (role(C0 , R)) with C = (r i , r0 Cri a0 (u0 , r 0 )),
(w, p)  prefix L(A) and (u0 , u)  (w)I . Let w0 = wR. Then (w0 , q)  prefix L(A) and
(u0 , v)  (w0 )I , so Aq .C  a0 (v, r i )  a(v).
Api .C

Asi .C
u0

w

u

835

Aqi .C
R

v

fiMosurovic, Krdzavac, Henson & Zakharyaschev

For i = 0 (i.e., when C is a concept C and Ap .C  a0 (u, r 0 )), suppose Aq .C 
/ a0 (v, r 0 ).
0
By the definition of a (v, r 0 ), this can be for two reasons (Horrocks et al., 2006):
 There is S2 . . . Sn  L(Aq ) and v 
/ (S2 .    Sn .C)I . However, this implies that
p
RS2 . . . Sn  L(A ) and u 
/ (R.S2 .    Sn .C)I , contrary to Ap .C  a0 (u).
   L(Aq ) and v 
/ C I . But then R  L(Ap ) and u 
/ (R.C)I , which is again a
contradiction.
Therefore, Aq .C  a0 (v, r 0 ), and so a(u)|R  a(v).
To show (p21) and (p22), suppose Aai .C  a(u), where a is an accepting state. By the
definition of a(u), there are S
v  I and w  (role(C0 , R)) such that (w, a)  prefix L(Ai ),
I
(v, u)  (w) and C = (r i , r0 Cri a0 (v, r 0 )) = (r i , a(v)). Since a is an accepting state, we
have w  L(Ai ), and so (v, u)  (Ri )I .
For (p21)i.e., i  kwe have C = inv (Pimi ).    inv (Pi1 ).(tr , t , t ), where tr = Qi ,
t = a(v)|Qi , t = a(v)  qc(C0 , R)|inv (Qi ) . We also have (v, u)  (Qi Pi1 . . . Pimi )I , and
there are v0 , v1 , . . . , vmi = u such that (vj1 , vj )  PijI and (v, v0 )  QIi . Therefore, by
(p20), t  a(v0 ) and a(v0 )|inv (tr )  t .
W i r  


r
For (p22)i.e., i > kwe have C = m
h=1 (th , th , th ), where th = Tih , th = a(v)|Tih ,


th = a(v)  qc(C0 , R)|inv (Tih ) for 1  h  mi . We also have (v, u)  (Ti1 t    t Timi )I ,
and there is j  {1, . . . , mi } such that (v, u)  TijI . Therefore, by (p20), tj  a(u) and
a(u)|inv (tr )  t
j .
j

q

(p23) is considered in the same way as (p20).

Appendix B. The Tableau Algorithm
The tableau algorithm, for SR+ OIQ concepts C0 and RBoxes R, works on completion
graphs similarly to the algorithms given by Horrocks et al. (2006) and Horrocks and Sattler
(2007). To present it, we require some additional notation. We assume that the given R is
same as in Appendix A with RC,D,E,F = {r i | i = 1, . . . , l}. For 1  i  l and a basic role
P , where P = Qi for i  k, and P  {Ti1 , . . . , Timi } for i > k, let
qc (r i , P ) = {Ap .C | there exists q  A (p, P ), Ap .C 

[

qc(r j )}

r j Cr i

 {Aq .C | q  A (p, inv (P )), Ap .C 

[

qc(r j )}.

r j Cr i

S i

We now set qc (r i ) = qc (r i , Qi ), for i  k, and qc (r i ) = m
j=1 qc (r i , Tij ), for i > k.
The set qc (r i ) of quasi-concepts is to be guessed by the algorithm. Let qc(C0 , R) be the
minimal set such that:
S
 qc(C0 , R)  {Ap .C | Ap .C  li=1 qc (r i )}  qc(C0 , R),
 if Ap .C  qc(C0 , R) then C  qc(C0 , R),
 if P.C  qc(C0 , R) then C  qc(C0 , R),
836

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

 if (

Wm

j=1 Cj )

 qc(C0 , R) then {C1 , . . . , Cm }  qc(C0 , R).

Unlike qc(C0 , R), the set qc(C0 , R) contains sub-quasi-concepts. (Quasi-concepts of the
form Ap .C will only be used to make sure that Ap .C does not belong to a label.)
Given an SR+ OIQ concept C0 and an RBox R, a completion graph for C0 and R is a
structure of the form G = (V1 , V2 , E1 , E2 , c, a, l, ), where
 V1  V2 = ; the elements of V1 are called root nodes, and the elements of V2 are called
internal (or non-root) nodes;
 (V, E1 ) is a directed forest with nodes V = V1  V2 and arcs E1 (its roots have no
incoming arcs);
 E2 is a set of arcs between nodes and root nodes, as well as arcs of the form (x, x),
for x  V2 ;
 for each (x, y)  E, where E = E1  E2 , we have l(x, y)  role(C0 , R); if R0  l(x, y)
and R0 v R, then y is called an R-successor of x; y is called an R-neighbour of x if y
is an R-successor of x or x is an inv (R)-successor of y; also, x is called an -neighbour
of x (cf. Horrocks et al., 2006);
 for each x  V , c(x)  con(C0 )  { mS.C | nS.C  con(C0 ), and m < n} and
a(x)  qc(C0 , R);
  is a symmetric binary relation on V ;
 for each o  nom(C0 ), there is x  V1 such that o  c(x).
Following Horrocks et al. (2006) and Horrocks and Sattler (2007), we distinguish between
two sets of nodes: those in V1 can be arbitrarily interconnected (they are called root nodes),
while those in V2 form a tree structure (they are called internal nodes). Intuitively, a
completion graph is a collection of trees whose root nodes can be arbitrarily connected and
there may also be arcs from internal nodes to root nodes (see Fig. 2 on page 841). We also
distinguish between two sets of arcs: those in E1 connect nodes in the same tree, while
those in E2 are the remaining arcs in the graph.
To illustrate the difference between R-successors and neighbours, suppose (R0 v R)  R
x

l(x, y) = {R0 }

y

and l(x, y) = {R0 }, as in the picture above. Then y is both an R0 - and R-successor of x,
but x is neither an inv (R0 )- nor an inv (R)-successor of y; y is both an R0 - and R-neighbour
of x, and x is an inv (R0 )- and inv (R)-neighbour of y.
To ensure that the tableau algorithm eventually comes to a stop, we use a blocking
technique that is similar to the one of Horrocks et al. (2006). A node x  V2 is called
blocked if it is either directly or indirectly blocked. A node x  V2 is directly blocked if none
of its (not necessarily immediate) E1 -ancestors is blocked, and there are nodes x0 , y and y 0
such that:
 y 0 is not a root,
837

fiMosurovic, Krdzavac, Henson & Zakharyaschev

 (x0 , x)  E1 , (y 0 , y)  E1 and y is an E1 -ancestor of x0 ,
 c(x) = c(y), c(x0 ) = c(y 0 ), a(x) = a(y), a(x0 ) = a(y 0 ) and l(x0 , x) = l(y 0 , y).
In this case we say that y blocks x.
root

0
y 0 l(y , y) y

0
x0 l(x , x) x

A node y is indirectly blocked if one of its E1 -ancestors is blocked.
For a simple role S, x  V and C  con(C0 ), let
S G (x, C) ={y | y is an S-neighbour of x, C  c(y) and
if x  V1 then y is not indirectly blocked}.
We say that a completion graph G contains a clash if there is x  V such that at least one
of the following conditions holds:
   c(x),
 {A, A}  c(x), for a concept name A,
 {Ap .C, Ap .C}  a(x), for a quasi-concept Ap .C 

Sl

i=1 qc

 (r

i ),

 x is an S-neighbour of x and S.Self  c(x),
 Dis(R, S)  R, while y is both an R- and an S-neighbour of x, for some y  V ,
 ( nS.C)  c(x), while {y0 , . . . , yn }  S G (x, C) with yi  yj , for 0  i < j  n,
 for some o  nom(C0 ), there is node y  x with o  c(x)  c(y),
 C = (tr , t , t )  a(x) and a(x)|inv (tr ) 6 t .
A completion graph that does not contain a clash is called clash-free.
To simplify the tableau rules, we require some terminology and notation originally used
by Horrocks et al. (2006) and Horrocks and Sattler (2007). An R-neighbour y of x is
said to be safe if either x  V2 or x  V1 and y is not blocked. The result (and the
procedure) of pruning a node y in G = (V1 , V2 , E1 , E2 , c, a, l, ), denoted Prune(y), is the
graph obtained from G in the following way: we remove every (y, z) from E and, if z  V2 ,
Prune(z); we also remove y from V . The result (and the procedure) of merging nodes y
and x in G = (V1 , V2 , E1 , E2 , c, a, l, ), denoted Merge(y, x), is the graph obtained from G
as follows:
1. for all z such that (z, y)  E:
 if {(x, z), (z, x)}  E = , then add (z, x) to E (to E1 if x  V2 , otherwise to E2 )
and set l(z, x) := l(z, y),
 if (z, x)  E, then set l(z, x) := l(z, x)  l(z, y),
 if (x, z)  E, then set l(x, z) := l(x, z)  {inv (R) | R  l(z, y)}, and
838

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

 remove (z, y) from E;
2. for all root nodes z such that (y, z)  E2 :
 if {(x, z), (z, x)}  E = , then add (x, z) to E2 and set l(x, z) := l(y, z),
 if (x, z)  E, then set l(x, z) := l(x, z)  l(y, z),
 if (z, x)  E, then set l(z, x) := l(z, x)  {inv (R) | R  l(y, z)}, and
 remove (y, z) from E2 ;
3. set c(x) := c(x)  c(y) and a(x) := a(x)  a(y);
4. add x  z, for all z with y  z;
5. Prune(y).
Let G = (V1 , V2 , E1 , E2 , c, a, l, ) be a completion graph. The completion rules can extend
G in two ways: by adding a new leaf and by adding a new root. We say that a node x  V2 ,
with (y, x)  E1 , is of level i in the forest (V, E1 ) if either i = 1 and y  V1 , or i > 1 and
y  V2 is of level i  1 in (V, E1 ). A node x  V1 is of level i in the graph (V1 , E2  (V1  V1 ))
if either i = 0 and there exists o  nom(C0 ) such that o  c(x), or i > 0, x is not of level
 (i  1) in (V1 , E2  (V1  V1 )) and there is y  V1 of level i  1 in (V1 , E2  (V1  V1 )) with
(y, x)  E2 .
The tableau rules will be applied according to the following strategy: the (o)-rule is of
highest priority; after that we apply the (=r )- and (r )-rules, starting with root nodes of
lower levels; applications of all other rules follow.
Our tableau algorithm is non-deterministic. It takes a SR+ OIQ concept C0 and an
RBox R as input and returns yes or no to indicate whether C0 is satisfiable w.r.t. R or not.
The algorithm starts by constructing the completion graph G = (V1 , V2 , E1 , E2 , c, a, l, ),
where
 V1 = {xo | o  nom(C0 )}  {xC0 },
 V2 = ,
 E1 = , E2 = ,
 c(xo ) = {o}, c(xC0 ) = {C0 },
 a(xo ) = , a(xC0 ) = ,
 l = ,
  is empty.
Then the algorithm non-deterministically applies one of the completion rules given in Tables 1 and 2; it keeps doing so till either the current completion graph contains a clash, in
which case the answer is no, or none of the rules is applicable, in which case the algorithm
returns yes.
To prove that this algorithm always comes to a stop and returns a correct answer, we
require the following lemma:
839

fiMosurovic, Krdzavac, Henson & Zakharyaschev

Lemma 21 Let G = (V1 , V2 , E1 , E2 , c, a, l, ) be the structure constructed at some step of
the algorithm. Then, for every x  V2 , there exists exactly one y  V such that (y, x)  E1 .
Proof. The proof is by induction on the number of steps. The basis of induction (V2 = )
is trivial. So suppose that our claim holds for some step and consider what happens after
an application of a completion rule. By applying the rules (), (r8) and () to a node x,
we add one or more nodes to V2 , with x being the only predecessor of these nodes. Also,
we have to consider the rules (), (o) and (=r ), which merge nodes, because other rules
do not change V2 and E1 . Merging nodes changes the graph by possibly adding new edges,
deleting some edges and pruning some nodes. Observe that if we prune a node y then our
claim still holds because we delete the successors of y which belong to V2 . Deleting an edge
does not spoil the claim either. Thus, it is enough to examine the cases when a new edge
is added to E1 . If Merge(y, x) and x  V1 then all newly added edges belong to E2 , and so
after applying Merge(y, x) the claim still holds. The only interesting case is when we apply
Merge(z, y) in the rule () with ( nS.C)  c(x), y, z  S G (x, C) and y, z  V2 . Because
of y, z  V2 , the nodes y and z have only one parent node, although z is not an ancestor of
y; so the following fours cases are possible.
Case 1: (y, x)  E1 and (x, z)  E1 . Then no new edge is added to E1 , and the claim
holds.
Case 2: (x, y)  E1 and (x, z)  E1 . Again, we do not add a new edge to E1 .
Case 3: (y, x)  E1 and (z, x)  E1 . In this case x  V2 and x has two parent nodes y
and z, which is impossible by IH.
Case 4: (y, x)  E2 or (z, x)  E2 . This case is not possible either because x  V1 ,
( nS.C)  c(x), y (or z) is an S-neighbour of x, and so before applying () we have to
apply (r ) or (=r ) in view of their higher priority.
q
We can now show termination.
Lemma 22 The tableau algorithm always terminates.
Proof. The sets con(C0 ), qc(C0 , R), role(C0 , R) we use in the labels of nodes and edges
are finite. Let l0 = ]nom(C0 ), l1 = ]con(C0 ), l2 = ]qc(C0 , R), l3 = ]role(C0 , R) and
nmax = max{n | ( nR.C)  con(C0 ) or ( nR.C)  con(C0 )}. The completion graph
and the completion rules have following properties. Each node x is labelled with two sets
c(x)  con(C0 ) and a(x)  qc(C0 , R). The number of different pairs of such labels does not
exceed 2l1 +l2 . Each edge (x, y) is labelled with a set l(x, y)  role(C0 , R), so the number
of different labels of edges is at most 2l3 . The number of different labels for a pair of nodes
connected by an arc is at most L = 2l3 +2l1 +2l2 . Therefore, any path in the forest (V, E1 ),
which starts from a root node and is of length  L + 2, contains a blocked node. Every
application of any rule is determined by some (quasi-) concept and node, with the same rule
applicable to the same (quasi-) concept and node only once. The completion rules never
remove labels from nodes in the graph, and the only rules that remove nodes are (), (o)
and (=r ). Only (), (), (r ) and (r8) generate new nodes, and each such generation is
triggered by a (quasi-) concept of the form R.C,  nR.C,  nR.C or P.C in the label of
a node x. The number of the concepts is  l1 + l2 . The rules () and (r ) can generate at
most nmax successors of a given node, for each concept of the form  nR.C or  nR.C. The
840

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

other two rules generate only one successor for each concept. It follows that the number of
created outgoing arcs for a node does not exceed l1  nmax + l2 . If a node y is removed from
G by (), (o) or (=r ), its label migrates to the node z. So the rules (), (), (r ) and
(r8), which generate y that is later merged by (), (o) or (=r ), will not be applied again to
the same node.
Now, we show that the number of nodes in the completion graph is limited. Together
with the observations above, this will mean that we can apply the completion rules finitely
many times, and so the algorithm will eventually come to a stop.
To this end, we require the following claim: if x  V1 is of level i in (V1 , E2  (V1  V1 )),
y  V2 is not indirectly blocked and (y, x)  E2 , then y is of level  L + 2  i in (V, E1 ).
Indeed, for i = 0, y is of level  L + 2 because y is not indirectly blocked and each node of
> L + 2 level is indirectly blocked. If x is of level i > 0 then the only way to add an edge
(y, x)  E2 is first to apply the rule (o), which will add an E2 -edge between some y0  V2
and x0  V1 , and then repeatedly apply (=r ). The node x0 is of level 0 in (V1 , E2 (V1 V1 )),
while y0 is of level  L + 2 in (V, E1 ). If we apply the rule (=r ), then y0 will be merged
with some successor x1  V1 of x0 created by an application of (r ). The node x1 is of level
 1, and we add the edge (y1 , x1 )  E2 , where y1 is a parent of y0 and of level  L + 2  1
in (V, E1 ); see Fig. 2. By repeating the same argument, we see that the node y is of level
 L + 2  i in (V, E1 ).
x0

x0
x1

y1

y1

x1

y0

Figure 2: Before and after an application of (=r ); the bold arcs are in E2 and the nodes 
are in V1 .
The claim proved above means that if a node x  V1 is of level L+2 in (V1 , E2 (V1 V1 )),
then there is no y  V2 with (y, x)  E2 . Hence, the rule (r ) cannot be applied to a node
x of level L + 2, and so there is no root node of level L + 3.
The only rule that can add new nodes to V1 is (r ). It can be applied at most l1 times
to a given node and add at most l1  nmax successors. At the beginning V1 \{xC0 } contains
l0 nodes, so (r ) can create at most l0  l1  nmax successors of these nodes. By applying
(r ) to them again, we obtain  l0  (l1  nmax )2 new nodes (of level  2). It follows that
]V1  1 +

L+2
X

l0  (l1  nmax )i = O(l0  (l1  nmax )L+3 ).

i=0

The number of nodes in V2 is also limited. At the beginning V2 = . For each node in V1 ,
the algorithm can create  l1  nmax + l2 arcs that lead to nodes in V2 . Thus, the number
841

fiMosurovic, Krdzavac, Henson & Zakharyaschev

of nodes of level 1 in V2 does not exceed ]V1  (l1  nmax + l2 ); the number of their successors
is at most ]V1  (l1  nmax + l2 )2 ; and finally,
]V2 

L+2
X

]V1  (l1  nmax + l2 )i = O(]V1  (l1  nmax + l2 )L+3 ).

i=1

q

This completes the proof of the lemma.
The next lemma shows that the answers returned by the algorithm are correct.

Lemma 23 The tableau algorithm returns yes if and only if there exists a tableau for C0
w.r.t. R.
Proof. () Suppose the algorithm returns yes by generating a clash-free completion graph
G = (V1 , V2 , E1 , E2 , c, a, l, ) to which no completion rule is applicable. Let V = V1  V2
and E = E1  E2 . We write (x) = x, if x  V1 or x  V2 is not blocked; and (x) = y, if
x  V2 and y blocks x.
Define a set paths(G) inductively by taking (cf. Horrocks et al., 2006):
 if x0  V1 then (x0 , x0 )  paths(G); in this case we write Root(x0 ) = (x0 , x0 ),
 if   paths(G), a node z  V2 is not indirectly blocked and (tail (), z)  E1 , then
the sequence , ((z), z) is in paths(G).
Here tail () = xn and tail 0 () = x0n , for  = (x0 , x00 ), . . . , (xn , x0n ). The members  of
paths(G) will be called paths in G.
We now define a tableau T = (S, c0 , a0 , E) by taking S = paths(G), c0 () = c(tail ()),
a0 () = a(tail ())  qc(C0 , R), for   paths(G), and
E(R) ={(Root(x), Root(y)) | y is an R-neighbour of x} 
{(u, Root(y)) | y is an R-neighbour of tail (u)} 
{(Root(x), u) | tail (u) is an R-neighbour of x} 
{(u, u) | tail (u) is an R-neighbour of tail (u)} 
{(u, v)  S  S | v = u, ((y), y) and y is an R-neighbour of tail (u), or
u = v, ((y), y) and y is an inv (R)-neighbour of tail (v)}.
We prove that T is a tableau for C0 w.r.t. R. Indeed, (p1) and (p14) follow from the
initial step of the tableau algorithm and the fact that the labels of the root nodes are never
removed; (p2) follows from the definition and the fact that the completion graph G is clashfree; (p3) follows from the rules creating new nodes and that G is clash-free; (p9) and (p16)
follow from the definitions of E(R) and R-neighbour (and R-successor); (p6) and (p7) follow
from the fact that the rules (u) and (t) are not applicable; (p12) follows from the definitions
of E(R) and R-neighbour and the fact that the rule (guess) is not applicable; (p15) and (p5)
follow from the definitions of E(R) and R-neighbour and that the completion graph G is
clash-free; (p17) and (p18) follow from the fact that (r1) and (r3) are not applicable; (p19)
from the fact that (r5i ) cannot be applied; and (p20) and (p23) follow from the definitions
of E(R) and the fact that (r2) and (r6) are not applicable. The remaining cases are less
straightforward. In some of them, we require the following:
842

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

(u)

if C1 u C2  c(x), x is not indirectly blocked and {C1 , C2 } 6 c(x), then
c(x) := c(x)  {C1 , C2 }

(t)

if C1 t C2  c(x), x is not indirectly blocked and {C1 , C2 }  c(x) = , then
c(x) := c(x)  {D}, for some D  {C1 , C2 }

()

if S.C  c(x), x is not blocked and has no safe S-neighbour y with C  c(y)
then create a new node y  V2 with l(x, y) := {S}, c(y) := {C, >}, a(y) := 

(self)

if S.Self  c(x), x is not blocked and x is not S-neighbour of x
then add (x, x) to E2 , if it is not there yet, and set l(x, x) := l(x, x)  {S}

(guess)

if ( nS.C)  c(x), x is not indirectly blocked and
there is an S-neighbour y of x such that {C, C}  c(y) = ,
then set c(y) := c(y)  {D}, for some D  {C, C}

()

if ( nS.C)  c(x), x is not blocked and there are no distinct and safe
y1 , . . . , yn  S G (x, C), then create n new successors y1 , . . . , yn  V2 of x; set
l(x, yi ) := {S}, c(yi ) := {C, >}, a(yi ) := , yi  yj , for 1  i < j  n

()

if ( nS.C)  c(x), x is not indirectly blocked, ]S G (x, C) > n and there are
y, z  S G (x, C) for which y  z does not hold
then (1) if z is a root node or an E1 -ancestor of y, then Merge(y, z),
(2) otherwise Merge(z, y)

(o)

if, for o, o0  nom(C0 ), there is a node y 6= xo with o0  c(xo )  c(y)
and such that xo  y does not hold in the completion graph,
then Merge(y, xo )

(r )

if ( nS.C)  c(x), x  V1 , and there is an S-neighbour y of x
such that y  V2 , (y, x)  E2 , C  c(y) and y is not indirectly blocked;
and if there is no n0  n with ( n0 S.C)  c(x) and there are S-neighbours
z1 , . . . , zn0  V1 of x with C  c(zi ) and zi  zj , for 1  i, j  n0 , i 6= j,
then (1) guess m, 1  m  n, and set c(x) := c(x)  {( mS.C)},
(2) create m new nodes y1 , . . . , ym  V1 with
l(x, yi ) := {S}, c(yi ) := {C, >}, a(yi ) :=  and yi  yj , 1  i < j  m

(=r )

if ( mS.C)  c(x), x  V1 , and there is an S-neighbour y  V2 of x with
C  c(y), y is not indirectly blocked and there are S-neighbours
z1 , . . . , zm  V1 of x with C  c(zi ) and zi  zj , for 1  i, j  m, i 6= j, and
there is j0 , 1  j0  m for which y  zj0 does not hold,
then Merge(y, zj0 )
Table 1: Completion rules for the SR+ OIQ tableau algorithm.

843

fiMosurovic, Krdzavac, Henson & Zakharyaschev

(r1)

if R.C  c(x), x is not indirectly blocked and AsR .C 6 a(x), s the initial state,
then a(x) := a(x)  {AsR .C}

(r2)

if ApR .C  a(x), q  AR (p, T ) (where T can be ), x is not indirectly blocked,
y is a T -neighbour of x and AqR .C 6 a(y),
then a(y) := a(y)  {AqR .C}

(r3)

if AaR .C  a(x), a an accepting state, x is not indirectly blocked and C 
/ c(x),
then c(x) := c(x)  {C}

(r4i )

if x is not indirectly blocked and there is C  qc (r i ) with {C, C}  a(x) = ,
then a(x) := a(x)  {D}, for some D  {C, C}

(r5i )

if x is not indirectly blocked, (r4i ) is not applicable, Asi .C 6 a(x),
C = (r i , a(x)), for the initial state s, then a(x) := a(x)  {Asi .C}

(r6)

if Ap .C  a(x), x is not indirectly blocked, q  A (p, T ) (where T can be ),
y is a T -neighbour of x and Aq .C 6 a(y),
then a(y) := a(y)  {Aq .C}

(r7)

if Aa .C  a(x), a an accepting state, x is not indirectly blocked and C 
/ a(x),
then a(x) := a(x)  {C}

(r8)

if P.C  a(x), x is not blocked and x has no safe P -neighbour y with C  a(y),
then create a new node y  V2 and set l(x, y) := {P }, c(y) := {>}, a(y) := {C}
W
if C  a(x), C = m
j=1 Cj , x is not indirectly blocked, {C1 , . . . , Cm }  a(x) = ,
then a(x) := a(x)  {D}, for some D  {C1 , . . . , Cm }

(r9)
(r10)

if C  a(x), for C = (tr , t , t ), x is not indirectly blocked and t 6 a(x),
then set a(x) := a(x)  t
Table 2: Completion rules for the SR+ OIQ tableau algorithm (cont.)

844

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

Proposition 24 Suppose u  paths(G), x = tail (u) and x has a safe R-neighbour y  V .
Then there is v  paths(G) with (u, v)  E(R), c0 (v) = c(y) and a0 (v) = a(y)  qc(C0 , R).
Proof. As y is an R-neighbour of x, either (x, y)  E or (y, x)  E. Four cases are possible:
 If (x, y)  E1 , we set v = u, ((y), y).
 If (x, y)  E2 , then y  V1 and we set v = Root(y).
 If (y, x)  E1 then y is the only predecessor of x. In the case tail 0 (u) = x, there
exists a path v such that tail (v) = y and u = v, (x, x); and in the case tail 0 (u) 6= x
(i.e., when x blocks tail 0 (u)), there exist a predecessor y 0 of tail 0 (u) (c(y 0 ) = c(y),
a(y 0 ) = a(y) and y 0 is an R-neighbour of tail 0 (u)) and a path v such that tail (v) = y 0
and u = v, (x, tail 0 (u)).
 If (y, x)  E2 then x  V1 , u = Root(x). We set v = Root(y) if y  V1 . If y  V2 then
y is not blocked (since y is safe), and so there exists a path v such that tail (v) = y.
q

In all of these cases, v is as required.

(p4) If S.Self  c0 (u) then S.Self  c(tail (u)). Since (self) is not applicable, tail (u) is
an S-neighbour of tail (u), and so (u, u)  E(R).
(p8) If R.C  c0 (u) then R.C  c(x), where x = tail (u). Since () is not applicable, x
has a safe R-neighbour y with C  c(y). By Proposition 24, there exists v  paths(G) such
that (u, v)  E(R) and C  c0 (v).
(p10) If  nS.C  c0 (u) then  nS.C  c(x), where x = tail (u). Since the completion
graph G is clash-free and () and (=r ) are not applicable, ]S G (x, C)  n. Suppose that
(u, v)  E(R) and C  c0 (v). By the definition of E(R), the following cases are possible:
 u = Root(x), v = Root(y) and y is an R-neighbour of x. We have y  S G (x, C) and,
since y  V1 , there is no v 0  paths(G) different from v and such that y = tail (v 0 ) or
y = tail 0 (v 0 ).
 x  V2 , v = Root(y) and y is an R-neighbour of x. This case is considered analogously.
 u = Root(x), y = tail (v)  V2 , v 6= u, (y, tail 0 (v)) and y is an R-neighbour of x. This
case is not possible since  nS.C  c(x), x  V1 , (y, x)  E2 and the rules (r ) and
(=r ) are not applicable.
 v = u and x is an R-neighbour of x. Then x  S G (x, C) and there is no v 0  paths(G)
different from u and such that (u, v 0 )  E(R) and x = tail (v 0 ) or x = tail 0 (v 0 ).
 v = u, ((y), y) and y is an R-neighbour of x. Then y  S G (x, C), y  V2 and x is
the only predecessor of y. So there is no v 0  paths(G) different from v and such that
(u, v 0 )  E(R) and y = tail (v 0 ) or y = tail 0 (v 0 ).
 u = v, (x, y), x = (y) and y is an inv (R)-neighbour of tail (v). Then tail (v) 
S G (x, C), y  V2 and tail (v) is only one predecessor of y; so there is no v 0  paths(G)
different from v such that (u, v 0 )  E(R) and tail (v) = tail (v 0 ) or tail (v) = tail 0 (v 0 ).
845

fiMosurovic, Krdzavac, Henson & Zakharyaschev

Therefore, ]{v  S | (u, v)  E(S) and C  c(v)}  ]S G (x, C)  n.
(p11) If ( nS.C)  c0 (u) then ( nS.C)  c(x), where x = tail (u). Since () is not
applicable, x has safe S-neighbours y1 , . . . , yn with C  c(yi ) and yi  yj , for 1  i, j  n
and j 6= i. By Proposition 24, there exists vi  paths(G) such that (u, vi )  E(S) and
C  c0 (vi ), for 1  i  n. In addition, there can be at most one y with (y, x)  E1 and, by
the proof of Proposition 24, if (yi , x) 6 E1 then tail (vi ) = yi or tail 0 (vi ) = yi . So, vi and vj
are distinct for i 6= j, since tail (vi ) 6= tail (vj ) or tail 0 (vi ) 6= tail 0 (vj ) (in the case (yi , x)  E1 ,
(x, yj )  E1 and yi block yj , i.e., tail (vi ) = tail (vj ) = yi , we have tail 0 (vi ) 6= tail 0 (vj )).
(p13) If o  c0 (u)  c0 (v) then o  c(tail (u)), and so there is o0  nom(C0 ) such that
xo0 = tail (u) and u = Root(xo0 ). Similarly, there is o00  nom(C0 ) with xo00 = tail (v) and
v = Root(xo00 ). Since the rule (o) is not applicable, xo0 = xo00 , and soWu = v.
r  
i
(p22) Let Aai .C  a0 (u), where a is an accepting state and C = m
h=1 (th , th , th ). Then
Aai .C  a(x), where x = tail (u). Since (r7) cannot be applied, C  a(x), and since (r9) is
not applicable, there is j such that Cj = (trj , tj , t
j )  a(x). Now, as (r10) is not applicable,


0
we have tj  a(x); and since G is clash-free, a(x)|inv (tr )  t
j . Thus, tj  a (u) and
j

a0 (u)|inv (tr )  t
j .
j

(p21) Let Aai .C  a0 (u), where C = inv (Pimi ).    inv (Pi1 ).(tr , t , t ) and a is an
accepting state. Then Aai .C  a(tail (u)). We prove by induction on j that there is vj such
that inv (Pij ).    inv (Pi1 ).(tr , t , t )  a(tail (vj )). For j = mi , set vmi = u. As (r7) is not
applicable to tail (vmi ), inv (Pimi ).    inv (Pi1 ).(tr , t , t )  a(tail (vmi )), which establishes
the induction basis. Assume now that our claim holds for j and prove it for j 1. As tail (vj )
is not blocked and (r8) is not applicable, there is a safe inv (Pij )-neighbour yj1 of tail (vj )
such that inv (Pi(j1) ).    inv (Pi1 ).(tr , t , t )  a(yj1 ). By Proposition 24, there is
vj1  paths(G) with (vj , vj1 )  E(inv (Pij )) and inv (Pi(j1) ).    inv (Pi1 ).(tr , t , t ) 
a(tail (vj1 )). For j = 0, we have (tr , t , t )  a(tail (v0 )). Further, as (r10) cannot be
applied, we have t  a(tail (v0 )); and since G is clash-free, a(tail (v0 ))|inv (tr )  t . Thus,
t  a0 (v0 ) and a0 (v0 )|inv (tr )  t .
() Take a tableau T = (S, c0 , a0 , E) for C0 w.r.t. R and extend it in the following way:
(e1) If Aai .C  a0 (u), where C = inv (Pimi ).    inv (Pi1 ).(tr , t , t ) and a is an accepting
state, then, by (p21), there are v0 , v1 , . . . , vmi = u such that (vj , vj1 )  E(inv (Pij )),
for 1  j  mi , t  a0 (v0 ) and a0 (v0 )|inv (tr )  t . In this case, we extend a0 (vj ) by
taking a0 (vj ) := a0 (vj )  {inv (Pij ).    inv (Pi1 ).(tr , t , t )}, for 0  j  mi .
W i r  
(e2) If Aai .C  a0 (u), where C = m
h=1 (th , th , th ), then, by (p22), there is j  {1, . . . , mi }
0
such that tj  a0 (u) and a0 (u)|inv (tr )  t
j . In this case, we extend a (u) by taking
j

a0 (u) := a0 (u)  {C, Cj }.
S
(e3) If there exists C  li=0 qc (r i ) such that C 6 a0 (u), then a0 (u) := a0 (u)  {C}.
(e4) If ( nS.C)  c0 (u) and S T (u, C) = {v  S | (u, v)  E(S), C  c(v)} = {v1 , . . . , vm }
then, in view of (p10), we have m  n. In this case, we extend c0 (u) by taking
c0 (u) := c0 (u)  { mS.C}.
We now apply the completion rules using the extended tableau T so that in the end the
algorithm obtains a clash-free completion graph G = (V1 , V2 , E1 , E2 , c, a, l, ) and returns
846

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

yes. For this purpose, we define a map  : V  S and steer the applications of the nondeterministic completion rules in such a way that c(x)  c0 ((x)) and a(x)  a0 ((x)), for
all nodes x  V (cf. Horrocks, Kutz, & Sattler, 2005; Horrocks et al., 2006). Furthermore,
we require that, for each pair of nodes x, y and each role R, if y is an R-successor of x, then
((x), (y))  E(R), and x  y implies (x) 6= (y). This will ensure that G is clash-free,
since tableau T is clash-free.
We define  by induction as follows. To begin with, by (p14), for each o  nom(C0 ),
there is some vo with o  c0 (vo ), and by (p1), there is some u0 with C0  c0 (u0 ). The
algorithm starts by constructing nodes xo , for each o  nom(C0 ), and xC0 with c(xo ) = {o}
and c(xC0 ) = {C0 }. We set (xo ) = vo and (xC0 ) = u0 .
Observe that c(xo )  c0 ((xo )) and c(xC0 )  c0 ((xC0 )); also a(xo ) =   a0 ((xo )) and
a(xC0 ) =   a0 ((xC0 )). We now consider applications of the completion rules.
If (u) can be applied to x  V with C1 u C2  c(x), then C1 u C2  c0 ((x)), and so, by
(p6), C1 , C2  c0 ((x)). When we apply (u), C1 , C2 are added to c(x), so we again
have c(x)  c0 ((x)).
If (t) can be applied to x  V with C1 t C2  c(x), then C1 t C2  c0 ((x)), and so,
by (p7), {C1 , C2 }  c0 ((x)) 6= . We apply (t) so that c(x) := c(x)  {D} for some
D  {C1 , C2 }  c0 ((x)), and again c(x)  c0 ((x)).
If () can be applied to x  V with S.C  c(x), then S.C  c0 ((x)), and so, by (p8),
there is a v with ((x), v)  E(S) and C  c0 (v). By (p3), >  c0 (v) and, by (p16)
for S v S 0 , we have ((x), v)  E(S 0 ). We apply () so that a new node y is created
with l(x, y) := {S}, c(y) := {C, >} , a(y) :=  and (y) = v. But then c(y)  c0 ((y)),
a(y)  a0 ((y)) and ((x), (y))  E(S 0 ).
If (self) can be applied to x  V with S.Self  c(x), then S.Self  c0 ((x)), and so,
by (p4), ((x), (x))  E(S). By (p16) for S v S 0 , we have ((x), (x))  E(S 0 ).
We apply (self) by adding the arc (x, x), if it is not there yet, and setting l(x, x) :=
l(x, x)  {S}. Then we obtain ((x), (x))  E(S 0 ).
If (guess) can be applied to x  V with ( nS.C)  c(x) and an S-neighbour y of x, then
( nS.C)  c0 ((x)), ((x), (y))  E(S), and so, by (p12), {C, C}  c0 ((y)) 6= .
We apply (guess) so that c(y) := c(y)  {D}, for some D  {C, C}  c0 ((y). Hence
c(y)  c0 ((y)).
If () can be applied to x  V with ( nS.C)  c(x), then ( nS.C)  c0 ((x)). By (p11),
there are v1 , . . . , vn  S T ((x), C), where S T (u, C) = {v  S | (u, v)  E(S), C 
c(v)}. We apply () by creating n new successors y1 , . . . , yn of x and setting l(x, yi ) :=
{S}, c(yi ) := {C, >}, a(yi ) := , yi  yj and (yi ) = vi , for 1  i, j  n, j 6= i. Then,
for S v S 0 , we have ((x), (yi ))  E(S 0 ) and also c(yi )  c0 ((yi )), for 1  i  n.
If () can be applied to x  V with ( nS.C)  c(x) and {y1 , . . . , yn+1 }  S G (x, C), then
( nS.C)  c0 ((x)) and {(y1 ), . . . , (yn+1 )}  S T ((x), C). By (p10), we have
]S T ((x), C)  n, so there are j1 , j2 such that (yj1 ) = (yj2 ) = v. Instead of yj1 ,
yj2 , we will write y, z; more precisely if yj1 is a root or an E1 -ancestor of yj2 then
847

fiMosurovic, Krdzavac, Henson & Zakharyaschev

we set z = yj1 and y = yj2 , otherwise we set z = yj2 and y = yj1 . We apply () by
performing Merge(y, z). Since (z) = v, the required conditions on  hold.
For (=r ), the proof is similar to the previous case.
If (o) can be applied to y  V with o0  c(xo )  c(y), for some o, o0  nom(C0 ), then
o0  c0 ((xo ))  c0 ((y)). By (p13), we have (xo ) = (y), and therefore c(xo )  c(y) 
c0 ((xo ))  c0 ((y)) = c0 ((xo )). Similarly, a(xo )  a(y)  a0 ((xo )). We apply (o) by
performing Merge(y, xo ), so the required conditions for  hold again.
If (r ) can be applied to x  V1 and an S-neighbour y of x with ( nS.C)  c(x),
y  V2 , (y, x)  E2 and C  c(y), then ( nS.C)  c0 ((x)), ((x), (y))  E(S) and
C  c0 ((y)). By (p10), we have ]S T ((x), C)  n, so S T ((x), C) = {v1 , . . . , vm },
m  n. We apply (r ) so that c(x) := c(x)  {( mS.C)}, create m new nodes
y1 , . . . , ym  V1 with l(x, yi ) := {S}, c(yi ) := {C, >}, a(yi ) := , yi  yj and (yi ) = vi
for all 1  i  m, 1  j < i. Then, for S v S 0 , we have ((x), (yi ))  E(S 0 ),
c(yi )  c0 ((yi )), for 1  i  m, and also, by (e4), c(x)  c0 ((x)).
If (r1) can be applied to x  V with R.C  c(x), then R.C  c0 ((x)), and so, by
(p17), AsR .C  a0 ((x)), where s is the initial state of AR . We apply (r1) so that
a(x) := a(x)  {AsR .C}. Clearly, we have a(x)  a0 ((x)).
If (r2) can be applied to x  V with ApR .C  a(x), q  AR (p, T ), and y is a T -neighbour
of x, then ApR .C  a0 ((x)). If T 6=  then ((x), (y))  E(T ) and, by (p20),
AqR .C  a0 ((y)). If T =  then y = x and, by (p23), AqR .C  a0 ((y)). In both
cases we apply (r2) so that a(y) := a(y)  {AqR .C}, and again a(y)  a0 ((y)).
If (r3) can be applied to x  V with AaR .C  a(x), where a is an accepting state, then
AaR .C  a0 ((x)). By (p18), C  c0 ((x)). We apply (r3) so that c(x) := c(x)  {C}.
Thus, c(x)  c0 ((x)).
If (r4i ) can be applied to x  V with C  qc (r i ), then, by (e3), {C, C}  a0 ((x)) 6= .
We apply (r4i ) so that a(x) := a(x)  {D}, for some D  {C, C}  a0 ((x)). Thus,
a(x)  a0 ((x)).
If (r5i ) can be applied to x  V , then Asi .C 6 a(x), where s is the initial state of Ai and
C = (r i , a(x)). By (p19), Asi .C0  a0 ((x)), where C0 = (r i , a0 ((x))). Suppose
C 6= C0 . Since a(x)  a0 ((x)), there exists C1  qc (r i ) such that C1  a0 ((x)) and
C1 6 a(x). As (r4i ) is not applicable, we have C1  a(x), and so C1  a0 ((x)),
which is a contradiction. Hence C = C0 . We apply (r5i ) so that a(x) := a(x){Asi .C}.
Thus, a(x)  a0 ((x)).
If (r6) can be applied to x  V with Ap .C  a(x), q  AR (p, T ), and y is a T -neighbour
of x, then Ap .C  a0 ((x)). If T 6=  then ((x), (y))  E(T ) and, by (p20),
Aq .C  a0 ((y)). If T =  then y = x and, by (p23), Aq .C  a0 ((y)). In either case,
we apply (r6) so that a(y) := a(y)  {Aq .C}. Thus, a(y)  a0 ((y)).
If (r7) can be applied to x  V with Aa .C  a(x), where a is an accepting state, then
Aa .C  a0 ((x)). By (e1) and (e2), C  a0 ((x)). We apply (r7) in such a way that
a(x) := a(x)  {C}. Thus, a(x)  a0 ((x)).
848

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

If (r8) can be applied to x  V with P.C  a(x), then P.C  a0 ((x)), and so, by (e1),
there is some v with ((x), v)  E(P ) and C  a0 (v). By (p16) for P v S 0 , we
have ((x), v)  E(S 0 ). We apply (r8) by creating a new node y with l(x, y) := {P },
c(y) := {>}, a(y) := {C} and (y) = v. Thus, c(y)  c0 ((y)), a(y)  a0 ((y)) and
((x), (y))  E(S 0 ).
W
0
If (r9) can be applied to x  V with C  a(x), for C = m
j=1 Cj , then C  a ((x)). This
0
means that C is added to a ((x)) by (e2), and so, there is j such that Cj  a0 ((x)).
We apply (r9) so that a(x) := a(x)  {Cj }. Thus, a(x)  a0 ((x)).
If (r10) can be applied to x  V with C  a(x), for C = (tr , t , t ), then C  a0 ((x)). This
means that C is added to a0 ((x)) by (e1) for (x) = v0 , or by (e2). In either case,
t  a0 ((x)). We apply (r10) so that a(x) := a(x)  t . Thus, a(x)  a0 ((x)).
q

This completes the proof of the lemma.

As an immediate consequence of Lemmas 19, 22 and 23, we obtain our main Theorem 15
according to which concept satisfiability w.r.t. SR+ OIQ KBs is decidable. It is worth
noting that if the given RBox R does not contain RAs of the form (C)(F) then our tableau
algorithm behaves in the same way as the algorithm for SROIQ (Horrocks et al., 2006).
However, if R contains one RA of the form (C)(F) the algorithm will have to construct
the set qc(C0 , R) of quasi-concepts, which contains subsets of the previously constructed
sets of quasi-concepts qc(r 0 ), and so may suffer an exponential blow-up. More precisely,
the new quasi-concepts in qc(C0 , R) are built from triples of the form (tr , t , t ), where
t  qc(r 0 )|tr and t  qc(r 0 )|inv (tr ) . Furthermore, the algorithm may suffer one more
exponential blow-up every time we add an extra RA of the form (C)(F) and extend the
sequence r i1 C r i2 , r i2 C r i3 , . . . , r ih1 C r ih because again the set of quasi-concepts may
become exponentially larger.

References
Baader, F. (2003). Restricted role-value-maps in a description logic with existential restrictions and terminological cycles. In Proccedings of the 2003 International Workshop
on Description Logics (DL2003).
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). The Description Logic Handbook: Theory, Implementation and Applications.
CUP. (2nd edition, 2007).
Baldoni, M. (1998). Normal Multimodal Logics: Automatic Deduction and Logic Programming Extension. Ph.D. thesis, Universita degli Studi di Torino.
Bock, C., Zha, X., Suh, H., & Lee, J. (2010). Ontological product modeling for collaborative
design. Advanced Engineering Informatics, 24, 510524.
Bock, C. (2004). UML 2 Composition Model. Journal of Object Technology, 3 (10), 4774.
Brachman, R. J., & Schmolze, J. G. (1985). An overview of the KL-ONE knowledge representation system. Cognitive Science, 9 (2), 171216.
849

fiMosurovic, Krdzavac, Henson & Zakharyaschev

Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U.
(2008). OWL 2: The next step for OWL. Journal of Web Semantics, 6 (4), 309322.
del Cerro, L. F., & Penttonen, M. (1988). Grammar logics. Logique et Analyse, 121, 123134.
Demri, S. (2001). The complexity of regularity in grammar logics and related modal logics.
Journal of Logic and Computation, 11 (6), 933960.
Halpern, J., & Moses, Y. (1992). A guide to completeness and complexity for modal logics
of knowledge and belief. Artificial Intelligence, 54 (2), 319379.
Horrocks, I., Kutz, O., & Sattler, U. (2005). The irresistible SRIQ. In Proc. of the First
OWL Experiences and Directions Workshop.
Horrocks, I., Kutz, O., & Sattler, U. (2006). The even more irresistible SROIQ. In Proceedings of the Tenth International Conference on Principles of Knowledge Representation
and Reasoning (KR 2006), pp. 5767.
Horrocks, I., & Sattler, U. (2004). Decidability of SHIQ with complex role inclusion axioms.
Artificial Intelligence, 160 (1-2), 79104.
Horrocks, I., & Sattler, U. (2007). A tableau decision procedure for SHOIQ. Journal of
Automated Reasoning, 39 (3), 249276.
Kazakov, Y. (2010). An extension of complex role inclusion axioms in the description logic
SROIQ. In Proceedings of IJCAR, pp. 472486.
Krdzavac, N., & Bock, C. (2008). Reasoning in manufacturing part-part examples with
OWL2. Tech. rep., U.S. National Institute of Standards and Technology, Gaithersburg,
United States of America.
Motik, B., Cuenca Grau, B., Horrocks, I., & Sattler, U. (2009). Representing ontologies
using description logics, description graphs, and rules. Artificial Intelligence, 173 (14),
12751309.
Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning for description logics.
Journal of Artificial Intelligence Research (JAIR), 36 (1), 165228.
Rector, A. (2002). Analysis of propagation along transitive roles: formalisation of the
GALEN experience with medical ontologies. In Proceedings of the International Workshop on Description Logics 2002 (DL2002).
Schmidt-Schau, M. (1989). Subsumption in KL-ONE is undecidable. In Proccedings of the
1st Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR89),
pp. 421431.
Simanck, F. (2012). Elimination of complex RIAs without automata. In Proceedings of the
International Workshop on Description Logics 2012 (DL2012).
Tseitin, G. (1956). Associative calculi with undecidable equivalence problems. Dokl. Akad.
Nauk SSSR, 107 (3), 370371. (In Russian).
Wessel, M. (2001). Obstacles on the way to qualitative spatial reasoning with description
logics: some undecidability results. In Proceedings of the International Workshop on
Description Logics (DL2001).
850

fiA Decidable Extension of SROIQ with Complex Role Chains and Unions

Wessel, M. (2002). On spatial reasoning with description logics. In Proceedings of the
International Workshop on Description Logics 2002 (DL2002).

851

fiJournal of Artificial Intelligence Research 47 (2013) 697740

Submitted 05/13; published 08/13

Heuristic Search When Time Matters
Ethan Burns
Wheeler Ruml

eaburns at cs.unh.edu
ruml at cs.unh.edu

Department of Computer Science
University of New Hampshire
Durham, NH 03824 USA

Minh B. Do

minh.b.do at nasa.gov

Planning and Scheduling Group
SGT Inc.
NASA Ames Research Center
Moffett Field, CA 94035 USA

Abstract
In many applications of shortest-path algorithms, it is impractical to find a provably
optimal solution; one can only hope to achieve an appropriate balance between search
time and solution cost that respects the users preferences. Preferences come in many
forms; we consider utility functions that linearly trade-off search time and solution cost.
Many natural utility functions can be expressed in this form. For example, when solution
cost represents the makespan of a plan, equally weighting search time and plan makespan
minimizes the time from the arrival of a goal until it is achieved. Current state-of-theart approaches to optimizing utility functions rely on anytime algorithms, and the use
of extensive training data to compute a termination policy. We propose a more direct
approach, called Bugsy, that incorporates the utility function directly into the search,
obviating the need for a separate termination policy. We describe a new method based on
off-line parameter tuning and a novel benchmark domain for planning under time pressure
based on platform-style video games. We then present what we believe to be the first
empirical study of applying anytime monitoring to heuristic search, and we compare it
with our proposals. Our results suggest that the parameter tuning technique can give the
best performance if a representative set of training instances is available. If not, then Bugsy
is the algorithm of choice, as it performs well and does not require any off-line training.
This work extends the tradition of research on metareasoning for search by illustrating the
benefits of embedding lightweight reasoning about time into the search algorithm itself.

1. Introduction
Many problems in artificial intelligence can be formulated as shortest path problems, which
can be solved using heuristic search algorithms such as A* (Hart, Nilsson, & Raphael,
1968). Unfortunately, because state spaces often grow exponentially with problem size, it is
usually infeasible to find optimal solutions to shortest path problems of practical interest.
Instead, practitioners tend to settle for suboptimal solutions, which can often be found more
efficiently but will be more expensive to execute. One is left with the choice of spending a
long time searching for a cheap solution, or a little time searching for an expensive one. We
argue for a new approach that is not strictly concerned with optimizing solution cost, but
with optimizing a utility function given in terms of both solution cost and search time. With
c
2013
AI Access Foundation. All rights reserved.

fiBurns, Ruml, & Do

such a utility function, a user can specify a preference between search time and solution
cost, and the algorithm handles the rest.
We consider utility functions given as a linear combination of search time and solution
cost. This is an important form of utility function for two reasons. First, it is easily elicited
from a user if not already explicitly in their application domain. For example, if cost is
given in monetary terms, it is usually possible to ask how much time one is willing spend to
decrease the solution cost by a certain amount. Second, if the solution cost is given in terms
of time (i.e., the cost represents the time required for the agent to execute the solution), then
this form of utility function can be used to optimize what we call goal achievement time;
by weighting search time and execution time equally, a utility-aware search will attempt to
minimize the sum of the two, thus attempting to behave such that the agent will achieve
its goal as quickly as possible.
Most existing techniques for this problem are based on anytime algorithms (Dean &
Boddy, 1988), a general class of algorithms that emit a stream of solutions of decreasing
cost until converging on an optimal one. With sufficient knowledge about the performance
profile of an anytime algorithm, which represents the probability that it will decrease its
solution cost by a certain amount given its current solution cost and additional search time,
it is possible to create a stopping policy that is aware of the users preference for trading
solving time for solution cost (Hansen & Zilberstein, 2001; Finkelstein & Markovitch, 2001).
There are two disadvantages to using anytime algorithms to trade-off solving time for
solution cost. The first is that the profile of the anytime algorithm must be learned off-line
on a representative set of training instances. In many settings, such as domain-independent
planning, the problem set is unknown, so one cannot easily assemble a representative training set. Also, it is often not obvious which parameters of a problem affect performance so
it can be difficult to tell if a problem set is representative. Even if an instance generator
is available, the instances that it generates may not represent those seen in the real world.
The second issue is that, while the stopping policy is aware of the users preference for time
and cost, the underlying anytime algorithm is oblivious and will emit the same stream of
solutions regardless of the desired trade-off. The policy must simply do the best that it can
with the solutions that are found, and the algorithm may waste a lot of time finding many
solutions that will simply be discarded. Only the search algorithm itself is fully aware of
the possible candidate solutions that are available and their relative estimated merits.
This paper presents four main contributions. First, we combine anytime heuristic search
with the dynamic programming-based monitoring technique of Hansen and Zilberstein
(2001). To the best of our knowledge, we are the first to apply anytime monitoring to
anytime heuristic search. Second, we present a very simple portfolio-based method that
estimates a good parameter to use for a bounded-suboptimal search algorithm to optimize
a given utility function. Third, we present Bugsy, a best-first search algorithm that does
not rely on any off-line training, yet accounts for the users preference between search time
and solution cost.1 One important difference between Bugsy and most previous proposals
for trading-off deliberation time and solution cost is that Bugsy considers the trade-off directly in the search algorithm, whereas previous techniques, such as those based on anytime
algorithms, only consider the trade-off externally to the actual search algorithm. Finally,
1. A previous version of Bugsy was proposed by Ruml and Do (2007), see Appendix A for a discussion of
the improvements incorporated in the version presented here.

698

fiHeuristic Search When Time Matters

we present the results of a set of experiments comparing the portfolio-based method, anytime monitoring, and Bugsy, along with utility-oblivious algorithms such as A* and greedy
best-first search, real-time search algorithms, and decision-theoretic A* (DTA*, Russell &
EricWefald, 1991), a previously proposed utility-aware search. While there has been much
work discussing the trade-off between deliberation and solution cost, to the best of our
knowledge we are the first to implement and thoroughly evaluate many of these ideas in the
context of heuristic search.
The results of our experiments reveal two surprises. First, if a representative set of
training instances is available, the most effective approach is the very simple technique of
selecting a bound to use for a bounded-suboptimal search. Surprisingly, this convincingly
dominates anytime algorithms with monitoring in our tests. Second, neither Bugsy or
anytime search with monitoring dominates the other. Bugsy does not require any off-line
training, yet surprisingly, Bugsy can perform as well as the methods that use training data.
If a representative problem set is not available, then Bugsy is the algorithm of choice. This
work extends the tradition of research on metareasoning for planning by illustrating the
benefits of embedding lightweight reasoning about time into the search algorithm itself.

2. Background
In this section we briefly describe heuristic search, present some terminology used in the
remainder of this paper, and discuss the type of utility functions we are addressing.
2.1 Heuristic Search
As considered in this paper, heuristic search is a technique for finding the shortest path
between nodes in a weighted graph; many problems can be specified in this form. Since it
is typical for these graphs to be much too large to represent explicitly, algorithms usually
generate the graph lazily using a function called expand. The expand function returns the
successors of a node in the graph. We call the process of evaluating the expand function on
a node expanding the node, and when expanding a node we say that we are generating its
successors.
A* (Hart et al., 1968) is probably the best-known heuristic search algorithm. It maintains two sets of nodes: the open list contains the frontier nodes that have been generated
but not yet expanded, and the closed list contains nodes that have already been expanded
(a common optimization is is for the closed list to also include nodes that are already on the
open list too), and therefore represent duplicate states if encountered again. The open list
is sorted on f (n) = g(n) + h(n), where g(n) is the cost of the path from the initial node to
node n, and h(n) is the heuristic estimate of the cheapest path cost from n to any goal node
reachable from n. The algorithm proceeds by removing a node with the minimum f value
from the open list, expanding it, putting its children on the open list, and putting the node
on the closed list. If A* removes a goal node from its open list, then it stops searching and
returns the path to the goal as the solution. Finally, if the heuristic never over-estimates
the cost to go then it is called admissible. With an admissible heuristic, A* returns optimal
solutions.
Dechter and Pearl (1988) prove that if the heuristic satisfies a property called consistency
(for all nodes n and m, h(n)  h(m) + c(n, m), where c(n, m) is the cost of the cheapest
699

fiBurns, Ruml, & Do

path between n and m), then A* expands the fewest possible nodes required to prove the
optimality of its solution with the given heuristic. In practice A* often takes too long
(Helmert & Roger, 2008), thus given its optimal efficiency it is infeasible to look for optimal
solutions to many problems. Instead, one must settle for suboptimal solutions, with the
hope that it is possible to find a sufficiently cheap solution within a reasonable amount of
time and memory.
2.2 Suboptimal Search
Greedy best-first search (Michie & Ross, 1969) is a popular suboptimal search algorithm.
It proceeds like A*, but orders its open list only on the heuristic, h(n), with the idea that
remaining search effort correlates with remaining solution cost. In other words, it assumes
that it will be easier to find a path to the goal from nodes with low h. When strictly
attempting to minimize search time, Thayer and Ruml (2009) show that greedy best-first
search on a different heuristic, d, can be more effective. Instead of estimating cost to go, as
is done by traditional h functions, the d heuristic, called a distance estimate, estimates the
number of remaining search nodes on the path to the cheapest solution beneath a node. In
practice, distance estimates are as readily available as cost-to-go heuristics and can provide
much better performance when used in greedy best-first search on domains where less cost
to go is not directly correlated with less search to go. We call greedy best-first search using
the d heuristic Speedy search, in analogy to greedy search.
While greedy best-first search can find solutions very quickly, there is no bound on the
cost of its solutions. Bounded-suboptimal search algorithms remedy this problem. Weighted
A* (Pohl, 1970) is perhaps the most common of these techniquesit proceeds just like A*,
but it orders the open list on f  (n) = g(n) + w  h(n), with w  1. The weighting parameter,
w, puts more emphasis on the heuristic estimate than the cost of arriving at a node, thus
it is greedier than A* and it often finds suboptimal solutions much faster than A* finds
optimal ones. In addition, the weight provides a bound on the suboptimality of its solutions:
the solutions are no more than w times the cost of the optimal solution (Pohl, 1970). Unlike
greedy best-first search, weighted A* lets the user select a weight, allowing it to provide
either cheaper solutions or faster solutions depending on their needs.
We refer the reader to the work of Thayer (2012) for a more in-depth study of suboptimal
and bounded-suboptimal search algorithms, including many that use d heuristics.
2.3 Utility Functions
So far, we have described A*, which optimizes solution cost, bounded-suboptimal search,
which finds solutions within a constant factor of optimal, and greedy best-first search, which
attempts to minimize solver time. Often, none of these are really desired: optimal solutions
require an impractical amount of resources, one rarely requires solutions strictly within a
given bound of optimal, and unboundedly suboptimal solutions are too costly. Instead, we
propose optimizing a simple utility function given as a linear combination of search time
and solution cost:
U (s, t) = (wf  g  (s) + wt  t)
(1)
where s is a solution, g  (s) is the cost of the solution, t is the time at which the solution is
returned, wf and wt are user-specified weights used to express a preference for trading-off
700

fiHeuristic Search When Time Matters

search time and solution cost. The number of time units that the user is willing to spend to
achieve an improvement of one cost unit is wf /wt . This quantity is usually easily elicited
from users if it is not already explicit in the application domain. The cost of the empty
solution, g  ({}), is a user-specified value that defines the utility achieved in the case that
the search gives up without returning a solution
A linear utility function has two main benefits. First, they are fairly expressive. For
example, one can optimize for cost if the both the solution and search time are given in
monetary terms. This situation can occur in cloud computing environments where computation time costs money. A linear utility function can also capture optimal or greedy search
by using 0 for the weight on execution time and solution cost respectively. Additionally, a
linear utility function can express goal achievement time by weighting search time equally
with solution makespan. Some practical examples where minimizing goal achievement time
is desired include robotic and video game pathfinding problems. In these settings, a user
often does not care about optimal solutions if they take too long to find, they may only
care about achieving the goal as quickly as possible.
As a demonstration of minimizing goal achievement time, we have made a video of
A*, Speedy search, and Bugsy solving a pathfinding video game pathfinding problem.
It is available in the online appendix of this paper or on the web: http://youtu.be/
Yluf88V1PLU. The video includes three panels, each showing an agent using a different
search algorithm. Since they do not focus on finding cost-optimal solutions, both the
Speedy and Bugsy agents begin moving almost immediately. The A* agent stands still for
a long time while it plans an optimal path, and it doesnt start moving until after Bugsy
has arrived at the goal. While all of this is occurring, the Speedy agent is following an
extremely circuitous path; it doesnt reach the goal until approximately 30 seconds after
A*. We didnt show these agonizing seconds in the video, and instead stopped the recording
as soon as A* reached the goal. Clearly, the Bugsy agent, which optimizes goal achievement
time, not solution cost or search time, is preferred in this scenario.
While quite expressive, linear utility functions are also rather simple. One main benefit
of the simplicity is that, with a fixed utility function, the passage of time decays all utility
values at the same rate. This simplification allows us to ignore all time that has passed before
the current decision point. We can then express utility values in terms of the utility of each
outcome starting at the current moment in time. Without this benefit, the mere passage
of time would change the relative ordering between the utilities of different outcomes; we
would need to re-compute all utility values at every point in time in order to select the best
outcome.
We only consider linear utility functions in this work, but it should be noted that one
could consider other more expressive functions. Step functions, for example, can represent
deadlines where after a certain amount of time has elapsed the utility of acting greatly
decreases. Bugsy does not support such functions, but the anytime monitoring technique
discussed in Section 3.1 has no restrictions on the utility functions that it can optimize.
Anytime monitoring can naturally handle more expressive functions, like step functions.

3. Previous Work
Next we describe some previous techniques for trading-off solver time for solution cost.
701

fiBurns, Ruml, & Do

3.1 Monitoring Anytime Algorithms
Much previous work in optimizing utility functions of solving time and cost, such as Equation 1, has focused on finding stopping policies for anytime algorithms. Anytime algorithms
(Dean & Boddy, 1988) are a general class of algorithms that find not one solution, but
a stream of solutions with strictly decreasing cost. They get their name because one can
stop an anytime algorithm at any time to get its current best solution. Anytime algorithms
are an attractive candidate for optimizing a utility function: since there is more than just
a single solution from which to pick, there is more opportunity to choose a solution with
a greater utility than when using an algorithm that just finds a single solution. Different
solutions will be found at different times, and if we knew the time at which the algorithm
would find each of its solutions and the cost of those solutions, then we could compute
their utilities and return the solution that maximizes utility. Unfortunately, it is usually
not possible to know what solutions an anytime algorithm will return without running it.
Instead, while the algorithm is running, one must continually make the decision: stop now,
or keep going?
Deciding when to stop is no easy task, because the utility of a solution depends not only
on its cost but also on the time needed to find it. On one hand, stopping early can reduce
the amount of computation time at the expense of having a more costly solution. On the
other hand, if the algorithm continues, it may not reduce the solution cost by enough to
justify the extra computation time. In this case, the final utility can be worse than it would
have been had the algorithm stopped earlier. With a little extra information, however, it
is possible to create a reasonable policy.
The Near Optimal Response-Time Algorithm (NORA, Shekhar & Dutta, 1989) provides
one very simple stopping policy for optimizing goal achievement time. NORA, simply stops
the anytime algorithm when the current search time is a user-specified factor of the current
incumbent solutions execution time. Shekhar and Dutta (1989) prove that the, if the search
stops when the time is a factor  of the incumbent solution cost, then the goal achievement
time will be within a factor of min(1 + , 1 + 1 ) of the optimal goal achievement time.
Our use of NORA is slightly different than that of Shekhar and Dutta (1989). They
did not apply NORA to anytime heuristic search. Instead, they evaluated it empirically
on database query optimization problems, which are tree search problems, where every leaf
node is a possible solution. They also describe how one could use NORA in an A* search,
but they make the assumption that if A* is stopped early without reaching the goal then
a heuristic planning procedure can be used to achieve the goal after executing the partial
solution found by A*. Such a procedure is often not available. When using NORA with
anytime heuristic search, as we do here, each incumbent solution is guaranteed to reach
the goal. The only disadvantage is that, as with all anytime stopping policies, it cannot do
better than the best solution found by the utility-oblivious anytime algorithm.
NORA finds a solution within a specified bound on the optimal goal achievement time.
Instead, Hansen and Zilberstein (2001) present a dynamic programming-based technique
for building an optimal stopping policy for any utility function. It requires one extra piece
of information: the profile of the anytime algorithm. Hansen and Zilberstein define the
profile as a probability distribution over the cost of the solution returned by the algorithm,
conditioned on its current solution cost and the additional time it is given to improve
702

fiHeuristic Search When Time Matters

its solution: P (qj |qi , t), where qj and qi are two possible solution costs and t is the
additional time. The profile allows for reasoning about how the solution cost may decrease
if the algorithm is given more time to improve it. While this requires extra knowledge, we
performed a small experiment (not shown here) and found that the optimal policy found
using dynamic programming performs better than the simpler NORA technique.
Hansen and Zilbersteins technique monitors the progress of the anytime algorithm by
evaluating the stopping policy at discrete time intervals. If the algorithm considers stopping
every t time units, then the utility achievable at time t when the algorithms current
solution costs qi is:

U (qi , t)
if d = stop,
(2)
V (qi , t) = max P
P
(q
|q
,
t)V
(q
,
t
+
t)
if
d = continue
d
j i
j
j
and the stopping policy is:

(qi , t) = argmax
d



U (qi , t)
if d = stop,
P
P
(q
|q
,
t)V
(q
,
t
+
t)
if
d = continue
j i
j
j

(3)

where U is the user-specified utility function and P is the profile of the anytime algorithm.
They also show a more sophisticated technique that accounts for the cost of evaluating the
policy, however, for the algorithms presented in this paper, the cost of evaluating the policy
consists of a mere array lookup and is essentially free.
Since the profile of an anytime algorithm is usually not known, it must be estimated. It
is possible to estimate the profile off-line if one has access to a representative set of training instances. To estimate the profile, the algorithm can be run on each of the training
instances and a 3-dimensional histogram can be created to represent the conditional probability distribution, P (qj |qi , t), needed to compute the stopping policy (cf. Equation 3).
Appendix C gives a more detailed description of our implementation of this procedure.
3.2 Anytime Heuristic Search
Anytime algorithms are a very general class and there are many anytime algorithms for
heuristic search (Likhachev, Gordon, & Thrun, 2003; Hansen & Zhou, 2007; Richter,
Thayer, & Ruml, 2010; van den Berg, Shah, Huang, & Goldberg, 2011; Thayer, Benton, &
Helmert, 2012). In this paper we use Anytime Repairing A* (ARA*, Likhachev et al., 2003)
since it tended to give the best performance over other approaches according to experiments
done by Thayer and Ruml (2010). ARA* executes a series of weighted A* searches, each
with a smaller weight than the previous. Since the weight bounds the solution cost, the
looser bounds on early iterations tend to find costly solutions quickly. As time passes and
the weight decreases, so does solution cost, eventually converging to optimal. ARA* also
has special handling for duplicates that are encountered during search that enables it to be
more efficient while still guaranteeing a bound on each of its solutions.
Like most anytime heuristic search algorithms, ARA* has parameters. Before running
ARA*, the user must select the weight schedule, which is typically comprised of an initial
weight and the amount by which to decrement the weight after each solution is found. The
behavior of ARA* varies with different weight schedules. For our experiments, we used
an initial weight of 3.0 and a decrement of 0.02. This schedule was used by Likhachev
703

fiBurns, Ruml, & Do

A
1
100
h=2
B
d=2
D

1

h=100
d=1

h=1
C
d=1
100
1
E

Figure 1: A small example graph.
et al. (2003), and we found that it gave the best performance when compared to several
alternative schedules on the domains we considered.
Given a fixed weight schedule, an anytime heuristic search algorithm will emit a fixed
stream of solutions for a given problem instance; the algorithm does not take the users
utility function into account. The same solutions will be found regardless of whether the
user wants any solution as fast as possible or the optimal solution at all costs. Figure 1
shows a small, concrete example, where the goal is to find a path from node A to node E.
Each node is labelled with its heuristic value (h) and the number of nodes remaining
to the goal (d), and edges are labelled with their costs. If the user wants an optimal
solution, then the algorithm would ideally return the path A, B, C, E. However, if the user
wants any solution as fast as possible, then it may be better to find the solution A, D, E,
as it has fewer nodes, and may be found in fewer expansions. ARA* only considers cost,
not distance, so with an initial weight less than 66 23 , the longer, cheaper, solution will be
found regardless of the users preference. It is up to the monitoring technique to select the
best that it can from the solutions that are found.
3.3 Contract Search
Dionne, Thayer, and Ruml (2011) consider the problem of contract search, where a goal
must be returned by a hard deadline. Unlike real-time search (Korf, 1990), where only the
agents next action must be ready by the deadline, contract search requires the algorithm to
return a complete path to a goal. Like optimizing a utility function, contract search must be
aware not only of the cost of solutions but also of the amount of time required to find them.
While the conventional approaches to contract search use anytime algorithms, Dionne et al.
(2011) present Deadline-Aware Search (DAS) which considers search time directly.
The basic idea behind DAS is to consider only states that lead to solutions deemed
reachable within the deadline. Two different estimates are used to determine this set of
nodes: an estimate of the maximum-length solution path that the search can explore before
the deadline arrives, called dmax , and an estimate of the distance to the solution beneath
704

fiHeuristic Search When Time Matters

each search node on the open list, in other words d. States for which d  dmax , are deemed
reachable, all other states are pruned. The search expands non-pruned nodes in best-first
order on f = g + h, updating its dmax and d estimates on-line. If the updates cause all
remaining nodes to be pruned while there is remaining time before the deadline, DAS uses a
recovery mechanism to repopulate the open list from the set of pruned nodes and continues
searching until the deadline is reached.
As mentioned previously, d estimates are as readily available as normal cost-to-go heuristics, h, for most domains. This leaves the question of how to estimate dmax . Dionne et al.
(2011) show that simply using the remaining number of possible expansions, computed via
the expansion rate and remaining time, is not appropriate due to a phenomenon that they
call search vacillation. When a best-first search expands nodes, it typically does not expand straight down a single solution path, instead it considers multiple solution paths at the
same time, expanding some nodes from each. When it does this, it is said to be vacillating
between many different paths, and it may not return to work on a particular path until
it has performed many expansions along others. To account for vacillation, Dionne et al.
introduce a metric called expansion delay that estimates the number of additional expansions performed by a search between the expansion of two successive nodes along a single
t
texp
path. They define dmax = rem
delay , where trem is the time remaining before the deadline,
texp is the average expansion rate, and delay is the average expansion delay. They compute
the average expansion delay by averaging the difference in the algorithms total expansion
count between when each node is expanded and when it was generated.
Dionne et al. (2011) showed experimentally that DAS performs favorably to anytimebased approaches and alternative contract search algorithms, indicating that an approach
that directly considers search time may also be beneficial for utility function optimization.

4. Off-line Bound Selection
We now turn to the first of the two new methods introduced in this paper.
In this section, we will present a very simple technique for trading search time for
solution cost that is based on bounded-suboptimal search. Recall that bounded-suboptimal
search algorithms return solutions that are guaranteed to be within a user-specified factor of
the optimal solution cost. In practice, few applications require an actual bound, instead the
bound is used by practitioners as a parameter that can be tweaked to speed-up the search
if it is not finding solutions quickly enough. The fact that the bound can trade search time
for solution cost makes it a prime candidate for automatic parameter tuning (Rice, 1976).
That is exactly what we propose.
As with the anytime methods discussed in the previous section, off-line bound selection requires a representative set of training instances. The instances are used to gather
information about how a bounded-suboptimal search trades-off search time for solution
cost. The only other requirement is that the user select a set of diverse bounds to try as
parameters to the search algorithm. The algorithm is then run on each of the N training instances with each suboptimality bound, creating a list of N pairs for each bound:
sols b = h(c1 , t1 ), ..., (cN , tN )i where b is the bound passed as a parameter to the algorithm,
ci is the cost of the solution to the ith training instance and ti is the time at which the ith
solution was found. Given a utility function U : cost  time  R, we can select the bound
705

fiBurns, Ruml, & Do

that gives the greatest expected utility on the training set:


X
1
bound U = argmax 

U (c, t)
|sols b |
b

(4)

(c,t)sols b )

In our experiments, we select a different weight to use for each utility function from the
set 1.1, 1.5, 2, 2.5, 3, 4, 6, and 10. It may be possible to reduce the number of weights
in the training set by using linear interpolation to estimate the performance of parameters
between those used for training. This simple approach can also be extended to select over
a portfolio of different algorithms in addition to different bounds. It may be beneficial, for
example, to include both A* and Speedy search in the portfolio, as these algorithms will
likely be selected if cheap solutions are required or if a solution must be found very quickly.
We will see in Section 6 that this very simple technique outperforms ARA* using an anytime
monitor in our experimental evaluation. In fact, if a representative set of training instances
is available, then this technique tends to perform better than all other algorithms that we
evaluate.
A related technique is the dove-tailing method of Valenzano, Sturtevant, Schaeffer, Buro,
and Kishimoto (2010). Their approach is presented as a way of side-stepping the need for
parameter tuning by running all parameter settings simultaneously. They found that, with
dove-tailing, weighted IDA* (Korf, 1985) was able to return its first solution much faster,
as the dove-tailing greatly reduced the high variance in solving times for any given weight.
They also found that dove-tailing over different operator orderings was effective for IDA*.
The main difference between the work by Valenzano et al. and ours is that we have quite
different goals. Our concern is not to find the first solution more quickly, but rather to select
a setting that better optimizes a user-specified utility function. As such, our approach does
not run multiple settings at the same time and instead selects a single parameter to run in
a single search. In fact, the approaches are complementary. Given any of our utility-aware
algorithms that have parameters, one could use dove-tailing to avoid the need to perform
offline parameter selection.

5. Best-First Utility-Guided Search
Anytime search is not aware of utility. Monitoring and bound selection require training. In
this section, we present Bugsy2 , a utility-aware search algorithm that does not require any
off-line training.
5.1 Expansion Order
Like A*, Bugsy is a best-first search, but instead of ordering its open list on f , Bugsy
orders its open list on an estimate of the utility of the outcome resulting from each node
expansion. Since utility is dependent on time, the mere passage of time affects the utility
values. This differs from most traditional search algorithms where the values used to order
expansions remain constant. Recall, however, that when using a linear utility function,
all utility values decay at the exact same rate. Given this, Bugsy ignores all past time
2. Bugsy is an acronym for Best-first Utility-Guided SearchYes!

706

fiHeuristic Search When Time Matters

and compares the utility estimates assuming that time begins at the current decision point.
While these utility values will not match the utility of the ultimate outcome, they still
preserve relative order of the different choices that the agent can make.
To understand Bugsys ordering function, we will first consider the best utility of the
outcome resulting from each node expansion as computed by an oracle. If we had foreknowledge of a maximum utility outcome, the only purpose of the search algorithm would be to
achieve it by expanding the nodes along the path from the initial node in order to build the
solution path. Since our utility function is given as a linear combination of solution cost
and search time, the utility value of this outcome can be written in terms of the cost and
length of a (possibly empty) maximum utility outcome, s:
U  = (wf  g  (s) + wt  d (s)  texp )

(5)

where g  (s) is the cost of the path s (recall that the cost of the empty path is a user-specified
constant), d (s) is the number of nodes on s, and texp is the time required to expand a node.3
Given the maximum utility value U  , the best utility of the outcome resulting from
expanding a node n is:
 
U
if n leads to a maximum utility outcome

(6)
u (n) =

U  wt  texp otherwise
In other words, the utility we get from expanding a node that leads to a maximum utility
outcome is the maximum utility; expanding any other node is simply a waste of time, and
has a utility of the maximum utility minus the cost of performing the unnecessary expansion.
In practice, we do not know the maximum utility, so we must rely on estimates. Bugsy
uses two estimates to approximate the maximum utility. First, it estimates the cost of the
solution that it will find beneath each node as, f . Note that f is an estimate, not only
because the heuristic is an estimate of the true cost to go, but also because the cheapest
solution beneath a node may not be the solution of greatest utility. See Appendix A for
possible alternatives. Second it estimates the number of expansions required to find a
solution beneath each node n, exp(n). One crude estimate for remaining expansions is d,
the distance heuristic that estimates the remaining nodes on the solution path. In reality,
Bugsy will experience search vacillation, as discussed earlier, expanding more nodes than
just those along a single solution path. To account for this vacillation, we use the expansion
delay technique of Dionne et al. (2011) and we estimate exp(n) = delay  d(n). That is, we
expect each of the remaining d(n) steps to a goal will require delay expansions.
Bugsy can either choose to expand a node, or it can stop and return the empty solution.
This is one way in which Bugsy differs from A*: Bugsy decides among actions at the search
level (such as terminating the search, or expanding one of the many open nodes), whereas
A* is committed to expanding nodes in a fixed order. In Bugsy each node on the open
list represents a possible outcome, so Bugsys maximum utility can be estimated using the
maximum of the utility estimates of all open nodes and Equation 5:


U = max max (wf  f (n) + wt  d(n)  delay  texp ), U ({}, 0)
(7)
nopen

3. Note that expansion time is not constant in general, because it includes time to add and remove elements
from data structures like the open list.

707

fiBurns, Ruml, & Do

Bugsy(initial , u())
1. open  {initial }, closed  {}
2. do
3.
n  remove node from open with highest u(n) value
4.
if n is a goal, return it
5.
add n to closed
6.
for each of ns children c,
7.
if c is not a goal and u(c) < 0 or an old version of c is in open or closed
8.
skip c
9.
else add c to open
10.
if the expansion count is a power of two
11.
re-compute u(n) for all nodes on the open list using the most recent estimates
12.
re-heapify the open list
13. loop to step 3
Figure 2: Pseudo-code for Bugsy.
Once the estimate U is found, it would be possible to substitute it for U  in Equation 6
to estimate u (n), the utility of the outcome from expanding each node on the open list.
However, Bugsy is only going to expand one node, so there is no need to estimate u (n)
for each open node; Bugsy simply expands the node with the best estimated outcome.
Additionally, instead of computing the maximization in Equation 7 from scratch each time
it is about to expand a node, Bugsy simply orders its open list on u(n) = (wf  f (n) + wt 
d(n)delay texp ), each iteration popping off the node with the maximum u(n) for expansion.
In this way, the algorithm directly attempts to maximize utility.
Recall Figure 1, which shows two paths from an initial node, A, to a goal node, E.
Because Bugsy accounts for distance in its utility function, it will find the shorter path A,
D, E if their utility function sufficiently emphasizes finding solutions quickly over finding
cheaper solutions. On the other hand, if the utility function gives a preference to finding
cheap solutions then Bugsy will spend the extra search time to find the cheaper path, A,
B, C, E.
5.2 Implementation
Figure 2 shows high-level pseudo-code for Bugsy. For clarity, the code elides the details of
computing u(n) values. The algorithm proceeds like A*, selecting the open node with the
highest u(n) for expansion (line 3). If this node is a goal, then it is returned as the solution
(line 4), otherwise the node is put on the closed list (line 5) and its children are generated.
Each new child is put onto the open list (line 9) except duplicate nodes and nodes for which
expansion is estimated to have a negative utility (which occurs when the utility of returning
no solution is greater than that of continuing the search); these are discarded (lines 78).
Bugsy estimates the current expansion time and the expansion delay online, and these
estimates can change after each expansion. Instead of re-sorting the open list after each
expansion, Bugsy re-sorts whenever the number of nodes that it has expanded is a power
708

fiHeuristic Search When Time Matters

of two, the utility of each open node is re-computed using the latest set of estimates for
texp and expansion delay (as described in Section 3.3), and the open list is re-heapified
(lines 1012). We describe this re-sorting step in greater detail in Section 5.5.
5.3 Stopping
Bugsy orders its open list in decreasing order of u(n), and stops searching when the maximum estimated utility is less than that of returning the empty solution. While it may be
possible to continue searching in an anytime fashion after the first goal is found, from a
utility perspective this is not the correct approach. We prove that here:
Theorem 1 Assuming the expansion time texp is constant, h is admissible, and exp never
overestimates the expansions to go, at the time that Bugsy finds its first solution, s, the
solutions Bugsy would find beneath the remaining nodes would result in less utility than
immediately returning s.
Proof: Let T be the current time at which Bugsy found solution s. The utility of returning
s is U (s, T ) = u (s) = (wf f  (s)+wt T ), where u (s) is the utility of returning s now, and
f  (s) is the cost of solution s. Note that because h is admissible and s is a goal, h(s) = 0,
g(s) = g  (s), f (s) = f  (s), and therefore u(s) = u (s). Also exp never overestimates the
expansions to go so exp(s) = 0. Since s was chosen for expansion u(n)  u (s) for every
node n on the open list.
Let t(n) be the minimum amount of additional time Bugsy requires to find the solution
beneath any unexpanded node n. t(n)  texp since Bugsy must at least expand n. So for
each node n on the open list, the best utility that Bugsy could achieve by going straight
to the cheapest goal under n is:
u (n) = (wf  f  (n) + wt  (t(n) + T ))

 (wf  f (n) + wt  (t(n) + T )), since f (n)  f  (n) due to the admissibility of h

 (wf  f (n) + wt  (exp(n)  texp + T )), since exp never overestimates

= u(n), by the definition of u(n)

 u (s), since u (s) = u(s) and s was chosen for expansion, not n

This justifies Bugsys strategy of returning the first goal node that it selects for expansion.
It should be noted that Bugsys estimate of exp(n) = delay  d(n) is not a lower bound, but
as we will see in the later sections, this stopping criterion performs quite well in practice.
5.4 Heuristic Corrections
Many best-first search algorithms use admissible heuristic estimates that never overestimate
the true cost to go. The proof of optimality of A* and the proofs of bounded suboptimality
of bounded suboptimal search algorithms rely crucially on the admissibility property of the
heuristic. Bugsy does not fixate on cost-optimal solutions and does not guarantee bounded
cost. Instead, Bugsy attempts to optimize a utility function for which solution cost is only
one of two terms. Since there are no strict cost guarantees, Bugsy is free to drop the
admissibility requirement if more informed but inadmissible estimates are available.
709

fiBurns, Ruml, & Do

Thayer, Dionne, and Ruml (2011) show that inadmissible estimates can provide better
performance for bounded suboptimal search. One such technique attempts to correct the
heuristic estimates on-line using the average single-step error between the heuristic values
of a node and its best child. Thayer et al. show that while this technique provides good
search guidance, it is actually less accurate at estimating the true cost-to-go values than
the standard admissible heuristics. For Bugsy, this is undesirable, as it does not need
good guidance, but proper estimates. Thayer et al. also show that learning the heuristic
off-line with linear regression can provide more accurate estimates. Unfortunately, using
such off-line training would negate one of Bugsys main benefits. It is a matter of empirical
evaluation as to whether any of these techniques will provide better performance for Bugsy.
In Section 6.5, we show that using the standard admissible heuristics often gives the best
performance anyway.
5.5 Resorting
Instead of requiring off-line training as in the previous approaches, Bugsy uses on-line
estimates to order nodes on its open list. First, while many analyses regard texp as as a
constant, it can in practice depend on log-time heaps, cache behavior, and multiprogramming overhead, among other factors, so our implementation of Bugsy estimates texp as
a global average computed during search. Second, Bugsys expansion delay estimate is
calculated as the global average of the difference in expansion count from when each node
was generated to when it was expanded; this too must be done on-line. Unfortunately, the
on-line estimates may change at each node expansion, and navely using the latest estimates
to compute the u value for newly generated nodes can lead to poor performance. This is
due to the comparisons used to order the open list: instead of fair comparisons based on
the estimated utility of each node, the recent and very fresh estimates of new nodes will be
compared with the old and possibly more stale estimates of nodes that have been open for
a long time.
To alleviate this problem, our implementation of Bugsy uses two sets of estimates:
one stable set used to order the open list, and one ever-changing set maintaining the most
recent estimates. At certain points throughout the search, Bugsy copies the most upto-date estimates into the stable set, recomputes the utility values of all open nodes, and
re-sorts the open list. Our open list is implemented as a binary heap so it can re-establish
the heap property in linear time in the number of elements on the heap. Unfortunately, it
would still be very expensive to do this at every node expansion, so, instead, Bugsy reorders
the open list exponentially less frequently as the search progressesit only reorders when
the number of expansions is a power of two. We prove that this logarithmic scheme only
adds a constant amount of overhead per-expansion when amortized over the entire search.
Theorem 2 In a search space that grows geometrically with a finite branching factor, the
overhead of reordering the open list on power-of-two expansions is constant for each expansion when amortized over the search.
Proof: Let b be the maximum branching factor. The maximum number of nodes that can
be on the open list after n expansions is N (n) = bn  n = n(b  1). The total cost of all
710

fiHeuristic Search When Time Matters

re-sorting after n expansions is no more than:
lg n

X
i=0

lg n
i

O(N (2 )) =

X
i=0

O(2i  (b  1)), by definition of N
lg n

= c(b  1)

X

2i , for some c > 0, by definition of O

i=0
lg n+1

= c(b  1)(2

 1), by the identity

 c(b  1)(2  2lg n  1)

Pj

i
i=0 2

= 2j+1  1

= c(b  1)(2n  1)

= O(n)


So, the overhead per-expansion is constant when amortized over all expansions. It is a
matter of empirical evaluation to determine if this constant overhead is detrimentalwe
address this in Section 6.4.

6. Experimental Evaluation
All the techniques discussed above involve approximations and estimations that may or may
not work well in practice. In this section, we present results of an experimental comparison
of the techniques to better understand their performance. All of these algorithms and
domains were implemented in C++; the source code is available at https://github.com/
eaburns/search.
6.1 Overview
In the following sections, we answer several questions experimentally. First, we would like
to ensure that our monitored ARA* algorithm is performing at its best by comparing the
profile learned off-line with an oracle. As we will see, the off-line profile, while only an
estimate of the true profile of the algorithm, is quite well-informed.
In Section 5.5, we proved that re-sorting only adds a constant overhead per-expansion
when amortized over the entire search. It is a matter of empirical evaluation to determine
whether or not the benefits outweigh this overhead. Our experiments show that re-sorting
with a logarithmic schedule greatly outperforms Bugsy without re-sorting.
In Section 5.4 we pointed out that Bugsy does not require admissible heuristic estimates,
and in fact it may perform better with inadmissible, but more accurate heuristics. We show
how Bugsy performs with admissible heuristics, and with two different types of corrected
heuristics. Overall, we conclude that the best configuration is Bugsy with the standard
admissible heuristics.
We discussed expansion delay in Section 5.1. We show results that demonstrate that
using expansion delay is better than simply using d as the estimate of expansions to the
goal. Then we compare two variants of Bugsy: one that ignores newly generated nodes
that are found to already be on the closed list (we call these duplicate nodes) and one
that reinserts these nodes onto the open list if they have better utility estimates than their
711

fiBurns, Ruml, & Do

previously closed version. Ignoring duplicates always performs better in some domains, and
in others it performs better only when the preference is for very short search times.
Then, we compare A*, Speedy search, monitored ARA*, weighted A* with a learned
weight, and Bugsy. We find that the simplest approach of learning a good weight for
weighted A* gives the best performance. We also find that Bugsy, which doesnt use
any off-line training, performs about as well as monitored ARA*, which does use off-line
training. Therefore, if training instances are available, we recommend the simple weighted
A* approach where the weight is selected based on performance on the training set. If no
training instances are available Bugsy is the algorithm of choice.
Lastly, we compare Bugsy to both real-time search and DTA* on the platform pathfinding domain. In both experiments, Bugsy achieves the best utility.
6.2 Domains
In order to verify that our results hold for a variety of different problems, we performed our
experiments on four different domains. The domains that we used are described briefly in
the following paragraphs, with more detailed descriptions given in Appendix B.
6.2.1 15-Puzzle
The 15-puzzle is a popular heuristic search benchmark that has a small branching factor
and few duplicates. For this domain, we used the reasonably informed Manhattan distance
heuristic, and our implementation followed the heavily optimized solver presented by Burns,
Hatem, Leighton, and Ruml (2012). We ran the 100 instances created by Korf (1985), and
in plots including A* we only use results on the 94 instances solvable by A* in 6GB of
memory.
6.2.2 Pancake Problem
The pancake problem is another standard puzzle with a large constant branching factor.
In our experiments, we used instances with 50 pancakes, and the gap heuristic (Helmert,
2010). Since many of these problems were too difficult for A*, we used IDA* instead of A*
on this domain.
6.2.3 Platform Pathfinding
The platform domain is a pathfinding domain of our own creation with dynamics based on
a 2-dimensional platform-style video game, where the player must jump between platforms
to traverse a maze. Video games often naturally have an element of time pressure. It
has a large state-space and many cycles, but a reasonably informed heuristic based on
visibility navigation. The instances used in our experiments were created randomly, using
the generator described in Appendix B. This domain is also of particular interest because its
action costs are given in units of time (each action is 50ms), so the objective of minimizing
goal achievement time can be expressed as a linear combination of search time and solution
cost.
712

fiHeuristic Search When Time Matters

6.2.4 Grid Pathfinding
Grid pathfinding is a popular heuristic search benchmark, motivated by robotics and video
games. In our experiments, we used two different cost models, and two different movement
models. The cost models were the standard unit-cost model and the life-cost model which
assigns action costs such that the shortest, most direct path is more expensive than a longer,
more circuitous path. This captures the popular adage that time is money. Instances were
5,0005,000 grids with uniformly distributed obstacles. Our heuristics were based on the
Manhattan distance heuristic for four-way grids, and the octile distance heuristic for eightway grids. The octile distance heuristic is a simple modification to Manhattan
distance

that multiplies the shorter of the horizontal and vertical displacement by 2 to accounts
for eight-way move costs.
6.3 Anytime Profile Accuracy
We want to ensure that our implementation works well and our training instance sets are
representative enough that monitored ARA* can perform at its best. In this subsection, we
evaluate the accuracy of the stopping policies created using the estimated anytime profiles
by comparing them to an oracle. Since the stopping policy is only guaranteed to be optimal
for the true algorithm profile, it is a matter of empirical study to determine whether or not
the estimated profile will lead to a good policy.
To estimate the profile used by the monitored version of ARA*, we ran ARA* with a
6GB memory limit or until convergence on 1,000 separate test instances for each domain.
Next, we created a histogram by discretizing the costs and times of each of the solutions into
10,000 bins (100  100). We experimented with different utility functions by varying the
ratio wf /wt in Equation 1. Small values of wf /wt give a preference to finding solutions more
quickly, whereas large values prefer finding cheaper solutions. In the case of the platform
game, for example, this can be viewed as a way to change the speed at which the agent
moves: a slow agent might benefit from more search in order to find a shorter path, but a
fast agent can execute a path quickly, and may prefer to find any feasible solution as fast
as possible.
Figure 3 shows the results of this experiment. The box plots represent the distribution
of utility values found by ARA* using the estimated stopping policy, given as the factor of
the oracles utility. The oracle finds all solutions of the anytime algorithm until it converges
on the optimal solution, then it picks the solution which would have maximized the utility
function. Since the utility values are negative, larger factors represent smaller (more negative) utilities and thus a worse outcome. The boxes surround the second and third quartiles,
the whiskers extend to the extremes, and circles show values that are more than 1.5 the
inter-quartile range outside of the box. The center line of each box shows the median, and
the gray rectangles show the 95% confidence interval on the means. Each box represents a
different wf /wt as shown on the x axis. There is a reference line drawn across at y = 1 (the
point where the oracle and estimated policy performed equally as well), and in many cases
the boxes are so narrow that they are indistinguishable from this line.
Some points in these figures lie very slightly below the y = 1 line, indicating instances
where the oracle performed worse than the estimated policy. This is possible due to the
variance in solving times. In our experiment, the ARA* runs used to compute the oracles
713

fiBurns, Ruml, & Do

platform
factor of oracle

factor of oracle

15-puzzle
2
1.6
1.2
1e-4

1e-3 1e-2 1e-1
cost/time preference

3
2
1

1

1e-4

1e-3
1e-2
1e-1
cost/time preference

4-way unit grids

50-pancake
factor of oracle

1.3

factor of oracle

1

1.2
1.1
1

1.08
1.04
1
0.96

1e-4

1e-3 1e-2 1e-1
cost/time preference

1

1e-4

1e-3 1e-2 1e-1
cost/time preference

8-way unit grids

1

4-way life grids
factor of oracle

factor of oracle

1.014
1.2
1.12
1.04

1.008
1.002
0.996

1e-4

1e-3 1e-2 1e-1
cost/time preference

1

1e-4

1e-3 1e-2 1e-1
cost/time preference

1

8-way life grids
factor of oracle

1.012
1.008
1.004
1
1e-4

1e-3 1e-2 1e-1
cost/time preference

1

Figure 3: Comparison of the optimal stopping policy and the learned stopping policy.
714

fi15-puzzle

1

0.6

0

0
-6

-3

log10 cost/time preference

0

-6

0.6

0
-3

log10 cost/time preference

0.5

0
-6

0

0

1.6

0.8

0
-9

-6

log10 cost/time preference

-3

log10 cost/time preference

0

8-way life grids
log10 factor of best utility

1.2

-6

-3

log10 cost/time preference

No Resort
Resort

1

4-way life grids
log10 factor of best utility

8-way unit grids
log10 factor of best utility

4-way unit grids

platform
1.2

log10 factor of best utility

2

log10 factor of best utility

log10 factor of best utility

Heuristic Search When Time Matters

-3

1

0
-9

-6

log10 cost/time preference

-3

Figure 4: Bugsy: Resorting the open list (circles) vs not (boxes).
utilities occasionally found solutions more slowly than the ARA* runs using the estimated
stopping policy. In other words, it is caused by the non-determinism inherent in a utility
function that depends on solving time. As is obvious in the figure, these instances are quite
rare and usually happened for small values of wf /wt , where miniscule time differences have
a large effect on utility.
From these results, we conclude that our monitored ARA* implementation performs
quite well, as the stopping policy often stopped on the best solution available from those
emitted by the underlying anytime algorithm.
6.4 To Resort or Not to Resort?
In Section 5.5 we proved that re-sorting Bugsys open list on power-of-two expansions only
added a constant overhead per-expansion when amortized over the search. It is a matter of
empirical evaluation to determine whether or not this overhead is worth the effort. While
other re-sorting schedules are possible, we only tried re-sorting on power-of-two expansions.
Figure 4 shows the utility achieved by Bugsy both with and without re-sorting. The
x axes show the wf /wt ratio determining the preference for solution cost and search time
on a log10 scale. As with the previous plots, smaller values indicate a preference for faster
search times and larger values indicate a preference for cheaper solutions. The y axes show
the factor of the utility achieved by the best technique on each instance, again on a log10
scale. A y value of log10 1 = 0 indicates the best utility achieved by any technique on a given
715

fiBurns, Ruml, & Do

instance; values greater than zero indicate less utility. Points show the mean value over all
test instances with error bars giving the 95% confidence intervals. From these plots, we
can see that re-sorting the open list led to significant improvements in all domains. On the
pancake puzzle, Bugsy without re-sorting was unable to solve any of the instances within
a 6GB memory limit. In our remaining experiments, we always enable re-sorting on an
exponential schedule.
6.5 Heuristic Corrections
In Section 5.4, we mentioned that Bugsy does not require admissible heuristic estimates,
as it provides no guarantees on solution cost. In this section we compare Bugsy using the
standard admissible heuristics to Bugsy using both on-line and off-line corrected heuristics.
Following Thayer et al. (2011), our on-line heuristic correction used a global average of the
single-step heuristic error between each node and its best offspring, and our off-line heuristic
was a linear combination of h, g, depth, and d, for each node. The coefficients for each term
in the off-line heuristic were learned by solving a set of training problems and using linear
least squares regression.
The comparison is shown in Figure 5. The plots are in the same style as Figure 4. Typically the on-line correction technique performed worstsome times significantly worse
than the other two. We attribute this to its poor accuracy as observed by Thayer et al.
(2011). On some problems, such as the 15-puzzle and the 8-way unit-cost grid pathfinding, the off-line correction technique performed best, but in general the simple admissible
heuristics were the best or were competitive with the best. For the remainder of our experiments, we chose to use the simplest variant without any corrections as it did not require
any off-line training (which is one of Bugsys main benefits), and it was never the worst
and was often the best or near the best.
6.6 Expansion Delay
In Section 5.1 we described why simply using d as an approximation for exp(n), the number
of nodes expanded to arrive at the goal beneath node n, is inaccurate. The search algorithm
does not expand just the nodes along the path to a goal, but instead it vacillates between
different solutions. To account for the search vacillation, we choose to estimate exp(n) =
delay  d(n), where delay is the average expansion delaythe average number of nodes
expanded before the search makes progress along a single path to a goal. In this subsection,
we show experimentally that using expansion delay provides much better performance than
using d alone.
Figure 6 shows two versions of Bugsy: one that uses expansion delay, labelled With
Exp. Delay, and one that does not, labelled Without Exp. Delay. It is clear from this
figure that using expansion delay is beneficial. Also, we can see that on the right side of the
plots, where cheaper solutions are preferred to short search times, using expansion delay is
about the same as just using d by itself. This is because wf is relatively large compared to
wt for these utility functions, so the exp(n) term has little influence on the utility estimates.
716

fiHeuristic Search When Time Matters

15-puzzle

0.6

log10 factor of best utility

log10 factor of best utility

 

log10 factor of best utility

50-pancake

platform
Online
None
Offline

0.2

0.1

0

0
-6

-3

-3

log10 factor of best utility

log10 factor of best utility

0.06

0
-3

0.04

0
-6

-3

0

log10 cost/time preference
8-way life grids

log10 factor of best utility

4-way life grids
log10 factor of best utility

0

0.08

0

log10 cost/time preference

0.1

0.05

0
-6

-2

log10 cost/time preference
8-way unit grids

0.12

-9

-4

0

log10 cost/time preference

4-way unit grids

-6

0.08

0
-6

0

log10 cost/time preference

0.16

-3

0.06

0.03

0
-9

log10 cost/time preference

-6

-3

log10 cost/time preference

Figure 5: Bugsy: Heuristic corrections.

6.7 Duplicate Dropping
Suboptimal search algorithms do not expand nodes in strict order of increasing f . Consequently, they can expand a node, and later re-generate the same node via a cheaper path.
We call such re-generations duplicates, and when they are generated via cheaper paths we
say that they are inconsistent, because their current path cost (and subsequently the cost
of the paths to all of their descendants) is more expensive than necessary (Likhachev et al.,
2003). In the face of inconsistent nodes, a search algorithm can put the already expanded
node back on the open list with a cost that accounts for the new, cheaper path. When
the node comes to the front of the open list, it will be re-expanded and the inconsistency
717

fiBurns, Ruml, & Do

platform

2.4

1.6

50-pancake

1.2

log10 factor of best utility

log10 factor of best utility

log10 factor of best utility

15-puzzle

0.8

0.4

0

0
-6

-4

-2

log10 cost/time preference

0

-6

-4

-2

log10 cost/time preference

Without Exp. Delay
With Exp. Delay

log10 factor of best utility

log10 factor of best utility

0.4

-5

0

-3

-4

-2

-1

log10 cost/time preference

0

8-way unit grids

0

0.9
0.6
0.3
0

-6

-4

-2

log10 cost/time preference

0

-6

-4

-2

log10 cost/time preference

4-way life grids

0

8-way life grids

1.8

log10 factor of best utility

log10 factor of best utility

0.8

0

4-way unit grids
1.2

1.2

1.2

0.6

0

1.8

1.2

0.6

0
-8

-6

-8

-4

log10 cost/time preference

-6

-4

log10 cost/time preference

Figure 6: Bugsy: Expansion delay.
will propagate through to all of its descendants. Unfortunately, if there are a lot of such
inconsistencies, then the search algorithm can spend a lot of time re-expanding the same
nodes over and over again. An alternative technique is to simply ignore the inconsistency
and drop all duplicate nodes when they are generated. Dropping duplicates can reduce the
search effort needed to find a goal at the cost of finding more expensive solutions. Whether
or not dropping duplicates is beneficial typically depends on the domain (Thayer & Ruml,
2008).
Figure 7 shows a comparison of Bugsy both with and without duplicate dropping. On
the platform, tiles, and pancake domains using duplicate dropping is nearly always better
than re-expanding duplicates. On the grid pathfinding problems,with the notable excep718

fiHeuristic Search When Time Matters

0.15

0.1

0.05

50-pancake
log10 factor of best utility

platform
log10 factor of best utility

log10 factor of best utility

15-puzzle
0.8
0.6
0.4
0.2

0.24

0.16

0

0
-6

-4

-2

log10 cost/time preference

0

0
-6

-4

-2

log10 cost/time preference

Duplicate Reexpansion
Duplicate Dropping

0.12

0.06

0

-3

-2

-1

0

0.04

0.02
0.01
0

-6

-4

-2

log10 cost/time preference

0

-6

-4

-2

log10 cost/time preference

0

8-way life grids
log10 factor of best utility

4-way life grids
log10 factor of best utility

-4

log10 cost/time preference

8-way unit grids
log10 factor of best utility

log10 factor of best utility

4-way unit grids
0.18

-5

0

1.8
1.2
0.6
0

0.018
0.012
0.006
0

-8

-6

-8

-4

log10 cost/time preference

-6

-4

log10 cost/time preference

Figure 7: Bugsy: Duplicate dropping.

tion of 4-way life-cost gridsre-expanding duplicate nodes seems to give better performance
except where solutions are needed as quickly as possible (on the left-hand side of the plots).
This is reasonable, because duplicate dropping tends to sacrifice solution cost in order to
reduce search time. Note also, that the values on the y axes of these plots are very small, so
while the results are statistically significant, the difference between the two techniques on
the grid problems where duplicate re-expansion performs better is quite small. In the next
section we will see that A* actually achieves the most utility in many of the cases where
duplicate re-expansion outperforms duplicate dropping.
719

fiBurns, Ruml, & Do

platform

log10 factor of best utility

log10 factor of best utility

15-puzzle
0.4

0.2

0

0
-6

-3

log10 cost/time preference

0

-6

-3

log10 cost/time preference

0

log10 factor of best utility

50-pancake
A
pee y
ugsy
A A

0.4

0.

0
-4

-

log10 cost/time preference

0

Figure 8: Comparison of techniques.

6.8 Comparing Techniques
Now that we understand the most promising configurations of the techniques we are studying, we can finally turn our attention to comparing them.
Figures 8 and 9 show a comparison of the three different techniques for utility-aware
search. These plots are larger than the previous plots to improve clarity, because they
have more lines. The plots include A*, Speedy Search, Bugsy, ARA* with monitoring
(ARA*), and weighted A* with the weight chosen automatically for each different utility
function from the set 1.1, 1.5, 2, 2.5, 3, 4, 6, and 10 (wA*). As we would expect, when the
preference was for shorter search times (on the left-end of the x axis), A* performed poorly,
as it stubbornly stuck to optimal solutions. Speedy search, however, performed quite well.
As the preference shifted toward desiring cheaper solutions, A* began to do better whereas
Speedy did worse. The utility-aware techniques were much more robust than both A* and
720

fiHeuristic Search When Time Matters

8-way unit grids
log10 factor of best utility

log10 factor of best utility

4-way unit grids

0.1

0

0.1

0
-6

-3

log10 cost/time preference

0

-6

0

log10 factor of best utility

8-way life grids

log10 factor of best utility

4-way life grids

-3

log10 cost/time preference

0

0
-9

-6

log10 cost/time preference

-9

-3

-6

log10 cost/time preference

-3

Figure 9: Comparison of techniques (continued).
Speedy, neither of which take the users preference for search time and solution cost into
account at all.
Of the utility-aware techniques, both Bugsy and weighted A* with an automatically
selected weight performed the best. Bugsy was better on both the 15-puzzle and the
platform domain. On the grid problems, Bugsy and weighted A* had roughly the same
performance on the right side of the x axes. On the left side, Bugsy tended to get worse
relative to the other utility-aware techniques, and ARA* with an anytime monitor was often
the best performer. However, ARA* performed significantly worse in the middle and on
the right-hand portion of the plot in some domains, leading us to recommend the weighted
A* technique as a simpler and more robust approach.
The utility-aware techniques often performed as well as A* when low-cost solutions were
preferred. When fast solutions were preferred, these techniques sometimes outperformed
Speedy search. This likely indicates that solution cost still played a roll in the final utility
721

fiBurns, Ruml, & Do

log10 factor of best utility

orz100d

0.2

0.1

0
-6

Bugsy

ARA*

-3

0

log10 cost/time preference
wA*
A*
Speedy

Figure 10: Grid pathfinding on a video game map.
on the left-most points in some of the plots. ARA* tended to achieve greater utility than
Bugsy when solutions were needed quickly, but when cheaper solutions were preferred,
Bugsy tended to be better than ARA*. On most domains, ARA* had a spike of low utility
for ratios between 0.001, and 1, with the peak appearing between 106 and 103 for life-cost
grids. This peak approximately coincides with the utility functions for which the estimated
profile performed worse than the oracle as shown in Figure 3, possibly indicating that more
than 1,000 training instances were required for these utility functions.
Overall, the utility-aware techniques were able to achieve much greater utility than the
utility-oblivious A* and Speedy algorithms. This is not terribly surprising. Surprisingly, the
results also suggest that our very simple parameter tuning technique can often give the best
performance if a representative set of training instances is available. If not, then Bugsy
is the algorithm of choice as it performs well and does not require any off-line training.
Indeed, by putting reasoning about search time into the search algorithm itself, Bugsy can
be competitive with techniques requiring previous experience.
6.9 Limitations
In the previous set of experiments, we saw that the utility-aware algorithms outperformed
both Speedy search and A* for a wide range of utility functions. In this section, we look at
one domain for which this tends not to be the case: video game grid maps.
Video games are one of the main motivations for research in grid pathfinding problems.
Sturtevant (2012) observed that grid maps created by game designers often exhibit very
different properties from maps generated algorithmically. Figure 10 shows a comparison of
Bugsy, monitored ARA*, weighted A* with an automatically selected weight, Speedy, and
722

fiHeuristic Search When Time Matters

Figure 11: Grid pathfinding on a video game map.

150
100
50

200

% Speedy time

200

planning time

execution time

200

% Speedy time

% Speedy nodes

nodes expanded

150
100
50

0

8

16

instance

24

150
100
50

0

8

16

instance

24

0

8

16

instance

24

Figure 12: Nodes expanded, search time, and execution time.
A* on the Dragon Age Origins map orz100d from the benchmark set of Sturtevant. This
map is shown in Figure 11. It has a fairly wide-open area at the top, with a more closed-off
bottom half containing rooms and hallways. The format of the plot in this figure is the
same as those in the previous subsection. As we can see, A* gave the best performance for
a large range of utility functions, and Bugsy actually never outperformed Speedy or A*
in the entire experiment (neither did ARA*, and wA* only gave the best performance at
a single data point). We hypothesized that Bugsys poor performance was because these
problems are very easy to solve, and Bugsys extra computation overhead, while very small,
was more prominent.
To explore this hypothesis, we plotted the performance of Bugsy given as the difference
from that of Speedy using a single utility function given by wf = 106 , wt = 1. This is the
723

fiBurns, Ruml, & Do

left-most utility function in Figure 10, a function for which here Speedy search performed
the best and Bugsy performed poorly. Figure 12 shows the number of nodes expanded,
the time spent executing, and the time spent searching for Bugsyas percentages of the
equivalent values for Speedy search. The data points were gathered on a random sample
of 25 instances from Sturtevants (2012) scenario set for the orz100d map. Values below
the line at 100% represent instances where Bugsy expanded fewer nodes or spent less
time searching or executing, and values above the line represent instances where Bugsy
expanded more nodes or spent more time than Speedy. The x axes shows the rank of the
instances in the sample in increasing order of their optimal solution lengths.
As we can see in Figure 12, Bugsy expanded about the same number of nodes and
had very similar execution times to Speedy. For problems with larger optimal solution
costs Bugsy had slightly less execution time. The major difference in performance between
these two algorithms, however, is shown in the right-most plot where we can see that Bugsy
required more search time than Speedy search on almost every instance. Since Bugsy and
Speedy expanded about the same number of nodes, this additional time must be due to
Bugsys small amount of extra overhead incurred from re-sorting and computing utility.
We conclude that, barring this extra overhead, Bugsy would have performed as well as the
best performer for this utility function. In domains where node expansion and heuristic
computation isnt so simplistic, this overhead would be insignificant.
6.10 Training Set Homogeneity
In Section 6.8 we showed that our weighted A* approach outperformed other techniques
in all domains, with the notable exception of the platform domain and 15-puzzle, where
Bugsy was the best. Additionally, compared to other domains, the weighted A* technique
performed relatively poorly on video game pathfinding (cf. Figure 10 where wA* is outperformed by the utility oblivious approaches at all points except for one). We believe
that the poor performance of wA* on these domains is due to heterogeneous training sets.
To verify this, we looked at the mean and standard deviation in the optimal path lengths
for problems in all of our domains. The optimal path length can be viewed as a proxy
for problem difficulty, and a high standard deviation in this statistic points to a diverse
set of instancessome very easy to solve, and some quite difficult. For both the platform
and video game path finding domains, the standard deviation in optimal path length was
greater than 50% of the mean; more than twice that of the other domains. Note that, in
domains such as the video game map, the variety in the layout of different areas of the map
means that instances will inherently differ in their characteristicsmerely gathering more
instances will not produce a more homogeneous set. This evidence supports our hypothesis
that weighted A*s performance can be greatly hindered in situations where a representative
training set is not available.
6.11 Real-time Search
The main focus for our study is algorithms for off-line searchthey find entire paths to the
goal before any execution begins. In real-time search (Korf, 1990), search and execution
can happen in parallel, but an agent is only allowed a fixed amount of time to plan before
it must perform each action. Real-time search has the possibility of being more efficient
724

fiHeuristic Search When Time Matters

than off-line search in terms of goal achievement time, because if all search happens in
parallel with execution, then goal achievement time is simply the execution time plus the
small amount of time required to find the very first action. This in contrast to the off-line
approach where goal achievement time is the sum of the entire search time and the execution
time. In some situations, however, starting execution before having a complete plan to a
goal is not acceptable, as it may lead an agent into a dead-end from which it can no longer
reach any goal, so real-time search may not be applicable. Examples of domains with deadends include robotics, manufacturing (Ruml, Do, Zhou, & Fromherz, 2011), and spacecraft
control: exactly those applications involving high value or danger, where automation is
most worthwhile. In these cases, it is desirable to find an entire plan guaranteed to reach
the goal, before any execution begins.
Hernandez, Baier, Uras, and Koenig (2012) introduce a model for comparing real-time
algorithms to off-line techniques such as A*, called the game time model. The game time
model partitions time into uniform intervals, and the agent can execute a single action
during each interval. Path planning can happen in parallel with execution (the agent can
plan step t during the execution of step t-1), and the goal is to move the agent from its start
location to its goal location in as few time intervals as possible, minimizing goal achievement
time, the same objective that we discuss in Section 1. The game time model is a special
case of the utility functions considered in this paper where solution cost is given in discrete,
fixed-duration units of time.
Real-time search provides two benefits: first, it may be possible to reduce the goal
achievement time by allowing search and execution to happen at the same time, and second,
the agent can start moving toward its goal right awaya necessary property for video games.
This leaves us with the question of whether or not real-time search algorithms can achieve
better goal achievement time than the off-line utility-aware methods. On one hand, realtime search algorithms spend very little time searching without making progress toward
the goal. On the other hand, real-time search algorithms tend to make decisions based on
very local information and can find more costly solutions. In their results, Hernandez et al.
report that their best approach solves problems on initially-known grid maps in about the
same number of time intervals as A*. In the previous section, we showed that the utilityaware techniques outperformed A* for most utility functions. In this section, we compare
a state-of-the-art real-time search algorithm called LSS-LRTA* (Koenig & Sun, 2009) to
Bugsy on the platform pathfinding domain.4
As with our previous experiments, we tested all algorithms with a variety of values for
the ratio wf /wt . Since we are interested in goal achievement time, we set wt = 1 and
we calculate search time in units of seconds. This means that wf represents the number
of seconds in one unit of execution costthe speed of the agent. We set the real-time
constraint for LSS-LRTA* such that it was allowed to plan for the duration of one unit
of execution, and so it always had its next action ready for execution when its currently
executing action completed.
4. We do not compare to Time-bounded A* (TBA, Bjornsson, Bulitko, & Sturtevant, 2009), the method
that performed the best for Hernandez et al. (2012), because the platform domain forms a directed search
graph, and TBA* only works on undirected search graphs. We also did not compare with the newer
f -LRTA* (Sturtevant, 2011), because it did not perform as well as LSS-LRTA* on the platform domain,
which has directed edges.

725

fiBurns, Ruml, & Do

platform
log10 factor of best GAT

2.4

LSS-LRTA*
A*
Speedy
Bugsy

1.6

0.8

0
-4

-3

-2

log10 w_f / w_t

-1

0

Figure 13: Comparison of Bugsy and real-time search.
Figure 13 shows the results of the comparison. As we can see, LSS-LRTA* gives rather
poor performance; its goal achievement times nearly match that of A*, but Bugsy was able
to achieve the goal much faster. This shows that simply allowing search and execution to
take place in parallel is not sufficient to reduce goal achievement time; it can be better to
spend time searching the solution all of the way to the goal if the alternative is to spend a
long time executing a poor plan.
6.12 Decision-theoretic A*
Decision-theoretic A* (DTA*, Russell & EricWefald, 1991) is a utility-aware algorithm that
allows for concurrent search and execution. It is based on ideas from real-time heuristic
search, but unlike traditional real-time search, where each action is emitted after a fixed
amount of search, DTA* decides when to stop searching and emit an action using a decisiontheoretic analysis. At any time there is a single best top-level action with the lowest cost
estimate. The search emits this action if it is decided that the utility of emitting the
action outweighs the utility of further search. DTA* uses an approximation (found by
off-line training) of how the solution cost estimate for each top-level action improves with
additional search. Using a consistent heuristic, this estimate can only increase (Nilsson,
1980), so DTA* stops searching when it decides that the time required to raise the best
actions estimated cost to the point that it is no longer the best action will be more costly
than the expected gain from determining that there is a different best action.
Compared to Bugsy, DTA* is relatively myopic because it only considers the cost of
search involved in selecting individual actions. DTA* does not consider the additional search
required by the solution path to which it commits by choosing an action. Where Bugsy
uses both d and expansion delay to reason about the required search effort for the entire
726

fiHeuristic Search When Time Matters

log10 factor of best utility

platform (small instances)
Speedy
A*
DTA*
Bugsy

1.2

0.8

0.4

0
-4

-3

-2

log10 w_f / w_t

-1

0

Figure 14: Comparison of Bugsy and DTA*.
path beneath a node, DTA* only reasons about the search required to determine the best
action to emit right now.
We implemented DTA* to assess how a utility-aware real-time search might compare
to a utility-aware off-line search for planning under time pressure. Figure 14 shows the
results of a comparison between DTA* and Bugsy on the platform pathfinding domain.
Unfortunately, DTA* had fairly poor performance, so our experiment used smaller instances
consisting of 25x25 blocks, instead of the 50x50 block instances used in previous experiments.
Following Russell and EricWefald (1991), we gathered off-line training data for DTA* using
states sampled uniformly with a probability of 0.1 from among those visited by a real-time
search algorithm. Russell and EricWefald (1991), used an algorithm called SLRTA*, but
we used LSS-LRTA*, because it is the current state of the art. Our training set consisted
of 100 25x25 platform instances. We also verified our implementation by ensuring that
it compared favorably to A* and Speedy search on the 15-puzzlethe same domain used
by Russell and EricWefaldusing a variety of different utility functions. From Figure 14,
we can see DTA* often had significantly worse utility than Bugsy, often performing only
slightly better than Speedy search, and sometimes performing worse than A*, for example,
when cheap solutions were desired.

7. Related Work
Because Bugsy uses estimates of its own search time to select whether to terminate or continue, and to select which node to expand, it may be said to be engaging in metareasoning,
that is, reasoning about which reasoning action to take. There has been much work on this
topic in AI since the late 1980s (Dean & Boddy, 1988) and continuing today (Cox & Raja,
2011).
727

fiBurns, Ruml, & Do

Dean and Boddy (1988) consider the problem faced by an agent that is trying to respond
to predicted events while under time constraints. Unlike our setting, their concern is with
choosing how much time to allocate for prediction and how much to allocate for deliberation. To solve this type of time-dependent planning problem, they suggest the use of (and
also coined the term) anytime algorithms. Unlike the anytime-based techniques discussed
previously, which attempt to find a stopping policy to optimize a utility function, Dean
and Boddy used anytime algorithms as a means for allowing different allocations of time
between predicting and deliberation. Later, Boddy and Dean (1989) show how anytime
algorithms and their time-dependent planning framework can be used by a delivery agent
that must traverse a set of waypoints on a grid, by allocating time between the ordering of
waypoints and the planning used to travel between them. Dean, Kaelbling, Kirman, and
Nicholson (1993) also adapt the technique for scheduling deliberation and execution when
planning in the face of uncertainty.
Garvey and Lesser (1993) present design-to-time methods that advocate using all available time to find the best possible solution. Unlike anytime approaches that can be interrupted at any time, the design-to-time method requires the time deadline to be given
upfront. This way, the algorithm can spend all of its time focusing on finding a single
good solution, instead of possibly wasting time finding intermediate results. Design-to-time
also differs from contract techniques like DAS (Dionne et al., 2011), because in the designto-time framework there must be a predefined set of solvers with known (or predictable)
solution times and costs. The design-to-time method will select an appropriate solver for
the problem and deadline, possibly interleaving different solvers if deemed appropriate. The
information about cost and solutions times, which design-to-time methods require, is usually unavailable and must be learned off-line. Techniques like DAS and Bugsy, on the other
hand, only use information that can be computed on-line.
Hansen, Zilberstein, and Danilchenko (1997) show how heuristic search with inadmissible
heuristics can be used to make anytime heuristic search algorithms. Like the techniques
presented in this paper, they consider the problem of trading-off search effort for solution
quality. To this end, they propose one possible optimization function for anytime heuristic
search search that attempts to maximize the rate at which the algorithm decreases solution
cost. Like the anytime monitoring technique shown in Section 3.1, their evaluation function
relies on learning the profile of the anytime algorithm offline. In their analysis of the 8puzzle, they conclude that, while their method had good anytime behavior, there was little
benefit of using it instead of trial-and-error-based hand tuning. This is not surprising given
the strong performance demonstrated by offline-tuned weighted A* in our experiments.
More recently, Thayer et al. (2012) proposed an approach for minimizing the time between solutions in anytime algorithms. They demonstrate that their new state-of-the-art
algorithm performs well on a wide variety of domains, and it can be more robust than
previous approaches. Like Bugsy, their technique relies on using d heuristics to estimate
the search effort required to find solutions. However, they only focus on solutions that will
require the least amount of effort, and do not optimize for a trade-off of search time for
solution cost.
In addition to controlling expansion decisions, metareasoning can also be used during
heuristic evaluation. Often search algorithms will use the maximum value computed over
multiple heuristics as a more accurate estimate of cost to goal. For some problems, like
728

fiHeuristic Search When Time Matters

domain-independent planning, heuristics are quite expensive, so the increased accuracy
gained via maximizing over many heuristics may not be worth the increased computation
time. Domshlak, Karpas, and Markovitch (2010) introduce an on-line learning technique
to decide on a single heuristic to compute for each state, instead of computing many and
taking the max.
Other related work using metareasoning to control combinatorial search has been done in
the area of constraint satisfaction problems (CSPs), and boolean satisfiability (SAT). Tolpin
and Shimony (2011) use rational metareasoning to decide when to compute value ordering
heuristics in a CSP solver. The focus of the work was on value ordering heuristics that
gave solution count estimates; the solver only bothered to compute the heuristic at decision
points where it was deemed worthwhile. Their experiments demonstrate that the new
metareasoning variant outperformed both the variant that always computed the heuristic
and one that computed the heuristic randomly. Horvitz, Ruan, Gomes, Kautz, Selman,
and Chickering (2001) apply Bayesian structure learning to CSPS and SAT problems. They
consider the problem of quasi-group completion, and unlike Tolpin and Shimony (2011) who
use on-line metareasoning to control search, they use off-line Bayesian learning over a set
of hand-selected variables to predict whether instances will be long or short running.
There has been a lot of work on attempting to estimate the size of search trees offline (Burns & Ruml, 2013; Knuth, 1975; Chen, 1992; Kilby, Slaney, Thiebaux, & Walsh,
2006; Korf, Reid, & Edelkamp, 2001; Zohavi, Felner, Burch, & Holte, 2010). This is a
related topic, as it is concerned with estimating search effort before an entire search has
been performed. One may imagine leveraging such a technique to predict search time in
an algorithm like Bugsy. Unfortunately, these estimation methods can be rather costly in
terms of computation time, so they are not suitable as an estimator that is needed at every
single node generation. Another possibility is to use off-line estimations to find parameters
that affect the performance of search on a given domain. This knowledge could be helpful for
creating the representative training sets used by algorithms like weighted A* and anytime
monitoring, which require off-line training.

8. Conclusions
We have investigated utility-aware search algorithms that take into account a user-specified
preference trading-off search time and solution cost. We presented three different techniques
for addressing this problem. The first method was based on previous work in the area of
learning stopping policies for anytime algorithms. To the best of our knowledge, we are the
first to demonstrate these techniques in the area of heuristic search. The second method was
a novel use of algorithm selection for bounded-suboptimal search that chooses the correct
weight to use for weighted A* for a given utility function. The last technique that we
presented was the Bugsy algorithm. Bugsy is the only technique of the three that does
not require off-line training.
We performed an empirical study of these techniques in the context of heuristic search,
investigated the effect of the parameters of each algorithm on performance, and compared
the different techniques to each other. Surprisingly, the simplest technique of learning a
weight for weighted A* was able to achieve the greatest utility on many problems, outperforming the conventional anytime monitoring approach. Also surprisingly, Bugsy, an
729

fiBurns, Ruml, & Do

algorithm that does not use any off-line training, performed just as well as the off-line
techniques that had the advantage of learning from thousands of off-line training instances.
If a representative set of training instances is not available then Bugsy is the algorithm
of choice. Overall, the utility-aware methods outperformed both A* and Speedy search
for a wide range of utility functions. This demonstrates that heuristic search is no longer
restricted to solely optimizing solution cost, freeing a user from the choice of either slow
search times or expensive solutions.
Unlike previous methods for trading deliberation time for solution quality, Bugsy considers the trade-off directly in the search algorithmdeciding, for each node, whether the
result of expansion is worth the time. This new approach provides an alternative to anytime algorithms. Instead of returning a stream of solutions and relying on an external
process to decide when additional search effort is no longer justified, the search process
itself makes such judgments based on the node evaluations available to it. Our empirical
results demonstrate that Bugsy provides a simple and effective way to solve shortest-path
problems when computation time matters. We would suggest that search procedures are
usefully thought of not as black boxes to be controlled by an external termination policy
but as complete intelligent agents, informed of the users goals and acting rationally on the
basis of the information they collect so as to directly maximize the users utility.

Acknowledgments
We greatly appreciate feedback and suggestions from Shlomo Zilberstein and Scott Kiesel.
We would also like to think Richard Korf for pointing out the work of Shekhar and Dutta
(1989). We are also grateful for support from the NSF (grant 0812141 and grant 1150068),
the DARPA CSSG program (grant D11AP00242), and a University of New Hampshire
Dissertation Year Fellowship. A preliminary version of Bugsy was presented by Ruml and
Do (2007); see Appendix A. Elisabeth Crawford assisted with the original version during a
summer internship at PARC.

Appendix A. Previous Bugsy
A previous version of Bugsy was proposed by Ruml and Do (2007), however, this early
realization differs substantially from the one presented here. It used aggressive duplicate
re-expansion, heuristic corrections, and it only used d to estimate the remaining expansions
until a goal is reached. In Section 6.7 we showed that duplicate dropping outperforms duplicate re-expansion in many domains. We found that inadmissible heuristics performed
poorly (cf Section 6.5) in practice, even when compared to the standard admissible estimates. Also, to temper the inadmissible of its corrected estimates, the previous Bugsy
multiplied its heuristic estimates by an arbitrary weight (min(200, (wt /wf ))/1000); our version does not require this ad hoc fix. We discussed why d is a poor estimate for the number
of remaining expansions in Section 5.1, and in Section 6.6 we showed, experimentally, that
using expansion delay performs much better than just using d alone.
Recall that Bugsy uses f and d to approximate the cost and path length of the best
utility outcome that is enabled by the expansion of the node. Note, however, that the f and
d function used throughout this paper refer to the cheapest solution beneath a node n, and
730

fiHeuristic Search When Time Matters

this may not be the goal that results in the maximum utility. To better assess the available
outcomes, the previous version of Bugsy computed two utility estimates for each node: one
for the cheapest solution beneath the node and the other for the nearest solution in terms of
node expansions. In non-unit-cost domains, these two estimates may differ. For example, on
the life-cost grid pathfinding domains, the cheapest solution usually involves moving toward
the top of the grid where actions are cheap, but the nearest solution will follow a straight-line
path to the goal. In general, there can be a large number of different solutions through each
search node, and the solutions may cover a whole spectrum of different cost/time trade-offs.
By considering more than just the cheapest solution, as was done in our implementation,
it may be possible to find solutions with better utility. On the other hand, it may be too
costly to compute multiple heuristics for each node, so whether or not this modification is
beneficial depends on the domain.

Appendix B. Domains
We performed our experiments on a variety of different domains, which we describe in
further detail here.
B.1 15-Puzzle
The 15-puzzle is one of the most popular benchmark domains for heuristic search algorithms.
It consists of a 4-by-4 frame into which 15 tiles have been placed. One slot of the board
does not contain a tile, it is called the blank. Tiles that are above, below, left of or right
of the blank may be slid into the blank slot. The objective of the 15-puzzle is to slide tiles
around in order to transform an initially scrambled puzzle into the goal state with the blank
in the upper-left corner and the tiles ordered 115 going from left to right, top to bottom.
This domain is interesting because plans are hard to find, the branching factor is small and
varies little from its mean of about 2.13 (Korf et al., 2001), there are few duplicates, and
the heuristic is reasonably informed.
In our experiments we use the popular 100 15-puzzle instances created by Korf (1985).
In plots that include A*, however, we only used the 94 instances solvable by A* in 6GB of
memory. The average optimal solution length for these instances was 52.4. For our training
set, we generated 1,000 instances using a 1 million step random walk back from the goal
position. We used the Manhattan distance heuristic, which sums the vertical and horizontal
distance that each tile must move to arrive at its goal position. Our implementation follows
the heavily optimized solver presented by Burns et al. (2012).
B.2 Pancake Puzzle
The pancake puzzle (Dweighter, 1975; Gates & Papadimitriou, 1979) is another permutation
puzzle. It consists of a stack of differently sized pancakes numbered 1N . The pancakes
must be presented at a fancy breakfast, so a chef needs to sort the originally unordered
stack of pancakes by continually sticking a spatula into the stack and reversing the order of
the pancakes above. Said another way, the pancake problem involves sorting a sequence of
numbers by using only prefix reversal operations. This simple problem is interesting because
it creates a search graph with a large branching factor (the number of pancakes minus one).
731

fiBurns, Ruml, & Do

Figure 15: A screenshot of the platform pathfinding domain (left), and a zoomed-out image
of a single instance (right). The knight must find a path from its starting
location, through a maze, to the door (on the right-side in the left image, and
just above the center in the right image).

For our experiments, we used 25 randomly generated 50-pancake puzzle instances, and our
training set consisted of 1,000 randomly generated instances. We used the powerful GAP
heuristic of Helmert (2010), which sums the number of pairs of adjacent pancakes that are
not in sequence.
B.3 Platform Pathfinding
The platform domain is a pathfinding domain of our own creation with dynamics based
on a 2-dimensional platform-style video game, written partially by the first author, called
mid5 . The left image of Figure 15 shows a screenshot from mid. The goal is for the knight
to traverse a maze from its initial location, jumping from platform to platform, until it
reaches the door. Mid is an open source game available from http://code.google.com/p/
mid-game. For our experiments the game physics of the game were ported from C to C++
and were embedded in our C++ search codebase. We generated 1,000 training instances
and 100 test instances using the level generator from mid. An example instance is shown
on the right panel in Figure 15. The domain is unit-cost and has a large state space with
a well-informed heuristic.
The available actions are different combinations of controller keys that may be pressed
during a single iteration of the games main loop: left, right, and jump. Left and right move
to the knight in the respective directions (holding both at the same time is never considered
by the search domain, as the movements would cancel each other out, leaving the knight in
5. The other author was Steve McCoy, who also drew the tile graphics shown in Figure 15.

732

fiHeuristic Search When Time Matters

place), and the jump button makes the knight jump, if applicable. The knight can jump
to different heights by holding the jump button across multiple actions in a row up to a
maximum of 8. The actions are unit cost, so the cost of an entire solution is the number of
game loop iterations, called frames, required to execute the path. Each frame corresponds
to 50ms of game play.
Each state in the state space contains the x, y position of the knight using doubleprecision floating point values, the velocity in the y direction (x velocity is not stored as
its determined solely by the left and right actions), the number of remaining actions for
which pressing the jump button will add additional height to a jump, and a boolean stating
whether or not the knight is currently falling. The knight moves at a speed of 3.25 units per
frame in the horizontal direction, it jumps at a speed of 7 units per frame, and to simulate
gravity while falling, 0.5 units per frame are added to the knights downward velocity up to
a maximum of 12 units per frame.
For further details on the platform domain, please refer to the source code repository
given at the start of Section 6.
B.3.1 Level Generator
The instances used in our experiments were created using the level generator from mid, a
special maze generator that builds 2-dimensional platform mazes on a 5050 grid of blocks.
Each block is either open or occluded, and to ensure solvability given the constraints imposed
by limited jump height, the generator builds the maze by stitching together pieces from a
hand-created portfolio. Each piece consists of a number of blocks that are either free or
occluded, and a start and end location for which traversability is ensured within the piece.
A piece can be added to the grid at any location for which it fits. A piece fits if it does not
occlude a block that belongs to a previously placed piece. The maze is built using a depthfirst procedure: a piece is selected at random and if it fits in the grid with its start location
lined up with the end location of its predecessor then it is placed and the procedure recurs.
The number of successors of each node is chosen uniformly from the range 39 inclusive, and
the procedure backtracks when there are no pieces that fit on the previous block. Once the
maze is constructed, blocks that do not belong to any piece are marked as occluded. The
right image in Figure 15 shows a sample of a level generated by this procedure. The source
code for the level generator is available in the mid source repository mentioned above.
B.3.2 Heuristic
We developed a heuristic for the platform domain that is based on visibility navigation
(Nilsson, 1969). Each maze is pre-processed to convert its grid representation into a set of
polygons representing each connected component of occluded cells in the level. The space
is then scaled to account for the movement speed of the knight; the knight can fall faster
than it can move in the horizontal direction, so the polygons end up squished vertically
and stretched horizontally. The visibility navigation problem is then solved in reverse from
the four corners of the goal cell to the center of every non-occluded cell of the maze. To
maintain admissibility, the cost of each edge in the visibility problem is not the length
of

the visibility line, but instead is the maximum of the length of the line divided by 2 and
the X and Y displacements between the end points of the line. This accounts for the fact
733

fiBurns, Ruml, & Do

Figure 16: The visibility navigation instance for the platform domains heuristic. The visibility path between the initial state and the goal state is drawn in red.

that the knight can bemoving both horizontally and vertically at the same time, and that
moving a distance of 2 in the scaled space still takes only a single frame.
During search, the heuristic value of a state is computed in one of two different ways.
If the straight-line path from the center of the knight to the goal is not occluded then the
maximum of the X and Y distances to the goal scaled down by travel speed is used as the
heuristic estimate. Otherwise, the heuristic is the cost of the path in the visibility graph
from the center of the cell that contains the knights center point minus the maximum of
the X and Y distance (in number of frames) of the knights center point to the center of
its cell. Figure 16 shows the same map from the right image of Figure 15, scaled, broken
into polygon components, and with the visibility path between the initial state and the goal
state drawn in red.
B.4 Grid Pathfinding
Our final domain was grid pathfinding. This is a very popular domain in both video games
and robotics, as such it has garnered much attention in the heuristic search community. In
our experiments, we used 5,000x5,000 grids with both four-way and eight-way connectivity
and uniform obstacle distributions. For four-way connected grids, each cell was blocked
with a probability of 0.35, and for eight-way connected grids cells were blocked with a
probability 0.45. We also consider two different cost models, the standard
 unit cost model
in which horizontal and vertical moves cost 1 and diagonal moves cost 2. The other is
called the life cost model, where each move has a cost equal to the row number from which
the move took place, causing cells toward the top of the grid to be preferred. With the
life cost model, short direct solutions can be found quickly, however they will be relatively
expensive, while a least-cost solution involves many annoying economizing steps (Ruml &
Do, 2007). This model can be viewed as an instantiation of the popular belief that time is
money, as one can choose to incur additional cost for a shorter and simpler path. For each
combination of movement model and cost model, we generated 25 test instances and 1,000
training instances. Finally, we used the Manhattan distance heuristic for four-connected
grids and the octile distance heuristic for eight-connected grids. For the life cost model our
734

fiHeuristic Search When Time Matters

heuristics also took into account the fact that moving toward the top of the grid then back
down may be cheaper than a direct path.

Appendix C. Anytime Policy Estimation
It can be challenging to write algorithms that rely on off-line training data. If the algorithm
behaves unexpectedly, then it is unclear if there is a bug in the implementation, a bug in the
off-line learning procedure, or if the training set is merely insufficiently representative. In
this appendix, we describe how we implemented and verified our procedure for estimating
an anytime profile.
Figure 17 shows the pseudocode for building a profile based on the description given
by Hansen and Zilberstein (2001). The algorithm accepts a set of solution streams as
input, one stream for each solved instance, then proceeds in two steps. The first step is
the Count-Solutions function that counts the number of times each solution cost was
improved upon. The function iterates through each solution (line 5), computes the bin of a
histogram into which its cost value falls (line 7), then for each subsequent solution a count
is added to qqtcounts for each time step for which the first solution improved to the second
solution. In addition, the number of total improvements for each solution and time bin are
counted in the qtcounts array. The costbin and timebin functions bin cost and time values
respectively by returning an integer for the corresponding index in the histogram:


q  qmin
costbin(q) =
(qmax  qmin )/ncost


t  tmin
timebin(t) =
(tmax  tmin )/ntime
.
The second step is the Probabilities function that converts the counts computed in
the first step into normalized probability values. This is achieved by dividing the number
of t steps for which a solution of cost qi improved to a solution of cost qj (qqtcounts)
by the total number of steps for which a solution of cost qi was improved (qtcounts, and
lines 28). The probability values are smoothed by adding half of the smallest probability
to each bin representing a solution cost improvement. This step removes zero-probabilities,
allowing improvement to be considered. Finally, the probabilities are normalized so that
the probability of all non-decreasing-cost solutions for each current cost and time step sum
to one (lines 3137). Once the profile is computed, it is saved to disk for later use when
computing the stopping policy.
We found that it was extremely useful to have a simple way to validate our policies
while debugging our implementation. One option is to create a stopping policy, run ARA*
with monitoring on a handful of instances and with a handful of utility functions to verify
that it gives the expected behavior. Unfortunately, this approach was rather cumbersome
and prone to error, as it only evaluated the policy on the small number of instances that we
were willing to run by hand. Instead, we chose to validate our implementation by plotting
the polices generated from the training data on different utility functions. By plotting
the extreme policies that only care about solution cost and search time, along with some
intermediate policies that trade-off the two, it was much simpler to debug our code.
735

fiBurns, Ruml, & Do

Profile(streams)
1. qtcounts, qqtcounts  Count-Solutions(streams)
2. return Probabilities(qtcounts, qqtcounts)
Count-Solutions(streams)
3. qtcounts  new int[ncost][ntime] // Initialized to zero.
4. qqtcounts  new int[ncost][ncost][ntime] // Initialized to zero.
5. for s in streams
6.
for i from 1 to |s|
7.
qi  costbin(s[i].cost)
8.
qcur  qi , tcur  0
9.
// Count cost at each time increment after solution i.
10.
for j from i + 1 to |s|
11.
qnext  costbin(s[j].cost)
12.
tnext  timebin(s[j].time  s[i].time)
13.
// Current solution cost up to the time of solution j.
14.
for t = tcur to tnext  1
15.
increment qtcounts[qi ][t]
16.
increment qqtcounts[qcur ][qi ][t]
17.
qcur  qnext , tcur  tnext
18.
// Last solution cost up to the final time.
19.
for t = tcur to ntime
20.
increment qtcounts[qi ][t]
21.
increment qqtcounts[qcur ][qi ][t]
22. return qtcounts, qqtcounts
Probabilities(qtcounts, qqtcounts)
23. probs  new float[ncost][ncost][ntime]
24. for qi from 1 to ncost
25.
for t from 1 to ntime
26.
if qtcounts[qi ][t] = 0 then continue
27.
for qj from 1 to ncost
28.
probs[qj ][qi ][t]  qqtcounts[qj ][qi ][t]/qtcounts[qi ][t]
29. Smoothing: add half of smallest probability to all elements of probs with improving solution cost.
30. // Normalize.
31. for qi from 1 to ncost
32.
for t from 1 to ntime
33.
sum  0
34.
for qj from 1 to ncost
35.
sum  sum + probs[qj ][qi ][t]
36.
for qj from 1 to ncost
37.
probs[qj ][qi ][t]  probs[qj ][qi ][t]/sum
38. return probs

Figure 17: Pseudocode for profile estimation.

Figure 18 shows some of the plots created for the platform domain. Each plot has cost
on the y axis and time on the x axis. Green circles represent inputs for which the policy
says to keep searching, and red crosses represent inputs for which the policy says to stop
736

fiHeuristic Search When Time Matters

(a)

(b)

1000

100

200

300

2000

cost

2000

cost

cost

2000

(c)

1000

100

time

200

time

300

1000

100

200

300

time

Figure 18: Three different policies: (a) prefers cheaper solutions at any expense (wf =
1, wt = 0), (b) attempts to trade some search time for some solution cost (wf =
0.6, wt = 1), and (c) prefers to have any solution as fast as possible (wf = 0, wt =
1).

searching and return the solution. As expected, the policy always continues when the goal
is to minimize solution cost and always stops when the goal is to minimize search time (cf.
the left-most and right-most plots in Figure 18 respectively). The center plot shows that
we also successfully found policies that trade search time for solution cost by only stopping
once the cost is sufficiently low. Finally, in the left-most plot, the bottom-most and rightmost sides of the policy always stop as our implementation chose to stop when there was
no training data available to estimate the profile for the given input values.

References
Bjornsson, Y., Bulitko, V., & Sturtevant, N. (2009). TBA*: time-bounded A*. In Proceedings
of the Twenty-first International Joint Conference on Artificial Intelligence (IJCAI09), pp. 431436.
Boddy, M., & Dean, T. (1989). Solving time-dependent planning problems,. In Proceedings
of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89),
Vol. 2, pp. 979984.
Burns, E., Hatem, M., Leighton, M. J., & Ruml, W. (2012). Implementing fast heuristic
search code. In Proceedings of the Fifth Annual Symposium on Combinatorial Search
(SoCS-12).
Burns, E., & Ruml, W. (2013). Iterative-deepening search with on-line tree size prediction.
Annals of Mathematics and Artificial Intelligence, S68, 123.
Chen, P. C. (1992). Heuristic sampling: a method for predicting the performance of tree
searching programs. SIAM Journal on Computing, 21 (2), 295315.
Cox, M. T., & Raja, A. (2011). Metareasoning: Thinking about thinking. MIT Press.
737

fiBurns, Ruml, & Do

Dean, T., & Boddy, M. (1988). An analysis of time-dependent planning. In Proceedings of
the Seventh National Conference on Artificial Intelligence (AAAI-88), pp. 4954.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in
stochastic domains. In Proceedings of the eleventh national conference on Artificial
intelligence, Vol. 574, p. 579. Washington, DC.
Dechter, R., & Pearl, J. (1988). The optimality of A*. In Kanal, L., & Kumar, V. (Eds.),
Search in Artificial Intelligence, pp. 166199. Springer-Verlag.
Dionne, A. J., Thayer, J. T., & Ruml, W. (2011). Deadline-aware search using on-line measures of behavior. In Proceedings of the Fourth Annual Symposium on Combinatorial
Search (SoCS-11).
Domshlak, C., Karpas, E., & Markovitch, S. (2010). To max or not to max: Online learning
for speeding up optimal planning. In AAAI Conference on Artificial Intelligence
(AAAI-10), pp. 17011706.
Dweighter, H. (1975). Elementary problem E2569. American Mathematical Monthly, 82 (10),
1010.
Finkelstein, L., & Markovitch, S. (2001). Optimal schedules for monitoring anytime algorithms. Artificial Intelligence, 126 (1), 63108.
Garvey, A. J., & Lesser, V. R. (1993). Design-to-time real-time scheduling. Systems, Man
and Cybernetics, IEEE Transactions on, 23 (6), 14911502.
Gates, W. H., & Papadimitriou, C. H. (1979). Bounds for sorting by prefix reversal. Discrete
Mathematics, 27 (1), 4757.
Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. Journal of Artificial Intelligence
Research, 28 (1), 267297.
Hansen, E. A., & Zilberstein, S. (2001). Monitoring and control of anytime algorithms: A
dynamic programming approach. Artificial Intelligence, 126, 139157.
Hansen, E. A., Zilberstein, S., & Danilchenko, V. A. (1997). Anytime heuristic search: First
results. Tech. rep., University Massachusetts, Amherst.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics,
SSC-4 (2), 100107.
Helmert, M. (2010). Landmark heuristics for the pancake problem. In Proceedings of the
Third Symposium on Combinatorial Search (SoCS-10).
Helmert, M., & Roger, G. (2008). How good is almost perfect. In Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-08).
Hernandez, C., Baier, J., Uras, T., & Koenig, S. (2012). Time-bounded adaptive A*. In
Proceedings of the Eleventh International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS-12).
Horvitz, E., Ruan, Y., Gomes, C., Kautz, H., Selman, B., & Chickering, M. (2001). A
Bayesian approach to tackling hard computational problems. In Proceetings of the
Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI-01).
738

fiHeuristic Search When Time Matters

Kilby, P., Slaney, J., Thiebaux, S., & Walsh, T. (2006). Estimating search tree size. In
Proceedings of the twenty-first national conference on artificial intelligence (AAAI06).
Knuth, D. E. (1975). Estimating the efficiency of backtrack programs. Mathematics of
computation, 29 (129), 121136.
Koenig, S., & Sun, X. (2009). Comparing real-time and incremental heuristic search for
real-time situated agents. Autonomous Agents and Multi-Agent Systems, 18 (3), 313
341.
Korf, R. E. (1985). Iterative-deepening-A*: An optimal admissible tree search. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, pp.
10341036.
Korf, R. E. (1990). Real-time heuristic search. Artificial intelligence, 42 (2-3), 189211.
Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity of iterative-deepening-A*.
Artificial Intelligence, 129 (1), 199218.
Likhachev, M., Gordon, G., & Thrun, S. (2003). ARA*: Anytime a* with provable bounds
on sub-optimality. Advances in Neural Information Processing Systems (NIPS-03),
16.
Michie, D., & Ross, R. (1969). Experiments with the adaptive graph traverser. In Machine
Intelligence 5, pp. 301318.
Nilsson, N. J. (1969). A mobile automaton: an application of artificial intelligence techniques. In Proceedings of the First International Joint Conference on Artificial intelligence (IJCAI-69), pp. 509520.
Nilsson, N. J. (1980). Principles of Artificial Intelligence. Tioga Publishing Co.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1, 193204.
Rice, J. R. (1976). The algorithm selection problem. Advances in Computers, 15, 65118.
Richter, S., Thayer, J. T., & Ruml, W. (2010). The joy of forgetting: Faster anytime search
via restarting. In Proceedings of the Twentieth International Conference on Automated
Planning and Scheduling (ICAPS-10), pp. 137144.
Ruml, W., Do, M., Zhou, R., & Fromherz, M. P. (2011). On-line planning and scheduling: An
application to controlling modular printers. Journal of Artificial Intelligence Research,
40 (1), 415468.
Ruml, W., & Do, M. B. (2007). Best-first utility-guided search. In Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI-07), pp. 23782384.
Russell, S., & EricWefald (1991). Do the right thing: studies in limited rationality. The
MIT Press.
Shekhar, S., & Dutta, S. (1989). Minimizing response times in real time planning and
search. In Proceedings of the Eleventh International Joint Conference on Artificial
Intelligence (IJCAI-89), pp. 238242. Citeseer.
739

fiBurns, Ruml, & Do

Sturtevant, N. (2012). Benchmarks for grid-based pathfinding. Transactions on Computational Intelligence and AI in Games, 4 (2), 144  148.
Sturtevant, N. R. (2011). Distance learning in agent-centered heuristic search. In Proceedings
of the Fourth Annual Symposium on Combinatorial Search (SoCS-11).
Thayer, J. (2012). Faster Optimal and Suboptimal Heuristic Search. Ph.D. thesis, University
of New Hampshire.
Thayer, J. T., Benton, J., & Helmert, M. (2012). Better parameter-free anytime search by
minimizing time between solutions. In Proceedings of the Fifth Annual Symposium on
Combinatorial Search (SoCS-12).
Thayer, J. T., Dionne, A., & Ruml, W. (2011). Learning inadmissible heuristics during
search. In Proceedings of the Twenty-first International Conference on Automated
Planning and Scheduling (ICAPS-11).
Thayer, J. T., & Ruml, W. (2008). Faster than weighted a*: An optimistic approach to
bounded suboptimal search. In Proceedings of the Eighteenth International Conference
on Automated Planning and Scheduling (ICAPS-08).
Thayer, J. T., & Ruml, W. (2009). Using distance estimates in heuristic search. In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS-09).
Thayer, J. T., & Ruml, W. (2010). Anytime heuristic search: Frameworks and algorithms.
In Proceedings of the Third Annual Symposium on Combinatorial Search (SoCS-10).
Tolpin, D., & Shimony, S. E. (2011). Rational deployment of CSP heuristics. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence
(IJCAI-11).
Valenzano, R. A., Sturtevant, N., Schaeffer, J., Buro, K., & Kishimoto, A. (2010). Simultaneously searching with multiple settings: An alternative to parameter tuning for
suboptimal single-agent search algorithms. In Proceedings of the Twentieth International Conference on Automated Planning and Scheduling (ICAPS-10).
van den Berg, J., Shah, R., Huang, A., & Goldberg, K. (2011). ANA*: Anytime nonparametric A*. In Proceedings of Twenty-fifth AAAI Conference on Artificial Intelligence
(AAAI-11).
Zohavi, U., Felner, A., Burch, N., & Holte, R. (2010). Predicting the performance of ida*
using conditional distributions. Journal of Artificial Intelligence Research, 37 (1), 41
84.

740

fiJournal of Artificial Intelligence Research 47 (2013) 853-899

Submitted 03/13; published 08/13

Framing Image Description as a Ranking Task:
Data, Models and Evaluation Metrics
Micah Hodosh
Peter Young
Julia Hockenmaier

mhodosh2@illinois.edu
pyoung2@illinois.edu
juliahmr@illinois.edu

Department of Computer Science
University of Illinois
Urbana, IL 61801, USA

Abstract
The ability to associate images with natural language sentences that describe what
is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to
frame sentence-based image annotation as the task of ranking a given pool of captions.
We introduce a new benchmark collection for sentence-based image description and search,
consisting of 8,000 images that are each paired with five different captions which provide
clear descriptions of the salient entities and events. We introduce a number of systems
that perform quite well on this task, even though they are only based on features that
can be obtained with minimal supervision. Our results clearly indicate the importance of
training on multiple captions per image, and of capturing syntactic (word order-based) and
semantic features of these captions. We also perform an in-depth comparison of human
and automatic evaluation metrics for this task, and propose strategies for collecting human
judgments cheaply and on a very large scale, allowing us to augment our collection with
additional relevance judgments of which captions describe which image. Our analysis shows
that metrics that consider the ranked list of results for each query image or sentence are
significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems
may be fully automated.

1. Introduction
The ability to automatically describe the entities, events and scenes depicted in an image
is possibly the most ambitious test of image understanding. Any advances on this task
have significant practical implications, since there are billions of images on the web and in
personal photo collections. Our ability to efficiently access the wealth of information they
contain is hampered by limitations of standard image search engines, which must rely on
text that appears near the image (Datta, Joshi, Li, & Wang, 2008; Popescu, Tsikrika, &
Kludas, 2010). There has been a lot of work on the multi-label classification problem of
associating images with individual words or tags (see, e.g., Blei & Jordan, 2003; Barnard,
Duygulu, Forsyth, Freitas, Blei, & Jordan, 2003; Feng & Lapata, 2008; Deschacht & Moens,
2007; Lavrenko, Manmatha, & Jeon, 2004; Makadia, Pavlovic, & Kumar, 2010; Weston,
Bengio, & Usunier, 2010), but the much harder problem of automatically associating images
with complete sentences that describe them has only recently begun to attract attention.

2013 AI Access Foundation. All rights reserved.

fiHodosh, Young & Hockenmaier

1.1 Related Work
Although a few approaches have framed sentence-based image description as the task of mapping images to sentences written by people (Farhadi, Hejrati, Sadeghi, Young, Rashtchian,
Hockenmaier, & Forsyth, 2010; Ordonez, Kulkarni, & Berg, 2011), most research in this
area has focused on the task of automatically generating novel captions (Kulkarni, Premraj,
Dhar, Li, Choi, Berg, & Berg, 2011; Yang, Teo, Daume III, & Aloimonos, 2011; Li, Kulkarni, Berg, Berg, & Choi, 2011; Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch,
Berg, Berg, & Daume III, 2012; Kuznetsova, Ordonez, Berg, Berg, & Choi, 2012; Gupta,
Verma, & Jawahar, 2012). We argue in this paper that framing image description as a natural language generation problem introduces a number of linguistic difficulties that detract
attention from the underlying image understanding problem we wish to address. Since any
sentence-based image description or retrieval system requires the ability to associate images
with captions that describe what is depicted in them, we argue it is important to evaluate
this mapping between images and sentences independently of the generation aspect. Research on caption generation has also ignored the image search task, which is arguably of
much greater practical importance.
All of the systems cited above are either evaluated on a data set that our group released
in earlier work (Rashtchian, Young, Hodosh, & Hockenmaier, 2010), or on the SBU Captioned Photo Dataset (Ordonez et al., 2011). Our data set consists of 1,000 images from the
PASCAL VOC-2008 object recognition challenge that are each annotated with five descriptive captions which we purposely collected for this task. The SBU data set consists of one
million images and captions harvested from Flickr. Gupta et al. (2012) is the only system to
use Grubinger, Clough, Mller, and Deselaerss (2006) IAPR TC-12 data set, which consists
of 20,000 images paired with longer descriptions.
Although details differ, most models rely on existing detectors to define and map images
to an explicit meaning representation language consisting of a fixed number of scenes, objects
(or stuff), their attributes and spatial relations (Farhadi et al., 2010; Kulkarni et al., 2011;
Li et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Mitchell et al., 2012). But it is
unclear how well these detector-based approaches generalize: the models evaluated on our
PASCAL VOC-2008 data set (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011;
Yang et al., 2011; Mitchell et al., 2012) all rely on detectors that may have been trained on
images contained in this corpus, and Kuznetsova et al. (2012) select a test set of 1,000 images
from the SBU data set for which their detectors work well. Moreover, among the systems
evaluated on our PASCAL VOC-2008 data set, only Kulkarni et al. (2011), Li et al. (2011),
Li et al. (2011) and Mitchell et al.s (2012) results may be directly comparable, since different
research groups report different evaluation metrics and use a different parts of the data set
as test or training data. The evaluation of generation systems is generally well known to be
difficult (see, e.g., Dale & White, 2007; Reiter & Belz, 2009), and typically requires expensive
human judgments that have to consider the quality of both content selection (what is being
described) and surface realization (the fluency of the generated text). These syntactic and
pragmatic issues confound the purely semantic question of whether the image is correctly
described by the caption.

854

fiFraming Image Description as a Ranking Task

1.2 Our Approach
In this paper, we focus on the task of associating images with sentences drawn from a large,
predefined pool of image descriptions. These descriptions are not generated automatically
or harvested from the web (Feng & Lapata, 2008; Ordonez et al., 2011), but are written
by people who were asked to describe them. We argue that evaluating the ability to select
or rank, rather than generate, appropriate captions for an image is the most direct test
of the fundamental semantic question of how well we can associate images with sentences
that describe them well. Framing image description as a ranking task also has a number
of additional advantages. First, it allows us to handle sentence-based image annotation
and search in a unified framework, allowing us to evaluate whether advances in one task
carry over to the other. Second, framing image description as a ranking problem greatly
simplifies evaluation. By establishing a parallel between description and retrieval, we can
use the same metrics to evaluate both tasks. Moreover, we show that the rank of the
original caption, which is easily determined automatically, leads to metrics that correlate
highly with systems rankings obtained from human judgments, even if they underestimate
actual performance. We also show that standard automatic metrics such as Bleu (Papineni,
Roukos, Ward, & Zhu, 2002) or Rouge (Lin, 2004) that have also been used to evaluate
caption generation systems show poor correlation with human judgments, leading us to
believe that the evaluation of caption generation system should not be automated. We also
perform a large-scale human evaluation, but since the sentences in our data set are image
descriptions written by people, we only need to collect purely semantic judgments of whether
they describe the images the system associated them with. And since these judgments are
independent of the task, we can use them to evaluate both image description and retrieval
systems. Since we collect these judgments over image-caption pairs in our publicly available
data set, we also establish a common benchmark that enables a direct comparison of different
systems. We believe that this is another advantage over the caption generation task. Since
there are many possible ways to describe an image, generation systems are at liberty to be
more or less specific about what they describe in an image. This makes a direct comparison
of independently obtained judgments about the quality of two different systems very difficult,
since one system may be aiming to solve a much harder task than the other, and implies
that unless system outputs for a common benchmark collection of images were made publicly
available, there cannot be any shared, objective evaluation that would allow the community
to measure progress on this difficult problem. But since caption generation systems also
need to be able to determine how well a caption describes an image, our data set could
potentially be used to evaluate their semantic component.
1.3 Contributions and Outline of this Paper
In Section 2, we discuss the need for a new data set for image description and introduce a
new, high quality, data set for image description which will enable the community to compare
different systems against the same benchmark. Our PASCAL VOC-2008 data set of 1,000
images (Rashtchian et al., 2010) has been used by a number of image description systems
(Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al.,
2012; Gupta et al., 2012), but has a number of shortcomings that limit its usefulness. First,
its domain is relatively limited, and the captions are relatively simple. Second, since its
855

fiHodosh, Young & Hockenmaier

images are drawn from the data used for the PASCAL VOC-2008 object classes challenge,
it is difficult to guarantee a fair evaluation of description systems which rely on off-the-shelf
object detectors (e.g., Felzenszwalb, McAllester, & Ramanan, 2008) on this data set, since
it may not be possible to identify which images these detectors have been trained on. The
experiments in this paper are therefore based on a larger, and more diverse, data set of 8,000
images. Unlike other data sets that pair images with sentences that are merely related to
the image (Feng & Lapata, 2008; Ordonez et al., 2011), each image in our data sets are
paired with five different captions that were purposely written to describe the image.
In Section 3, we describe our own image description systems. Because image description
is such a novel task, it remains largely unknown what kind of model, and what kind of
visual and linguistic features it requires. Instead of a unidirectional mapping from images
to sentences that is common to current caption generation systems, we map both images
and sentences into the same space. This allows us to apply our system to image search
by retrieving the images that are closest to a query sentence, and to image description
by annotating images with those sentences that are closest to it. The technique we use,
Kernel Canonical Correlation Analysis (KCCA; Bach & Jordan, 2002), has already been
successfully used to associate images (Hardoon, Szedmak, & Shawe-Taylor, 2004; Hwang
& Grauman, 2012; Hardoon, Saunders, Szedmak, & Shawe-Taylor, 2006) or image regions
(Socher & Li, 2010) with individual words or sets of tags, while Canonical Correlation
Analysis (Hotelling, 1936) has also been used to associate images with related Wikipedia
articles from ten different categories (Rasiwasia, Pereira, Coviello, Doyle, Lanckriet, Levy,
& Vasconcelos, 2010). However, the performance of these techniques on the much more
stringent task of associating images with sentences that describe what is depicted in them
has not been evaluated. We compare a number of text kernels that capture different linguistic
features. Our experimental results (discussed in Section 4) demonstrate the importance of
robust textual representations that consider the semantic similarity of words, and hence take
the linguistic diversity of the different captions associated with each image into account. Our
visual features are relatively simple. A number of image description systems (Farhadi et al.,
2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012) largely
rely on trained detectors, e.g. to obtain an explicit intermediate meaning representation of
the depicted objects, scenes and events. But this approach would ultimately require separate
detectors, and hence labeled training data, for each term or phrase in the chosen meaning
representation language. We show here that image features that capture only low-level
perceptual properties can in fact work surprisingly well on our larger data set for which no
in-domain detectors are available.
In Section 4, we consider the question of evaluation, and use a number of different metrics
to compare our systems. Since we focus on the problem of learning an appropriate mapping
between images and captions, we follow standard machine learning practice and evaluate
the ability of this function to generalize to unseen examples. Hence, we separate the pool of
captions and images used for testing from those used to train our systems. We first consider
metrics for the quality of a single image-caption pair, and compare automatically computed
scores with detailed human judgments. We then examine metrics that evaluate the ranked
lists returned by our systems. Our analysis reveals that, at the current level of performance,
differences between models may not become apparent if only a single caption per image is
considered, as is commonly done for caption generation systems. But even if two models
856

fiFraming Image Description as a Ranking Task

are equally likely to fail to return a suitable caption as the first result, we should still prefer
the one that is more likely to rank good captions higher than the other, since it arguably
provides a better approximation of the semantic space in which images are near captions
that describe them well. Since the test pool contains a single gold item for each query, we
first consider metrics that are based on the rank and recall of this gold item. We then show
that simpler, binary judgments of image descriptions that are good approximations of more
fine-grained human judgments can be collected on a very large scale via crowdsourcing. We
augment the test pool of our data set with these relevance judgments, in the hope that
this will add to its usefulness as a community resource and benchmark. These judgments
show that the actual performance of our systems is higher than the recall of the gold item
indicates. However, a comparison of the system rankings obtained via different metrics also
suggests that differences in the rank or recall of the gold item correlate very highly with
difference in performance according to the binary relevance judgments.

2. A New Data Set for Image Description
We have used crowdsourcing to collect descriptive captions for a large number of images of
people and animals (mostly dogs). Before describing our data set and annotation methodology, we discuss what kind of captions are most useful for image description, and motivate
the need to create new data sets for this task.
2.1 What Do We Mean by Image Description?
Since automatic image description is a relatively novel task, it is worth reflecting what it
means to describe images, and what we wish to say about an image. There is in fact a
substantial body of work on image description related to image libraries (Jaimes, Jaimes,
& Chang, 2000; Shatford, 1986) that is useful to revisit for our purpose. We argue that
out of the three different kinds of image descriptions that are commonly distinguished, one
type, the so-called conceptual descriptions, is of most relevance to the image understanding we aim to achieve with automatic captioning. Conceptual image descriptions identify
what is depicted in the image, and while they may be abstract (e.g., concerning the mood
a picture may convey), image understanding is mostly interested in concrete descriptions
of the depicted scene and entities, their attributes and relations, as well as the events they
participate in. Because they focus on what is actually in the image, conceptual descriptions
differ from so-called non-visual descriptions, which provide additional background information that cannot be obtained from the image alone, e.g. about the situation, time or location
in which the image was taken. Perceptual descriptions capture low-level visual properties
of images (e.g., whether it is a photograph or a drawing, or what colors or shapes dominate) are of little interest to us, unless they link these properties explicitly to the depicted
entities. Among concrete conceptual descriptions, a further distinction can be drawn between specific descriptions, which may identify people and locations by their names, and
generic descriptions (which may, e.g., describe a person as a woman or a skateboarder, and
the scene as a city street or a room). With the exception of iconic entities that should be
recognized as such (e.g., well-known public figures or landmark locations such as the Eiffel
Tower) we argue that image understanding should focus on the information captured by

857

fiHodosh, Young & Hockenmaier

BBC captions
(Feng and Lapata 2010)

Consumption
has soared as
the real price of
drink has fallen

AMD destroys
central vision

SBU Captioned Photo Dataset (Flickr)
(Ordonez et al. 2011)

At the Downers Grove
I don't chew up the couch
train station (our condo and pee in the kitchen
building is in the
mama!
background), on our
way to the AG store in
Chicago.

IAPR-TC12 data set
(Grubinger et al. 2006)

a blue and white airplane is standing on a grey airport;
a man and red cones are standing in front of it and two
red-dressed hostesses and two passengers are directly
on the stairs in front of the airplane; a brown landscape
with high dark brown mountains with snow-covered
summits and a light grey sky in the background;

Figure 1: Other data sets of images and captions
generic descriptions. This leaves the question of where to obtain a data set of images paired
with suitable descriptions to train automatic description systems on.
2.2 The Need for New Data Sets
While there is no dearth of images that are associated with text available online, we argue
that most of this text is not suitable for our task. Some work, notably in the natural language
processing community, has focused on images in news articles (Feng & Lapata, 2008, 2010).
However, images are often only used to illustrate stories, and have little direct connection to
the text (Figure 1, left). Furthermore, even when captions describe the depicted event, they
tend to focus on the information that cannot be obtained from the image itself. Similarly,
when people provide captions for the images they upload on websites such as Flickr (Figure 1,
center), they often describe the situation that the images were taken in, rather than what
is actually depicted in the image. That is, these captions often provide non-visual or overly
specific information (e.g., by naming people appearing in the image or the location where the
image was taken). There is a simple reason why people do not typically provide the kinds
of generic conceptual descriptions that are of most use for our purposes: Gricean maxims of
relevance and quantity (Grice, 1975) entail that image captions that are written for people
usually provide precisely the kind of information that could not be obtained from the image
itself, and thus tend to bear only a tenuous relation to what is actually depicted. Or, to
state it more succinctly, captions are usually written to be seen along with the images they
accompany, and users may not wish to bore other readers with the obvious.
Ordonez et al. (2011) harvested images and their captions from Flickr to create the
SBU Captioned Photo Dataset, but had to discard the vast majority of images because
their captions were not actually descriptive. Further analysis of a random sample of 100
images of their final data set revealed that the majority (67/100) of their captions describe
information that cannot be obtained from the image itself (e.g., by naming the people or
locations appearing in the image), while a substantial fraction (23/100) only describe a small
detail of the image or are otherwise just commentary about the image. Examples of these
issues are shown in Figure 1 (center). This makes their data set less useful for the kind
of image understanding we are interested in: unless they refer to specific entities one may
actually wish to identify (e.g., celebrities or famous landmarks that appear in the image),
proper nouns are of little help in learning about visual properties of entity types unless one
858

fiFraming Image Description as a Ranking Task

Our data set of 8,000 Flickr images with 5 crowd-sourced captions
A man is doing tricks on a bicycle on ramps in front of a crowd.
A man on a bike executes a jump as part of a competition while the crowd watches.
A man rides a yellow bike over a ramp while others watch.
Bike rider jumping obstacles.
Bmx biker jumps off of ramp.
A group of people sit at a table in front of a large building.
People are drinking and walking in front of a brick building.
People are enjoying drinks at a table outside a large brick building.
Two people are seated at a table with drinks.
Two people are sitting at an outdoor cafe in front of an old building.

Figure 2: Our data set of images paired with generic conceptual descriptions
can infer what kind of entity they refer to.1 The IAPR TC-12 data set (Grubinger et al.,
2006), which consists of 20,000 photographs is potentially more useful for our purposes, since
it contains descriptions of what can be recognized in an image without any prior information
or extra knowledge. However, the descriptions, which consist often of multiple sentences
or sentence fragments, have a tendency to be lengthy (average length: 23.1 words) and
overly detailed, instead of focusing on the salient aspects of the photograph. For example,
in the photo of an airplane in Figure 1 (right), the two hostesses are barely visible but
nevertheless described in detail.
2.3 Our Data Sets
Since the kinds of captions that are normally provided for images do not describe the images
themselves, we have collected our own data sets of images and captions. The captions
are obtained by using the crowdsourcing service provided by Amazon Mechanical Turk
to annotate each image with five descriptive captions. By asking people to describe the
people, objects, scenes and activities that are shown in a picture without giving them any
further information about the context in which the picture was taken, we were able to
obtain conceptual descriptions that focus only on the information that can be obtained
from the image alone. Our annotation process and quality control are described in detail in
Rashtchian et al. (2010)s paper. We have annotated two different data sets in this manner:
2.3.1 The PASCAL VOC-2008 Data Set
The first data set we produced is relatively small, and consists of only 1,000 images randomly selected from the training and validation set of the PASCAL 2008 object recognition
challenge (Everingham, Gool, Williams, Winn, & Zisserman, 2008). It has been used by a
large number of image description systems (Farhadi et al., 2010; Kulkarni et al., 2011; Li
et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Gupta et al., 2012), but since almost
all of these systems (the only exception being Gupta et al., 2012) rely on detectors trained
1. The data set of Ordonez et al. (2011) also differs significantly in content from ours: while our collection
focuses on images of eventualities, i.e. people or animals doing something, the majority of Ordonez et
al.s images (60/100) do not depict people or animals (e.g., still lifes, landscape shots).

859

fiHodosh, Young & Hockenmaier

on images from the same data set (Felzenszwalb et al., 2008), it is unclear how well these
approaches would generalize to other domains where no labeled data to train detectors is
available. The captions in the PASCAL data set are also relatively simple. For example,
since the data set contains many pictures that do not depict or focus on people doing something, 25% of the captions do not contain any verb, and an additional 15% of the captions
contain only the common static verbs sit, stand, wear, or look.
2.3.2 The Flickr 8K Data Set
For the work reported in this paper we therefore collected a larger, more diverse data set
consisting of 8,092 images from the Flickr.com website. Unlike the more static PASCAL
images, the images in this data set focus on people or animals (mainly dogs) performing
some action. Examples from this data set are shown in Figure 2. The images were chosen
from six different Flickr groups,2 and tend not to contain any well-known people or locations,
but were manually selected to depict a variety of scenes and situations. In order to avoid
ungrammatical captions, we only allowed workers from the United States who had passed
a brief spelling and grammar test we devised to annotate our images. Because we were
interested in conceptual descriptions, annotators were asked to write sentences that describe
the depicted scenes, situations, events and entities (people, animals, other objects). We
collected multiple captions for each image because there is a considerable degree of variance
in the way many images can be described. As a consequence, the captions of the same
images are often not direct paraphrases of each other: the same entity or event or situation
can be described in multiple ways (man vs. bike rider, doing tricks vs. jumping), and
while everybody mentions the bike rider, not everybody mentions the crowd or the ramp.
The more dynamic nature of the images is also reflected in how they are being described:
Captions in this data set have an average length of 11.8 words, compared to 10.8 words
in the PASCAL data set, and while 40% of the PASCAL captions contain no verb other
than sit, stand, wear, or look, only 11% of the captions for the Flickr 8K set contain no
verb, and an additional 10% contain only these common verbs. Our data sets, the Flickr
training/test/development splits and human relevance judgments used for evaluation of the
test items (Section 4) are publicly available.3 The online appendix to this paper contains
our instructions to the workers, including the qualification test they had to pass before being
allowed to complete our tasks.

3. Systems for Sentence-Based Image Description
Since image description requires the ability to associate images and sentences, all image
description systems can be viewed in terms of an affinity function f (i, s) which measures the
degree of association between images and sentences. We will evaluate our ability to compute
such affinity functions by measuring performance on two tasks that depend directly on them.
Given a candidate pool of sentences Scand and a candidate pool of images Icand , sentencebased image retrieval aims to find the image i  Icand that maximizes f (i, sq ) for a query
sentence sq  Scand . Conversely, image annotation aims to find the sentence s  Scand that
2. These groups were called strangers!, Wild-Child (Kids in Action), Dogs in Action (Read the Rules),
Outdoor Activities, Action Photography, Flickr-Social (two or more people in the photo)
3. http://nlp.cs.illinois.edu/HockenmaierGroup/data.html

860

fiFraming Image Description as a Ranking Task

maximizes f (iq , s) for a query image iq  Icand . In both cases, f (i, s) should of course be
maximized for image-sentence pairs in which the sentence describes the image well:
Image search:
Image annotation:

i = arg maxiIcand f (i, sq )
s

(1)

= arg maxsScand f (iq , s)

This formulation is completely general: although we will, for evaluation purposes, define
Scand as the set of captions originally written for the images in Icand , this does not have to
be the case, and Scand could also, for example, be defined implicitly via a caption generation
system. In order to evaluate how well f generalizes to unseen examples, we will evaluate our
system on test pools Itest and Stest that are drawn from the same domain but are disjoint
from the training data Dtrain = (Itrain , Strain ) and development data Ddev = (Idev , Sdev ).
The challenge in defining f lies in the fact that images and sentences are drawn from two
different spaces, I and S. In this paper, we present two different kinds of image description
systems. One is based on nearest-neighbor search (NN), the other uses a technique called
Kernel Canonical Correlation Analysis (KCCA; Bach & Jordan, 2002; Hardoon et al., 2004).
Both rely on a set of known image-sentence pairs Dtrain = {hi, si}.
3.1 Nearest-Neighbor Search for Image Description
Nearest-neighbor based systems use unimodal text and image similarity functions directly
to first find the image-sentence pair in the training corpus Dtrain that contains the closest
item to the query, and then score the items in the other space by their similarity to the
other item in this pair:
Image retrieval: fNN (i, sq ) = fI (iNN , i)

for hiNN , sNN i = arg max fS (sq , st ) (2)
hit ,st iDtrain

Image annotation: fNN (iq , s) = fS (sNN , s) for hiNN , sNN i = arg max fI (iq , it )
hit ,st iDtrain

Despite their simplicity, such nearest-neighbor systems are non-trivial baselines: for the
task of annotating images with tags or keywords, methods which annotate unseen images
with the tags of their nearest neighbors among training images are known to achieve competitive performance (Makadia et al., 2010), and similar methods have recently been proposed
for image description (Ordonez et al., 2011). Since the task we address here does not allow
us to return items from the training data, but requires us to rerank a pool of unseen captions
or images, our nearest-neighbor search requires two similarity functions. All of our nearestneighbor systems use the same image representation as our KCCA-based systems, described
in Section 3.3. Our main nearest-neighbor system, NN (NN5idf
F1 ), treats the five captions
associated with each training image as a single document. It then reweights each token by
its inverse document frequency (IDF) w , and defines the similarity of two sentences as the
F1-measure (harmonic mean of precision and recall) computed over their IDF-reweighted
bag-of-words representation. If Dtrain (w) is the subset of training images in whose captions
word w appears at least once, the inverse document frequency (IDF) of w is defined as
|Dtrain |
w = log |Dtrain
(w)|+1 . IDF-reweighting is potentially helpful for our task, since words that
describe fewer images may be particularly discriminative between captions.
861

fiHodosh, Young & Hockenmaier

In the appendix, we provide results for NN systems that use the same text representation
as two of our KCCA systems.
3.2 Kernel Canonical Correlation Analysis for Image Description
Most of the systems we present are based on a technique called Kernel Canonical Correlation
Analysis (Bach & Jordan, 2002; Hardoon et al., 2004). We first provide a brief introduction,
and then explain how we apply it to our task.
3.2.1 Kernel Canonical Correlation Analysis (KCCA)
KCCA is an extension of Canonical Correlation Analysis (Hotelling, 1936), which takes
training data consisting of pairs of corresponding items hxi , yi i drawn from two different
feature spaces (xi  X , yi  Y), and finds maximally correlated linear projections x and
y of both sets of items into a newly induced common space Z. Since linear projections of
the raw features may not capture the patterns that are necessary to explain the pairing of
the data, KCCA implicitly maps the original items into higher-order spaces X 0 and Y 0 via
kernel functions KX = hX (xi )  X (xj )i, which compute the dot product of two data points
xi and xj in a higher-dimensional space X 0 without requiring the explicit computation of
the mapping X . KCCA then operates on the two resulting kernel matrices KX [i, j] =
hX (xi )  X (xj )i and KY [i, j] = hY (yi )  Y (yj )i which evaluate the kernel functions on
pairwise combinations of items in the training data. It returns two sets of projection weights,
 and   , which maximize the correlation between the two (projected) kernel matrices:
( ,   ) = arg max q
,

0 KX KY 
(0 K2X  + 0 KX )( 0 K2Y  +  0 KY )

(3)

This can be cast as a generalized eigenproblem (KX +I)1 KY (KY +I)1 KX  = 2 ,
and solved by partial Gram-Schmidt orthogonalization (Hardoon et al., 2004; Socher & Li,
2010). The regularization parameter  penalizes the size of possible solutions, and is used
to avoid overfitting, which arises when the matrices are invertible.
One disadvantage of KCCA is that it requires the two kernel matrices of the training
data to be kept in memory during training. This becomes prohibitive with very large data
sets, but does not cause any problems here, since our training data consists of only 6,000
items (see Section 4.1).
3.2.2 Using KCCA to Associate Images and Sentences
KCCA has been successfully used to associate images (Hardoon et al., 2004; Hwang &
Grauman, 2012; Hardoon et al., 2006) or image regions (Socher & Li, 2010) with individual
words or sets of tags. In our case, the two original spaces X = I and Y = S correspond to
images and sentences that describe them. Images i  I are first mapped to vectors KI (i)
whose elements KI (i)(t) = KI (it , i) evaluate the image kernel function KI on i and the t-th
image in Dtrain . Similarly, sentences s  S are mapped to vectors KS (s) that evaluate the
sentence kernel function KS on s and the sentences in Dtrain . The learned projection weights
( ,   ) then map KI (i) and KS (s) into our induced space Z, in which we expect images
to appear near sentences that describe them well. In a KCCA-based image annotation or
862

fiFraming Image Description as a Ranking Task

search system, we therefore define f as the cosine similarity (sim) of points in this new space:
fKCCA (i, s) = sim(KI (i), KS (s))

(4)

We now describe the image and text kernels used by our KCCA systems.
3.3 Image Kernels
In contrast to much of the work done on image description, which assumes the existence of
a large number of preexisting detectors, the image representations used in this paper are
very basic, in that they rely only on three different kinds of low-level pixel-based perceptual
features that capture color, texture (Varma & Zisserman, 2005) and shape information in
the form of SIFT descriptors (Lowe, 2004; Vedaldi & Fulkerson, 2008). We believe that
this establishes an important baseline, and leave the question of how more complex image
representations affect performance to future work. We use two different kinds of kernels:
a histogram kernel K Histo , which represents each image as a single histogram of feature
values and computes the similarity of two images as the intersection of their histograms,
and a pyramid kernel K Py (Lazebnik, Schmid, & Ponce, 2009), which represents each image
as a pyramid of nested regions, and computes the similarity of two images in terms of the
intersection of the histograms of corresponding regions. In both cases, we compute a separate
kernel for each of the three types of image features and average their result.
3.3.1 The Histogram Kernel (K Histo )
Each image xi is represented as a histogram Hi of discrete-valued features, such that Hi (v)
is the fraction of pixels in xi with value v. The similarity of two images xi and xj is defined
as the intersection of their histograms, i.e. the percentage of pixels that can be mapped
onto a pixel with the same feature value in the other image:
K(xi , xj ) =

V
X

min(Hi (v), Hj (v))

(5)

v=1

We combine three kernels based on different kinds of visual features: KC captures color,
represented by the three CIELAB coordinates. KT captures texture, represented by descriptors which capture edge information at different orientations centered on the pixel (Varma
& Zisserman, 2005). KS is based on SIFT descriptors, which capture edge and shape information in a manner that is invariant to changes in rotation and illumination, and have been
shown to be distinct across possible objects of an image Lowe, 2004; Vedaldi & Fulkerson,
2008. We use 128 color words, 256 texture words and 256 SIFT words, obtained in an unsupervised fashion by K-means clustering on 1,000 points of 200 images from the PASCAL
2008 data set (Everingham et al., 2008). Our final histogram kernel K Histo is the average
of the responses of the three kernels KCHisto , KTHisto , KSHisto , taken to the pth power:

p
1 X
K Histo (xi , xj ) =
KFHisto (xi , xj )
(6)
3
F {C,S,T}

863

fiHodosh, Young & Hockenmaier

3.3.2 The Pyramid Kernel K Py
The spatial pyramid kernel (Lazebnik et al., 2009) is a generalization of the histogram kernel
that captures similarities not just at a global, but also at a local level. Each image xi is
represented at multiple levels of scale l (l  {0, 1, 2}) such that each level partitions the
image into a smaller and smaller grid of Cl = 2l  2l cells (C0 = 1, C1 = 4, C2 = 16), and
each cell c is represented as a histogram Hic . The similarity of images xi and xj at level l,
Iijl , is in turn defined as the sum of the histogram similarities of their corresponding cells
0l , ..., Cl at this level:
Iijl

=

Cl X
V
X

min(Hic (v), Hjc (v))

(7)

c=0l v=1

Although similarities at level l subsume those at a more fine-grained level l + 1 (Iijl 
Iijl+1 ), similarities that hold at a more fine-grained level are deemed more important, since
they indicate a greater local similarity. The pyramid kernel therefore proceeds from the
most fine-grained (l = L) down to the coarsest (whole-image) scale (l = 0), and weights the
1
similarities first encountered at level l (Iijl  Iijl+1 ) by 2Ll
:

K

Py

(xi , xj ) =

IijL

+

L1
X
l=0

1
(I l  Iijl+1 )
2Ll ij

(8)

L

=

1 0 X 1
I +
Il
2L ij
2Ll+1 ij
l=1

We again compute three separate pyramid kernels KCPy , KTPy , KSPy based on the same
color, texture and SIFT features as described above, and combine them into a single pyramid
kernel K Py , as in equation 6.
3.4 Basic Text Kernels
We examine three different basic text kernels: a bag of words (BoW) kernel, Hwang and
Graumans (2012) TagRank kernel, and a truncated string kernel (Tri).
3.4.1 The Bag of Words Kernel (BoW)
Since bag-of-words representations have been successfully used for other tasks involving text
and images (e.g., Grangier & Bengio, 2008; Hardoon et al., 2006), we include a basic bag
of words kernel, which ignores word order and represents each caption simply as a vector
of word frequencies. The BoW kernel function is defined as the cosine similarity of the
corresponding bag of words vectors. We either merge the five captions of each training item
into a single document (BoW5), or reduce each training item to a single, arbitrarily chosen,
caption (BoW1). A words frequency can also be reweighted by its IDF-score. As in the
|Dtrain |
nearest neighbor approach, the IDF-weight of a word w is defined as w = log |Dtrain
(w)|+1 ,
where Dtrain (w) is the subset of training images in whose captions word w appears at least

864

fiFraming Image Description as a Ranking Task

once. We found the square root of w (BoW5
IDF-score w (BoW5idf ).



idf )

to give better results than the standard

3.4.2 The Tag Rank Kernel (TagRank)
Hwang and Grauman (2012) apply KCCA to keyword-based image annotation and retrieval.
They focus on a data set where each image is paired with a list of tags ranked by their importance, and propose a new kernel for this kind of data. This so-called tag rank kernel
(TagRank) is a variant of the bag of words kernel that aims to capture the relative importance of tags by reweighting them according to their position in this list. Although Hwang
and Grauman do not evaluate the ability of their system to associate images with entire
sentences, they also consider another data set in which the lists of tags correspond to the
words of descriptive captions, and argue that the linear order of words in these captions also
reflects the relative importance of the corresponding objects in the image, so that words
that appear at the beginning of the sentence describe more salient aspects of the image.
In the TagRank kernel, each sentence is represented as two vectors, ~a and ~r. In ~a, the
weight of each word is based on its absolute position, so that the first words in each sentence
are always assigned a high weight. In this absolute tag rank representation, each caption
s is mapped to a vector ~a = [~a(1) . . . ~a(|V |)], where |V | is the size of the vocabulary. ~a(i)
depends on the absolute position pi of wi in s (if wi occurs multiple times in s, pi is averaged
over all its positions). If wi does not occur in s, ~a(i) = 0. Otherwise,
~a(i) =

1
log2 (1 + pi )

(9)

In ~r, the weight of a word depends on how its current position compares to the distribution of positions it occupies in the training data. The intuition behind this relative rank
representation is that words should have a higher weight when they occur earlier in the sentence than usual. Here, each caption s is mapped to a vector ~r = [~r(1) . . . ~r(V )] of relative
tag ranks. Again, when wi does not appear in s, ~r(i) = 0. Otherwise wi s relative tag rank
~r(i) indicates what percent of its occurrences in the training data appear after position pi .
Defining
P nik as the number of times word wi appears in position k in the training data, and
ni = k nik as the total frequency of wi in the training data:
Ppi
~r(i) = 1 

k=1 nik

(10)

ni

The final kernel KT is given by the average of two 2 kernels computed over ~r and ~a ( and
0 are normalization terms):
"



#
V
V
1
1 X (~ri (k)  ~rj (k))2
1 X (~ai (k)  ~aj (k))2
KT (xi , xj ) =
exp
+ exp
(11)
2
2
~ri (k) + ~rj (k)
20
~ai (k) + ~aj (k)
k=1

k=1

Since each image in our training data is associated with multiple, independently generated captions, we evaluate the kernel separately on each sentence pair and average the
response, instead of treating the multiple sentences as a single document.
865

fiHodosh, Young & Hockenmaier

The TagRank kernel is relatively sensitive to overall sentence length, especially in cases
where the subject is preceded by multiple adjectives or other modifiers (a very large brown
dog vs. a dog ). In English, the absolute tag rank will generally assign very high weights
to the subjects of sentences, lower weight to verbs, and even lower weight to objects or scene
descriptions, which tend to follow the main verb. The relative tag rank may not downweight
verbs, objects and scene descriptions as much (as long as they are always used in similar
positions in the sentence).
3.4.3 The Trigram Kernel (Tri)
Since bag-of-words representations ignore which words appear close to each other in the
sentence, they lose important information: an image of a small child with red hair playing
with a large brown dog on white carpet looks quite different from one of a small white dog
playing with a large red ball on brown grass, although both descriptions share the majority
of their words. To capture this information, we define a trigram kernel as a truncated variant
of string kernels (Shawe-Taylor & Cristianini, 2004) that considers not just how many single
words two captions share, but also how many short sequences (pairs and triples) of words
occur in both.
A word sequence w = w1 ...wk is an ordered list of words. A sentence s = s1 ...sn contains
w (w  s) as long as the words in w appear in s in the order specified by w. That is, the
sentence A large white dog runs and catches a red ball on the beach (when lemmatized)
contains both the subject-verb-object triple dog catch ball and the subject-verb-location
triple dog run beach. Formally, every substring (i, j) = si ...sj in s that starts with si = w1 ,
ends in sj = wk , and contains w is considered a match between s and w. Ms,w is the set of
all substrings in s that match the sequence w:
Ms,w = {(i, j) | w = w1 ...wk  si ...sj , w1 = si , wk = sj }

(12)

When w is restricted to individual words (k = 1), string kernels are identical to the standard
BoW kernel.
A match between strings s and s0 is a pair of substrings (i, j)  s and (i0 , j 0 )  s0 that
both match the same word sequence w. Standard string kernels K(s, s0 ) weight matches
0
0
by a factor (ji+1)+(j i +1) that depends on an adjustable parameter  and the respective
length of the matching substrings:
K(s, s0 ) =

X

X

X

0

0

(ji+1)+(j i +1)

(13)

w (i,j)Ms,w (i0 ,j 0 )Ms0 ,w

In order to distinguish between the length of the matching subsequence, l(w), and the
length of the gaps in (i, j) and (i0 , j 0 ), we replace  by two parameters m , g , and reformulate
this as:
K(s, s0 ) =

X

X

X

0

0

i +1)2l(w)
2l(w)
(ji+1)+(j
m
g

(14)

w (i,j)Ms,w (i0 ,j 0 )Ms0 ,w

We found that a gap score of g = 1, which means that gaps are not penalized, and a
match score of m = 0.5 perform best on our task.
866

fiFraming Image Description as a Ranking Task

Although string kernels are generally defined over sequences of arbitrary length (k  ),
we found that allowing longer sequences did not seem to impact performance on our task but
incurred a significant computational cost. Intuitively, word pairs and triplets represent most
of the linguistic information we need to capture beyond the BoW representation, since they
include head-modifier dependencies such as large-dog vs. small-dog and subject-verb-object
dependencies such as child-play-dog vs. dog-play-ball. We therefore consider only sequences
up to length k  3. With w restricted to sequences of length k  3 and ms,w = |Ms,w |, this
yields the following trigram kernel (Tri):
KTri (s, s0 ) =

X

ms,w ms0 ,w 2l(w)
m

(15)

w:k3

To deal with differences in sentence length, we normalize the kernel response between
two examples by the geometric mean of the two example responses with themselves.
Since the trigram kernel also captures sequences that are merely coincidental, such as
large white red, it may seem advantageous to use richer syntactic representations such
as dependency tree kernels (Moschitti, Pighin, & Basili, 2008), which only consider word
tuples that correspond to syntactic dependencies. However, such kernels are significantly
more expensive to compute, and initial experiments indicated that they may not perform as
well as the trigram kernel. We believe that this is due to the fact that our image captions
contain little syntactic variation, and that hence surface word order may be sufficient to
differentiate e.g. between the agent of an action (whose mention will be the subject of the
sentence) and other participants or entities (whose mentions will appear after the verb).
On the other hand, many of our image captions contain a lot of syntactic ambiguity (e.g.
multiple prepositional phrases), and a vocabulary that is very distinct from what standard
parsers are trained on. It may be that we were not able to benefit from using a richer
representation simply because we were not able to recover it with sufficient accuracy.
In order to capture
the relative importance of words, we can also reweight sequences

by the IDF (or idf) weight of theirQwords. With 
w defined as before, the IDF-weight of
j
a sequence w = wi ...wj is w =
idf-weighted trigram kernel KTriidf
k=i wk . The
(Tri5



idf )

is therefore

KTriidf (s, s0 ) =

X

w ms,w ms0 ,w 2l(w)
m

(16)

w:k3

3.5 Extending the Trigram Kernel with Lexical Similarities
One obvious shortcoming of the basic text kernels is that they require exact matches between
words, and cannot account for the fact that the same situation, event, or entity can be
described in a variety of ways (see Figure 2 for examples). One way of capturing this
linguistic diversity is through lexical similarities which allow us to define partial matches
between words based on their semantic relatedness. Lexical similarity have found success in
other tasks, e.g. semantic role labeling (Croce, Moschitti, & Basili, 2011), but have not been
fully exploited for image description. Ordonez et al. (2011) define explicit equivalence classes
of synonyms and hyponyms to increase the natural language vocabulary corresponding to
each of their object detectors (e.g. the word Dalmatian may trigger the dog detector),
867

fiHodosh, Young & Hockenmaier

but do not change the underlying, pre-trained detectors themselves, ignoring the potential
variation of appearance between, e.g., different breeds of dog. Similarly, Yang et al.s (2011)
generative model can produce a variety of words for each type of detected object or scene,
but given an object or scene label, the word choice itself is independent of the visual features.
We therefore also investigate the effect of incorporating different kinds of lexical similarities
into the trigram kernel that allow us to capture partial matches between words. We did
not explore the effect of incorporating lexical similarities into the tag-rank kernel, since it is
unclear how they should affect the computation of ranks within a sentence.
3.5.1 String Kernels with Lexical Similarities
Since standard lexical similarities simS (w, wi ) do not necessarily yield valid kernel functions,
we follow Bloehdorn, Basili, Cammisa, and Moschitti (2006) and use these similarities to
map each word w to vectors w
~ S in an N -dimensional space, defined by a fixed vocabulary of
size N . Each vector component w
~ S (i) corresponds to the similarity of w and wi as defined
by S :
(17)

w
~ S (i) = simS (w, wi )

We then define the corresponding word kernel function S (w, w0 ), which captures the partial
match of words w and w0 according to S , as the cosine of the angle between w
~ S and w
~ S0 :
S (w, w0 ) = cos(~
wS , w
~ S0 )

(18)

S may only be defined over a subset of the vocabulary. The similarity of words outside
of its vocabulary is defined by the identify function, as in the standard string kernel.
The similarity of sequences w and w0 of length l is defined as the product of the word
kernels over the corresponding pairs of sequence elements wi , wi0 :
S (w, w0 ) =

l
Y

S (wi , wi0 )

(19)

i=1

If S (w) = {w0 |S (w0 , w) > 0, l(w0 ) = l(w)} is the set of sequences that have a non-zero
match with w, the string kernel KS with similarity S is:
KS (s, s0 ) =

X

X

ms,w ms0 ,w0 2l(w)
S (w0 , w)
m

(20)

w w0 S (w)


idf
0
To obtain
 the IDF-weighted version of this kernel, KS (s, s ), the inner term is multiplied by w w0 :
X X p
KS (s, s0 ) =
w w0 ms,w ms0 ,w 2l(w)
S (w0 , w)
(21)
m
w w0 S (w)

In our experiments, we use the trigram variants of these kernels, and restrict w again to
sequences of length k  3.
We consider three different kinds of lexical similarities: the WordNet-based Lin similarity
(Lin, 1998) (Lin ), a distributional similarity metric (D ), and a novel alignment-based
868

fiFraming Image Description as a Ranking Task

similarity metric (A ), which takes advantage of the fact that each image is associated with
five independently generated captions. All metrics are computed on our training corpus.
Distributional similarity is also computed on the British National Corpus (BNC Consortium,
2007). Both corpora are lemmatized, and stop words are removed before similarities are
computed. Since almost any pair of words will have a non-zero similarity, the word kernel
matrices are very dense, but since most of these similarities are very close to zero, they have
very little effect on the resulting kernel. We therefore zero out entries smaller than 0.05 in
the alignment-based kernel A and less than 0.01 in any distributional kernel DC .
3.5.2 The Lin Similarity Kernel (Lin )
Lins (1998) similarity relies on the hypernym/hyponym relations in WordNet (Fellbaum,
1998) as well as corpus statistics. WordNet is a directed graph in which the nodes (synsets)
represent word senses and the edges indicate is-a relations: a parent sense (e.g., dog1 ) is a
hypernym of its children (e.g., poodle1 or dachshund1 ). Kernels based on Lins similarity
have been found to perform well on tasks such as text categorization (Bloehdorn et al., 2006).
But with the exception of Farhadi et al. (2010), who incorporate Lins similarity into their
model, but do not evaluate what benefit they obtain from it, WordNets hypernym-hyponym
relations have only been used superficially for associating images and text (Weston et al.,
2010; Ordonez et al., 2011; Gupta et al., 2012). The Lin similarity of two word senses si , sj
is defined as
2 log P (LCS(si , sj ))
simLin (si , sj ) =
(22)
log P (si ) + log P (sj )
LCS(s1 , s2 ) refers to the lowest common subsumer of s1 and s2 in WordNet, i.e. the most
specific synset that is an ancestor (hypernym) of both s1 and s2 . P (s) is the probability that
a randomly drawn word is an instance of synset s or any of its descendants (hyponyms). We
use our training data to estimate P (s), and follow Bloehdorn et al. (2006) in assigning each
word w its most frequent (first) noun sense sw in WordNet 3.0. Hence, we represent each
word w with WordNet sense s as a vector w
~ Lin of Lin similarities over its hypernyms H(sw ):
 2log(f (si ))

 log(f (s))+log(f (si ))
w
~ Lin (i) =
1


0

si  H(s)
sw = si
otherwise

(23)

3.5.3 Distributional Similarity (DC )
Distributional similarity metrics are based on the observation that words that are similar to
each other tend to appear in similar contexts (Jurafsky & Martin, 2008). The components of
w
~ DC are the non-negative pointwise mutual information scores (PMI) of w and wi , computed
on the corpus C:


PC (w, wi )
(24)
w
~ DC (i) = max 0, log2
PC (w)PC (wi )
PC (w) is the probability that a random sentence in C contains w, and PC (w, wi ) is the
probability that a random sentence in C contains both w and wi . We compute two variants
869

fiHodosh, Young & Hockenmaier

of the same metric: Dic is computed on the image captions in our training corpus, and is
defined over the cooccurrences of the 1,928 words that appear at least 5 times in this corpus,
while DBNC uses the British National Corpus (BNC Consortium, 2007), and is defined for
the 1,874 words that appear at least 5 times in both corpora, but considers their PMI scores
against the 141,656 words that appear at least 5 times in the BNC.
3.5.4 Alignment-Based Similarity (A )
We also propose a novel, alignment-based, similarity metric (A ), which takes advantage
of the fact that each image is associated with five independently generated captions, and
is specifically designed to capture how likely two words are to describe the same event
or entity in our data set. We borrow the concept of alignment from machine translation
(Brown, Pietra, Pietra, & Mercer, 1993), but instead of aligning the words of sentences in two
different languages, we align pairs of captions that describe the same image. This results in a
similarity metric that has better coverage on our data set than WordNet based metrics, and
is much more specific than distributional similarities which capture broad topical relatedness
rather than semantic equivalence. Instead of aligning complete captions, we have found it
beneficial to align nouns and verbs independently of each other, and to ignore all other parts
of speech. We create two versions of the training corpus, one consisting of only the nouns
of each caption, and another one consisting only of the verbs of each caption. We then
use Giza++ (Och & Ney, 2003) to train IBM alignment models 12 (Brown et al., 1993)
over all pairs of noun or verb captions of the same image to obtain two sets of translation
probabilities, one over nouns (Pn (|w)) and one over verbs (Pv (|w)). Finally, we combine
the noun and verb translation probabilities as a sum weighted by the relative frequency with
which the word w was tagged as a noun (Pn (w)) or verb (Pv (w)) in the training corpus.
The ith entry in wA is therefore:
w
~ A (i) = Pn (wi |w)Pn (w) + Pv (wi |w)Pv (w)

(25)

We define the noun and verb vocabulary as follows: words that appear at least 5 times
as a noun, and are tagged as a noun in at least 50% of their occurrences, are considered
nouns. But since verbs are more polysemous than nouns (leading to broader translation
probabilities) and are often mistagged as nouns in our domain, we only include those words
as verbs that are tagged as verbs at least 25 times, and in at least 25% of their occurrences.
This results in 1180 noun and 143 verb lemmas, including 11 that can be nouns or verbs.
We use the OpenNLP POS tagger before lemmatization.
3.5.5 Comparing the Similarity Metrics (Figure 3)
Figure 3 illustrates the different similarity metrics, using the words rider and swim as
examples. While distributional similarities are high for words that are topically related
(e.g., swim and pool ), the alignment similarity tends to be high for words that can be used
to describe the same entity (usually synonyms or hyper/hyponyms) or activity such as swim
or paddle. Distributional similarities that are obtained from the image captions are very
specific to our domain. The BNC similarities are much broader and help overcome data
sparsity, although the BNC has relatively low coverage of the kinds of sports that occur in
our data set. The Lin similarity associates swim with hypernyms such as sport and activity,
870

fiFraming Image Description as a Ranking Task

Comparing similarity metrics: The five words most similar to rider and swim
Alignment
Strain
wi
A

wi

rider

biker
bicyclist
cyclist
bmx
bicycler

0.86
0.82
0.79
0.75
0.73

bike
dirt
motocross
motorcycle
ride

0.41
0.35
0.33
0.33
0.33

ride
horse
race
bike
jockey

swim

retrieve
paddle
dive
come
wade

0.56
0.54
0.52
0.38
0.31

pool
trunk
water
dive
goggles

0.53
0.35
0.34
0.30
0.29

fish
water
sea
pool
beach

Corpus
w

Distributional
Strain
BNC
Dic wi
DBNC

Lin
Strain
wi

Lin

0.21
0.20
0.19
0.17
0.16

traveler
cyclist
bicyclist
horseman
jockey

0.94
0.89
0.89
0.84
0.84

0.21
0.18
0.18
0.18
0.17

bathe
sport
football
activity
soccer

0.85
0.85
0.77
0.75
0.73

Figure 3: A comparison of lexical similarities for the noun rider and the verb swim
or other kinds of sport such as football or soccer. This makes it the least suitable similarity
for our task (see also Section 4.3.4 for experimental results), since these terms should not be
considered similar for our purposes of identifying the different ways in which visually similar
events or entities can be described.
3.5.6 Combining Different Similarities
Combining the different distributional and the alignment-based similarities allows us to
capture the different strengths of each method. We define an averaged similarity which
captures aspects of the distributional similarities computed over both corpora:
DBNC (w, w0 ) + Dic (w, w0 )
(26)
2
For every distributional kernel D (w, w0 ), we also define a variant D+A (w, w0 ) which
incorporates alignment-based similarities by taking the maximum of either kernel:4
DBNC,ic (w, w0 ) =

D+A (w, w0 ) = max(A (w, w0 ), A (w, w0 ))

(27)

4. Evaluation Procedures and Metrics for Image Description
In order to evaluate scoring functions f (i, s) for image-caption pairs, we need to evaluate
their ability to associate previously unseen images and captions with each other. In analogy
to caption generation systems, we first examine metrics that aim to measure the quality of
a single image-description pair (Section 4.2). Here, we focus on the image annotation task,
and restrict our attention to the first caption returned for each test item, and a subset of
our systems. We collect graded human judgments from small number of native speakers of
American English, and investigate whether these expert judgments can be approximated
4. This operation may not preserve the positive definiteness of the matrix required to be a valid kernel, but
this simply means we effectively use (plain) CCA with this representation.

871

fiHodosh, Young & Hockenmaier

with automatically computed Bleu (Papineni et al., 2002) or Rouge (Lin & Hovy, 2003)
scores, or with simpler crowdsourced human judgments that can be collected on a much
larger scale. In Section 4.3, we consider approaches to evaluation that aim to measure the
quality of the ranked list of image-caption pairs returned by each system, and allow us to
evaluate a large number of systems. For reasons of space, we focus most of our discussion
again on only a subset of our systems, and refer the interested reader to Appendix B for
complete results. Since the candidate pool contains one sentence or image that was originally
associated with the query image or sentence, we first compare systems by the rank and recall
of this original item. These metrics can be computed automatically, but should only be
considered lower bounds on actual performance, since each image may be associated with a
number of captions that describe it well or perhaps with only minor errors. We then show
that the crowdsourced human judgments can be mapped to binary relevance judgments that
correlate well with the more fine-grained expert judgments, and consider metrics based on
these relevance judgments.
4.1 Experimental Setup
We now describe the data, the tasks, and the systems we evaluate in our experiments.
4.1.1 The Data
Since the PASCAL 2008 data set contains only a total of 1,000 images, we perform our
experiments exclusively on the Flickr 8K set. We split 8,000 images from this corpus (see
Section 2.3) into three disjoint sets. The training data Dtrain = hItrain , Strain i consists of
6,000 images, each associated with five captions, whereas the test and development data,
Dtest and Ddev , each consist of 1,000 images associated with one, arbitrarily chosen, caption.
All captions are preprocessed by spellchecking with Linux spell, normalizing compound words
(e.g., t-shirt, t shirt, and tee-shirt  t-shirt), stop word removal, and lemmatization.
4.1.2 The Tasks
We evaluate our systems on two tasks, sentence-based image annotation (or description)
and sentence-based image search. For image search, the task is to return a ranked list of the
1,000 images in Itest for each of the captions (queries) in Stest . Image annotation is defined
analogously as a retrieval problem: the task is to return a ranked list of the 1,000 captions
in Stest for each of the 1,000 test (query) images in Itest . In both cases, the ranked lists are
produced independently for each of the 1,000 possible queries.
4.1.3 The Systems
We have a total of 30 different systems, each of which uses either a nearest-neighbor approach
or KCCA, paired with a different combination of image and text representations. But for
the purposes of discussing different evaluation metrics, we will focus on only a small number
of these systems: the best-performing nearest-neighbor-based system, NN (NN5idf
F1 ), and a
small number of KCCA-based systems with with different text kernels: BoW1 and BoW5
both use the simple bag-of-words kernel. TagRank uses
Hwang and Graumans (2012)

idf
kernel, Tri5 uses the trigram kernel, and Tri5Sem (Tri5A,DBNC+ic in Appendix B) uses the
872

fiFraming Image Description as a Ranking Task



idf-reweighted trigram kernel with all distributional and the alignment-based similarities.
With the exception of BoW1, where we have arbitrarily selected a single caption for each
training image, all other models use all five captions for the training images. For BoW5,
we merge them into a single document. In all other cases, we follow Moschitti (2009) and
sum the kernel responses over the cross product of sentences before normalization. All of
these systems (including NN) use the pyramid kernel as their image representation. For the
large-scale evaluations in Section 4.3, the scores of all models are given in Appendix B.
All our systems use Hardoon et al.s (2004) KCCA implementation, which allows us to
vary the regularization parameter . We also vary n, the number of dimensions (largest
eigenvalues) in the learned projection The allowable values for these parameters were based
on early exploratory experiments. In the experiments reported in this paper,  is sampled
from 4 possible values (0.1, 0.5, 1, 5), and n is chosen from 46 possible values in the range
of (10, 6000). There are two additional parameters that are fixed in advance for each text
image kernel pair: the image kernels are either squared or cubed, and the text kernels are
regularized by multiplying the values on the diagonal by a factor d in the range of (1, 15).
For each kernel and for each of the two tasks (image annotation and search), we then use
the development set to pick five settings of n and  that maximize the recall of the original
item as the first result, five settings that maximize its recall among the first five results,
and five settings that maximize its recall among the first ten results, yielding a total of 15
different models for each pair of kernels and each task. For each query image (annotation)
or caption (search) in the test set, each of these 15 models returns a ranking of all 1,000 test
items (sentences or images). To combine these 15 rankings, we use Borda counts (van Erp &
Schomaker, 2000), a simple, deterministic method for rank aggregation: with N items to be
ranked, each system assigns a score of N  r to the item it ranks in position r = 0...N  1,
and the final rank of each item is determined by the sum of its scores across all systems.
We break ties between items by the median of their ranks across all models.
4.2 Metrics for the Quality of Individual Image-Caption Pairs
Before we consider metrics that consider the quality of the ranked list of results (Section 4.3),
we first examine metrics that measure the quality of individual image-caption pairs.
4.2.1 Human Evaluation with Graded Expert Judgments
Expert scores The decision of how well a caption describes an image ultimately requires
human judgment. For the caption generation task, a number of different evaluation schemes
have been proposed for image description: Ordonez et al. (2011) presented judges with a
caption produced by their model and asked them to make a forced choice between a random
image and the image the caption was produced for, and Kuznetsova et al. (2012) asked judges
to choose between captions from two of their models for a given test image. Such forced
choice tasks may give a clear ranking of models, but cannot be compared across different
experiments unless the output of each system is made publicly available. One advantage
of framing image description as a ranking task is that different systems can be compared
directly on the same test pool. Forced choice evaluations also do not directly measure the
quality of the captions. Following common practice in natural language generation, Yang
et al. (2011) and Kulkarni et al. (2011) evaluated captions on a graded scale for relevance
873

fiHodosh, Young & Hockenmaier

... describes the image
without any errors
(score = 4)

The selected caption ...
... describes the image
with minor errors
(score = 3)

... is somewhat
related to the image
(score = 2)

... is unrelated
to the image
(score = 1)

A girl wearing a
yellow shirt and
sunglasses smiles.

A man climbs
up a sheer wall
of ice.

A Miami basketball
player dribbles by an
Arizona State player.

A group of people
walking a city street in
warm weather.

A boy jumps into
the blue pool
water.

A dog in a grassy field,
looking up.

Basketball players in
action.

A man riding a motor
bike kicks up dirt.

Dogs pulling
a sled in a
sled race.

Two little girls
practice martial
arts.

A snowboarder in the
air over a snowy
mountain.

A child jumping
on a tennis court.

A boy in a blue life
jacket jumps into the
water.

A black dog with
a purple collar
running.

Figure 4: Our 14 rating scale for the fine-grained expert judgments, with actual examples
returned by our best model (Tri5Sem)
and readability, while Li et al. (2011) added a creativity score, and Mitchell et al. (2012)
compared systems based on whether the captions describe the main aspects of the images,
introduce objects in an appropriate order, are semantically correct, and seemed to have been
written by a human.
Since the captions in our test pool are all produced by people, we do not need to evaluate
their linguistic quality, and can focus on their semantic correctness. In order to obtain a
fine-grained assessment of description quality, we asked three different judges to score imagecaption pairs returned by our systems on a graded scale from 1 to 4. The judges were 21
adult native speakers of American English, mostly recruited from among the local graduate
student population. In contrast to the anonymous crowdsourcing-based evaluation described
in Section 4.3.2, we will refer to them as experts. The rating scale is illustrated in Figure 4
with actual examples returned by our models. A score of 4 means that the caption describes
the image perfectly (without any mistakes), a score of 3 that the caption almost describes
the image (minor mistakes are allowed, e.g. in the number of entities), whereas a score of
2 indicates that the caption only describes some aspects of the image, but could not be
used as its description, and a score of 1 indicates that the caption bears no relation to the
image. The online appendix to this paper contains our annotation guidelines. Annotators
took on average ten minutes per 50 image-caption pairs, and all image-caption pairs were
judged independently by three different annotators. Inter-annotator agreement, measured
as Krippendorffs (2004) , is high ( = 0.81) (Artstein & Poesio, 2008). The final score of
each image-caption pair was obtained by averaging the three individual scores. Since this
is the most time-consuming evaluation, we only judged the highest-ranked caption for each
test image on the annotation task, and only focused on the subset of our models described
above. To gauge the difficulty of this task on our data set, we also include a random
baseline. Since we only evaluate a single caption for each image, we are interested in the
percentage of images for which a suitable caption was returned. We therefore show each
models cumulative distribution of test items with scores at or above thresholds ranging
874

fiFraming Image Description as a Ranking Task

Quality of first caption (image annotation)
Cumulative distribution of expert scores (%  X)
= 4.0  3.66  3.33 3.0  2.66  2.33  2.0
0.5 0.6
3.4 4.1
BoW1
6.6 8.1
BoW5
9.7
11.8
TagRank 9.6
12.3

0.7
5.2
9.9
13.6
14.2

1.1
8.5
18.3
19.8
21.1

1.5
11.4
22.9
24.7
25.8

2.9
16.3
29.7
33.0
32.9

7.8
27.1
44.2
46.9
46.2

Tri5Sem 11.0

15.7

23.0

28.1

36.9

53.0

Random
NN

13.3

Table 1: Cumulative distribution of expert judgments on our 14 scale (Figure 4), indicating
what percentage of image-caption pairs are judged to be at or above a given score. Scores
are averaged over three judges. Superscripts indicate statistically significant difference to
Tri5Sem ( : p  0.1,  : p  0.05,  : p  0.01).
from 4.0 to 2.0. Each threshold can be interpreted as a more or less strict mapping of the
fine-grained scores into binary relevance judgments. In order to assess whether the difference
between models at any given threshold reaches statistical significance, we use McNemars
significance test, a paired, non-parametric test that has been advocated for the evaluation
of binary classifiers (Dietterich, 1998). Given the output of models A and B on the same
set of items, McNemars test considers only the items on which A and Bs output differ (the
discordant pairs of output) to test the null hypothesis that both outputs are drawn from
the same underlying population. Among these discordant pairs, it compares the proportion
of items for which model A is successful but model B is not with the proportion of items for
which Model B is successful but model A is not. In our results tables,  superscripts indicate
whether the difference between a model and Tri5Sem is statistically significant ( : p  0.1,
 : p  0.05,  : p  0.01) .
Expert results (Table 1) We first interpret the expert scores as binary relevance
judgments, and therefore show their cumulative distribution for different thresholds in. We
see very clear differences between the random baseline, NN, and the KCCA models at all
thresholds. The differences between NN and the random model, as well as between any
KCCA model and NN are highly significant (p < 0.001) at any threshold. While the random
baseline returns a perfect caption for 0.5% of the images, and a good caption (assuming
a threshold of  2.66) for 1.5% of the images, our best KCCA model, Tri5Sem, returns
a perfect caption for 11.0% and a good caption for 28.1% of the images. However, the
differences among the KCCA models are more subtle, and may only become apparent at
lower thresholds. There is no significant difference between BoW5 and TagRank at any
threshold, but they are both significantly better than BoW1 (p < 0.001) at thresholds of
3.33 and above. Tri5Sem outperforms all other models, but the differences to BoW5 and
TagRank only reach statistical significance when the threshold of what is considered a
suitable caption is lowered to either 3.33 (p = 0.06) or 3.0 (p = 0.01), or to 2.66 (p = 0.08)
or to 2.33 (p = 0.005). This lack of statistical significance can be partially explained by
the fact that McNemars test has relatively low power when the percentage of items on
which the two models are successful is very low, as is the case for the higher thresholds here.

875

fiHodosh, Young & Hockenmaier

We will show in Sections 4.3.1 and 4.3.2 below that there is a very significant difference
between Tri5Sem and these two models on image annotation once we extend the analysis
beyond the highest-ranked caption. This shows that evaluations that are only based on a
single caption returned per image may fail to uncover significant differences between models
that become apparent once multiple results are considered. It may also be important to
consider performance on both annotation and retrieval. On the image retrieval task, we
will see that Tri5Sem significantly outperforms all other models even when only the first
result is considered. Table 1 also reveals another artefact of McNemars test: since it is
not based on absolute differences in performance but on the number of discordant pairs,
the difference between BoW1 and Tri5Sem at thresholds 2.66 and 2.0 is considered less
significant than that between BoW5 and Tri5Sem at the same thresholds, even though
BoW1s scores are lower than BoW5s. In Table 2, we present the systems average expert
scores, and use Fishers Randomization Test to determine statistical significance. According
to this evaluation, Tri5Sem is very significantly better than all other models (p  0.0001 in
all cases), but since the average score of Tri5Sem is only 2.08, this difference is not reflected
at the higher thresholds in the cumulative distribution shown in Table 1.
4.2.2 Automatic Evaluation with Bleu and Rouge
Since human judgments are expensive and time-consuming to collect, we now examine how
well they can be approximated by Bleu (Papineni et al., 2002) and Rouge (Lin, 2004),
two standard metrics for machine translation and summarization.
Bleu and Rouge scores Bleu and Rouge scores can be computed automatically
from a number of reference captions, and have been used to evaluate a number of caption
generation systems (Kulkarni et al., 2011; Ordonez et al., 2011; Li et al., 2011; Kuznetsova
et al., 2012; Yang et al., 2011; Gupta et al., 2012), although it is unclear how well they
correlate with human judgments on this task.
Given a caption s and an image i that is associated with a set of reference captions Ri ,
the Bleu score of a proposed image-caption pair (i, s) is based on the n-gram precision of
s against Ri , while Rouge is based on the corresponding n-gram recall. As is common for
image description, we only consider unigram-based scores (only 3.5% of all possible imagecaption pairs in the test have a non-zero bigram-based Bleu-2 score, but 39.4% set have
a non-zero Bleu-1 score). We also ignore Bleus brevity penalty, since our data set has
relatively little variation in sentence length, and we would like to avoid penalizing short, but
generic captions that include few details but are otherwise correct. Hence, if cs (w) is the
number of times word w occurs in s:
P

Bleu(i, s) =
Rouge(i, s)

=

min(cs (w),maxrRi cr (w))
P
ws cs (w)
P
P
min(cs (w),cr (w))
rRi
P wrP
rR
wr cr (w)
ws

(28)

i

Both reference and candidate captions are preprocessed. We first tokenize the sentences with
the OpenNLP5 tools. Then we break up hyphenated words, stripping out non-alphanumeric
5. http://opennlp.apache.org

876

fiFraming Image Description as a Ranking Task

Avg. score of first caption
(image annotation)
Expert

Bleu


Rouge

BoW1
BoW5
TagRank

1.22
1.57
1.90
1.98
1.99

0.31
0.35
0.43
0.46
0.46

0.04
0.11
0.14
0.15
0.15

Tri5Sem

2.08

0.48

0.17

Random
NN



Table 2: Comparison of averaged scores according to the 4-point expert evaluation (Figure 4), Bleu and Rouge, using all five test captions as reference. Superscripts indicate
statistically significant difference to Tri5Sem (  : p  0.1,  : p  0.05,  : p  0.01).
and hyphen characters, and converting all words to lower case. Following the work of Lin
(2004), we use a stemmer (Porter, 1980) and remove stopwords before compute Rouge
scores. We compute the Bleu and Rouge score of a system as the average Bleu or
Rouge scores of all items in the test set.6
We use Fishers Randomization Test (Fisher, 1935; Smucker, Allan, & Carterette, 2007)
to assess the statistical significance of the difference between models. This is a paired,
sampling-based test that evaluates the null hypothesis that the results of models A and
B are produced by the same underlying distribution. In each sample, the scores that A
and B assign to each test item are randomly reassigned to the two models, and p-values
are obtained by comparing the actual difference between A and Bs performance to the
fraction of samples with equal or greater difference between the models. We sample 100,000
reassignments of the entire test set.
Bleu and Rouge results (Table 2) Table 2 shows the average Bleu and Rouge scores
of the highest ranked caption pairs returned by each image annotation systems, computed
against a reference pool consisting of the five original captions for each test image (including
the caption that was randomly selected to be part of the candidate pool). These scores
lead to the same broad conclusions as the average expert scores: all metrics find very clear
differences (p < 0.0001) between the random baseline and any of the other models, as
well as between NN and any of the KCCA models, and none find any significant difference
between BoW5 and TagRank. Tri5Sem outperforms the other KCCA models according
to all metrics, but both the expert evaluation and Rouge find a much larger difference to
BoW5 (Experts: p  0.0001, Rouge: p < 0.001) than to TagRank (Experts: p = 0.001,
Rouge: p = 0.005). Bleu only finds a significant difference to TagRank (p < 0.05), but
not to BoW5 (p < 0.05), which indicates Bleu may be less well suited to identify more
subtle differences between systems.
Agreement of Bleu and Rouge with expert scores Since it is difficult to measure
directly how well the Bleu and Rouge scores agree with the expert judgments, we consider
6. A systems Bleu score is usually computed at the corpus level, but since we are only dealing with
unigram scores and evaluate all systems on sentences from the same corpus, the averaged sentence-level
Bleu scores of our systems we report are almost identical (r > 0.997) to their corpus-level Bleu scores.

877

fiHodosh, Young & Hockenmaier

a number of different relevance thresholds for each type of score (B , R , and E ), and turn
them into binary relevance judgments. This allows us to use Cohens (1960)  to measure
the agreement between the corresponding binarized scores. Since Bleu and Rouge both
require a set of reference captions for each test image, we compare four different ways of
defining the set of reference captions (for detailed scores, see Tables 8 and 9 in the appendix).
Since our data set contains multiple descriptions for each image, we first use all five
captions as reference. In this setting, Bleu reaches the best agreement ( = 0.72) against
E = 4.0 with B = 1.0 or against E  3.6 with B  0.8. However, such high Bleu
scores are generally only obtained when the system proposes the original caption. Rouge
has much lower agreement ( = 0.54) against the expert scores, obtained at R  0.4 vs.
E  4.0 or E  3.6, or R  0.3 against E  3.0. Since other data sets may have
only one caption per image, we also evaluate against a reference corpus that consists only
of the single caption in the test pool. In this case, both metrics reach again the highest
agreement against an expert threshold of E = 4.0 (Bleu:  = 0.71, Rouge:  = 0.69),
with thresholds of B  0.8, and R  0.9. We conclude that neither Bleu nor Rouge are
useful in this scenario, since they require such high thresholds that they only capture how
often the system returned the reference caption.
When Bleu and Rouge are used to evaluate caption generation systems, we cannot
assume that the generated caption is identical to one of the reference captions. We therefore
examine to what extent Bleu and Rouge scores agree with human judgments when the
candidate pool contains human generated captions, but is disjoint from the reference captions. We first use a reference corpus of four captions per image, excluding the caption we
use in the candidate pool. In this case, all three metrics show significantly lower agreement
with human judgments than when the candidate pool contains the reference caption. Bleu
reaches only  = 0.52 (with B  0.7 against E  3.3) and Rouge reaches only  = 0.51
(with R  0.2 against E  2.6). To simulate the case where only a single caption per
image is available, we also evaluate against a reference corpus consisting of only one of these
four captions. In this case, agreement with human judgments is even lower: Bleu reaches
 = 0.36, and Rouge reaches  = 0.42. These results suggest that Bleu and Rouge are
not appropriate metrics when the pool of candidate captions does not contain the reference
captions, and lead us to question their usefulness for the evaluation of caption generation
systems. This is consistent with the findings of Reiter and Belz (2009), who have studied
Bleu and Rouge scores to evaluate natural language generation systems, and concluded
that they may be useful as metrics of fluency, but are poor measures of content quality.
4.3 Metrics for the Large-Scale Evaluation of Image Description Systems
Metrics that only consider the first caption returned for each image cannot capture the fact
that a better model should score good captions higher than most other captions, even if fails
to consider them the best possible caption. Since our systems return a ranked list of results
for each item, we now examine metrics that allow us to evaluate the quality of this list.
In contrast to the human evaluations described in Section 4.2 above, we now also evaluate
our image retrieval systems. We first consider metrics that can be computed automatically:
recall and median rank of the item (image or sentence) that was originally associated with
the query sentence or image (Section 4.3.1). We then show how to use crowdsourcing to

878

fiFraming Image Description as a Ranking Task

Performance: Rank of the original item
R@k: percentage of queries with the original item among top X responses.
Median r: median rank of the original item
R@1

Image annotation
R@5 R@10 Median r

BoW1
BoW5
TagRank
Tri5

2.5
4.8
6.2
6.0
7.1

7.6
13.5
17.1
17.0
17.2

9.7
19.7
24.3
23.8
23.7

Tri5Sem

8.3

21.6

30.3

NN

251.0
64.0
58.0
56.0
53.0
34.0

R@1

Image retrieval
R@5 R@10 Median r

2.5
4.5
5.8
5.4
6.0

4.7
14.3
16.7
17.4
17.8

7.2
20.8
23.6
24.3
26.2

7.6

20.7

30.1

272.0
67.0
60.0
52.5
55.0
38.0

Table 3: Model performance as measured by the rank of the original image or caption (=
correct response). R@k: percentage of queries for which the correct response was among
the first X results. Median r: Median position of the correct response in the ranked list of
results. Superscripts indicate statistically significant difference to Tri5Sem ( : p  0.05,
 : p  0.01).
collect a very large number of human judgments (Section 4.3.2), and use these relevance
judgments to define two additional metrics: the rate of success, which is akin to recall,
and R-precision, an established information retrieval metric (Section 4.3.3). Although these
metrics allow us to evaluate all of our systems, we will focus our discussion on the small set
of systems considered so far, and refer the interested reader to Section B of the appendix
for the scores of all systems.
4.3.1 Recall and Median Rank of the Original Item
One advantage of our ranking framework is that the position of the original caption or image
among the complete list of 1,000 test items can be determined automatically. Since a better
system should, on average, assign a higher rank to the original items than a worse system,
we can use their ranks to define a number of different evaluation metrics.
Recall (R@k) and median rank scores Since each query is only associated with a
single gold result, we need not be concerned with precision. However, recall at position k
(R@k), i.e. the percentage of test queries for which a model returns the original item among
the top k results, is a useful indicator of performance, especially in the context of search,
where a user may be satisfied if the first k results contain a single relevant item. We focus on
k = 1, 5, 10 (R@1, R@5, R@10). Since this is a binary metric (for each query, the gold item
is either found among the top k results or not), we use again McNemars test to identify
statistically significant differences between models. Conversely, the median rank indicates
the k at which a system has a recall of 50% (i.e. the number of results one would have
to consider in order to find the original item for half the queries). Here, we use Fishers
randomization to identify significant differences between models.
Recall (R@k) and median rank results (Table 3) The results in Table 3 confirm our
earlier observation that the NN baseline is clearly beaten by all KCCA models (p < 0.001
for all metrics and models, except for R@1 search, where the difference to BoW1 has a
879

fiHodosh, Young & Hockenmaier

p-value of p < 0.01). Since the R@1 annotation scores are based on the same image-caption
pairs as the expert scores in Table 1, we can compare them directly. The difference between
the R@1 and expert scores, even at the strictest threshold of 4.0 for the experts, indicates
that measures which capture how often the original caption was returned should be viewed
as a lower bound on actual performance: while Tri5Sem returns the original caption first
for 8.3% of the images, our human judges found that these captions describe 11.0% of the
images without any errors. This discrepancy is even larger for BoW5 (6.2% vs. 9.7%) and
TagRank (6.0% vs. 9.6%). As a consequence, the automatically computed R@1 scores
indicate erroneously that there is a statistically significant difference between the quality
of the first captions returned by Tri5Sem and those returned by BoW5 or TagRank, even
though these differences are not significant according to the human evaluation. However,
metrics that are only based on the first caption may fail to identify differences between
models that become very apparent under all other metrics. For example, R@1 reveals
no significant difference between Tri5 and Tri5Sem on the annotation task, although their
difference is highly significant according to all other metrics. In Section 4.3.3, we present the
results of a large-scale human evaluation which confirm that the actual differences between
Tri5Sem and Tri5 on annotation can only be identified when more than the first caption is
taken into account.
Table 11 in Section B provides recall and median rank scores for all models.
4.3.2 Collecting Binary Relevance Judgments on a Large Scale
In order to perform a human evaluation of a system that goes beyond measuring the quality
of the highest ranked result, we would have to obtain relevance judgments for all imagecaption pairs among the top k results for each query. Since we have two tasks, and a
total of 30 different systems, this set consists of 113,006 distinct image-caption pairs for
k = 10, rendering an exhaustive evaluation on the four-point scale described in Section 4.2.1
infeasible. We therefore needed to reduce the total number of judgments needed, and to
define a simpler annotation task that could be completed in less time. Crowdsourcing
platforms such as Amazon Mechanical Turk offer new possibilities for evaluation because
they enable us to collect a large number of human judgments rapidly and inexpensively,
and a number of researchers have evaluated caption generation systems on Mechanical Turk
(Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012; Kulkarni et al., 2011; Li
et al., 2011). But these experiments have not been performed at the scale of our analysis, and
have also not evaluated how well crowdsourced judgments for this task approximate what
can be obtained from a smaller pool of judges that can be given more detailed instructions.
We examine here whether crowdsourcing allows us to collect reliable relevance judgments
for a large scale evaluation of all of our image description systems.
The crowdsourcing task We presented workers with images that were paired with ten
different captions, and asked them to indicate (via checkboxes) which of the captions describe
the image. We adapted the guidelines developed for the fine-grained annotation such that
a caption that describes the image with minor errors (corresponding to a score of 3 on our
4-point scale) would still be permitted to receive a positive score. These guidelines can also
be found in the online appendix to this paper. Each individual task consisted of six different
images, each paired with ten captions, and included a copy of the guidelines. We accessed
880

fiFraming Image Description as a Ranking Task

Amazon Mechanical Turk through a service provided by Crowdflower.com, which makes it
easy to include control items for quality control. One of the six images in each task was
such a control item, which we generated by taking random images from the development
set, using between one and three of their original captions as correct responses, and adding
another nine to seven randomly selected captions (which we verified manually that they
did not describe the image) as incorrect responses. We only used workers who judged 70%
of their control items correctly. Each image-caption pair was annotated by three different
annotators (at a total cost of 0.9), and the final score of each image-caption pair was
computed as the average number of positive judgments it received.
Filtering unlikely image-caption pairs In order to reduce the number of annotations
needed, we devised a filter based on Bleu scores (Papineni et al., 2002) to filter out imagecaption pairs whose caption is so dissimilar from the five captions originally written for the
image that it is highly unlikely it describes the image. We found that a filter based on
unigram Bleu-1 scores in combination with the stemming and stop word removal that is
standardly done by Lins (2004) Rouge script (Bleupre ) proved particularly effective: a
threshold of Bleupre  0.25 filters out 86.0% of all possible (1,0001,000) image-caption
pairs in our test set, but eliminates only 6.7% of the pairs with an expert score of 2 32 or
greater, and 3.5% of the pairs with an expert score of 3 or greater. A slightly higher cutoff
of Bleupre  0.26 would filter out 90.4% of all image caption pairs, but discard 12.3% of all
image-caption pairs with an expert score of  2 23 and 7.5% of all image-caption pairs with
an expert score of  3. Among the 113,006 image-caption pairs that we actually wished
to obtain judgments for, the 0.25 filter eliminates 72.8%, reducing the number of pairs we
needed to annotate to 30,781. Since our setup required us to pair each image with a number
of captions that was a multiple of 10, we also annotated an additional 10,374 image caption
pairs that had been filtered out, allowing us to evaluate the performance of our filter. For
98.3% of these filtered out pairs, all Mechanical Turk judges decided that the caption did not
describe the image, and for 99.8% of them, the majority of annotators thought so. We also
found that standard Bleu-1 without preprocessing is not a very effective filter: a threshold
of Bleu  0.330 misses 6.9% of the good captions (with an expert score of  2 23 ), while
only filtering out 55% of the entire data set, whereas a threshold of Bleu  0.333 filters out
65% of the entire data set, but misses 11.9% of the good captions.
Agreement of crowdsourced and expert judgments We again use Cohens  to
measure the agreement between the crowdsourced and the expert judgments (Table 10 in the
appendix). The best agreement is obtained between crowdsourced scores with a threshold
of 0.66 or above (i.e. at least two of the three judges think the caption describes the image)
and expert scores with a threshold of 3.33 (one expert thinks the caption describes the
image perfectly and the other two agree or think it describes the image with only minor
errors, or two experts think it describes the image perfectly and the other one thinks it is
at least related). At  = 0.79, this is a significantly better approximation to the expert
scores than was possible with either Bleu or Rouge. We also examine the precision, recall
and f-scores that these approximate relevance judgments achieve when compared against
relevance judgments obtained from binarizing expert judgments (Table 10). 98.6% of all
items with a perfect expert score (and 95.0% of all items with an almost perfect expert
score of 3.7) are identified, and at least 94.7% of the items that pass this threshold have an
881

fiHodosh, Young & Hockenmaier

expert score of 2.7 or greater (i.e. the majority of experts agreed that the caption describes
the image perfectly or with minor errors). Using a threshold of 0.66 adds 2,031 suitable
image-caption pairs to the 1,000 test images paired with their original caption. Among the
1,000 test captions, 446 still describe only a single image, 202 describe two test images, 100
three, and 252 describe four or more images. Among the 1,000 test images, 331 have only
a single (i.e. the original) caption, 202 have two possible captions, 100 have three possible
captions, and 317 have four or more captions.
4.3.3 Large-Scale Evaluation with Relevance Judgments
The crowdsourced relevance judgments allow us to define two new metrics, the rate of
success (S@k) and R-precision. We believe R-precision to be the more reliable indicator
of overall performance, since it summarizes the human judgments in a single number that
does not depend on an arbitrary cutoff. We therefore use it in Section 4.3.4 for an in-depth
analysis of the impact of the different linguistic features our models incorporate. The S@k
rate of success scores are motivated by the fact that search engines commonly return multiple
results at once. Since users may be satisfied as long as these results contain at least one
relevant item, S@k scores provide a more direct measure of utility for hypothetical users.
Rate of success (S@k) scores The rate of success metric (S@k) is analogous to the
recall-based R@k-scores used in Table 3, and is intended to measure the utility of our
system for a hypothetical user. It indicates the percentage of test items for which at least
one relevant result is found among the highest ranked k results. Following the analysis in
Section 4.3.2, an image-caption pair is considered relevant if the majority of the judges say
that the caption describes the image.
Rate of success results (Table 4) Table 4 confirms again that NN performs clearly
worse than any of the KCCA models. The differences between Tri5Sem and the other
models shown in Table 4 are highly statistically significant (p < 0.001) for all metrics except
for the S@1 annotation scores, where, in agreement with the expert scores from Table 1, only
the differences to NN and BoW1 are significant. It is unclear why the quality of the first
caption that Tri5Sem returns for annotation is not significantly better than those returned
by the other models, since it outperforms them on all other metrics. The S@k scores in
Table 4 indicate that Tri5Sem returns a relevant caption among the top 10 responses for
49.1% of the images, and a relevant image for 48.5% of the captions. A comparison with
the expert scores in Table 1 shows that all S@1 annotation scores lie between expert scores
with a threshold of 3.66 and 3.0, while a comparison with the R@k results in Table 3 shows
that the S@1 scores are at least twice as high as the corresponding R@1 scores. That is, the
highest ranked response is just as often a relevant item that was not originally associated
with the query as it is the original gold item itself.
R-precision scores Given the crowdsourced relevance judgments, each test image may
now be associated with multiple relevant captions, and each test caption may have been
deemed relevant for multiple images besides the one it was originally written for. When
queries have a variable number of relevant answers, the performance of retrieval systems is
commonly measured in terms of R-precision (Manning, Raghavan, & Schtze, 2008). Unlike
the S@k scores, this metric does not depend on an arbitrary cutoff, but summarizes the
882

fiFraming Image Description as a Ranking Task

Rate of success (S@k)
(Percentage of items with relevant response among top X results)
Image annotation
S@1
S@5
S@10

Image retrieval
S@1
S@5
S@10

BoW1
BoW5
TagRank
Tri5

5.8
12.2
15.0
16.2
16.4

15.4
30.3
34.1
34.2
32.9

20.2
39.7
42.7
42.9
43.4

5.0
11.4
12.1
12.4
13.1

13.3
30.5
31.5
31.5
33.1

18.4
40.2
40.8
41.6
43.8

Tri5Sem

16.6

37.7

49.1

15.7

36.9

48.5

NN

Table 4: The rate of success (S@k) indicates the percentage of test items for which the top X
results contain at least one relevant response. Superscripts indicate statistically significant
difference to Tri5Sem ( : p  0.1,  : p  0.05,  : p  0.01)
R-precision
Annotation
NN



Search


Total

BoW1
BoW5
TagRank
Tri5

5.2
10.7
11.1
11.7
11.6

3.8
9.6
10.5
10.5
11.0

4.5
10.1
10.8
11.1
11.3

Tri5Sem

13.7

13.4

13.5

Table 5: Model performance as measured by R-precision, with statistically significant differences to Tri5Sem ( : p  0.1,  : p  0.05,  : p  0.1)
performance of each system in a single number, allowing us to rank models according to
their overall performance (see Section 4.3.4 below). And while the S@k scores measure only
whether at least one of the relevant items is ranked highly, R-precision requires all relevant
items to be ranked highly. It is therefore a better indicator of the quality of the mapping
between images and sentences, since a better mapping should prefer all relevant captions or
images over any irrelevant caption or image.
The R-precision of system s on a query qi with ri known relevant items in the test data
is defined as its precision at rank ri (i.e. the percentage of relevant items among the top ri
responses returned by s). The R-precision of s is obtained by averaging over all test queries.
We again use Fishers randomization test to assess whether the differences between models
reaches statistical significance.
R-precision results (Table 5) Table 5 gives the R-precision of the model types that were
used when collecting expert judgments (Section 4.2.1). We see that the nearest neighbor
baseline is again very clearly below all KCCA models (p < 0.001). R-precision indicates that
there is little difference between BoW1, BoW5 and TagRank in terms of their overall performance. Although TagRank and Tri5 outperform BoW1 slightly on search (p = 0.062),
the only statistically significant difference among these three models is that between BoW1
and Tri5 on search (p = 0.01). In contrast to the human evaluation that considered only
883

fiHodosh, Young & Hockenmaier

R-precision

Tri5
Ann. Search

+IDF
Ann. Search

Ann.

+Align
Search

+Align&IDF
Ann. Search

Tri5

11.6

11.0

12.5ii

11.3

13.4aaa

12.3aa

13.4a

13.2aaa,ii

+DBNC
+Dic
+DBNC+ic

12.7dd
12.7dd
12.5dd

12.1dd
12.8ddd
12.7ddd

12.9
12.8
13.3d

12.2ddd
13.1ddd
13.0ddd

13.2
13.0
13.4aa

12.8a
12.8
13.2dd

12.9
13.3
13.7

12.9a
13.4
13.4

Table 6: The effect of adding IDF weighting (i), alignment-based similarities (a) and distributional similarities (d) to the Tri5 model. The bolded scores indicate Tri5 (top left) and
Tri5Sem (Tri5 +Align&IDF+DBNC+ic ; bottom right). Superscripts indicate statistically
significant differences that result from the addition of the corresponding feature (x : p  0.1,
xx : 0.05, xxx : p  0.01). Dc = distributional similarities computed over corpus c (the
BNC, our training corpus of image captions (ic), or both)
the first result (Table 1), Tri5Sem clearly outperforms all other models on both annotation
and retrieval (for all differences p  0.0001). Table 12 in Appendix B shows scores for all
models.
4.3.4 Measuring the Impact of Linguistic Features (Table 6)
The results presented so far indicate clearly that Tri5Sem outperforms the simpler Tri5
model, but have not considered the impact of the individual text features that distinguish
the two models. Since R-precision summarizes the performance of each system in a single
number, it allows us to easily perform this analysis.
Using R-precision for model comparison Table 6 shows the results of an ablation
study which compares the R-precision of Tri5 and Tri5Sem with that of other trigrambased KCCA models that use a subset of Tri5Sems additional features. The basic Tri5
model yields the bolded scores shown in the top left corner. Tri5Sems scores are given in
the bottom right corner. The top row contains models that do not capture any distributional
similarities, while each of the bottom three rows corresponds to the addition of one kind
of distributional similarity (computed on the BNC, on the image captions in our training
corpus, or on both corpora) to the corresponding model in the top column. The first column
contains models that do not capture any IDF reweighting or alignment-based similarities.
The second column corresponds to the addition of IDF reweighting to models in the first
column, while the third column adds alignment-based similarities to the models in the first
column. The last column adds both IDF-reweighting and alignment-based similarities, and
these scores should be compared to both the second and third column. Superscripts indicate that the addition of a particular feature leads to a statistically significant improvement
over the model that does not include this feature but is otherwise identical. That is, d
superscripts show that the addition of a distributional similarity metric leads to a significant improvement over the model in the top cell of the same column. The i superscripts
indicate that the addition of IDF reweighting leads to a significant improvement over the
corresponding model without IDF reweighting in the immediately preceding cell in the same

884

fiFraming Image Description as a Ranking Task

row. The a superscripts in the third column show that the addition of the alignment-based
similarity leads to a significant improvement over the model without IDF reweighting shown
in the first column of the same row, and a superscripts in the fifth column show that the
addition of the alignment-based similarity to the model with IDF reweighting shown in the
second column of the same row leads to a significant improvement.
The impact of IDF weighting, distributional and alignment-based similarities
While IDF weighting is almost always beneficial, the improvements obtained by adding
IDF weighting to a given text kernel reach statistical significance (indicated by i superscripts in Table 6) in only two cases: the performance of the basic Tri5 model on image
annotation, and the performance of the alignment-based Tri5 model on image search. By
contrast, adding lexical similarities leads almost always to a significant or highly significant
improvement. Distributional similarities (d superscripts) are very beneficial for the basic
Tri5 model on both tasks, and help the IDF weighted Tri5 model on image search. Distributional similarities computed on both corpora also significantly improve the performance
of the alignment-based Tri5 model that does not incorporate IDF weighting. Adding them
to the alignment-based Tri5 model without IDF weighting leads to further improvement
on search (while not helping or slightly decreasing performance on annotation, albeit not
significantly so). The improvements on search only reach statistical significance when the
similarities computed over both corpora are added. Conversely, adding alignment-based
similarities to the non-IDF weighted Tri5 model with distributional similarities from both
corpora leads to a significant improvement on annotation. Finally, the top cell of the last
column shows that adding alignment-based similarities to the IDF-weighted Tri5 model
leads to a significant improvement on both tasks, although the impact on search is even
greater. Comparing this models performance to the alignment-based Tri5 model without
IDF weighting shows that in this case, IDF weighting only helps on search. The bottom
cells of this column show that adding alignment-based similarities to models that already
use IDF weighting and distributional similarities, or adding IDF weighting to models with
distributional and alignment-based similarities generally lead to minor improvements.
Table 6 shows only whether the difference in performance obtained by the addition of
one kind of feature reaches statistical significance, but it is worth noting that any model
that captures lexical similarities of any kind is significantly better than the basic Tri5
model on both tasks (p  0.02 search; p < 0.0001 annotation), while IDF-reweighting by
itself only leads to a significant improvement on the annotation task (p < 0.03). Moreover,
the difference between Tri5Sem (13.7 search; 13.4 annotation) and the basic Tri5 kernel
with IDF-reweighting (12.5 search; 11.3 annotation) are highly significant (p < 0.03 search;
p < 0.0001 annotation).
The impact of Lins similarity Not shown in Table 6 is the performance of Tri5Lin ,
the model which augments the trigram kernel with Lins (1998) WordNet-based similarity.
Tri5Sem does not include Lins similarity, since we found during development that Tri5Lin
performed similarly to or worse than the basic Tri5 model on the automatic R@k and median
rank scores. This is also reflected in Tri5Lin s R-precision scores of 11.7 for annotation (Tri5:
11.6) and 10.7 for search (Tri5: 11.0). Lins similarity may simply be too coarse for our
purposes. As shown in Table 3, the hypernym relations in WordNet lead it to associate terms
such as swimming and football with each other. But even though these are semantically
885

fiHodosh, Young & Hockenmaier

Correlation of system rankings
between S@k and R@k
Annotation


@1
@5
@10

0.86
0.92
0.96

0.69
0.76
0.82

Correlation of system rankings
between R-precision

Search


0.97
0.97
0.97

(a) S@k vs. R@k

0.87
0.88
0.87

Annotation


R@1
R@5
R@10
Median rank

0.85
0.93
0.94
-0.92

0.68
0.78
0.79
-0.79

Search


0.94
0.96
0.97
-0.97

0.83
0.87
0.89
-0.89

(b) R-precision vs. R@k and median rank

Table 7: Correlation (Spearmans  and Kendalls  ) of system rankings obtained from
human metrics (S@k and R-precision) and automated scores (R@k and median rank)
related by the fact that they are both different kinds of sports or activities, they are visually
very dissimilar, and should not be considered related by our systems.
4.3.5 Can Human Evaluations Be Approximated by Automatic Techniques?
R-precision and the S@k scores require human judgements, and therefore cannot be applied
to datasets where these judgements have not yet been collected or whose scale may prohibit
ever creating a definitive set of judgements. However, if the evaluation is intended to measure
relative progress on image description rather than absolute performance, our automatic
metrics may be a sufficient approximation, since they yield a similar ranking of systems to
R-precision and the S@k scores. Table 7(a) shows the correlations between the rankings of
all of our NN and KCCA systems (n = 30) obtained from the S@k scores and those obtained
from the corresponding R@k scores. Table 7(b) shows the correlations between R-precision
and our automatic metrics. We report two rank correlation coefficients, Spearmans  and
Kendalls  . We first observe that system rankings obtained via R@1 do not correlate highly
with either R-precision or S@1 based rankings. On the other hand, we also observe that
R@5, R@10, and the median rank scores correlate well with R-precision and that R@5 and
R@10 correlate well with their corresponding S@k metrics. This suggests that rankingbased metrics are significantly more robust than metrics that consider only the quality of
the first result. Moreover, these results indicate that our framework, in which systems are
expected to rank a pool of images or sentences written by people, may enable a large-scale,
fully automated, evaluation of image description systems that does not require an equally
large-scale effort to collect human judgments.

5. Summary of Contributions and Conclusions
In this paper, we have proposed to frame image description as the task of selecting or ranking
descriptions among a large pool of descriptions provided by people, because this framework
provides a direct test of the purely semantic aspects of image description and does not need
to be concerned with the difficulties involved in the automatic generation of syntactically
correct and pragmatically appropriate sentences. We have also introduced a new data set of
images paired with multiple captions, and have used this data set to evaluate a number of
nearest-neighbor and KCCA-based models on sentence-based image annotation as well as on
886

fiFraming Image Description as a Ranking Task

the converse task of sentence-based image search. Our experiments indicate the importance
of capturing lexical similarities. Finally, we have performed an in-depth analysis of different
evaluation metrics for image description.
5.1 The Advantages of Framing Image Description as a Ranking Task
One of our main motivations for framing image description as a ranking rather than a
generation problem was the question of an objective, comparable evaluation of our ability
to understand what is depicted in images. In order to make progress on this challenging
task, it is important to define tasks and evaluation metrics that allow for an objective
comparison of different approaches. We have argued that the task of ranking a pool of
captions written by people is attractive for a number of reasons: first, results obtained on
the same data set can be compared directly; second, human evaluation is easier than for
generated captions since it needs to only focus on factual correctness of the description rather
than grammaticality, fluency, or creativity; third, statistically significant differences between
systems may not become apparent when only a single caption per image is considered; and
finally, ranking makes it possible to automate evaluation, e.g. by considering the position of
the original caption. Moreover, framing image description as a ranking task also establishes
clear parallels to image retrieval, allowing the same metrics to be used for both tasks.
5.2 Our Data Set
Our Flickr 8k data set of over 8,000 images, each paired with five crowdsourced captions, is
a unique resource for image description. Although it is much smaller than the SBU corpus
(Ordonez et al., 2011), we believe that the generic conceptual descriptions in our corpus are
more useful for image understanding than the original Flickr captions in the SBU data set.
The data set that is perhaps most similar to ours is the IAPR data set (Grubinger et al.,
2006), but the captions in our corpus are shorter, and focus on the salient aspects of the
image. And while we focus on images of people and animals, the IAPR data set covers a
slightly different domain, including city pictures and landscape shots which typically do not
depict or focus on people. A distinct advantage of our corpus is that it pairs each image
with multiple, independently written captions. Our results indicate that using more than a
single caption at training time leads to a significant increase in performance. We have also
shown how to use these multiple captions to define an alignment-based lexical similarity that
may be more useful for image description than standard distributional or WordNet-based
similarities.
5.3 Our Models
This paper is the first to apply Kernel Canonical Correlation analysis (KCCA) to sentencebased image description. Our results show that KCCA significantly outperforms nearest
neighbor-based approaches on our data set of 6,000 training images and 1,000 test images
(although these may scale up better to very large data sets such as Ordonez et al.s (2011)
SBU corpus, where the memory requirements to train KCCA may be prohibitive). One
advantage of KCCA-based approaches over other image description systems that are geared
specifically towards caption generation is that they can not only be applied to image de-

887

fiHodosh, Young & Hockenmaier

scription, but also to image retrieval, and our results indicate that performance on both
tasks is fairly similar.
An important difference between our approach taken in this paper and most other image
description systems is that all the features used by the models presented here can be computed with minimal supervision. The only feature that relies on a supervised classifier is the
alignment-based similarity, which uses a POS-tagger to identify nouns and verbs. Despite
the simplicity of the underlying features, our models achieve relatively high performance,
considering the difficulty of the task: Although there is only a 1.5% chance that a randomly
chosen test caption will describe a test image well, fine-grained human judgments reveal
that for image annotation, the first caption returned by our best KCCA system is a good
description for 28% of the test images. Furthermore, our large-scale evaluation shows that
with our best system, there is an almost 50% chance that a suitable image or caption will be
returned among the first ten results. Our results indicate that there are two main reasons
for this high performance: the availability of multiple captions for each image at training
time, and the use of robust text representations that capture lexical similarities rather than
requiring strict equality between words. However, it is also clear that this task remains
far from being solved, and we leave the question of how KCCA may benefit from models
that rely on richer visual or linguistic features such as detector responses or rich syntactic
analyses for future work.
5.4 Evaluating Ranking-Based Image Description Systems
The main advantage of framing image description as a ranking problem is that it allows for a
direct comparison of different approaches, since they can be evaluated on the same data set.
It also makes it possible to borrow established evaluation metrics from information retrieval,
and to use the same metrics and data sets for sentence-based image annotation and image
search.
On the one hand, we have shown that crowdsourcing can be used to collect a large number
of binary judgments of image-caption pairs for a relatively low price, and that these crowdsourced judgments correlate well with more fine-grained judgments. Being able to collect
human judgments on a large scale is particularly important for retrieval-based approaches
to image description, since the number of relevance judgments that need to be collected
for a test collection may be significantly larger than the number of judgments commonly
used to evaluate a single caption generation system. However, our experiments on image
annotation have provided an example where human judgments of a first caption returned for
each test image did not reveal differences between systems that become apparent when more
results are taken into account. Our fine-grained evaluation also indicates that evaluations
that are based on a single result may require a potentially much larger number of test items
in order to reveal robust statistically significant differences. Among the human evaluation
metrics we have compared, we believe that R-precision computed over the crowdsourced
relevance judgments is the most robust. R-precision is a standard metric for evaluating
ranked retrieval results when items have a varying number of relevant responses, and since
it yields a single score, it also makes it particularly easy to compare systems. However, the
S@k scores, which measure the percentage of items for which the top k responses contain a
relevant result, are perhaps a more direct measure of how useful a system may be in prac-

888

fiFraming Image Description as a Ranking Task

tice. We will release the crowdsourced relevance judgments we have collected in order to
enable others to evaluate their image description system on our data. We hope that this will
establish a benchmark that can be used for a direct and fair large-scale comparison between
an arbitrary number of image description systems.
On the other hand, we have also shown that our framework in which systems are evaluated on their ability to rank a pool of images or sentences may make it possible to perform
a fully automated evaluation. Contrary to current practice, our analysis indicates clearly
that standard metrics such as Bleu or Rouge are not very reliable indicators for how
well captions describe images, even if Bleu with Rouge-style preprocessing can be used
as an effective filter of implausible image-caption pairs. Although we only consider humangenerated captions, we stipulate that similar observations may hold for automatically generated captions, since similar criticisms about Bleus appropriateness for generation and
machine translation evaluation are well known (Reiter & Belz, 2009; Callison-Burch, Osborne, & Koehn, 2006). However, in a ranking-based framework each test query is associated
with a gold response that it was originally associated with, and our results indicate that
metrics based on this rank of the gold item lead to very similar conclusions as human judgments. This suggest that the evaluation of the ranking-based image description task can be
automated, and performed on a potentially much larger scale than we have examined here.
5.5 Implications for the Evaluation of Caption Generation Systems
Image description can, and should, also be treated as a problem for the natural language
generation community. But automatically generating captions that are indistinguishable
from captions written by people (an evaluation criterion used by Mitchell et al. (2012)
for their comparison of caption generation systems) requires much more than the ability
to provide factually correct information about the image. We believe that the linguistic
issues that need to be solved in a generation setting need to be evaluated separately from
ability to decide whether a given caption describes an image. It is unclear that the kinds of
evaluations performed by e.g. Mitchell et al. could ever be automated, since the question of
how natural an automatically produced caption seems may always require human judgment.
But human experiments are expensive, and since each system generates its own captions,
such judgments have to be collected anew for each system and experiment. Since there is
no consensus on what constitutes a good image description, independently obtained human
assessments of different caption generation systems should not be compared directly. This
means that a direct comparison of systems, e.g. as performed by Mitchell et al., is typically
only possible within one research group, since there is no common data set for which different
system outputs are publicly available. Although automatic scores such as Bleu and Rouge
may still be useful for caption generation as measures of fluency (Reiter & Belz, 2009), we
have shown that they are not reliable metrics for how well a caption describes an image,
especially when the candidate pool is disjoint from the reference captions. This suggests
that the evaluation of the syntactic and pragmatic aspects of the caption generation task
should not be automated, and may have to rely on human judgments. However, it may
be possible to use the framework proposed in this paper to evaluate the semantic affinity
functions f (i, s) that are implicitly used in caption generations systems.

889

fiHodosh, Young & Hockenmaier

Acknowledgments
We gratefully acknowledge support for this project from the National Science Foundation
through IIS Medium grant 0803603, CAREER award 1053856, and CNS-1205627 CI-P.

Appendix A. Agreement Between Approximate Metrics and Expert
Human Judgments
Tables 8 and 9 use Cohens Kappa () to measure the agreement between Bleu and Rouge
scores and expert judgments. We have selected a few thresholds that yield optimal results.
Table 10 (a) shows the agreement between the crowdsourced judgments and the expert
judgments. Since the best agreement to the expert scores is obtained with the crowdsourced
judgments using a threshold of 0.6, Table 10 (b) measures precision and recall of the resulting binary relevance judgments against binarized expert judgments obtained with varying
thresholds.

Appendix B. Performance of All Systems
The following tables give results for all models. In Section 4 of the body of the paper, NN
idf
corresponds to NN5idf
F1 , while Tri5Sem corresponds to Tri5A,DBNC+ic .
R@k and median rank scores Table 11 gives the recall and median rank of the original
item (Section 4.3.1) for all of our models.
Agreement between expert and Bleu/Rouge scores (Cohens )
Case 1: Scand  Sref
5 reference captions/test image (Scand  Sref ; R5 )
Expert
E

0.9

Bleu B
0.8
0.7

0.4

Rouge R
0.3
0.2

=4.0
3.6
3.3
3.0
2.6

0.72
0.71
0.64
0.45
0.35

0.70
0.72
0.67
0.54
0.45

0.54
0.54
0.50
0.45
0.38

0.47
0.50
0.50
0.54
0.51

0.59
0.61
0.63
0.57
0.51

0.29
0.33
0.37
0.49
0.53

1 reference caption/test image (Scand = Sref ; R1 (gold))
Expert
E

 0.8

Bleu
 0.6  0.5

 0.9

=4.0
3.6
3.3
3.0
2.6

0.71
0.68
0.60
0.41
0.32

0.70
0.68
0.59
0.42
0.32

0.69
0.65
0.57
0.39
0.30

0.52
0.56
0.56
0.48
0.42

Rouge
 0.7  0.3
0.67
0.64
0.56
0.40
0.32

0.35
0.39
0.40
0.45
0.43

Table 8: Agreement (Cohens ) between binarized expert and Bleu/Rouge scores when
the pool of candidate captions contains each test images reference caption(s).

890

fiFraming Image Description as a Ranking Task

Agreement between expert and Bleu/Rouge scores (Cohens )
Case 2: Scand 6 Sref
4 reference captions/ test image (R4 )
Expert
E

 0.7

Bleu
 0.6  0.5

 0.4

Rouge
 0.3  0.2

=4.0
3.6
3.3
3.0
2.6

0.50
0.51
0.52
0.47
0.41

0.40
0.43
0.46
0.48
0.47

0.44
0.43
0.40
0.39
0.33

0.40
0.43
0.44
0.50
0.48

0.23
0.28
0.32
0.41
0.44

0.26
0.30
0.34
0.47
0.51

1 reference caption/test image (R1 (other))
Expert
E

 0.5

Bleu
 0.4  0.3

 0.4

Rouge
 0.3  0.2

=4.0
3.6
3.3
3.0
2.6

0.33
0.34
0.34
0.32
0.30

0.27
0.29
0.32
0.36
0.35

0.33
0.34
0.35
0.39
0.37

0.30
0.32
0.35
0.42
0.41

0.16
0.19
0.22
0.29
0.31

0.18
0.21
0.24
0.34
0.38

Table 9: Agreement (Cohens ) between binarized expert and Bleu/Rouge scores when
the pool of candidate captions may not contain each test images reference caption(s).

Agreement between expert
and lay scores (Cohens )
Expert
E
=4.0
3.6
3.3
3.0
2.6

=1.0
0.75
0.78
0.74
0.56
0.45

Lay vs. expert
relevance judgments (L = 0.66)

Lay L
 0.6  0.3
0.69
0.76
0.79
0.71
0.62

0.49
0.57
0.65
0.74
0.73

(a) Agreement (Cohens ) between relevance judgments obtained from expert
scores (relevance = score  E ) and lay
scores (relevance = score  L )

E

Precision

Recall

F1

=4.0
3.6
3.3
3.0
2.6
2.3

55.9
65.4
75.2
90.0
94.7
98.2

98.6
95.0
88.0
64.7
53.4
40.1

71.4
77.5
81.1
75.3
68.3
57.0

(b) Precision, recall, and F1 scores of binarized lay scores (L = 0.66) against
binarized expert scores with varying
thresholds E .

Table 10: Comparing the relevance judgments obtained from the lay scores against those
obtained from expert scores

891

fiHodosh, Young & Hockenmaier

S@k and R-precision scores Table 12 gives the S@k success rate (Section 4.3.3) and
R-precision scores (Section 4.3.3) for all of our models, based on the crowdsourced human
judgments (Section 4.3.2).

892

fiFraming Image Description as a Ranking Task

Performance of all models (automatic evaluation)
(R@k: percentage of queries with original item in top X results
Median r: median rank of original item)
R@1

Image annotation
R@5 R@10 Median r

R@1

Image search
R@5 R@10 Median r

NN5F1
NN5idf
F1
NN5BoW5
NN5Tri(best)

1.9
2.5
2.1
2.1

5.9
7.6
5.9
5.9

8.7
9.7
9.6
9.4

251.0
251.0
258.5
248.0

2.1
2.5
2.8
2.3

5.2
4.7
6.4
6.1

7.1
7.2
9.1
9.0

278.0
272.0
266.0
240.0

BoW1
Tri1

4.8
4.6

13.5
14.4

19.7
21.0

64.0
68.0

4.5
4.5

14.3
14.0

20.8
22.5

67.0
71.0

BoW5Histo
BoW5
BoW5idf

BoW5 idf
TagRank

5.9
6.2
6.1
6.1
6.0

14.9
17.1
17.0
17.3
17.0

21.2
24.3
23.2
23.9
23.8

69.0
58.0
60.5
56.0
56.0

4.8
5.8
6.4
6.1
5.4

14.2
16.7
16.5
16.9
17.4

20.8
23.6
24.5
24.5
24.3

74.0
60.0
59.5
60.5
52.5

Tri5Histo
Tri5

6.0
7.1

15.0
17.2

21.7
23.7

63.5
53.0

5.7
6.0

14.5
17.8

22.1
26.2

67.0
55.0

Tri5Lin
Tri5DBNC
Tri5Dic
Tri5DBNC+ic

6.2
7.5
7.0
7.3

16.7
19.8
19.5
20.0

23.7
26.1
27.1
27.0

53.5
40.0
36.0
36.0

6.0
7.2
7.0
6.9

16.7
18.4
19.3
19.2

24.4
27.4
27.5
28.0

61.0
44.5
41.0
42.0

Tri5A
Tri5A,DBNC
Tri5A,Dic
Tri5A,DBNC+ic

7.2
7.9
6.9
7.6

20.2
20.3
20.2
20.7

28.0
28.4
29.5
30.0

41.0
39.0
35.0
35.0

6.8
7.8
7.3
7.4

18.5
19.0
19.9
19.4

27.7
27.4
28.7
29.2

41.5
39.0
39.5
38.0

7.6

18.8

25.1

46.0

6.2

18.0

26.5

52.0

6.8
7.3
6.7

18.7
20.4
20.0

28.9
27.5
28.9

40.0
38.0
35.0

6.7
8.1
7.0

18.2
19.1
19.7

27.6
28.4
28.8

45.0
40.5
39.0

7.3
7.5
7.2

21.1
21.3
20.8

28.3
30.0
29.6

37.0
38.0
34.0

7.5
7.8
7.4

19.2
18.9
21.4

28.7
29.0
30.1

38.0
41.5
37.5

idf,Histo
Tri5A,D
BNC+ic

6.5

18.0

26.7

45.0

6.0

18.0

24.2

48.5

idf
A,DBNC+ic

8.3

21.6

30.3

34.0

7.6

20.7

30.1

38.0



Tri5

idf



idf
Tri5
DBNC
idf
Tri5
Dic
idf
Tri5DBNC+ic


idf
Tri5A

idf
Tri5
A,DBNC
idf
Tri5A,D
ic



Tri5

Table 11: Performance of all models, measured as the percentage of test items for which the
original item was returned among the top 1, 5 or 10 results,
as well as as the median rank

idf
idf
of the original item. In Section 4, NN5F1 = NN, Tri5A,DBNC+ic = Tri5Sem.

893

fiHodosh, Young & Hockenmaier

Performance of all models (human evaluation)
S@k: Percentage of items with relevant response among top X results
R-prec: R-precision computed over relevant responses
S@1
NN5F1
NN5idf
F1
NN5BoW5
NN5Tri(best)

Image annotation
S@5 S@10 R-prec.

S@1

Image search
S@5 S@10 R-prec.

4.9
5.8
6.4
7.2

13.3
15.4
14.8
17.4

19.1
20.2
20.6
23.1

4.2
5.2
5.4
6.2

4.9
5.0
5.7
4.4

13.2
13.3
13.4
13.5

17.8
18.4
18.4
19.8

3.8
3.8
4.6
4.3

BoW1
Tri1

12.2
12.8

30.3
32.2

39.7
40.2

10.7
10.5

11.4
12.2

30.5
30.6

40.2
41.5

9.6
9.9

BoW5Histo
BoW5
BoW5idf

BoW5 idf
TagRank

13.9
15.0
13.9
15.0
16.2

29.8
34.1
32.7
34.0
34.2

39.6
42.7
42.4
42.7
42.9

9.9
11.1
11.0
11.3
11.7

11.5
12.1
13.3
12.9
12.4

28.0
31.5
30.8
31.3
31.5

38.1
40.8
41.8
41.0
41.6

9.3
10.5
10.6
10.7
10.5

Tri5Histo
Tri5

15.0
16.4

29.0
32.9

38.9
43.4

9.9
11.6

12.9
13.1

28.9
33.1

39.9
43.8

10.5
11.0

Tri5Lin
Tri5DBNC
Tri5Dic
Tri5DBNC+ic
Tri5A
Tri5A,DBNC
Tri5A,Dic
Tri5A,DBNC+ic

15.5
16.8
15.8
16.4
17.3
16.6
15.8
16.4

34.1
37.4
36.7
37.2
36.9
36.5
37.0
37.4

43.8
45.5
47.0
47.1
47.4
47.4
48.2
48.3

11.7
12.7
12.7
12.5
13.4
13.2
13.0
13.4

12.7
14.5
14.5
14.8
14.3
15.3
15.2
15.4

32.5
35.3
36.6
36.3
35.4
35.0
37.4
37.0

41.7
44.9
46.1
45.8
46.6
45.8
47.6
46.8

10.7
12.1
12.8
12.7
12.3
12.8
12.8
13.2

16.9

35.4

44.2

12.5

13.0

33.4

43.9

11.3

16.1
15.9
16.2

36.0
36.9
37.4

47.5
46.8
47.5

12.9
12.8
13.3

15.0
16.0
15.3

34.0
35.8
34.8

44.6
47.5
46.7

12.2
13.1
13.0

17.4
15.7
15.7

37.6
36.9
37.3

46.3
48.1
47.3

13.4
12.9
13.3

15.8
15.8
15.5

36.3
35.3
38.3

47.3
47.2
47.8

13.2
12.9
13.4

13.6

30.6

41.9

11.1

14.4

33.8

42.7

12.2

16.6

37.7

49.1

13.7

15.7

36.9

48.5

13.4

Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5
Tri5


idf

idf
D
BNC
idf
D
ic
idf
DBNC+ic

idf
A

idf
A,D
 BNC
idf
A,Dic

idf,Histo
A,DBNC+ic

idf
A,DBNC+ic

Table 12: Performance of all models, measured as the percentage of test items for which
they return an item that was deemed relevant according to the crowdsourced judgments
among the top 1, 5 or 10 results,
and as R-precision computed over these judgments. In

idf
idf
Section 4, NN5F1 = NN, Tri5A,DBNC+ic = Tri5Sem.

894

fiFraming Image Description as a Ranking Task

References
Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics.
Computational Linguistics, 34 (4), 555596.
Bach, F. R., & Jordan, M. I. (2002). Kernel independent component analysis. Journal of
Machine Learning Research, 3, 148.
Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. D., Blei, D. M., & Jordan, M. I. (2003).
Matching words and pictures. Journal of Machine Learning Research, 3, 11071135.
Blei, D. M., & Jordan, M. I. (2003). Modeling annotated data. In SIGIR 2003: Proceedings of
the 26th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 127134, Toronto, Ontario, Canada.
Bloehdorn, S., Basili, R., Cammisa, M., & Moschitti, A. (2006). Semantic kernels for text
classification based on topological measures of feature similarity. In Proceedings of the
6th IEEE International Conference on Data Mining (ICDM 2006), pp. 808812, Hong
Kong, China.
BNC Consortium (2007). The British National Corpus, version 3 (BNC XML edition).
http://www.natcorp.ox.ac.uk.
Brown, P. F., Pietra, V. J. D., Pietra, S. A. D., & Mercer, R. L. (1993). The mathematics
of statistical machine translation: parameter estimation. Computational Linguistics,
19 (2), 263311.
Callison-Burch, C., Osborne, M., & Koehn, P. (2006). Re-evaluation the role of bleu in
machine translation research. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pp. 249256,
Trento, Italy.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20 (1), 3746.
Croce, D., Moschitti, A., & Basili, R. (2011). Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 10341046, Edinburgh, UK.
Dale, R., & White, M. (Eds.). (2007). Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation: Position Papers, Arlington, VA, USA.
Datta, R., Joshi, D., Li, J., & Wang, J. Z. (2008). Image retrieval: Ideas, influences, and
trends of the new age. ACM Computing Surveys, 40 (2), 5:15:60.
Deschacht, K., & Moens, M.-F. (2007). Text analysis for automatic image annotation. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pp. 10001007, Prague, Czech Republic.
Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification
learning algorithms. Neural Computation, 10 (7), 18951923.
Everingham, M., Gool, L. V., Williams, C., Winn, J., & Zisserman, A. (2008). The
PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results. http://www.
pascal-network.org/challenges/VOC/voc2008/workshop/.
895

fiHodosh, Young & Hockenmaier

Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells a story: Generating sentences from images. In
Proceedings of the European Conference on Computer Vision (ECCV), Part IV, pp.
1529, Heraklion, Greece.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Bradford Books.
Felzenszwalb, P., McAllester, D., & Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 18, Anchorage, AK, USA.
Feng, Y., & Lapata, M. (2008). Automatic image annotation using auxiliary text information. In Proceedings of the 46th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-08: HLT), pp. 272280, Columbus,
OH, USA.
Feng, Y., & Lapata, M. (2010). How many words is a picture worth? automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 12391249, Uppsala, Sweden.
Fisher, R. A. (1935). The Design of Experiments. Olyver and Boyd, Edinburgh, UK.
Grangier, D., & Bengio, S. (2008). A discriminative kernel-based approach to rank images
from text queries. IEEE Transactions on Pattern Analysis and Machine Intelligence,
30, 13711384.
Grice, H. P. (1975). Logic and conversation. In Davidson, D., & Harman, G. H. (Eds.), The
Logic of Grammar, pp. 6475. Dickenson Publishing Co., Encino, CA, USA.
Grubinger, M., Clough, P., Mller, H., & Deselaers, T. (2006). The IAPR benchmark: A new
evaluation resource for visual information systems. In OntoImage 2006, Workshop on
Language Resources for Content-based Image Retrieval during LREC 2006, pp. 1323,
Genoa, Italy.
Gupta, A., Verma, Y., & Jawahar, C. (2012). Choosing linguistics over vision to describe
images. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,
Toronto, Ontario, Canada.
Hardoon, D. R., Saunders, C., Szedmak, S., & Shawe-Taylor, J. (2006). A correlation approach for automatic image annotation. In Li, X., Zaane, O. R., & Li, Z.-H. (Eds.),
Advanced Data Mining and Applications, Vol. 4093 of Lecture Notes in Computer Science, pp. 681692. Springer Berlin Heidelberg.
Hardoon, D. R., Szedmak, S. R., & Shawe-Taylor, J. R. (2004). Canonical correlation
analysis: An overview with application to learning methods. Neural Computation, 16,
26392664.
Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28 (3/4), 321377.
Hwang, S., & Grauman, K. (2012). Learning the relative importance of objects from tagged
images for retrieval and cross-modal search. International Journal of Computer Vision,
100 (2), 134153.

896

fiFraming Image Description as a Ranking Task

Jaimes, A., Jaimes, R., & Chang, S.-F. (2000). A conceptual framework for indexing visual
information at multiple levels. In Internet Imaging 2000, Vol. 3964 of Proceedings of
SPIE, pp. 215, San Jose, CA, USA.
Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing (2nd edition). Prentice
Hall.
Krippendorff, K. (2004). Content analysis: An introduction to its methodology. Sage.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).
Baby talk: Understanding and generating simple image descriptions. In Proceedings
of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 16011608.
Kuznetsova, P., Ordonez, V., Berg, A., Berg, T., & Choi, Y. (2012). Collective generation
of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 359368, Jeju
Island, Korea.
Lavrenko, V., Manmatha, R., & Jeon, J. (2004). A model for learning the semantics of pictures. In Thrun, S., Saul, L., & Schlkopf, B. (Eds.), Advances in Neural Information
Processing Systems 16, Cambridge, MA, USA.
Lazebnik, S., Schmid, C., & Ponce, J. (2009). Spatial pyramid matching. In S. Dickinson,
A. Leonardis, B. S., & Tarr, M. (Eds.), Object Categorization: Computer and Human
Vision Perspectives, chap. 21, pp. 401415. Cambridge University Press.
Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning (CoNLL), pp. 220228, Portland, OR,
USA.
Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In MarieFrancine Moens, S. S. (Ed.), Text Summarization Branches Out: Proceedings of the
ACL-04 Workshop, pp. 7481, Barcelona, Spain.
Lin, C.-Y., & Hovy, E. H. (2003). Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
(HLT-NAACL), pp. 7178, Edmonton, AB, Canada.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML), pp. 296304, Madison,
WI, USA.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. Internationa
Journal of Computer Vision, 60 (2), 91110.
Makadia, A., Pavlovic, V., & Kumar, S. (2010). Baselines for image annotation. International
Journal of Computer Vision, 90 (1), 88105.
Manning, C. D., Raghavan, P., & Schtze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press.

897

fiHodosh, Young & Hockenmaier

Mitchell, M., Dodge, J., Goyal, A., Yamaguchi, K., Stratos, K., Han, X., Mensch, A., Berg,
A., Berg, T., & Daume III, H. (2012). Midge: Generating image descriptions from
computer vision detections. In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguistics (EACL), pp. 747756, Avignon, France.
Moschitti, A. (2009). Syntactic and semantic kernels for short text pair categorization. In
Proceedings of the 12th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), pp. 576584, Athens, Greece.
Moschitti, A., Pighin, D., & Basili, R. (2008). Tree kernels for semantic role labeling.
Computational Linguistics, 34 (2), 193224.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Information Processing Systems 24,
pp. 11431151.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic
evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 311318, Philadelphia, PA, USA.
Popescu, A., Tsikrika, T., & Kludas, J. (2010). Overview of the Wikipedia retrieval task at
ImageCLEF 2010. In CLEF (Notebook Papers/LABs/Workshops), Padua, Italy.
Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14 (3), 130137.
Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using Amazons Mechanical Turk. In NAACL Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk, pp. 139147, Los Angeles, CA,
USA.
Rasiwasia, N., Pereira, J. C., Coviello, E., Doyle, G., Lanckriet, G. R., Levy, R., & Vasconcelos, N. (2010). A new approach to cross-modal multimedia retrieval. In Proceedings
of the International Conference on Multimedia (MM), pp. 251260, New York, NY,
USA.
Reiter, E., & Belz, A. (2009). An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics,
35 (4), 529558.
Shatford, S. (1986). Analyzing the subject of a picture: A theoretical approach. Cataloging
& Classification Quarterly, 6, 3962.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge
University Press.
Smucker, M. D., Allan, J., & Carterette, B. (2007). A comparison of statistical significance
tests for information retrieval evaluation. In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM), pp. 623632, Lisbon,
Portugal.

898

fiFraming Image Description as a Ranking Task

Socher, R., & Li, F.-F. (2010). Connecting modalities: Semi-supervised segmentation and
annotation of images using unaligned text corpora. In Proceedings of the 2010 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 966973, San
Francisco, CA, USA.
van Erp, M., & Schomaker, L. (2000). Variants of the Borda count method for combining
ranked classifier hypotheses. In Proceedings of the Seventh International Workshop on
Frontiers in Handwriting Recognition (IWFHR), pp. 443452, Nijmegen, Netherlands.
Varma, M., & Zisserman, A. (2005). A statistical approach to texture classification from
single images. International Journal of Computer Vision, 62, 6181.
Vedaldi, A., & Fulkerson, B. (2008). VLFeat: An open and portable library of computer
vision algorithms. http://www.vlfeat.org/.
Weston, J., Bengio, S., & Usunier, N. (2010). Large scale image annotation: learning to rank
with joint word-image embeddings. Machine Learning, 81 (1), 2135.
Yang, Y., Teo, C., Daume III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 444454, Edinburgh, UK.

899

fiJournal of Artificial Intelligence Research 47 (2013) 313-349

Submitted 03/13; published 06/13

Learning by Observation of Agent Software Images
Paulo Costa
Luis Botelho

paulo.costa@iscte.pt
luis.botelho@iscte.pt

Instituto de Telecomunicacoes
ISCTE-Instituto Universitario de Lisboa
Avenida das Forcas Armadas, 1649-026 Lisbon, Portugal.

Abstract
Learning by observation can be of key importance whenever agents sharing similar
features want to learn from each other. This paper presents an agent architecture that
enables software agents to learn by direct observation of the actions executed by expert
agents while they are performing a task. This is possible because the proposed architecture
displays information that is essential for observation, making it possible for software agents
to observe each other.
The agent architecture supports a learning process that covers all aspects of learning
by observation, such as discovering and observing experts, learning from the observed
data, applying the acquired knowledge and evaluating the agents progress. The evaluation
provides control over the decision to obtain new knowledge or apply the acquired knowledge
to new problems.
We combine two methods for learning from the observed information. The first one, the
recall method, uses the sequence on which the actions were observed to solve new problems.
The second one, the classification method, categorizes the information in the observed data
and determines to which set of categories the new problems belong.
Results show that agents are able to learn in conditions where common supervised
learning algorithms fail, such as when agents do not know the results of their actions a
priori or when not all the effects of the actions are visible. The results also show that
our approach provides better results than other learning methods since it requires shorter
learning periods.

1. Introduction
This paper describes the important aspects of our approach for enabling software agents
to learn control mechanisms by directly observing the actions of an expert agent while it
is performing a task. It shows the innovations of our proposal for an agent software image
(Costa & Botelho, 2011) and presents a complete learning by observation process, following
previous work on this subject (Costa & Botelho, 2012). The paper also presents the results
of our approach in two different scenarios (see section 5). The learning method used by our
approach is usually known in the artificial intelligence community as learning by observation,
imitation learning, learning from demonstration, programming by demonstration, learning
by watching or learning by showing. For consistency, learning by observation will be used
from here on.
Learning by observation is one of the least common and most complex forms of learning amongst animals because it is singular to humans and to a strict number of superior
mammals. It is also one of the most powerful socialisation mechanisms (Ramachandran,
c
2013
AI Access Foundation. All rights reserved.

fiCosta & Botelho

2003; Bandura, 1977; Meunier, Monfardini, & Boussaoud, 2007). Research in neurology
and psychology shows that learning by observation may well be one of the causes of the
exponential growth of human technologies in the last centuries. Unlike natural selection, observation allows these capabilities to spread amongst individuals within the same generation
(Ramachandran, 2000).
Section 2 shows that learning by observation is already being used in robotic agents,
which proves the applicability of this learning technique in artificial systems. However,
progress in learning by observation is limited to robotics because software agents cannot
observe one another in the same way tangible entities can be observed. Learning by observation is useful for software agents because it provides a more direct approach to the problem
to solve when compared with other techniques where agents learn from experience, such as
reinforcement learning. Instead of spending time testing several hypotheses, artificial agents
acquire the knowledge from an expert by directly observing its actions while it performs
a task. This allows the artificial agents to directly know which actions are necessary to
perform a specific task (Argall, Chernova, Veloso, & Browning, 2009; Chernova, 2009).
Being able to observe an expert agent performing its actions (as opposed to merely rely
on the observation of their effects) can be advantageous in situations where the effects of
those actions are not directly visible in the environment (for example, agent communication,
manipulation of software objects). Even when part of the effect of the actions is visible,
directly observing the actions is still advantageous when the same effects could be achieved
by different alternative actions but using one of them is clearly better than using others
(for example, using a set of sums instead of a simple multiplication). Observing the actions
performed by an expert is also advantageous when the representation of world states requires
too much memory making it impossible to build large enough training sets (for example,
web agents, manipulation of large databases) and especially when the agent does not know
the effects of its actions a priori (for example, executing or invoking a software program or
API).
The application of learning by observation in software agents is ideal for societies where
agents share common features but have their own internal representation methods (for
example, integration of legacy systems). Without common internal representation methods, directly transferring knowledge between agents is impossible or, at least very difficult.
Learning by observation makes it possible to learn without the need of common internal
representations because each agent makes its own interpretations of what it is observing.
The advantages described above provided the motivation for developing our approach to
learning by observation in software agents. The major contribution of our approach is the
definition of the whole learning process, which includes the discovery, observation, storage
and interpretation of the observed data, the application of the acquired knowledge and a
continuous internal evaluation (see section 4). Agents that learn by observation are capable
of solving tasks in the same conditions as they were performed by the observed experts.
They are also capable of performing a similar task when facing different conditions. The
scenarios described in section 5 show these possibilities.
As section 2 shows, to the best of our knowledge, the only approach to learning by observation in software agents is the one presented by Machado and Botelho (2006). However,
in Machado and Botelhos proposal the agents were only capable of learning vocabulary
whereas in the work described in this paper the agents are capable of learning control
314

fiLearning by Observation of Agent Software Images

mechanisms, which requires developing a different kind of learning algorithm. In addition,
our approach also introduces several improvements to the software image (see section 3)
initially proposed by Machado and Botelho (2006).
The software image provides software agents with an accessible representation of their
constituents and capabilities, the static image, and of the actions they perform, the dynamic
image. Our approach heavily relies on an agent architecture with software image. Agents
with software image can be seen (consulted) by other agents and even by themselves. It is
the software image that allows software agents to learn by observation because, in software
environments, the information required for learning by observation is not automatically
visible in the agents body as it happens in the physical world (Quick, Dautenhahn, Nehaniv,
& Roberts, 2000). To avoid misconceptions with the observation of tangible entities in the
physical world, the act of observing a software agent will be defined as:
Reading meta-data about an agents constituents, its actions and the conditions holding
when the actions were executed, without a direct intervention of the observed agent.

This allows the observed agent (the expert) to have only a passive role in the observation process. As with learning by observation in humans and superior mammals, it is the
apprentice who takes action to obtain new knowledge, and does it without interfering with
the expert agent. In spite of the similarities with Machado and Botelhos (2006) approach,
our approach introduced several improvements to the software image, in particular:
 The inclusion of the agent sensors in the static image, in addition to the visible
attributes, the actuators and the actions from Machado and Botelhos approach. This
improvement provides a better description of the agent and allows other agents to
know the way that agent understands the world, as explained in section 3.1.
 The dynamic image, besides displaying the executed action as in Machado and Botelhos
approach, also displays information on the conditions holding before the action was
executed, that is, the state of the environment, as perceived by the agent sensors,
and the instances of the visible attributes. This improvement enables our agents to
provide data for observation that is similar to the training sequences used for training
machine learning algorithms (condition-action pairs), as explained in section 3.1.
 Our software image displays historical information about the actions executed in the
past and about the conditions holding before each action was executed. This enables
our agents to acquire a great amount of knowledge at the beginning of the observation,
as section 3.2 shows.
 Our software image also uses ontologies to hold the knowledge on the designations of
the different kinds of sensors, visible attributes, actions and tasks. This improvement
allows agents that follow the same ontology to use the same designations for the same
kinds of sensors, visible attributes, actions and tasks, as section 3.2 shows.
The contributions of our approach are not restricted to the agent software image. The
other important contributions are the complete approach to learning by observation, in
particular:
315

fiCosta & Botelho

 The discovery of experts which provides the agent with the necessary tools to discover,
by itself, expert agents from which it is possible to learn (see section 4.1).
 The definition of two learning algorithms, which are used to convert the observed
information into mechanisms for choosing the actions to perform in each condition
- the recall algorithm and the classification algorithm. The recall algorithm was
totally developed for our approach and uses the sequence on which the expert actions
were observed to choose the actions to perform. The classification algorithm is an
adaptation of the existing KStar algorithm (Cleary & Trigg, 1995). It categorizes the
information in the observed data and determines which actions to perform according
to the categories of the new problems (see section 4.2).
 The definition of an internal evaluation which provides the agent with a measure of
the confidence on its knowledge. Depending on this confidence, the agent can be in
one of two states of the learning process: the learning state or the execution state (see
section 4). When agents are in the learning state, their only objective is to observe experts and acquire new knowledge from them. When agents are in the execution state,
their only objective is to perform their task using the acquired knowledge. Switching
between these two states depends on two configurable thresholds, the UpperConfidenceThreshold and the LowerConfidenceThreshold, as explained in section
4.3.
 The ability to mimic the mirror neurons, which allows the agent to use the same mechanisms to propose actions both when learning and when using the acquired knowledge.
This allows the agent to directly associate the observed actions with its own actions.
It also allows the agent to propose actions for the conditions faced by the observed
expert and determine if the agent is capable of proposing the same action as the
observed expert.
The motivation that drives an agent to observe an expert is only partially covered by
our approach. For simplification purposes, all apprentice agents have the observation of
experts as their top priority. The apprentice agents are equipped with a specialized sensor
that focuses their attention on observing an expert. The internal evaluation allows agents
to decide whether or not they need more knowledge, providing them with the necessary
motivation for observing experts, the only way they know how to obtain new knowledge.
Another simplification relates with determining if the experts to observe are performing
relevant actions for the task to learn, which is also usually disregarded by the approaches for
learning by observation. In normal circumstances, although an expert has the same features
as the apprentice, it does not necessarily mean that it is performing the actions that are
necessary for the apprentice to learn (by observation) how to perform a specific task. Like
in all the surveyed approaches, the experts developed for the application scenarios in section
5 are prepared to only execute the actions that are necessary for the task to be learnt.
In section 5 the capabilities of our learning approach are tested in two different scenarios.
The first scenario was especially conceived as a situation in which the majority of the effects
of agent actions are not visible in the environment. Therefore, it will not be possible for
a common machine learning solution to learn only from the effects of the actions. For a
316

fiLearning by Observation of Agent Software Images

complete understanding of what is happening, it is necessary to observe the agent and its
actions. The second scenario presents the mountain car problem as described by Sutton and
Barto (1998). It is used to compare our learning approach with a reinforcement learning
(RL) algorithm in a situation in which this kind of learning algorithm has already shown
to be a good approach (Mitchell, 1997).
The results from the tests show that, with our approach, agents can correctly learn how
to perform a task when the majority of the effects of agent actions are not visible in the
environment (see section 5.1). In addition, when tested in situations where other learning
methods may provide good results (see section 5.2), such as in a reinforcement learning
scenario, results show that our approach is able to learn faster than the reinforcement
learning approach. Besides learning faster, the agents using our learning method also require
fewer actions to achieve the goal.
The following section presents a survey on research on the visual representation of
agents and on learning by observation. Section 3 presents the improvements we have made
on previous proposals regarding the software image. Section 4 describes the important
aspects of the learning by observation process. Section 5 describes the test scenarios and
experimental results. Finally section 6 presents conclusions and future work.

2. Literature Review
This section presents a survey of the approaches for learning by observation and for the
visual representation of agents. It describes the important aspects of approaches related to
learning by observation or that may contribute to solve the problems faced when applying
learning by observation in software agents.
2.1 The Visible Representation of Software Agents
The literature overview regarding learning by observation shows that, with the exception of
Machado and Botelhos (2006) approach, software agents are disregarded from the advances
on learning by observation since they are usually related to robotics (Argall et al., 2009).
Software agents are not able to distinguish themselves or other software agents from the
remaining elements of a computer program because of the disembodied nature of software.
Because of this, software agents are not able to observe each other as tangible entities would
be observed (Etzioni, 1993; Quick et al., 2000).
Almost all software approaches for learning are constrained to observe the changes in the
environment (the effects of agent actions) and the knowledge obtained to perform a task is
limited to state change information (Quick et al., 2000; Argall et al., 2009). However, several
authors have emphasized that not every action produces visible changes in the environment
(Byrne, 1999; Dautenhahn & Nehaniv, 2002; Botelho & Figueiredo, 2004; Machado, 2006),
thus, using state change information alone is not always a good option. In such cases, it
is important to actually see what the expert agent is doing by observing its actions in
addition to their effects. This allows the apprentice agents to know exactly what actions are
necessary to perform a task, thus overcoming the problems that arise when the effects of
those actions are not visible in the environment or when the agent does not know the effects
317

fiCosta & Botelho

of its actions a priori (Byrne, 1999; Dautenhahn & Nehaniv, 2002; Botelho & Figueiredo,
2004; Machado, 2006).
To be able to learn by observing the actions of other agents, the software agent needs
some kind of accessible representation of its body that displays the necessary visible features (Mataric, 1997; Botelho & Figueiredo, 2004; Machado, 2006). Research in neurology
reveals that the human brain also uses a representation of the human body in its activities
(Ramachandran, 2003). This representation provides information on the body constituents
and their possible actions and comes into existence in the initial stages of infant development. It is an important factor for learning by observation, since it allows children to
acknowledge their bodies and capabilities. It also provides the ability to identify entities
that are similar to them and thus may be worthwhile observing (Rao, Shon, & Meltzoff,
2004).
Despite these neurological findings, the literature review on embodiment and embodied
cognition shows that, besides our approach (Costa & Botelho, 2011), only another one
addresses the problem of creating such kind of accessible representation for software agents
(Machado & Botelho, 2006). To the best of our knowledge, these are the only known
approaches with a concrete proposal deploying a visible image for software agents, which is
called the visible software image or simply the software image. The literature review on
learning by observation also provides no alternatives since all approaches, with the exception
of the referred ones, apply exclusively to robotics (Argall et al., 2009).
Although Machado and Botelhos (2006) approach for the software image provides a
description of the agent constituents and actions, it does not describe the kind of input the
agent can collect from the software environment. Etzioni (1993) was one of the first authors
to realize that the lack of a physical body was not an obstacle for the application of the
principles of embodiment in software agents in the same way they are applied in robotics.
For Etzioni, a software agent can be situated in the software environment in the same way
as a robot is situated in the physical world, if it is characterized by what it can do (its
actions) and also by the kind of inputs it is able to collect from the software environment
(Etzioni, 1993).
For this reason, in our approach (Costa & Botelho, 2011), the software image provides
software agents with a visible representation of their constituents, which includes their
sensing and action capabilities, of the actions executed by the agent and the conditions
holding when the agent decided to execute those actions. Our proposal also keeps an
historical record of the actions performed by the agent and the conditions holding when the
agent decided to execute those actions, for a limited amount of time.
2.2 Learning by Observation
The survey on learning by observation shows that one of the most important aspects of an
approach to learning by observation is the learning algorithm. It defines how the knowledge,
obtained from observation, is stored and how it is used, that is, how the agent proposes
actions to execute when facing new problems (Argall et al., 2009). One possibility for the
learning algorithm is to follow the same sequence of actions as the expert, which requires the
agent to store the sequence on which it has observed the actions performed by the expert.
318

fiLearning by Observation of Agent Software Images

This possibility for the learning algorithm, named sequencing, is one of the most commonly used in learning by observation approaches (Argall et al., 2009). It is also closely
related to sequence learning in humans since it handles the same kind of problems, such as
predicting the elements of a sequence based on the preceding element, finding the natural
order of the elements in a sequence and selecting a sequence of actions to achieve a goal
(Clegg, DiGirolamo, & Keele, 1998; Sun, 2001). The best way to maintain the temporal
relations between the elements in the sequence consists of using the properties of the data
structure where the sequences are stored (Byrne, 1999; Heyes & Ray, 2000; Kulic, Ott, Lee,
Ishikawa, & Nakamura, 2011; Billing, Hellstrom, & Janlert, 2011).
Linear structures such as lists and vectors are the most commonly used (Byrne, 1999;
Heyes & Ray, 2000). However these linear structures lack the ability to represent alternatives, which is an important aspect of sequential learning that opens the possibility of
making choices inside the sequence (Sun, 2001). The representation of alternative paths is
essential for representing different approaches to perform the same task, that is, when the
same objective can be reached by different sequences of actions. To hold this information,
each different approach needs to be stored as an alternative sequence of actions. Tree structures are ideal for these situations. Each element of the sequence is represented as a node
in the tree and the following element is chosen from one of the branches of that node (Kulic
et al., 2011).
Sequencing is best suited for situations where agents face the same conditions (the same
sequence of states of the environment and internal states) as the observed experts, which
often means that the agent is following the expert or that it is possible to cover all the
possible combinations of the conditions in the time the agent is observing.
Other possibilities for the learning algorithm are to generalize the acquired knowledge
or to use analogies between the acquired knowledge and the new problems. This allows
the agent to face future conditions that have never been observed, because in a real world
situation it is almost always impossible to observe all possible conditions (Argall et al., 2009;
Sullivan, 2011). Unlike in sequencing, there is no specific sequence of actions to follow. The
agent determines what actions it should perform supported exclusively by the conditions.
One way of generalizing the acquired knowledge is using the observed conditions and
actions to train neural networks, as described by Billard and Hayes (1999). Other hypothesis
consist of using the observed conditions and actions to feed statistical approaches such as
Bayesian algorithms and Hidden Markov models (HMMs) (Rao et al., 2004; Hajimirsadeghi
& Ahmadabadi, 2010). The conditions and actions can also be used to train supervised
learning algorithms, such as the classification algorithms (Argall et al., 2009; Chernova,
2009; Sullivan, 2011).
Given the advantages of the sequencing and of the generalization or analogy possibilities our approach to learning by observation presents a sequencing possibility, through the
recall method of learning, and classification possibility, trough the classification method of
learning. The two methods of learning are combined to increase the adaptability of the
apprentice agents. The classification method allows the agent to extend its knowledge to
conditions that have not been observed and recall method allows the agent to easily learn
sequences of actions with different alternatives.
The survey on learning by observation also reveals that a learning by observation approach cannot be limited to the learning algorithm. In addition to the algorithm, it must
319

fiCosta & Botelho

also include the agents motivation to learn, the discovery and observation of agents, the
storage and interpretation of the information acquired in observation and the application
of the newly acquired knowledge (Demiris & Hayes, 2002; Tan, 2012). One of the major
flaws detected on the surveyed approaches, besides their focus on robot agents, was the fact
that this global view is still missing (Tan, 2012). To the best of our knowledge, with the
exception of our approach (Costa & Botelho, 2012), all approaches are focused on solving
specific problems, and the solutions they provide are supported exclusively by the learning
algorithm.
Demiris and Hayes (2002) provide a good starting point for building an approach that
includes all aspects of learning by observation. Their approach presents a learning process
that, excluding motivation, is consistent with Banduras (1977) social learning theory, which
approximates their approach to learning by observation in humans and superior mammals.
The inclusion of an internal evaluation to Demiris and Hayess (2002) approach provides
the learning process with a simple motivation mechanism. The evaluation allows the agent
to measure how its performance is affected both when it is consolidating the knowledge
acquired from observation or when executing actions (Wood, 2008; Hajimirsadeghi & Ahmadabadi, 2010).
The agent can be intrinsically motivated to learn because it knows when it has acquired
sufficient knowledge to perform a task on its own or because it detects that portions of
its knowledge need improvement and thus require the agent to go back learning (Wood,
2008; Billing, Hellstrom, & Janlert, 2010). The ability to enhance the agents knowledge
through new observations is an important factor for learning by observation. According to
Argall et al. (2009), one of the downsides of learning by observation is the fact that the
agents knowledge is limited to what it was able to observe. Using an evaluation stage,
that operates when the agent is learning and when it is executing actions, provides the
knowledge on when it is necessary to observe experts and when agents are ready to execute
actions.
Several authors use specialized experts, or teachers, who monitor and reinforce the
agents actions (Sullivan, 2011; Hajimirsadeghi & Ahmadabadi, 2010; Chernova, 2009).
They also measure the agents performance and provide the necessary evaluation. Besides
monitoring, the teachers can also take corrective measures like providing the appropriate
actions for the faced conditions when the agent chooses incorrect actions (Hajimirsadeghi
& Ahmadabadi, 2010). The teacher can also decide when the agent needs to acquire more
knowledge (Sullivan, 2011).
The relation between the teacher and the agent can be extended by allowing them to
communicate with each other. This allows the agent to request the teacher to perform
a specific task (Chernova, 2009). However, this requires an additional effort in designing
the expert agents since they need to communicate with the apprentice agents and teach
them how to perform a task. This also requires apprentice agents to wait for the teacher
to be available to communicate with them, which can extend the amount of time spent on
learning. This does not happen when the expert plays a passive role in observation and, in
addition, using teachers makes the approaches closer to learning by teaching, which goes
beyond learning by observation.
A different approach for evaluation is mimicking the properties of mirror neurons. The
mirror neurons are brain structures that exist in humans and superior mammals which are
320

fiLearning by Observation of Agent Software Images

responsible for the emergence of learning by observation. They are involved in the tight
coupling of perception and motor control, providing similar responses both when observing
and when performing an activity. This allows the agents to feel like they are performing
the actions they observe on an expert (Ramachandran, 2000), which greatly improves the
easiness of identifying the observed actions, grounding them in the agent own actions.
Through this ability, the agent is able to propose actions, using its own mechanisms,
for what it is observing, without effectively executing them. The proposed actions can be
compared with those observed to determine if the agent is able to propose the same actions
as the expert. The information provided by this comparison feeds the agents internal
confidence that it is able to propose the correct actions. In this case, the agents internal
confidence builds on the successes and failures of the actions that were previously proposed
instead of the metrics on the current actions, provided by the learning algorithm, as in the
work of Chernova (2009), and Billing et al. (2010).
Several approaches use specific structures, such as the forward models, to emulate the
behaviour of mirror neurons (Rizzolatti, Fadiga, & Gallese, 1996; Demiris & Hayes, 2002;
Maistros & Hayes, 2004; Rao et al., 2004; Lopes & Santos-Victor, 2007). However these
structures are specifically designed for robots and use hardware inhibitors to prevent the
robot actuators from executing the estimated actions. The main objective of these kind of
structures is to create a distinction between the description of the action and its execution,
that is, to create abstract representations of agent actions (Kulic et al., 2011). This solution
is simpler and provides the agent with control over the execution of actions.
One of the characteristics of the reviewed approaches is that they usually focus on specific
problems, which implies adaptations of some of their features whenever they are applied
in new domains. Despite this problem, as shown in this section, the reviewed approaches
provide important features that can be adapted to our approach to learning by observation.

3. The Software Image
This section presents a summary of the additional functionalities that our approach introduced to previous work regarding the software image (Machado & Botelho, 2006). The
section describes these new functionalities, explains the reasons for their inclusion in the
software image and how they are advantageous for learning by observation.
Our approach to learning by observation proposes a software image that allows software
agents to learn by observing the actions of other agents. The act of observing a software
agent does not imply the use of computer vision; instead the agents use specialized sensors
that read meta-data about the observed agent. This meta-data is what we call the software
image, which is defined by software objects and the relationships among them, as displayed
in Figure 1. Despite the current version of the software image being primarily focused
on learning by observation, we believe that this image can be useful for other purposes
(for example, improve the agents interaction with the surrounding environment through
embodiment). Additional work will incrementally reveal the characteristics of a software
image that is independent of any particular use.
Our proposal for the software image (Costa & Botelho, 2011) provides an accessible and
domain independent description of the agents constituents, the actions it executes and the
conditions holding when it decided to execute those actions. This description is used by
321

fiCosta & Botelho

software agents that are interested in observing the represented agent for comparison with
their own description, to check if both agents share the same capabilities, as described in
section 4.1. Figure 1 shows a representation of the key elements of the software image. Like
in Machado and Botelhos (2006) proposal, the elements of the agent software image are
arranged in two categories, the static image and the dynamic image. The static image is
immutable (does not change over time) and describes the constituents of the agent whereas
the dynamic image changes with time and describes the activities of the agent.
1
1..*
1

StaticImage

1
1

1

SoftwareImage

+historySize: int

+AgentUUID: String

AgentPart

DynamicImage

1

1

0..*
0..*

1..*

Action
+descriptor: String

1

Sensor
0..*

1

+descriptor: String

<<Interface>>

DataSource

1

0..*
1

Condition

1..*
1

+value

1

VisibleAttribute

CompositeAction

0..*

+descriptor: String

ActionInstance
0..*

SimpleAction

1..*

Actuator

0..*

Snapshot
+order: int

1

+parameters

1

Figure 1: The class diagram of the software image
The improvements on Machado and Botelhos (2006) proposal for the software image
consist of including the agent sensors in the description of its components (the static image), combining the information on the state of the software environment (provided by the
agent sensors) with information on important aspects of the agents internal state with the
observed actions and enabling the representation of composite actions, that is, actions composed of sequences of simpler actions. Other important improvements include the ability to
store historic data on the agent actions and on the conditions holding for those actions, and
the use of an ontology to represent the knowledge on the agent sensors, actions and visible
attributes, on the tasks to be accomplished and on the concepts and relationships that exist
between those elements. Our proposal for the software image also defines a protocol for
observing the snapshots.
The following sections describe these improvements on the software image in greater
detail.
3.1 The Agent Sensors and the Snapshots of the Agent Activity
Machado and Botelhos (2006) version of the software image described software agents as
a collection of parts with visible attributes and actuators. The actuators, on their turn,
described a collection of actions that the agent was able to perform. However, it is also
important to include the agent sensors in this description, especially when software agents
only have access to part of the state of the environment, which is acquired by their sensors.
The information collected by the agent sensors represents the way the agent understands
the world - the agents perspective of the world. If the sensors are not included in the static
image, the agents have no way of knowing if the experts they observe understand the world
in the same way as them. The ability to understand the world in the same way as the
322

fiLearning by Observation of Agent Software Images

expert is important for a better understanding of the reasons behind the experts actions
(Bandura, 1977; Ramachandran, 2000).
Given the importance of the agent sensors, our proposal for the software image includes
them as constituents of the agent part (see Figure 1). This provides an accurate description
of the agent and allows the agents to compare with others not only by the actions they can
perform but also by the kind of information they can obtain from the environment.
On our proposal for the software image we also consider the conditions holding for an
agent action as the state of the environment, as it is acquired by the agent sensors, and the
instances of the agents visible attributes (the important aspects of the agents internal state)
at the moment the agent selects that action. The information contained in the conditions
depends exclusively on the information provided by the agent sensors and by the visible
attributes. The agent sensors and visible attributes, as well as the type of information they
provide, are defined when designing the agent. Section 5 shows an example of how the
sensors and visible attributes are set for an agent and how this affects the conditions.
The conditions holding for an action play an important role in the information provided
for observation in the dynamic image. Unlike Machado and Botelhos (2006) proposal
where agents could only observe the action being currently performed, in our proposal,
agents acquire snapshots of the activity of the observed agent. Each snapshot contains
information on the executed action and on the conditions holding at the moment the agent
decided to select it. This way, the information provided for observation is similar to the
training sequences used for training machine learning algorithms (a sequence of conditionaction pairs), which is an important aspect of the learning methods described in section
4.2.
In addition to including the conditions in the information provided for observation, the
action provided in the snapshot can either be simple or composite (see Figure 1), that is, an
action composed of a sequence of actions. This allows agents to handle sequences of actions
in the same was as single actions, when observing.
3.2 Additional Innovations on the Software Image
This section describes additional innovations on our proposal for the software image. The
most important innovation is the ability to store historic data. The historic data provides
a limited amount of past snapshots. It allows observers to gather knowledge much faster,
when compared with observing only the current action, because it is not necessary to wait
for the agent to perform those actions. However, this innovation requires the conditions
to hold the perspective of the agent executing the actions, that is, both the state of the
environment and the instances of the visible attributes are provided as the agent perceives
them. Without the agents perspective it would be hard, if not even impossible, for the
observer to know the conditions holding in the past.
Using the agents perspective can be seen as a limitation when compared with using
the observers perspective when acquiring the conditions because it requires the observer
agents to have the same kind of sensors and visible attributes as the expert agents they
observe, so they can understand the information contained in the conditions (see section 4.1).
However, it is not always ensured that the observer has a direct access to the environment
of the observed agent, like for example, when the software environment of the observed
323

fiCosta & Botelho

agent is running on a distinct process. In these cases, it would be necessary to use complex
communication mechanisms for the observer to get access to the information on the different
process. This would not be necessary if the observer only used the information provided
by the software image because of an additional mechanism, the software image index
described in section 4.1, which provides a shared repository where all registered software
images can be easily accessed.
Another innovation of the software image is the use of an ontology to describe the knowledge on the agent sensors, visible attributes and actions, on the tasks to be accomplished
and on the relationships between these elements. Using ontologies allows different agents
that follow the same ontology to use the same designations for the same kinds of sensors,
actions and visible attributes and for the same tasks. The ontology also enables specific
kinds of sensors, actions and visible attributes to be associated to specific tasks, which
allows the agents to know which elements are required to perform a task (see section 4.1).
Another important aspect of the ontology is the possibility of associating two different elements, which opens the possibility of translations between different ontologies. A
meta-ontology, which we call the software image meta-ontology, was created to facilitate
this translation. The meta-ontology defines the basic elements of the ontology and the
possible relationships between those elements. Additional information on this subject will
be presented in future work.
In addition to these innovations, our proposal also defines a new protocol for observing
the snapshots in the software image. A functionality developed for the software image,
the dynamic image notification, allows the subscribed observers to receive notifications
each time a new snapshot is created on the dynamic image of the observed agent. This
allows the observers to know exactly when they have to observe, that is, when they can
collect a new snapshot from the dynamic image of the observed agent. The following section
explains the way agents learn by observing (acquiring information from the agents software
image) an expert.

4. Learning by Observation
This section summarizes the approach regarding the complete learning by observation process, following previous work on this subject (Costa & Botelho, 2012). It shows a new
insight on the learning process and focuses on important aspects such as the process of discovering and observing experts and the methods of learning from the information provided
by observation. The section also describes the agents internal evaluation and how it affects
the agents behaviour.
The approach to learning by observation requires both the expert and the apprentice
agent to have software image since it provides the means for comparing the agents and
also the data for observation (see section 3). It presents a global view of the learning
process which comprises six activities, presented in Figure 2, that not always happen in
strict sequence. The agent may also be in one of two states regarding the learning by
observation process: the learning state and the execution state. In each of these states, the
agent will have access to only a subset of the learning process activities. Figure 2 shows
which activities are available to each state.
324

fiLearning by Observation of Agent Software Images

Learning
state

Discovery of
similar expert

Expert
observation

Storage of acquired
information

Learn control knowledge
and propose actions

Evaluation of
agents progress

Execution
state

Acquisition of current
state from sensors

Learn control knowledge
and propose actions

Application of
learnt knowledge

Figure 2: The activities of the learning process on each state
As Figure 2 shows, the possible activities of the learning state concern discovering and
observing the expert, retaining the information acquired in observation, learning control
knowledge to propose actions and evaluating the proposed actions. The possible activities of
the execution state concern acquiring the current state of the environment (from the agent
sensors), learning control knowledge to propose actions, executing the proposed actions
and evaluating the executed actions. Figure 2 also shows that, as result of mimicking the
properties of the mirror neurons, agents are capable of proposing actions both when learning
and when executing actions (see section 4.2).
The approach provides two distinct methods of learning from the information acquired
by observation, the recall and the classification methods, that where specifically developed
for the approach. They were inspired on the two most used algorithms for learning by
observation (see section 2.2). The two methods are combined to present a single solution for
apprentice agents (see section 4.2), which increases the agents ability to adapt to different
situations. The agents are able to perform the observed task when facing the same conditions
as the experts and also a similar task when facing different conditions.
The internal evaluation, described in section 4.3, is one of the most important activities
in the learning process because it monitors the agents ability to propose the correct actions
over time. The result of this monitoring is a measure of the agent confidence on the learnt
knowledge, the confidence value. The agent will be on the learning or on the execution
state depending on its confidence regarding the acquired knowledge.
The following sections describe important aspects of the approach to learning by observation and of the learning process such as how experts are discovered, how the agent learns
from the information acquired in observation and how the internal evaluation works.
4.1 Discovering and Observing Expert Agents
The software image, described in section 3, addresses the problem of providing information
that is necessary for observation in a way that is universally accessible to software agents.
With the software image, apprentice software agents can compare themselves with the
discovered experts and collect information about the actions executed by the expert and
about the conditions holding at the time the expert decided to execute those actions. A
discovery service, the software image index, was developed to facilitate the discovery
of expert agents. It allows software agents to register their software images in a shared
repository so that other agents are able to find them. The agents use this service to discover
the software images of the expert agents.
325

fiCosta & Botelho

Before starting to observe an expert, the agent must know if it is potentially possible
to learn by observing that particular expert. When it is not possible to determine what
agent structures are necessary for the task to learn (see section 3.2), the agents follow
Banduras (1977) social learning theory and learn by observing a similar expert, that is,
an expert whose static image has the same structure and the same instances of the atomic
elements as the agents static image. The agent uses the comparison functionalities of the
software image described by Costa and Botelho (2011) to compare its static image SIagent
with the static image of the expert SIexpert . If both images match, SIagent  SIexpert , the
expert can be observed because the agent is able to immediately recognize the actions and
conditions on the snapshots (see section 3.1), solving most of the correspondence problems
(Alissandrakis, Nehaniv, & Dautenhahn, 2002; Argall et al., 2009).
When the agent knows the task to learn T and the structures and abilities that are
necessary for that task, siT (see section 3.2), the concept of an expert from which it is
potentially possible to learn is extended, allowing agents to observe an expert as long as
the intersection of their software images contains those structures and abilities, (SIagent 
SIexpert ) 3 siT . This is enough to ensure that the apprentice agent is able to recognize
all the conditions and activities on the expert snapshots that are necessary for learning a
specific task. The agents determine which structures and abilities are necessary to perform
a task through an ontology, as explained in section 3.2.
After discovering the expert to observe, the agent subscribes the experts dynamic
image notification and acquires all the snapshots in its history record (see section 3.2).
The notifications facilitate the process of observing the expert since they determine the
ideal moment for the agent to observe, which is when a new snapshot is created in the
experts dynamic image. While the agent is acquiring the snapshots in the history record,
the new snapshots created in the experts dynamic image are also acquired and stored in a
temporary memory.
The temporary memory functions as a buffer for observation because it allows the agent
to handle snapshots at a different rate from which they are acquired. It also allows the agent
to keep record of the experts actions that might take place while it is reading the history.
The snapshots stored in the temporary memory are only handled after the history snapshots
are handled, which provides the agent with an uninterrupted sequence of snapshots from a
moment in the past until the current moment.
When compared with other solutions such as searching the history record on demand,
that is, to find relevant information for a particular problem, collecting all the information
on the history record is a more efficient solution because it allows the agent to obtain a
large set of experiences in a small amount of time. Acquiring all the history record not
only ensures that the solution for a particular problem is found (if it really exists in the
history) but also provides the agent with an increased amount of information that might
be important to solve other problems in the future.
An important aspect of the approach to learning by observation is the preference for
observing different experts. The process of discovering an expert and collecting snapshots
from its dynamic image is referred to as the observation period. This process is cyclical
meaning that the agent may go through several observation periods while learning. At the
beginning of each observation period the agent is free to choose a different expert to observe,
326

fiLearning by Observation of Agent Software Images

increasing the diversity of its knowledge because different experts might provide different
points of view on the task to learn.
The different perspectives provided by the experts may increase the agents knowledge
with conditions that were never observed on previous agents or with a different approach for
performing a task. The following section shows the way the agent increases its knowledge
and handles these different approaches.
4.2 Learning from the Observed Snapshots
This section describes how the agent learns from the observed snapshots and how it uses
the acquired knowledge to propose actions for a set of conditions. It describes how the
agent holds the information contained in the snapshot in its memory. It presents the two
methods for learning from the observed snapshots, the recall and classification methods,
and explains how they are combined to present a single solution for proposing actions. The
section also explains how the behaviour of mirror neurons is mimicked by the approach and
why this is useful for learning.
The snapshots acquired from observation describe a relation between an optimal set of
conditions (C1      Cn) and an action A, C1  C2      Cn  A (see section 3.1). This
relation, which we call experience, is stored in the agents memory which is held by a
tree structure. Using a tree structure enables the sequence on which the snapshots were
observed to be intrinsically preserved by the structure, as shown in the literature review
in section 2. The tree structure also facilitates the consolidation of the agents knowledge
because it stores different approaches for the same task as alternative paths.
SET
SET
SET
FOR

Newexp as the new experience to store in memory
Previous as the experience from the snapshot observed before
Stored to false
each Exp <- experience in the agents memory
IF Exp has same conditions as Newexp
AND Exp has same action as Newexp THEN
PUT Exp in sub-tree of Previous
SET Stored to true
BREAK
ENDIF
ENDFOR
IF Stored is false THEN
ADD Newexp to the agents memory
PUT Newexp in sub-tree of Previous
ENDIF

Figure 3: The process storing experiences in the memory
Given that agents can observe different experts, the sequence on which the snapshots are
observed is broken each time the agent starts observing another expert. When this happens,
the snapshot observed before represents the activity of another expert and therefore cannot
be followed by the new snapshots. To prevent the fragmentation and duplication (multiple
instances of the same experience) of the agents memory, the process of storing new
327

fiCosta & Botelho

knowledge in the agents memory compares each new experience with the ones existing
in memory before storing it. Figure 3 describes how the process takes place.
The process described in Figure 3 allows the agent to create new knowledge using the
information that already exists in memory by merely creating new connections between
the existing experiences. A new experience is stored only when it does not exist in
the agents memory. The tree structure holding the agents memory allows the agents
knowledge to be expressed as a decision tree, as shown in Figure 4, where each node of the
tree is an experience. Depending on the number of branches starting from the node, each
experience can be followed by one or more experience, which adds the possibility of
choosing which sequence to follow and therefore provide different alternatives for executing
a task.
Previous
experiences

REFERENCE
EXPERIENCE

Experience

Experience

Experience

Experience

Experience

Following
experiences

Experience

Task

Figure 4: A representation of the tree structure in the agents memory
The recall and the classification methods of learning use the information contained in
the agents memory (the tree of experiences shown in Figure 4) to propose actions for
given conditions, which we call the currentConditions. The two methods distinguish
themselves by the way they use the information in the agents memory to propose the
actions. The recall method proposes actions by determining the way the experiences are
connected with each other in the tree, whereas the classification method proposes actions by
categorizing the individual experiences and comparing those categories with the category
of the current problem.
Both the recall and classification methods provide several possibilities of actions. To
determine which of these actions is the best suited for execution, a reliability is associated
to each proposed action. The reliability is calculated in each method and ranges from
zero to one, inclusive. It determines how reliable an action is (zero not reliable; one fully
reliable) for the method that proposed it.
328

fiLearning by Observation of Agent Software Images

The recall method proposes actions by following the connections between the experiences in the agents memory (see Figure 4). To propose actions, the method requires a
reference to an experience in the agents memory which indicates where to start following
the connections. This reference, which we call the referenceExperience, represents the
last action executed by the agent Act and the conditions holding for that action Cond.
Figures 5 and 6 show the way the referenceExperience is discovered in the agents
memory.
FUNCTION discoverReferenceExperience(Cond,Act)
SET Similarity to zero
SET RefExp
FOR each Exp <- experience in memory
IF action from Exp is the same as Act THEN
IF Exp has same conditions as Cond
RETURN Exp
ELSE
SET Val as similarityBetween(conditions from Exp,Cond)
IF Val is bigger than Similarity THEN
SET RefExp to Exp
SET Similarity to Val
ENDIF
ENDIF
ENDIF
ENDFOR
RETURN Exp
ENDFUNCTION

Figure 5: Obtaining the referenceExperience from the agents memory
As the process in Figure 5 shows, the referenceExperience is the experience in
memory whose action is the same as the last action executed by the agent and whose
conditions are the same as the conditions holding for the last executed action. Given that
the agent might not be facing the same conditions as the expert, it is possible that the
conditions holding for the last executed action are not found in the agents memory. In
these cases, the referenceExperience is the one with the same action who shares the
largest number of similar conditions as calculated by the process shown in Figure 6.
After discovering the referenceExperience in the agents memory, its tree subset
(the set of tree branches) is retrieved. The tree subset represents the choices of the experiences that follow the referenceExperience (see Figure 4). The actions from those
experiences represent the several action possibilities to be proposed by the recall method.
The reliability of each action is obtained from the similarity between the conditions
holding for the action (in the experience) and the given currentConditions as calculated by the process shown in Figure 6. The highest reliability is given to the action
whose conditions most resemble the currentConditions.
Unlike the recall method, the classification method does not require a reference to an
experience in the agents memory to propose actions. The proposed actions are provided
by an adaptation of a classification algorithm that is trained with the experiences in the
329

fiCosta & Botelho

REQUIRE CondA to have the same size as CondB
FUNCTION similarityBetween(CondA,CondB)
SET Sum to zero
SET Size as length of set of CondA
FOR each C1 <- condition in Exp
Inner <- (FOR each C2 <- condition in Cond)
IF C1 equals C2 THEN
ADD one to Sum
BREAK Inner
ENDIF
ENDFOR
ENDFOR
RETURN (Sum / Size)
ENDFUNCTION

Figure 6: Obtaining the similarity between two sets of conditions

agents memory. Previous experiments (Costa & Botelho, 2012) revealed that the most
suited algorithms for the classification method are the KStar from Cleary and Trigg (1995)
and the NNGE (Nearest Neighbour like algorithm using non-nested Generalized Exemplars
from Martin, 1995). Since the latter consumes more resources the choice falls on the KStar
algorithm.
The KStar algorithm was adapted to be able to propose representations of agent actions.
The implementation of the KStar algorithm was modified to allow the experiences in the
agents memory to be regarded as positive examples from which the proposed actions are
deduced. The categorization capabilities were also enhanced to allow the conditions to
define the differences between the classes. To propose actions, the classification method
calculates the distances between the conditions to find the experiences whose conditions
are closer to the currentConditions. The KStar algorithm uses entropy to measure the
distances between two conditions (Cleary & Trigg, 1995).
The reliability of the actions proposed by the classification method is directly associated with how much the conditions holding for that action are close to the currentConditions. Once again, entropy is used to measure this distance. For example, if the conditions
holding for a proposed action are identical to the currentConditions the reliability of
that action is 1, that is, the action is fully reliable from the standpoint of the classification
method.
The recall and classification methods are combined in a single solution. Agents use
both methods to propose actions and choose the best one for the current situation. To
help deciding which action is best for the current situation, both the recall and the classification methods are associated to a weightFactor, whose initial and minimum value
is zero. The weightFactor determines which of the methods is capable of proposing the
most suitable action for the current situation. The reliability of the proposed action is
combined with the weightFactor of the method that proposed it, which results in the
finalReliability (f inalReliability = reliability  weightF actor). The action with the
highest finalReliability is the best one for the current situation.
330

fiLearning by Observation of Agent Software Images

The weightFactors change each time the agents capacity of proposing actions is
evaluated (see section 4.3). The weightFactor of a method increases if the action with
the highest reliability proposed by that method is proven to be an appropriate choice by
the internal evaluation (see section 4.3). If the evaluation determines that the action was
not appropriate, the weightFactor of the method decreases. As explained in section 4.3,
the amount on which the weightFactor increases or decreases depends on the value of
the reliability of the proposed action.
The recall and classification methods of learning propose actions when the agent is in the
learning state and in the execution state. This is possible because the actions proposed by
the methods are simply a representation of the agent actions, that is, the proposed actions
are not automatically executed. The agent has the control over which action is going to
be actually executed. This control allows the agent to behave in the same exact way both
when observing (in the learning state) and when preparing to execute its own actions (the
execution state), which in a sense is similar to what happens with the mirror neurons.
The ability to propose actions in the learning state allows the agent to experience the
actions it observes as if it was preparing to perform them, by proposing actions for the
conditions in the observed snapshots. This is advantageous since it allows the agent to
realize if it is capable (or not) of making the same decisions as the expert. It is also useful
for the agents internal evaluation to decide if the agent has acquired sufficient knowledge
to change to the execution state, as explained in the following section.
4.3 The Agents Internal Evaluation
This section describes the agents internal evaluation. It shows the way evaluation operates
and how it affects the transition between the two states of the learning process, the learning
and the execution state. It describes the way the agents confidence is updated and how it
is related to the agents ability to propose appropriate actions.
The evaluation is a transversal process that covers both the learning and the execution
state. The main purpose of evaluation is to ensure that the agents knowledge is appropriate
for mastering a task, which influences the agents internal confidence. The agents internal
confidence expresses the successes and failures in proposing the appropriate actions for the
faced conditions. Depending on the value of the internal confidence the agent may be in the
learning state or in the execution state of the learning process. Two configurable thresholds,
the UpperConfidenceThreshold and the LowerConfidenceThreshold determine
the values at which the agent changes to the learning or to the execution state. Section 5.1
shows how the values selected for these thresholds influence the agents capabilities. Figure
7 shows how the thresholds affect the transition between the two states of the learning
process.
As Figure 7 shows, when the agents confidence goes over the UpperConfidenceThreshold the agent is confident enough on its knowledge and switches to the execution state
where it executes the actions it proposes (see section 4.2) for the conditions provided by its
sensors and by the relevant aspects of its internal state (the visible attributes). When the
confidence goes under the LowerConfidenceThreshold the agent stops being confident
on its knowledge and switches to the learning state where it acquires more knowledge by
observing experts. The value of the internal confidence is affected by the agents capacity to
331

fiCosta & Botelho

Execution State

Learning State
UpperConfidenceTreshold

LowerConfidenceTreshold
Confidence

Figure 7: The influence of the confidence thresholds in state transition
propose the appropriate actions whether it is observing (in the learning state) or preparing
to execute those actions (in the execution state).
The agents internal evaluation is constantly testing the agents capacity to propose the
correct actions, both when the agent is in the learning state and in the execution state. In
the learning state, the agent is tested for its ability to propose actions for the conditions in
the observed snapshots. The agents confidence increases when the best action it proposes
(the one with the highest finalReliability as explained in section 4.2) is the same as
the action observed in the snapshot, otherwise the confidence decreases. The amount on
which the confidence increases or decreases depends on the reliability of the best action
proposed. For example, if A is the action in the snapshot, B is the best action proposed
(with a reliability of 0.8) and A 6= B, the agents confidence decreases 0.8.
Using the reliability of the best proposed action as a factor for increasing or decreasing
the agents confidence is the simplest way of including the confidence that the methods of
learning have on the actions they propose (see section 4.2) in the calculation of the agents
internal confidence. The way the internal confidence is updated gives more importance to
the actions proposed with a high reliability, that is, when the learning methods have a
high confidence on the actions they propose. In these cases, if the actions are not correct
the penalization in the internal confidence should be larger than when the methods have
low confidence on the actions they propose because it means the agent learnt something
wrong.
In the execution state, the agent is tested for its ability to execute the correct action for
the faced conditions, which reflect the agents perception from its sensors and the important
aspects of its internal state (see section 3.1). As section 4.2 shows, when in the execution
state the agent selects the action with the highest finalReliability, from those proposed
by the recall and classification methods, to be executed. A simple monitoring detects if
there was any problem that prevented the correct execution of that action. Whenever a
problem is detected the agents confidence decreases. Once again the amount on which the
confidence decreases depends on the reliability of the executed action (see section 4.2).
The simple monitoring of the execution of the actions has some limitations because even
though there are no problems when executing the actions it is not possible to ensure that
the action was the most appropriate for the faced conditions. To overcome this problem, our
approach allows the evaluation to receive external feedback on the executed actions from
specialized experts, the teachers, or from other evaluators that are specific to the applica332

fiLearning by Observation of Agent Software Images

tion domain. This possibility for evaluation approximates our approach to the paradigm
of learning by teaching which goes beyond the scope of this paper and therefore will be
presented in future work.
Another important feature of the agents internal evaluation is the ability to force the
agent to switch to the learning state independently of its confidence. This forced switching
happens when the agent is faced with a certain amount of unfamiliar conditions, that is,
conditions not found on the observed snapshots. The amount of unfamiliar conditions is
controlled by a configurable threshold whose value depends on the application domain. As
explained in section 3.1, the conditions consist of the information provided by the agent
sensors and visible attributes. The conditions are familiar when the agent has observed an
expert facing the same conditions and therefore knows exactly the correct action to propose.
If the conditions are not familiar, the agent has never observed an expert facing them
which probably means that it does not know what actions to propose. Under these circumstances, the amount of unfamiliar conditions rises only when the evaluation determines
that the executed action was inappropriate. To turn this forced switching in a measure of
last resort, the amount of unfamiliar conditions resets each time the agent faces a familiar
condition. Therefore, this amount represents the number of consecutive times the agent
faces unfamiliar conditions.
After being forced to switch to the learning state, the agents confidence decreases to
a configurable reference value, the UnfamilarConfidenceReference, which has to be
lower than the LowerConfidenceThreshold to allow the agent to remain in the learning
state for a while. The lower the value of UnfamilarConfidenceReference in relation
to the LowerConfidenceThreshold the longer the agent takes to change back to the
execution state. This is the simplest solution to keep the agent in the learning state for
some time.
Other solutions which take the unfamiliar conditions into account are more complex
because they require the agent to find an expert that is facing those conditions. Since it is
impossible to determine when the agent finds such experts, it would be necessary to develop
complex mechanisms to allow the agent to return to the execution state after some time,
thus preventing the agent from spending too much time learning. A similar behaviour can
be achieved by reducing the confidence and letting the internal evaluation decide when to
return to the execution state by testing the agents knowledge while it is observing.
The role played by evaluation, namely making the agent return to the learning state,
contributes to overcome one of the major problems faced by learning by observation, the
fact that the agents are limited to performing only the actions they have observed. Forcing
the agent to return to the learning state (and thus to observe other experts) gives the agent
an opportunity to increase its knowledge since other experts may have different experiences
that might provide the agent with new knowledge.

5. Experimental Results
This section describes two scenarios in which our approach was implemented. Since both
scenarios are software implementations, errors in the acquisition of the snapshots or of the
conditions make no sense and therefore are not considered in the experiments. The first
scenario was designed to be appropriate for learning by observation because the majority of
333

fiCosta & Botelho

expert activity does not affect the state of the environment. The second scenario compares
our approach with a reinforcement learning approach in a typical reinforcement learning
experiment, the mountain car experiment from Sutton and Barto (1998). Through this
scenario we present a direct comparison between the solutions provided by a reinforcement
learning algorithm and by our approach to learning by observation. The statistical relevance
of the data collected for the comparisons is guaranteed by the students t-test.
The scenarios are software implementations where all developed agents have a software
image. When apprentice software agents observe the experts they are effectively acquiring
snapshots from the experts software image. As a simplification, the software images of the
apprentice agents have the same constituents as the software images of the experts they
observe. In both scenarios the apprentice agents do not receive external feedback from
teachers or other evaluators (see section 4.3) so that the results reflect a pure learning by
observation approach.
Despite the process of discovering and identifying a similar expert being an important
step for the approach to learning by observation, the results presented in this section do not
disclose this process for the purpose of focusing the results on the benefits of learning by
observation. In both scenarios, the apprentice agents use the software image for discovering,
identifying and collecting the information that is necessary for observation from the experts.
Previous work on the software image (Costa & Botelho, 2011) presents some results on these
aspects.
The first scenario simulates an agent with a virtual hand that displays numbers in sign
language. The numbers are provided to the agent by a software number generator which
provides numbers between one and five. Each agent is associated with a single number
generator and has no knowledge on the other number generators associated to the agents
that participate in the simulation. Each time a new number is provided to the agent, the
virtual hand is changed to display that number by means of sign language. The virtual
hand is used only for communicating with users through a graphical display and it is not
accessible to other agents.
However, the agents virtual hand has an accessible representation of its current state
in the agents software image as a visible attribute (see section 3). The only way software
agents can obtain information on the state of the agents virtual hand is through the software
image. The visible attribute represents the state of the visible hand as an object with five
attributes each representing a finger on the hand. Each attribute can be in one of two
states, UP or DOWN.
The expert agent designed for this scenario has a specialized part which holds the
knowledge on the most efficient way of changing the virtual hand so that it represents
the number provided by the number generator. For example, when the agent perceives the
number one, if the virtual hand is showing the number two (the index and middle fingers
are UP) it is only necessary to move the middle finger DOWN, whereas if the hand was
showing the number four, it would be necessary to move the middle, ring and pinky fingers
DOWN. The part perceives the numbers from the number generator through a specialized
sensor. It has an actuator with five actions, one for each finger, that change the state of
that finger.
To increase the complexity of the scenario, the number generators need to be reset from
time to time or else they will stop generating new numbers. The number sources can either
334

fiLearning by Observation of Agent Software Images

be in the Active state or in the Inactive state. The source stops providing numbers when
it is Inactive. The reset changes the source back to the Active state. Because of this,
the expert agent has another specialized part which holds the knowledge on when and how
to refresh the random number generator. The state of the number generator is perceived
by this specialized part through a sensor which indicates the state of the generator. The
agent part has one actuator with a single action that resets the source.
The apprentice agent developed for this scenario has to learn how to master these
two tasks, manipulating the agents virtual hand to display the perceived numbers and
resetting the source, by observing experts with the same constituents and capabilities. Like
the experts, apprentice agents have two parts. One of them specializes in manipulating
the virtual hand and has one sensor that perceives the numbers provided by the number
generator, one visible attribute that displays the state of the virtual hand and one actuator
with five actions that change the state of each finger.
The other part of the apprentice agent specializes in managing the source that provides
the numbers and has one sensor that perceives the state of the number source and one
actuator with a single action that resets the source. Given the description of the agent
sensors and visible attributes, the conditions for this scenario (see section 3.1) consist of
the number provided by the agent source, the state of the virtual hand and the state of the
number source.
The second scenario is a software implementation of an agent that learns how to climb
a mountain simulated by a sinusoidal wave. The agent must abide by the laws of physics
to climb the mountain, and because it has no sufficient force, it will not be able to climb
the mountain by going forward only, it needs to accelerate backwards and forwards to gain
momentum. The goal of this scenario is to reach the top of the mountain, that is, the
peak of the sinusoidal wave, taking the least number of decisions and travelling the smallest
distance (up and down the mountain) as possible.
The experts, provided for this scenario, know the optimal way (the exact moment and
direction they need to accelerate) to climb the mountain and reach its top. The experts
perceive their current speed and direction and their location in the mountain through their
sensors and use this information to decide on the direction which they should accelerate next.
They can choose between accelerating forward, accelerating backward or not accelerating
(which maintains their current speed).
The apprentice agents have to learn to decide which direction to accelerate according
to their location, speed and direction. The scenario includes two kinds of apprentice agents
because we are comparing two different learning methods. The first kind of apprentice
uses a reinforcement learning algorithm (Q-Learning, implemented in the PIQLE tool from
Comite, 2005) to provide the agent with the knowledge that is required to climb the mountain. The reinforcement learning agent is able to perform three actions, accelerate forward,
accelerate backwards or maintain speed.
The reinforcement learning agent is configured with a learning rate  = 0.2 and a
discount factor/rate  = 0.9, which are the settings that present the best results. The
learning rate also decreases with time following a geometrical decay, which allows the agent
to eventually stop learning after a period of time. A simple reward scheme is used for the
reinforcement learning algorithm. The agent is only rewarded when it reaches the goal of
this scenario, the top of the mountain. Any other reward schemes would require the use
335

fiCosta & Botelho

of some kind of supervisor to determine the ideal situations to apply the reinforcements.
This would have changed the comparison we intended for this scenario, which is to compare
learning by observation with a situation where apprentice agents need no supervisors. This
is the case for the reinforcement learning agent with this simple reward scheme. The only
information provided is the goal, which is embedded in the agent.
The second kind of apprentice agent uses learning by observation to acquire the knowledge of climbing the mountain. The agent shares the same constituents and capabilities
of the experts it observes, being made out of a single part with two sensors and one actuator. The sensors provide the agent with its location in the mountain and its current
speed (which can be positive if the agent is moving forwards or negative if the agent is moving backwards). The actuator provides the agent with three actions, accelerate forwards,
accelerate backwards and maintain speed.
The following sections present the results of the simulations of these two scenarios.
To provide the learning by observation agent with a broader set of experiences, all the
simulations use more than one expert and each expert experiences the scenario in different
ways, that is, they receive different information from the environment through their sensors.
On the other hand, only one apprentice agent is used on the simulations. This simplification
ensures that there is no risk for the tested apprentice agent to observe other apprentices,
which may mislead it with incorrect actions, instead of observing the experts.
The unit of time used for the simulation results is the simulation step. A simulation
step represents the time slot where the participant agents take a decision. For learning by
observation agents, a simulation step can either represent an observation (the acquisition
of a snapshot from the expert) and the subsequent learning (when the agent is in the
learning state), or updating the facing conditions (see section 3.1), proposing and executing
the action best suited for the faced conditions (when the agent is in the execution state)
(see section 4). For reinforcement learning agents a simulation step represents updating
the facing conditions, selecting an action for those conditions (from the action-state pairs
stored in their memories) executing the action and interpreting the rewards (updating the
action-state pairs). For expert agents, a simulation step represents updating the facing
conditions and selecting and executing the action best suited for those conditions.
5.1 Results of the Virtual Hand Scenario
This section shows the impact of changing the confidence thresholds (see section 4.3) on
the time spent on learning and on the total number actions that are appropriately executed
by the agent. It also shows the way the agents perform, in terms of the number of correct
actions, in two different settings.
The two settings depend on the sequence of numbers provided by the number generators. To control the results of the simulation, the numbers provided by the generators are
designated from a pre-determined sequence. The number generators provide the numbers
following that sequence and when the end of the sequence is reached the source needs to be
reset by the agent. The reset makes the number generator provide the numbers following
the same sequence.
For the first setting (exp1 ) the number generators of both apprentice and expert agents
provide the same sequence of numbers. In this setting the apprentice agent faces the same
336

fiLearning by Observation of Agent Software Images

conditions in the same sequence as the experts it observes which provides a good testing
ground for the recall method of learning (see section 4.2). The setting shows how the agent
is capable of performing the task as it was observed on the experts.
On the second setting (exp2 ), each number generator has a different sequence of numbers
with different sizes, that is, the number generators need to be reset at different times. The
apprentice agent faces conditions that are different from those faced by the observed experts
which provides a good testing ground for the classification method of learning (see section
4.2). The setting shows how the agent is capable of performing a task that is similar to
what has been observed when facing different conditions.

Figure 8: The impact of changing the confidence thresholds
Besides being a testing ground for the two learning methods, this scenario also determines the impact of changing the values for the UpperConfidenceThreshold and
the LowerConfidenceThreshold (see section 4.3). For this reason, both settings were
initially tested on agents using different values for these thresholds. Figure 8 presents a
summary of the results obtained for the second setting in simulations with 4000 steps (each
variation of the thresholds is tested in a simulation lasting 4000 steps). The choice for the
337

fiCosta & Botelho

second setting in Figure 8 is because it is where changing the thresholds had more influence
and also because it is a more realistic approach for the scenario, since the chances for the
agent to find itself on the same situation as the experts it observes are very small.
As explained in section 4.3, the confidence thresholds affect the length of the learning period and the number of subsequent learning periods which in turn affects the total number of
correct actions performed by the agent throughout the simulation. When the UpperConfidenceThreshold is low, the agent may not have enough time to learn, which decreases
the agents performance in terms of being able to execute the correct actions. When the
UpperConfidenceThreshold is too high the agent spends a lot of time learning, which
reduces the time spent in the execution state and therefore the total number of actions
executed by the agent is lower.
When the LowerConfidenceThreshold is too close to the UpperConfidenceThreshold the agent switches between the learning and execution state more often (see section 4).
This may hinder the agents ability to complete a task because the slightest error causes
the agent to return to the learning state. When the LowerConfidenceThreshold is
too far apart from the UpperConfidenceThreshold, the agent takes longer to switch
between learning and execution. This slows down the ability to recognize the mistakes and
switching to the learning state. It also slows down the recovery from the learning state to
the execution state.
The results presented in Figure 8 fall in the region where the LowerConfidenceThreshold changes between zero and fifty and the UpperConfidenceThreshold changes from
being equal to the LowerConfidenceThreshold and up to a difference of ten units more
than the LowerConfidenceThreshold. This is where the agent is able to perform the
maximum number of correct actions in the time provided by the simulation (4000 steps). If
the difference between the thresholds is larger than ten, the total number of correct actions
decreases since the agent takes longer to change between the learning and execution states.
Figure 8 also shows that the length of the initial learning period increases as the LowerConfidenceThreshold increases. The longer the agent spends learning the less time
it has to perform the actions. Therefore, as Figure 8 shows, the optimal values (for this
scenario) for the thresholds are ten for the LowerConfidenceThreshold and fifteen for
the UpperConfidenceThreshold. This provides the agent with a learning period that
is long enough for the agent to learn all the necessary skills (so that the majority of the actions it performs are correct) but short enough to allow the agent to perform the maximum
amount of actions (which is 3923 actions according to Figure 8).
After determining the best values for the confidence thresholds, the scenario was simulated under the two settings (exp1 and exp2 ) in a 4000 step simulation which was repeated
100 times to encompass the time variations that might exist on the simulations. Since the
two settings were prepared with the two learning methods in mind, it is important to look
at the importance given by the agent to these methods on each setting. Table 1 shows the
average values of the weights of the recall and classification methods of learning (see section
4.2) on each setting.
Table 1 shows that, as expected, the recall method has more influence on the proposed
actions when the agent is faced with the same conditions as the expert (exp1 ) than when
it faces different conditions (exp2 ). In the first setting, the weight of the recall method
makes it more likely (more than 50 % chance) for the actions proposed by this method
338

fiLearning by Observation of Agent Software Images

Setting
exp1
exp2

Recall Weight
0.509
0.317

Classification Weight
0.491
0.683

Table 1: The average weights of the recall and classification methods on each setting
to be executed, even though the actions proposed by the classification method can also
be executed. In the second setting, the classification method has more influence on the
proposed actions because it has the largest weight. The agent is not able to use the recall
method very often to follow the same sequence of actions as the expert because it is facing
conditions that are different from those observed on the expert (see section 4.2).
The overview of the results of simulating the scenario in the two settings is presented
in Table 2. The table compares the time spent on the learning state and on the execution
state, the number of actions the agent was able to execute, how many of those actions
were appropriate and also the time of a simulation step when learning and when executing
actions. The results in Table 2 present both the average values and the standard deviation
from running the simulation 100 times.

Time spent
learning (s)
Time spent
execution (s)
Total actions
executed
Amount of
appropriate actions
Step time in
learning state
Step time in
execution state

average
stdev
average
stdev
average
stdev
average
stdev
average
stdev
average
stdev

Expert
2.415
0.606
4000
0
100 %
0
0.603
0.151

Apprentice
(exp1)
1.62
0.12
10.435
0.957
3905
0
100 %
0
17.052
1.305
2.672
0.245

Apprentice
(exp2)
134.98
22.503
23.365
6.017
3709.25
31.13
92.95 %
7.73 p.p.
35.834
2.129
6.363
1.815

Table 2: Overview of the results on the simulation of the virtual hand scenario
The results in Table 2 show that when the agent faces the same conditions as the experts
it observes (exp1 ) it spends less time in the learning state and is able to perform more actions
throughout the simulation than when it faces different conditions (exp2 ). When the agent
faces different conditions (exp2 ) it spends more time learning because it needs to acquire
more knowledge and requires this knowledge to come from different sources to improve the
generalization in the classification method (see section 4.2). This leads the agent to spend
more time discovering different experts to learn from and also causes the agent to return to
the learning state more often.
In addition, the high standard deviation values show that when the agent faces different
conditions (exp2 ) the number of actions executed throughout the simulation, the time spent
learning and the time spent executing actions differs considerably. This variation is directly
339

fiCosta & Botelho

associated with the randomness of the observed experts. Each time the simulation is run,
the apprentice observes different subsets of the available experts in a different sequence,
which changes the knowledge retained by the agent throughout the simulation and affects
the number of actions executed and the time spent on learning and on executing actions.
Table 2 also shows that when facing the same conditions (exp1 ) all the actions executed
by the agent were appropriate, whereas when facing different conditions (exp2 ) only approximately 92% of the totality of executed actions were appropriate. This is an important
indicator that the agent is more likely to return to the learning state, after starting to
execute actions, when it is facing different conditions (exp2 ).
As for the average time spent in the simulation steps, Table 2 shows that in both cases,
the simulation step is longer when agents are learning. This was expected since when the
agent is in the learning state it has to perform various tasks such as discovering expert
agents, comparing its software image with the software images of the discovered experts,
acquiring the snapshots and storing its information, proposing actions for the conditions
from the snapshots and evaluating those proposals (see section 4).
When executing actions, the simulation step of the apprentice agents is longer than
the simulation step of the experts, which is understandable given that apprentice agents
require more processing than the expert. Besides using two methods for proposing actions
it is also necessary to take into account the influence of the internal evaluation. The step
time while executing actions is also longer when the apprentice agent is facing different
conditions (exp2 ), which influences the time spent executing actions. Even though the
apprentice agent executes fewer actions (only 3709), when compared with when facing the
same conditions (3905), it spends more time executing those fewer actions. The same effect
is observed in the time spent by the simulation step when learning.
This happens because when facing different conditions the agent needs to acquire more
knowledge which increases the amount of information in the agents memory. Since the
recall and classification methods of learning need to process the information contained in
memory, the larger the amount of information the longer it takes to process it. This effect is
felt both when learning and when executing actions because the agent uses these methods
in both cases.
A closer look at the simulation results is presented in Figure 9, which shows the progress
of the apprentice agents, in terms of the number of correct actions they have executed,
throughout the simulation. The figure also shows the progress of the state of the learning
process (see section 4) throughout the simulation in the apprentice agent. To provide a
clearer presentation, the results are combined in groups of 100 simulation steps.
Figure 9 shows that, when apprentice agents face the same conditions as the expert
(exp1 ) after a short learning period (of about 200 steps) all the actions they execute are
correct. When the agent faces different conditions from the experts it observes (exp2 ), the
initial learning period lasts a little longer (about 300 steps) and only approximately 90% of
the executed actions are correct.
The agent also experiences additional learning periods (of short duration) throughout
the simulation. The additional learning periods are mainly caused by the evaluation activity
mechanism that forces apprentices to switch to the learning state when facing conditions
that were not observed (see section 4.3). Although the subsequent learning periods affect
the time spent on learning they are of short duration. As Figure 9 shows, the overall
340

fiLearning by Observation of Agent Software Images

Figure 9: The behaviour of the apprentice agent in the two settings throughout the simulation

permanence in these subsequent learning periods does not exceed more than 3% of the total
simulation steps.
5.2 Results of the Mountain Car Scenario
The mountain car scenario compares expert agents, which are specialized in climbing mountains, with the learning by observation agents (LbO) that learn by observing experts performing the task to learn and the reinforcement learning agents (RL) that learn through
reinforcements. All the participant agents face the same conditions, that is, they are placed
in the same mountain at the same starting points. The observed experts also face the same
conditions as the learning by observation agent.
The results present a comparison based on the number of actions, the distance travelled
and the time spent until reaching the top of the mountain (the goal of the simulation). The
results also determine the average time of a simulation step for each agent and how much
time it takes for an agent to learn, that is, how much time it takes for an agent to reach
the top of the mountain for the first time.
341

fiCosta & Botelho

Since this scenario provides a goal, the simulation time-frame is grouped by attempts
of achieving the goal. Each attempt lasts a variable number of simulation steps, with a
maximum duration of 500 simulation steps. If the agent achieves the goal before the 500
steps the attempt is completed and is regarded as successful. If after the 500 steps the goal
is not achieved, the attempt is regarded as failed. The simulation lasts for 50000 attempts,
to provide the reinforcement learning agents enough time for testing all the hypotheses and
provide their best results.
Table 3 presents a summary of the important aspects of the scenario such as the amount
of time, number of attempts, number of actions and distance travelled by each agent to reach
the top for the first time. The table also shows how many times the agent reached the top of
the mountain, the average time of a simulation step and the average amount of simulation
steps in an attempt to reach the goal. The data in Table 3 was obtained from running the
scenario 100 times to encompass time variations. The students t-test was used to ensure
the statistical relevance of the data collected from running the scenario.

Time to reach top
first time (ns)
Attempts
for
reaching top first
time
Actions executed
to reach top first
time
Distance travelled
to reach top first
time
Number of times
reached top
Average
simulation step time
(ms)
Average simulation steps spent
in an attempt

Expert

Apprentice
(LbO)

Apprentice
(RL)

T-TEST
(LbO - RL)

30

3300

14660

2.2  107

1

1

105

3.4  1010

136

278

52471

3.2  1010

2.73

2.73

610.37

2.5  109

50000

50000

49895

2.9  1010

0.28

4.94

0.31

2.7  1039

136

136.01

229.92

4.9  1012

Table 3: Overview of the results on the simulation of the mountain car scenario
Table 3 shows that, as expected, the expert agent exhibits the best results in all aspects.
The table also shows that the learning by observation agent outperforms the reinforcement
learning agent in all aspects with the exception of the time of a simulation step. The
learning by observation agent also gets close to the same results as the expert in all aspects
with the exception of the time it takes to reach the top and the simulation step time.
When compared with the reinforcement learning agent, the simulation step of a learning
by observation agent lasts longer because of the amount of processing behind that decision.
342

fiLearning by Observation of Agent Software Images

Nevertheless, the learning by observation agent requires less simulation steps to complete
an attempt, that is, to reach the goal. The main reason for the longer simulation step is
because the learning by observation agent uses two methods for proposing the actions (the
recall and classification methods as explained in section 4.2) which requires an additional
effort of combining the results and choosing the best action from them. The agent also
has to perform a set of other tasks, like for example the internal evaluation (see section
4.3). The performance of the software used by the agent is likely to be improved in future
versions, which would lead to an improvement in the simulation step time.
Although the simulation step of a reinforcement agent takes less time, the agent requires
more steps and attempts to reach the top of the mountain for the first time and eventually
takes more time to learn how to reach the top of the mountain as the results in Table 3
show. This puts the reinforcement agent in last place when considering the time it takes
to reach the top for the first time. The results of the t-test in Table 3 show that the data
acquired in the simulations is statistically relevant.

Figure 10: General view of the results for the second scenario
343

fiCosta & Botelho

In addition to the information on achieving the goal for the first time, it is also important
to know how the agents performed throughout the simulation. Figure 10 shows the progress
of the expert and the two apprentice agents (learning by observation and reinforcement
learning) throughout the simulation in terms of the distance travelled, the number of times
reaching the top and the number of simulation steps required to reach the top. For a clearer
presentation, the results are combined in groups of 100 attempts to reach the goal. Each
attempt lasts a number of simulation steps that ranges from 136 to 500, depending on the
number of steps that are necessary to reach the top of the mountain (see Table 3).
The results presented in Figure 10 show that the learning by observation agent performs
better than the reinforcement learning agent in all considered dimensions. The figure shows
that the learning by observation agent requires less simulation steps (in comparison with the
reinforcement learning apprentice) to go from the starting point to the top of the mountain
after learning how to do it. The simulation steps spent in each attempt stabilizes at 136
(which is the same number steps spent by the expert) right after the agent has learnt the
task of climbing the mountain. The learning by observation agent also requires less attempts
to learn how to reach the top (1 attempt as shown in Table 3).
The distance travelled by the learning by observation agent is also smaller than the
distance travelled by the reinforcement learning agent. The value of the distance, in the
learning by observation agent, stabilizes at 2.73 (the same as the expert as Table 3 shows)
right after the agent learns the task of climbing the mountain, unlike what happens with
the reinforcement learning agent. As Figure 10 shows, even after a long learning period,
the reinforcement learning agent is not able to learn the most efficient way to climb the
mountain (the one that requires the least amount of actions and the smallest distance). Even
the lowest values of the fluctuations of the number of actions and the travelled distance are
still far away from those obtained by the learning by observation agent.
Besides these inabilities, the reinforcement learning agents capability of reaching the
goal fluctuates between 90% and 100% of the times, until almost the end of the simulation.
This means that even after learning how to reach the goal, the reinforcement learning agent
is not always able to do it, something that is also observed in the total number of times the
agent reaches the top of the mountain in Table 3. In contrast, the learning by observation
agent exhibits more stable results after learning the task of reaching the top of the mountain.

6. Conclusions and Future Work
The adoption of learning by observation solutions is relatively new in computer science and,
as the latest approaches show, it is still under development (Sullivan, 2011; Kulic et al., 2011;
Tan, 2012; Fonooni, Hellstrom, & Janlert, 2012). With the exception of our work (Costa
& Botelho, 2011, 2012) and Machado and Botelhos (2006) work, all the contributions for
learning by observation are focused on robotic agents and their physical properties. Software
agents and even the software used in robots are neglected. Even Machado and Botelhos
work has limitations since it only addressed the problem of learning vocabulary and it does
not allow generalizations of the acquired knowledge. This means that, unlike our approach,
her apprentice agent is not able to learn control mechanisms neither is capable of dealing
with conditions that are different from those observed on the expert.
344

fiLearning by Observation of Agent Software Images

Therefore, our learning approach clearly contributes to advance the state of the art on
learning by observation, providing software agents with a learning solution that is different
from all other methods that are usually applied for software agents. Unlike the robotic
approaches for learning by observation, our software approach is not limited to software
agents. It can also be used by robotic agents with minor adaptations, since all their physical
actions are controlled by or reflected in software events. Even the data collected by robotic
sensors needs a software representation (however complex it can be), since in its core the
robot is effectively running a program and all high level decisions are made by that program.
Thus, a software approach provides a broader solution than approaches limited to the
physical properties of robotic environments.
The experimental results in section 5 show that software agents are capable of improving
their ability to perform a task using our approach. The first scenario also shows that the
classification method of learning is essential for situations where the agent is not facing
exactly the same sequence of events as the expert. Namely, situations in which the agent
is faced with conditions that are different from those faced by experts it has observed. The
most usual scenarios for learning by observation, reported in the literature, address cases
in which the apprentice and the expert agents face exactly the same conditions. This kind
of situation is ideal for sequence learning, which is the reason why this method is one of the
most used in learning by observation approaches. With the inclusion of the classification
method, the ability to learn by observation is extended to other problems and domains.
The combination of the classification method with the recall method, which is inspired
in sequence learning, ensures that the agent is able to adapt, by itself, to a larger number
of circumstances, including those that are most usual for learning by observation scenarios.
Besides the combination of these two methods, our approach also provides an internal
evaluation mechanism that constantly tests the agents knowledge. This allows the agent to
know when it needs to acquire more knowledge or when it has acquired sufficient knowledge
and therefore can execute actions. The approach also offers the possibility of using external
feedbacks to enhance the agents evaluation, which gives agents the ability of knowing if
the actions they execute are appropriate.
The agents internal evaluation enables learning by observation agents to enhance their
knowledge even after they stop observing experts and start using the acquired knowledge,
since after starting to use their knowledge they may go back to the learning state. This was
one of the major drawbacks of previous learning approaches. The usual way of handling
this is by manually feeding new examples whenever the agent requires them. In the case
of our approach the agent is able to decide by itself when it needs to return to a learning
state, to observe experts. The process does not require any intervention since the agent can
find the experts from which to collect the new training examples.
The application of our approach in two distinct scenarios shows that it can be adapted
to different domains. The scenarios also show that the amount of time taken by our learning
approach to decide the actions to execute can sometimes be high. However, as the second
scenario shows, our agents are able to achieve the goal in less time than a reinforcement
learning approach, even in situations that have already been shown by the literature to be
adequate for reinforcement learning. The learning by observation agents are also able to
achieve approximately the same results as the experts they observe.
345

fiCosta & Botelho

Although apprentice agents take more time to choose the actions to execute than the
experts, this difference is smaller on the first scenario. Considering that, in the first scenario, the experts knowledge is expressed with more rules than in the second scenario, our
approach, unlike reinforcement learning, is able to cope with the increase of the complexity
in terms of the number of rules required to express the experts knowledge.
Besides providing a new insight for learning in software agents, through the software
image, our approach can also contribute to the software embodiment problem. Although its
main purpose is directed to learning by observation, the software image also allows visible
software agents to represent themselves on what can be called a body. As future work we
may continue developing the software image to better adapt it to the software embodiment
problem.
We also intend to continue testing our approach in different scenarios, especially in
situations where the actions have effects both in the agent and in the environment. In such
situations only a part of the effect of the actions is visible in the environment. We expect
the results to be similar to a situation where all the effects of the actions are visible in the
environment when the visible effects are enough to distinguish the actions. However, when
the visible effects are not enough to distinguish the actions, agents who learn only from the
effects of the actions might not be able to learn properly.
For example, the effect of action one is removing a number from the environment and
adding it to the agents internal memory and the effect action two is removing a number
from the environment and subtracting it to the agents internal memory. Both actions have
different effects in the agent, but the visible effects on the environment are exactly the same.
In this situation, an agent that only learns from the visible effects of the actions will not be
able to distinguish these two actions.
Finally, we also consider improving the approach to open the possibility of an agent
performing a task that is radically different from the tasks performed by the observed
experts. The agent should be able to use the acquired knowledge in a way that allows it to
perform new tasks that were never observed before.

Acknowledgments
This paper reports PhD research work, for the Doctoral Program on Information Science
and Technology of ISCTE-Instituto Universitario de Lisboa. It is partially supported by
Fundacao para a Ciencia e a Tecnologia through the PhD Grant number SFRH/BD/44779/2008
and by FCT project PEst-OE/EEI/LA0008/2013

References
Alissandrakis, A., Nehaniv, C., & Dautenhahn, K. (2002). Imitation with ALICE: learning
to imitate corresponding actions across dissimilar embodiments. Systems, Man and
Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 32 (4), 482496.
Argall, B. D., Chernova, S., Veloso, M., & Browning, B. (2009). A survey of robot learning
from demonstration. Robotics and Autonomous Systems, 57 (5), 469483.
Bandura, A. (1977). Social Learning Theory. Prentice Hall.
346

fiLearning by Observation of Agent Software Images

Billard, A., & Hayes, G. (1999). Drama, a connectionist architecture for control and learning
in autonomous robots. Adaptive Behavior, 7, 35-64.
Billing, E. A., Hellstrom, T., & Janlert, L. E. (2010). Behavior Recognition for Learning
from Demonstration. Proceedings of IEEE International Conference on Robotics and
Automation, Alaska.
Billing, E., Hellstrom, T., & Janlert, L. E. (2011). Simultaneous control and recognition
of demonstrated behavior. Tech. rep., UmeaUniversity, Department of Computing
Science.
Botelho, L. M., & Figueiredo, P. (2004). What your body and your living room tell my
agent. In Proceedings of the AAMAS 2004 Workshop Balanced Perception and Action
in Embodied Conversational Agents.
Byrne, R. W. (1999). Imitation without intentionality. Using string parsing to copy the
organization of behaviour. Animal Cognition, 2 (2), 6372.
Chernova, S. (2009). Confidence-based Robot Policy Learning from Demonstration. Phd,
Carnegie Mellon University.
Cleary, J. G., & Trigg, L. E. (1995). K*: An Instance-based Learner Using an Entropic
Distance Measure. In 12th International Conference on Machine Learning, pp. 108
114. Morgan Kaufmann.
Clegg, B. A., DiGirolamo, G. J., & Keele, S. W. (1998). Sequence learning. Trends in
Cognitive Sciences, 2 (8), 275281.
Comite, F. D. (2005). A Java Platform for Reinforcement Learning Experiments. Journees
Problemes Decisionnels de Markov et Intelligence Artificielle PDMIA 2005, 100107.
Costa, P. A. M., & Botelho, L. M. (2011). Software Image for Learning by Observation.
In Antunes, L., Pinto, H. S., Prada, R., & Trigo, P. (Eds.), Proceedings of the 15th
Portuguese Conference on Artificial Intelligence, pp. 872884, Lisbon, Portugal.
Costa, P. A. M., & Botelho, L. M. (2012). Learning by Observation in Software Agents.
Proceedings of the 4th International Conference on Agents and Artificial Intelligence
(ICAART 2012), 2 (Agents), 276281.
Dautenhahn, K., & Nehaniv, C. L. (2002). The agent-based perspective on imitation.
Imitation in animals and artifacts, 140.
Demiris, J., & Hayes, G. (2002). Imitation as a dual-route process featuring predictive and
learning components: a biologically plausible computational model. In Dautenhahn,
K., & Nehaniv, C. (Eds.), Imitation in animals and artifacts (MIT Press edition).,
pp. 327361. Cambridge.
Etzioni, O. (1993). Intelligence without Robots (A Reply to Brooks). AI MAGAZINE, 14,
7-13.
Fonooni, B., Hellstrom, T., & Janlert, L.-E. (2012). Learning High-Level Behaviors from
Demonstration through Semantic Networks. Proceedings of the 4th International Conference on Agents and Artificial Intelligence (ICAART).
Hajimirsadeghi, H., & Ahmadabadi, M. (2010). Conceptual imitation learning: An application to human-robot interaction. Machine Learning, 331346.
347

fiCosta & Botelho

Heyes, C., & Ray, E. (2000). What is the significance of imitation in animals?. Advances
in the Study of Behavior, 29, 215245.
Kulic, D., Ott, C., Lee, D., Ishikawa, J., & Nakamura, Y. (2011). Incremental learning of
full body motion primitives and their sequencing through human motion observation.
The International Journal of Robotics Research, 31 (3), 330345.
Lopes, M., & Santos-Victor, J. (2007). A developmental roadmap for learning by imitation
in robots.. IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics
: a publication of the IEEE Systems, Man, and Cybernetics Society, 37 (2), 30821.
Machado, J., & Botelho, L. (2006). Software agents that learn through observation (short
paper). Proceedings of the International Joint Conference on Autonomous Agents and
MultiAgent Systems.
Machado, J. a. (2006). Imagem Visual do Corpo de Software: Aquisicao de Vocabulario por
Observacao. Masters thesis, ISCTE.
Maistros, G., & Hayes, G. (2004). Towards an Imitation System for Learning Robots.
In Vouros, G., & Panayiotopoulos, T. (Eds.), Methods and Applications of Artificial
Intelligence, pp. 246255. Springer Berlin / Heidelberg.
Martin, B. (1995). Instance-Based learning : Nearest Neighbor With Generalization. Masters thesis, University of Waikato, Hamilton, New Zealand.
Mataric, M. J. (1997). Studying the Role of Embodiment in Cognition. Cybernetics and
Systems, 28, 457-470.
Meunier, M., Monfardini, E., & Boussaoud, D. (2007). Learning by observation in rhesus
monkeys.. Neurobiology of learning and memory, 88 (2), 2438.
Mitchell, T. (1997). Machine learning. McGraw-Hill Publishing Company, New York.
Quick, T., Dautenhahn, K., Nehaniv, C., & Roberts, G. (2000). The essence of embodiment:
A framework for understanding and exploiting structural coupling between system and
environment. In AIP conference proceedings, pp. 649660. Citeseer.
Ramachandran, V. (2000). Mirror neurons and imitation learning as the driving force behind
the great leap forward in human evolution. Edge Website article http://www. edge.
org/3rd\ culture/ramachandran/ramachandran\ p1. html.
Ramachandran, V. (2003). The emerging mind: the Reith Lectures 2003. Profile Books.
Rao, R. P. N., Shon, A. P., & Meltzoff, A. N. (2004). A Bayesian Model of Imitation
in Infants and Robots. In Imitation and Social Learning in Robots, Humans, and
Animals, 217-247.
Rizzolatti, G., Fadiga, L., & Gallese, V. (1996). Premotor cortex and the recognition of
motor actions. Cognitive brain research, 3 (2), 131141.
Sullivan, K. (2011). Multiagent Hierarchical Learning from Demonstration. International
Joint Conference on Artificial Intelligence, 28522853.
Sun, R. (2001). Introduction to Sequence Learning. Lecture Notes in Computer Science,
110.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. MIT Press, Cambridge, MA.
348

fiLearning by Observation of Agent Software Images

Tan, H. (2012). Implementation of a Framework for Imitation Learning on a Humanoid
Robot Using a Cognitive Architecture. In Zaier, D. R. (Ed.), The Future of Humanoid
Robots - Research and Applications, chap. 10. InTech.
Wood, M. A. (2008). An agent-independent task learning framework. Phd thesis, University
of Bath.

349

fiJournal of Artificial Intelligence Research 47 (2013) 71-93

Submitted 11/12; published 05/13

Identifying the Class of Maxi-Consistent Operators in Argumentation
Srdjan Vesic

VESIC @ CRIL . FR

CRIL - CNRS
Rue Jean Souvraz SP 18
F 62307 Lens Cedex
FRANCE

Abstract
Dungs abstract argumentation theory can be seen as a general framework for non-monotonic
reasoning. An important question is then: what is the class of logics that can be subsumed as
instantiations of this theory? The goal of this paper is to identify and study the large class of
logic-based instantiations of Dungs theory which correspond to the maxi-consistent operator, i.e. to
the function which returns maximal consistent subsets of an inconsistent knowledge base. In other
words, we study the class of instantiations where every extension of the argumentation system
corresponds to exactly one maximal consistent subset of the knowledge base. We show that an
attack relation belonging to this class must be conflict-dependent, must not be valid, must not be
conflict-complete, must not be symmetric etc. Then, we show that some attack relations serve as
lower or upper bounds of the class (e.g. if an attack relation contains canonical undercut then it is
not a member of this class). By using our results, we show for all existing attack relations whether
or not they belong to this class. We also define new attack relations which are members of this
class. Finally, we interpret our results and discuss more general questions, like: what is the added
value of argumentation in such a setting? We believe that this work is a first step towards achieving
our long-term goal, which is to better understand the role of argumentation and, particularly, the
expressivity of logic-based instantiations of Dung-style argumentation frameworks.

1. Introduction
A question whether Dungs (1995) abstract theory can be used as a general framework for nonmonotonic reasoning has drawn a particular amount of attention among researchers in artificial
intelligence. More precisely, the question is: can existing or new approaches to reasoning be seen
as instantiations of Dungs theory? This is certainly a very general question. Furthermore, different
approaches suppose that the available knowledge is represented in different form. This paper studies the problem setting when one is given a finite inconsistent set of classical propositional logic
formulae, which we refer to as a knowledge base. There are a number of approaches for dealing
with inconsistent information: a notable example are paraconsistent logics (Priest, 2002) where one
is able to draw some (but not all) conclusions from an inconsistent set of formulae. Indeed, each
paraconsistent logic allows for a subset of the inferences that could be obtained using classical logic
with the same knowledge. Other examples of dealing with inconsistent knowledge include belief
revision (Gardenfors, 1988), belief merging (Konieczny & Perez, 2011) or voting (Arrow, Sen, &
Suzumura, 2002). To be completely precise, note that there are approaches where one is given a
multiset instead of a set, for example where several voters can express their knowledge or preferences and the number of agents stating / voting for a proposition is important. However, in this
paper, we suppose that the information is represented in form of a set.
c
2013
AI Access Foundation. All rights reserved.

fiV ESIC

Generally speaking, we call an operator a function which provides a way to go from an inconsistent knowledge base to a set of subsets of that knowledge base. Examples of operators are: a
function returning maximal for set inclusion consistent subsets of a knowledge base, called maxiconsistent operator, a function returning maximal for cardinality consistent subsets of a knowledge
base, called maxi-card operator, a function returning all consistent subsets of a knowledge base...
To understand how and to which extent Dungs theory can be used as a general framework for
reasoning, it is essential to study the link between the result obtained by applying an operator to a
knowledge base  and the extensions of the argumentation framework F = (Arg(), R), where
for a set S  , we denote by Arg(S) the set of all arguments that can be built from S, and by R
the relation used for identifying attacks between arguments. There are papers (Cayrol, 1995; Caminada & Amgoud, 2007; Amgoud & Besnard, 2009, 2010; Amgoud & Vesic, 2010; Gorogiannis
& Hunter, 2011) studying the notions which are somehow related to a link between a knowledge
base and the corresponding argumentation framework. However, since the work of Dung (1995),
there are almost no papers studying the link between an operator and an instantiation of Dungs theory. Cayrol (1995) showed that the instantiation of Dungs theory using stable semantics and direct
undercut as attack relation, corresponds to maxi-consistent operator. In argumentation community,
this one-to-one correspondence is sometimes identified as a main objection against pure logic-based
argumentation, because the additional value of constructing the argumentation framework is then
said to be questionable (since computing the extensions does not do more than applying maxiconsistent operator). However, a recent work by Vesic and van der Torre (2012) shows that there
exists a large class of logic-based instantiations of Dungs abstract theory having two interesting
features: (i) it returns extensions that do not correspond to maximal consistent subsets of the initial
knowledge base, and (ii) its result satisfies basic argumentation postulates (Caminada & Amgoud,
2007), e.g. consistency, closure... That paper shows that the space of logic-based instantiations of
Dungs theory is much larger than it was believed.
The previous result makes the question what is the class of operators that can be viewed as
instantiations of Dungs theory? even more relevant as a research topic. We now aim only at
giving a broad overview of this class. First note that, interestingly, there is a rather big class of
instantiations of Dungs theory returning inconsistent results, as showed by Caminada and Amgoud
(2007). However, one would normally prefer to avoid this type of behaviour, and to study the class
of instantiations returning consistent results.1 Thus, our long-term goal is to identify the whole
class of instantiations of Dungs theory that yield a consistent result. However, that is certainly a
hard task. We start by noticing that, given a set , the most common and a well-known way to deal
with inconsistent information is to use the maxi-consistent operator, i.e. to select maximal consistent
subsets of . Also, we conjecture that one of the biggest2 sub-classes of the class of instantiations
returning a consistent result is the class of instantiations corresponding to maxi-consistent operator.
That is why our first goal, and the main goal of this paper, is to study this class. (Note that a more
general approach would consider the set of maxi-consistent subsets of  and a selection function
f among the maxi-consistent subsets. Thus, only some maxi-consistent sets would be used for
reasoning. However, the present paper studies the case when all the maxi-consistent subsets of 
are taken into account since this already captures a significant number of systems.)
1. Note that while there is no consensus regarding some of the postulates (e.g. indirect consistency), some postulates
(e.g. direct consistency) enjoy much wider acceptance.
2. informally, but in the sense: in number of known instantiations

72

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

More particularly, the paper aims to answering the following questions: What are the properties
of attack relations and semantics used in instantiations of Dungs theory that correspond to maxiconsistent operators? What are the necessary and sufficient conditions so that an attack relation
belongs to this class (under a given semantics)? What properties do they satisfy: must (not) they be
conflict-dependent, valid, symmetric, ... ? Can we identify sub-classes of attack relations belonging / not belonging to this class? Can we find the lower and / or the upper bound (in terms of set
inclusion) of this class of attack relations? Which existing (in the literature) attack relations belong
to this class under which semantics? Are there new attack relations belonging to this class?
The paper is organised as follows: Section 2 introduces the main notions of argumentation theory we use in the rest of the paper. Section 3 formally defines the class of instantiations corresponding to maxi-consistent operator. Section 4 shows what properties are satisfied by attack relations
belonging to this class. Section 5 identifies several classes of attack relations (not-)corresponding to
maxi-consistent operator. Section 6 shows for all (to the best of our knowledge) existing attack relations from literature whether or not they belong to this class, and also defines a new attack relation
which is a member of this class. The last section concludes and discusses related work.

2. Basics of Argumentation
As already mentioned, this paper supposes that one is given a set of classical propositional logic
formulae . We use the well-known (e.g. Besnard & Hunter, 2001; Amgoud & Cayrol, 2002;
Gorogiannis & Hunter, 2011) logic-based approach for instantiating Dungs theory. L denotes the
set of well-formed formulae, ` stands for classical entailment, and  for logical equivalence. We
use the notation MC() for the set of all maximal consistent subsets of .
A logical argument is defined as a pair (support, conclusion).
Definition 1 (Argument). An argument is a pair (, ) such that    is a minimal (for set
inclusion) consistent set of formulae such that  ` .
For an argument a = (, ), we use the function Supp(a) =  to denote its support and
Conc(a) =  to denote its conclusion.
Example 1. Let  = {,   , }. a = ({,   }, ), b = ({  },   ) and
c = ({, },   ) are some of the arguments that can be constructed from . For example,
Supp(a) = {,   } and Conc(a) = .
For a given set of formulae S, we denote by Arg(S) the set of arguments constructed from
S. Formally, Arg(S) = {a | a is an argument and Supp(a)  S}. Let Arg(L) denote the set of
all arguments that can be constructedSfrom the language of propositional logic. For a given set of
arguments E, we denote Base(E) = aE Supp(a). We suppose that function Arg is defined on L
and that function Base is defined on Arg(L); by slightly abusing the notation, we sometimes write
Arg (respectively Base) for the restriction of these functions on any set of formulae (respectively
arguments).
Definition 2 (Argumentation system). An argumentation system (AS) is a pair (A, R) where A 
Arg(L) is a set of arguments and R  A  A a binary relation. For each pair (a, b)  R, we say
that a attacks b. We also sometimes use notation aRb instead of (a, b)  R.
73

fiV ESIC

In order to simplify notation, we do not explicitly mention an argumentation system when it
is clear from the context which argumentation system we refer to. Since arguments are built from
formulae, we suppose that an attack relation is defined by specifying a condition such that for every
two arguments a and b, we have that a attacks b if and only if the condition from the definition of
attack relation is satisfied. For example, such a condition can be that the conclusion of a is logically
equivalent to the negation of the conclusion of b. We suppose that all attack relations are defined on
the set Arg(L)  Arg(L), and that for every set A  Arg(L), we use the restriction of the attack
relation on the set AA. That is why, in order to simplify notation, we simply write R for an attack
relation defined on the set Arg(L)  Arg(L) as well as for the restriction of that attack relation on
every set A  A, with A  Arg(L).
In order to determine mutually acceptable sets of arguments, different semantics have been
introduced in argumentation. We first introduce the basic notions of conflict-freeness and defence.
Definition 3 (Conflict-free, defence). Let F = (A, R) be an AS, E  A and a  A.
 E is conflict-free if and only if there exist no arguments a, b  E such that a R b
 E defends a if and only if for every b  A we have that if b R a then there exists c  E such
that c R b.
Let us now define the most commonly used semantics.
Definition 4 (Acceptability semantics). Let F = (A, R) be an AS and B  A. We say that a set B
is admissible if and only if it is conflict-free and defends all its elements.
 B is a complete extension if and only if B defends all its arguments and contains all the
arguments it defends.
 B is a preferred extension if and only if it is a maximal (with respect to set inclusion) admissible set.
 B is a stable extension if and only if B is conflict-free and for all a  A \ B, there exists b  B
such that b R a.
 B is a semi-stable extension if and only if B is a complete extension and the union of the set
B and the set of all arguments attacked by B is maximal (for set inclusion).
 B is a grounded extension if and only if B is a minimal (for set inclusion) complete extension.
 B is an ideal extension if and only if B is a maximal (for set inclusion) admissible set contained
in every preferred extension.
For an argumentation system F = (A, R) we denote by Extx (F); or, by a slight abuse of
notation, by Extx (A, R) the set of its extensions with respect to semantics x. We use abbreviations
c, p, s, ss, g and i for respectively complete, preferred, stable, semi-stable, grounded and ideal
semantics. For example, Extp (F) denotes the set of preferred extensions argumentation system F.
Example 2. Let F = (A, R) be an argumentation framework with A = {a, b, c, d} and R =
{(b, c), (c, b), (b, d), (c, d)}. There are two preferred/stable/semi-stable extensions: {a, b} and {a, c};
three complete extensions: {a}, {a, b} and {a, c}; and one grounded/ideal extension: {a}.
74

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

3. Defining the Problem Setting
Until now, we specified how to, from a knowledge base , construct an argumentation system F =
(Arg(), R), and then, using a chosen semantics, calculate extensions. Since all the components
of the system except a semantics and an attack relation are fixed, then whether an instantiation
corresponds to maxi-consistent operator depends exclusively on those two components. The next
definition provides a formal definition of what we mean by saying that an instantiation of Dungs
framework corresponds to maxi-consistent operator. The idea is that the function Arg should be
a bijection between MC() and the extensions of the corresponding argumentation system.
Definition 5 (MC  Ext). Let x be an argumentation semantics. We say that attack relation R
satisfies (MC  Extx ) if and only if for every finite set of propositional formulae  we have that
Arg is a bijection between MC() and Extx (Arg(), R)
This means that for every set S  MC(), it holds that Arg(S)  Ext(Arg(), R) and for
every E  Ext(Arg(), R), there exists S  MC() such that E = Arg(S). For example, we say
that an attack relation R satisfies (MC  Extc ) if and only if for every finite , we have that Arg
is a bijection between MC() and Extc (Arg(), R). Sometimes, when it is clear from the context
which semantics we refer to or when a semantics is not important, we use the simplified notation
(MC  Ext). We say that an attack relation R falsifies (MC  Extx ) if and only if R does not satisfy
(MC  Extx ). The following example shows an attack relation that does not satisfy (MC  Exts ).
Example 3. Consider the attack relation known as defeating rebut, denoted by Rdr and defined
as follows: for two arguments a and b, we say that a attacks b and write aRdr b if and only if
Conc(a) ` Conc(b). This attack relation falsifies (MC  Exts ). To see why, it is sufficient to find
a set of formulae  such that Arg is not a bijection between MC() and (Arg(), Rdr ). To that end,
consider  = {  ,   } and denote F = (Arg(), Rdr ). We see that MC() = {S1 , S2 }
with S1 = {  } and S2 = {  }. Denote E1 = Arg(S1 ) and E2 = Arg(S2 ). If Exts (F) 6=
{E1 , E2 } then Rdr does not satisfy (MC  Exts ). Consider argument a = ({  },   ),
and note that a  E1 . Observe that for every argument b  Arg(), we have that bRdr a if and
only if Conc(b) ` (  ). In other words, for every argument b  Arg(), b attacks a if and
only if Conc(b) `   . Recall that from Definition 1 we know that for every argument b,
Supp(b) ` Conc(b). Thus, for every argument b  Arg(), if bRdr a then Supp(b) `   .
Since  = {  ,   } then there is no argument b  Arg() such that Supp(b) `   .
Thus, argument a is not attacked by any argument of E2 . This means that E2 is not a stable extension
of F. Consequently, Arg is not a bijection between MC() and Exts (F). Hence, Rdr falsifies
(MC  Exts ).
3.1 Complete and Incomplete Systems
There are two ways to study the link between an instantiated argumentation system F (containing
arguments and attacks between them) and the corresponding knowledge base  (containing formulae). The first scenario is as follows:
 choose an attack relation R and a semantics x
 start with a finite knowledge base 
75

fiV ESIC

 consider the system F = (Arg(), R), containing all the arguments that can be built from 
 compare the result obtained by using an operator on  and the one obtained by calculating
the extensions of F
In this case, we say that the obtained argumentation system is complete. Every complete system
has an infinite number of arguments, but for every complete system F, there exists a finite system
F 0 such that F and F 0 are equivalent. How to formally define equivalence between argumentation
systems is not the topic of the present paper; for more details the reader is invited to consult the
literature on this subject (Amgoud, Besnard, & Vesic, 2011).
The second possibility is to do the converse:
 for a given attack relation R and semantics x,
 start with an argumentation system F = (A, R),
 define  as the set of all formulae used in the supports of arguments of F, that is, define
def
 = Base(A)
 compare the result obtained by using an operator on  and the one obtained by calculating
the extensions of F
The obtained argumentation system may be incomplete, in the sense that A =
6 Arg(Base(A)).
There is an important difference between those two scenarios. Namely, in the first case, all the
arguments that can be built from  are considered when calculating Extx (F). In the second case, 
contains all the formulae from A, but in F, not all formulae are equally represented. Let us illustrate
this situation.
Example 4. Let R be defined as: for every a, b  Arg(L), (a, b)  R if and only if there exists
  Supp(b) such that Conc(a)  . Let us use preferred semantics. Let F = (A, R) and
A = {a, b} with a = ({,   }, ) and b = ({}, ). In this case, since b attacks a and not
vice versa, the only extension is E = {b}. Note that the conclusion of the only accepted argument is
. However, if we take  to be the union of all formulae used in supports of the arguments of F,
we obtain  = {, ,   }. There are two maximal consistent subsets of this knowledge base:
MC() = {{,   }, {,   }}.
It is clear that in a setting similar to that in the previous example, or, more generally, in the
second scenario, one cannot expect Arg to be a bijection between MC() and Ext(A, R). But, what
does an incomplete argumentation system stand for? How is it obtained? To conclude that a system
such that A 6= Arg(Base(A)) is meaningless would certainly be too hasty. Let us consider this
question in more detail. Namely, we know that missing arguments can be added by an intelligent
agent. Should we first add all the missing arguments and then calculate the extensions of the
complete version of our system? There are two possible answers: (1) yes, we must add missing
arguments in order to take into account all available information; (2) no, since we are given an
argumentation system and not all arguments have been constructed (in case of monological argumentation) or uttered (in the case of dialogical argumentation). Both arguments (1) and (2) make
sense in different applications: the first possibility corresponds to a case when we want to simulate a
resource unbounded agent, and take into account all the information (where information is seen as
76

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

formulae) known by agent(s). Note that this has its disadvantages since by doing so, we ignore the
argumentational representation of the problem. The second possibility is to be used when we want
to know what the output of an argumentation system is, where we do take into account the fact that
not all arguments are constructed (e.g. because of the lack of computational resources, or since the
given argumentation framework is representing a dialogue in which not everything has been said).
Note that numerous works in the 1990s (Pollock, 1992; Vreeswijk, 1997; Loui, 1998) yield
conceptual and philosophical arguments supporting partial computation (i.e. incomplete systems).
An important part of Vreeswijks (1997) work is devoted to defining and constructing complete
argumentation systems. Loui (1998) discusses the philosophical difference between demonstrative
reasoning and non-demonstrative reasoning and claims that in a realistic (i.e. resource-bounded)
setting, not all reasons are demonstrative, and that process and disputation are essential to reasoning. Note, however, that none of those frameworks is an instantiation of Dungs system, and that
formalisations in those works differ a lot from the framework we studied in this paper. The goal of
the present paper is not to argue that complete systems are in any sense better than incomplete
ones (or vice versa), but only to study the possibilities and limits related to instantiating Dungs abstract theory. We will not further analyse the difference between complete and incomplete systems,
but we find it necessary to point out that they exist, in order to make the context of our research
question clear. In the second scenario, it is not reasonable to expect any correlation between the
result obtained directly from  and from F. That is why, in the rest of the paper we suppose the
first scenario.

4. Properties of Relations Satisfying (MCExt)
In this section, we analyse properties of attack relations satisfying (MC  Ext). We first show that
if this condition is satisfied, then the function Base : Ext(F)  MC() is the inverse function of
the function Arg : MC()  Ext(F).
Proposition 1. Let R be an attack relation and x an acceptability semantics. If relation R satisfies
(MC  Extx ) then:
 for every S  MC(), we have that S = Base(Arg(S)),
 for every E  Extx (F), we have that E = Arg(Base(E)).
Proof. Let  be a finite set of propositional formulae and let F = (Arg(), R).
 Let S  MC() and E = Arg(S). Since R satisfies (MC  Ext), then E  Extx (F). Let
S 0 = Base(E) and let us suppose that S 6= S 0 . Let us study two cases.
 Let S \ S 0 6=  and 0  S \ S 0 . This means that there is an argument a  E such that
0  Supp(a) with 0 
/ S, contradiction.
 Let S 0 \ S 6=  and 0  S 0 \ S. This means that there is an argument a  E such that
0  Supp(a). Contradiction with 0 
/ S.
Since S \ S 0 =  and S 0 \ S = , then S = S 0 ; In other words, Base(Arg(S)) = S.
 Let E  Extx (F) and S = Base(E). Since R satisfies (MC  Extx ), then there exists a
unique S 0  MC() such that Arg(S 0 ) = E. Let us prove that S = S 0 .
77

fiV ESIC

 Let us suppose that S \ S 0 6=  and let   S \ S 0 . This means that there is an argument
a  E such that   Supp(a). Contradiction with the fact  
/ S0.
 Suppose that S 0 \ S 6=  and that 0  S 0 \ S. From 0  S 0 , we conclude that there
exists a  E such that 0  Supp(a). Contradiction with the fact that 0 
/ S.
From S \ S 0 =  and S 0 \ S = , we conclude S = S 0 . Thus, Arg(Base(E)) = E.

Let us illustrate this result by the following example.
Example 5. Consider the attack relation known as direct undercut, denoted by Rdu and defined as
follows: for two arguments a and b, we say that a attacks b and we write aRdu b if and only if there
exists   Supp(b) such that Conc(a)  . It is known that direct undercut satisfies (MC  Exts )
(Cayrol, 1995). From Proposition 1, we see that for every , for every S  MC(), it holds that
S = Base(Arg(S)) and, more interestingly, that for every E  Exts (Arg(), Rdu ), we have that
E = Arg(Base(E)).
The previous result allows to easily show that if an attack relation satisfies (MC  Ext), then
every extension has a consistent base and the union of its arguments conclusions is consistent.
Corollary 1. Let R be an attack relation and x a semantics. Let R satisfy (MC  Extx ) and let 
be a finite set of formulae. Denote F = (Arg(), R). Then, for every E  Extx (F), we have:
 Base(E) is consistent
S
 aE Conc(a) is consistent
Proof. Let E  Extx (F). Since R satisfies (MC  Extx ), then there exists S  MC() such that
E = Arg(S). From Proposition 1, we obtain E = Arg(Base(E)). Since Arg is an injective function,
for every S 0  MC(), if E = Arg(S 0 ) then S = S 0 . Thus, S = Base(E). Consequently, Base(E) is
a consistent set. It is clear that
S for every argument a  E, we have that Base(E) ` Conc(a). Since
Base(E) is consistent, then aE Conc(a) is consistent as well.
Note that we can use the previous result to show that an attack relation does not satisfy (MC  Ext).
Namely, if an attack relation returns extensions having inconsistent bases, then it violates (MC  Ext).
Corollary 2. Let R be an attack relation, and x an acceptability semantics. If there exists a finite
knowledge base  such that there exists an extension E  Extx (Arg(), R) such that Base(E) is
inconsistent, then R does not satisfy (MC  Extx ).
4.1 On Conflict-Dependence and Validity
In this subsection, we study the link between satisfying (MC  Ext) and conflict-dependence and
validity. An attack relation is conflict-dependent if whenever an argument attacks another one, the
union of their supports is inconsistent (Amgoud & Besnard, 2009).
Definition 6 (Conflict-dependent). Let R  Arg(L)  Arg(L) be an attack relation. We say that R
is conflict-dependent if and only if for every a, b  Arg(L), if (a, b)  R then Supp(a)  Supp(b) `
.
78

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

We now prove that conflict-dependence is a necessary condition for satisfying (MC  Ext). To
be completely precise, we here specify that we say that a semantics x returns conflict-free sets if
and only if for every argumentation system (A, R), for every E  Extx (A, R), it holds that E is
conflict-free with respect to R. All the semantics from Definition 4 return conflict-free sets.
Proposition 2. Let R be an attack relation and x a semantics returning conflict-free sets. If R
satisfies (MC  Extx ), then R is conflict-dependent.
Proof. Let us suppose the contrary, i.e. let R be an attack relation that is not conflict-dependent, let
 be a knowledge base and let a, b  Arg() with aRb and Supp(a)  Supp(b) being consistent.
Thus, there exists a set S  MC() such that Supp(a)Supp(b)  S. Since R satisfies (MC  Extx )
then E = Arg(S) is an extension of the corresponding argumentation system F = (Arg(), R).
This means that a, b  E. Contradiction with the assumption that x returns conflict-free extensions.
Thus, R must be conflict-dependent.
Having proved this, we know that a relation satisfying (MC  Ext) enjoys all the properties
of conflict-dependent relations. For example, it was shown that if an attack relation is conflictdependent, then there are no self-attacking arguments (Amgoud & Besnard, 2009).
Corollary 3. Let R be an attack relation and x a semantics returning conflict-free sets. If R satisfies
(MC  Extx ) then for every argument a  Arg(L), we have that such that (a, a) 
/ R.
Proof. From Proposition 2, we have that R is conflict-dependent. Then, there are no self-attacking
arguments (Amgoud & Besnard, 2009, Prop. 4).
This means that we have another way to identify (some of the) attack relations not satisfying
(MC  Ext): namely, if for an attack relation there exists a self-attacking argument, then the given
attack relation falsifies (MC  Ext) for all semantics returning conflict-free sets. Let us now study
the notion of validity (Amgoud & Besnard, 2010).
Definition 7 (Valid). Let R  Arg(L)  Arg(L) be an attack relation. We say that R is valid if
and only if for every E  Arg(L) it holds that if E is conflict-free, then Base(E) is consistent.
Let us now show that this property is incompatible with conflict-dependence.
Proposition 3. There exists no attack relation which is both conflict-dependent and valid.
Proof. Let R be an attack relation and suppose that R is both conflict-dependent and valid. Let
a = ({}, ), b = ({}, ), c = ({  },   ) and let E = {a, b, c}. Since R is
valid and Base(E) is inconsistent, then E is not conflict-free. Since R is conflict-dependent, then
(a, b) 
/ R, (b, a) 
/ R, (a, c) 
/ R, (c, a) 
/ R, (b, c) 
/ R, (c, b) 
/ R. Thus, E is conflict-free.
Contradiction.
This means that if an attack relation R satisfies (MC  Ext) then there must exist a set E which
is conflict-free with respect to R but whose base is inconsistent.
Corollary 4. Let R be an attack relation and x an acceptability semantics returning conflict-free
sets and let R satisfy (MC  Extx ). Then, R is not valid.
79

fiV ESIC

The proof of the previous fact is a consequence of Proposition 2 and Proposition 3. It is useful
since if an attack relation is valid, we can immediately conclude that it violates (MC  Extx ) for all
(possible) semantics returning conflict-free sets.
On the more general level, we see that asking for every conflict-free set to have a consistent base
is very demanding. Roughly speaking, this is due the fact that attacks are binary whereas minimal
conflicts may be ternary (or of a greater cardinality). Some authors argue that to obtain a consistent
result, one should concentrate on admissibility and not on conflict-freeness. For example, Caminada and Vesic (2012) claim that n-ary attacks, for n  3, are simulated in Dungs framework
throughout the notion of admissibility. Thus, an idea for future work could be to study an alternative
condition, which is that every admissible set has a consistent base.
4.2 Satisfying (MCExt) and Different Acceptability Semantics
In this subsection, we study the properties related to particular semantics. We show that if an attack
relation satisfies (MC  Ext) for stable semantics, then it satisfies it for semi-stable semantics also.
Then we identify conditions under which an attack relation satisfies (MC  Ext) for stable semantics. We provide a similar result for preferred semantics. We also identify a sufficient condition so
that an attack relation falsifies (MC  Ext) under complete semantics. Then, we discuss the case of
single-extension semantics, like grounded and ideal.
First, suppose that R satisfies (MC  Exts ). This means that for every finite set of formulae ,
function Arg is a bijection between MC() and Exts (Arg(), R). Since every finite set of formulae
has at least one maximal consistent subset (even if that is the empty set) then for every , it must
be that (Arg(), R) has at least one stable extension. Since there are stable extensions, then stable
and semi-stable semantics coincide (Caminada, 2006). Thus, we obtain the following proposition.
Proposition 4. Let R be an attack relation. If R satisfies (MC  Exts ) then:
 for every finite set of formulae  and F = (Arg(), R), we have that Exts (F) = Extss (F)
 R satisfies (MC  Extss ).
Let us now prove that in the case of stable semantics, if the image with respect to Arg of every
maximal consistent set is an extension and if the base of every extension is consistent, then the
attack relation in question satisfies (MC  Ext).
Proposition 5. Let R be an attack relation. If for every set of formulae  and F = (Arg(), R),
we have:
 for all S  MC(), Arg(S)  Exts (F), and
 for all E  Exts (F), Base(E) is consistent
then R satisfies (MC  Exts ).
Proof. Let us prove that R satisfies (MC  Exts ). We already know that for every S  MC(),
Arg(S)  Exts (F). Let us suppose that E  Exts (F) and let us prove that there exists a unique set
S  MC() such that Arg(S) = E. If we prove that such a set exists, then uniqueness is guaranteed
since for S, S 0  , if S 6= S 0 then Arg(S) 6= Arg(S 0 ) trivially holds. Thus, let us prove that there
exists S  MC() such that Arg(S) = E. Let S 0 = Base(E) and let us prove that S 0  MC()
80

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

and Arg(S 0 ) = E. By means of contradiction, suppose that S 0 is a consistent but not maximal
consistent set. Then, there exists S 00  MC() such that S 0  S 00 . From the assumptions of this
proposition, we have that E 00 = Arg(S 00 ) is a stable extension of F. But we also have E ( E 00 . Since
no stable extension is a proper subset of another stable extension, then E is not a stable extension.
Contradiction. Thus, it must be that S 0  MC(). It is easy to see that E  Arg(Base(E)) (namely,
for every set of arguments, by applying the function Arg on its base, we obtain its superset). Let
us prove Arg(S 0 ) = E. Suppose the contrary. Then, E ( Arg(S 0 ). Since S 0  MC() then
Arg(S 0 )  Exts (F). Thus, E is not a stable extension (since no stable extension is a proper subset
of another stable extension).
We prove that similar two conditions are sufficient to guarantee that R satisfies (MC  Ext)
under preferred semantics.
Proposition 6. Let R be an attack relation. If for every set of formulae  and F = (Arg(), R),
we have:
 for all S  MC(), Arg(S)  Extp (F), and
 for all E  Extp (F), Base(E) is consistent
then R satisfies (MC  Extp ).
Proof of this property is similar to the proof of Proposition 5.
As a consequence of the two previous results, we can identify a sufficient condition so that R
satisfies both (MC  Exts ) and (MC  Extp ).
Corollary 5. Let R be an attack relation. If for every set of formulae  and F = (Arg(), R), we
have:
 for all S  MC(), Arg(S)  Exts (F), and
 for all E  Extp (F), Base(E) is consistent
then R satisfies both (MC  Exts ) and (MC  Extp ).
Proof. Since every stable extension is a preferred one (Dung, 1995), it is clear that R satisfies both
conditions of Proposition 5 and Proposition 6. By applying those propositions, we have that R
satisfies (MC  Exts ) and (MC  Extp ).
Let us now show that if an attack relation returns a stable extension having an inconsistent base,
then it violates (MC  Ext) for stable, semi-stable, preferred and complete semantics.
Proposition 7. Let R be an attack relation. If there exists a finite set of formulae  such that
F = (Arg(), R) has a stable extension E such that Base(E) is inconsistent, then R falsifies
(MC  Extx ) for x  {s, ss, p, c}.
Proof. We supposed that there exists a stable extension E  Exts (F) such that Base(E) ` . It
has been proved (Dung, 1995) that every stable extension is a preferred and a complete one. We also
know (Caminada, 2006) that E must be a semi-stable extension. By using Corollary 2, we conclude
that R does not satisfy (MC  Ext) for stable, semi-stable, preferred and complete semantics.
81

fiV ESIC

Let us now study the case of complete semantics. We show that it is not possible for an attack
relation to satisfy (MC  Extc ). The only condition we use in our result is that for every argument
a, if a has a formula  in its support, and   , then there exists an argument b  Arg() such
that b attacks a.
Proposition 8. Let R be an attack relation such that for every finite set of formulae , for every
a  Arg(), for every   Supp(a), if there exists    such that    then there exists
b  Arg() such that (b, a)  R. Then, R does not satisfy (MC  Extc ).
Proof. We prove that if an attack relation R satisfies the condition from this proposition, then it
falsifies (MC  Extc ). We use the proof by contradiction. In other words, the plan of the proof is as
follows: first, we suppose that R satisfies the given condition. Second, by means of contradiction,
we suppose that R satisfies (MC  Extc ). Third, we draw conclusions and obtain a contradiction.
Fourth, by reductio ad absurdum, we conclude that it must be that R falsifies (MC  Extc ).
So, let us start by supposing the condition from the proposition and suppose that R satisfies
(MC  Extc ). Thus, from Proposition 2, we obtain the R is conflict-dependent. Since R satisfies
(MC  Extc ), then for every , Arg is a bijection between MC() and Extc (Arg(), R). Consider
 = {, , } and denote F = (Arg(), R). It is clear that MC() = {S1 , S2 } with S1 = {, }
and S2 = {, }. Since R satisfies (MC  Extc ) then Extc (F) = {E1 , E2 } with E1 = Arg(S1 )
and E2 = Arg(S2 ).
Let us now obtain a contradiction by proving that E3 = Arg({}) is a complete extension. First,
prove that this set is conflict-free. Let a, b  E3 . Since R is conflict-dependent, then (a, b) 
/ R.
Thus, E3 is conflict-free.
Let us now prove that for all a  E3 , for all b  Arg() \ E3 , we have that (a, b) 
/ R and
(b, a) 
/ R. By means of contradiction, suppose the contrary. Again from conflict-dependence, we
have that Supp(a)  Supp(b) ` . It must be that {, }  Supp(a)  Supp(b). Since the support
of every argument is consistent, then Supp(a) contains either  or . Contradiction with the fact
a  Arg({}). Thus, E3 is an admissible set.
Let us now prove that E3 does not defend any arguments in Arg() \ E3 . To show this, we only
need to prove that every argument in Arg() \ E3 is attacked by at least one argument. Note that
for every a  Arg() \ E3 , it holds that a  E1 \ E2 or a  E2 \ E1 . Without loss of generality,
let a  E1 \ E2 . Let us prove that a is attacked. Note that in every argumentation system, every
non-attacked argument is in all complete extensions. Since a 
/ E2 , then a must be attacked. To sum
up:
 E3 is an admissible set
 E3 does not attack any argument in Arg() \ E3
 Arg() \ E3 does not attack any argument in E3
 every argument in Arg() \ E3 is attacked by at least one argument.
Thus, E3 is a complete extension. Contradiction with the claim that Extc (F) = {E1 , E2 }. By
reductio ad absurdum, we conclude that R does not satisfy (MC  Extc ).
What about the semantics which always return a unique extension, like grounded and ideal
semantics? In such a case, it is not reasonable to expect that there is a bijection between MC() and
82

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

the set of extensions, since there can be several maximal consistent subsets of . Let us formally
state this fact.
Proposition 9. If x is a semantics such that for every argumentation system F we have |Extx (F)| =
1 then there is no attack relation R which satisfies (MC  Extx ).
Proof. Let  = {, , }. Denote F = (Arg(), R). There are two maximal consistent subsets
of , i.e. |MC()| = 2. Since we supposed that every argumentation system has exactly one
extension under semantics x, then there is no bijection between MC() and Extx (F).
The previous simple result is not surprising. The idea between those semantics is to have one
extension that contains all the arguments that should be accepted according to every point of view.
Thus, we can expect a link between the set of formulae not belonging to any minimal inconsistent
set and those extensions. Note that the sufficient conditions for R were identified (Gorogiannis
& Hunter, 2011) so that for every finite set  and F = (Arg(), R) we have that the grounded
and the ideal semantics coincide and that the extension is exactly Arg( \ (1  . . .  k )) where
{1 , . . . , k } is the set of all minimal (for set inclusion) inconsistent subsets of .

5. Identifying Classes of Attack Relations (Not-)Satisfying (MCExt)
The previous sections show how to identify properties that an attack relation satisfying (MC  Ext)
must satisfy. They also provide several results closely related to the choice of a specific acceptability semantics. In this section, we identify classes of attack relations which satisfy, do not satisfy
(MC  Ext), or serve as lower (upper) bounds (with respect to set inclusion) for (non-)satisfying
(MC  Ext).
We first show that the whole class of symmetric attack relations violates (MC  Ext) for all
semantics from Definition 4.
Proposition 10. If R is a symmetric attack relation, then for every x  {s, ss, p, c, g, i}, R falsifies
(MC  Extx ).
Proof. From Proposition 9, we see that R violates (MC  Extg ) and (MC  Exti ). Let us now
prove the same for other acceptability semantics.
If R is a symmetric attack relation, then every conflict-free set is admissible. Furthermore, it is
easy to see that in this case every maximal conflict-free set is a stable extension (and vice versa).
Since every finite argumentation system has at least one maximal conflict-free set, then every finite
argumentation system using a symmetric attack relation has at least one stable extension. Let R be
a symmetric relation and suppose that for at least one x  {s, ss, p, c}, R satisfies (MC  Extx ).
From Corollary 4, we conclude that R is not valid. This means that there exists a finite propositional
knowledge base  and F = (Arg(), R) such that there is a conflict-free set E  Arg() having
an inconsistent base. Let E 0  Arg() be a maximal conflict-free set containing E, i.e. such that
E  E 0 . Since E 0 is a maximal conflict-free set, then it is a stable extension of F. Since E 0 is a
stable extension, then it is also a semi-stable, preferred and a complete one. Since Base(E 0 ) ` 
then Corollary 2 implies that for all x  {s, ss, p, c}, R fails to satisfy (MC  Extx ).
We now identify another class of attack relations that do not satisfy (MC  Ext). Namely, we
show that every (possible) attack generating too many attacks falsifies (MC  Ext). First, we
83

fiV ESIC

need to formally define what we mean by too many attacks. We do this by introducing the notion
of conflict-completeness.
Definition 8 (Conflict-complete). Let R  Arg(L)  Arg(L) be an attack relation. We say that
R is conflict-complete if and only if for every minimal conflict C  L (i.e. for every inconsistent
set whose every proper subset is consistent), for every C1 , C2  C such that C1 6= , C2 6= ,
C1  C2 = C, for every argument a1 such that Supp(a1 ) = C1 , there exists an argument a2 such
that Supp(a2 ) = C2 and (a2 , a1 )  R.
Intuitively, an attack relation is conflict-complete if when two sets form a minimal conflict, then
every argument built from one of the two sets can be attacked by an argument from the other set.
This notion is inspired by the desire to describe properties of a class of existing (and new) attack
relations. For example, canonical undercut is conflict-complete.
We can show that if an attack relation is conflict-complete, then it falsifies (MC  Ext) for stable,
semi-stable, preferred and complete semantics.
Proposition 11. Let R be an attack relation. If R is conflict-complete then R does not satisfy
(MC  Extx ) for x  {s, ss, p, c}.
Proof. Let R be a conflict-complete attack relation and let us use the proof by contradiction. Thus,
suppose that there exist x  {s, ss, p, c} such that R satisfies (MC  Extx ) and obtain a contradiction. From Proposition 2, we have that R is conflict-dependent. Let  = {,   , }. Let
F = (Arg(), R) and E = Arg({})  Arg({  })  Arg({}). We prove that E is a stable
extension of F. First, prove that E is conflict-free. Let a, b  E and suppose (a, b)  R. From
conflict-dependence, we obtain Supp(a)  Supp(b) ` . Contradiction with the definition of E,
since there are no two arguments of E such that the union of their supports is inconsistent. Now,
prove that E attacks every argument in Arg() \ E. Let a0  Arg() \ E. There are three cases.
Case 1: Supp(a0 ) = {,   }. In this case, since R is conflict-complete, then a0 is attacked by
at least one argument from the set Arg(). Case 2: Supp(a0 ) = {, }. Again from conflictcompleteness, such an argument is attacked by an argument from the set Arg({  }). Case 3:
Supp(a0 ) = {  , } is also similar, since a0 is then attacked by an argument having support
{}. We conclude that E  Exts (F). It is easy to see that Base(E) ` . Proposition 7 now implies
that R does not satisfy (MC  Extx ) for every x  {s, ss, p, c}. Contradiction.
The previous part of this paper studies classes of attack relations. Let us now define
some
V
particular cases of attack relations. If  = {1 , . . . , k } is a set of formulae, notation  stands
for 1  . . .  k .
Definition 9 (Attack relations). Let a, b  Arg(L). We define the following attack relations:
V
 defeat: aRd b if and only if Conc(a) ` Supp(b)
 direct defeat: aRdd b if and only if there exists   Supp(b) such that Conc(a) ` 
V
 undercut: aRu b if and only if there exists   Supp(b) such that Conc(a)   
 direct undercut: aRdu b if and only if there exists   Supp(b) such that Conc(a)  
V
 canonical undercut: aRcu b if and only if Conc(a)   Supp(b)
84

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

 rebut: aRr b if and only if Conc(a)  Conc(b)
 defeating rebut: aRdr b if and only if Conc(a) ` Conc(b)
 conflicting attack: aRc b if and only if Supp(a)  Supp(b) ` 
 rebut + direct undercut: aRrdu b if and only if aRr b or aRdu b
 big argument attack: aRba b if and only if there exists   Supp(b) such that Supp(a) ` .
The first seven items from the previous definition list are, to the best of our knowledge, all the
attack relations used in the logic-based argumentation literature. Finding the exact paper in which
each of them occurs for the first time would be quite a challenging task. We can say that rebut was
defined by Pollock (1987, 1992). Direct undercut was introduced in the work of Elvang-Gransson,
Fox, and Krause (1993) and Elvang-Gransson and Hunter (1995). Undercut and canonical undercut were defined in this form by Besnard and Hunter (2000, 2001). To the best of our knowledge,
conflicting attack was not used in the argumentation literature. A possibility to use such a relation
was mentioned (Besnard & Hunter, 2008, p. 35). We show that it is not enough to capture the
presence of inconsistency to make a good attack relation. Namely, we show later that this attack
relation may return inconsistent extensions. Rebut + direct undercut is added by the author of the
present paper, as an attempt to investigate the possibility to use rebut to detect some conflicts not
detected by direct undercut, but to avoid using a symmetric relation (rebut). The name big argument
attack and the idea behind this attack relation are due to L. van der Torre (personal communication,
June 18, 2012). This attack relation was coined with the goal to show that there are reasonable attack relations not taking into account the conclusion of an argument. We later show (Proposition 16)
that this attack relation also satisfies (MC  Ext). (The idea behind the name of this attack relation
is that it is sufficient to use only one argument per support since the conclusions are not important.
Those arguments are called big since one big argument plays a role of a whole class of normal
arguments, i.e. all the arguments having the same support. The attack relation is called big since it
is to be used between big arguments.)
The reader can easily check that canonical undercut is conflict-complete, which leads to the
conclusion that every attack relation containing canonical undercut (in the set-theoretic sense) is
also conflict-complete.
Proposition 12. Let R  Arg(L)  Arg(L) be an attack relation. If Rcu  R then R is conflictcomplete.
Thus, from Proposition 11, we conclude that every attack relation containing canonical undercut
falsifies (MC  Ext) for stable, semi-stable, preferred and complete semantics.
Corollary 6. Let R be an attack relation. If Rcu  R, then R does not satisfy (MC  Extx ) for
x  {s, ss, p, c}.
Since Rcu  Ru  Rd  Rc , then we obtain that as soon as an attack relation R contains Ru ,
or Rd or Rc then it falsifies (MC  Ext) for stable, semi-stable, preferred and complete semantics.
Corollary 7. Let R be an attack relation. If Ru  R, or Rd  R or Rc  R then R falsifies
(MC  Extx ) for x  {s, ss, p, c}.
85

fiV ESIC

Hence, there is a whole class of attack relations based on undercutting which do not satisfy
(MC  Ext). We also identified another class of attack relations, this time based on rebutting,
which do not satisfy (MC  Exts ). Namely, every attack relation contained in defeating rebut must
falsify (MC  Exts ). Observe how the proof of the following proposition is based on the idea from
Example 3.
Proposition 13. Let R be an attack relation. If R  Rdr then R does not satisfy (MC  Exts ).
Proof. Let us suppose the contrary, i.e. let R satisfy (MC  Exts ). Let  = {  ,   } and
F = (Arg(), R). We have that MC() = {S1 , S2 }, with S1 = {  } and S2 = {  }. Thus,
it must be that Exts (F) = {E1 , E2 } with E1 = Arg(S1 ) and E2 = Arg(S2 ). It is obvious that for an
argument a1 = ({  },   ) we must have a1  E1 . Since E2 is a stable extension, then there
must exist an argument a2  E2 such that (a2 , a1 )  R. Thus, it must be that Conc(a2 ) `   .
Consequently, Conc(a2 ) ` . Recall that Supp(a2 ) = {  } or Supp(a2 ) = {  }.
Contradiction.
Since Rr  Rdr then the previous conclusion holds for every relation contained in Rr .
Corollary 8. Let R be an attack relation. If R  Rr then R does not satisfy (MC  Exts ).
Proof. Let R  Rr . Since Rr  Rdr , then R  Rdr . From Proposition 13, R falsifies
(MC  Exts ).

6. Particular Attack Relations and (MCExt)
In the previous section, we identified classes of relations which do not satisfy (MC  Ext). In this
section, we examine in detail all the attack relations from Definition 9.
By using the results presented until now, we prove that direct undercut, direct defeat and big
argument attack satisfy (MC  Ext) for stable, semi-stable and preferred semantics, and falsify it
for other semantics, whereas other attack relations fail to satisfy (MC  Ext) for any semantics.
Note that it has been proved (Cayrol, 1995) that direct undercut satisfies (MC  Ext) in the case
of stable semantics. From Proposition 4, we conclude that direct undercut satisfies (MC  Ext)
for semi-stable semantics. So, we only need to prove that Rdu satisfies (MC  Ext) in the case of
preferred semantics.
Proposition 14. Attack relation Rdu satisfies (MC  Extx ) for x  {s, ss, p}.
Proof. We have already seen why Rdu satisfies (MC  Ext) under stable and semi-stable semantics. We now study the case of preferred semantics. Let  be a finite set of formulae and F =
(Arg(), Rdu ). Since it was already proved (Cayrol, 1995) that stable extensions of F are exactly
Arg(S), when S ranges over MC(), and since every stable extension is a preferred one, then it
is clear that for every S  MC() we have that Arg(S) is a preferred extension of F. Thanks to
Proposition 6, we now only need to prove that the base of every preferred extension is consistent.
This result follows from Prop. 34 by Gorogiannis and Hunter (2011), since relation Rdu satisfies
all the conditions of that proposition. Thus, direct undercut satisfies (MC  Extp ).
Example 6. Consider a relation ; for inferring from an inconsistent knowledge base defined as
follows: given a set , we write  ;  if and only if for every maximal consistent subset S of , it
86

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

holds that S ` , where ` stands for classical entailment. Now, consider an argumentation system
using direct undercut as attack relation and stable semantics. From Proposition 14, we conclude
that for every , Arg is a bijection between MC() and Exts (Arg(), Rdu ). Roughly speaking, this
means that the set of formulae that can be inferred from  with respect to ; is equal to the set of
formulae that are conclusions of all the extensions of the corresponding argumentation framework
based on direct undercut and stable semantics. More formally: for every   L, for every formula
  L we have that:  ;  if and only if for every extension E  Exts (Arg(), Rdu ), there exists
a  E such that  = Conc(a).
Let us now show that Rdd also satisfies (MC  Ext) for stable, semi-stable and preferred semantics.
Proposition 15. Attack relation Rdd satisfies (MC  Extx ) for x  {s, ss, p}.
Proof. Let  be a finite knowledge base and F = (Arg(), Rdd ). Let S  MC(). It is easy to
see that E = Arg(S) is a stable extension of F. Namely, E is conflict-free since Rdd is conflictdependent. Furthermore, for every argument a0  Arg() \ E, it must be that Supp(a0 ) contains at
least one formulae    \ S. From this fact, it is easy to conclude that there exists an argument
a  E such that Supp(a)  S and Conc(a)   (since S is a maximal consistent set). Thus,
a attacks a0 which ends this part of the proof and shows why E  Exts (F). Since every stable
extension is a semi-stable and a preferred one, then E  Extss (F) and E  Extp (F). Let us now
suppose that E is a preferred extension of F. Since direct defeat satisfies conditions of Prop. 34 by
Gorogiannis and Hunter (2011), then we conclude that Base(E) is consistent. From Corollary 5,
we conclude that Rdd satisfies (MC  Ext) for stable and preferred semantics. Now, Proposition 4
implies that Rdd also satisfies (MC  Extss ).
We now show that it is not necessary to look at conclusions of arguments in order to satisfy
(MC  Ext). Namely, we can show that big argument attack satisfies (MC  Ext) for stable, semistable and preferred semantics.
Proposition 16. Attack relation Rba satisfies (MC  Extx ) for x  {s, ss, p}.
Proof. Let us first show that for every S  MC(), it holds that E = Arg(S) is a stable extension in
F = (Arg(), Rba ). Since Rba is conflict-dependent, then E is conflict-free. Let a0  Arg() \ E
and let us prove that there exists a  E such that aRba a0 . Since a0 
/ E, then there exists  
Supp(a0 ) such that  
/ S. Since S is a maximal consistent subset of , then S ` . Let S 0  S
be a minimal with respect to set inclusion consistent set such that S 0 `  (such a set exists since
S is consistent) and let a = (S 0 , ). a is an argument since S 0 is a minimal consistent set from
which  can be deduced. We see that (a, a0 )  Rba . This means that the image with respect to Arg
of every maximal consistent subset of  is a stable extension of F. Thus, it is also a semi-stable
and a preferred extension of F. Let us now prove that for every  and the corresponding F =
(Arg(), Rba ), the base of every preferred extension E of F is a consistent set. Let E  Extp (F)
and S = Base(E). Aiming to a contradiction, suppose the contrary, i.e. let S be an inconsistent set.
Let S 0  S be a minimal (with respect to set inclusion) inconsistent set. Denote S 0 = {1 , . . . , n }.
Let a  E be an argument such that n  Supp(a), and let a0 = (S 0 \ {n }, n ). It is clear that
(a0 , a)  Rba . Since E is a preferred extension, it is conflict-free, thus a0 
/ E. Furthermore, E
is admissible, so there must exist b  E such that (b, a)  Rba . Since (b, a)  Rba , then there
87

fiV ESIC

exists i  {1, . . . , n  1} such that Supp(b) ` i . Since i  S 0 , then there exists an argument
c  E such that i  Supp(c). According to the definition of Rba , that would mean that b attacks
c. Contradiction with the fact that E is conflict-free. So, S must be a consistent set. This shows that
for every  and the corresponding F = (Arg(), Rba ), the base of every preferred extension E of
F is a consistent set. From Corollary 5, we conclude that Rba satisfies (MC  Ext) for stable and
preferred semantics. Now, Proposition 4 implies that Rba also satisfies (MC  Extss ).
We already know that no relation satisfies (MC  Ext) for the grounded or ideal semantics. By
using Proposition 8, it is easy to conclude that Rdu , Rdd and Rba falsify (MC  Extc ).
Let us now prove that the remaining attack relations from Definition 9 do not satisfy (MC  Ext)
for neither of semantics from Definition 4.
Proposition 17. Attack relations Rd , Ru , Rcu , Rr , Rdr , Rrdu , Rc falsify (MC  Ext) for stable,
semi-stable, preferred, complete, grounded and ideal semantics.
Proof. Note that we already showed that no attack relation satisfies (MC  Ext) for grounded or
ideal semantics. So, in the rest of the proof, we only need to consider stable, semi-stable, preferred
and complete semantics.
Let us first consider the attack relations Rcu , Ru , Rd and Rc . By using Proposition 11, we
conclude that those relations violate (MC  Ext) for stable, semi-stable, preferred and complete
semantics.
It is obvious that relations Rr and Rc are symmetric. Note that Rdr is also symmetric: this
comes from the fact that  `  if and only if ,  `  if and only if  ` . Thus, Proposition
10 yields a conclusion that they do not satisfy (MC  Ext) for neither of the considered acceptability
semantics.
Let us now study the relation Rrdu . Let  = {,   , } and F = (Arg(), Rrdu ). Let
us define a set E of arguments as follows: E = {a  Arg() | Conc(a) 6  and Conc(a) 6
(  ) and Conc(a) 6 }.
Prove that E is conflict-free. Let a, b  E and let aRrdu b. Whether aRr b or aRdu b is not
important, since in both cases, we obtain Supp(a)  Supp(b) ` . Contradiction, since there are no
two formulae in  whose union is an inconsistent set. So E is a conflict-free set.
Suppose that a0  Arg() \ E. So, Conc(a0 )   or Conc(a0 )  (  ) or Conc(a0 )  .
In any of those cases, a0 is attacked by at least one argument from E, namely by ({}, ), or by
({  },   ), or by ({}, ). So, E is a stable extension, and consequently, semi-stable,
preferred and complete extension. It is obvious that Base(E) is an inconsistent set, so by Corollary 2
we conclude that Rrdu does not satisfy (MC  Ext) for stable, semi-stable, preferred and complete
semantics.

7. Discussion, Related and Future Work
This paper identified and studied the large class of instantiations of Dungs abstract theory corresponding to the maxi-consistent operator. In other words, we studied the instantiations where every
extension of the argumentation system corresponds to exactly one maximal consistent subset of the
knowledge base. We proved properties of attack relations belonging to this class: they must be
conflict-dependent, must not be valid, must not be conflict-complete, must not be symmetric etc.
We also identified some attack relations serving as lower or upper bounds of the class. By using our
88

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

results, we showed for all existing attack relations from the argumentation literature whether or not
they belong to this class. We also showed for the first time that an attack relation not depending on
arguments conclusions can return reasonable results. Furthermore, we showed that such a relation
is a member of (MC  Ext) class.
Practical benefits of the work reported in this paper, and more generally, any work devoted to
studying the link between a class of instantiations of Dungs theory and an operator, can be resumed
as follows.
(I) A case when an instantiation of Dungs theory is shown to correspond to an existing operator.
First, such a work can help to validate an argumentation-based approach by showing in which
cases it returns a result comparable with that of a non argumentation-based approach. The possible
criticism of such an instantiation is that it is useless, since one can obtain the same result without
using argumentation. But, this is far from being true; namely, argumentation can be used for explanatory purposes. For example, if one wants to know why a certain conclusion is accepted, an
argument having that conclusion can be presented. That argument can be attacked by other arguments and so on. Also, it might be possible to construct only a part of the argumentation graph
related to the argument in question, thus having a better knowledge representation (i.e. ignoring the
parts of the knowledge base unrelated to the argument one wants to concentrate on).
The second benefit of this type of work is that it can help to reduce computational complexity
by using the simpler approach in the cases when the result obtained by an argumentation-based approaches and a non argumentation-based approaches is the same. Please note that the work in this
category (capturing an operator with an instantiation of Dungs theory) is far from being limited to
the case of the maxi-consistent operator, as it was shown by Vesic and van der Torre (2012) that
there exists a large class of instantiations of the abstract argumentation theory returning a consistent
result substantially different from the one returned by the maxi-consistent operator.
(II) A case when an instantiation of Dungs theory does not correspond to any existing operator.
Working on the links between instantiations of Dungs theory and operators can be even more
beneficial in the case when an instantiation of the abstract argumentation theory does not corresponding to any known operator happens to be found. We distinguish three possible situations.
(a) A case when an instantiation calculates a useful result which can be obtained by an operator, but that operator was unknown until now. In such a case, a new operator is discovered thanks to
argumentation. The question is then, in which situations to use argumentative approach, and when
to apply the operator? The answer depends on the balance between the need for computational
efficiency (which we conjecture is often on the side of the approach directly applying the operator)
and the need to represent knowledge in a format that is easy to grasp, argue and justify an accepted
piece of knowledge, which are the usual advantages of argumentation.
(b) A case when an instantiation of Dungs abstract theory returns a useful result which cannot
be obtained by any operator. Recall that an operator is a function that, for every finite knowledge
base, returns a set of its subsets. But, an argumentative approach could return a result that cannot
be represented in that form, for instance, if an argument (, ) is in an extension, whereas (, ) is
not, with  6= . Thus, the expressive power of the operator-based approach might be not enough
to distinguish those subtleties. A very important question of how to define such an instantiation is
still open. Another relevant issue is to see in which context such instantiations make sense and how
they can be applied.
89

fiV ESIC

(c) A case when an instantiation returns a bad result. This class regroups a set of instantiations
representing a behaviour one would like to avoid. The general question: how to distinguish useful
from bad instantiations? is certainly a hard one. Apart from a scientific debate, evaluation can
include tests on a set of benchmark examples. Note that the limits of testing a reasoning formalism
on a set of benchmark examples have been pointed out by Vreeswijk (1995). Another, more principled (and more demanding) way to proceed is to define a set of postulates to be satisfied by an
argumentation formalism (Caminada & Amgoud, 2007; Caminada, Carnielli, & Dunne, 2012).
As a remark, note that the fact that an instantiation may return an inconsistent result, does not
mean that it is completely useless. Namely, there might be cases when arguments are constructed
from an inconsistent knowledge base, and when one resolves just some of the existing inconsistencies by an argumentative approach, and then applies another inconsistency-tolerant approach.
Also, inconsistency handling is not the only use of argumentation. Thus, still in the same setting,
a drastic case would be to first use argumentation for another purpose (not dealing with at all with
inconsistencies) and then apply a different approach to reason with inconsistency.
We now review the related work. Maxi-consistent sets play a major role in the characterization of various forms of non-classical logical reasoning (Bochman, 2001) and in belief revision
(Alchourron, Gardenfors, & Makinson, 1985). The remainder of the section considers the papers
having a link with argumentation.
The paper by Cayrol (1995) is one of the early works relating the results obtained directly from
a knowledge base and by using an argumentative approach. In that paper, it was shown that direct
undercut satisfies (MC  Ext) for stable semantics, but no results for other semantics or attack
relations were provided. We not only studied other attack relations and other semantics, but also
provided a general study of properties an attack relation satisfying (MC  Ext) must also satisfy.
Amgoud and Vesic (2010) generalised the result by Cayrol (1995) for the case of prioritised
knowledge base, by showing that Arg is a bijection between preferred sub-theories (Brewka, 1989),
which generalise maximal consistent sets in case of prioritised knowledge base, and stable extensions of the corresponding preference-based argumentation system using direct undercut as an
attack relation and the weakest link principle as a preference relation.
Amgoud and Besnard (2009, 2010) also studied the link between a knowledge base and the corresponding argumentation system. Those papers introduced some important notions like conflictdependence and validity of an attack relation and proved numerous results related to consistency in
the underlying logic. However, note that the criterion (MC  Ext) was neither defined nor studied
in those papers; they provided (Amgoud & Besnard, 2010, Corollary 1) a link between MC() and
maximal conflict-free sets of F = (Arg(), R). Furthermore, this result is proved under hypotheses which are impossible to satisfy: the attack relation should be both valid and conflict-dependent,
which is impossible (as proved in Proposition 3). Some other results in that paper (Amgoud &
Besnard, 2010, e.g. Prop. 4) are proved only for an attack relation which is both conflict-dependent
and conflict-sensitive, which is not the case for any of the well-known attack relations. Consequently, the majority of the negative results of those papers are only applicable to a minority of
attack relations. Furthermore, examples in those papers are often incomplete systems; thus, it is
not surprising that there is no link between MC(Base(A)) and Ext(A, R) in those examples.
A recent paper by Gorogiannis and Hunter (2011) studied the properties of attack relations
in the case when a Dung-style argumentation system is instantiated with classical propositional
logic. Our work is related to those ideas, however, the focus of our paper is different. Our main
goal is to study to which extent Dungs theory can be used as a general framework for reasoning.
90

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

On the technical side, we concentrate on studying the properties of the class of attack relations
satisfying (MC  Ext) and identifying attack relations serving as lower and upper bounds of classes
of relations non-satisfying (MC  Ext).
One of the open questions is to find a set of conditions such that an attack relation satisfies
those conditions if and only if it satisfies (MC  Ext). Until recently, direct undercut and direct
defeat were the only known attack relations satisfying this condition (Vesic, 2012). Consequently, it
seemed that the space of attack relations satisfying this condition is rather narrow (note the similarity
between direct undercut and direct defeat). However, the present paper shows that Rba also belongs
to (MC  Ext), this indicating that the class of instantiations corresponding to the maxi-consistent
operator is much larger.
The formal framework studied in this paper is that of classical propositional logic-based argumentation. The vast majority of ideas and considerations from the present paper hold for other
instantiations of Dungs theory, for example in the setting studied by Modgil and Prakken (2013).
In other words, the result obtained from those argumentation frameworks could also be compared
with that obtained by an operator. After slightly adapting the definition of an operator, one can study
the same questions: is there a link between the result obtained from an argumentation system and
that obtained by an operator (from the same strict and defeasible rules)? Can argumentation help us
find new operators? Are there argumentation systems returning a result that cannot be captured by
an operator? Answering those questions will certainly be a part of our future work.

Acknowledgments
The author would like to thank Leendert van der Torre for his assistance and advice. His useful
comments helped to improve the paper significantly. The author also thanks the three reviewers for
their helpful comments.
This paper is a revised and extended version of the conference paper: S. Vesic. Maxi-Consistent
Operators in Argumentation. Proceedings of the 20th European Conference on Artificial Intelligence (ECAI12), pages 810-815.
The major part of the work on this paper was carried out while the author was affiliated with
the Computer Science and Communication Research Unit at the University of Luxembourg. First,
the author started this work during the tenure of an ERCIM Alain Bensoussan Fellowship Programme, which is supported by the Marie Curie Co-funding of Regional, National and International Programmes (COFUND) of the European Commission. During this time, the author was also
funded by the National Research Fund, Luxembourg. Second, while still at the University of Luxembourg, the author continued the work on this paper during a FNR AFR postdoc project which
was supported by the National Research Fund, Luxembourg, and cofunded under the Marie Curie
Actions of the European Commission (FP7-COFUND). Third, at the time when he was finishing
the work on this paper, the author was a CRNS researcher affiliated with CRIL.

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change: Partial
meet contraction and revision functions. Journal of Symbolic Logic, 50, 510530.
91

fiV ESIC

Amgoud, L., & Besnard, P. (2009). Bridging the gap between abstract argumentation systems
and logic. In International Conference on Scalable Uncertainty Management (SUM09), pp.
1227.
Amgoud, L., & Besnard, P. (2010). A formal analysis of logic-based argumentation systems. In
International Conference on Scalable Uncertainty Management (SUM10), pp. 4255.
Amgoud, L., Besnard, P., & Vesic, S. (2011). Identifying the core of logic-based argumentation
systems. In 23rd International Conference on Tools with Artificial Intelligence (ICTAI11),
pp. 633636.
Amgoud, L., & Cayrol, C. (2002). A reasoning model based on the production of acceptable arguments. Annals of Mathematics and Artificial Intelligence, 34, 197216.
Amgoud, L., & Vesic, S. (2010). Handling inconsistency with preference-based argumentation.
In Proceedings of the 4th International Conference on Scalable uncertainty Management
(SUM10), pp. 5669.
Arrow, K. J., Sen, A. K., & Suzumura, K. (Eds.). (2002). Handbook of Social Choice and Welfare.
North-Holland.
Besnard, P., & Hunter, A. (2000). Towards a logic-based theory of argumentation. In Proceedings of
the 17th National Conference on Artificial Intelligence (AAAI00), pp. 411416. AAAI Press
/ The MIT Press.
Besnard, P., & Hunter, A. (2001). A logic-based theory of deductive arguments. Artificial Intelligence Journal, 128, 203235.
Besnard, P., & Hunter, A. (2008). Elements of Argumentation. MIT Press.
Bochman, A. (2001). A logical theory of nonmonotonic inference and belief change - numerical
methods. Springer.
Brewka, G. (1989). Preferred subtheories: An extended logical framework for default reasoning.
In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI89), pp.
10431048.
Caminada, M. (2006). Semi-stable semantics. In Proceedings of the 1st International Conference
on Computational Models of Argument (COMMA06), pp. 121130. IOS Press.
Caminada, M., & Amgoud, L. (2007). On the evaluation of argumentation formalisms. Artificial
Intelligence Journal, 171 (5-6), 286310.
Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. Journal of Logic and
Computation.
Caminada, M., & Vesic, S. (2012). On extended conflict-freeness in argumentation.. In Proceedings
of the 24th Benelux Conference on Artificial Intelligence (BNAIC12), pp. 4350.
Cayrol, C. (1995). On the relation between argumentation and non-monotonic coherence-based entailment. In Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI95), pp. 14431448.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence Journal, 77,
321357.
92

fiI DENTIFYING THE C LASS OF M AXI -C ONSISTENT O PERATORS IN A RGUMENTATION

Elvang-Gransson, M., Fox, J., & Krause, P. (1993). Acceptability of arguments as logical uncertainty. In Proceedings of the 2nd European Conference on Symbolic and Quantitative
Approaches to Reasoning with Uncertainty (ECSQARU93), pp. 8590.
Elvang-Gransson, M., & Hunter, A. (1995). Argumentative logics: Reasoning with classically
inconsistent information. Data Knowl. Eng., 16(2), 125145.
Gardenfors, P. (1988). Knowledge in Flux  Modeling the dynamics of epistemic states. Cambridge,
MA, MIT Press.
Gorogiannis, N., & Hunter, A. (2011). Instantiating abstract argumentation with classical logic
arguments: Postulates and properties. Artificial Intelligence Journal, 175, 14791497.
Konieczny, S., & Perez, R. P. (2011). Logic based merging. Journal of Philosophical Logic, 40(2),
239270.
Loui, R. (1998). Process and policy: Resource-bounded nondemonstrative reasoning. Computational Intelligence, 14(1), 138.
Modgil, S., & Prakken, H. (2013). A general account of argumentation with preferences. Artificial
Intelligence, 195, 361397.
Pollock, J. (1987). Defeasible reasoning. Cognitive Science, 11(4), 481518.
Pollock, J. (1992). How to reason defeasibly. Artificial Intelligence Journal, 57, 142.
Priest, G. (2002). Paraconsistent logic. In Gabbay, D., & Guenthner, F. (Eds.), Handbook of Philosophical Logic, Vol. 6, pp. 287393. Dordrecht: Kluwer Academic Publishers.
Vesic, S. (2012). Maxi-consistent operators in argumentation. In 20th European Conference on
Artificial Intelligence (ECAI12), pp. 810815.
Vesic, S., & van der Torre, L. (2012). Beyond maxi-consistent argumentation operators. In 13th
European Conference on Logics in Artificial Intelligence (JELIA12), pp. 424436.
Vreeswijk, G. (1995). Interpolation of benchmark problems in defeasible reasoning. In Proceedings
of the Second World Conference on the Fundamentals of Artificial Intelligence (WOCFAI95),
pp. 453468.
Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence Journal, 90, 225279.

93

fiJournal of Artificial Intelligence Research 21 (2013) 205-251

Submitted 10/12; published 05/13

Analysis of Watsons Strategies for Playing Jeopardy!
Gerald Tesauro
David C. Gondek
Jonathan Lenchner
James Fan
John M. Prager

gtesauro@us.ibm.com
dgondek@us.ibm.com
lenchner@us.ibm.com
fanj@us.ibm.com
jprager@us.ibm.com

IBM TJ Watson Research Center, Yorktown Heights, NY 10598 USA

Abstract
Major advances in Question Answering technology were needed for IBM Watson1 to
play Jeopardy!2 at championship level  the show requires rapid-fire answers to challenging
natural language questions, broad general knowledge, high precision, and accurate confidence estimates. In addition, Jeopardy! features four types of decision making carrying
great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3)
selecting the next square when in control of the board; (4) deciding whether to attempt
to answer, i.e., buzz in. Using sophisticated strategies for these decisions, that properly
account for the game state and future event probabilities, can significantly boost a players
overall chances to win, when compared with simple rule of thumb strategies.
This article presents our approach to developing Watsons game-playing strategies,
comprising development of a faithful simulation model, and then using learning and MonteCarlo methods within the simulator to optimize Watsons strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in
particular on validating the accuracy of the simulators predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown
with respect to both simple heuristic strategies, and actual human contestant performance
in historical episodes. We further extend our analysis of human play to derive a number of
valuable and counterintuitive examples illustrating how human contestants may improve
their performance on the show.

1. Introduction
Jeopardy! is a fast-paced, demanding, and highly popular TV quiz show that originated in
the US, and now airs in dozens of international markets. The show features challenging
questions (called clues in the shows parlance) drawn from a very broad array of topics;
the clues may embody all manner of complex and ambiguous language, including vague
allusions and hints, irony, humor and wordplay.
The rules of game play in regular episodes (Jeopardy! Gameplay, 2013) are as follows.
There are two main rounds of play, wherein each round uses a board containing 30 squares,
organized as five squares in six different categories, with each square containing a hidden
clue. Second-round clues have higher dollar values, presumably reflecting greater difficulty.
In typical play, a square will be selected according to category and dollar amount by the
player in control of the board, and its clue will be revealed and read aloud by the host.
1. Registered trademark of IBM Corp.
2. Registered trademark of Jeopardy Productions Inc.
c
2013
AI Access Foundation. All rights reserved.

fiTesauro, Gondek, Lenchner, Fan & Prager

When the host finishes reading the clue, players may attempt to answer, i.e., buzz in,
by pressing a signaling device. The first player to buzz in obtains the right to attempt
to answer; if the answer is correct, the players score increases by the clues dollar value,
whereas if the answer is incorrect, the players score decreases by the dollar value, and the
clue is re-opened for the other players to attempt to answer on the rebound.
One clue in the first round, and two clues in the second round have a special Daily
Double status. The player who selected the clue has the exclusive right to answer, and she
must specify a wager between $5 and either her current score or the round limit, whichever
is greater. The game concludes with a single clue in the Final Jeopardy round. The
players write down sealed-bid wagers, and then have 30 seconds to write an answer after
the clue is revealed. The player finishing with the highest dollar-value score3 wins that
amount, and can continue playing on the next episode.
When IBM began contemplating building a computing system to appear on Jeopardy!
it was readily apparent that such an undertaking would be a hugely daunting challenge for
automated Question Answering (QA) technology. State-of-the-art systems at that time were
extremely poor at general open-domain QA, and had considerable difficulty if the questions
or the supporting evidence passages were not worded in a straightforward way. Building the
DeepQA architecture, and advancing its performance at Jeopardy! to a competitive level
with human contestants, would ultimately require intense work over a four-year period
by a team of two dozen IBM Researchers (Ferrucci, Brown, Chu-Carroll, Fan, Gondek,
Kalyanpur, ..., & Welty, 2010; Ferrucci, 2012).
Rather than discussing Watsons QA performance, which is amply documented elsewhere, the purpose of this paper is to address an orthogonal and significant aspect of
winning at Jeopardy!, namely, the strategic decision-making required in game play. There
are four types of strategy decisions: (1) wagering on a Daily Double (DD); (2) wagering in
Final Jeopardy (FJ); (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., buzz in. The most critical junctures of a game
often occur in the Final Jeopardy round and in playing Daily Doubles, where wagering is
required. Selecting a judicious amount to wager, based on ones confidence, the specific
game situation, and the likely outcomes of the remaining clues, can make a big difference
in a players overall chance to win. Also, given the importance of Daily Doubles, it follows
that a players square selection strategy when in control of the board should result in a
high likelihood of finding a DD. Allowing ones opponents to find the DDs can lead to devastating consequences, especially when playing against Grand Champions of the caliber of
Ken Jennings and Brad Rutter. Furthermore, a contestants optimal buzz-in strategy can
change dramatically in certain specific end-game scenarios. For example, a player whose
score is just below half the leaders score may need to make a desperation buzz on the
last clue in order to avoid a guaranteed loss. Conversely, at just above half the leaders
score, the correct strategy may be to never buzz in.
There is scant prior literature on Jeopardy! game strategies, and nearly all of it is qualitative and heuristic, with the sole exception of Final Jeopardy strategy, where substantial
quantitative analysis is embodied in the J! Archive (2013) Wagering calculator. Additionally, Dupee (1998) provides a detailed analysis of betting in Final Jeopardy, with particular
3. Multiple players may win if they finished tied for first place. This deviation from a strict zero-sum game
can lead to fascinating counter-intuitive strategies.

206

fiAnalysis of Watsons Strategies for Playing Jeopardy!

emphasis on the so-called two-thirds scenario, where betting nothing may increase winning chances for the second-place player, provided that he has at least two-thirds of the
leaders score. Some qualitative guidelines for aggressive or conservative Daily Double betting are also given, depending on confidence in the category, ability to win the buzz, and
positioning for Final Jeopardy. A specific last-clue DD bet is presented where the best bet
either takes the lead, or drops to exactly half the leaders score (i.e., a lock-tie), resulting
in extra chances to win. Harris (2006), one of the shows top contestants, provides numerous
qualitative insights into strategic thinking at championship level, including the importance
of seeking DDs in the bottom rows, wagering to position for Final Jeopardy, and protecting
a lead late in the game by being cautious on the buzzer.
This article describes our teams work in developing a collection of game-strategy algorithms deployed in Watsons live Jeopardy! contests against human contestants. To our
knowledge, this work constitutes the first-ever quantitative and comprehensive approach to
Jeopardy! strategy that is explicitly based on estimating and optimizing a players probability of winning in any given Jeopardy! game state. Our methods enable Watson to find
DDs faster than humans, and to calculate optimal wagers and buzz-in thresholds to a degree
of precision going well beyond human capabilities in live game play. A brief overview of our
work (Tesauro, Gondek, Lenchner, Fan, & Prager, 2012) recently appeared in a special issue
of the IBM Journal of Research and Development. The present article provides expanded
descriptions of each of our strategy algorithms, and presents substantial new quantitative
documentation of the performance advantages obtained by our approach, when compared
both to simple heuristic strategies as well as to actual human strategies.
The overall organization of the article is as follows. We first provide in section 1.1 a glossary of important technical terms and notation used throughout the article. Section 2 then
overviews our general approach to developing a Jeopardy! simulator, which we use to simulate contests between Watson and human contestants. Studying game-playing programs
in simulation is a well-established practice in computer games research. However, modeling
Jeopardy! is a much more difficult undertaking than in traditional games like Checkers and
Chess, due to its rich language content and extensive imperfect information. It is essential
to model the statistical performance profiles of human contestants, as well as their tendencies in wagering and square selection, by mining historical data on contestant performance
in thousands of previously aired episodes. In this respect, Jeopardy! is similar to other
imperfect-information games like Poker (Billings, Davidson, Schaeffer, & Szafron, 2002),
where effective dynamic modeling of ones opponents is a requisite ingredient for strong
play by both computers and humans. The general overview is followed in sections 2.1-2.6
by specific designs and construction methodologies for our simulation component models,
emulating Daily Double placement, human performance in Daily Doubles, Final Jeopardy,
regular clues and square selection, as well as extensions of such models from single-game
to multi-game format. The modeling section concludes in section 2.7, which presents a
statistically meaningful validation study, documenting how well various game statistics predicted by the simulator match up with actual statistics in live matches between Watson
and human contestants. As detailed in Appendix 1, Watson played more than 100 such
Sparring Games before appearing on television, and the validation study specifically focuses on the final 55 games in which Watson faced off against extremely strong former
Jeopardy! champions.
207

fiTesauro, Gondek, Lenchner, Fan & Prager

Section 3 presents specific techniques for designing, learning and optimizing Watsons
four strategy modules over the course of many simulated games. These techniques span a
range of widely used methods in current AI/OR research studies. Specifically, section 3.1
details our approach to DD wagering, which combines nonlinear regression with Reinforcement Learning to train a Game State Evaluator over the course of millions of simulated
games. Section 3.2 presents methods to calculate a Best-Response wagering strategy (a standard game-theoretic concept) in Final Jeopardy using either offline or online Monte-Carlo
sampling. Section 3.3 describes Watsons square selection strategy, the most important ingredient of which is live Bayesian inference calculation of the probabilities of various squares
containing a Daily Double. Finally, section 3.4 documents how Watson computes buzz-in
thresholds in endgame states using a combination of Approximate Dynamic Programming
with online Monte-Carlo trials, i.e., rollouts (Tesauro & Galperin, 1996; Bertsekas &
Castanon, 1999).
As our work has led to many new insights into what constitutes effective Jeopardy!
strategy, section 4 of the paper presents some of the more interesting and counterintuitive
insights we have obtained, with the hope of improving human contestant performance. Section 4.1 gives an overview of the most important decision boundaries we found in Watsons
Best-Response FJ strategy. Section 4.2 discusses our most important finding regarding human DD wagering, namely that humans should generally bet more aggressively. Section 4.3
presents buzz threshold analysis yielding initial buzz thresholds that are surprisingly aggressive, and rebound thresholds that are surprisingly conservative. Finally, section 4.4
discusses unusual and seemingly paradoxical implications of the lock-tie FJ scenario,
where the leader must bet $0 to guarantee a win.
After summarizing our work and lessons learned in section 5, Appendix 1 provides details
on Watsons competitive record, and Appendix 2 gives mathematical details of the buzz
threshold calculation.
1.1 Glossary
In this section we provide definitions of various technical terms and notation used in subsequent sections to describe our simulation models, strategies, or aspects of Jeopardy! game
play.
 A, B and C - The players with the highest, second highest and third highest scores,
respectively, or their current scores.
 Accuracy - The probability that a player will answer a clue correctly, in situations
where answering is mandatory (Daily Doubles or Final Jeopardy).
 Anti-two-thirds bet - Potential counter-strategy for A in the Two-thirds Final Jeopardy scenario (see Two-thirds bet). After a two-thirds bet, Bs score is at most
4B-2A. This will be less than A if B is less than three-fourths of A. Hence, A could
guarantee a win by small bet of at most 3A-4B. However, such a bet is vulnerable to
B making a large bet that significantly overtakes A.
 Average Contestant model - A model based on aggregate statistics of all J! Archive
regular episode data (excluding special-case contestant populations).
208

fiAnalysis of Watsons Strategies for Playing Jeopardy!

 Bet to cover, Shut-out bet - Standard strategy for the leader, A, in Final Jeopardy.
A usually bets at least 2B-A (frequently 2B-A+1). After a correct answer, As score
is at least 2B, which guarantees a win.
 Board Control - The right to select the next clue; usually belonging to the player who
gave the last correct answer. Daily Doubles can only be played by the player with
control of the board.
 Buzz attempt rate - Parameter b in the regular clue simulation model, denoting the
average probability that a player will attempt to buzz in on a clue.
 Buzz correlation - In the regular clue model, a quantity ij indicating the degree to
which the buzz-in decisions of player i and j tend to be the same on a given clue
(see Correlation coefficient). For two humans, ij = b (empirically  0.2), whereas
ij = 0 between a human and Watson.
 Buzzability - Short for buzzer ability. The probability of a given player winning a
contested buzz when multiple players buzz in.
 Buzzing In - Pressing a signaling device, indicating that a player wishes to attempt
to answer a clue. After the host finishes reading the clue, the first player to buzz in
is allowed to answer.
 Champion model - A model based on aggregate statistics of the 100 best players in
the J! Archive dataset, ranked according to number of games won.
 Correlation coefficient - In the simulator, a quantity ij indicating the degree to which
randomized binary events (Buzz/NoBuzz or Right/Wrong) for players i and j tend
to be the same on a given clue. As a simple example, suppose i and j have 50%
chance each of answering Final Jeopardy correctly. Let P (xi , xj ) denote the joint
probability that i has correctness xi and j has correctness xj . The correlation coefficient is then given by ij = P (Right,Right) + P (Wrong,Wrong) - P (Right,Wrong) P (Wrong,Right). Note that if xi and xj are independent, then all four joint outcomes
are equally likely, so that ij = 0. If xi and xj always match, then ij = 1 and if they
always mismatch, then ij = 1.
 Exhibition Match - (See Appendix 1) The televised two-game match, aired in Feb.
2011, in which Watson competed against Brad Rutter and Ken Jennings, arguably
the two best-ever human contestants (see Multi-game format).
 Grand Champion model - A model based on aggregate statistics of the ten best players
in the J! Archive dataset, ranked according to number of games won.
 Lockout, locked game - A game state in which the leaders current score cannot be
surpassed by the opponents in play of the remaining clues, so that the leader has a
guaranteed win. Usually refers to Final Jeopardy states where the leader has more
than double the opponents scores.
209

fiTesauro, Gondek, Lenchner, Fan & Prager

 Lock-tie - A Final Jeopardy situation in which the player in second place has exactly
half the leaders score. The leader has a guaranteed win by betting $0, enabling the
second place player to achieve a tie for first place by betting everything and answering
correctly.
 Match equity - Objective function optimized by Watson in the two-game Exhibition
Match, defined as probability of finishing first plus 0.5 times probability of finishing
second. By contrast, Watson simply maximized probability of winning in the Sparring
Games.
 Multi-game format - A special-case format used in the finals of the Tournament of
Champions, and in the Exhibition Match with Ken Jennings and Brad Rutter. First,
second and third place are awarded based on point totals over two games. In the
event of a first-place tie, a sudden death tie-break clue is played. Depending on the
prize money, there can be a significant incentive to finish in second place.
 QA - Short for Question Answering. A computing system or a suite of Natural
Language Processing techniques used to search for, evaluate, and select candidate
answers to clues.
 Precision, Precision@b - For regular clues, the average probability that a player will
answer correctly on the fraction of clues (b) in which the player chooses to buzz in
and answer (Ferrucci et al., 2010).
 Rebound - The situation after the first player to buzz in gets the clue wrong, and the
remaining players then have another chance to buzz in and try to answer.
 Regular episode format - In regular episodes, a returning champion plays a single
game against two new challengers. First, second and third place are determined by
point totals in that game, and multiple players may finish tied for first. The player(s)
finishing first will continue to play in the next episode. There is little incentive to
finish second, as it only pays $1000 more than finishing third.
 Right/wrong correlation - In the regular clue model, a quantity ij indicating the
degree to which the correctness of player i and j tend to be the same on a given clue
(see Correlation coefficient). For two humans, ij = p (empirically  0.2), whereas
ij = 0 between a human and Watson.
 Rollouts - Extensive Monte-Carlo simulations used to estimate the probability of a
player winning from a given game state.
 Sparring Games - (See Appendix 1) Two series of practice games (Series 1, 2) played
by Watson against former Jeopardy! contestants. Series 1 games were against contestants selected to be typical of average contestants appearing on the show. Series
2 games were played against champions, i.e., contestants who had reached the finals
or semi-finals of the annual Tournament of Champions.
 Tip-off effect - Information revealed in an initial incorrect answer that helps the rebound player deduce the correct answer. For example, a clue asking about a New
210

fiAnalysis of Watsons Strategies for Playing Jeopardy!

Jersey university is likely to have only two plausible answers, Rutgers and Princeton.
After an initial answer What is Princeton? is ruled incorrect, the rebounder can be
highly confident that the correct answer is What is Rutgers?
 Two-thirds bet - A plausible Final Jeopardy strategy for B in cases where B has at
least two-thirds of As score. Assuming A makes a standard bet to cover of at least
2B-A, Bs winning chances are optimized by betting at most 3B-2A. With such a bet,
B will win whenever A is wrong, whereas for larger bets, B also needs to be right.
This strategy is vulnerable to a counter-strategy by A (see Anti-two-thirds bet).

2. Simulation Model Approach
Since we optimize Watsons strategies over millions of synthetic matches, it is important
that the simulations be faithful enough to give reasonably accurate predictions of various
salient statistics of live matches. Developing such a simulator required significant effort,
particularly in the development of human opponent models.
The use of a simulator to optimize strategies is a well-established practice in computer
games research. Simulated play can provide orders of magnitude more data than live game
play, and does not suffer from overfitting issues that are commonly encountered in tuning
to a fixed suite of test positions. While it is usually easy to devise a perfect model of the
rules of play, simulation-based approaches can face a significant challenge if accurate models
of opponent strategies are required. In traditional two-player zero-sum perfect-information
board games (Backgammon, Checkers, Chess, etc.) such modeling is normally not required 
one can simply aim to compute the minimax-optimal line of play, as there is limited potential
to model and exploit suboptimal play by the opponents. By contrast, in repeated normalform games such as Prisoners Dilemma and Rock-Paper-Scissors, a one-shot equilibrium
strategy is trivial to compute but insufficient to win in tournament competitions (Axelrod,
1984; Billings, 2000). The best programs in these games employ some degree of adaptivity
and/or modeling based on the observed behaviors of their opponents. Poker is another
prominent game where opponent modeling is essential to achieve strong play (Billings et al.,
2002) . Playing an equilibrium strategy when the opponent is bluffing too much or too little
would forego an opportunity to significantly boost ones expected profit.
In contrast to the above-mentioned games, the Jeopardy! domain introduces entirely
new modeling issues, arising from the natural language content in its category titles, clues,
and correct answers. Obviously our simulator cannot generate synthetic clues comparable
to those written by the shows writers, nor can we plausibly emulate actual contestant responses. Even the most basic analysis of categories and clues (e.g., which categories tend
to be similar, co-occurrence likelihood of categories in a board, what type of information
is provided in clues, what type of mental process is needed to infer the correct response,
how the clue difficulty is calibrated based on the round and dollar value) seemed daunting
and the prospects for success seemed remote. Likewise, modeling the distributions of human contestant capabilities over thousands of categories, and correlations of abilities across
different categories, seemed equally implausible.
Due to the above considerations, our initial simulator design was based on an extremely
simplified approach. We avoided any attempt to model the games language content, and
decided instead to devise the simplest possible stochastic process models of the various
211

fiTesauro, Gondek, Lenchner, Fan & Prager

events that can occur at each step of the game. Our plan was to examine how accurately
such a simulator could predict the outcomes of real Watson-vs-human Jeopardy! matches,
and refine the models as needed to correct gross prediction errors. As it turned out, our
simple simulation approach predicted real outcomes much more accurately than we initially
anticipated (see section 2.7 below), so that no major refinements were necessary.
The only noteworthy enhancement of our simple stochastic process models occurred in
2010, after Watson had acquired the ability to dynamically learn from revealed answers
in a category (Prager, Brown, & Chu-Carroll, 2012). The effect was substantial, as Watsons accuracy improved by about 4% from the first clue to the last clue in a category.
We captured this effect by using historical data: each category in a simulated game would
be paired with a randomly drawn historical category, where a sequence of five right/wrong
Watson answers was known from prior processing. Instead of stochastically generating
right/wrong answers for Watson, the simulator used the recorded sequence, which embodied the tendency to do better on later clues in the category. The ability to simulate this
learning effect was instrumental in the ultimate development of Watsons square selection
algorithm, as we describe in section 3.3.
Our stochastic process simulation models are informed by:
 (i) properties of the game environment (rules of play, DD placement probabilities,
etc.)
 (ii) performance profiles of human contestants, including tendencies in wagering and
square selection;
 (iii) performance profiles of Watson, along with Watsons actual strategy algorithms;
 (iv) estimates of relative buzzability of Watson vs. humans, i.e., how often a player
is able to win the buzz when two or more contestants are attempting to buzz in.
Our primary source of information regarding (i) and (ii) is a collection of comprehensive
historical game data available on the J! Archive (2013) web site. We obtained fine-grained
event data from approximately 3000 past episodes, going back to the mid-1990s, annotating
the order in which clues were played, right/wrong contestant answers, DD and FJ wagers,
and the DD locations. After eliminating games with special-case contestants (Teen, College,
Celebrity, etc. games), the remaining data provided the basis for our model of DD placement
(section 2.1), and models of human contestant performance in Daily Doubles (section 2.2),
Final Jeopardy (section 2.3), regular clues (section 2.4), and square selection (section 2.5).
We devised three different versions of each human model, corresponding to three different levels of contestant ability encountered during Watsons matches with human contestants (see Appendix 1 for details). The Average Contestant model was fitted to all
non-tournament game data  this was an appropriate model of Watsons opponents in the
Series 1 sparring games. The Champion model was designed to represent much stronger
opponents that Watson faced in the Series 2 sparring games; we developed this model
using data from the 100 best players in the dataset, ranked by number of games won. Finally, for our Exhibition Match with Ken Jennings and Brad Rutter, we devised a Grand
Champion model which was informed by performance metrics of the 10 best players. Since
212

fiAnalysis of Watsons Strategies for Playing Jeopardy!

the Exhibition Match used a multi-game format (1st, 2nd and 3rd place determined by
two-game point totals), we developed specialized DD and FJ wagering models for Game 1
and Game 2 of the match, as described in section 2.6.
2.1 Daily Double Placement
We calculated the joint row-column frequencies in the J! Archive data of Round 1 and Round
2 DD placement; the Round 2 frequencies are illustrated in Figure 1. Our analysis confirms
well-known observations that DDs tend to be found in the lower rows (third, fourth and
fifth) of the board, and basically never appear in the top row. However, we were surprised
to discover that there are also column dependencies, i.e., some columns are more likely to
contain a DD than others. For example, DDs are most likely to appear in the first column,
and least likely to appear in the second column. (We can only speculate4 why the shows
producers place DDs in this fashion.)

Figure 1: Illustration of row-column frequencies of second-round DD placement in  3000
previous Jeopardy! episodes. Red denotes high frequency and blue denotes low
frequency.

Additional analytic insights from the data include: (i) The two second-round DDs never
appear in the same column. (ii) The row location appears to be set independently of the
column location, and independently of the rows of other DDs within a game. (iii) The
Round 2 column-pair statistics are mostly consistent with independent placement, apart
from the constraint in (i). However, there are a few specific column pair frequencies that
exhibit borderline statistically significant differences from an independent placement model.
Based on the above analysis, the simulator assigns the DD location in Round 1, and
the first DD location in Round 2, according to the respective row-column frequencies. The
4. We noted that the second column often features pop-culture categories (TV shows, pop music, etc.)
which could account for its relative paucity of DDs.

213

fiTesauro, Gondek, Lenchner, Fan & Prager

remaining Round 2 DD is assigned a row unconditionally, but its column is assigned conditioned on the first DD column.
2.2 Daily Double Accuracy/Betting Model
Round 2 DD bets vs. score

Round 2 DD bets vs. time

4500

3800
3600

4000

3400
3200

3500
Mean bet

Mean bet

3000
3000
2500

2800
2600
2400

2000

2200
2000

1500

A
B
C

1000
0

5000

10000
15000
Player score

20000

A
B
C

1800
1600
25000

0

5

10
15
20
Previously revealed clues in round

25

30

Figure 2: Average Round 2 mean DD bets of human contestants in first place (A), second
place (B) and third place (C), as a function of player score (left), and clues played
in round (right).

Based on the appropriate historical statistics in our J! Archive regular episode dataset,
we set the mean DD accuracy parameter in our human contestant models at 64% for Average Contestants, 75% for Champions, and 80.5% for Grand Champions. Bets made by
human contestants tend to be round number bets such as $1000 or $2000, and rarely exceed
$5000. The main dependencies we observed are that players in the lead tend to bet more
conservatively, and become extremely conservative near the end of the game, presumably
to protect their lead going into Final Jeopardy. These dependencies are clearly seen in
Figure 2, where we plot average bets as functions of player score and of clues played in the
second round.
While the above wagering tendencies were built into our Average Contestant model, we
surmised (correctly as it turned out) that much stronger Champion and Grand Champion
players would quickly realize that they need to bet DDs extremely aggressively when playing
against Watson. These models therefore employed an aggressive heuristic strategy which
would bet nearly everything, unless a heuristic formula indicated that the player was close
to a mathematically certain win.
2.3 Final Jeopardy Accuracy/Betting Model
The historical dataset obtained from J! Archive reveals that mean human accuracy in answering Final Jeopardy correctly is approximately 50% for average contestants, 60% for
Champions, and 66% for Grand Champions. Furthermore, from statistics on the eight
possible right/wrong triples, it is also clear that accuracy is positively correlated among
contestants, with a correlation coefficient F J  0.3 providing the best fit to the data. We
214

fiAnalysis of Watsons Strategies for Playing Jeopardy!

use these parameter values in simulating stochastic FJ trials, wherein we implement draws
of three correlated random binary right/wrong outcomes, with means and correlations tuned
to the appropriate values. This is performed by first generating correlated real numbers
using a multi-variate normal distribution, and then applying suitably chosen thresholds to
convert to 0/1 outcomes at the desired mean rates (Leisch, Weingessel, & Hornik, 1998).
As many such draws are required to determine the precise win rate of a given FJ score combination, we also implement a lower-variance simulation method. Rather than generating a
single stochastic outcome triple, the simulator evaluates all eight outcome triples, weighted
by analytically derived probabilities for each combination.
Second-place (B) FJ bet distribution
1

0.8

0.8
B bet / A score

A bet / A score

First-place (A) FJ bet distribution
1

0.6

0.4

0.2

0.6

0.4

0.2

0

0
0.5

0.6

0.7
0.8
B score / A score

0.9

1

0.5

0.6

0.7
0.8
B score / A score

0.9

1

Figure 3: Distribution of human FJ bets by first-place player A (left) and second-place
player B (right), normalized by leader score, as a function of B/A ratio.
The most important factor in FJ wagering is score positioning, i.e., whether a player is
in first place (A), second place (B) or third place (C). To develop stochastic process
models of likely contestant bets, we first discarded data from lockout games (where the
leader has a guaranteed win), and then examined numerous scatter-plots such as those
shown in Figure 3. We see a high density line in As bets corresponding to the well-known
strategy of betting to cover in case Bs score doubles to 2B. Likewise, there are two high
density lines in the plot of Bs bets, one where B bets everything, and one where B bets
just enough to overtake A. Yet there is considerable apparent randomization apart from
any known deterministic wagering principles.
After a thorough examination, we decided to segment the wagering data into six groups:
we used a three-way split based on strategic breakpoints (as detailed in section 4.1) in Bs
score relative to As score (less than 2/3, between 2/3 and 3/4, and more than 3/4), plus
a binary split based on whether or not B has at least double Cs score. We then devised
wagering models for A, B, and C5 that choose among various types of betting logic, with
probabilities based on observed frequencies in the data groups. As an example, our model
for B in the case (B  3/4 A, B  2C) bets as follows: bet bankroll (i.e., nearly everything)
5. Curiously enough, we saw no evidence that Cs wagers vary with strategic situation, so we implemented
a single betting model for C covering all six groups.

215

fiTesauro, Gondek, Lenchner, Fan & Prager

with 26% probability, keepout C (i.e., just below B-2C) with 27% probability, overtake
A (i.e., slightly above A-B) with 15% probability, two-thirds limit (i.e., just below 3B-2A)
with 8% probability, and various types of random bets with the remaining 24% probability
mass.
A
B
C

Real
65.3%
28.2%
7.5%

Model
64.8%
28.1%
7.4%

Table 1: Comparison of actual human win rates with model win rates by historical replacement in 2092 non-locked FJ situations from past episodes.

The betting models described above were designed solely to match human bet distributions, and were not informed by human FJ win rates. However, we subsequently verified
by a historical replacement technique that the models track actual human win rates quite
closely, as shown in Table 1. We first measured the empirical win rates of the A, B, C roles6
in 2092 non-locked FJ situations from past episodes. We then took turns recalculating the
win rate of one role after replacing the bets of that role by the bet distribution of the corresponding model. The models match the target win rates very well, considering that the
human bets are likely to reflect unobservable confidence estimates given the FJ category.
While we were satisfied that our human FJ model accurately fit the historical data, there
was nevertheless room for doubt as to how accurately it would predict human behavior in
the Sparring Games. Most notably, each of the six data groups used to estimate model
parameters contained only a few hundred samples, so that the error bars associated with
the estimated values were likely to be large. We could have addressed this by performing
so-called second order Monte-Carlo trials (Wu & Tsang, 2004), using Gaussian draws of
parameter values in each FJ trial instead of constant values, but we were concerned about
significantly higher computational overhead of such an approach. There were also possibilities that contestant behavior might be non-stationary over time, which we did not attempt
to model, or that contestants might alter their behavior specifically to play against Watson. As we discuss later in section 3.2, we generally accepted the FJ model as a basis for
optimizing Watsons decisions, with the sole exception of the case where Watson is A, and
the B player may plausibly make a two-thirds bet (see Glossary definition in section 1.1).
2.4 Regular Clue Model
Our stochastic process model of regular (non-DD) clues generates a random correlated
binary triple indicating which players attempt to buzz in, and a random correlated binary
triple indicating whether or not the players have a correct answer. In the case of a contested
buzz, a buzz winner is randomly selected based on the contestants relative buzzability
(ability to win a contested buzz, assumed equal in all-human matches). As mentioned in
the Glossary of section 1.1, the buzz-in outcomes are governed by two tunable parameters,
mean buzz attempt rate b and buzz correlation b . The right/wrong outcomes are
6. The human win rates sum to 101%, reflecting 1% chance of a first-place tie.

216

fiAnalysis of Watsons Strategies for Playing Jeopardy!

Figure 4: Frequencies of seven possible regular-clue outcomes in J! Archive average contestant dataset.

likewise governed by two parameters, mean precision@b (Ferrucci et al., 2010) or simply
mean precision p, and right/wrong correlation p . We set the four parameter values b,
b , p, and p by running extensive Monte-Carlo simulations for many different parameter
combinations, and selecting the combination yielding the best fit to observed historical
frequencies of the seven possible outcomes for regular clues, as depicted in Figure 4. The
outcome statistics are derived from J! Archive records of more than 150K regular clues.
The parameter values we obtained for average contestants are: b = 0.61, b = 0.2, p = 0.87
and p = 0.2. The right/wrong correlation is derived directly from rebound statistics, and
is particular noteworthy: while a positive value is reasonable, given the correlations seen
in FJ accuracy, it might be surprising due to the tip-off effect on rebounds. When the
first player to buzz gives a wrong answer, this eliminates a plausible candidate and could
significantly help the rebound buzzer to deduce the right answer. We surmise that the data
may reflect a knowledge correlation of  0.3 combined with a tip-off effect of  0.1 to
produce a net positive correlation of 0.2.
In the Champion model, there is a substantial increase in attempt rate (b = 0.80) and
a slight increase in precision (p = 0.89). In the Grand Champion model, we estimated
further increases in these values, to b = 0.855 and p = 0.915 respectively. As depicted in
Figure 5, we also developed a refined model by segregating the regular clue data according
to round and dollar value (i.e., row number), and estimating separate (b, p) values in each
case. Such refinements make the simulations more accurate, but do not meaningfully impact
the optimization of Watsons wagering and square selection strategies. While we expect
that a slight improvement in Watsons endgame buzzing could have been achieved using
separate (b, p) values, there was insufficient data for Champions and Grand Champions to
estimate such values. Hence the deployed algorithm used constant (b, p) values for all clues.
2.5 Square Selection Model
Most human contestants tend to select in top-to-bottom order within a given category, and
they also tend to stay within a category rather than jumping across categories. There is a
further weak tendency to select categories moving left-to-right across the board. Based on
these observations, and on the likely impact of Watsons square selection, we developed
217

fiTesauro, Gondek, Lenchner, Fan & Prager

Average Contestant Attempt Rate / Precision vs. Row Number
0.95

Mean Attempt Rate / Precision

0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5

b(row)
b=0.61
p(row)
p=0.87

0.45
0.4
1

2

3

4

5
6
Row Number

7

8

9

10

Figure 5: Estimates of Average Contestant buzz attempt rate (b) and precision (p) as a
function of round and row number. Rows 1-5 denote first round clues, and rows
6-10 denote second round clues.

an Average Contestant model of square selection which stays in the current category with
60% probability, and otherwise jumps to a random different category. When picking within
a category there is high probability (90%) of picking the topmost available square. By
contrast, we model Champion and Grand Champion square selection as DD seeking based
on the known row statistics of DD placement. Strong players generally exhibit more Daily
Double seeking when selecting squares, and when playing against Watson, they quickly
adopt overt DD seeking behavior.
2.6 Multi-game Wagering Model
In most Jeopardy! contests, the winner is determined by performance in a single game.
However, the show also conducts several annual tournaments, such as the Tournament of
Champions, in which the final match utilizes point totals over two games to determine first,
second and third place. This clearly implies that wagering strategies must differ in Game 1
and Game 2 of the match, and both need to be different from single-game wagering.
Since there is very limited multi-game match data available from J! Archive (only about
two dozen Tournament of Champions final matches), it would be quite difficult to model the
expected wagering of Jennings and Rutter in our Exhibition Match purely from historical
data. Fortunately, we were able to make some educated guesses that considerably simplified
the task. First, we predicted that they would wager DDs very aggressively in both games,
unless they had an overwhelming lead. This implied that we could continue to use the
aggressive heuristic DD model for single games, with a revised definition of what constitutes
an overwhelming match lead. Second, we also expected them to bet very aggressively in
Final Jeopardy of the first game. This meant that we could treat Game 1 FJ as if it were
a DD situation, and again use the revised aggressive heuristic model.
218

fiAnalysis of Watsons Strategies for Playing Jeopardy!

The only situation requiring significant modeling effort was Game 2 FJ. We generalized
the definition of A, B, and C roles for matches, based on the sum of Game 1 score plus
two times Game 2 score. With this definition, all of the established single-game strategies
for A, B and C carry over to two-game matches.
Given the limited available match data, only crude estimates could be assigned of the
probabilities of various betting strategies. However, it is clear from the data that the
wagering of human champions is much more coherent and logical than the observed wagering
in regular episodes, and champion wagers frequently satisfy multiple betting constraints.
These observations guided our development of revised betting models for Game 2 FJ. As
an example, in the case where B has a legal generalized two-thirds bet (suitably defined for
two-game matches), and B can also keep out C, our model for B bets as follows: bankroll
bet with 35% probability, bet a small random amount that satisfies both the two-thirds and
keepout-C limits with 43% probability, or bet to satisfy the larger of these two limits with
22% probability.
2.7 Model Validation
Our first efforts to validate the simulators predictions occurred about half-way through
Watsons first series of Sparring Games. At that point in time, the simulator had only
been used to develop Watsons Final Jeopardy wagering algorithm, so the simulator was
basically running a model of Watson with heuristic strategies against the Average Contestant model. The predicted outcomes were ex-post (after the fact) predictions, in that
we needed data from the live games to set certain simulation parameter values, particularly
relating to Watsons buzzability. We were encouraged to see that the predicted rates of
Watson winning a game (62%), leading going into Final Jeopardy (72%) and winning by
lockout (27%) were within the standard error over 42 games of the actual rates (64%, 76%,
and 26% respectively). There were more significant deviations on the low side in predicted
final scores of Watson (15800 vs. 18400 actual) and of the humans (9300 vs. 10900 actual)
but both were still within 95% confidence intervals.
By the start of the second series of Sparring Games, we were able to make ex-ante
(before the fact) predictions, before Watson had actually played against human champions.
These predictions were based mostly on using J! Archive data to tune parameters of the
Champion model, as well as semi-educated guesses regarding the improvement in buzzability
of human champions, and how aggressively they would seek out and wager on the Daily
Doubles. The actual vs. predicted statistics are reported below in Table 2. Most of the
ex-ante simulation stats turned out to be remarkably close to the actual results; only the
rates of Watson leading in FJ, Watsons board control (i.e., how often Watson selected
the next square) and human lockout rate differed by more than one standard error.
We then examined how much improvement could be obtained by ex-post recalibration of
the Champion model, based on actual stats in the Sparring Games. As seen in Table 2, our
best ex-post predictions failed to significantly improve on the ex-ante predictions. While
there was notable improvement in Watson FJ lead and human lockout rates, the predictions
of Watson lockout rate and human final score were noticeably worse.
219

fiTesauro, Gondek, Lenchner, Fan & Prager

Statistic
Watson win rate
Watson lockout
Watson FJ lead
Watson board control
Watson DDs found
Watson final score
Human final score
Human lockout

Actual
0.709  0.061
0.545  0.067
0.891  0.042
0.500  0.009
0.533  0.039
23900  1900
12400  1000
0.018  0.018

Ex-ante sim
0.724
0.502
0.806
0.516
0.520
24950
12630
0.038

Ex-post sim
0.718
0.493
0.830
0.515
0.517
24890
13830
0.023

Table 2: Comparison of actual mean statistics ( std. error) in 55 Series 2 Sparring Games
vs. ex-ante and ex-post predicted results in 30k simulation trials.

3. Optimizing Watsons Strategies Using the Simulation Model
The simulator described in the previous section enables us to estimate Watsons performance for a given set of candidate strategy modules, by running extensive contests between
a simulation model of Watson and two simulated human opponents. The Watson stochastic process models use the same performance metrics (i.e., average attempt rate, precision,
DD and FJ accuracies) as in the human models. The parameter values were estimated
from J! Archive test sets, and were updated numerous times as Watson improved over
the course of the project. The Watson model also estimates buzzability, i.e., its likelihood
to win the buzz against humans of various ability levels. These estimates were initially
based on informal live demo games against IBM Researchers, and were subsequently refined based on Watsons performance in the Sparring Games. We estimated Watsons
buzzability against two humans at 80% for average contestants, 73% for Champions, and
70% for Grand Champions.
Computation speed was an important factor in designing strategy modules, since wagering, square selection and buzz-in decisions need to be made in just a few seconds. Also,
strategy runs on Watsons front-end, a single server with just a few cores, as its 3000-core
back-end was dedicated to QA computations. As a result, most of Watsons strategy
modules run fast enough so that hundreds of thousands of simulated games can be performed
in just a few CPU hours. This provides a solid foundation for evaluating and optimizing
the individual strategy components, which are presented below. Some strategy components
(endgame buzz threshold, endgame DD betting, and Game 2 FJ betting) are based on
compute-intensive Monte-Carlo trials; these are too slow to perform extensive offline evaluation. Instead, these strategies perform live online optimization of a single strategy decision
in a specific game state.
3.1 Daily Double Wagering
We implemented a principled approach to DD betting, based on estimating Watsons
likelihood of answering the DD clue correctly, and estimating how a given bet will impact
Watsons overall winning chances if he gets the DD right or wrong. The former estimate
is provided by an in-category DD confidence model. Based on thousands of tests on
220

fiAnalysis of Watsons Strategies for Playing Jeopardy!

historical categories containing DDs, the model estimates Watsons DD accuracy given
the number of previously seen clues in the category that Watson got right and wrong.
To estimate impact of a bet on winning chances, we follow the work of Tesauro (1995)
in using Reinforcement Learning (Sutton & Barto, 1998) to train a Game State Evaluator (GSE) over the course of millions of simulated Watson-vs-humans games. Given a
feature-vector description of a current game state, the GSE implements smooth nonlinear
function approximation using a Multi-Layer Perceptron (Rumelhart, Hinton, & Williams,
1987) neural network architecture, and outputs an estimate of the probability that Watson
will ultimately win from the current game state. The feature vector encoded information
such as the scores of the three players, and various measures of the remaining amount of
play in the game (number of remaining DDs, number of remaining clues, total dollar value
of remaining clues, etc.).
The combination of GSE with in-category confidence enables us to estimate E(bet), i.e.
the equity (expected winning chances) of a bet, according to:
E(bet) = pDD  V (SW + bet, ...) + (1  pDD )  V (SW  bet, ...)

(1)

where pDD is the in-category confidence, SW is Watsons current score, and V () is the
game state evaluation after the DD has been played, and Watsons score either increases
or decreases by the bet. We can then obtain an optimal risk-neutral bet by evaluating
E(bet) for every legal bet, and selecting the bet with highest equity. During the Sparring
Games, our algorithm only evaluated round-number bets (i.e., integer multiples of $100),
due both to computational cost as well as the possibility to obtain extra winning chances
via a first-place tie or a lock-tie scenario as described in section 1.1. For the Exhibition
Match, tie finishes were not possible, and we had sped up the code to enable evaluation of
non-round wagers. This accounted for the strange wager values that were the subject of
much discussion in the press and among viewers.
In practice, a literal implementation of risk-neutral betting according to Equation 1
takes on a frightening amount of risk, and furthermore, the calculation may contain at least
three different sources of error: (i) the GSE may exhibit function approximation errors; (ii)
the simulator used to train the GSE may exhibit modeling errors; (iii) confidence estimates
may have errors due to limited test-set data. We therefore chose to adjust the risk-neutral
analysis according to two established techniques in Risk Analytics. First, we added a
penalty term to Equation 1 proportional to a bets volatility (i.e., standard deviation over
right/wrong outcomes). Second, we imposed an absolute limit on the allowable downside
risk of a bet, defined as the equity difference between the minimum bet and the actual
bet after getting the DD wrong. Due to function approximator bias, the latter technique
actually improved expectation in some cases, in addition to reducing risk. We observed this
in certain endgame states where the neural net was systematically betting too much, due
to underestimation of lockout potential.
The overall impact of risk mitigation was a nearly one-third reduction in average risk
of an individual DD bet (from 16.4% to 11.3%), at the cost of reducing expected winning
chances over an entire game by only 0.3%. Given that Watson finds on average 1.5-2.0
DDs per game (depending on how aggressively the opponents also seek DDs), this implies
that the equity cost per DD bet of risk mitigation is quite small, and we regarded the overall
tradeoff as highly favorable.
221

fiTesauro, Gondek, Lenchner, Fan & Prager

3.1.1 Illustrative Example
Figure 6 illustrates how the DD bet analysis operates, and how the resulting bet depends
strongly on in-category confidence. The example is taken from one of the Sparring Games,
where Watson got four consecutive clues right in the first category at the start of Double
Jeopardy, and then found the first DD in attempting to finish the category. At this point,
Watsons score was 11000 and the humans each had 4200. Watsons in-category confidence took its maximum value, 75%, based on having gotten four out of four correct answers
previously in the category. Watson chose to wager $6700, which is a highly aggressive bet
by human standards. (Fortunately, he got the DD clue right!)
(11000, 4200, 4200) Watson Daily Double Bet

(11000, 4200, 4200) Watson Daily Double Bet
0.8

0.8

Expected Winning Chances

Expected Winning Chances

0.9

0.7
0.6
0.5
0.4
DD wrong equity
DD right equity

0.3
0

2000

4000

0.75

0.7

0.65
conf 0.45
conf 0.55
conf 0.65
conf 0.75
conf 0.85
best bet

0.6

0.55
6000
Bet Amount

8000

10000

12000

0

2000

4000

6000
8000
Bet Amount

10000

12000

Figure 6: (left) Equity estimates getting the DD right (top curve) and wrong (bottom
curve). (right) Bet equity curves at five differences in-category confidence levels,
from 45% to 85%. Black dots show how the optimal risk-neutral bet increases
with confidence.
The left figure shows neural net equity estimates for getting the DD right (top curve)
and wrong (bottom curve) at various bet amounts. These curves are extremely smooth
with gently decreasing slopes. The right plot shows the resulting equity-vs-bet curve at
Watsons actual 75% confidence level (magenta curve), along with four other curves at
different confidence values ranging from 45% to 85%. Black dots on each curve indicate the
best risk-neutral bet, and we can see how the bet steadily increases with confidence, from
$5 at 45%, to approximately $9300 at the actual 75%, and finally to the entire $11000 at a
(hypothetical) confidence of 85%.
We also note the effect of risk mitigation, which reduced Watsons actual bet from
$9300 to $6700. According to extensive Monte-Carlo analysis of this bet, risk mitigation
reduced Watsons equity by only 0.2% (from 76.6% to 76.4%), but it entailed significantly
less downside risk (more than 10%) in the event that Watson got the DD wrong. With
a protection-to-cost ratio of over 50 to 1, we consider risk mitigation to have provided in
this case an inexpensive form of disaster insurance, and the Watson team members were
relieved to see that Watson did not risk his lead on this DD bet.
222

fiAnalysis of Watsons Strategies for Playing Jeopardy!

3.1.2 Endgame Monte-Carlo Wagers
For the Series 2 Sparring Games, we significantly boosted the simulation speed for regular
clues and Final Jeopardy. This enabled replacement of neural net wagering in endgame
states by a routine based on live Monte-Carlo trials. This analysis gives essentially perfect
knowledge of which bet achieves the highest win rate in simulation, although it is still subject
to modeling errors and confidence estimation errors. It also eliminated the primary weakness
in Watsons DD strategy, as neural net misevaluations in endgames often resulted in serious
errors that could considerably exceed 1% equity loss. As detailed below in section 3.1.3,
usage of Monte-Carlo analysis for endgame wagers yielded a quite significant reduction
(more than a factor of two) in Watsons overall error rate in DD betting.
With few clues remaining before Final Jeopardy, the dependence of equity on a players
score can exhibit complex behavior and discontinuities, in contrast to the smooth monotone
behavior observed in early and mid-game states. A striking example is plotted in Figure 7.
This was an endgame DD bet from the Series 2 Sparring Games where Watson had 19800,
the humans had 13000 and 14300, and there were only four remaining clues (two $400 and
two $800). Watson was 4-for-4 in the category, which translated into 71.8% DD confidence.
(We opted for a more conservative estimate than the 75% figure mentioned earlier, due to
possible confidence estimation errors.)
We see on the left that the right/wrong equity curves exhibit complex acceleration and
deceleration, as well as periodic jumps with periodicity of $400. These may reflect scores
where a discrete change occurs in the combinations of remaining squares needed to reach
certain FJ breakpoints, such as a lockout. The equity-vs-bet curve on the right also displays
interesting multi-modal behavior. There is a peak lead-preserving bet around $3000. At
$6400, the curve begins a steep ascent  this is the point at which a lockout becomes
mathematically possible. The curve continues to rise until about $12000, where a correct
answer assures the lockout, and then it falls off.
(19800, 13000, 14300) Watson Daily Double Bet
0.76

0.9

0.74

0.8

0.72

Expected Winning Chances

Expected Winning Chances

(19800, 13000, 14300) Watson Daily Double Bet
1

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

2000

4000

6000

0.7
0.68
0.66
0.64
0.62
0.6
0.58

DD wrong equity
DD right equity

0

Equity @ 71.8% confidence

0.56
8000 10000 12000 14000 16000 18000 20000
Bet Amount

0

2000

4000

6000

8000 10000 12000 14000 16000 18000 20000
Bet Amount

Figure 7: MC DD bet analysis. (left) Equity estimates getting the DD right (top curve)
and wrong (bottom curve). (right) Bet equity curve at Watsons estimated
in-category confidence of 71.8%.

223

fiTesauro, Gondek, Lenchner, Fan & Prager

3.1.3 Performance Metrics and Error Analysis
We assessed the performance of neural net DD wagering by two different methods. First,
we noted an improved win rate in simulations when compared to Watsons previous DD
wagering algorithm, a set of heuristic betting rules that tried to safely add to Watsons lead
or safely catch up, without dropping below certain strategically important score breakpoints.
While the heuristic rules embodied sound logic, they suffered a major limitation of not taking
Watsons in-category confidence into account, so that they would generate the same wager
regardless of confidence.
Using the heuristic DD betting rules, Watsons simulated win rate was 61%. With
neural net DD wagering using a default confidence value for every DD, the win rate improved
to 64%. When we added emulation of live DD confidence values to the simulation, the result
was a further jump in win rate, to 67%. We regarded this as a quite significant performance
improvement, given that the DD betting algorithm is only used about 1.5-2.0 times per
game.
The second performance metric utilizes extensive offline Monte-Carlo analysis of many
neural network bets to estimate average equity loss per DD bet, i.e., the average difference
between the equity of the true best bet, with highest simulated win rate, and the equity
of the bet selected by the neural net. This figure was approximately 0.6% per DD bet,
which was quite good, as it implied that the overhead to improve Watsons win rate
via improved DD wagering was less than 1.2%. Most of the equity loss was due to large
errors in endgame states. As mentioned earlier, we substantially reduced the loss rate
by implementing DD wagering in endgames based on live Monte-Carlo simulations. This
reduced Watsons average equity loss per DD bet from 0.6% to 0.25%, with about half of
that loss rate resulting from deliberately imposed risk mitigation. Hence we were satisfied
that Watsons DD algorithm was close enough to optimal for all practical purposes.
3.1.4 Human DD Error Analysis
An interesting and nontrivial question arising from the above analysis is how does Watsons equity loss rate in DD wagering compare with that of human contestants. The main
difficulty in attempting such analysis is that contestants confidence in answering the DD
clue correctly is largely unobservable in historical data. We have no way to know their
confidence in the category, and their knowledge on previous clues in the category is only
revealed if they won the buzz and gave an answer. In the absence of confidence information,
it is hard to ascribe an error level to any individual DD bet, although we may be able to
assess average wagering error over an entire population of contestants.
We devised two methods that respectively should provide lower and upper bounds on
population average wagering error, given sufficient samples of historical DD bets. The
first method is a historical replacement technique similar to those presented in sections 2.3
and 3.2. For each historical DD, we first use the Average Contestant simulator to run
many trials starting from the actual outcome state of the DD, reflecting the contestants
actual bet and actual right or wrong answer. We then replace the contestants bet with
some algorithmically computed bet, wherein confidence is estimated solely from observable
information, and we rerun trials from the modified outcome state, where the contestants
score is changed due to the change of bet. Averaged over many DDs, the equity difference
224

fiAnalysis of Watsons Strategies for Playing Jeopardy!

between human bets and algorithmic bets should indicate which approach is better. If the
algorithmic bets prove to be better, the equity difference should provide a lower bound on
the true human error rate: since the algorithm does not have access to private confidence
information, it would presumably obtain even better results given such information.
There could be issues with such an approach in faithfully simulating what would have
happened if the contestant had bet differently, as changing the bet might have changed
subsequent square selection (DD seeking) and subsequent DD wagering. We minimized these
issues by limiting the analysis to last-DD situations. Our historical dataset contained more
than 2200 regular episodes where both DDs were played in the second round. By analyzing
last-DD states in these episodes, we avoid having to simulate subsequent DD wagers, and
the effect of subsequent square selection should be minimal since there are no more DDs to
be found. Also, the last-DD states allow us to use endgame Monte-Carlo for algorithmic
wagering, which should give stronger results than neural net wagering. Confidence estimates
in the Monte-Carlo calculation were based on the historical mean accuracy on second-round
DDs given the row location. This ranges from 72% for the top two rows to 57% for the
bottom row.
As seen in Table 3, results of the analysis showed that contestant bets on average
obtained about 2.9% less equity per bet than Monte-Carlo algorithmic bets. As this should
constitute a lower bound on the true human error rate in last-DD states, whereas Watsons
error rate is 0.25% overall and near-perfect in endgames states, this provides compelling
evidence of Watsons superiority to human contestants, at least for last-DD wagers.
Our second analysis method ignores the actual right/wrong contestant answers, and
instead uses the Monte-Carlo analysis to calculate the equity loss of a human bet, assuming
that the row-based mean DD accuracies provide correct confidence estimates. This type
of analysis should overestimate human errors, as it unduly penalizes small bets based on
low private confidence, and large bets based on high private confidence. Results of this
analysis on the last-DD dataset show an average error rate of 4.0% per DD bet. This result
is consistent with an estimated lower bound on the error rate of 2.9%, and in combination,
both results provide reasonable evidence that the actual human error rate lies in between
the estimated bounds.
Last-DD Player
All
A (leader)
B,C (trailer)

Lower Bound
2.9%
0.9%
4.8%

Upper Bound
4.0%
2.2%
5.5%

Avg. Human Bet
$2870
$2590
$3120

Avg. MC Bet
$6220
$5380
$6970

Table 3: Lower and upper bounds on average equity loss rates of historical human contestant
wagers on the last DD of the game. The average human bets vs. recommended
MC bets (at default confidence values) are also displayed.

A closer examination of the analysis reveals that humans systematically wager these
DDs far too conservatively. After segregating the data according to whether the DD player
is leading or trailing, we found that this conservatism is manifest in both cases. Leading
players on average bet $2590, whereas the average recommended MC bet is $5380. For
trailing players, the average bet is $3120 vs. an average recommended bet of $6970. A
225

fiTesauro, Gondek, Lenchner, Fan & Prager

more startling finding is that these errors are far more costly to trailers than to leaders, in
terms of equity loss. The lower and upper bounds on error rate for leaders are 0.9% and
2.2%, while for trailers, the respective bounds are 4.8% and 5.5%! We further discuss the
implications of these results for human contestants in section 4.2.
3.1.5 Multi-game DD Wagering
As mentioned in section 2.6, Game 1 and Game 2 of our Exhibition Match required distinct
wagering strategies, with both differing from single-game wagering. We trained separate
neural networks for Game 1 and Game 2. The Game 2 net was trained first, using a plausible
artificial distribution of Game 1 final scores.
Having trained the Game 2 neural net, we could then estimate the expected probabilities
of Watson ending the match in first, second, or third place, starting from any combination
of Game 1 final scores, by extensive offline Monte-Carlo simulations. We used this to create
three lookup tables, for the cases where Watson ends Game 1 in first, second, or third
place, of Watson match equities at various Game 1 final score combinations, ranging from
(0, 0, 0) to (72000, 72000, 0) in increments of 6000. (Since adding or subtracting a constant
from all Game 1 scores has no effect on match equities, we can without loss of generality
subtract a constant so that the lowest Game 1 score is zero.) Since match equities are
extremely smooth over these grid points, bilinear interpolation provides a fast and highly
accurate evaluation of Game 1 end states. Such lookup tables then enabled fast training of
a Game 1 neural net, using simulated matches that only played to the end of Game 1, and
then assigned expected match-equity rewards using the tables.
Based on our earlier experience, we added a heuristic lockout-potential feature to the
Game 2 input representation, using a heuristic sigmoidal formula to estimate the probability
that Watson would win the match by lockout, given the Game 1 and Game 2 scores and
the dollar value of remaining clues. This feature appeared to enable highly accurate Game
2 evaluations and eliminated the large endgame equity losses that we had observed in using
the single-game neural net.
An important and difficult new issue we faced in the Exhibition Match format was how
to assign relative utilities to finishing in first, second and third place. Unlike the Sparring
Games where the only reasonable objective was to finish first, the team extensively debated
how much partial credit should be ascribed to a second-place finish, in which Watson
defeated one of the two greatest Jeopardy! contestants of all time. Ultimately we decided to
base the match DD wagering on full credit for first, half credit for second, and zero credit
for a third place finish. Such an objective acts as an additional type of risk control, where
Watson would only prefer a large bet over a smaller and safer bet if the large bets upside
(increased chances to win) exceeded its downside (increased chances of finishing third).
There was unanimous team consensus that such a risk would always be worth taking.
Defining Watsons match equity as probability of finishing first plus 0.5 times probability of finishing second, we estimated Watsons average match equity loss per DD bet at
about 0.3% in Game 1 and 0.55% in Game 2. The majority of loss in each case was due to
risk controls.
226

fiAnalysis of Watsons Strategies for Playing Jeopardy!

3.2 Final Jeopardy Wagering
Our approach to Final Jeopardy wagering involves computation of a Best-Response strategy (Fudenberg & Tirole, 1991) (a standard game-theoretic concept) to the human FJ model
presented in section 2.3. We considered attempting to compute a Nash Equilibrium strategy (Fudenberg & Tirole, 1991), but decided against it for two reasons. First, due to the
imperfect information in Final Jeopardy (contestants know their own confidence given the
category title, but do not know the opponents confidence), we would in principle need to
compute a Bayes-Nash equilibrium (BNE), which entails considerably more modeling and
computational challenges. Second, it seems far-fetched to assume that Watsons opponents
would play their part of a Nash equilibrium or BNE, since average contestants have not
studied game theory.
By using a Best-Response instead of a BNE strategy (assuming we could calculate it), we
aim to more effectively exploit typical human wagering errors as recorded in the historical
data. We realized that this potentially exposes Watson to two types of risk. First, if
Watsons live opponents turned out to use BNE or other more sophisticated strategies
than those built into our human models, Watson might do better by playing the BNE
strategy. We judged this risk to be sufficiently rare that Watson would surely do better
over the course of many Sparring Games by simply playing the Best-Response. Second,
since the Best-Response is essentially a deterministic strategy, a contestant who observed
Watson play many Final Jeopardy rounds might be able to detect and optimally exploit
Watsons wagering strategy. However, such observations were limited by the procedures
for conducting the Sparring Games, as any contestant would only play 2-3 games against
Watson, and would observe only another 1-2 games as a spectator. In some situations, we
were additionally able to randomize Watsons bet over a range of bets with approximately
equal expected outcomes; this made it more difficult for humans to infer Watsons wagering
logic from a limited number of observations.
Computation of the Best-Response proceeds as follows. First, we consult an FJ prior
accuracy regression model to estimate Watsons confidence given the category title. This
model was trained on samples of Watsons performance in thousands of historical FJ
categories, using NLP-based feature vector representations of the titles. Second, given
Watsons confidence and the human accuracy/correlation parameters, we derive analytic
probabilities of the eight possible right/wrong outcomes. Third, for a given FJ score combination, we draw on the order of 10000 Monte-Carlo samples of bets from the human models.
Finally, we evaluate the equity of every legal bet, given the human bets and the right/wrong
outcome probabilities, and select the bet with highest equity.
Our initial implementation of the above algorithm was too slow to use in live play. Fortunately, after extensive offline analysis using Watsons default confidence, we discovered
that the Best-Response output could be expressed in terms of a fairly simple set of logical
betting rules. For example, one of the rules for B stipulates:
If B has at least two-thirds of A, and B has less than 2C, check whether 2C-B
(the amount to cover Cs doubled score) is less than or equal to 3B-2A (the
maximum two-thirds bet). If so, then bet 2C-B, otherwise bet everything.
Thus in the Series 1 Sparring Games, we deployed the rule-based encapsulation of the
Best-Response calculation, but with one specific exception where Watson is A, and the
227

fiTesauro, Gondek, Lenchner, Fan & Prager

B player may reasonably consider a so-called two-thirds bet, i.e., a small bet that aims
to guarantee a win whenever A is wrong, assuming A makes the standard shut-out bet
(2B-A+1). In some of these situations, the Best-Response calculation calls for Watson
to counter B by making a small, tricky anti-two-thirds bet. Whether this really wins
more often depends on exact model parameter values, which have considerable uncertainty,
as discussed earlier in section 2.3. Moreover, if Watson bets small and gets FJ wrong,
it suggests shakiness in his ability to play Final Jeopardy, which might be exploited in
subsequent games, as well as generally looking bad. Conversely, if Watson bets small and
gets FJ right, it risks an embarrassing loss where Watson could have won but failed to
bet enough to win. For all these reasons, the team preferred to override the Best-Response
calculation, and to have Watson simply make the standard bet in this scenario. This
proved to be a judicious choice in hindsight, as there were five games in Series 1 where the
Best-Response would have bet anti-two-thirds, but B did not bet two-thirds in any of those
games.
For the Series 2 Sparring Games, the Best-Response computation had been sped up
enough to enable live wager computations. The team continued to debate whether to allow
anti-two-thirds bets, and ultimately decided it was worth the risk if Watson had unusually
low confidence (as happens, for example, in categories such as US Presidents, Shakespeare,
and US Cities). As it turned out, there were only two games in this series where the BestResponse would have bet anti-two-thirds. B bet two-thirds in one game but not in the other
game. Watson did not have low confidence in either FJ category, so there were no live test
results of the anti-two-thirds strategy.
We assessed the performance of the Best-Response strategy via the historical replacement technique presented in section 2.3; results are displayed in Table 4. The first column
gives the actual human win rates in the A, B, C roles. The second column shows win rates
for the constrained Best-Response deployed in the Series 1 Sparring Games, in which A
always bets to cover 2B. Note that this algorithm considerably improves over actual human
results as B and C, and provides a smaller but noticeable improvement over human A bets.
We attribute the latter gain partly to more consistent betting, and partly to judicious bets
in some cases to tie 2B, rather than trying to surpass 2B by $1. The last column gives
results for the full Best-Response algorithm including general Best-Response bets as A.
There is nearly a 1% improvement in As win rate over the constrained Best-Response; this
provides support for the efficacy of anti-two-thirds and other sophisticated strategies, but
is not quite statistically significant at 2092 trials.

A
B
C

Human
65.3%
28.2%
7.5%

Constrained Best-Response
67.0%
34.4%
10.5%

Full Best-Response
67.9%
34.4%
10.5%

Table 4: Comparison of actual human win rates with win rates of the constrained and
full Best-Response strategies, by historical replacement in 2092 non-locked FJ
situations from past episodes.

228

fiAnalysis of Watsons Strategies for Playing Jeopardy!

For the Exhibition Match, we devised live Best-Response algorithms for Game 1 and
Game 2 based on Monte-Carlo samples of the human betting models of section 2.6, and
probabilities of the eight right/wrong outcomes given Watsons FJ category confidence.
For the first-game FJ, we cant evaluate directly from the FJ outcomes since there is still
a second game to play. The evaluation is instead based on interpolation over the lookup
tables discussed in section 3.1.5 denoting Watsons match equities from various first-game
score combinations.
Due to modeling uncertainties in Game 2 FJ, we devoted much effort to interpreting
the Best-Response output in terms of logical betting rules, as well as deciding whether any
Best-Response decisions should be overridden. We ultimately decided to allow Watson to
venture an anti-two-thirds bet as A only if the predicted category confidence was unusually
low; otherwise Watson would always bet to guarantee a win as A by answering correctly.
For wagering as B, the betting rules would only attempt to finish ahead of A if it did not
diminish Watsons chances of finishing ahead of C. This naturally emerged from the match
utility function which assigned half-credit for a second place finish. Finally, for wagering
as C, the Best-Response output was too complex to derive human-interpretable rules, so
Watson was prepared to run the live calculation in this case. As it turned out, all of the
above work was superfluous, since Watson had a lockout in Game 2 of the Exhibition
Match.
3.3 Square Selection
We considered four different factors that could conceivably be relevant to the optimal overall
objective for Watson in deciding which square to select in a given game state:
 Selecting a Daily Double square: Finding the DDs quickly can provide an excellent
opportunity for Watson to significantly boost his game standing, while also denying
that opportunity to the other players. The potential downside is that Watson may
only have a small bankroll to wager, and may have little or no evidence as to assess
his likelihood of answering the DD clue correctly.
 Retaining control of the board: this involves estimating categories and/or square
values where Watson has the greatest chance to win the buzz and answer correctly.
This would give Watson another chance to try to find a DD, if the selected square
turns out to be a regular clue.
 Learning the essence of a category, i.e., gathering information about the category
such as the type of correct answers, so as to improve accuracy on subsequent clues
in the category (Prager et al., 2012). This consideration would suggest selecting lowvalue squares first, so that accuracy would be improved on higher-value squares.
 Maximizing expected score change: This concept seeks the best combination of high
expected accuracy with highest dollar value of available clues to obtain the biggest
boost in Watsons score on the next square.
We used the simulator to systematically investigate numerous weighted combination of
the above four factors. These studies were performed using Champion and Grand Champion human models, which featured overt DD seeking, aggressive DD wagering, and high
229

fiTesauro, Gondek, Lenchner, Fan & Prager

DD accuracy. Our results showed that, prior to all DDs being revealed, finding DDs is
overwhelmingly the top factor in maximizing Watsons win rate, and retaining control is
second in importance. Learning the essence of a category appears to provide an effective
strategy only after all DDs have been found, and maximizing expected score change did not
appear to be useful in improving Watsons win rate.
These findings led us to deploy an algorithm that selects squares as follows. First, if
there are any unrevealed DDs, a square i is selected that maximizes pDD (i)+pRC (i) where
pDD (i) is the probability that square i contains a DD, pRC (i) is an estimated probability
that Watson will retain control of the board if i does not contain a DD, and  = 0.1 yielded
the best win rate. The first term is calculated using Bayesian inference, as described below
in section 3.3.1. The second probability is estimated by combining the simulation model
of human performance on regular clues with a model of Watson that adjusts its attempt
rate, precision and buzzability as function of number of right/wrong answers previously
given in the category. Second, after all DDs in the round have been found, the algorithm
then switches to selecting the lowest dollar value in the category with the greatest potential
for learning about the category: this is based on the number of unrevealed clues in the
category and their total dollar value.
3.3.1 Bayesian DD Probability Calculation
We calculate pDD (i), the probability that square i contains a DD, according to principles
of Bayesian inference: we combine Bayesian prior probabilities, taken from historical frequencies of DD locations, with evidence from revealed questions according to Bayes rule,
to obtain posterior probabilities. The computation is easy to perform incrementally as each
individual question is revealed, and works somewhat differently in Round 1 than in Round
2, due to the different number of available DDs.
In Round 1, there is only one DD, so computation of posterior probabilities is easy. Let
p(i) denote the prior probability that square i contains a DD. Let p(i) = 1  p(i) denote
the prior probability that i does not contain a DD. Now assume that a square j is revealed
not to contain a DD. The posterior probability p(i|j) according to Bayes rule is given by:
p(i|j) =

p(j|i)p(i)
p(j)

(2)

where p(j|i) = 1 by definition, assuming i 6= j. Of course, once the DD has been revealed,
all p(i) values for other squares are set to zero.
In Round 2, there are two DDs, and their probabilities are not independent, since both
DDs cannot be located in the same column, plus there are column pair frequencies in
the historical data that may not be explainable by an independent placement model. We
therefore maintain a joint probability distribution p(i, j) indicating the probability that
squares i and j both contain DDs. We initialize p(i, j) to prior values, using joint DD
location frequencies in the historical data. Now assume that square k is revealed not to
contain a DD. The posterior probability p(i, j|k) is computed according to Bayes rule as:
p(i, j|k) =

p(k|i, j)p(i, j)
p(k)
230

(3)

fiAnalysis of Watsons Strategies for Playing Jeopardy!

where p(k) = 1  p(k) is the marginal distribution of a single DD, integrating over possible
locations of the second DD, and p(k|i, j) = 1 if k 6= i and k 6= j, else it equals 0. Note
that the constraint of two DDs never appearing in the same column is enforced by setting
the prior p(i, j) = 0 if squares i and j are in the same column. This guarantees that the
posterior will always equal 0, since Bayes rule performs multiplicative updates.
If a square k is discovered to contain a DD, then the rest of the board can be updated
similarly:
p(i, j|k) =

p(k|i, j)p(i, j)
p(k)

(4)

where p(k|i, j) = 1 if k = i or k = j, else it equals 0.
3.3.2 Square Selection Performance Metrics
Live DD strategy
LRTB
Simple DD seeking
Bayes (max pDD )
Bayes (max pDD )
max(pDD + 0.1pRC )

No live DD strategy
LRTB
LRTB
LRTB
Post-DD learning
Post-DD learning

Win rate
0.621
0.686
0.709
0.712
0.714

DDs found
0.322
0.510
0.562
0.562
0.562

Board control
0.512
0.518
0.520
0.520
0.520

Table 5: Simulation results in two-game matches vs. Grand Champions using various square
selection strategies (500k trials). LRTB denotes left-to-right, top-to-bottom square
selection.
In Table 5 we report on extensive benchmarking of Watsons performance using five
different combinations of various square selection algorithms. The first column denotes
strategy when there are available DDs to be played in the round, while the second column
denotes strategy after all DDs in the round have been played. These experiments utilized
the two-game match format with Grand Champion models of the human contestants. As
stated earlier, these human models employ aggressive DD and FJ wagering, and simple DD
seeking using the known row statistics when DDs are available. Simulations of Watson use
right/wrong answers drawn from historical categories, so that Watson will exhibit learning
from revealed answers in a category. As an interesting consequence of Watsons learning,
we model human square selection with no remaining DDs according to an anti-learning
strategy, intended to frustrate Watsons learning, by selecting at the bottom of the category
with greatest potential benefit from learning. We actually observed this behavior in informal
testing with very strong human players (Ed Toutant and David Sampugnaro) just before
the Exhibition Match, and there was evidence that Jennings and Rutter may have selected
some clues in the Exhibition based on this concept.
Results in Table 5 show that the weakest performance is obtained with an extremely
simple baseline strategy of Left-to-Right, Top-to-Bottom (LRTB), i.e., always selecting the
uppermost square in the leftmost available column, while our actual deployed strategy in
the Exhibition gives the strongest performance. Consistent with all our earlier findings,
231

fiTesauro, Gondek, Lenchner, Fan & Prager

we see in Table 5 that DD seeking is extremely important, especially when playing against
strong humans that overtly seek DDs. Our Bayesian DD seeking method is significantly
better than simple DD seeking based solely on the row frequencies of DDs. When Watson
and humans all use simple DD seeking, Watson finds 51.0% of the DDs (roughly in line
with its 51.8% average board control) and its match win rate is 68.6%. When Watson
switches to Bayesian DD seeking, its rate of finding DDs jumps to 56.2%, even though
board control is virtually unchanged at 52.0%, and its win rate increases by 2.3% to 70.9%.
On the other hand, if Watson does no DD seeking, and simply uses Left-to-Right, Top-toBottom selection, its rate of finding DDs plunges to 32.2% and its overall win rate drops to
62.1%.
The additional effects of seeking to retain control of the board, and selecting categories
with greatest potential for learning after all DDs are revealed, are smaller but statistically
significant after 500k trials. We find that optimizing the weight on pRC increases win rate
by 0.2%, and maximizing learning potential with no remaining DDs adds another 0.3% to
Watsons win rate.
3.4 Confidence Threshold for Attempting to Buzz
Watson will attempt to buzz in if the confidence in its top-rated answer exceeds an adjustable threshold value. In the vast majority of game states, the threshold was set to a
default value near 50%. While we did not have analysis indicating that this was an optimal
threshold, we did have a strong argument that a 50% threshold would maximize Watsons
expected score, which ought to be related to maximizing Watsons chance to win. Furthermore, it was clear that general default buzzing at 50% confidence was better than not
buzzing, since Watsons expected score change (0) would be the same in both cases, but
the opponents would have a much better chance to improve their scores if Watson did not
buzz.
From an approximate threshold calculation based on a Max-Delta objective (described
in Appendix 2), we had suggestive evidence that the initial buzz threshold should be more
aggressive. Subsequent more exact Monte-Carlo analysis for endgames (Appendix 2) and
for early game states (section 4.3) provides substantial backing for an aggressive initial
threshold below 50%. Nevertheless, since Watson tended to be slightly overconfident in
the vicinity of 50% nominal confidence, and since many of Watsons wrong answers in this
vicinity clearly revealed the correct answer to the opponents, the 50% default threshold
may have been a prudent choice.
Near the end of the game the optimal buzz threshold may vary significantly from the
default value. One special-case modified buzz policy that we devised for endgames uses a
lockout-preserving calculation. For Round 2 states with no remaining DDs, if Watson
has a big lead, we calculate whether he has a guaranteed lockout by not buzzing on the
current square. If so, and if the lockout is no longer guaranteed if Watson buzzes and is
wrong, we prohibit Watson from buzzing, regardless of confidence.
In principle, there is an exact optimal binary buzz-in policy within our simulation model
~  (c, D) = (B  (c, D), B  (c, D), B  (c, D), B  (c, D)) for any game state with a clue currently
B
3
2
1
0
in play, given Watsons confidence c and the dollar value D of the current clue. The policy
components Bi (c, D) result from testing whether c exceeds a set of optimal threshold values
232

fiAnalysis of Watsons Strategies for Playing Jeopardy!

{i , i = 0, 1, 2, 3}. There are four such values corresponding to the four possible states in
which Watson may buzz: the initial state, first rebound where human #1 answered incorrectly, first rebound where human #2 answered incorrectly, and the second rebound where
both humans answered incorrectly. The optimal policy can be calculated using Dynamic
Programming (DP) techniques (Bertsekas, 1995). This involves writing a recursion relation
between the value of a current game state with K clues remaining before FJ, and values of
the possible successor states with K  1 clues remaining:
VK (s) =

Z

(c)

5
X

j=1

p(Dj ) max

X

~
B(c,D
j) 

~ c)VK1 (s (, Dj ))dc
p(|B,

(5)

where (c) is the probability density of Watsons confidence, p(Dj ) denotes the probability
that the next square selected will be in row j with dollar value Dj = $400  j, the max
~ c) denotes the probabiloperates over Watsons possible buzz/no-buzz decisions, p(|B,

ity of various unit score-change combinations , and s denotes various possible successor
states after the Dj square has been played, and a score change combination  occurred.
(See Appendix 2 for a detailed discussion of how the recursion relation in Equation 5 is
calculated.)
We implemented an exact DP solver which successively expands the root state to all
successor states with K  1, K  2, ..., 0 clues remaining, where 0 denotes Final Jeopardy
states. The FJ states are evaluated by Monte-Carlo trials, and values are propagated
backward according to Equation 5 to ultimately compute the optimal buzz policy in the
root node state. While this computation is exact within our modeling assumptions, it is
too slow to use in live play if K  2, due to very high branching factor in the search tree.
In order to achieve acceptable real-time computation taking at most 1-2 seconds, we
therefore implemented an Approximate DP calculation in which Equation 5 is only used
in the first step to evaluate VK in terms of VK1 , and the VK1 values are then based
on plain Monte-Carlo trials (Tesauro & Galperin, 1996; Ginsberg, 1999; Sheppard, 2002).
Due to slowness of the exact DP calculation, we were unable to estimate accuracy of the
approximate method for K > 5. However, we did verify that Approximate DP usually
gave quite good threshold estimates (within 5% of the exact value) for K  5 remaining
squares, so this was our switchover point to invoke Approximate DP as deployed in the
live Series 2 Sparring Games against human champions. An analogous algorithm based on
match equities was also deployed in Game 2 of the Exhibition Match, but was indifferent
on the final five clues in the live game, since Watson had a guaranteed win after either
buzzing or not buzzing.
3.4.1 Illustrative Examples
The Approximate DP buzz-in algorithm easily handles, for example, a so-called desperation
buzz on the last clue, where Watson must buzz in and answer correctly to avoid being
locked out (e.g., suppose Watson has 4000, the human contestants have 10000 and 2000,
and the final clue value is $1200). Generally speaking, optimal endgame buzzing shows the
greatest deviation from default buzzing near certain critical score breakpoints, such as the
crossover from third to second place, or from second to first place. When a players score is
just below one of these breakpoints, aggressive buzzing is usually correct. Conversely, with
233

fiTesauro, Gondek, Lenchner, Fan & Prager

a score just above a critical breakpoint, players should buzz much more conservatively, to
guard against dropping below the breakpoint.
The most critical breakpoint is where a contestant achieves a guaranteed lockout. In
near-lockout situations, the algorithm may generate spectacular movements of the buzz
threshold that are hard to believe on first glance, but which can be appreciated after detailed
analysis. An example taken from the Sparring Games is a last-clue situation where Watson
had 28000, the humans had 13500 and 12800, and the clue value was $800. The (initially)
surprising result is that the optimal buzz threshold drops all the way to zero! This is because
after buzzing and answering incorrectly, Watson is no worse off than after not buzzing.
In either case, the human B player must buzz and answer correctly in order to avoid the
lockout. On the other hand, buzzing and answering correctly secures the win for Watson,
so this is a risk-free chance to try to buzz and win the game.
A more complex example occurred in a later game, where there were two squares remaining (the current one was $1200 and the final one was $2000), and Watson had 31600,
vs. 13000 and 6600 for the humans. Once again, any correct answer by Watson wins the
game. The analysis shows that Watson can buzz regardless of confidence on both this
clue and the next clue, and do just as well or better than not buzzing. On the first clue,
if Watson buzzes and is wrong, B needs to buzz and answer correctly, to reach a score
of 14200, otherwise Watson has a lockout at 30400. Now suppose Watson also gets the
second clue wrong, dropping to 28400. The score pair (28400, 14200) is now just as good
for Watson as the state (31600, 14200) if Watson did not attempt either clue. In both
cases, Watson has a guaranteed win unless B answers correctly. In fact, if B is alert,
she might deliberately not answer at (28400, 14200) as it is a lock-tie situation; this is
actually better for Watson than (31600, 14200), although our simulator does not model
such behavior.
A critical example of the converse situation, where Bs buzz-in threshold is much higher
than normal, occurred in an earlier game. On the final clue ($2000 value) Watson (A)
had 25200, B had 12800, and C had 2600, and Watson answered incorrectly on the initial
buzz, dropping to 23200. Despite being a Jeopardy! champion, B unfortunately buzzed on
the rebound and answered incorrectly, thereby locking himself out. Our analysis shows that
the rebound buzz is a massive blunder (although understandable in the heat of live play):
it offers no improvement in FJ chances if B is right, and forfeits any chance to win if B is
wrong. If the roles were reversed, Watson would have buzzed fairly aggressively on the
initial buzz, to prevent A from achieving the lockout, but never would have buzzed on the
rebound.
Finally, Figure 8 presents a $2000 last-square situation where we fix the human scores
at (13000, 6600) and systematically study how Watsons initial buzz threshold varies with
score. There are several examples of huge threshold changes as breakpoints are crossed.
For example, Watsons threshold goes from very aggressive (0.12) just below 13000, to
fairly conservative (0.65) just above 13000. There is additional complex behavior arising
from specific score combinations involving all three players. For example, at 6400 Watson
can take extra risk of answering incorrectly, due to the chance that A may also answer
incorrectly. This creates a situation where A=B+C and Watson has extra chances to
achieve a tie for first place. The same principle applies at 10400, where A=B+C arises if
Watson is wrong and A is right. A different combination comes into play at 10800 where
234

fiAnalysis of Watsons Strategies for Playing Jeopardy!

$2000 Last Clue Threshold, Human Scores (13000, 6600)
1

Confidence Threshold

0.8

0.6

0.4

0.2

0
5000

10000

15000
20000
Watson Score

25000

30000

Figure 8: Watsons initial buzz threshold vs. score on the last clue ($2000) before FJ.
Watson has extra incentive not to buzz: if either A or C are right, Watson can bet to
cover 2C and it will still constitute a two-thirds bet.
Figure 8 also shows huge swings where Watson is close to achieving a lockout. At 23000,
Watson will never buzz, since there is no chance to get a lockout. At 25000, Watson has
a free shot to try for the lockout, as discussed earlier. At 27000, Watson has a provisional
lockout but needs to take some risk to block a correct answer by B, and at 29000, Watson
has a free shot to prevent B from answering.

4. Lessons for Human Contestants
Now that Watson has retired as a Jeopardy! contestant, any future impact of our work
in improving Jeopardy! performance will relate specifically to human contestants. In this
section, we present a number of interesting insights that may help future contestants improve
their overall winning chances.
4.1 Basics of Final Jeopardy Strategy
The observed FJ wagers in our J! Archive dataset suggest that many contestants appearing
on the show devote scant effort to learning good strategies for FJ wagering, apart from the
elementary concept of A wagering at least 2B-A to cover Bs doubled score. While we dont
intend in this section to provide a definitive treatise on FJ strategy, we can illustrate what
we found to be the most important regions and separating boundaries in FJ strategy space
in a single plot, shown below in Figure 9.
Since FJ scenarios are scale invariant, any scenario is uniquely determined by two variables: Bs score relative to A, and Cs score relative to B. The ratio of B to A is the most
important quantity in Final Jeopardy, and the most important breakpoint (apart from
B<A/2 which is a lockout) is B=2A/3, illustrated by the solid red line. All contestants
should be familiar with the implications of this scenario, which is analogous to the game of
235

fiTesauro, Gondek, Lenchner, Fan & Prager

Matching Pennies, where A wins if the pennies match, and B wins if the pennies mismatch.
If B has at least two-thirds of A, B can secure a win whenever A is wrong by making a
small two-thirds bet (3B-2A), assuming that A bets to cover 2B. However, this strategy
is vulnerable to A making a small anti-two-thirds bet, which would give B no chance to
win. Conversely, As anti-two-thirds bet is vulnerable to B making a large bet to overtake
A. A related breakpoint is the case where B3A/4: in this situation Bs two-thirds bet can
overtake A, so that As anti-two-thirds option is eliminated.
Other important breakpoints include C=B/2 (green line): below this line, B can keep
out C with a small bet (B-2C), while above the line, B needs to bet at least (2C-B) to cover
Cs doubled score. The latter case may lead to a dilemma if (2C-B) exceeds the maximum
two-thirds bet (3B-2A). The demarcation where this dilemma occurs is the magenta curve
(2B=A+C), which is also known as the equal spacing breakpoint, since A-B=B-C.
Breakpoints that primarily affect C are the curves C=(A-B) (dark orange) and C=2(AB) (gray). Basically, C needs to be able to reach the 2(A-B) curve to have any chance to
win, so that C has no chance below the (A-B) curve. For scenarios lying between the two
curves, C has a minimum rational bet of at least 2(A-B)-C, although a larger bet may be
reasonable, for example, if CA/2 (dotted black curve) and can overtake A. Such a scenario
would also dissuade A from trying an anti-two-thirds bet against B.
When C has at least 2(A-B), the general rule is to bet small enough to stay above this
value. An additional upper bound emerging from our Best Response calculation occurs
when C2B/3 (blue line), in cases where B had more than 3A/4. In this case, B has an
incentive to bet to cover 2C, so that C has an opportunity to execute a two-thirds bet
against B, which may yield more wins than simply staying above 2(A-B).
Final Jeopardy Strategy Boundaries
1

0.8

C/B

0.6

0.4

B=2A/3
B=3A/4
C=B/2
C=2B/3
2B=A+C
C=A-B
C=2(A-B)
C=A/2

0.2

0
0.5

0.55

0.6

0.65

0.7

0.75
B/A

0.8

0.85

0.9

0.95

1

Figure 9: Illustration of important Final Jeopardy strategy regions and boundaries, as a
function of Bs score relative to A, and Cs score relative to B.

236

fiAnalysis of Watsons Strategies for Playing Jeopardy!

4.2 More Aggressive DD Wagering
As we mentioned earlier in section 3.1.4, our analysis indicates that human contestants
systematically err on the conservative side in DD wagering, given their actual likelihood of
answering the DD clue correctly. This may reflect underestimation or ignorance of their
likely DD accuracy, as well as a lack of quantitative means to estimate the impact of a
score increase or decrease on ones overall winning chances. Another possibility is that
contestants may recognize at some level that an aggressive bet is called for, but are too risk
averse to actually try it.
In this section we present a specific historical example to illustrate how our analysis
works, and to motivate the potential advantages of more aggressive wagering, as long as
the player has reasonably good confidence in being able to answer correctly. Our example
wager is taken from a J! Archive episode which aired during the last decade. The second
place player B found the last DD on the $800 clue in a category where one previous clue
($400) had been played; this clue was answered correctly by B. At that point, the scores
were all quite close, with B having 10400 while the opponents had 11400 and 9400. There
were eight remaining clues to be played, worth a total of $10800. B chose to wager only
$1000, got the DD right, and ultimately won the game.
We of course do not know Bs confidence in this situation, but we suspect it was reasonably good, because: (a) $800 second-round DDs tend to be easy, with average contestant
accuracy of about 72%; (b) B had already answered correctly the one previous clue in the
category; (c) wagers of $1000 tend not to be indicative of unusually low DD accuracy. Given
the above considerations, we suspect that B had at least a 70% chance of answering the DD
correctly, and the low wager was due to a desire to avoid dropping into third place after an
incorrect answer.
As one might surmise from reading section 3.1.4, our analysis suggests that at 70% confidence, the best wager is a True Daily Double, $10400. A plot of Monte-Carlo right/wrong
equity curves, and equity at 70% confidence, as a function of amount wagered is shown in
Figure 10. Note that for large bets, the red curve has a smaller magnitude slope than the
green curve, and it decelerates more rapidly. Once the bet is sufficiently large, there is little
incremental equity loss in increasing the bet, since the player has almost no chance to win at
that point. Conversely, there is strong incremental gain from increasing the bet and getting
the DD right. These factors tilt the calculation decidedly in favor of a maximal bet. Paradoxically, the almost certain loss after getting the DD wrong may be exactly why humans
avoid betting True DDs in this situation. Many psychological studies have documented irrational preferences for taking immediate gains, or avoiding immediate losses, which have
been attributed to so-called hyperbolic discounting (Ainslie, 2001). Since Jeopardy! is an
undiscounted game, correcting any natural tendencies towards overly short-term thinking
may be advisable for prospective contestants.
After a $1000 bet, B is either tied for first or tied for second, but the game is still very
close so there is little change in equity. MC estimates Bs equity at 39% if right and 31%
if wrong. However, after a true DD wager, B would either obtain a commanding lead with
20800 and an estimated equity of 70% after answering correctly, or drop to zero with only
about 3% equity after answering incorrectly. The equity difference between these two bets
237

fiTesauro, Gondek, Lenchner, Fan & Prager

is compelling at 70% confidence: betting $1000 gives 36.6% equity, while betting $10400
gives 49.9% equity, a hefty improvement of 13.3% in overall winning chances.
Historical (10400, 11400, 9400) last DD
0.8

DD wrong equity
DD right equity
Equity @ 70% confidence

0.7

DD player winprob

0.6
0.5
0.4
0.3
0.2
0.1
0
0

2000

4000

6000
DD bet

8000

10000

12000

Figure 10: Equities after getting the DD right or wrong and at 70% confidence, in the
example historical last-DD situation with scores (10400, 11400, 9400).

4.3 Counterintuitive Buzz-in Thresholds
Probably the most counterintuitive result of our analysis of buzz-in confidence thresholds is
that attempting to answer may be correct even if a player has negative equity expectation
in doing so. When we discussed Watsons 50% default threshold with human contestants
during the Sparring Games, many of them seemed surprised at such a low value, and some
even objected vociferously. While their arguments were not based on quantitative equity
estimates, they seem to intuitively recognize that Watsons game standing would diminish
on average after buzzing at 50% confidence, since Watsons score change would be zero
on average, but one of the opponent scores would likely increase. We tried to explain that
this is clearly better than the alternative of not buzzing, where Watson would again have
zero expected score change, but the unimpeded opponents would have greater chances for
a score increase.
Having developed an offline Monte-Carlo method for computing buzz-in thresholds for
human contestants (see Appendix 2 for details), we will compare the actual MC results
with a simple approximate formula for the threshold, derived below, which gives insight into
how a negative-expectation threshold comes about. A more complex analytic calculation,
yielding closed-form analytic expressions for all four threshold values, is detailed in the
Max-Delta approximation section of Appendix 2. This analysis also agrees closely with
our MC results.
We consider equities for one player (the strategic player) relative to a baseline state,
with equity E000 , where the clue expires with no score change. We aim to calculate a
238

fiAnalysis of Watsons Strategies for Playing Jeopardy!

confidence threshold 0 on the initial buzz such that if the players confidence c = 0 , then
the equities buzzing and not buzzing are equal, i.e., EN B (c) = EB (c).
Let N = EN B  E000 denote the expected equity change if the player does not buzz,
either initially on any rebound. This will depend on how often the opponents buzz and
their precision, but for good opponents, N should be some negative value. In our MC
simulations of early-game states with the refined Average Contestant model (where b and p
are estimated based on round and clue row number), N is approximately -0.86% for a $1000
clue in the first round. In order to understand the effects of correlated opponent buzzing
and precision, we also ran simulations with a corresponding uncorrelated model, obtaining
N -0.96% in the same situation.
Now consider the case where the player buzzes initially. For confidence values c in the
vicinity of 0 , if the player loses the buzz, we argue that the outcome should again be N :
since rebound thresholds are higher than 0 , as shown below, the player will not buzz on
any rebounds. Hence buzzing should only differ from not buzzing when the player wins the
buzz. After winning the buzz and answering correctly, the players equity will increase by
some positive equity gain G. For early $1000 first-round clues, G appears to be +3.25%,
regardless of whether the opponents are correlated or uncorrelated with the player. After an
incorrect answer, due to approximate linearity of equity with respect to score changes early
in the game, the player will have an equity loss  G, plus a likely further loss when the
opponents play the rebound. For uncorrelated opponents, the extra loss should be about
N , while for correlated opponents, it should be some lesser but still negative value N  ,
due to the fact that rebound precision is less than initial buzz precision in the correlated
model. Assuming N  = N for simplicity, confidence values c where buzzing is better than
not buzzing are determined by the following inequality:
cG + (1  c)(N  G)  N

(6)

Rearranging terms, we obtain: c  0 = G/(2GN ). Since N is negative, the confidence
threshold will be less than G/2G, i.e., less than 50%. For the above quoted values of G
and N , equation 6 yields a threshold value of 0 = 0.436 in the uncorrelated model, and
0 = 0.442 in the correlated model.
We now compare with the actual MC threshold calculation, seen in Table 6, for one
player (the leader) in a typical early game situation, where the first column has been
played, the scores are (1800, -600, 1000), and the DD has not been played. Similar threshold
values are robustly obtained in most early states, regardless of whether the player is leading
or trailing. As per section 3.4, 0 denotes the initial threshold, 1 and 2 denote the
first rebound thresholds, and 3 denotes the second rebound threshold. Note that the 0
values for $1000 clues in the correlated and uncorrelated models match very well with the
approximate formula values. As they are also statistically equal, this suggests that the MC
calculation for 0 is robust in that there is no sensitive dependence on an assumed modest
level of contestant correlation.
We also note an increase in first rebound thresholds for both models, and a further
increase in second rebound thresholds. This makes sense as the expected loss when not
buzzing, N , should diminish when the opponents are not eligible to buzz. For double
rebounds, N should equal 0, leading to a threshold of 0.5 according to the approximate
formula. The increase in rebound thresholds is modest for the uncorrelated model, but quite
239

fiTesauro, Gondek, Lenchner, Fan & Prager

significant for the correlated model. This is due to positive correlation of precision, implying
that a players posterior confidence is reduced after observing one or both opponents answer
incorrectly.
Similar experiments for $200 clues obtain a more aggressive initial threshold (42% vs
44%). This is as expected: since $200 clues are easier, the opponents are more likely to
buzz and answer correctly if the strategic player does not buzz. Hence the magnitude of
N relative to G should increase, yielding a lower threshold. While not shown in Table 6,
thresholds for $400, $600, and $800 clues take on plausible intermediate values between the
$200 and $1000 limiting cases.
clue value
$1000
$1000
$200
$200

(b , p )
(0.2, 0.2)
(0, 0)
(0.2, 0.2)
(0, 0)

0
0.44
0.44
0.42
0.42

1
0.67
0.49
0.69
0.47

2
0.68
0.50
0.68
0.48

3
0.78
0.53
0.83
0.54

Table 6: Early buzz-in thresholds in correlated and uncorrelated refined Average Contestant
models, based on 800K MC trials of the 20 end-of-clue states. Test position scores
(1800, -600, 1000), one column played, DD remains to be played.

In summary, while human contestants do not make precise confidence estimates, we
suspect that their buzz attempts are safely above 50%, where they are more likely to be
right than wrong. We would also be surprised if they became more cautious on rebounds,
after one or both opponents answered incorrectly. By contrast, our analysis suggests that
early in the game, it may be profitable to make slightly more speculative buzz attempts on
the initial buzz, where the odds are even or slightly against getting it right. An important
caveat is that such speculative guesses should not have a strong tip-off effect that would
significantly aid a rebounder.
We would also advocate exercising caution on rebounds. Despite the tip-off effect, there
is clear historical evidence that human precision is positively correlated, and declines on
rebounds. As seen in our correlated threshold calculation, contestants should have well
above 50% initial confidence to venture a rebound attempt, especially on a double rebound.
4.4 Lock-Tie Implications
We conclude this section by examining some of the strange and amusing consequences of
the Lock-Tie scenario in Final Jeopardy, where Bs score is exactly half of As score. In
this scenario, A is likely to bet nothing, so that B can achieve a tie for first place by betting
everything and getting FJ right. This is decidedly preferable to having more than half of
As score, where B would need to get FJ right and A to get FJ wrong in order to win. The
preference of B for a lower score can lead to some unusual strategy decisions (to say the
least) near the end of the game. For example, Dupee (1998) discusses DD wagering on the
last clue before Final Jeopardy, where the DD player has 7100 and the opponents have 9000
and 1000. Dupee advocates wagering $2600, which takes the lead with 9700 if correct, and
drops to a lock-tie at 4500 if incorrect.
240

fiAnalysis of Watsons Strategies for Playing Jeopardy!

Watsons analysis turns up many such last-clue DD situations where lock-tie considerations lead to unusual or even paradoxical bets. For example, in episode 5516, Greg Lindsay7
faced a last-clue DD decision, trailing with 6200 vs. 19200 and 9500. Greg wagered $6000
and ultimately won the game. However, Watson regards this as a serious error, and recommends wagering $3400 instead, which achieves a lock-tie at 9600 after a correct answer. We
also frequently find Watson wagering more than necessary on a last-clue DD to achieve a
lockout. In one such example, Watson had 26000 and the opponents had 19800 and 4400.
Watson only needed to bet $13600 to secure the lockout, but this puts Watson at 12400
after answering incorrectly. Watson instead bet $16100, which also achieves a lockout if
correct, but drops to a second-place lock-tie score of 9900 after answering incorrectly.
Watson also discovered that the lock-tie can influence wagering several squares before
the end of the game. In our analysis of historical last-DD human bets, we found a class of
situations where the DD player is trailing badly, and Watson recommends betting exactly
$100 less than a True Daily Double. An example situation is where the DD player has 5000,
the opponents have 21400 and 2800, and there are five remaining clues after the DD to be
played, worth a total of $6000. Watson recommends betting $4900, which certainly seems
weird and inconsequential, but there appears to be a real point to it. Note that the leaders
score of 21400 happens to be an odd multiple of 200 (107x200). Since all remaining clues
are even multiples of 200, the leaders score entering FJ will always be an odd multiple of
200. Now, in order to reach a lock-tie FJ, it follows that the DD players score must be
an odd multiple of 100. This is achieved by wagering $4900, with appreciable chances of a
lock-tie, instead of $5000 which makes the lock-tie impossible. As the $4900 bet offers 7.2%
equity instead of 6.4%, the lock-tie potential constitutes a substantial portion of overall
winning chances in this situation.
Finally, we note that lock-ties can provide an incentive for players to intentionally give
wrong answers. We were first alerted to this possibility by puzzle editor Peter Gordon, who
emailed a last-clue DD scenario where Watson has 6000 and the opponents have 10000
and -1000. Peter recommended that Watson bet $1000 and get it wrong on purpose! Such
a course of action would only require Watson to get FJ right in order to win, whereas after
a large DD bet to take the lead, Watson needs to get both the DD clue and FJ right in
order to win.
In subsequent testing of Watsons buzz-in strategy, we found a number of fascinating
last-clue scenarios where Watson reported that buzz/wrong on the rebound offers better
equity than either buzz/right or not buzzing. It turns out that these scenarios all occur
when A=2B-V, where V is the value of the last clue, and C is out of contention. (As an
example, suppose A=18800, B=10000, C=7600 and V=1200.) This situation allows B a
double chance to achieve a lock-tie! First of all, B should never buzz initially, both because
a wrong answer results in getting locked out, and because A may buzz and get it right,
which results in a lock-tie. Additionally, A may buzz and get it wrong, reducing As score
to A-V = 2(B-V). If this happens, B can again reach a lock-tie by buzzing and answering
incorrectly. This scenario is not as remote as one might think  it seems to occur about
once per season  and in the majority of cases, the lock-tie is spoiled by incorrect behavior
of B.
7. Greg Lindsay was the only contestant to win three Sparring Games against Watson.

241

fiTesauro, Gondek, Lenchner, Fan & Prager

5. Conclusions
By combining an original simulation model of Jeopardy! with state-of-the-art statistical
learning and optimization techniques, we created a set of real-time game strategy algorithms
that made Watson a much more formidable Jeopardy! contestant. As we have documented
in detail, our strategy methods resulted in a significant boost in Watsons expected win
rate in the Sparring Games and in the Exhibition Match, when compared with simple
heuristic strategies. For DD wagering, our neural net method obtained a 6% improvement
in win rate compared with our prior heuristic, and we estimate a further 0.6% improvement
by using live Monte-Carlo analysis for endgame DDs. For FJ betting, simulations show a
3% improvement over a simple heuristic that always bets to cover when leading, and bets
everything when trailing. As seen in Table 5, our best square selection method improves over
heuristics by 3-9%, depending on how much DD seeking is done by the heuristic. We have
no data on heuristic endgame buzzing, but a conservative guess is that our Approximate
DP method would achieve 0.5% to 1% greater win rate. The aggregate benefit of these
individual strategy improvements appears to be additive, since simulations put Watsons
win rate at 50% using all baseline strategies, versus 70% using all advanced strategies.
There is also ample evidence that each of our strategy algorithms exceeds human capabilities in real-time decision making. Historical replacement shows that Watsons BestResponse FJ wagering clearly outperforms human wagers. Watson is also better at finding
DDs than humans, as seen from the excess fraction of DDs found relative to its average
board control. According to simulations, this translates into a greater overall win rate.
In the cases of Daily Double wagering and endgame buzzing, it is clear that humans are
incapable in real time of anything like the precise equity estimates, confidence estimates, and
complex calculations performed by our algorithms to evaluate possible decisions. Watsons
error rate improves over humans by an order of magnitude in last-DD situations, as we saw
in section 3.1.4. There is likely to be a lesser but still significant improvement on earlier
DDs. It is difficult to identify human buzzing errors: a failure to buzz is indistinguishable
from losing the buzz, and even if a contestant buzzes and is wrong, the decision may be
correct at high enough confidence. We surmise that in most cases, human buzz errors are
small. Our algorithms main advantage is likely in handling special-case endgame states
where humans can make major errors, such as mishandling a lock-tie or needlessly locking
themselves out.
In addition to boosting Watsons results, our work provides the first-ever means of
quantitative analysis applicable to any Jeopardy! game state and covering all aspects of
game strategy. Consequently, we have unearthed a wealth of new insights regarding what
constitutes effective strategy, and how much of a difference strategy can make in a contestants overall ability to win. While we have illustrated numerous examples of such insights
throughout the paper, we expect that even greater understanding of proper Jeopardy! strategy will be obtained by further development and deployment of algorithms based on our
approaches. Just as top humans in classic board games (Chess, Backgammon, etc.) now
use computer software as invaluable study aids, we envision that studying with Jeopardy!
strategy software could become a vital part of contestant preparation to appear on the
show. Toward that end, we are currently developing a version of our DD wager calculator
242

fiAnalysis of Watsons Strategies for Playing Jeopardy!

to be deployed on J! Archive. This will nicely complement the existing FJ wager calculator,
and will make our DD analysis widely accessible for study by prospective contestants.
For simulation modelers, perhaps the most important take-home lesson from our work
on Watson is a reminder of the merits of starting from an approach based on extreme
simplification. It is generally appreciated that simulation predictions may be insensitive
to many low-level details. However, given the central role of the natural language clues
and category titles in Jeopardy! gameplay, it is at least mildly surprising that successful
simulation models may completely ignore the natural language content. One might have
also thought that simple mean-rate models would be inadequate, as they fail to capture
potentially important hot and cold streaks in specific categories, as well as variance across
contestants in general QA ability. Such factors are apparently not critically important to
model for purposes of optimizing Watsons strategies. Finally, it was not clear that we
could adequately predict expected outcomes of Watson vs. two humans from scant livegame data. We had only crude estimates of relative buzzability, etc., and made no attempt
to model the impact of Watsons unusual gameplay on human performance, or the tipoff benefit to humans when Watson answers incorrectly. Despite these limitations, the
validation studies of section 2.7 demonstrate remarkably accurate predictions of performance
metrics.
Looking beyond the immediate Jeopardy! domain, we also foresee more general applicability of our high-level approach to coupling Decision Analytics to QA Analytics, which
consists of building a simulation model of a domain (including other agents in the domain),
simulating short-term and long-term risks and rewards of QA-based decisions, and then
applying learning, optimization and Risk Analytics techniques to develop effective decision
policies. We are currently investigating applications of this high-level approach in health
care, dynamic pricing, and security (i.e., counter-terrorism) domains.

Acknowledgments
We are grateful to the entire worldwide team at IBM that made the Watson project possible, the team at Sony/JPI that made the Exhibition Match and Sparring Games possible,
and all the former Jeopardy! contestants who volunteered to participate in live test matches
with Watson. We thank the anonymous reviewers and Ed Toutant for many helpful comments and suggestions to improve the manuscript.

Appendix A. Watsons Competitive Record
Prior to appearing on Jeopardy!, Watson played more than 100 Sparring Games against
former Jeopardy! contestants in a realistic replica of a TV studio, which was constructed
at the IBM Research Center in Yorktown Heights, NY. The studio featured real Jeopardy!
contestant lecterns and signaling devices, and made use of the actual JPI (Jeopardy Productions Inc.) game-control system. The content for each game (categories, clues, answers)
was supplied directly by JPI, and consisted of actual Jeopardy! episodes that had already
been taped, but not yet aired. (This eliminated the possibility that contestants could have
previously seen the content used in their games.) A professional actor, Todd Crain, was
243

fiTesauro, Gondek, Lenchner, Fan & Prager

hired to host the games. To incentivize the contestants, they were paid $1000 for each
first-place finish, and $250 for each second-place finish.
An initial series of 73 games took place between Oct. 2009 and Mar. 2010. Contestants
were recruited by JPI, most of whom had appeared on the show only once or twice, and none
had appeared in more than three episodes. We considered these players as representative of
average human contestants that appear on the show. Results in this series of games were
that Watson finished first in 47 games (64.4%), second in 15 games (20.5%), and third
in 11 games (15.1%). We also note that 21 of Watsons 47 wins were by lockout, i.e.,
guaranteed wins where Watson could not be caught in Final Jeopardy.
Watson additionally played a second series of 55 games during Fall 2010, this time
against much stronger human opposition. These were contestants who had competed in the
shows annual Tournament of Champions, and had done well enough to reach the final or
semi-final rounds. Watson was also considerably improved in all respects, and in particular,
its full complement of advanced quantitative strategies was deployed in these games. (By
contrast, the only advanced strategy in most of the Series 1 games was for Final Jeopardy
betting.) Results in this series were as follows: Watson finished first in 39 games (70.9%),
second in 8 games (14.55%) and third in 8 games (14.55%). Watsons rate of winning by
lockout also improved, to 30 out of 39 games (76.9%), vs. 21/47 = 44.7% in the previous
series.
Finally, as witnessed by millions of viewers, Watson played a two-game Exhibition
match against Ken Jennings and Brad Rutter, arguably the two best human Jeopardy!
contestants of all time. Watson took the $1,000,000 first-place prize by lockout, with a
total score of 77,147. Ken Jennings took second place ($300,000) with a score of 24,000,
and Brad Rutter finished in third place ($200,000) with a score of 21,600.

Appendix B. Buzz Threshold Calculation Details
This Appendix presents computational details of our method for calculating initial buzz and
rebound buzz decisions in an endgame state with no remaining Daily Doubles, a current
selected square with dollar value D, and K remaining squares to be played after the current
square. We assume that we are optimizing the buzz decision of one player (the strategic
player), and that the two opponents are non-strategic players in that their buzz decisions
are determined by some fixed stochastic process. We further assume that the opponents
buzz decisions do not change going from the initial buzz to a rebound or second rebound. By
default the strategic player is Watson, although we have also developed a similar method
to compute confidence thresholds for human contestants.
The calculation as invoked by Watson in live play assumes that Watsons confidence
is not yet known, since computation must begin before the QA system has returned a
confidence value. We therefore pre-compute buzz decisions over discretized confidence values
between 0 and 1 with discretization interval typically set to 0.01.
B.1 Calculation for Watson vs. Two Humans
Calculation proceeds by diagramming a tree of possible events starting from the initial buzz
state and leading to all possible end-of-clue states, each corresponding to a different score
change combination. We denote the end-of-clue equities by Exyz , where the first index
244

fiAnalysis of Watsons Strategies for Playing Jeopardy!

denotes Watsons score change, and possible values of x, y and z are + (the players
score increased by D), 0 (score remained unchanged), and - (score decreased by D).
Since at most one contestant can have a score increase, there are a total of 20 end-of-clue
states: 12 where a contestant got the clue right, and eight where no one got it right.
The tree allows for the possibility that Watson may buzz or may not buzz in each of
the four live states (0=initial buzz, 1=rebound with human #1 wrong, 2=rebound with
human #2 wrong, 3=rebound with both humans wrong) where Watson is eligible to buzz.
Descending the tree starting from the initial buzz state, it assigns transition probabilities to
every branch, using our regular-clue model of human performance, along with probabilities
that Watson will win a contested buzz when one or two humans are attempting to buzz.
Having defined the tree, the transition probabilities, and a set of end-of-clue states, the
algorithm first estimates the end-of-clue equities by Monte-Carlo trials over the remaining
clues and Final Jeopardy. The MC trials make use of the stochastic process models of
human and Watson performance on regular clues that we presented earlier.
One useful trick we employ here is to reuse each MC trial over remaining clues in each
of the 20 end-of-clue states, instead of generating independent trials in those states. This
is done by first performing a trial in the (0, 0, 0) state, where no player attempted to buzz
in, and then offsetting the sequence of scores and the scores going into FJ by the specific
score change of each end-of-clue state. This enables faster computation and achieves more
statistically significant comparisons between states than would result from independent
trials. Additionally, while each trial is being performed, we monitor at each step whether
Watson has achieved a guaranteed lockout given the starting scores and the specific score
change combinations of each end-of-clue state. If so, we mark the trial as a guaranteed win
for that end-of-clue state: this obviates the need to simulate FJ in that trial, and makes the
simulations more faithful, since Watson actually uses lockout-preserving buzzing in live
play.
Having evaluated the end-of-clue states as described above, the calculation works backwards to evaluate progressively higher interior tree nodes. We first calculate confidenceindependent values of the Watson-ineligible states where Watson buzzes and is wrong.
There are three such states: IS0 (Watson wrong on the initial buzz), IS1 (Watson wrong
after human #1 is wrong) and IS2 (Watson wrong after human #2 is wrong). Formulas
for these values are written below.
To establish notation in these formulas, recall that our human models generate correlated
binary events at the start of a regular clue indicating whether the contestants attempt to
buzz, and whether they have a correct answer. These binary variables persist as the clue is
played, so that their buzz decisions and correctness do not change during rebounds. With
this in mind, we let b00 , b01 , b10 , b11 denote the probabilities of the four possible buzz/nobuzz joint decisions for a pair of humans, and p00 , p01 , p10 , p11 denote probabilities of the
four possible joint right/wrong outcomes. We typically assume symmetric human models
where b01 = b10 and p01 = p10 . We further let pH = p10 + p11 = p01 + p11 denote the singlecontestant human precision, and bH = b10 + b11 = b01 + b11 denote the single-contestant
human buzz attempt rate. These values may be fixed for all clue values, or we may use
estimated values that depend on round and row number, as depicted in Figure 5.
245

fiTesauro, Gondek, Lenchner, Fan & Prager

With the above notation, the formula for IS0 value is:
V (IS0) = b00 E00 + pH (b10 + b11 /2)(E+0 + E0+ ) + (1  pH )b10 (E0 + E0 )
+p01 b11 (E+ + E+ )/2 + p00 b11 E

(7)

Similarly, values of the IS1 and IS2 states are given by:
b11
b10
E0 +
V (IS1) =
bH
bH



p01 E+ + p00 E
1  pH



(8)

b11
b01
E0 +
V (IS2) =
bH
bH



p10 E+ + p00 E
1  pH



(9)

Note in the above expressions that we require conditional probabilities for the remaining
eligible human to buzz and answer correctly, given that the first human buzzed and answered
incorrectly. Using unconditional probabilities would correspond to a model that re-draws
buzz/no-buzz and right/wrong outcomes on each rebound, which is not consistent with our
model.
Next we evaluate the live states LS0, LS1, LS2, LS3 where Watson is eligible to buzz,
starting from the double-rebound state LS3, and working backwards to the first rebound
states LS1 and LS2, and finally the initial-buzz state LS0. We compute separate evaluations
in the cases where Watson buzzes or does not buzz; the larger of these determines the
optimal policy and optimal value function.
At a given Watson confidence level c, the values of the double-rebound state when
Watson buzzes or does not buzz are given respectively by:
VB (LS3, c) = cE+ + (1  c)E
VN B (LS3, c) = E0

(10)
(11)

Thus Watsons optimal buzz decision B  (LS3, c) = arg maxB,N B {VB (LS3, c), VN B (LS3, c)}
and the optimal state value is: V  (LS3, c) = max{VB (LS3, c), VN B (LS3, c)}.

Figure 11: Event tree for live state 2, where human #2 buzzed initially and was wrong.
246

fiAnalysis of Watsons Strategies for Playing Jeopardy!

Calculation of the first rebound state values proceeds as diagrammed in Figure 11,
which considers the LS2 state where human #2 buzzed first and answered incorrectly.
Red arrows indicate outcomes where Watson does not buzz. Green and brown arrows
indicate respective cases where Watson wins the buzz and loses the buzz. Z1 denotes the
probability that Watson wins a contested buzz against one human. The analogous tree for
LS1 is obtained by interchanging human #1  #2 indices.
b11 p10 E0+ + p00 V  (LS3, c)
b01
E00 +
bH
bH
1  pH
b11 Z1 + b01
[cE+0 + (1  c)V (IS2)]
VB (LS2, c) =
bH


b11 (1  Z1 ) p10 E0+ + p00 V  (LS3, c)
+
bH
1  pH


VN B (LS2, c) =



(12)

(13)

Figure 12: Event tree for live state 0, i.e., the initial buzz state.
Finally, Figure 12 illustrates the analysis of the initial buzz state LS0. Z2 denotes the
probability that Watson wins the buzz when both humans are buzzing.
VN B (LS0, c) = b00 E000 + (b10 + b11 /2)[pH (E0+0 + E00+ ) +
(1  pH )(V  (LS1, c) + V  (LS2, c))]

(14)

VB (LS0, c) = b00 (cE+00 + (1  c)E00 )
+((b01 + b10 )Z1 + b11 Z2 )(cE+00 + (1  c)V (IS0))
+(b10 (1  Z1 ) + b11 (1  Z2 )/2)(pH E0+0 + (1  pH )V  (LS1, c))
+(b01 (1  Z1 ) + b11 (1  Z2 )/2)(pH E00+ + (1  pH )V  (LS2, c)) (15)
247

fiTesauro, Gondek, Lenchner, Fan & Prager

B.2 Calculation for Human vs. Two Humans
Here we present an extension of the above calculation where the strategic player is a
human instead of Watson. This scenario introduces additional complexity in that, unlike
Watson, the strategic players performance is correlated with that of the opponents (and
vice versa).
Our approach hypothesizes a mechanism to generate correlated private confidence estimates (c0 , c1 , c2 ) for each player when the current clue is revealed, drawn from a suitable
multi-variate confidence distribution. We assume that the non-strategic players attempt
to buzz in when their private confidence value exceeds a fixed threshold, chosen so that
the probability mass above the threshold matches the desired attempt rate, and the first
moment above the threshold matches the desired precision. In the uniform (b, p) model over
all clues, as described in section 2.4, we would match the target values b = 0.61, p = 0.87
by using Beta distribution for each player, Beta(0.69, 0.40), with a buzz threshold value set
to 0.572. However, we obtain a more accurate and meaningful threshold model for humans
by fitting a different Beta distribution to each (b, p) parameter combination estimated by
round and row number, as plotted in Figure 5.
We obtain correlated draws from the resulting multi-variate Beta distribution via the
copula technique (Nelsen, 1999). This entails drawing ~x = (x0 , x1 , x2 ) from a suitably
~ =
correlated multi-variate normal distribution, mapping to the respective CDF values X
(X0 , X1 , X2 ) which lie in the unit interval, and then mapping these values to the inverse
CDF of the Beta distribution. Since the confidence draws are correlated, this will result
in correlated buzzing by the non-strategic players. We further obtain correlated precision
by similarly generating correlated uniform numbers in the unit interval to compare with
the players confidence values as a basis for assigning right/wrong answers. A correlation
coefficient of  0.4 matches the observed precision correlation in Average Contestants.
With such a model of correlated confidence-based buzzing and precision, we are now
equipped to make necessary modifications to the calculations described in Equations 7-15
and Figures 11,12. In every case, the opponent attempt rates and precisions need to be
conditioned on the strategic players confidence value c. We can make accurate numeric
estimates of these conditional probabilities by running many millions of trials with the
simulation model, discretizing the observed c in each trial, and recording at each discrete
level the number of times that 0, 1 or 2 opponents buzzed, and 0, 1, or 2 opponents had a
correct answer. Additionally, when considering buzzing on a first or second rebound, the
strategic player needs to estimate a posterior confidence given that one or two opponents
have already buzzed and answered incorrectly. This can result in a significant drop in
estimated confidence: for example, an initial confidence of 80% will drop to a posterior
value of only 50% in the double-rebound state LS3. Finally, in any of the ineligible states
IS0, IS1, IS2, the expected opponent precisions must also be conditioned upon the strategic
player buzzing and answering incorrectly.
B.3 Max-Delta Approximation
We developed a greatly simplified analytic approximation to the calculations given in Equations 7-15 by making the following assumptions: (i) the current game state is far from the
end of the game (i.e., the current clue value D is much smaller than the total value of all
248

fiAnalysis of Watsons Strategies for Playing Jeopardy!

remaining clues); (ii) all three players have intermediate probabilities of winning (i.e., not
close to 0 or 1). Under these assumptions, a players equity change at the end of the clue
should be approximately linear in the score changes of the three players. Intuitively, we may
write: E  (1 (S0  S1 ) + 2 (S0  S2 )), where S0 is the players score and S1 , S2 are
the opponent scores, recognizing that the chances of winning depend on score positioning
relative to the opponents. If we further assume that the opponent scores are similar (which
is often true early in the game), we then have 1  2  , an overall scaling factor. As
the clue value D is also an overall scaling factor, we can express the Max-Delta objective
of the buzz decision by rewriting the end-of-clue equities Exyz appearing in Equations 7-15
as Exyz = 2x  y  z.
To further facilitate analytic calculation, we also assume that the opponents buzz attempts and the precisions of the three players are all uncorrelated. The equities of the
ineligible states {IS0,IS1,IS2} then reduce to:
V (IS0) = 2(1  b)2  6bp(1  b/2)  2(1  p)b(1  b)  4p(1  p)b2
V (IS1) = V (IS2) = b  1  2bp

(16)

We can similarly rewrite the equities in the four live states {LS0,LS1,LS2,LS3} after
buzzing or not buzzing. For the double-rebound state LS3, these reduce to VB (LS3) = 4c
and VN B (LS3) = 2. At the threshold confidence c = 3 , we have 43 = 2, so that 3 = 0.5.
By likewise equating the buzz/no-buzz equities in the other live states, we can obtain closedform analytic expressions for the respective thresholds {0 , 1 , 2 }. For the first-rebound
thresholds, we have:
1 = 2 =

2 + b2 (1  Z1 )(1  2p) + b(2p  3)(1  Z1 )
(b(Z1  1) + 1)(b(2p  1) + 4)

(17)

This expression assumes that 1 , 2  3 so that the player will not buzz in the secondrebound state. Finally, the initial buzz threshold 0 (again assuming no-buzz in the rebound
states) is:
wm + 2(1  b)2  2wV (IS0)
0 =
(18)
4(1  b)2 + 4w  2wV (IS0)
where m = 2p + 2(1  p)(1 + b  2bp) is the total equity change after losing the buzz to
either opponent, and w = b(1  b)Z1 + 0.5b2 Z2 is the probability of beating one opponent
on the buzzer.
For average human contestants, we set b = 0.61, p = 0.87, Z1 = 1/2, and Z2 = 1/3,
yielding first-rebound thresholds 1 = 2 = 0.478 and initial buzz threshold 0 = 0.434. The
four Max-Delta thresholds computed here are quite close to the uncorrelated Monte-Carlo
values reported in Table 6 for simulations of average contestants in early game states.

References
Ainslie, G. (2001). Breakdown of Will. Cambridge Univ. Press.
Axelrod, R. (1984). The Evolution of Cooperation. Basic Books.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific.
249

fiTesauro, Gondek, Lenchner, Fan & Prager

Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms for stochastic scheduling
problems. J. of Heuristics, 5, 89108.
Billings, D. (2000). The first international RoShamBo programming competition. Intl.
Computer Games Assn. Journal, 23 (1), 4250.
Billings, D., Davidson, A., Schaeffer, J., & Szafron, D. (2002). The challenge of poker.
Artificial Intelligence, 134 (1-2), 201240.
Dupee, M. (1998). How to Get on Jeopardy! and Win! Citadel Press.
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., ..., & Welty,
C. (2010). Building Watson: an Overview of the DeepQA Project. AI Magazine, 31 (3),
5979.
Ferrucci, D. A. (2012). Introduction to This is Watson. IBM J. of Research and Development, 56 (3/4), 1:11:15.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press, Cambridge, MA.
Ginsberg, M. L. (1999). GIB: Steps toward an expert-level bridge-playing program. In
Dean, T. (Ed.), Proc. of the Sixteenth Intl. Joint Conf. on Artificial Intelligence, pp.
584589, San Francisco. Morgan Kaufmann Publishers.
Harris, B. (2006). Prisoner of Trebekistan: a decade in Jeopardy! Crown Publishers.
J! Archive (2013). J! Archive. http://www.j-archive.com. Online; accessed 22-March-2013.
Jeopardy! Gameplay (2013). Jeopardy! Gameplay  Wikipedia, the Free Encyclopedia.
http://en.wikipedia.org/wiki/Jeopardy!#Gameplay. Online; accessed 22-March-2013.
Leisch, F., Weingessel, A., & Hornik, K. (1998). On the generation of correlated artificial
binary data. Vienna Univ. of Economics and Business Administration, Working Paper
No. 13.
Nelsen, R. B. (1999). An Introduction to Copulas. Springer.
Prager, J. M., Brown, E. W., & Chu-Carroll, J. (2012). Special questions and techniques.
IBM J. of Research and Development, 56 (3/4), 11:111:13.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1987). Learning internal representations
by error propagation. In Rumelhart, D. E., McClelland, J. L., et al. (Eds.), Parallel
Distributed Processing: Volume 1: Foundations, pp. 318362. MIT Press, Cambridge.
Sheppard, B. (2002). World-championship-caliber Scrabble. Artificial Intelligence, 134 (1-2),
241275.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Commun. ACM, 38 (3),
5868.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using Monte-Carlo search.
In Advances in Neural Information Processing 9, pp. 10681074.
Tesauro, G., Gondek, D. C., Lenchner, J., Fan, J., & Prager, J. M. (2012). Simulation,
learning, and optimization techniques in Watsons game strategies. IBM J. of Research
and Development, 56 (34), 16:116:11.
250

fiAnalysis of Watsons Strategies for Playing Jeopardy!

Wu, F.-C., & Tsang, Y.-P. (2004). Second-order Monte Carlo uncertainty/variability analysis using correlated model parameters: application to salmonid embryo survival risk
assessment. Ecological Modelling, 177, 393414.

251

fiJournal of Artificial Intelligence Research 47 (2013) 441-473

Submitted 12/12; published 07/13

Decentralized Anti-coordination
Through Multi-agent Learning
Ludek Cigler
Boi Faltings

ludek.cigler@epfl.ch
boi.faltings@epfl.ch

Artificial Intelligence Laboratory
Ecole Polytechnique Federale de Lausanne
CH-1015 Lausanne, Switzerland

Abstract
To achieve an optimal outcome in many situations, agents need to choose distinct
actions from one another. This is the case notably in many resource allocation problems,
where a single resource can only be used by one agent at a time. How shall a designer of a
multi-agent system program its identical agents to behave each in a different way?
From a game theoretic perspective, such situations lead to undesirable Nash equilibria.
For example consider a resource allocation game in that two players compete for an exclusive
access to a single resource. It has three Nash equilibria. The two pure-strategy NE are
efficient, but not fair. The one mixed-strategy NE is fair, but not efficient. Aumanns
notion of correlated equilibrium fixes this problem: It assumes a correlation device that
suggests each agent an action to take.
However, such a smart coordination device might not be available. We propose using
a randomly chosen, stupid integer coordination signal. Smart agents learn which action
they should use for each value of the coordination signal.
We present a multi-agent learning algorithm that converges in polynomial number of
steps to a correlated equilibrium of a channel allocation game, a variant of the resource
allocation game. We show that the agents learn to play for each coordination signal value
a randomly chosen pure-strategy Nash equilibrium of the game. Therefore, the outcome
is an efficient correlated equilibrium. This CE becomes more fair as the number of the
available coordination signal values increases.

1. Introduction
In many situations, agents have to coordinate their actions in order to use some limited
resource: In communication networks, a channel might be used by only one agent at a time.
When driving a car, an agent prefers to choose a road with less traffic, i.e. the one chosen
by a smaller number of other agents. When bidding for one item in several simultaneous
auctions, an agent prefers the auction with less participants, because this will usually lead
to a lower price. Such situations require agents to take each a different decision. However,
all the agents are identical, and the problem each one of them face is the same. How can
they learn to behave differently from everyone else?
Second problem arises when agents have common preferences over which action they
want to take: In the communication networks problem, every agent prefers to transmit over
being quiet. In the traffic situation, agents might all prefer the shorter path. But in order
to achieve an efficient allocation, it is necessary precisely for some agents to stay quiet, to
take the longer path. How can we achieve that those agents are not exploited? How can
c
2013
AI Access Foundation. All rights reserved.

fiCigler & Faltings

the agents learn to alternate, taking the longer road on one day, while taking the shorter
road the next day?
A central coordinator who possesses complete information about agents preferences
and about the available resources can easily recommend each agent which action to take.
However, such an omniscient central coordinator is not always available. Therefore, we
would like to be able to use a distributed scheme. We consider scenarios where the same
agents try to use the same set of resources repeatedly, and can use the history of the past
interactions to learn to coordinate the access to the resources in the future.
In particular, consider the problem of radio channel access. In this problem, N agents
try to transmit over C non-overlapping channels. Fully decentralized schemes, such as
ALOHA (Abramson, 1970), can only achieve a throughput of 1e  37%, i.e. any transmission only succeeds with probability 1e  0.37. More complex schemes, such as those based
on distributed constraint optimization (Cheng, Raja, Xie, & Howitt, 2009), can reach a
throughput close to 100%. By throughput, we mean the probability of successful transmission on a given channel. However, the messages necessary to implement these schemes
create an overhead that eliminates part of their benefits.
In this paper we propose instead to use a simple signal that all agents can observe and
that ergodically fluctuates. This signal could be a common clock, radio broadcasting on
a specified frequency, the decimal part of a price of a certain stock at a given time, etc.
Depending on this stupid signal, smart agents can then learn to take a different action
for each of its value. We say that the signal is stupid, because it doesnt have anything to
do with the game  it is the agents who give it a meaning themselves by acting differently
for each value of the coordination signal.
Lets look at the simplest example of a channel access problem: one where 2 agents try
to transmit over one shared channel. We can model this situation as a game in normal
form. Agents can choose between two actions: to stay quiet (Q) or to transmit (T ). Only
one agent may transmit successfully at a time. If an agent transmits alone, she receives a
positive payoff. If an agent does not transmit on the channel, her payoff is 0. If both agents
try to transmit on the channel at the same time, their transmissions fail and they incur a
cost c.
The payoff matrix of the game looks as follows:

Q
T

Q
0, 0
1, 0

T
0, 1
c, c

Such a game has two pure-strategy Nash equilibria (NE), where one player stays quiet
and the other one transmits. It has also one mixed-strategy NE, where each player stays
1
quiet with probability c+1
. The two pure-strategy NE are efficient, in that they maximize
the social welfare, but they are not fair: Only one player gets the full payoff, even though
the game is symmetric. The mixed-strategy NE is fair, but not efficient: The expected
payoff of both players is 0.
As such, the Nash equilibria of the game are rather undesirable: they are either efficient
or fair, but not both at the same time. In his seminal paper, Aumann (1974) proposed the
notion of correlated equilibrium that fixes this problem. A correlated equilibrium (CE) is
a probability distribution over the joint strategy profiles in the game. A correlation device
442

fiDecentralized Anti-coordination Through Multi-agent Learning

samples this distribution and recommends an action for each agent to play. The probability
distribution is a CE if agents do not have an incentive to deviate from the recommended
action.
In the simple game described above, there exists a CE that is both fair and socially
efficient: the correlation device samples from each of the two pure-strategy NE with probability 21 and then recommends the players which NE they should play. This corresponds
to an authority that tells each player whether to stay quiet or transmit.
Correlated equilibria have several nice properties: They are easier to find  for a succinct
representation of a game, in polynomial time, see (Papadimitriou & Roughgarden, 2008).
Also, every Nash equilibrium is a correlated equilibrium. Also, any convex combination of
two correlated equilibria is a correlated equilibrium. However, a smart correlation device
that randomizes over joint strategy profiles might not always be available.
It is possible to achieve a correlated equilibrium without the actual correlation device.
Assume that the game is played repeatedly, and that agents can observe the history of
actions taken by their opponents. They can learn to predict the future action (or a distribution of future actions) of the opponents. These predictions need to be calibrated, that is,
the predicted probability that an agent i will play a certain action aj should converge to
the actual frequency with which agent i plays action aj . Agents always play an action that
is the best response to their predictions of opponents actions. Foster and Vohra (1997)
showed that in such a case, the play converges to a set of correlated equilibria.
However, in their paper, Foster and Vohra did not provide a specific learning rule to
achieve a certain CE. Furthermore, their approach requires that every agent were able to
observe actions of every other opponent. If this requirement is not met, convergence to a
correlated equilibrium is not guaranteed anymore.
In this paper, we focus on a generalization of the simple channel allocation problem
described above. There are N agents who always have some data to transmit, and there
are C channels over which they can transmit. We assume that N  C. Access to a channel
is slotted, that is, all agents are synchronized so that they start transmissions at the same
time. Also, all transmissions must have the same length. If more than one agent attempts
to transmit over a single channel, a collision occurs and none of the transmissions are
successful. An unsuccessful transmission has a cost for the agent, since it has to consume
some of its (possibly constrained) power for no benefit. Not transmitting does not cost
anything.
We assume that agents only receive binary feedback. If they transmitted some data,
they find out whether their transmission was successful. If they did not transmit, they can
choose some channel to observe. They receive information whether the observed channel
was free or not.
When described as a normal-form game, this problem has several efficient (but unfair)
pure-strategy Nash equilibria, where a group of C agents gets assigned all the channels. The
remaining N  C agents get stranded. It has also a fair but inefficient mixed-strategy NE,
where agents choose the transmission channels at random. As in the example of a resource
allocation game above, there exists a correlated equilibrium that is efficient and fair.
The stupid coordination signal introduced in this paper helps the agents to learn to
play a (potentially) different efficient outcome for each of its value. This way, they can
reach an efficient allocation while still preserving some level of fairness.
443

fiCigler & Faltings

The main contributions of this work are the following:
 We propose a learning strategy for agents in the channel allocation game that, using
minimal information, converges in polynomial time to a randomly chosen efficient
pure-strategy Nash equilibrium of the game.
 We show that when the agents observe a common discrete correlation signal, they
learn to play such an efficient pure-strategy NE for each signal value. The result is
a correlated equilibrium that is increasingly fair as the number of available signals K
increases.
 We experimentally evaluate how sensitive the algorithm is to a player population
that is dynamic, i.e. when players leave and enter the system. We also evaluate
the algorithms resistance to noise, be it in the feedback players receive or in the
coordination signal they observe.
The channel allocation algorithm proposed in this paper has been implemented by Wang,
Wu, Hamdi, and Ni (2011) in a real-world wireless network setting. They showed how the
wireless devices can use all use the actual data which are being transmitted as a coordination
signal. That way, they were able to achieve 2-3 throughput gain compared to random
access protocols such as ALOHA.
It is worth noting that in this work, our focus is on reaching a correlated and fair
outcome, provided that the agents are willing to cooperate. In situations where using
resources costs nothing, a self-interested agent could stubbornly keep using it. Everyone
else will then be better off not trying to access the resource. This is sometimes called the
watch out I am crazy or bully strategy (Littman & Stone, 2002).
In order to prevent this kind of behavior, we would need to make sure that in order to
use a resource, an agent has to pay some cost. Such a cost may already be implicit in the
problem, such as the fact that wireless transmission costs energy, or it may be imposed by
external payments. In our recent work (Cigler & Faltings, 2012), we show how this leads to
equilibria where rational agents are indifferent between accessing the resource and yielding,
and how these equilibria implement our allocation policy for rational agents. We consider
this issue to be beyond the scope of this paper and refer the reader to our other work for a
deeper analysis.
The rest of the paper is organized as follows: In Section 2, we give some basic definitions
from game theory and the theory of Markov chains which we will use throughout the paper.
In Section 3, we present the algorithm agents use to learn an action for each possible
correlation signal value. In Section 4 we prove that such an algorithm converges to an
efficient correlated equilibrium in polynomial time in the number of agents and channels.
We show that the fairness of the resulting equilibria increases as the number of signals K
increases in Section 5. Section 6 highlights experiments that show the actual convergence
rate and fairness. We also show how the algorithm performs in case the population is
changing dynamically. In Section 7 we present some related work from game theory and
cognitive radio literature, and Section 8 concludes.
444

fiDecentralized Anti-coordination Through Multi-agent Learning

2. Preliminaries
In this section, we will introduce some basic concepts of game theory and of the theory of
Markov chains that we are going to use throughout the paper.
2.1 Game Theory
Game theory is the study of interactions among independent, self-interested agents. An
agent who participates in a game is called a player. Each player has a utility function
associated with each state of the world. Self-interested players take actions so as to achieve
a state of the world that maximizes their utility. Game theory studies and attempts to
predict the behaviour, as well as the final outcome of such interactions. Leyton-Brown and
Shoham (2008) give a more complete introduction to game theory.
The basic way to represent a strategic interaction (game) is using the so-called normal
form.
Definition 1. A finite, N -person normal-form game is a tuple (N, A, u), where
 N is a set of N players;
 A = A1  A2  . . .  AN , where Ai is a set of actions available to player i. Each vector
a = (a1 , a2 , . . . , aN )  A is called an action profile;
 u = (u1 , u2 , . . . , uN ), where ui : A  R is a utility function for player i that assigns
each action vector a certain utility (payoff).
When playing a game, players have to select their strategy. A pure strategy i for
player i selects only one action ai  Ai . A vector of pure strategies for each player  =
(1 , 2 , . . . , N ) is called a pure strategy profile. A mixed strategy selects a probability
distribution over the entire action space, i.e. i  (Ai ). A mixed strategy profile is a
vector of mixed strategies for each player.
Definition 2. We say that a mixed strategy i of player i is a best response to the strategy
profile of the opponents i if for any strategy i0 ,
ui (i , i )  ui (i0 , i )
One of the basic goals of game theory is to predict an outcome of a strategic interaction.
Such outcome should be stable  therefore, it is usually called an equilibrium. One requirement for an outcome to be an equilibrium is that none of the players has an incentive to
change their strategy, i.e. all players play their best-response to the strategies of the others.
This defines perhaps the most important equilibrium concept, the Nash equilibrium:
Definition 3. A strategy profile  = (1 , 2 , . . . , N ) is a Nash equilibrium (NE) if for
every player i, her strategy i is a best response to the strategies of the others i .
As essential as Nash equilibria are, they have several disadvantages. First, they may
be hard to find: Chen and Deng (2006) show that finding NE is PPAD-complete. Second,
there might be multiple Nash equilibria, as shown in the example in Section 1. Third, the
most efficient NE may not be the most fair one, even in a symmetric game. We give the
formal definition of the correlated equilibrium which fixes some of these issues:
445

fiCigler & Faltings

Definition 4. Given an N -player game (N, A, u), a correlated equilibrium is a tuple (v, , ),
where v is a tuple of random variables v = (v1 , v2 , . . . , vN ) whose domains are D =
(D1 , D2 , . . . , DN ),  is a joint probability distribution over v,  = (1 , 2 , . . . , N ) is a
vector of mappings i : Di 7 Ai , and for each player i and every mapping 0i : Di 7 Ai it
is the case that
X
X

(d)ui (1 (d1 ), 2 (d2 ), . . . , N (dN )) 
(d)ui 01 (d1 ), 02 (d2 ), . . . , 0N (dN ) .
dD

dD

2.2 Markov Chains
The learning algorithm we propose and analyze in this paper can be described as a randomized algorithm. In a randomized algorithm, some of its steps depend on the value of a
random variable. One useful technique to analyze randomized algorithms is to describe its
execution as a Markov chain.
A Markov chain is a random process with the Markov property. A random process is a
collection of random variables; usually it describes the evolution of some random value over
time. A process has a Markov property if its state (or value) in the next time step depends
exclusively on its value in the previous step, and not on the values further in the past.
We can say that the process is memoryless. If we imagine the execution of a randomized
algorithm as a finite-state automaton with non-deterministic steps, it is easy to see how its
execution maps to a Markov chain.
The formal definition of a Markov chain is as follows:
Definition 5. (Norris, 1998) Let I be a countable set. Each i  I is called a state and I
is called the state space. We say that P= (i : i  I) is a measure on I if 0  i <  for
all i  I. If in addition the total mass iI i equals 1, then we call  a distribution. We
work throughout with a probability space (, F, P). Recall that a random variable X with
values in I is a function X :   I. Suppose we set
i = Pr(X = i) = Pr ({   : X() = i}) .
Then  defines a distribution, the distribution of X. We think of X as modelling a random
state that takes value i with probability i .
We say that a matrix P = (pij : i, j  I) is stochastic if every row (pij : j  I) is a
distribution.
We say that (Xt )t0 is a Markov chain with initial distribution  and a transition matrix
P if
1. X0 has distribution ;
2. for t  0, conditional on Xt = i, Xt+1 has distribution (pij : j  I) and is independent
of X0 , X1 , . . . , Xt1 .
More explicitly, the conditions state that, for t  0 and i0 , . . . , it+1  I,
1. Pr(X0 = i0 ) = i0 ;
2. Pr(Xt+1 = it+1 |X0 = i0 , . . . , Xt = it ) = pit it+1 .
446

fiDecentralized Anti-coordination Through Multi-agent Learning

Theorem 1. Let A be a set of states. The vector of hitting probabilities hA = (hA
i : i 
{0, 1, . . . , N }) is the minimal non-negative solution to the system of linear equations

1
for i  A
A
hi = P
A for i 
p
h
/A
j{0,1,...,N } ij j
Intuitively, the hitting probability hA
i is the probability that when the Markov chain
starts in state i, it will ever reach some of the states in A.
One property of randomized algorithms that we are particularly interested in is its
convergence. If we have a set of states A where the algorithm has converged, we can define
the time it takes to reach any state in the set A from any other state of the corresponding
Markov chain as the hitting time:
Definition 6. (Norris, 1998) Let (Xt )t0 be a Markov chain with state space I. The hitting
time of a subset A  I is a random variable H A :   {0, 1, . . .}  {} given by
H A () = inf{t  0 : Xt ()  A}
Specifically, we are interested in the expected hitting time of a set of states A, given that
the Markov chain starts in an initial state X0 = i. We will denote this quantity
kiA = Ei (H A ).
In general, the expected hitting time of a set of states A can be found by solving a
system of linear equations.
Theorem 2. The vector of expected hitting times k A = E(H A ) = (kiA : i  I) is the
minimal non-negative solution to the system of linear equations
 A
ki = 0 P
for i  A
(1)
A for i 
kiA = 1 + j A
p
k
/A
ij
j
/
Convergence to an absorbing state may not be guaranteed for a general Markov chain.
To calculate the probability of reaching an absorbing state, we can use the following theorem
(Norris, 1998):
Theorem 3. Let A be a set of states. The vector of hitting probabilities hA = (hA
i : i 
{0, 1, . . . , N }) is the minimal non-negative solution to the system of linear equations

1
for i  A
A
hi = P
A for i 
p
h
/A
ij
j
j{0,1,...,N }
Solving the systems of linear equations in Theorems 2 and 3 analytically might be
difficult for many Markov chains though. Fortunately, when the Markov chain has only
one absorbing state i = 0, and it can only move from state i to j if i  j, we can use
the following theorem to derive an upper bound on the expected hitting time, proved by
Rego (1992):
Theorem 4. Let A = {0}. If
i  1 : E(Xt+1 |Xt = i) <
for some  > 1, then


kiA < log i +

447


1

i


fiCigler & Faltings

3. Learning Algorithm
In this section, we describe the algorithm that the agents use to learn a correlated equilibrium of the channel allocation game.
Let us denote the space of available correlation signals K := {0, 1, . . . , K  1}, and the
space of available channels C := {1, 2, . . . , C}. Assume that C  N , that is there are more
agents than channels (the opposite case is easier). An agent i has a strategy fi : K  {0}C
that she uses to decide which channel she will access at time t when she receives a correlation
signal kt . When fi (kt ) = 0, the agent does not transmit at all for signal kt . The agent stores
its strategy simply as a table.
Each agent adapts her strategy as follows:
1. In the beginning, for each k0  K, fi (k0 ) is initialized uniformly at random from C.
That is, every agent picks a random channel to transmit on, and no agent will monitor
other channels.
2. At time t:
 If fi (kt ) > 0, the agent tries to transmit on channel fi (kt ).
 If otherwise fi (kt ) = 0, the agent chooses a random channel mi (t)  C that she
will monitor for activity.
3. Subsequently, the agent observes the outcome of its choice: if the agent transmitted
on some channel, she observes whether the transmission was successful. If it was,
the agent will keep her strategy unchanged. If a collision occurred, the agent sets
fi (kt ) := 0 with probability p. With probability 1  p, the strategy remains the same.
4. If the agent did not transmit, she observes whether the channel mi (t) she monitored
was free. If that channel was free, the agent sets fi (kt ) := mi (t) with probability 1.
If the channel was not free, the strategy fi remains the same.

4. Convergence
An important property of the learning algorithm is if, and how fast it can converge to a
pure-strategy Nash equilibrium of the channel allocation game for every signal value. The
algorithm is randomized. Therefore, instead of analyzing its worst-case behavior (that may
be arbitrarily bad), we will analyze its expected number of steps before convergence.
4.1 Convergence for C = 1, K = 1
For single channel and single coordination signal, we prove the following theorem:
Theorem 5. For N agents and C = 1, K = 1, 0 < p < 1, the expected number of steps
before the allocation algorithm
converges
to a pure-strategy Nash equilibrium of the channel


1
allocation game is O p(1p) log N .
To prove the convergence of the algorithm, it is useful to describe its execution as a
Markov chain.
448

fiDecentralized Anti-coordination Through Multi-agent Learning

When N agents compete for a single signal value, a state of the Markov chain is a
vector from {0, 1}N that denotes which agents are attempting to transmit. For the purpose
of the convergence proof, it is only important how many agents are trying to transmit, not
which agents. This is because the probability with which the agents back-off is the same
for everyone. Therefore, we can describe the algorithm execution using the following chain:
Definition 7. A Markov chain describing the execution of the allocation algorithm for
C = 1, K = 1, 0 < p < 1 is a chain whose state at time t is Xt  {0, 1, . . . , N }, where
Xt = j means that j agents are trying to transmit at time t.
The transition probabilities of this chain look as follows:
Pr (Xt+1 = N |Xt = 0) = 1
Pr (Xt+1 = 1|Xt = 1) = 1
 
i ij
Pr (Xt+1 = j|Xt = i) =
p (1  p)j
j

(restart)
(absorbing)
i > 1, j  i

All the other transition probabilities are 0. This is because when there are some agents
transmitting on some channel, no other agent will attempt to access it.
The probability Pr (Xt+1 = N |Xt = 0) is equal to 1 because once the channel becomes
free (Xt = 0), agents will spot this and time t + 1, everyone will transmit (therefore the
chain will be in state Xt+1 = N ). The probability Pr (Xt+1 = 1|Xt = 1) is equal to one
because once a single agent successfully transmits on the channel, she will keep transmitting
forever after, and no other agent will attempt to transmit there. Finally, the probability
Pr (Xt+1 = j|Xt = i) expresses the fact that when Xt = i (i agents transmit at time t), the
probability that an agent who transmitted at time t will keep transmitting at time t + 1
with probability 1  p.
We are interested in the number of steps it will take this Markov chain to first arrive
at state Xt = 1 given that it started in state X0 = N . This would mean that the agents
converged to a setting where only one of them is transmitting, and the others are not.
Definition 6 defined the hitting time which describes this quantity.
We will show the expected value E[h1 ] of the hitting time of state Xt = 1 (and by
corollary, prove Theorem 5) in the following steps:
1. We show the expected hitting time for a set of states A = {0, 1} (Lemma 6)
2. We then show probability that the Markov chain enters state 1 before entering state
0, when it starts from state i > 1 (Lemma 7)
3. Finally, using the law of iterated expectations, we combine the two lemmas to show
the expected hitting time of state 1.
Lemma 6. Let A = {0, 1}. The expected
hitting

 time of the set of states A in the Markov
1
chain described in Definition 7 is O p log N .
Proof. We will first prove that
hitting time of a set A0 = {0} in a slightly
 the expected

modified Markov chain is O p1 log N .
449

fiCigler & Faltings

Let us define a new Markov chain (Yt )t0 with the following transition probabilities:
Pr (Yt+1 = 0|Yt = 0) = 1
 
i ij
Pr (Yt+1 = j|Yt = i) =
p (1  p)j
j

(absorbing)
j  0, i  1

Note that the transition probabilities are the same as in the chain (Xt )t0 , except for
states 0 and 1. From state 1 there is a positive probability of going into state 0, and state 0
is now absorbing. Clearly, the expected hitting time of the set A0 = {0} in the new chain
is an upper bound on the expected hitting time of set A = {0, 1} in the old chain. This is
because any path that leads into state 0 in the new chain either does not go through state 1
(so it happened with the same probability in the old chain), or goes through state 1, so in
the old chain it would stop in state 1 (but it would be one step shorter).
If the chain is in state Yt = i, the next state Yt+1 is drawn from a binomial distribution
with parameters (i, 1  p). The expected next state is therefore
E(Yt+1 |Yt = i) = i(1  p)
1
We can therefore use the Theorem 4 with  := 1p
to derive that for A0 = {0}, the
hitting time is:


l
m 1
1
A0
ki < log 1 i +  O
log i
1p
p
p

that is also an upper bound on kiA for A = {0, 1} in the old chain.
Lemma 7. The probability hi that the Markov chain defined in Definition 7 enters state 1
before entering state 0, when started in any state i > 1, is greater than 1  p.
Proof. Calculating the probability that the chain X enters state 1 before state 0 is equal
to calculating the hitting probability, i.e. the probability that the chain ever enters a
given state, for a modified Markov chain where the probability of staying in state 0 is
Pr (Xt+1 = 0|Xt = 0) = 1. For a set of states A, let us denote hA
i the probability that the
Markov chain starting in state i ever enters some state in A. To calculate this probability,
we can use Theorem 3. For the modified Markov chain that cannot leave neither state 0 nor
state 1, computing hA
i for A = 1 is easy, since the matrix of the system of linear equations
is lower triangular.
Well show that hi  q = 1  p for i > 1 using induction. The first step is calculating hi
for i  {0, 1, 2}.
h0 = 0
h1 = 1
h2 = (1  p)2 h2 + 2p(1  p)h1 + p2 h0
2p(1  p)
2(1  p)
=
=
 1  p.
1  (1  p)2
2p
Now, in the induction step, derive a bound on hi by assuming hj  q = 1  p for all
j < i, j  2.
450

fiDecentralized Anti-coordination Through Multi-agent Learning

i  
X
i ij
hi =
p (1  p)j hj
j
j=0



i  
X
i ij
p (1  p)j q  ipi1 (1  p)(q  h1 )  pi h0
j
j=0

= q  ipi1 (1  p)(q  1)  q = 1  p.
This means that no matter which state i  2 the Markov chain starts in, it will enter
into state 1 earlier than into state 0 with probability at least 1  p.
We can now finish the proof of the bound on the expected hitting time of state 1. We
will use the law of iterated expectations:
Theorem 8. (Billingsley, 2012) Let X be a random variable satisfying E(|X|) <  and Y
another random variable on the same probability space. Then
E[X] = E [E[X|Y ]] ,
i.e., the expected value of X is equal to the conditional expected value of X given Y .
Let h1 be the random variable corresponding to the hitting time of state 1. Define a
random variable Z denoting the number of passes through state 0 our Markov chain makes
before it reaches state 1. From Theorem 8, we get:
E[h1 ] = E[E[h1 |Z]].
In Lemma 6, we have shown the expected number of steps hA before the Markov chain
reaches the set of states A = {0, 1}. We can write
E[E[h1 |Z]] = E

" Z
X

#
hA = E[Z  hA ] = hA  E[Z].

i=1

From Lemma 7 we know that the probability that the chain passes through state 1
before passing through 0 is greater than 1  p. Therefore, we can say that E[Z]  E[Z 0 ]
where Z 0 is a random variable distributed according to a geometric distribution with success
probability 1  p. Then


1
hA
0
=O
log N .
E[h1 ] = hA  E[Z]  hA  E[Z ] =
1p
p(1  p)
This concludes the proof of Theorem 5.
We have just shown that the expected time for convergence of our algorithm is finite,
and polynomial. What is the probability that the algorithm converge in a finite number of
steps to an absorbing state? The following theorem shows that since the expected hitting
time of the absorbing state is finite, this probability is 1.
451

fiCigler & Faltings

Theorem 9. Let h1 be the hitting time of the state Xt = 1 in the Markov chain from
Definition 7. Then
Pr(h1 is finite) = 1.
Proof. From the Markov inequality, we know that since h1  0,
Pr(h1  ) 

E[h1 ]
.


Therefore,
Pr(h1 is finite) = 1  lim Pr(h1  )  1  lim




E[h1 ]
= 1.


This means that our algorithm converges almost surely to a Nash equilibrium of the
channel allocation game.
4.2 Convergence for C  1, K = 1
Theorem 10. For N agents and C  1, K = 1, the expected number of steps before the
learning algorithm

h converges toia pure-strategy Nash equilibrium of the channel allocation
1
game is O C 1p p1 log N + C .
Proof. In the beginning, in at least one channel,
 there can be at most N agents who want
1
to transmit. It will take on average O p log N steps to get to a state when either 1 or 0
agents transmit (Lemma 6). We will call this period a round.
If all the agents backed off, it will take them on average at most C steps before some of
them find an empty channel. We call this period a break.
The channels might oscillate between the round and break periods in parallel, but
in the worst case, the whole system will oscillate
these two periods.
 between

1
For a single channel, it takes on average O 1p oscillations between these two periods
before there
 is only
 one agent who transmits in that channel. For C  1, it takes on
1
average O C 1p steps between round and break before all channels have only one

h
i
1
1
agent transmitting. Therefore, it will take on average O C 1p
log
N
+
C
steps before
p
the system converges.
4.3 Convergence for C  1, K  1
To show what is the convergence time when K > 1, we will use a more general problem.
Imagine that there are K identical instances of the same Markov chain. We know that the
original Markov chain converges from any initial state to an absorbing state in expected
time T . Now imagine a more complex Markov chain: In every step, it selects uniformly at
random one of the K instances of the original Markov chain, and executes one step of that
instance. What is the time Tall before all K instances converge to their absorbing states?
This is an extension of the well-known Coupon collectors problem (Feller, 1968). The
following theorem (Gast, 2011, Thm. 4) shows an upper bound on the expected number of
steps after which all the K instances of the original Markov chain converge:
452

fiDecentralized Anti-coordination Through Multi-agent Learning

Theorem 11. (Gast, 2011) Let there be K instances of the same Markov chain that is
known to converge to an absorbing state in expectation in T steps. If we select randomly
one Markov chain instance at a time and allow it to perform one step of the chain, it will
take on average E[Tall ]  T K log K + 2T K + 1 steps before all K instances converge to their
absorbing states.
For arbitrary C  1, K  1, the following theorem follows from Theorems 10 and 11:
Theorem 12. For N agents and C  1, K  1, 0 < p < 1, 0 < q < 1, the expected number
of steps before the learning algorithm converges to a pure-strategy Nash equilibrium of the
channel allocation game for every k  K is




1
1
O (K log K + 2K)C
C + log N + 1 .
1p
p
Aumann (1974) shows that any Nash equilibrium is a correlated equilibrium, and any
convex combination of correlated equilibria is a correlated equilibrium. We also know that
all the pure-strategy Nash equilibria that the algorithm converges to are efficient: there are
no collisions, and in every channel for every signal value, some agent transmits. Therefore,
we conclude the following:
Theorem 13. The learning algorithm defined in Section 3 converges in expected polynomial
1
time (with respect to K, C, p1 , 1p
and log N ) to an efficient correlated equilibrium of the
channel allocation game.

5. Fairness
Agents decide their strategy independently for each value of the coordination signal. Therefore, every agent has an equal chance that the game converges to an equilibrium that is
favorable to her. If the agent can transmit in the resulting equilibrium for a given signal
value, we say that the agent wins the slot. For C available channels and N agents, an agent
C
wins a given slot with probability N
(since no agent can transmit in two channels at the
same time).
We analyse the fairness of our algorithm after it has converged to a correlated equilibrium. While algorithms which do not converge to an absorbing state (such as ALOHA),
need to be analysed for all the intermediate states of their execution, we believe that our approach is justified by the fact that our algorithm converges relatively quickly, in polynomial
time.
We can describe the number of signals for which an agent i wins some channel as a
random variable Xi . This variable is distributed according to a binomial distribution with
C
parameters K, N
.
As a measure of fairness, we use the Jain index (Jain, Chiu, & Hawe, 1984). The
advantage of Jain index is that it is continuous, so that a resource allocation that is strictly
more fair has higher Jain index (unlike measures which only assign binary values, such as
whether at least half of the agents access some resource). Also, Jain index is independent of
the population size, unlike measures such as the standard deviation of the agent allocation.
453

fiCigler & Faltings

For a random variable X, the Jain index is the following:
J(X) =

(E[X])2
E[X 2 ]

C
), its
When X is distributed according to a binomial distribution with parameters (K, N
first and second moments are

C
E[X] = K 
N


 2
C 2
C N C
E X = K
+K 

,
N
N
N
so the Jain index is
J(X) =

C K
.
C  K + (N  C)

(2)

For the Jain index it holds that 0 < J(X)  1. An allocation is considered fair if
J(X) = 1.

N
Theorem 14. For any C, if K =  N
C , that is the limit limN  CK = 0, then
lim J(X) = 1,

N 

so the allocation becomes fair as N goes to .
Proof. The theorem follows from the fact that
lim J(X) = lim

N 

N 

C K
C  K + (N  C)

For this limit to be equal to 1, we need
N C
=0
N  C  K
lim

that holds exactly when K = 
that we assume that C  N ).

N
C



(that is K grows asymptotically faster than

N
C;

note

For practical purposes, we may also need to know how big shall we choose K given C
and N . The following theorem shows that:
Theorem 15. Let  > 0. If
1
K>





N
1 ,
C

then J(X) > 1  .
Proof. The theorem follows straightforwardly from Equation 2.
454

fiDecentralized Anti-coordination Through Multi-agent Learning

5

Convergence steps

10

4

10

3

10

0

10

20

30

40

50

60

70

C

Figure 1: Average number of steps to convergence for N = 64, K = N and C 
{1, 2, . . . , N }.

6. Experimental Results
In all our experiments, we report average values over 128 runs of the same experiment.
Error bars in the graphs denote the interval which contains the true expected value with
probability 95%, provided that the samples follow normal distribution. The error bars
are missing either when the graph reports values obtained theoretically (Jain index for the
constant back-off scheme) or the confidence interval was too small for the scale of the graph.
6.1 Static Player Population
We will first analyze the case when the population of the players remains the same all the
time.
6.1.1 Convergence
First, we are interested in the convergence of our allocation algorithm. From Section 4
we know that it is polynomial. How many steps does the algorithm need to converge in
practice?
Figure 1 presents the average number of convergence steps for N = 64, K = N and
increasing number of available channels C  {1, 2, . . . , N }. Interestingly, the convergence
takes the longest time when C = N . The lowest convergence time is for C = N2 , and for
C = 1 it increases again.
What happens when we change the size of the signal space K? Figure 2 shows the
average number of steps to convergence for fixed N , C and varying K. Theoretically, we
455

fiCigler & Faltings

1400

Convergence steps

1200
1000
800
600
400
200
0
0

10

20

30

40

50

60

70

K

1

1

0.9

0.9

0.8

0.8

0.7

0.7

Jain index

Jain index

Figure 2: Average number of steps to convergence for N = 64, C =

0.6
0.5
K=N
K = Nlog N

0.4

0.2
0

20

40

60

80

100

120

0.5
K=2
K = 2log N
2

0.3

K = N2

0.2
0

140

N

and K  {2, . . . , N }.

0.6

0.4

2

0.3

N
2

K = 2N
20

40

60

80

100

120

140

N

(a) C = 1

(b) C =

N
2

Figure 3: Jain fairness index for different settings of C and K, for increasing N .

have shown that the number convergence steps is O(K log K) in Theorem 12. However, in
practice the convergence resembles linear dependency on K. This is because the algorithm
needs to converge for all the coordination signals.
6.1.2 Fairness

From Section 5, we know that when K =  N
C , the Jain fairness index converges to 1 as
N goes to infinity. But how fast is this convergence? How big do we need to choose K,
depending on N and C, to achieve a reasonable bound on fairness?
456

fiDecentralized Anti-coordination Through Multi-agent Learning

Figure 3 shows the Jain index as N increases, for C = 1 and C = N2 respectively, for
various settings of K. Even though every time when K =  N
C (that is, K grows faster
N
than C ) the Jain index increases (as shown in Theorem 14), there is a marked difference
between the various settings of K. When K = N
C , the Jain index is (from Equation 2):
J(X) =

N
.
2N  C

(3)

Therefore, for C = 1, the Jain index converges to 0.5, and for C =
equal to 23 for all N > 0, just as Figure 3 shows.

N
2,

the Jain index is

6.1.3 Optimizing Fairness
We saw how fair the outcome of the allocation algorithm is when agents consider the game
for each signal value independently. However, is it the best we can do? Can we further
improve the fairness, when each agent correlates her decisions for different signal values?
In a perfectly fair solution, every agent wins (and consequently can transmit) for the
same number of signal values. However, we assume that agents do not know how many
other agents there are in the system. Therefore, the agents do not know what is their fair
share of signal values to transmit for. Nevertheless, they can still use the information in
how many slots they already transmitted to decide whether they should back-off and stop
transmitting when a collision occurs.
Definition 8. For a strategy fit of an agent i in round t, we define its cardinality as the
number of signals for which this strategy tells the agent to access:
fi
	fi
|fit | = fi k  K|fit (k) > 0 fi
Intuitively, agents whose strategies have higher cardinality should back-off more often
than those with a strategy with low cardinality.
We compare the following variations of the channel allocation scheme, that differ from
the original one only in the probability with which agents back off on collisions:
Constant The scheme described in Section 3; Every agent backs off with the same constant
probability p.
Linear The back-off probability is p =

|fit |
K .

Exponential The back-off probability is p = 



|f t |
1 Ki

for some parameter 0 <  < 1.

Worst-agent-last In case of a collision, the agent who has the lowest |fit | does not back
off. The others who collided, do back off. This is a greedy algorithm that requires
more information than what we assume that the agents have.
To compare the fairness of the allocations in experiments, we need to define the Jain
index of an actual allocation. A resource allocation is a vector X = (X1 , X2 , . . . , XN ), where
Xi is the cardinality of the strategy used by agent i. For an allocation X, its Jain index is:
P
2
N
i=1 Xi
J(X) =
P
2
N N
i=1 Xi
457

fiCigler & Faltings

C = N/2, K = 2log2N
1
0.98

Jain index

0.96
0.94
0.92
0.9
Constant
Linear
Exponential
Worstagent

0.88
0.86
0

20

40

60

80

100

120

140

N

Figure 4: Jain fairness index of the channel allocation scheme for various back-off probabilities, C = N2 , K = 2 log2 N

Figure 4 shows the average Jain fairness index of an allocation for the back-off probability
variations. The fairness is approaching 1 for the worst-agent-last algorithm. It is the
worst if everyone is using the same back-off probability. As the ratio between the back-off
probability of the lowest-cardinality agent and the highest-cardinality agent decreases, the
fairness increases.
This shows that we can improve fairness by using different back-off probabilities. Nevertheless, the shape of the fairness curve is the same for all of them. Furthermore, the
exponential back off probabilities lead to much longer convergence, as shown on Figure 5.
For C = N2 , the convergence time for the linear and constant back-off schemes is similar.
The unrealistic worst-agent-last scheme is obviously the fastest, since it resolves collisions
in 1 step, unlike the other back-off schemes.
6.2 Dynamic Player Population
Now we will take a look at the performance of our algorithm when the population of players
is changing over time (either new players join or old players get replaced by new ones).
We will also analyze the case when there are errors in what the players observe  either
coordination signal or channel feedback is noisy.
6.2.1 Joining Players
In this section, we will present the results of experiments where a group of players joins the
system later. This corresponds to new nodes joining a wireless network. More precisely,
458

fiDecentralized Anti-coordination Through Multi-agent Learning

C = N/2, K = 2log2N
Constant
Linear
Exponential
Worstagent
Convergence steps

3

10

2

10

1

10

1

2

10

10
N

Figure 5: Convergence steps for various back-off probabilities.
25% of the players join the network from the beginning. The remaining 75% of the players
join the network later, one by one. A new player joins the network after the previous players
have converged to a perfect channel allocation.
We experiments with two ways of initializing a strategy of a new player.
Greedy Either, the joining players cannot observe how many other players there are already in the system. Therefore, their initial strategy tries to transmit in all possible
slots.
Polite Or, players do observe N (t), the number of other players who are already in the
system at time t, when the new player joins the system. Therefore, their initial
strategy tries to transmit in a slot only with probability N1(t) .
Figure 6 shows the Jain index of the final allocation when 75% of the players join later,
for C = 1. When the players who join are greedy, they are very aggressive. They start
transmitting in all slots. On the other hand, if they are polite, they are not aggressive
enough: A new player starts with a strategy that is as aggressive as the strategies of the
players who are already in the system. The difference is that the new player will experience
a collision in every slot she transmits in. The old players will only experience a collision in
1
N (t) of their slots. Therefore, they will back off in less slots.
Therefore, especially for the constant scheme, the resulting allocation is very unfair:
either it is better for the new players (when they are greedy) or to the older players (when
the players are polite).
This phenomenon is illustrated in Figure 7. It compares a measure called group fairness:
the average throughput of the last 25% of players who joined the network at the end (new
459

fiCigler & Faltings

C = 1, K = Nlog N, join delay = converge init population = 0.25, KPS
2

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

Jain index

Jain index

C = 1, K = Nlog2N, join delay = converge init population = 0.25
1

0.5
0.4
0.3

0.5
0.4
0.3

0.2

0.2

Constant
Linear
Worstplayer

0.1
0
5

10

15

20

25

30

35

40

45

Constant
Linear
Worstplayer

0.1
0
5

50

10

15

20

25

N

30

35

40

45

50

N

(a) Greedy

(b) Polite

Figure 6: Joining players, Jain index. C = 1 and K = N log2 N . The two graphs show the
results for the two ways of initializing the strategy of a new player.

C = 1, K = Nlog2N, join delay = converge init population = 0.25

C = 1, K = Nlog2N, join delay = converge init population = 0.25, KPS

5

0.8

Constant
Linear
Worstplayer

4.5

Constant
Linear
Worstplayer

0.7
0.6

3.5
Group fairness

Group fairness

4

3
2.5
2

0.5
0.4
0.3

1.5
0.2

1
0.5
0

10

20

30

40

50

N

0.1
0

10

20

30

40

50

N

(a) Greedy

(b) Polite

Figure 7: Joining players, group fairness. C = 1 and K = N log2 N . The two graphs show
the results for the two ways of initializing the strategy of a new player.

460

fiDecentralized Anti-coordination Through Multi-agent Learning

C = N/2, K = 2log2N, join delay = converge init population = 0.25

C = N/2, K = 2log2N, join delay = converge init population = 0.25, KPS

1

1

0.9

0.9

0.8

0.8
0.7

0.6

Jain index

Jain index

0.7

0.5
0.4
0.3

0.5
0.4
0.3

0.2

0.2

Constant
Linear
Worstplayer

0.1
0
5

0.6

10

15

20

25

30

35

40

45

Constant
Linear
Worstplayer

0.1

50

0
5

10

15

N

20

25

30

35

40

45

50

N

(a) Greedy

(b) Polite

Figure 8: Joining players, Jain index. C = N2 and K = 2 log2 N . The two graphs show the
results for the two ways of initializing the strategy of a new player.

players) divided by the average throughput of the first 25% of players who join the network
at the beginning (old players).
Lets look first at the case when the players are greedy. For the constant scheme, this
ratio is around 4.5. For the linear scheme, this ratio is lower, although increasing as N (the
total number of players) grows. For the worst-player-last scheme, the ratio stays constant
and interestingly, it is lower than 1, which means that old players are better off than
new players.
When players are polite, this situation is opposite. Old players are way better off than
new players. For the constant scheme, the throughput ratio is about 0.2.
Figures 8 and 9 show the same graphs for C = N2 . Here, the newly joining players
are worse off even when they start transmitting in every slot. This is because while they
experience a collision every time (because all channels in all slots are occupied), the old
players only experience a collision with a probability N1 . On the other hand, the overall
2

fairness of the whole population is better, because there are more channels to share and no
agent can use more than one channel.
The difference between the old and new players is even more pronounced when the new
players are polite.
6.2.2 Restarting Players
Another scenario we looked at was what happens when one of the old players switches off
and is replaced with a new player with a randomly initialized strategy. We say that such a
player got restarted. In a wireless network, this corresponds to a situation when a user
restarts their router. Note that the number of players in the network stays the same, it is
just that some of the players forget what they have learned and start from scratch.
Specifically, in every round, for every player there is a probability pR that she will be
restarted. After restart, she will start with a strategy that can be initialized in two ways:
461

fiCigler & Faltings

C = N/2, K = 2log N, join delay = converge init population = 0.25

C = N/2, K = 2log2N, join delay = converge init population = 0.25, KPS

2

0.95

1

Constant
Linear
Worstplayer

0.9

Constant
Linear
Worstplayer

0.9

0.85
Group fairness

Group fairness

0.8

0.8
0.75
0.7

0.6
0.5

0.65

0.4

0.6
0.55
0

0.7

10

20

30

40

50

N

0

10

20

30

40

50

N

(a) Greedy

(b) Polite

Figure 9: Joining players, group fairness. C = N2 and K = 2 log2 N . The two graphs show
the results for the two ways of initializing the strategy of a new player.

Greedy Assume that the player does not know N , the number of players in the system.
Then for each signal value k  K she chooses randomly fi (k)  C. That means that
she attempts to transmit in every slot on a randomly chosen channel.
Polite Assume the player does know N . For k  K, she chooses fi (k)  C with probability
C
N , and fi (k) := 0 otherwise.
Figure 10 shows the average overall throughput when N = 32, C = 1, and K = N log2 N
or K = N for the two initialization schemes. A dotted line in all the four graphs shows the
overall performance when players attempt to transmit in a randomly chosen channel with
C
probability N
. This baseline solution reaches 1e  37% average throughput.
As the probability of restart increases, the average throughput decreases. When players
get restarted and they are greedy, they attempt to transmit in every slot. If there is only
one channel available, this means that such a restarted player causes a collision in every slot.
Therefore, it is not surprising that when the restart probability pR = 101 and N = 32, the
throughput is virtually 0: In every step, in expectation at least one player will get restarted,
so there will be a collision almost always.
There is an interesting phase transition that occurs when pR  104 for K = N log2 N ,
and when pR  103 for K = N . There, the performance is about the same as in the
baseline random access scenario (that requires the players to know N though). Similar
phase transition occurs when players are polite, even though the resulting throughput is
higher, since the restarted players are less aggressive.
Yet another interesting, but not at all surprising, phenomenon is that while the worstplayer-last scheme still achieves the highest throughput, the constant back off scheme is
better than the linear back-off scheme. This is because for the average overall throughput,
it only matters how fast are the players able to reach a perfect allocation after a disruption.
The worst-player-last scheme is the fastest, since it resolves a collision in 1 step. The con462

fiDecentralized Anti-coordination Through Multi-agent Learning

N = 32, C = 1, K = Nlog2N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer

0.1
4

10

0.6

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

(a) Greedy, K = N log2 N

2

10

N = 32, C = 1, K = N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = N

0.6
0.5
0.4
0.3
0.2

0

10
Restart probability

(b) Polite, K = N log2 N

1

0.1

4

10

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

10

(c) Greedy, K = N

4

10
Restart probability

(d) Polite, K = N

Figure 10: Restarting players, throughput, N = 32, C = 1

463

2

10

fiCigler & Faltings

N = 32, C = N/2, K = Nlog2N, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = N/2, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer

0.1
4

10

0.6

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

(a) Greedy, K = 2 log2 N

10

N = 32, C = N/2, K = 2, KPS
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = N/2, K = 2

0.6
0.5
0.4
0.3
0.2

0

2

10
Restart probability

(b) Polite, K = 2 log2 N

1

0.1

4

10

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Restart probability

0

2

10

Constant
Linear
Worstplayer
6

10

(c) Greedy, K = 2

4

2

10
Restart probability

10

(d) Polite, K = 2

Figure 11: Restarting players, throughput, N = 32, C =

N
2

stant scheme with back-off probability p = 21 is worse (see Theorem 12). The linear scheme
is the slowest.
Figure 11 shows the average overall throughput for C = N2 , and K = log2 N or K = 2.
There is no substantial difference between when players are greedy or polite. Since there are
so many channels available, a restarted player will only cause a small number of collisions
(in one channel out of N2 in every slot), so the throughput will not decrease too much.
Also, the convergence time for linear and constant scheme is about the same when
C = N2 , so they both adapt to the disruption equally well.
6.2.3 Noisy Feedback
So far we assumed that players receive perfect feedback about whether their transmissions
were successful or not. They could also observe the activity on a given channel perfectly.
We are going to loosen this assumption now.
464

fiDecentralized Anti-coordination Through Multi-agent Learning

N = 32, C = N/2, K = Nlog2N
1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N
1

0.6
0.5
0.4
0.3
0.2
0.1
0

6

0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
10

0.6

0.1
4

10
Noisy feedback probability

0

2

10

Constant
Linear
Worstplayer
6

10

(a) C = 1, K = N log2 N

4

10
Noisy feedback probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 12: Noisy feedback, throughput, N = 32

N = 32, C = N/2, K = Nlog2N
1

0.98

0.98

0.96

0.96

0.94

0.94

0.92

0.92

Jain index

Jain index

N = 32, C = 1, K = Nlog2N
1

0.9
0.88
0.86

0.88
0.86

0.84

0.84
Constant
Linear
Worstplayer

0.82
0.8

0.9

6

10

4

10
Noisy feedback probability

0.8

2

10

Constant
Linear
Worstplayer

0.82
6

10

(a) C = 1, K = N log2 N

4

10
Noisy feedback probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 13: Noisy feedback, Jain index, N = 32
Suppose that in every step, every player has a probability pF that the feedback she
receives was wrong. That is, if the player transmitted, she will learn that the transmission
was successful when it was not, and vice versa. If the player observed some channel, she
will learn that the channel was free when in fact it was not (and vice versa). In the context
of wireless networks, this corresponds to an interference on the wireless channel.
How does this affect the learning?
In Figure 12 we show the average overall throughput when C = 1 and C = N2 respectively. For one channel, the constant scheme is better than the linear scheme, because it
adapts faster to disruptions. For C = N2 , both schemes are equivalent, because they are
equally fast to adapt. A phase transition occurs when the noisy feedback probability is
about pF = 102 .
465

fiCigler & Faltings

N = 32, C = N/2, K = Nlog2N

1

0.9

0.9

0.8

0.8

0.7

0.7

System throughput

System throughput

N = 32, C = 1, K = Nlog2N

1

0.6
0.5
0.4
0.3
0.2
0.1
0

0.6
0.5
0.4
0.3
0.2

Constant
Linear
Worstplayer
6

10

0.1
4

10
Noisy signal probability

0

2

10

Constant
Linear
Worstplayer
6

10

(a) C = 1, K = N log2 N

4

10
Noisy signal probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 14: Noisy coordination signal, throughput, N = 32
Figure 13 shows the Jain index of the allocation when players receive noisy feedback. As
usual, the linear scheme is better than the constant, even though its throuput is lower (as
we have shown above). Only when the overall throughput drops close to 0, all the schemes
obviously have almost the same fairness.
6.2.4 Noisy Coordination Signal
Our algorithm assumes that all players can observe the same coordination signal in every
step. But where does this signal come from? It may be some random noise on a given
frequency, an FM radio transmission etc. However, the coordination signal might be noisy,
and different players can observe a different value. This means that their learning would be
out of sync. In the wireless networks, this corresponds to clock drift.
To see what happens in such a case, we use the following experiment. In every step,
every player observes the correct signal (i.e. the one that is observed by everyone else) with
probability 1  pS . With probability pS she observes some other false signal (that is still
taken uniformly at random from the set {0, ..., K  1}).
The overall throughput is shown in Figure 14. We can see that the system is able to
cope with a fairly high level of noise in the signal, and the drop in throughput only occurs
as pS = 101 . As was the case for experiments with noisy feedback, the constant back-off
scheme is able to achieve a higher throughput thanks to its faster convergence.
The Jain index of the allocation (Figure 15) stays almost constant, only when the
throughput drops the Jain index increases. When the allocation is more random, it is also
more fair.
6.3 Generic Multi-agent Learning Algorithms
Several algorithms that are proved to converge to a correlated equilibrium have been proposed in the multi-agent learning literature. In the Introduction, we have mentioned three
such learning algorithms (Foster & Vohra, 1997; Hart & Mas-Colell, 2000; Blum & Man466

fiDecentralized Anti-coordination Through Multi-agent Learning

N = 32, C = N/2, K = Nlog2N
1

0.98

0.98

0.96

0.96

0.94

0.94

0.92

0.92

Jain index

Jain index

N = 32, C = 1, K = Nlog2N
1

0.9
0.88
0.86

0.88
0.86

0.84

0.84
Constant
Linear
Worstplayer

0.82
0.8

0.9

6

10

4

10
Noisy signal probability

0.8

2

10

Constant
Linear
Worstplayer

0.82
6

10

(a) C = 1, K = N log2 N

4

10
Noisy signal probability

(b) C =

N
2

2

10

, K = 2 log2 N

Figure 15: Noisy coordination signal, Jain index, N = 32
sour, 2007). However, the analysis of Foster and Vohra was only applicable to games of two
players. In this section, we will briefly recall the other two multi-agent learning algorithms
(Hart & Mas-Colell, 2000; Blum & Mansour, 2007), and compare their performance with
our algorithm presented in Section 3.
The two algorithms we will compare our algorithm to are based on the notion of minimizing regret the agents experience from adopting a certain strategy. Intuitively, we can
describe the concept of regret as follows: Imagine that an agent uses strategy  in a couple
of rounds of the game, and accumulates a certain payoff. We would like to know how does
this payoff compare to a payoff acquired by some simple alternative strategy  . The difference in the payoff between the strategy  and  is the regret the agent perceives (ex-post)
for choosing strategy  over strategy  .
What do we mean by simple strategy? One class of simple strategies are strategies
that always select the same action. The external regret compares the performance of the
strategy  to the performance of the best single action ex-post.
Another class of alternative strategies are strategies that modify strategy  slightly.
Every time the strategy  proposes to play action a, the alternative strategy  proposes
action a0 6= a instead. The internal regret is defined as the regret of strategy  compared to
the best such alternative strategy. When all the agents adopt a strategy with low internal
regret, they converge to a strategy profile that is close to a correlated equilibrium (also
shown in Blum & Mansour, 2007).
Hart and Mas-Colell (2000) present a simple multi-agent learning algorithm that is
guaranteed to converge to a correlated equilibrium. They assume that the players can
observe the actions of all their opponents in every round of the game. Players start by
choosing their actions randomly. Then they update their strategy as follows: Let ai be the
action that player i played in round t1. For each action aj  Ai , aj 6= ai , player i calculates
the difference between the average payoff she would have received had she played action aj
instead of ai in the past, and the average payoff she received so far while playing action ai .
As we mentioned above, we can call this difference the internal regret of playing action ai
467

fiCigler & Faltings

C = N/2

C=1
3

10

Constant backoff
HartMasColell
BlumMansour

Constant backoff
HartMasColell
BlumMansour
3

Convergence steps

Convergence steps

10
2

10

2

10

1

10

1

10

5

10

15
N

20

25

(a) C = 1

5

10

15
N

(b) C =

20

25

N
2

Figure 16: General multi-agent learning algorithms, convergence rate.

instead of action aj . The player then chooses the action to play in round t with probability
proportional to its internal regret compared to the previous action ai . Actions with negative
regret are never played. The previous action ai is played with positive probability  this
way, the strategy has a certain inertia.
Hart and Mas-Colell (2000) prove that if the agents adopt the adaptive procedure described above, the empirical distribution of the play (the relative frequency of playing a
certain pure strategy profile) converges almost surely to the set of correlated equilibria.
Blum and Mansour (2007) present a general technique to convert any learning algorithm
with low external regret to an algorithm with a low internal regret. The idea is to run
multiple copies of the external regret algorithm. In each step, each copy returns a probability
vector of playing each action. These probability vectors are then combined into one joint
probability vector. When the player observes the payoff of playing each action, she updates
the payoff beliefs of each external regret algorithms proportionally to the weight they had in
the joint probability vector. The authors then show that when the players all use a learning
algorithm with low internal regret, the empirical distribution of the game converges close
to a correlated equilibrium.
One of the low-external-regret algorithms that Blum and Mansour (2007) present is
the Polynomial Weights (PW) algorithm. There, a player keeps a weight for each of her
actions. In every round of the game, she updates the weight proportionally to the loss
(negative payoff) that action incurred in that round. Actions with higher weight get then
chosen with a higher probability.
We have implemented the two generic multi-agent learning algorithms: The internalregret-based algorithm of Hart and Mas-Colell (2000), and the PW algorithm of Blum and
Mansour (2007). In all our experiments, both algorithms always converge to a pure-strategy
Nash equilibrium of the channel allocation game, and therefore to an efficient allocation.
However, the resulting allocation is not fair, as only a subset of agents of size C can ever
access the channels.
468

fiDecentralized Anti-coordination Through Multi-agent Learning

C = N/2
1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

Jain index

Jain index

C=1
1

0.5
0.4
0.3

0.4
0.3

0.2

0.2
Constant backoff
HartMasColell
BlumMansour

0.1
0
5

0.5

10

15
N

20

Constant backoff
HartMasColell
BlumMansour

0.1

25

(a) C = 1

0
5

10

15
N

(b) C =

20

25

N
2

Figure 17: General multi-agent learning algorithms, Jain index.

Figure 16 shows the average number of rounds the algorithms take to converge to a
stable outcome. We compare their performance with our learning algorithm from Section 3.
For our learning algorithm, we set K = 1, so that it also only converges to a pure-strategy
Nash equilibrium of the game. We performed 128 runs of each algorithm for each scenario.
The error-bars in Figure 16 show the 95% confidence interval of the average, assuming that
the convergence times are distributed according to a normal distribution.
Not surprisingly, the generic algorithms of Hart and Mas-Colell (2000) and Blum and
Mansour (2007) cannot match the convergence speed of our algorithm, designed specifically
for the problem of channel allocation. As the generic algorithms converge to a pure-strategy
NE, the outcome is very unfair, and the Jain index is very low, as evidenced by Figure 17.
We dont report the confidence bounds for the Jain index, as in all of the experiments the
resulting Jain index was the same.

7. Related Work
Broadly speaking, in this paper we are interested in games where the payoff an agent receives
from a certain action is inversely proportional to the number of other agents who chose the
same action. How can we achieve efficient and fair outcome in such games? Variants of this
problem have been studied in several previous works.
The simplest such variant is the Minority game (Challet, Marsili, & Zhang, 2005). In
this game, N agents have to simultaneously choose between two actions. Agents who chose
an action that was chosen by a minority of agents receive a payoff of 1, whereas agents whose
action choice was in majority receive
 a payoff of 0. This game has many pure-strategy Nash
equilibria, where some group of N 21 agents chooses one action and the rest choose the
other action. Such equilibria are efficient, since the largest possible number of agents achieve
the maximum payoff. However, they are not fair: the payoff to the losing group of agents
is always 0. This game has also one mixed-strategy NE that is fair: every agent chooses its
469

fiCigler & Faltings

action randomly. This equilibrium,
 on the other hand, is not efficient: the expected size of
the minority group is lower than N 21 due to variance of the action selection.
Savit, Manuca, and Riolo (1999) show that if the agents receive feedback on which action
was in the minority, they can learn to coordinate better to achieve a more efficient outcome
in a repeated minority game. They do this by basing the agents decisions on the history
of past iterations. Cavagna (1999) shows that the same result can be achieved when agents
base their decisions on the value of some random coordination signal instead of using the
history. This is a direct inspiration for the idea of global coordination signal presented in
this paper.
The ideas from the literature on Minority games have recently found their way into
the cognitive radio literature. Mahonen and Petrova (2008) present a channel allocation
problem much like ours. The agents learn which channel they should use using a strategy
similar to the strategies for minority games. The difference is that instead of preferring the
action chosen by the minority, in the channel allocation problem, an agent prefers channels
which were not chosen by anyone else. Using this approach, Mahonen and Petrova are able
to achieve a stable throughput of about 50% even when the number of agents who try to
transmit over a channel increases. However, each agent is essentially choosing one out of a
fixed set of strategies, that they cannot adapt. Therefore, it is very difficult to achieve a
perfectly efficient channel allocation.
Wang et al. (2011) have implemented the algorithm from this work in an actual wireless
network. In their setting, wireless devices are able to monitor the activity on all the channels.
As a coordination signal, they have used the actual data packets that the agents send. The
authors have shown that in practice, the learning algorithm (which they call attachment
learning) improves the throughput 2-3 over the random access slotted ALOHA protocol.
Another, more general variant of our problem, called dispersion game was described by
Grenager, Powers, and Shoham (2002). In a dispersion game, agents can choose from several
actions, and they prefer the one that was chosen by the smallest number of agents. The
authors define a maximal dispersion outcome as an outcome where no agent can move to an
action with fewer agents. The set of maximal dispersion outcomes corresponds to the set of
pure-strategy Nash equilibria of the game. They propose various strategies to converge to
a maximal dispersion outcome, with different assumptions on the information available to
the agents. On the contrary with our work, the individual agents in the dispersion games
do not have any particular preference for the actions chosen or the equilibria which are
achieved. Therefore, there are no issues with achieving a fair outcome.
Verbeeck, Nowe, Parent, and Tuyls (2007) use reinforcement learning, namely linear
reward-inaction automata, to learn Nash equilibria in common and conflicting interest
games. For the class of conflicting interest games (to which our channel allocation game
belongs), they propose an algorithm that allows the agents to circulate between various
pure-strategy Nash equilibria, so that the outcome of the game is fair. In contrast with
our work, their solution requires more communication between agents, and it requires the
agents to know when the strategies converged. In addition, linear reward-inaction automata
are not guaranteed to converge to a pure-strategy NE in conflicting interest games; they
may only converge to pure strategies.
All the games discussed above, including the channel allocation game, form part of the
family of potential games introduced by Monderer and Shapley (1996). A game is called a
470

fiDecentralized Anti-coordination Through Multi-agent Learning

potential game if it admits a potential function. A potential function is defined for every
strategy profile, and quantifies the difference in payoffs when an agent unilaterally deviates
from a given strategy profile. There are different kinds of potential functions: exact (where
the difference in payoffs to the deviating agent corresponds directly to the difference in
potential function), ordinal (where just the sign of the potential difference is the same as
the sign of the payoff difference) etc.
Potential games have several nice properties. The most important is that any purestrategy Nash equilibrium is just a local maximum of the potential function. For finite
potential games, players can reach these equilibria by unilaterally playing the best-response,
no matter what initial strategy profile they start from.
The existence of a natural learning algorithm to reach Nash equilibria makes potential
games an interesting candidate for our future research. We would like to see to which kind of
correlated equilibria can the agents converge there, and if they can use a simple correlation
signal to coordinate.

8. Conclusions
In this paper, we proposed a new approach to reach efficient and fair solutions in multi-agent
resource allocation problems. Instead of using a centralized, smart coordination device to
compute the allocation, we use a stupid coordination signal, in general a random integer
k  {0, 1, . . . , K  1}, that has no a priori relation to the problem. Agents then are smart:
they learn, for each value of the coordination signal, which action they should take.
From a game-theoretic perspective, the ideal outcome of the game is a correlated equilibrium. Our results show that using a global coordination signal, agents can learn to play
a convex combination of pure-strategy Nash equilibria, that is a correlated equilibrium.
We showed a learning strategy that, for a variant of a channel allocation game, converges in expected polynomial number of steps to an efficient correlated equilibrium. We
also proved that this equilibrium becomes increasingly fair as K, the number of available
synchronization signals, increases.
We confirmed both the fast convergence as well as increasing fairness with increasing K
experimentally. We also investigated the performance of our learning strategy in case the
agent population is dynamic. When new agents join the population, our learning strategy is
still able to learn an efficient allocation. However, the fairness of this allocation will depend
on how greedy the initial strategies of the new agents are. When agents restart at random
intervals, it becomes more important how fast a strategy converges. A simple strategy where
everyone backs off from transmitting with a constant probability is able to achieve higher
throughput than a more sophisticated strategy where the back-off probability depends on in
how many slots an agent is already transmitting. We also showed experimentally that the
learning strategy is robust against noise in both the coordination signal, as well as in the
feedback the agents receive about channel use. In both of these noisy scenarios, faster convergence of the constant back-off scheme helped achieve a higher throughput than the more
fair linear back-off scheme. Finally, we compared the performance of out learning strategy with the generic multi-agent learning algorithms based on regret-minimization (Hart &
Mas-Colell, 2000; Blum & Mansour, 2007). While these generic algorithms are theoretically
proven to converge to a distribution of play which is close to a correlated equilibrium, they
471

fiCigler & Faltings

are not guaranteed to converge to a specific CE. Indeed, in our experiments, the algorithms
of Hart and Mas-Colell and Blum and Mansour always converged to the efficient but unfair
pure-strategy Nash equilibrium of the channel allocation game.
The learning algorithm presented in this paper has been implemented in a real wireless
network by Wang et al. (2011), who have shown that it achieves 2-3 higher throughput
than random access protocols such as ALOHA.
In this paper, we did not address the issue of whether non-cooperative but rational
agents would follow the protocol we outlined. In our other work (Cigler & Faltings, 2012), we
address this issue and show that under certain conditions, the protocol can be implemented
in Nash equilibrium strategies of the infinitely repeated resource allocation game.

References
Abramson, N. (1970). The ALOHA system: another alternative for computer communications. In Proceedings of the November 17-19, 1970, fall joint computer conference,
AFIPS 70 (Fall), pp. 281285, New York, NY, USA. ACM.
Aumann, R. (1974). Subjectivity and correlation in randomized strategies. Journal of
Mathematical Economics, 1 (1), 6796.
Billingsley, P. (2012). Probability and Measure (Wiley Series in Probability and Statistics)
(Anniversary Edition edition). Wiley.
Blum, A., & Mansour, Y. (2007). Algorithmic game theory. In Nisan, N., Roughgarden,
T., Tardos, E., & Vazirani, V. (Eds.), Algorithmic Game Theory, chap. 4. Cambridge
University Press.
Cavagna, A. (1999). Irrelevance of memory in the minority game. Physical Review E, 59 (4),
R3783R3786.
Challet, D., Marsili, M., & Zhang, Y.-C. (2005). Minority Games: Interacting Agents in
Financial Markets (Oxford Finance). Oxford University Press, New York, NY, USA.
Chen, X., & Deng, X. (2006). Settling the complexity of Two-Player nash equilibrium. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS06),
pp. 261272. IEEE.
Cheng, S., Raja, A., Xie, L., & Howitt, I. (2009). A distributed constraint optimization algorithm for dynamic load balancing in wlans. In The IJCAI-09 Workshop on Distributed
Constraint Reasoning (DCR).
Cigler, L., & Faltings, B. (2012). Symmetric subgame perfect equilibria for resource allocation. In (to appear) Proceedings of the 26th national conference on Artificial
intelligence (AAAI-12), Menlo Park, CA, USA. American Association for Artificial
Intelligence.
Feller, W. (1968). An Introduction to Probability Theory and Its Applications, Vol. 1, 3rd
Edition (3 edition). Wiley.
Foster, D. P., & Vohra, R. V. (1997). Calibrated learning and correlated equilibrium. Games
and Economic Behavior, 21 (1-2), 4055.
472

fiDecentralized Anti-coordination Through Multi-agent Learning

Gast, N. (2011). Computing hitting times via fluid approximation: application to the coupon
collector problem. ArXiv e-prints.
Grenager, T., Powers, R., & Shoham, Y. (2002). Dispersion games: general definitions and
some specific learning results. In Proceedings of the Eighteenth national conference
on Artificial intelligence (AAAI-02), pp. 398403, Menlo Park, CA, USA. American
Association for Artificial Intelligence.
Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68 (5), 11271150.
Jain, R. K., Chiu, D.-M. W., & Hawe, W. R. (1984). A quantitative measure of fairness and
discrimination for resource allocation in shared computer systems. Tech. rep., Digital
Equipment Corporation.
Leyton-Brown, K., & Shoham, Y. (2008). Essentials of Game Theory: A Concise, Multidisciplinary Introduction. Morgan & Claypool, San Rafael, CA.
Littman, M., & Stone, P. (2002). Implicit negotiation in repeated games intelligent agents
VIII. In Meyer, J.-J., & Tambe, M. (Eds.), Intelligent Agents VIII, Vol. 2333 of Lecture
Notes in Computer Science, chap. 29, pp. 393404. Springer Berlin / Heidelberg,
Berlin, Heidelberg.
Mahonen, P., & Petrova, M. (2008). Minority game for cognitive radios: Cooperating without cooperation. Physical Communication, 1 (2), 94102.
Monderer, D., & Shapley, L. S. (1996). Potential games. Games and Economic Behavior,
14 (1), 124  143.
Norris, J. R. (1998). Markov Chains (Cambridge Series in Statistical and Probabilistic
Mathematics). Cambridge University Press.
Papadimitriou, C. H., & Roughgarden, T. (2008). Computing correlated equilibria in multiplayer games. Journal of the ACM, 55 (3), 129.
Rego, V. (1992). Naive asymptotics for hitting time bounds in markov chains. Acta Informatica, 29 (6), 579594.
Savit, R., Manuca, R., & Riolo, R. (1999). Adaptive competition, market efficiency, and
phase transitions. Physical Review Letters, 82 (10), 22032206.
Verbeeck, K., Nowe, A., Parent, J., & Tuyls, K. (2007). Exploring selfish reinforcement
learning in repeated games with stochastic rewards. Autonomous Agents and MultiAgent Systems, 14 (3), 239269.
Wang, L., Wu, K., Hamdi, M., & Ni, L. M. (2011). Attachment learning for multi-channel
allocation in distributed OFDMA networks. Parallel and Distributed Systems, International Conference on, 0, 520527.

473

fiJournal of Artificial Intelligence Research 47 (2013) 281-311

Submitted 10/12; published 6/13

Sharing Rewards in Cooperative Connectivity Games
Yoram Bachrach

yobach@microsoft.com

Microsoft Research, Cambridge, UK

Ely Porat

porately@cs.biu.ac.il

Bar-Ilan University, Ramat-Gan, Israel

Jeffrey S. Rosenschein

jeff@cs.huji.ac.il

The Hebrew University, Jerusalem, Israel

Abstract
We consider how selfish agents are likely to share revenues derived from maintaining
connectivity between important network servers. We model a network where a failure of
one node may disrupt communication between other nodes as a cooperative game called
the vertex Connectivity Game (CG). In this game, each agent owns a vertex, and controls
all the edges going to and from that vertex. A coalition of agents wins if it fully connects
a certain subset of vertices in the graph, called the primary vertices.
Power indices measure an agents ability to affect the outcome of the game. We show
that in our domain, such indices can be used to both determine the fair share of the
revenues an agent is entitled to, and identify significant possible points of failure affecting
the reliability of communication in the network. We show that in general graphs, calculating
the Shapley and Banzhaf power indices is #P-complete, but suggest a polynomial algorithm
for calculating them in trees.
We also investigate finding stable payoff divisions of the revenues in CGs, captured
by the game theoretic solution of the core, and its relaxations, the -core and least core.
We show a polynomial algorithm for computing the core of a CG, but show that testing
whether an imputation is in the -core is coNP-complete. Finally, we show that for trees,
it is possible to test for -core imputations in polynomial time.

1. Introduction
A key aspect of multi-agent systems that has been the focus of research in the field is agent
collaboration. Cooperative game theory considers cooperation among self interested agents,
and has been used to analyze many collaborative domains (Goldman & Zilberstein, 2004;
Kraus, Shehory, & Taase, 2004; Branzei, Dimitrov, & Tijs, 2008; Dunne, van der Hoek,
Kraus, & Wooldridge, 2008; Chalkiadakis, Elkind, & Wooldridge, 2012). One important
application area for multi-agent systems is network analysis, examining issues ranging from
communication network design (Babaoglu, Meling, & Montresor, 2002), through sensor
network technologies (Lesser, Ortiz, & Tambe, 2003) to social network analysis (Sabater &
Sierra, 2002). Game theory has already been used to analyze interaction between selfish
agents in various network settings, such as network security (Roy, Ellis, Shiva, Dasgupta,
Shandilya, & Wu, 2010; Jain, Korzhyk, Vanek, Conitzer, Pechoucek, & Tambe, 2011),
resource sharing (Suris, DaSilva, Han, & MacKenzie, 2007) and agent cooperation in communication networks (Saad, Han, Debbah, Hjorungnes, & Basar, 2009; Easley & Kleinberg,
2010).
c
2013
AI Access Foundation. All rights reserved.

fiBachrach, Porat, & Rosenschein

Consider a computer network in which the servers are controlled by selfish agents, and
a principal interested in allowing communication between a certain set of critical servers.
To allow this connectivity, the principal can incentivize the agents to allow communication
by offering them a certain reward. These agents must then cooperate to allow a reliable
communication between the critical servers, as every single agent only controls a small part
of the entire network.
1.1 Key Questions and Game Theoretic Solutions
Given the above setting, several key questions arise. First, the reward is promised to the
entire group of agents, who must decide how to allocate this reward amongst themselves.
Even if the agents form a successful team, this team may not be stable, as agents who are
only allocated a small part of the reward may try to form a different coalition, so as to
increase their own share of the reward. Could the agents reach an agreement on sharing
the rewards that would prevent such deviations? Second, what is the fair share that each
agent should get? How can we measure the importance of each individual agent in bringing
about the desired outcome of communication between all the critical servers?
Cooperative game theory provides answers to questions regarding reward sharing between selfish agents, in the form of solution concepts. Some solution concepts, such as the
core and its relaxations (Gillies, 1953; Shapley & Shubik, 1966) focus on stability, whereas
concepts such as the power index proposed by Banzhaf (1965) and the Shapley value (1953,
1954) focus on fairness.
Power indices originated in work on analyzing power in voting scenarios. Researchers
analyzing the distribution of power in decision making bodies have tried to find a precise
way of measuring the influence of a single agent in the context of a team of agents who
attempt to reach a joint decision through a voting procedure. They have formalized such
measures of influence as so-called power indices, which measure the control a voter has
over decisions of a larger group (Elkind, Goldberg, Goldberg, & Wooldridge, 2007b). The
two most prominent such indices are the Banzhaf power index (1965) and the ShapleyShubik power index (1954). Each of these indices can be characterized using a set of axioms
which describe desirable properties of a measure of power in voting contexts (Lehrer, 1988;
Shapley, 1953; Dubey & Shapley, 1979; Straffin, 1988). The Shapley-Shubik power index
is a manifestation of the Shapley value (1953) which was designed to find the fair share
of each agent when a team of agents must cooperate to achieve a joint reward. Although
these indices were mostly used for measuring power in voting systems, they can easily be
adapted for other domains as well.
In this paper, we consider the use of the power indices to find fair ways for agents
to share rewards in the network setting described above. Further, we show that power
indices can be used to find key points of failure in a communication network. We model
the above communication network setting, consisting of its servers and the network links
connecting them, as a vertex connectivity game. The network is modeled as a graph, where
the servers are the vertices, and the network links are the edges. A certain subset of the
servers (vertices) are primarya failure to send information between any two of them
would constitute a major system failure. Another subset of the servers are always available
(backbone servers).
282

fiSharing Rewards in Cooperative Connectivity Games

In the vertex Connectivity Game (CG) that we introduce, each agent controls a different
vertex in the graph. A coalition of agents can use any of the vertices controlled by the
coalition members or the backbone vertices, and may send information between them.
The coalition wins if it connects all the primary vertices, so that it can send information
between any two of them. The power index of an agent in this game reflects its criticality in
maintaining this connectivity. This index can be used to determine the fair share of the total
reward that this agent should get, and could enable an administrator to identify potential
critical points of failure in the network (perhaps, for example, focusing more maintenance
resources on preventing their failure).
We consider the computational complexity of calculating the Banzhaf or Shapley-Shubik
power indices in this domain. We show that in general graphs, computing either of these
indices is a #P-complete problem. Despite this negative result, we provide a polynomial
algorithm for the restricted case where the graph is a tree. Many networks, including parts of
the internets backbone, are constructed as trees when the construction of a communication
line is expensive, so this algorithm can analyze important real-world domains.
We then turn to finding stable payoff distributions for the collaborating agents, using the
game theoretic solution concept of the core (Gillies, 1953). We show that the core can be
computed in polynomial time in CGs. When a coalition in the CG manages to connect all
the primary vertices, it wins and gains a certain profit. This profit should then be divided
among the members of the coalition. Choosing a payoff vector in the core guarantees that
no subcoalition would choose to split from the main coalition, and attempt to establish its
own network. Thus, the core indicates which payoff vectors are stable and allows allocating
the gains of a coalition in a CG domain so as to prevent subcoalitions from defecting. We
also consider the more relaxed solution concepts of the -core and the least core (Shapley
& Shubik, 1966), and show that testing for -core imputations is coNP-complete in CGs.
Under -core imputations, although the coalition may not be completely stable, the incentive
of any subcoalition to deviate is low. Finally, we show that in tree CGs, the core and least
core coincide.
The paper proceeds as follows. In Section 2, we provide background information regarding coalitional games and power indices, and fully define a vertex connectivity game
(CG). In Section 3 we examine game theoretic solutions to CGs. Section 3.1 discusses
fair reward distributions in CGs using power indices, and presents a hardness result for
the general case and a polynomial algorithm for the restricted case of trees. Section 3.2
examines stable reward distributions. It shows that the core of CGs can be computed in
polynomial time, discusses the complexity of -core and least-core-related problems, and
examines core-related problems in tree CGs. Section 4 examines related work, discussing
both previous work on solutions to cooperative games (Section 4.1) and related models of
cooperative games over networks (Section 4.2). We conclude in Section 5.

2. Preliminaries
A coalitional game is composed of a set of n agents, I = (a1 , . . . , an ), and a function
mapping any subset (coalition) of the agents to a real value v : 2I  R. The function v
is called the coalitional function (or sometimes the characteristic function) of the game.
In a simple coalitional game, v only gets values of 0 or 1, so v : 2I  {0, 1}. A coalition
283

fiBachrach, Porat, & Rosenschein

C  I wins if v(C) = 1, and loses if v(C) = 0. The set of all winning coalitions is denoted
W (v) = {C  2I |v(C) = 1}. An agent ai is critical in a winning coalition C if the agents
removal from that coalition would make the coalition lose: v(C) = 1 but v(C \ {i}) = 0.
Thus, an agent can only be critical in a coalition that contains it.
We are interested in finding the fair share of the rewards an agent should get in a
cooperative game, or to measuring the influence a given agent has on the result of the game.
Game theoretic solutions for doing so include various values or power indices. The two most
prominent such values are the Banzhaf power index (1965) and the Shapley value (1953).
Both these indices can be characterized using a slightly different sets of fairness axioms,
which reflect desired properties of a power index. Both indices have the property that
dummy agents, who never affect the value of any coalition, obtain an index of zero (the null
player axiom). Similarly, both under the Shapley value and the Banzhaf index, equivalent
agents, who increase the value of any coalition that contains neither of them by the same
amount, have the same index (the symmetry axiom). However, the two indices behave differently regarding the composition of two games with the same set of agents.1 Alternatively,
these indices can be interpreted as the probability that an agent would significantly affect
the outcome of the game, under slightly different models for uncertainty regarding agents
participation in the game (Straffin, 1988). Although power indices were widely used for
measuring political power in weighted voting systems, their definition does not rely on the
specific features of a voting domain.
The Banzhaf index depends on the number of coalitions in which an agent is critical.
Agent ai s marginal contribution in a coalition C where ai  C is defined as v(C)  v(C \
{ai }). Thus if ai is critical in C he has a marginal contribution of 1 in it, and if ai is
not critical in C he has a marginal contribution of 0. The Banzhaf index of agent ai is
his average marginal contribution in all coalitions that contain him , or equivalently the
proportion of coalitions where i is critical in out of all the coalitions that contain i.
Definition 1. The Banzhaf index is the vector (v) = (1 (v), . . . , n (v)) where
i (v) =

1
2n1

X

[v(C)  v(C \ {ai })].

CN |ai C

The Shapley value (1953), which is sometimes referred to as the Shapley-Shubik power
index (1954) when applied to a simple cooperative game, relies on the notion of the marginal
contribution of an agent in a permutation. This is the amount of additional utility generated
when that agent joins the coalition of her predecessors in the permutation. We denote
1. Given two games u, v over the same agent set, we can define the game u + v where the value of a coalition
C in the game u + v is the sum of values in the composing games so u + v(C) = u(C) + v(C). We can
also define the max game u  v where the value of a coalition in the composed game u  v is the maximal
value in the composing games, so u  v(C) = max(u(C), v(C)). Similarly we can define the min game
u  v, where u  v(C) = min(u(C), v(C)). The Shapley value fulfills a linear decomposition axiom: the
Shapley value of any agent in the sum game u + v is the sum of its Shapley values in the composing
games u and v. In contrast, the Banzhaf index fulfills a property regarding max and min games, where
the sum of powers in two games is the sum of the powers in their max and min games. Some related
work examines the fairness axioms of the Shapley value and Banzhaf index (Dubey & Shapley, 1979;
Straffin, 1988; Lehrer, 1988; Holler & Packel, 1983; Laruelle & Valenciano, 2001).

284

fiSharing Rewards in Cooperative Connectivity Games

by  a permutation of the n agents, so  : {1, . . . , n}  {1, . . . , n} and  is onto. We
denote by Sn the set of all such agent permutations. Denote by i the predecessors of ai
in , so i = {aj |(j) < (i)}. Agent ai s marginal contribution in the permutation  is
mi = v(i {ai })v(i ). Note that in a simple game an agent has a marginal contribution
of 1 in the permutation  iff it is critical for the coalition i  {ai }. The Shapley value of
an agent is her marginal contribution averaged across all possible agent permutations.
Definition 2. The Shapley value is the vector (1 (v), . . . , n (v)) where
i (v) =

1 X 
1 X
mi =
(v (i  {i})  v (i ))
n!
n!
Sn

Sn

Both the Shapley value and the Banzhaf index can be thought of as the expected marginal
contribution of an agent under certain assumptions about the coalition formation process.
The Shapley value reflects the assumption that agents are randomly added to a coalition,
so every ordering of the agents is equally probable. In contrast, the Banzhaf index reflects
the assumption that all coalitions are equally probable. More generally, power indices can
be viewed as probabilities of events in weighted voting domains (Straffin, 1988).
Our hardness result for calculating power indices in CGs considers the class #P. #P is
the set of integer-valued functions that express the number of accepting computations of a
nondeterministic Turing machine of polynomial time complexity. Let  be the finite input
and output alphabet for Turing machines.
Definition 3. #P is the class consisting of the functions f :   N such that there exists
a non-deterministic polynomial time Turing machine M that for all inputs x   , f (x) is
the number of accepting paths of M.
The complexity classes #P and #P-complete were introduced by Valiant (1979a). These
classes express the hardness of problems that count the number of solutions.2
The coalitional function v describes the total utility a coalition can achieve, but does not
define how the agents distribute this utility among themselves. An imputation (p1 , . . . , pn ),
sometimes also called a payoff vector, P
is a division of the gains of the grand coalition I
among the agents, where pi  R, and ni=1 pi = v(I).
P We call pi the payoff of agent ai ,
and denote the payoff of a coalition C as p(C) = i{j|aj C} pi . By assumption, agents
are rational and attempt to maximize their own share of the utility. Game theory offers
several solution concepts, determining which imputations are likely to occur when agents
act rationally.
The Shapley value has been shown to be an imputation, as the values of the individual
to sum up to the value of the grand coalition of all the agents:
Pn agents were shown
3 Given the fairness axioms that the Shapley value fulfills, it can thus

(v)
=
v(I).
i=1 i
2. Informally, NP and NP-hardness deal with checking if at least one solution to a combinatorial problem
exists, while #P and #P-hardness deal with calculating the number of solutions to a combinatorial
problem. Counting the number of solutions to a problem is at least as hard as determining if there is at
least one solution, so #P-complete problems are at least as hard (but possibly harder) than NP-complete
problems; These complexity classes have been thoroughly investigated by computation complexity researchers (Papadimitriou, 2003; Valiant, 1979b; Papadimitriou & Zachos, 1982).
3. Shapley provided a proof of this fact in his seminal paper on the Shapley value (1953).

285

fiBachrach, Porat, & Rosenschein

be viewed as a fair imputation. However, in many domains the agents are selfish and care
little for fairness. Rather, selfish agents are likely to be more interested in their ability to
improve their own utility by forming alternative coalitions. Stability based solution concepts
such as the core focus on such deviations (Gillies, 1953).
A basic stability requirement for an imputation is individual rationality: for all agents
ai  C, we have pi  v({ai }), otherwise that agent is better off on its own. Similarly, we say
that a coalition B blocks the imputation (p1 , . . . , pn ) if p(B) < v(B), since the members of
B would split off from the coalition and gain more utility without the rest of the agents. If
a blocked imputation is chosen, the coalition is unstable. It is possible to define the degree
by which a subcoalition is incentivized to deviate from the grand coalition.
Definition 4. Given an imputation p = (p1 , . . . , pn ), the excess of a coalition is e(C) =
v(C)  p(C), which quantifies the amount the subcoalition C can gain by deviating and
working on its own.
Given an imputation, a coalition C is blocking iff its excess is strictly positive e(C) > 0.
If a blocked payoff vector is chosen, the coalition is unstable, and the higher the excess
is, the more incentivized the agents are to split apart from the current coalition and form
their own coalition. A known solution concept that emphasizes stability is the core (Gillies,
1953).
Definition 5. The Core of a coalitional game is the set of all payment vectors (p1 , . . . , pn )
that are not blocked by any coalition, so for any coalition C we have p(C)  v(C).
A value distribution in the core makes sure that no subset of the agents would split off,
so the coalition is stable. In general the core can be empty, so every possible value division
is blocked by some coalition. In this paper, we give results regarding computing the core in
vertex connectivity games. When the core can be empty (i.e., every possible value division
is blocked by some coalition), it sometimes make sense to relax the requirements of the
solution concept. In some domains, splitting apart from the current coalition structure to
form an alternative coalition might be a costly process. In such cases, coalitions that only
have a small incentive to split apart from the grand coalition would not do so. A relaxed
solution concept embodying this intuition is the -core (Shapley & Shubik, 1966). The
-core slightly relaxes the inequalities of Definition 5.
Definition 6. The -core is the set of all imputations (p1 , . . . , pn ) such that the following
holds: for any coalition C  I, p(C)  v(C)  .
Under an imputation in the -core, the excess e(C) = v(C)  p(C) of any coalition C
is at most . For large enough values of , the -core is guaranteed to be non-empty. An
obvious problem is to find the smallest value of  that makes the -core non-empty. This
solution concept is known as the least core. Formally, consider the game G and the set
{|the -core of G is not empty}. This set is compact,4 so it has a minimal element min .
Definition 7. The least core of the game G is the min -core of G.
4. A formal a formal definition of compactness and its implications can be found in many introductory
books on topology (Royden & Fitzpatrick, 1988).

286

fiSharing Rewards in Cooperative Connectivity Games

Imputations in the least core distribute payoffs while minimizing the worst deficit. In
other words, the least core minimizes the maximal incentive of a coalition to split apart
from the grand coalition. Under a least core imputation, no coalition can gain more than
min by deviating, and for any  < min it is impossible to distribute the payoffs in a way
that causes the deficit of any coalition to be at most  . Another solution concept, called
the nucleolus (Schmeidler, 1969) refines the least core, minimizing the number of coalitions
that have the maximal excess by examining the sorted vector of excesses and defining a
lexicographical order over them.
2.1 Connectivity Games
Consider a network connecting various servers, where a certain subset of the servers are
designated primary servers. Our goal is to make sure that we can send information
between any two primary servers. A server in the network may malfunction, and if it does,
we cannot send information through it. If all the paths between two primary servers go
through a failed server, we cannot send information between these two primary servers. In
our model, we also assume that there can be a certain subset of servers that are guaranteed
never to fail (guaranteed, say, by heavy maintenance and fail-safe backup); we will call these
backbone servers.
In Section 1.1 we discussed several questions regarding agreements selfish agents are
likely to reach when each of them controls a server in such a network. We model the
network domain as a cooperative game and use game theoretic solutions to answer these
questions. The coalitional game at the heart of our model is called the vertex Connectivity
Game.
Definition 8. A vertex Connectivity Game Domain (CGD) consists of a graph G = hV, Ei
where the vertices are partitioned into primary vertices Vp  V , backbone vertices Vb  V ,
and standard vertices Vs  V . We require that Vp  Vb = , Vb  Vs = , Vp  Vs = , and
that V = Vp  Vb  Vs , so this is indeed a partition.
Given a CGD, we can define the vertex Connectivity Game. In this game, each agent
controls one of the standard servers. A coalition wins if it connects all pairs of primary
vertices (so it can send information between any two such primary servers). Let |Vs | = n, and
consider a set of n agents I = (a1 , . . . , an ), so that agent ai controls vertex vi  Vs . Given
a coalition C  I we denote the set of vertices that C controls as V (C) = {vi  Vs |ai  C}.
Coalition C can use either the vertices in V (C) or the always-available backbone vertices
Vb . In our model, we assume that the coalition can also use any of the primary vertices Vp
as well.5
We say a set of vertices V   V fully connects Vp if for any two vertices u, v  Vp there
is a path (u, p1 , p2 , . . . , pk , v) from u to v going only through vertices in V  , so for all i we
have pi  V  .
5. Another possibility would be to allow some of the primary vertices we want to connect to fail (this may
occur, for example, in network security domains, where external attacks may target key servers in the
network). In this case a coalition would win if it manages to connect all the non-failed primary vertices.
We could also disallow sending information through the primary vertices (so they can only be the final
destination). Most of the results in this paper hold for these different settings as well.

287

fiBachrach, Porat, & Rosenschein

Definition 9. A vertex Connectivity Game (CG) is a simple coalitional game, where the
value of a coalition C  I is defined as follows:
(
1 if V (C)  Vb  Vp fully connects Vp
v(C) =
0 otherwise

3. Solutions To The Connectivity Game
In Section 1.1 we raised several questions regarding agreements selfish agents are likely to
reach when each of them controls a server in such a network. We now answer these questions
by applying game theoretic solutions on our Connectivity Game.
We begin by characterizing fair payoff distribution and measuring an agents importance
in allowing reliable network communication. Specifically, given our desire to ensure communication paths between primary vertices, which servers on the network are most critical?
Which agents deserve a higher share of the reward? Given limited resources to make sure
that some servers do not fail (i.e., making them backbone servers), on which vertices should
we concentrate to ensure communication between primary servers? Section 3.1 answers
these questions using power indices.
We continue by examining stable allocations of the rewards. How can we determine
whether an agreement on sharing the rewards would incentivize a sub-coalition of agents to
defect? If no agreement is fully-resistant to such deviations, how can we find the most stable
agreement? Section 3.2.1 answers these questions using core and core-related solutions.
3.1 Network Reliability And Fair Reward Distribution
The CG of Definition 9 has a characteristic function that maps every coalition that fully
connects the primary vertices to a single unit of reward (and indicates that all other coalitions have a reward of zero). In Section 2 we have discussed the fairness axioms that the
Shapley value and the Banzhaf index fulfill. Due to these properties, we can apply these
concepts to CGs, and obtain a fair distribution of the reward in these games. For example,
dummy servers, which do not affect the ability of any coalition to allow full connectivity,
would have a power index of zero, and obtain no reward. Similarly, equivalent agents, that
have the same impact on achieving connectivity when added to any coalition, would obtain
the same reward. If the agents forming the coalition wish to share the rewards in a fair
manner, power indices thus make an excellent basis for forming the agreement,6 highlighting
the need to examine the computational aspects of calculating such indices, as we do in this
section.
We emphasize that beyond fulfilling these fairness properties, power indices can also
be viewed as network reliability constructs. Our model is based on a simple network goal
of allowing communication between any two primary servers. Under a network reliability
view, we want to identify the servers which, when failing, will cause us to lose connectivity
6. The specific index to be used depends on the agents notion of fairness. Different sets of axioms result in
different indices (Dubey & Shapley, 1979; Straffin, 1988; Holler & Packel, 1983; Laruelle & Valenciano,
2001).

288

fiSharing Rewards in Cooperative Connectivity Games

between primary servers. Suppose all the servers have an equal probability of working or
failing the next day (i.e. each has a probability of 50% to fail, and a probability of 50% to
work). When these failures are independent, any subset of the servers has an equal chance
of surviving (regardless of its size). Thus, we have a certain probability of having the
surviving set of servers fully connect the primary servers. Suppose we can make sure that
exactly one server, owned by agent ai , always survives. The Banzhaf power index measures
the increase in the probability of having the surviving subset of vertices fully connect the
primary servers by guaranteeing that vi survives.
When attempting to maximize the probability of achieving our goal, the higher the
Banzhaf index of a server is, the more we should try to make sure that server does not
fail. Thus, in order to find significant points of failure, we can calculate the Banzhaf power
index, and focus on the servers with the highest indices.7
We now consider the computational complexity of calculating power indices in general
vertex connectivity games. We first formally define the problems.
Definition 10. CG-BANZHAF / CG-SHAPLEY: We are given a CG over the graph G =
hV, Ei, with primary vertices Vp  V , backbone vertices Vb  V , and standard vertices
Vs  V . There are n = |Vs | agents, I = (a1 , . . . , an ), so agent ai controls vertex vi  Vs .
The games coalitional function v : 2I  {0, 1} is defined as in Definition 9. We are also
given a specific target agent ai . In the CG-BANZHAF we are asked to calculate its Banzhaf
power index in this game, i (v). Similarly, in the CG-SHAPLEY problem we are asked to
calculate its Shapley value, i (v).
We now show that in general CGs, both CG-BANZHAF and CG-SHAPLEY are #Pcomplete. We first prove the problems are in #P. We then reduce a #SET-COVER problem
to CG-BANZHAF. Then we obtain the #P-hardness of CG-SHAPLEY as a corollary. We
begin with a few definitions.
Definition 11. #SET-COVER (#SC): We are given a collection C = {S1 , . . . , Sn } of
subsets. We denote Si C Si = S. A set cover is a subset C   C such that Si C  = S.
We are asked to compute the number of covers of S.
A slightly different version requires finding the number of set covers of size at most k:
Definition 12. #SET-COVER-K (#SC-K): A set-cover with size k is a set cover C  such
|C  | = k. As in Definition 11, we are given S and C and a target size k, and are asked to
compute the number of covers of S of size at most k.
Both #SC and #SC-K are #P-hard. Garey and Johnson (1979) showed that #SC-K
is #P-hard: they considered several basic NP-complete problems, and showed that their
counting versions are #P-complete. The counting version of SET-COVER discussed there
is #SC-K. #VERTEX-COVER is a restricted form of #SC. Vadhan (2002) showed that
#VERTEX-COVER is #P-hard,8 so #SC is also #P-hard. We use #SC to prove that
CG-BANZHAF is #P-hard. It is easy to show that #SC-K is #P-complete, but the fact
7. Similarly, when there is uncertainty about the order of agent failures, the Shapley value reflects the
importance of vertices with regard to network reliability.
8. He also showed that the problem remains #P-hard even in very restricted classes of graphs.

289

fiBachrach, Porat, & Rosenschein

that #SC is #P-complete is more difficult to prove (and is thus not very well known). We
give the definitions of both #SC and #SC-K to avoid confusion between them, and use
Vadhans result (2002) which indicates that #SC is #P-complete. Using this, by reducing
#SC to CG-BANZHAF we show that CG-BANZHAF is #P-complete.
In order to show that CG-BANZHAF is #P-complete we need to show two things:
first, that CG-BANZHAF is in #P, and second, a reduction of a #P-hard problem to
CG-BANZHAF.
Lemma 1. CG-BANZHAF and CG-SHAPLEY are in #P.
Proof. The Banzhaf index of ai in a CG v is i (v), the proportion of coalitions where ai
is critical, out of all the coalitions that contain ai . Given a certain coalition C  I, it
is polynomial to check whether it winswe only need to check whether V (C)  Vb fully
connects Vp . We can do this by creating a new graph G , dropping all edges that miss
V (C)  Vb from G (i.e., we drop any edge (x, y)  E such that either x 
/ V (C)  Vb or

y
/ V (C)  Vb ). We then check if any two primary vertices in G are connected (there are
several polynomial algorithms to do this; a simple one is to run a depth-first search (DFS)
between all pairs of primary vertices). We can thus easily test if a certain agent ai is critical
for a coalition: we perform the above test when he is in the coalition, remove him, and
repeat the test. If the first test succeeds and the second fails, that agent is critical for that
coalition. Similarly, we can test whether an agent ai is critical in an agent permutation 
in polynomial time: we simply test whether ai s predecessors i form a losing coalition and
whether i {ai } form a winning coalition (we do both using the above test).
Since we can construct a deterministic polynomial Turing machine M that tests if an
agent is critical in a coalition, we can construct a non-deterministic Turing machine M  ,
that first non-deterministically chooses a coalition that ai is a member of, and then tests
if ai is critical in that coalition. The number of accepting paths of M  is the number of
coalitions that contain ai where ai is critical. Denote by k the number of such accepting
paths of M  , and denote |I| = |Vs | = n. Then the Banzhaf power index of agent ai is
k
.
i (v) = 2n1
Calculating the numerator of i (v) is thus, according to Definition 3, a problem in #P.
Since the denominator is constant (given a domain with n agents), CG-BANZHAF is in
#P. A similar argument using a non-deterministic generation of permutations instead of
coalitions holds for the Shapley value, so CG-SHAPLEY is in #P.
We now show that CG-BANZHAF is #P-hard. We do this by a reduction from #SC.
Figure 1 shows an example of such a reduction for a specific #SC instance.
Theorem 1. CG-BANZHAF is #P-hard, even if there are no backbone vertices, i.e., Vb =
.
Proof. We reduce a #SC instance to a CG-BANZHAF instance. Consider the #SC instance
with the collection C = {S1 , . . . , Sn }, so that Si C Si = S. Denote the items in S as S =
{t1 , t2 , . . . , tk }. Denote the items in Si as Si = {t(Si ,1) , t(Si ,2) , . . . , t(Si ,ki ) }. The reductiongenerated CGD is constructed with a graph G = hV, Ei as follows. For each subset Si  C,
the generated CG instance has a vertex vSi . We denote the set of all such vertices as
Vsets = {i|Si C} vSi . For each item ti  S the generated CG instance also has a vertex vti .
290

fiSharing Rewards in Cooperative Connectivity Games

We denote the set of vti vertices Vitems = i|ti S vti . The generated CG instance also has
two special vertices va and vb . These are all the vertices of the generated instance.
The vertices in the generated CG are connected in the following way. The vertices Vsets
are a clique: for every vi , vj  Vsets , (vi , vj )  E. The vertex va is also a part of that clique,
so for all vi  Vsets we have (vi , va )  E. The vertex va is connected to vb , and is the only
vertex connected to vb , so (va , vb )  E. Each set vertex vSi is connected to all the vertices
of the items in that set, vt(Si ,1) , vt(Si ,2) , . . . , vt(Si ,ki ) , so for any vSi  Vsets and any vt(Si ,j) (so
that t(Si ,j)  Si ) we have (vSi , vt(Si ,j) )  E.
We define the CG so that Vp = Vitems  {vb }, Vb = , Vs = Vsets  {va }, and the CG
game is defined as in Definition 9. The game has m = |Vs | = |Vsets | + 1 = |C| + 1 = n + 1
agents (where n is the number of subsets in C, the input to the #SC problem). The CGBANZHAF query is regarding va . Let i (v) be the answer to the CG-BANZHAF query,
and k be the number of set covers in the #SC instance. We show that k = va (v)  2m1 , by
providing a one-to-one mapping between a set-cover of the original problem and a winning
coalition where va is critical in the generated CG.
Consider a set-cover C   C for S. C  must cover all the items ti in S. We denote the
set of vertices corresponding to the sets in this vertex cover VC  = {vSi  Vsets |Si  C  }.
Since C  is a set cover for the original problem, each vertex vtj  Vitems in the generated
graph must be connected to at least one vertex vi  Vsets . Since the vertices Vsets are a
clique, in the generated CG all the vti s and vSj s are in the same connected component.
However, without va we cannot reach vb from any vertex. Thus, VC   {va }  VS is a
winning coalition in the generated CG, but VC  is not, so va is critical for that coalition.
We now show the mapping in the reverse direction. Consider a coalition V   Vs where
va is critical, and denote C  = {Si  C|vSi  V  }. By definition, V  must be winning and
contain va . Consider any vertex vti  Vitems . Since V  wins, it must allow any vertex in
Vsets to reach vti , which can only happen if V  contains some vSj so that ti  Sj . Thus, C 
is a set cover for the original problem.
Let x be the number of set covers in the #SC instance, and ca be the number of winning
coalitions where va is critical in the generated CG. Due to the one-to-one mapping we have
shown, x = ca . But by the definition of the Banzhaf index, in the generated CG we have
ca
, so ca = a (v)  2m1 , and then x = a (v)  2m1 .
a (v) = 2m1
We have shown that given a polynomial algorithm for CG-BANZHAF, we can solve
#SC in polynomial time, so CG-BANZHAF is #P-hard.
A recent result shows that for any reasonable representation language of a cooperative
game, if computing the Banzhaf index is #P-hard, then computing the Shapley value is
also #P-hard (Aziz, Lachish, Paterson, & Savani, 2009). A representation language is said
to be reasonable if it is possible to represent the game with an additional dummy agent
using the same representation language.9 Our CG representation is reasonableto add an
additional dummy agent we simply add a dummy vertex x which is not connected to any
9. More formally, a representation language is reasonable if for any game v that it can represent, it can
also represent the game v  defined as follows. The game v  has an additional agent x that is not present
in the original game v. For any agent set C such that x 
/ C we have v  (C) = v(C). For any agent set
C such that x  C we have v  (C) = v(C \ {x}). Note that v and v  are games with a slightly different
agent sets: v is defined over an agent set I whereas v  is defined over the agent set I  {x}.

291

fiBachrach, Porat, & Rosenschein

other vertex. Adding this isolated vertex to any coalition does not change its value. Thus
we obtain the following corollary.

Figure 1: Example of reducing #SC to CG-BANZHAF. The items are {t1 , t2 , t3 , t4 , t5 } and
the sets are S1 = {t1 , t3 }, S2 = {t1 , t2 , t3 }, S3 = {t3 , t5 }, S4 = {t3 , t4 , t5 }.

Corollary 1. CG-SHAPLEY is #P-hard, even if there are no backbone vertices, i.e., Vb =
.
Since CG-BANZHAF and CG-SHAPLEY are both in #P and #P-hard these are #Pcomplete problems, so it is unlikely that a polynomial algorithm for calculating these indices
in CGs would be found. We can circumvent this computational problem in several ways.
One is to try to find an approximation algorithm, and the other is to solve the problem for
restricted instances. In the next section, we adopt the second approach.
3.1.1 Computing Power Indices In Tree CGs
Although computing the Shapley and Banzhaf power indices in general CGs is #P-complete,
restricting the graphs structure may allow us to polynomially compute such indices. We
examine the restricted case where the graph is a tree. Consider a CG with graph G = hV, Ei
that is a tree, with primary vertices Vp  V , backbone vertices Vb  V , and standard vertices
Vs  V . We call the problem of calculating the Shapley and Banzhaf power index of an
agent in this domain TREE-CG-SHAPLEY / TREE-CG-BANZHAF. We assume that there
are at least two primary vertices va , vb  Vp (otherwise, any subset of the vertices trivially
fully connects the primary vertices). We first note that since the graph is a tree, some of
the vertices are veto agents present in any winning coalition.
292

fiSharing Rewards in Cooperative Connectivity Games

Lemma 2. Consider a CG where the graph G is a tree. Let va , vb  Vp be two primary
vertices, and a standard vertex vr  Vs is on a simple path from va to vb . Then vr is present
in all winning coalitions in the CG game.
Proof. Since G is a tree, there is only one simple path between va and vb . The removal of
any vertex along that simple path makes vb unreachable from va . Since vr is such a vertex,
any coalition C  Vs such that vr 
/ C loses, and any winning coalition must contain vr .
Due to Lemma 2 we call a standard vertex in a tree-CG essential if it lies on the simple
path between some two primary vertices.
Lemma 3. Consider a CG where the graph G is a tree. Let va , vb  Vp be two primary
vertices. Consider a vertex coalition C  Vs that contains all standard vertices vr  Vs
on the single simple path from va to vb . Then C allows the connecting of va and vb in the
CG game. If the coalition C  Vs contains all essential vertices (i.e., all standard vertices
vr  Vs on any simple path between any two primary vertices va , vb  Vp ), then C is a
winning coalition so v(C) = 1.
Proof. In the CG game we can use any primary vertex vp  Vp , and any backbone vertex
vb  Vb . Consider a coalition C that contains all vertices vr  V on the single simple
path from va to vb . Any vertex vx on the single simple path between va and vb is either
a backbone vertex (so vx  Vb ) or a primary vertex (so vx  Vp ) or a standard vertex (so
vx  Vs ). If it is a standard vertex, it is in the coalition, so vx  C. In any of these
cases we can use the vertex, so va and vb are in the same connected component for the
coalition C. If C contains all essential vertices, then any two primary vertices va and vb
are connected through a path composed of either backbone, primary, or coalition vertices,
so C fully connects all the primary vertices Vp , and is a winning coalition.
Lemma 2 states that having all essential vertices is a necessary condition for a coalition
to win, and Lemma 3 states that this is also a sufficient condition. We summarize this in
the following corollary.
Corollary 2. In Tree CGs, the winning coalitions are exactly those coalitions that contain
all essential vertices.
We denote the set of all essential vertices in a tree CG as Ves . We show that in a tree CG
the Shapley value distributes all the reward to the essential vertices equally, and provide a
similar result for the Banzhaf index.
Theorem 2. Consider a tree CG v with a set Ves of essential vertices. If vi  Ves then
i (v) = |V1es | and otherwise i (v) = 0.
Proof. Three of the fairness axioms the Shapley value fulfills are the null player axiom,
the symmetry axiom and the efficiency axiom (Shapley, 1953; Dubey & Shapley, 1979;
Straffin, 1988). The null player axiom states that if i is a null player, i.e., for any coalition
C we have v(C) = v(C  {i}) then i (v) = 0. Due to corollary 2 if a vertex i is not
essential then it is a null player, so i (v) = 0. The symmetry axiom deals with equivalent
agents. Two agents are i, j are called equivalent if for any coalition C that contains neither
293

fiBachrach, Porat, & Rosenschein

of them (so i 
/ C and j 
/ C) adding either agent results in the same change in value,
so v(C  {i}) = v(C  {j}). We note that all essential vertices are equivalentif there
exist two essential vertices, i, j  Ves , then any coalition C that does not contain either
of them is missing two essential vertices to be winning, so v(C  {i}) = v(C  {j}) = 0.
Due to the symmetry axiom, all essential vertices have the same Shapley value, which
we denote es . The efficiency axiom states that
Pthe Shapley values of all the agents sum
up to the value of the grand coalition I, so
iI i (v) = v(I). Due to the efficiency
and null player
P value of any vertex iP Ves is es we have
Paxioms and since
P the Shapley

+
1 = v(I) =

(v)
=
i
i
iV
/ es 0, so we have:
iV
/ es i = |Ves |  es +
iI
iVes
1
es = |Ves | .
Theorem 3. Consider a tree CG v with a set Ves of essential vertices. If i  Ves then
i (v) = 21|Ves | and otherwise i (v) = 0.
Proof. Due to corollary 2 if a vertex i is not essential then it is a null player, so for any
coalition C we have v(C)  v(C \ {i}) = 0. In this case, directly from Definition 1 we have
i (v) = 0. Now suppose i is an essential vertex. The entire agent set has n vertices, where
|Ves | are essential and n  |Ves | are non-essential. Due to Corollary 2 the winning coalitions
are exactly those containing all the essential vertices, including i. Thus i is critical in all
the winning coalitions. There are therefore 2n|Ves | different winning coalitions that i is a
critical in (any of the n  |Ves | non-essential vertices can can either be present or not), so
n|Ves |
we have i (v) = 2 2n1 = 2n|Ves |n+1 = 21|Ves | .
Corollary 3. TREE-CG-SHAPLEY and TREE-CG-BANZHAF are in P.
Proof. Due to Theorems 2 and 3 to compute the power index of a vertex we need only
know whether it is essential and the total number |Ves | of essential vertices. It is easy to
test if a vertex is essential in polynomial time, by checking whether it lies between two
primary vertices (for example using a DFS). We can apply this test to each of the vertices
and determine whether i is essential and obtain |Ves |, then apply the formulas of Theorem 2
or Theorem 3 (depending on the power index we are interested in).
Thus, despite the high complexity result for the general case given in Section 3.1, in tree
CGs we can polynomially calculate power indices. This result is important for analyzing
reliability in real-world networks. As an example, consider the situation where Internet
connectivity is established between companies, where one company is the supplier and
another company is the client. An example of a cycle in this relationship would be if
company A buys an Internet connection from company B, which in turn buys an Internet
connection from company C, which eventually buys an Internet connection from company
A. This would mean that, in a sense, company A would have become a client of itself, and
would be paying money for its own connection. This scenario which has a cycle is very
unlikely, so such a domain is likely to be a tree domain.
Yet another example is agent based smart-grid technology, where agents can negotiate
power supply (Massoud Amin & Wollenberg, 2005; Vytelingum, Voice, Ramchurn, Rogers,
& Jennings, 2010; Pipattanasomporn, Feroze, & Rahman, 2009). In this scenario various
agents are both suppliers and consumers of electricity. For example, each of several firms can
have its own solar panels for producing electricity, but could buy additional electricity when
294

fiSharing Rewards in Cooperative Connectivity Games

its demand is higher than its own production capability. In such a case, firms benefit from
being able to send and receive power from all other firms (perhaps through an intermediary).
Having multiple paths to transmit power between two firms does not offer any advantage,
so cycles are not likely to exist, making the network a tree.
3.2 Stable Reward Distributions
The key question regarding CGs raised in Section 1.1 was how agents are likely to share the
reward in a CG. Section 3.1 focused on fair allocations, according to an agents impact on
the entire coalition achieving its goal. In this section we focus on stable reward allocations.
Once a winning coalition is formed, a reward distribution may collapse if a subset of agents
who are only allocated a small share of the reward defect and form an alternative coalition.
This subset of agents would only defect if by doing so it can secure its agents a larger
share of the rewards. This reasoning is captured by the core (see Definition 5 in Section 2).
Thus, computing the core allows us to find or test for stable agreementswhen the core is
non-empty, it contains imputations that are stable; when it is empty, the coalition would
be unstable no matter how we divide the utility among the agents. However, how can we
compute the core of CG?
We first note that it is not always possible to concisely represent the core, since it may
contain an infinite number of imputations. However, in the case of CGs, there does exist a
concise representation for the core.
Definition 9 of CGs clearly indicate that CGs are simple cooperative games, as the
value of a coalition is either 1 or 0. The core is a very demanding concept in simple games.
An agent ai is a veto player if it is present in all winning coalitions, so if ai 
/ C we
have v(C) = 0. In Section 3.1.1 we noted that essential vertices, which lie on the only
simple path between two primary vertices, are veto players. It is a well-known fact that
in simple coalitional games, the core is non-empty iff there is at least one veto player in
the game (Chalkiadakis et al., 2012). Consider a simple coalitional game that has no veto
players, so for every agent ai we have a winning coalition
Pn C that does not contain ai . Take
a payoff vector
p
=
(p
,
.
.
.
,
p
)
where
p
>
0.
Since
1
n
i
i=0 pi = 1 and since pi > 0 we know
P
that p(C)  pj Ia pj < 1, so p(C) < v(C) = 1, which makes C a blocking coalition. On
i
the other hand, we can see that any payoff vector p where non-veto players get nothing is
in the core: any coalition C that can potentially block p must have
P v(C) = 1 (if v(C) = 0
then it cannot block), and must contain all the veto players, so pj C pj = 1, and thus
cannot block p.
Due to the above characterization of the core in simple cooperative games, in such games
the core can be represented as a set Iveto , consisting of all the veto players in that game.
This set represents all core imputations: an imputation
p = (p1 , . . . , pn ) is in the core if
P
P
p
=
1
(note
that
it
must
be
the
case
that
p
iI i = 1 for p to be an imputation).
iIveto i
We now consider computing the core in CGs, in the above representation as the set of
veto agents. We note that CGs are monotone games. Let W  I be a winning coalition in
a CG (so v(W ) = 1), and let C  I be any coalition in that game. Then W  C is also a
winning coalition, so v(W  C) = 1 (this can be restated as: for all coalitions A, B  I in
a CG we have v(A  B)  v(A)). The reason for this is that if C fully connects Vp then
W  C also fully connects Vp , as more vertices are available for us to use.
295

fiBachrach, Porat, & Rosenschein

We now denote the set of all the agents except ai as Ii = I \ {ai }. Let G be the CG
graph. We denote by Gi the same graph when we drop the vertex vi owned by ai , so
Gi = hVi , Ei i where Vi = V \ {vi } and Ei = {(u, v)  E|u 6= vi  v 6= vi }. We now
show a polynomial algorithm for testing if a player is a veto agent in CGs.
Lemma 4. Testing if agent ai is a veto agent in a CG is in P.
Proof. We first show that Ii is a losing coalition iff ai is a veto agent. If Ii is a losing
coalition then due to the monotonicity of CGs any sub-coalition of it, C  Ii , is also losing.
Thus, any coalition without ai is losing, so ai is a veto player. On the other hand, if Ii
is a winning coalition, it is a winning coalition where ai is not present, so by definition ai
is not a veto player. Thus, to test if ai is a veto agent we only need to test if Ii is losing
or winning. According to Definition 9 of the CG, to check if Ii wins we need to check if
Ii fully connects the primary vertices. This test can be performed in polynomial time by
trying all pairs va , vb  Vp , and performing a DFS between va and vb in the graph Gi .
Since computing the core in simple coalitional games just requires returning a list of all
the veto agents, we get the following corollary.
Corollary 4. It is possible to compute and return a concise representation of the core of a
CG in polynomial time. Under this representation, it is possible to test whether the core is
empty or test if an imputation is in the core in polynomial time.10
Proof. Computing the core of a CG requires returning Iveto , the set consisting of veto players
in the game. Using Lemma 4, we can check all the agents to determine which of them are
veto players. If there are no veto players, the core is empty. Otherwise, any payoff vector
that distributes 1 (the total utility v(I) = 1) among the veto players and gives none to the
non-veto players is in the core.
3.2.1 The -Core And Least Core
The core of CGs may be non-empty, in which case we can easily compute core imputations.
However, many real world networks have redundancy in terms of connectivity, and it might
be possible to connect the primary vertices even after eliminating an arbitrary single vertex.
In those networks, the CG has no veto agent and the core is empty. In such cases, any
imputation would be unstable, as some vertex subset would be incentivized to deviate and
form its own coalition. Thus, we may simply wish to minimize the incentive of any agent
subset to deviate, and examine problems related to the least core. Although core-related
problems in CGs can be solved in polynomial time (as we have shown above), we now show
that problems related to the -core may be hard.
Given a certain proposed imputation p = (p1 , . . . , pn ), we may wish to test whether this
imputation is in the -core, for a given .
10. In fact, the same can be done for any simple monotone coalitional game where the value of a coalition
can be computed in polynomial time: due to the same proof of Lemma 4, in such games we can test
whether an agent is a veto player, and in simple games computing the core simply requires finding out
who the veto players are.

296

fiSharing Rewards in Cooperative Connectivity Games

Definition 13. -CORE-MEMBERSHIP(ECM): Given an imputation p = (p1 , . . . , pn ),
decide whether it is in the -core of the game, or in other words, test whether any coalition
C has an excess e(C) of at most .
ECM tests whether an imputation (payoff division) is sufficiently stable, or -stable.
By definition of the -core, such imputations have the property that any coalition C has
an excess e(C) < . ECM is a more basic question than computing an imputation in the
-core or finding the the least core valuethe minimal  that admits a non-empty -core.
We show that ECM is coNP-complete, but that in can be solved in polynomial time in tree
CGs. For tree CGs, we show that the core is non-empty (so the least core coincides with
the core) and that it is possible to find -core imputations in polynomial time.
We show that ECM is coNP-complete in CGs using a reduction from VERTEX-COVER,
known to be NP-complete (Garey & Johnson, 1979).
Theorem 4. ECM is coNP-complete in general CGs, even if the imputation to be tested is
the equal imputation p = ( n1 , . . . , n1 ) and if there is a single backbone vertex.
Proof. ECM requires testing whether for a given imputation p = (p1 , . . . , pn ) there does
not exist a coalition C with excess e(C) of at least d (for a given d). Given a vertex subset
C  V , it is easy to test if C connects all the primary vertices in polynomial time, and
thus test whether C is winning or not. We can also easily compute the payoff p(C) of the
coalition under the imputation, and thus can also compute its excess e(C) = v(C)  p(C).
Thus, ECM is in coNP.
We now show that computing the maximal excess emax = max{e(C)|C  I} under the
imputation p is coNP-hard. Note that an imputation p is in the -core iff the maximal
deficit under this imputation is at most . We show that testing whether the maximal
excess is at most d (for a given d) is NP-hard by reducing a VERTEX-COVER instance to
this problem.
Let the graph G = hV, Ei and threshold t be the input to the VERTEX-COVER instance
(i.e., we are asked wether Gs edges could be covered by at most t vertices). We will assume
the VERTEX-COVER instance has at least two edges (otherwise, the problem is easy to
solve). Denote |V | = n. We construct a graph G = hV  , E  i as follows. For each vertex
v  V , we create a standard vertex v  V  (i.e., V  V  ). For each edge e  E, we create a
primary vertex ve  V  . These primary vertices are called the edge vertices. We also create
a single backbone vertex vb  V  . Thus we have Vs = V , Vp = {ve |e  E} and Vb = {vb }.
The agents are the standard vertices Vs = V , so there are n agents.
(u,v)
(u,v)
For each edge e = (u, v)  V we create two edges in G : e1
= (u, ve ) and e2
=
(v, ve ). In other words, we break each edge of the original graph into two parts, putting
a vertex in between. The original edges of the graph G are eliminated from G . Finally, we
connect any standard vertex v  Vs = V to vb . Figure 2 shows an example of the reduction
construction used in the proof of Theorem 4.
The imputation to be tested is the equal imputation p = ( n1 , . . . , n1 ) (i.e., the payoff of
all the agents is the same, n1 ). The threshold value  for the generated ECM instance is
 = 1  nt where t is the threshold in the VERTEX-COVER instance.
We first note that any coalition C that loses in the generated CG (i.e., fails to connect the
primary vertices Vp ) has a negative excess, as v(C) = 0 and p(C) = |C|
n , so e(C) = v(C) 
297

fiBachrach, Porat, & Rosenschein

p(C) =  |C|
n . Thus, the maximal excess coalition is the minimally paid winning coalition,
i.e., a coalition C that minimizes p(C) of all winning coalitions C: arg minC{C|v(C)=1} p(C).
Since p(C) = |C|
n , a maximal excess coalition is a winning coalition of a minimal size:
arg minC{C|v(C)=1} |C|. In other words, a maximal excess coalition is a minimally sized
coalition that connects all the primary vertices.11 A winning coalition C of size |C| = s
thus has an excess of v(C)  p(C) = 1  |C|  n1 = 1  ns .
Since the agents in the game are the standard vertices, and since Vs = V , we identify
the agents with the vertices of the original graph. We show that a coalition C  Vs is a
winning coalition iff it is a vertex cover in G.
If C  Vs wins, it must connect any two primary vertices vx , vy  Vp . We note that due
to our construction no two primary vertices are connected directly, as each primary vertex
ve was created for an edge e  E of the original graph. We have assumed the VERTEXCOVER instance has at least two edges, so there are at least two primary vertices in the
generated graph. Let ve be some primary vertex. Due to our construction ve is connected
to exactly two standard vertices u, w (the vertices that were connected through the edge e
in the original graph). If neither u nor w are part of C (i.e., both u 
/ C and w 
/ C), there
is no path from ve to any other vertex in the graph induced by C, so ve is not connected to
any other primary vertex and C loses so v(C) = 0. Thus, if C is a winning coalition then
for every primary vertex ve where e = (u, w)  E for vertices u, w  V in the original graph
G, the coalition C must contain either u or w. However, if u  C then C covers the edge e,
as u is a vertex on one side of the edge, and if w  C then C also covers edge e, as w is the
vertex on the other side of the edge. Thus, C covers any edge e  E, so it is a vertex cover.
On the other hand, suppose the coalition C  Vs = V is a vertex cover of G. As a
vertex cover, C must cover every edge e  E, so given an edge e = (u, w), C must contain
either u or w or both. If u  C we have a path from ve to vb : (ve , vu , vb ) (ve is the source
primary vertex, vu is in the coalition, and vb is a backbone vertex). Similarly, if w  C we
have a path from ve to vb : (ve , vw , vb ) (ve is the source primary vertex, vw is in the coalition,
and vb is a backbone vertex). Therefore, there is a path from any primary vertex vp to vb .
Thus, all of the primary vertices are connected: given vx , vy  Vp we have a path from vx
to vb and from vb to vy , so if C is a vertex cover, it is a winning coalition.
We have shown that C is a winning coalition and has an excess of 1 |C|
n in the generated
instance iff it is a vertex cover of size |C|. The maximal excess problem in the generated
instance requires finding a minimal size winning coalition, or in other words finding a vertex
cover of minimal size. We can restate this by saying that the ECM instance (with  = 1 nt )
is a yes instance iff the original graph has a vertex cover of size at most t.

3.2.2 The Core, -Core And Least Core In Tree CGs
We consider core related problems in tree CGs. A CG domain with less than two primary
vertices is a degenerate domain (where all coalitions win), and a CG where even the grand
coalition of all the standard vertices fails to connect all the primary vertices is also a degen11. Finding the minimally paid winning coalition under a general imputation is very similar to the famous
Steiner tree problem, which is known to be NP-hard. However, in our domain the weights are on the
vertices rather than the edges.

298

fiSharing Rewards in Cooperative Connectivity Games

Figure 2: Example of reducing VERTEX-COVER to ECM in a CG domain. The left side is
the original VERTEX-COVER instance, and the right side is the generated ECM
domain, with the primary vertices in orange (denoting the edges in the original VERTEX-COVER instance), standard vertices in blue (same as the original
vertices in the VERTEX-COVER instance) and the backbone vertex in green.

erate domain (where all coalitions lose). We assume that the CG domains in this section
are not degenerate. We also assume that no two primary vertices are directly connected to
each other or through a path containing only backbone vertices. If this is the case, we can
merge these primary vertices to a single vertex, as any coalition that is connected to one
of them is also connected to the other one without using any standard vertex.
We first prove that the core in (non-degenerate) tree CGs is non-empty.
Theorem 5. Tree CGs have non-empty cores (assuming the domain of the game is a nondegenerate CG domain).
Proof. Lemma 2 Corollary 2 shows that in tree CGs, the veto vertices are exactly those
essential vertices, that lie on a simple path between two primary vertices. We note that if
there is no standard vertex that is on some path between two primary vertices, the domain
is degenerate (either all primary vertices are connected even for the empty coalition of
standard vertices, or they are still disconnected even for the grand coalition of all standard
vertices). Thus, there must exist at least one veto agent. Since in simple games the core is
non-empty iff there are veto agents, the core of tree CGs is non-empty.
Due to Theorem 5, in tree CGs the least core value, the minimal  such that the -core
is non-empty, is 0 (the core is the 0-core, and it is always non-empty).
299

fiBachrach, Porat, & Rosenschein

In simple games, when the core is non-empty, any core imputation distributes the reward
solely to the veto agents. Since in tree CGs the veto agents are exactly the essential vertices
(Corollary 2) we obtain the following:
Corollary 5. In a tree CG,Plet Ves  Vs be the set of essential vertices, and denote |Ves | =
m. Any imputation where iVes pi = 1 is a core imputation, and these are the only core
1
imputations. Specifically, the imputation where pi = m
if vi  Ves and pi = 0 otherwise is
a core imputation.
Although the core is non-empty in tree CGs, given a potential agreement in the form of
a specific imputation, we may still wish to find the maximal excess under that imputation,
or in other words test whether the imputation is in the -core (for a given ). We now
examine the complexity of the ECM problem in tree CGs. Theorem 4 has shown that this
problem is coNP-complete for general graphs, but we show the problem can be solved in
polynomial time for tree CGs.
Theorem 6. In tree CGs, the ECM problem can be solved in polynomial time. An imputation is in the -core iff p(Ves ) > 1  .
Proof. Consider a tree CG and an imputation p = (p1 , . . . , pn ). Let the veto vertices
be Ves  V , and denote m = |Ves |. Denote the indices of the veto vertices as M , so
{vi |i  M } = Ves . As seen in Lemma 2, in tree CGs, if a vertex w is not a veto vertex
(essential), it is a dummy vertex, so for any coalition C we have v(C  {w}) = v(C). We
denote the non-veto vertices as Vd = V \ Ves , and these are all dummy vertices. We denote
the indices of the dummy vertices as D so {vi |i  D} = Vd .
P
We denote the total payoff of the vetoPvertices as p(Ves ) =
iM pi , and the total
payoff of the dummy vertices as p(Vd ) =
p
.
We
assumed
that
the domain is not
i
iD
degenerate, so the grand coalition wins, and we have v(I) = 1. Since p is an imputation,
p(I) = 1. Since in a tree CG all vertices are either veto vertices or dummy vertices, we
have 1 = p(I) = p(Ves ) + p(Vd ). The -core constraints require that p(C) > v(C)  . So
since all pi s are positive, the -core constraint holds for all losing coalitions. Any winning
coalition C must contain Ves so Ves  C, so if p(Ves ) > v(Ves )   = 1  , all the -core
constrains hold: when p(Ves ) > v(Ves )   = 1  , for any winning coalition C we have
p(C)  p(Ves ) so p(C)  p(Ves ) > v(Ves )   = 1   = v(C)  . On the other hand, if
we have p(Ves ) < 1  , then the -core constraint does not hold for the coalition Ves so the
imputation is not in the -core.
Thus, to test if an imputation is in the -core we only need to test if p(Ves ) > 1  .
Since we can compute Ves in polynomial time, this test can also be done in polynomial time,
so ECM is in P for tree CGs.

4. Related Work
In this paper we introduced a cooperative game called the Connectivity Game, and examined
computational aspects of calculating power indices or finding core solutions in this game.
We discuss related work regarding the solution concepts in Section 4.1, and examine similar
models of cooperative games over networks in Section 4.2.
300

fiSharing Rewards in Cooperative Connectivity Games

4.1 Solutions To Cooperative Games
The stability based solution concept of the core originated in the paper (Gillies, 1953). The
least core was introduced as a solution concept for games with empty cores (Shapley & Shubik, 1966). The further refinement of the nucleolus was by Schmeidler (1969). The -core
and nucleolus were studied in minimum cost spanning tree games (Granot & Huberman,
1984), which are somewhat reminiscent of our model (see below), in assignment games (Solymosi & Raghavan, 1994), and in weighted voting games (Elkind, Goldberg, Goldberg, &
Wooldridge, 2007a). Another related problem is the Cost of Stability (Bachrach, Elkind,
Meir, Pasechnik, Zuckerman, Rothe, & Rosenschein, 2009), measuring the required external
subsidy to stabilize a game, studied in weighted voting games (Bachrach, Meir, Zuckerman,
Rothe, & Rosenschein, 2009), network flow games (Resnick, Bachrach, Meir, & Rosenschein, 2009) and other game forms (Meir, Zick, & Rosenschein, 2012; Meir, Bachrach, &
Rosenschein, 2010).
Power indices originated in work on game theory and political science, attempting to
measure the power that players have in weighted voting games. In these games, each player
has a certain weight, and a coalitions weight is the sum of the weights of its participants;
a coalition wins if its weight passes a certain threshold. This is a common situation in
legislative bodies. Power indices have been suggested as a way of measuring the influence
that players in such games have on choosing outcomes. The most popular indices suggested
for such measurement are the Banzhaf index (1965) and the Shapley-Shubik index (1954).
The Shapley-Shubik index (1954) is a direct application of the Shapley value (1953) to
simple coalitional games. The Banzhaf index emerged from the study of voting in decisionmaking bodies, where a certain normalized form of the index was introduced (Banzhaf,
1965). The Banzhaf index was later mathematically analyzed (Dubey & Shapley, 1979),
and this normalization was shown to have certain undesirable features, focusing attention
on the non-normalized version of the Banzhaf index. These indices were applied to an
analysis of the voting structures of the IMF and the European Union Council of Ministers,
as well as many other bodies (Leech, 2002; Machover & Felsenthal, 2001).
The Shapley value is the only payoff division rule that exhibits natural fairness axioms (Shapley, 1953; Dubey & Shapley, 1979), so it has been used not only to measure
power but also to fairly allocate costs, revenues or credit in various domains (Shubik,
1962; Dubey, 1982; Young, 1985; Bachrach, 2010; Bachrach, Graepel, Kasneci, Kosinski, &
Van Gael, 2012a; Staum, 2012). Specific such examples include dividing the costs of multicast transmissions (Feigenbaum, Papadimitriou, & Shenker, 2001), dividing airport landing
fees (Littlechild & Owen, 1973), pollution reduction costs (Petrosjan & Zaccour, 2003),
sharing supply chain profits (Shi-hua & Peng, 2006; Bachrach, Zuckerman, Wooldridge, &
Rosenschein, 2010) and sharing the gains from regional cooperation in the electricity market (Gately, 1974). Although power indices allow finding fair allocations of rewards and
costs, they are susceptible to some forms of strategic behavior (Yokoo, Conitzer, Sandholm,
Ohta, & Iwasaki, 2005; Aziz, Bachrach, Elkind, & Paterson, 2011; Zuckerman, Faliszewski,
Bachrach, & Elkind, 2012).
The differences between the Banzhaf and Shapley-Shubik indices were analyzed (Straffin,
1977), and each index was shown to reflects specific conditions in a voting body. Different
301

fiBachrach, Porat, & Rosenschein

axiomatizations of these two indices have been proposed (Shapley, 1953; Dubey & Shapley,
1979; Lehrer, 1988; Straffin, 1988; Laruelle, 1999; Laruelle & Valenciano, 2001).
4.2 Cooperative Games Over Networks
Our model was based on a network goal of connecting a set of primary servers. Other
cooperative games over networks have been proposed, modeling other goals in networks.
Some related work studies one such model, which deals with a cost sharing mechanism for
multicast transmissions (Feigenbaum et al., 2001; Moulin & Shenker, 2001). This body
of related work focuses on a weakly budget-balanced implementation (where, as opposed
to our model the total payments by the agents may exceed the total cost incurred), or
on mechanisms that are only resistant to deviations by single agents. Yet another model
examines buying a path between a source and a target in a network (Archer & Tardos,
2002). This is a mechanism design model which examines the problem of eliciting truthful
reports from the edges regarding their true costs, in contrast to our work which focuses on
a cooperative game with full information, and in which the agents incur no cost for allowing
the use of their resources.
Another similar model considers a scenario where agents control edges in a network
flow graph, and a coalition wins if it can maintain a certain required flow between a source
and a target (Bachrach & Rosenschein, 2009). In that specific model finding the Banzhaf
index of an edge in that domain is #P-complete, though there is a polynomial algorithm
for some restricted cases. We handle a very different scenario where agents are required to
maintain connectivity, rather than a certain flow. Also, we are interested in maintaining
this connectivity between every two primary vertices, rather than two specific vertices (we
can simulate the case of two specific servers by having only two primary servers). Also, in
our work the agents are the servers in the communication network, rather than the links.
The model of Minimum Cost Spanning Tree Game (Bird, 1976; Granot & Huberman,
1981, 1984) (MCSTG) is also quite similar to our model. This is a cost sharing game, where
the agents are the vertices in a complete edge-weighted graph, and the cost of a coalition is
the minimal weight of a tree that connects all the coalitions vertices to a designated root
r. More formally, the vertices of the graph include the agents I = {1, 2, . . . , n} and the
designated root r 
/ I, so V = I  {r}, and the graph is a complete edge weighted graph
(with n(n + 1)/2 edges); the cost of a coalition S is the weight of the minimal spanning
tree on the subgraph induced by S  {r}. Granot et. al show that MCSTGs always have
non empty cores (1981) and provide algorithms for finding core imputations or computing
the nucleolus (1984). Our CG model is quite differentCGs are a revenue sharing game,
where no costs are associated with edges, and where backbone vertices are allowed. Most
notably, our CGs are simple games (where a coalition either wins or loses), and the core of
our CGs can sometimes be empty.
A generalization of MCSTGs called Steiner Tree Games (Skorin-Kapov, 1995), STGs,
is also somewhat similar to our model. STGs do allow nodes that are not players, similarly
to our backbone vertices, and may sometimes have an empty core. Related work on this
model discusses a certain sufficient, but not necessary, condition for the core on an STG to
be non-empty, and a polynomial algorithm for testing that condition (Skorin-Kapov, 1995).
Since the core may be non-empty even if that condition does not hold, this does not allow
302

fiSharing Rewards in Cooperative Connectivity Games

polynomially determining if the core of an STG is nonempty. Again, as opposed to our
CGs, STGs are non-simple cost sharing games. Further, we also examine computational
aspect of power indices and core-relaxations, rather than non-emptiness of the core that is
the focus of that related work. Our results do show that in CGs we can determine whether
the core is empty in polynomial time and even return a representation of the core.
Another twist on MCSTGs results in a model where the cost for a coalition S is the
weight of the minimal tree that spans the vertices in S  {r} in the entire graph, rather
than the graph induced by S  {r} (Faigle, Kern, Fekete, & Hochstattler, 1997). In other
words, it is allowed to use nonmembers of the coalition to connect members of the coalition
to the root. In this model the core may be empty, and testing for core-emptiness is an
NP-complete problem (Faigle et al., 1997). Further results regarding this model show that
computing the nucleolus for this game is NP-hard (Faigle, Kern, & Kuipers, 1998).
The variations of the MCSTG discussed above (Granot & Huberman, 1981; SkorinKapov, 1995; Faigle et al., 1997) are cost sharing games. In such games, the coalition
must achieve a certain goal and each agent is endowed with some resources (for example,
the ability to use the edges adjacent to the agent). However, using each such resource
is associated with a cost, and a coalition attempts to minimize the total cost it incurs.12
It is possible to convert a cost-sharing game to a reward sharing game (Moulin, 2002;
Chalkiadakis et al., 2012),13 for example by defining the utility of a coalition to be some
very high constant reward minus the cost the coalition incurs to achieve its goal in the
original cost sharing game. However, our model crucially depends on the assumption that a
node incurs no cost for allowing the use of its links, so all coalitions that achieve the network
goal have the same utility. We believe this better characterizes domains such as a computer
network that has already been constructed, where the links of a node are simply available
for it to use. The MCSTG better models domains where agents must make decisions about
which links to build in the future and where constructing a link requires an investment on
behalf of the agents.
Yet another related network based cooperative games is the Spanning Connectivity
Games (Aziz et al., 2009) (SCG for short). SCGs are similar to our CGs in that they are
cooperative network reward sharing game. However, as opposed to our model, in SCGs
the players are the edges of a multigraph, and a coalition wins if it manages to span all
the nodes of the network. Yet another similar reward sharing game is the Path Disruption
Games (Bachrach & Porat, 2010) where the coalition attempts to disrupt connectivity
between two specific vertices. Although those domains are combinatorially different from
CGs, this previous work examines similar solution concepts: the core and the Shapley value.
For example, this related work shows that computing power indices in these domains is hard
and that there are computationally tractable algorithms for solving core-related problems
(at least in somewhat restricted domains).

12. In some such games a coalition may not even be able to achieve its goal at all, in which case we can
define its cost to be infinite.
13. Sometimes reward sharing games are also called surplus sharing games.

303

fiBachrach, Porat, & Rosenschein

4.2.1 Computing Power Indices And The Core
As their name suggests, power indices can also be though of as a measure of the significance
of agents in a game. However, although both the Shapley and Banzhaf power indices are
defined not only for voting games but for any simple cooperative game, relatively little
work has examined the use of power indices for measuring the importance of players in
non-voting scenarios. The complexity of computing power indices depends on the concrete
representation of the game. When the game is defined only by the value of each coalition,
in the form of an oracle that tests a certain coalition and answers whether it wins or loses,
calculating power indices is difficult. A naive algorithm for calculating the power index
of an agent ai enumerates all coalitions or permutations containing ai . Since there are
exponentially many such coalitions or permutations, the naive algorithm is exponential in
the number of agents.
Related work discusses algorithms for calculating power indices in weighted majority
games (Matsui & Matsui, 2000), and shows that calculating the Banzhaf and Shapley-Shubik
indices in weighted voting games are both NP-complete problems (Matsui & Matsui, 2001).
Since weighted voting games are a restricted case of simple coalitional games, the problem
of calculating either index in a general coalitional game is of course NP-hard. In fact, in
certain cases, calculating power indices is not just NP-hard but also #P-hard. Deng and
Papadimitriou (1994) show that computing the Shapley-Shubik index in weighted voting
games is #P-complete. Other research has derived hardness results for power indices in
other game classes, such as Coalitional Skill Games (Bachrach & Rosenschein, 2008) which
are based on a set-covering problem, or in a rule based cooperative game representation
called the Multi-Attribute Coalitional Game language (Ieong & Shoham, 2006).
Our hardness results regarding computing the power indices might make using this concept seem less attractive. However, There are many results on comparing and approximating
power indices, in general and in restricted domains (Owen, 1975; Deng & Papadimitriou,
1994; Conitzer & Sandholm, 2004; Bachrach, Markakis, Resnick, Procaccia, Rosenschein, &
Saberi, 2010; Faliszewski & Hemaspaandra, 2009). This line of work shows that although
computing power indices exactly is generally hard, estimating them with a high degree of accuracy is computationally tractable. For example, problematic vertices in a network can
be tractably found by employing an approximation method (Bachrach et al., 2010) which
can handle arbitrary cooperative games, so long as it is possible to compute the value of
a coalition in polynomial time (which is easy to do in CGs). The algorithm provided by
Bachrach et al. (2010) estimates the power indices and returns a result that is probably
approximately correct: given a game in which a players true power index is , and given
a target accuracy level  and confidence level , the algorithm returns an approximation 
such that with probability at least 1   we have |  |   (i.e., the result is approximately
correct, and is within a distance  of the correct value). Its running time is logarithmic in
the confidence and quadratic in the accuracy, so the approach is tractable even for high
accuracy and confidence. Methods for computing power indices were also examined in the
context of games with uncertain agent failures (Bachrach, Meir, Feldman, & Tennenholtz,
2011; Bachrach, Kash, & Shah, 2012b).
While our treatment of the model is game theoretic, in network domains problems akin
to calculating power indices can also be formulated as network reliability problems. The
304

fiSharing Rewards in Cooperative Connectivity Games

computational complexity of such problems has been studied in several papers. Classical
network reliability problems consider an undirected graph G = hV, Ei, when each edge
e  E has a probability assigned to it, pe . This is the probability that edge e remains in
the surviving graph.
One prominent problem is that of s-t connectivity probability (STC-P): given the above
domain, compute the probability of having a path between s, t  V in the surviving graph.
Another prominent problem is that of full connectivity probability (FC-P): given the above
domain, compute the probability that the surviving graph is connected (so that there is a
path between any two vertices). One seminal paper by Valiant (1979a) proved that STC-P
is #P-hard. Provan and Ball (1983) showed that FC-P is also #P-hard.
Some of the problems we study are similar to FC-P. For example computing the Banzhaf
power index in CGs is a very specific case of FC-P, where the probability of every vertex
subset is equal (or equivalently, where each vertex has a 50% probability of failures, and
failures are independent). Since this is a restricted case, we cannot use the hardness result
of Provan and Ball (1983), and have to prove that even the restricted case is #P-complete
(which we did, in Section 3.1).

5. Conclusions
We have considered the computational aspects of reward sharing in a network connectivity
scenario, and its applications to network reliability. We modeled a communication network
as a simple coalitional game, and showed how various game-theoretic solution concepts can
be used to characterize reasonable reward sharing agreements agents might make and to to
find significant possible points of failure in the network. We have shown that in this domain,
for general graphs, computing the Shapley and Banzhaf power indices is #P-complete.
Despite this high complexity result for the general domain, we also gave a polynomial result
for the restricted domain where the graph is a tree.
We have also shown that computing the core can be done in polynomial time in any
CG, and gave a simple characterization of the instances when the core is non-empty in CGs.
On the other hand, we have shown that in general CGs, testing if an imputation is in the
-core (or equivalently computing the maximal excess of a coalition under the imputation)
is coNP-complete. We have also given a characterization of the core in tree CGs, and shown
how testing for -core imputations can be done in polynomial time for tree CGs.
It remains a topic of future research to tractably compute power indices in CGs over
restricted domains. We also note that the Shapley and Banzhaf indices are not the only
power indices studied in the literature, so studying computational aspects of other indices is
also of interest. We have also examined the core, -core and least core. Our hardness results
show that computing the maximal excess of a coalition is computationally hard in general
CGs. It would be interesting to see if it could be approximated, or exactly computed in
restricted domains other than trees. Yet another open question is that of computing other
game theoretic solution concepts in CGs or restricted CGs. One interesting problem is
computing the nucleolus (Schmeidler, 1969) in CGs.14 Another interesting direction is ex14. For example, in tree CGs, the imputation which equally allocates all the rewards to the essential vertices
Ves is the nucleolus. However, we believe that there may even exist restricted CG domains where
computing the -core or the least core is tractable, but computing the nucleolus is hard.

305

fiBachrach, Porat, & Rosenschein

amining coalition formation models (Dang, Dash, Rogers, & Jennings, 2006; Greco, Malizia,
Palopoli, & Scarcello, 2011) and analyzing the coalition structure generation problem (Rahwan, Ramchurn, Dang, & Jennings, 2007; Ohta, Conitzer, Ichimura, Sakurai, Iwasaki, &
Yokoo, 2009; Bachrach, Meir, Jung, & Kohli, 2010) in our domain.

Acknowledgments
This work was partially supported by Israel Science Foundation grants #898/05 and #1227/12,
and Israel Ministry of Science and Technology grant #3-6797.
A preliminary version of this paper was presented at the Seventh International Joint
Conference on Autonomous Agents and Multiagent Systems (AAMAS 2008). This extended
version includes a more complete analysis of power indices in CGs, complementing the
previous results regarding the Banzhaf index with new results regarding the Shapley value.
We have also expanded the work regarding stability based solutions, so this version contains
an analysis of problems related to the -core and least core, showing that such problems are
coNP-hard in general CGs, but can be solved in polynomial time for the restricted case of
CGs over trees. It also includes a more complete analysis of the core in CGs played over
trees.

References
Archer, A., & Tardos, E. (2002). Frugal path mechanisms. In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 991999. Society
for Industrial and Applied Mathematics.
Aziz, H., Bachrach, Y., Elkind, E., & Paterson, M. (2011). False-name manipulations in
weighted voting games. Journal of Artificial Intelligence Research, 40 (1), 5793.
Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009). Power indices in spanning connectivity games. Algorithmic Aspects in Information and Management, 5567.
Babaoglu, O., Meling, H., & Montresor, A. (2002). Anthill: A framework for the development of agent-based peer-to-peer systems. In Distributed Computing Systems, 2002.
Proceedings. 22nd International Conference on, pp. 1522. IEEE.
Bachrach, Y., Elkind, E., Meir, R., Pasechnik, D., Zuckerman, M., Rothe, J., & Rosenschein,
J. S. (2009). The cost of stability in coalitional games. In Proceedings of the 2nd
International Symposium on Algorithmic Game Theory (SAGT 09), pp. 122134.
Springer.
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A., Rosenschein, J. S., & Saberi, A.
(2010). Approximating power indices: theoretical and empirical analysis. Autonomous
Agents and Multi-Agent Systems, 20 (2), 105122.
Bachrach, Y., Meir, R., Zuckerman, M., Rothe, J., & Rosenschein, J. S. (2009). The cost of
stability in weighted voting games. In Proceedings of the 8th International Conference
on Autonomous Agents and Multiagent Systems, pp. 12891290.
306

fiSharing Rewards in Cooperative Connectivity Games

Bachrach, Y., & Porat, E. (2010). Path disruption games. In Proceedings of the 9th International Joint Conference on Autonomous Agents and Multiagent Systems, pp.
11231130.
Bachrach, Y., & Rosenschein, J. S. (2008). Coalitional skill games. In Proceedings of the
7th International Joint Conference on Autonomous Agents and Multiagent Systems,
pp. 10231030.
Bachrach, Y., & Rosenschein, J. S. (2009). Power in threshold network flow games. Autonomous Agents and Multi-Agent Systems, 18 (1), 106132.
Bachrach, Y., Zuckerman, M., Wooldridge, M., & Rosenschein, J. S. (2010). Proof systems
and transformation games. Mathematical Foundations of Computer Science 2010,
7889.
Bachrach, Y. (2010). Honor among thieves: collusion in multi-unit auctions. In Proceedings
of the 9th International Conference on Autonomous Agents and Multiagent Systems:
volume 1-Volume 1, pp. 617624. International Foundation for Autonomous Agents
and Multiagent Systems.
Bachrach, Y., Graepel, T., Kasneci, G., Kosinski, M., & Van Gael, J. (2012a). Crowd IQ:
aggregating opinions to boost performance. In Proceedings of the 11th International
Conference on Autonomous Agents and Multiagent Systems-Volume 1, pp. 535542.
International Foundation for Autonomous Agents and Multiagent Systems.
Bachrach, Y., Kash, I., & Shah, N. (2012b). Agent failures in totally balanced games and
convex games. In Internet and Network Economics, pp. 1529. Springer.
Bachrach, Y., Meir, R., Feldman, M., & Tennenholtz, M. (2011). Solving cooperative reliability games. In Proceedings of the Twenty Seventh Conference on Uncertainty in
Artificial Intelligence (UAI 2011).
Bachrach, Y., Meir, R., Jung, K., & Kohli, P. (2010). Coalitional structure generation in
skill games. In Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI).
Banzhaf, J. F. (1965). Weighted voting doesnt work: a mathematical analysis. Rutgers
Law Review, 19, 317343.
Bird, C. (1976). On cost allocation for a spanning tree: a game theoretic approach. Networks,
6 (4), 335350.
Branzei, R., Dimitrov, D., & Tijs, S. (2008). Models in cooperative game theory, Vol. 556.
Springer Verlag.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2012). Cooperative game theory: Basic
concepts and computational challenges. IEEE Intelligent Systems, 8690.
Conitzer, V., & Sandholm, T. (2004). Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains. In Proceedings
of the Nineteenth National Conference on Artificial Intelligence (AAAI 2004), pp.
219225.
Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalition
formation for efficient data fusion in multi-sensor networks. In Proceedings of the
307

fiBachrach, Porat, & Rosenschein

National Conference on Artificial Intelligence, Vol. 21, p. 635. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
Deng, X., & Papadimitriou, C. H. (1994). On the complexity of cooperative solution concepts. Math. Oper. Res., 19 (2), 257266.
Dubey, P., & Shapley, L. (1979). Mathematical properties of the Banzhaf power index.
Mathematics of Operations Research, 4(2), 99131.
Dubey, P. (1982). The shapley value as aircraft landing feesrevisited. Management Science,
28 (8), 869874.
Dunne, P., van der Hoek, W., Kraus, S., & Wooldridge, M. (2008). Cooperative boolean
games. In Proceedings of the 7th International Joint Conference on Autonomous
Agents and Multiagent Systems, pp. 10151022.
Easley, D., & Kleinberg, J. (2010). Networks, crowds, and markets. Cambridge University
Press.
Elkind, E., Goldberg, L. A., Goldberg, P., & Wooldridge, M. (2007a). Computational
complexity of weighted threshold games. In AAAI-2007, p. 718.
Elkind, E., Goldberg, L. A., Goldberg, P. W., & Wooldridge, M. (2007b). Computational
complexity of weighted threshold games. In The National Conference on Artificial
Intelligence, pp. 718723.
Faigle, U., Kern, W., Fekete, S., & Hochstattler, W. (1997). On the complexity of testing
membership in the core of min-cost spanning tree games. International Journal of
Game Theory, 26 (3), 361366.
Faigle, U., Kern, W., & Kuipers, J. (1998). Note computing the nucleolus of min-cost
spanning tree games is NP-hard. International Journal of Game Theory, 27 (3), 443
450.
Faliszewski, P., & Hemaspaandra, L. (2009). The complexity of power-index comparison.
Theoretical Computer Science, 410 (1), 101107.
Feigenbaum, J., Papadimitriou, C. H., & Shenker, S. (2001). Sharing the cost of multicast
transmissions. Journal of Computer and System Sciences, 63 (1), 2141.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman and Co.
Gately, D. (1974). Sharing the gains from regional cooperation: A game theoretic application
to planning investment in electric power. International Economic Review, 15 (1), 195
208.
Gillies, D. B. (1953). Some theorems on n-person games. Ph.D. thesis, Princeton University.
Goldman, C., & Zilberstein, S. (2004). Decentralized control of cooperative systems: Categorization and complexity analysis. J. Artif. Intell. Res. (JAIR), 22, 143174.
Granot, D., & Huberman, G. (1981). Minimum cost spanning tree games. Mathematical
Programming, 21 (1), 118.
Granot, D., & Huberman, G. (1984). On the core and nucleolus of minimum cost spanning
tree games. Mathematical Programming, 29 (3), 323347.
308

fiSharing Rewards in Cooperative Connectivity Games

Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2011). On the complexity of the core
over coalition structures. In Proceedings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume Volume One, pp. 216221. AAAI Press.
Holler, M., & Packel, E. (1983). Power, luck and the right index. Journal of Economics,
43 (1), 2129.
Ieong, S., & Shoham, Y. (2006). Multi-attribute coalitional games. In Proceedings of the
7th ACM Conference on Electronic Commerce, pp. 170179. ACM.
Jain, M., Korzhyk, D., Vanek, O., Conitzer, V., Pechoucek, M., & Tambe, M. (2011). A
double oracle algorithm for zero-sum security games on graphs. In Proceedings of
the Tenth International Joint Conference on Autonomous Agents and Multi-Agent
Systems (AAMAS), Taipei, Taiwan, pp. 327334.
Kraus, S., Shehory, O., & Taase, G. (2004). The advantages of compromising in coalition
formation with incomplete information. In Proceedings of the Third International
Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2004),
pp. 588595.
Laruelle, A. (1999). On the choice of a power index. Papers 99-10, Valencia  Instituto de
Investigaciones Economicas.
Laruelle, A., & Valenciano, F. (2001). Shapley-Shubik and Banzhaf indices revisited. Mathematics of Operations Research, 89104.
Leech, D. (2002). Voting power in the governance of the International Monetary Fund.
Annals of Operations Research, 109 (1-4), 375397.
Lehrer, E. (1988). An axiomatization of the Banzhaf value. International Journal of Game
Theory, 17 (2), 8999.
Lesser, V., Ortiz, C. L., & Tambe, M. (2003). Distributed sensor networks: A multiagent
perspective. Springer.
Littlechild, S., & Owen, G. (1973). A simple expression for the Shapley value in a special
case. Management Science, 370372.
Machover, M., & Felsenthal, D. S. (2001). The treaty of Nice and qualified majority voting.
Social Choice and Welfare, 18 (3), 431464.
Massoud Amin, S., & Wollenberg, B. (2005). Toward a smart grid: power delivery for the
21st century. Power and Energy Magazine, IEEE, 3 (5), 3441.
Matsui, Y., & Matsui, T. (2000). A survey of algorithms for calculating power indices of
weighted majority games. Journal of the Operations Research Society of Japan, 43.
Matsui, Y., & Matsui, T. (2001). NP-completeness for calculating power indices of weighted
majority games. Theoretical Computer Science, 263 (12), 305310.
Meir, R., Bachrach, Y., & Rosenschein, J. (2010). Minimal subsidies in expense sharing
games. In Algorithmic Game Theory: Proceedings of the Third International Symposium (SAGT 2010), Vol. 6386, p. 347. Springer.
Meir, R., Zick, Y., & Rosenschein, J. S. (2012). Optimization and stability in games with
restricted interactions..
309

fiBachrach, Porat, & Rosenschein

Moulin, H. (2002). Axiomatic cost and surplus sharing. Handbook of social choice and
welfare, 1, 289357.
Moulin, H., & Shenker, S. (2001). Strategyproof sharing of submodular costs: budget balance
versus efficiency. Economic Theory, 18 (3), 511533.
Ohta, N., Conitzer, V., Ichimura, R., Sakurai, Y., Iwasaki, A., & Yokoo, M. (2009). Coalition structure generation utilizing compact characteristic function representations.
Principles and Practice of Constraint Programming, 623638.
Owen, G. (1975). Multilinear extensions and the Banzhaf Value. Naval Research Logistics
Quarterly, 22 (4), 741750.
Papadimitriou, C., & Zachos, S. (1982). Two remarks on the power of counting. Theoretical
Computer Science, 269275.
Papadimitriou, C. H. (2003). Computational complexity. John Wiley and Sons Ltd.
Petrosjan, L., & Zaccour, G. (2003). Time-consistent Shapley value allocation of pollution
cost reduction. Journal of economic dynamics and control, 27 (3), 381398.
Pipattanasomporn, M., Feroze, H., & Rahman, S. (2009). Multi-agent systems in a distributed smart grid: Design and implementation. In Power Systems Conference and
Exposition, 2009. PSCE09. IEEE/PES, pp. 18. IEEE.
Provan, J. S., & Ball, M. O. (1983). The complexity of counting cuts and of computing the
probability that a graph is connected. SIAM Journal on Computing, 12 (4), 777788.
Rahwan, T., Ramchurn, S. D., Dang, V. D., & Jennings, N. R. (2007). Near-optimal
anytime coalition structure generation. In Proceedings of the 20th international joint
conference on Artifical intelligence, pp. 23652371. Morgan Kaufmann Publishers Inc.
Resnick, E., Bachrach, Y., Meir, R., & Rosenschein, J. S. (2009). The cost of stability in
network flow games. In Mathematical Foundations of Computer Science 2009: The
Thirty-Fourth International Symposium on Mathematical Foundations of Computer
Science, No. 5734 in Lecture Notes in Computer Science, pp. 636650. Springer, Novy
Smokovec, High Tatras, Slovakia.
Roy, S., Ellis, C., Shiva, S., Dasgupta, D., Shandilya, V., & Wu, Q. (2010). A survey of
game theory as applied to network security. In System Sciences (HICSS), 2010 43rd
Hawaii International Conference on, pp. 110. IEEE.
Royden, H. L., & Fitzpatrick, P. (1988). Real analysis, Vol. 3. Prentice Hall Englewood
Cliffs, NJ:.
Saad, W., Han, Z., Debbah, M., Hjorungnes, A., & Basar, T. (2009). Coalitional game theory
for communication networks. Signal Processing Magazine, IEEE, 26 (5), 7797.
Sabater, J., & Sierra, C. (2002). Reputation and social network analysis in multi-agent
systems. In Proceedings of the First International Joint Conference on Autonomous
Agents and Multiagent Systems, pp. 475482.
Schmeidler, D. (1969). The nucleolus of a characteristic function game. SIAM Journal on
Applied Mathematics, 17 (6), 11631170.
310

fiSharing Rewards in Cooperative Connectivity Games

Shapley, L. S. (1953). A value for n-person games. Contributions to the Theory of Games,
3140.
Shapley, L. S., & Shubik, M. (1954). A method for evaluating the distribution of power in
a committee system. American Political Science Review, 48, 787792.
Shapley, L. S., & Shubik, M. (1966). Quasi-cores in a monetary economy with nonconvex
preferences. Econometrica: Journal of the Econometric Society, 34 (4), 805827.
Shi-hua, M., & Peng, W. (2006). The study of profit allocation among partners in supply
chain based on the Shapley value [j]. Industrial Engineering and Management, 4,
4345.
Shubik, M. (1962). Incentives, decentralized control, the assignment of joint costs and
internal pricing. Management Science, 8 (3), 325343.
Skorin-Kapov, D. (1995). On the core of the minimum cost Steiner tree game in networks.
Annals of Operations Research, 57 (1), 233249.
Solymosi, T., & Raghavan, T. (1994). An algorithm for finding the nucleolus of assignment
games. International Journal of Game Theory, 23 (2), 119143.
Staum, J. (2012). Systemic risk components and deposit insurance premia. Quantitative
Finance, 12 (4), 651662.
Straffin, P. (1977). Homogeneity, independence and power indices. Public Choice, 30, 107
118.
Straffin, P. (1988). The Shapley-Shubik and Banzhaf power indices as probabilities. The
Shapley Value, 7181.
Suris, J. E., DaSilva, L. A., Han, Z., & MacKenzie, A. B. (2007). Cooperative game theory
for distributed spectrum sharing. In IEEE International Conference on Communications, 2007 (ICC07), pp. 52825287. IEEE.
Vadhan, S. P. (2002). The complexity of counting in sparse, regular, and planar graphs.
SIAM Journal on Computing, 31 (2), 398427.
Valiant, L. G. (1979a). The complexity of enumeration and reliability problems. SIAM
Journal on Computing, 8, 410421.
Valiant, L. (1979b). The complexity of enumeration and reliability problems. SIAM J.
Comput., 8 (3), 410421.
Vytelingum, P., Voice, T., Ramchurn, S., Rogers, A., & Jennings, N. (2010). Agent-based
micro-storage management for the smart grid. In Proceedings of the 9th International
Conference on Autonomous Agents and Multiagent Systems, pp. 3946.
Yokoo, M., Conitzer, V., Sandholm, T., Ohta, N., & Iwasaki, A. (2005). Coalitional games
in open anonymous environments. In The 20th National Conference on Artificial
Intelligence, pp. 509514.
Young, H. (1985). Cost allocation. Fair Allocation, 33, 6994.
Zuckerman, M., Faliszewski, P., Bachrach, Y., & Elkind, E. (2012). Manipulating the quota
in weighted voting games. Artificial Intelligence, 180, 119.

311

fiJournal of Artificial Intelligence Research 47 (2013) 95-156

Submitted 10/12; published 5/13

Distributed Reasoning for
Multiagent Simple Temporal Problems
James C. Boerkoel Jr.

boerkoel@csail.mit.edu

Computer Science and Artificial Intelligence Laboratory,
Massachusetts Institute of Technology, Cambridge, MA 02139, USA

Edmund H. Durfee

durfee@umich.edu

Computer Science and Engineering,
University of Michigan, Ann Arbor, MI 48109, USA

Abstract
This research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity
interdependencies outstrip peoples cognitive capacity. We address the critical challenge of
reasoning over individuals interacting schedules to efficiently answer queries about how to
meet scheduling goals while respecting individual privacy and autonomy to the extent possible. We formally define the Multiagent Simple Temporal Problem for naturally capturing
and reasoning over the distributed but interconnected scheduling problems of multiple individuals. Our hypothesis is that combining bottom-up and top-down approaches will lead
to effective solution techniques. In our bottom-up phase, an agent externalizes constraints
that compactly summarize how its local subproblem affects other agents subproblems,
whereas in our top-down phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others. We confirm this hypothesis
by devising distributed algorithms that calculate summaries of the joint solution space
for multiagent scheduling problems, without centralizing or otherwise redistributing the
problems. The distributed algorithms permit concurrent execution to achieve significant
speedup over the current art and also increase the level of privacy and independence in
individual agent reasoning. These algorithms are most advantageous for problems where
interactions between the agents are sparse compared to the complexity of agents individual
problems.

1. Introduction
Computational scheduling agents can assist people in managing and coordinating their
activities in environments where tempo, a limited view of the overall problem, and complex
activity interdependencies can outstrip peoples cognitive capacity. Often, the schedules
of multiple agents interact, which requires that an agent coordinate its schedule with the
schedules of other agents. However, each individual agent, or its user, may have strategic
interests (privacy, autonomy, etc.) that prevent simply centralizing or redistributing the
problem. In this setting, a scheduling agent is responsible for autonomously managing the
scheduling constraints as specified by its user. As a concrete example, consider Ann, who
faces the task of processing the implications of the complex constraints of her busy schedule.
A further challenge for Ann is that her schedule is interdependent with the schedules of her
colleagues, family, and friends with whom she interacts. At the same time, Ann would prefer
c
2013
AI Access Foundation. All rights reserved.

fiBoerkoel & Durfee

the independence to make her own scheduling decisions and to keep a healthy separation
between her personal, professional, and social lives.
The Simple Temporal Problem (STP) formulation is capable of representing scheduling problems where, if the order between pairs of activities matters, this order has been
predetermined. As such, the STP acts as the core scheduling problem representation for
many interesting planning problems (Laborie & Ghallab, 1995; Bresina, Jonsson, Morris,
& Rajan, 2005; Castillo, Fernandez-Olivares, & O. Garca-Perez, 2006; Smith, Gallagher,
Zimmerman, Barbulescu, & Rubinstein, 2007; Barbulescu, Rubinstein, Smith, & Zimmerman, 2010). One of the major practical advantages of the STP representation is that there
are many efficient algorithms that compute flexible spaces of alternative solutions (Dechter,
Meiri, & Pearl, 1991; Hunsberger, 2002; Xu & Choueiry, 2003; Planken, de Weerdt, &
van der Krogt, 2008; Planken, de Weerdt, & Witteveen, 2010a). This improves robustness
to the stochasticity of the real world and the unreliability of human users.
For instance, an STP can be used to schedule the morning agendas of Ann, her friend
Bill, and her doctor Chris by representing events such as the start time, ST , and end time,
ET , of each activity as variables, and capturing the minimum and maximum durations of
time between events as constraints. Ann will have a 60 minute recreational activity (RA+B )
with Bill before spending 90 to 120 minutes performing a physical therapy regimen (TR A )
to help rehabilitate an injured knee (after receiving the prescription left by her doctor
Chris); Bill will spend 60 minutes in recreation (RA+B ) with Ann before spending 60 to 180
minutes at work (W B ); and finally, Chris will spend 90 to 120 minutes planning a physical
therapy regimen (TP C ) for Ann and drop it off before giving a lecture (LC ) from 10:00 to
12:00. This example scheduling problem is displayed graphically in Figure 1a as a Simple
Temporal Network (STN)a network of temporal constraints where each variable appears
as a vertex with its domain of possible clock times described by a self-loop, and constraints
appear as weighted edges. Figure 1b represents one example solutionan assignment of
specific times to each event that respects constraints.
Suppose that Ann, Bill, and Chris would each like to task a personal computational
scheduling agent with coordinating and managing the possible schedules that can accomplish his/her agenda while also protecting his/her interests. Unfortunately, current solution
algorithms for solving STPs require representing the entire problem as a single, centralized
STP, as shown in Figure 1, in order to calculate a (space of) solution schedule(s) for all
agents. The computation, communication, and privacy costs associated with centralization
may be unacceptable in multiagent planning and scheduling applications, such as military,
health care, or disaster relief, where users specify problems to agents in a distributed fashion, and agents are expected to provide unilateral, time-critical, and coordinated scheduling
assistance, to the extent possible.
This motivates our first major contribution: a formal definition of the Multiagent Simple Temporal Problem (MaSTP). The MaSTP, along with its associated Multiagent Simple
Temporal Network (MaSTN) are distributed representations of scheduling problems and
their solutions. The MaSTN corresponding to our running example is displayed in Figure 2a. Like their centralized counterparts, MaSTPs can be used to help solve multiagent
planning problems and to monitor multiagent plan execution in a distributed manner. This
representation allows each person to independently specify a scheduling problem to his or
her own scheduling agent, and allows each agent, in turn, to independently maintain infor96

fiDistributed Reasoning for Multiagent Simple Temporal Problems

8: 00,12: 00



8: 00,12: 00

90,120




8: 00,12: 00

8: 00,12: 00
60,60

+


+






120,120

10: 00,10: 00



12: 00,12: 00




90,120

8: 00,12: 00







60,180

8: 00,12: 00

8: 00,12: 00

8: 00,12: 00

(a) The example problem represented as a network of temporal constraints.

8: 00



9: 30



9: 00

8: 00
+


+






10: 00


12: 00







9: 30

9: 00




10: 00

11: 00

(b) A fully assigned point solution to the example problem.

Figure 1: A typical, centralized STP representation of example problem.

mation about the interrelated activities of its user. Each scheduling agent, then, can use
this information to provide its user with exact times when queried about possible timings
and relationships between events, without unnecessarily revealing this information to other
agents.
The second major contribution of this paper is a distributed algorithm for computing
the complete joint solution space, as displayed in Figure 2b. This leads to advice that
is more robust to disturbances and more accommodating to new constraints than finding
single joint solutions like in Figure 1b. An advantage of this approach is that if a new
constraint arrives (e.g., Chris bus is late), the agents can easily recover by simply eliminating newly invalidated joint schedules from consideration. As agents make choices or
adjust to new constraints, our distributed algorithm can simply be reapplied to recompute
the resulting narrower space of remaining possible joint schedules, such that subsequent
choices are assured to also satisfy constraints. So long as agents wait for the ramifications
of choices/adjustments to propagate before making other choices, they eventually converge
to a schedule that works, rather than prematurely picking a brittle schedule that needs to
be replaced. However, this flexibility comes at the cost of ongoing communication, along
with coordinating the distributed decisions to ensure the implications of one agents decision propagates so that another agents decision will be compatible. For example, if Ann
decides to start recreating at 8:30, Bills agent must wait for this decision to propagate before advising Bill on his options of when he can recreate, otherwise he might inadvertently
choose a different time.
97

fiBoerkoel & Durfee

Chris

8: 00,12: 00

8: 00,12: 00

Chris
90,120

8: 00,12: 00
8: 
00,12:
00

Chris 

8: 00,12: 00

Ann

8: 00,12: 00

8: 00,12: 00

Ann
60,60

 00
8: 00,12:
8: 00,12:
00


Ann

Bill

8: 00,12: 00

Bill
60,60

 00
8: 00,12:
00
8: 00,12:


Bill
60,60

00
8: 00,12:
00 60,60 8: 00,12:
8: 00,12:
00
8: 00,12:


 00
0,0




60,60
60,60
90,120


















0,0






90,120
60,180
120,120





0,0



 00
10:00,10:
00
12: 00,12:
8:
00,12:
8: 00,12:
00
8: 00,12:
00
8: 00,12:
00
 00

90,120
60,180
120,120





00,10:



 00
 00Example

10:Our
00
12: 00,12:
8:
00,12:
8: 00,12:
00
00,12:
00 60,180
8: 00,12:
00 disOriginal
MaSTP.
(a)
first
contribution
is the(a)
Multiagent
Simple
Temporal
Network 8:
representation,
which
allows
90,120
120,120
tributed representation of multiagent STPs and their solutions.
10: 00,10: 00
8: 00,12: 00Example
00
8: 00,12: 00 Bill 8: 00,12: 00
Chris12: 00,12: 00 (a) Original
Ann 8: 00,12:MaSTP.
9:
00,10:
30
8:
00,9:
30
9: 30,10: 00
8: 00,9: 30
9: 00,10: 30
8: 00,8: 30
00
8: 
00,12:
0090,1208: 00,12:




 30
8:
00,8:




Chris
Chris

90,120
 00
9: 30,10:


(a) Original Example
MaSTP.
Ann
60,60

8: 
00,9:
30


Ann

9: 00,10:
  30


0,0

 30
8: 00,9:


Bill
Bill

60,60

 30
9: 00,10:


60,60 9: 00,10: 30
8:60,150
00,9:
30 60,60 9: 00,10:
8: 00,9:
 30


 30




0,0
60,60
60,60




60,150








0,0




90,120
60,180
60,150








 00
10:00,10:
00
12: 00,12:
9:
30,10:
11: 00,12:
00
9: 00,11:
00
10: 00,12:
00
 30

90,120
60,180
120,120









 00
 30

10:00,10:
00
12: 00,12:
30,10:
11: 00,12:
00
9: 00,11:
00 60,18010: 00,12:
00
(b) 9:Minimal
(PPC)
Version.
90,120
120,120
90,1209: 30,10: 00
8:
00,8:
 30



90,120








120,120

10:Our
00,10:
00
12: 00,12: 00
30,10: 30 Ann
11: 00,12:
00
9: 00,11:
00 Billspace
10: 00,12:
00
Chris
(b)
second
contribution
is a (b)
new9:distributed
algorithm
for computing
the complete
of solutions
Minimal
(PPC)
Version.
9:
45,9:
45
8:
45,8:
45
9:
30,10:
00
8:
45,8:
45
9:
45,9:
45
to8:an00,8:
MaSTP.
30
 30
8:
00,8:


Chris
Chris

90,120
 00
9: 30,10:


90,1209: 30,10: 00
8:
00,8:
 30



90,120








120,120

 00
10:00,10:
00
12: 00,12:

120,120

 00

10:00,10:
00
12: 00,12:
120,120

(b) MinimalAnn
(PPC) Version.
60,60

45
9: 45,9:
8: 
45,8:
45


Ann
8:60,105
45,8:
45 60,60 9: 45,9:45



60,60


60,105






90,120
60,105



10:
00,10:
30
11: 30,12:
00

90,120




10:
00,10:
3090,12011:Version.
30,12:
00
(c)
Decoupled

 45
8: 45,8:


Bill
Bill

60,60

45
9: 45,9:


60,60 9: 45,9: 45
8: 45,8:
 45



60,60








60,135


9: 45,11:
00



10: 45,12:
00
60,135


 00

9: 45,11:
60,13510: 45,12: 00

10: 00,10: 00
10: 00,10:
30Ann11:Version.
30,12: 00
9: 45,11: 00 Bill 10: 45,12: 00
Chris12: 00,12: 00 (c)
Decoupled
9: 00
8: 00
9: 30
8: 00
9: 00
8: 00
(c) Our thirdChris
contribution is a new(c)
distributed
temporal
decoupling algorithm for computing
Decoupled
Ann Version.
Bill locally independent
00solution spaces.
00
 00


9: 
00
9:
30
8:
9: 
00
8:


8:



Ann

Chris

Bill

9: 00
Figure 2: A summary
of our8:contributions
as
applied
to the
example problem.
9: 30
8:00
9: 00
8:00
 00



















10:
 00


12:00



10:
 00



12:00

10: 00

12: 00























9:
00


10:00





9:
30 Assigned
00
(d) Fully
Version.
98 11:



9:
00



10:00

9: 30
00
(d) Fully
Assigned11:Version.
(d) Fully Assigned Version.

9: 00

10: 00



9:
30




11:
00



fiDistributed Reasoning for Multiagent Simple Temporal Problems

This motivates our third major contribution: a distributed algorithm for computing
a temporal decoupling. A temporal decoupling is composed of independent spaces of
locally consistent schedules that, when combined, form a space of consistent joint schedules
(Hunsberger, 2002). An example of a temporal decoupling is displayed in Figure 2c, where,
for example, Chris agent has agreed to prescribe the therapy regimen by 10:00 and Anns
agent has agreed to wait to begin performing it until after 10:00. Here, not only will the
agents advice be independent of other agents, but the temporal decoupling also provides
agents with some resiliency to new constraints and enhances users flexibility and autonomy
in making their own scheduling decisions. Now when Chris bus is late by a minute, Chris
agent can absorb this new constraint by independently updating its local solution space.
The advantage of this approach is that once agents establish a temporal decoupling, there
is no need for further communication unless a new constraint renders the chosen decoupling
inconsistent. It is only if and when a temporal decoupling does become inconsistent (e.g.,
Chris bus is more than a half hour late, causing her to violate her commitment to finish
the prescription by 10:00) that agents must calculate a new temporal decoupling (perhaps
establishing a new hand-off deadline of 10:15), and then once again independently react to
newly-arriving constraints, repeating the process as necessary.
The rest of our paper is structured to more fully describe each of these contributions.
Section 2 builds foundational background necessary for understanding our contributions.
Section 3 formally defines the MaSTP and explains important properties of the corresponding MaSTN. Section 4 describes our distributed algorithm for computing complete MaSTP
solution spaces, analytically proving the algorithms correctness and runtime properties,
and empirically demonstrating the speedup it achieves over previous centralized algorithms.
Section 5 describes our distributed algorithm for decoupling MaSTP solution spaces, again
proving the algorithms correctness and runtime properties, and empirically comparing its
performance to previous art to demonstrate the trade-offs in completeness versus independence in reasoning. We give an overview of related approaches in Section 6 and conclude
our discussion in Section 7.

2. Background
In this section, we provide definitions necessary for understanding our contributions, using
and extending terminology from previous literature.
2.1 Simple Temporal Problem
As defined by Dechter et al. (1991), the Simple Temporal Problem (STP), S = hX, Ci,
consists of a set of n timepoint variables, X, and a set of m temporal difference constraints,
C. Each timepoint variable represents an event and has a continuous domain of values (e.g.,
clock times) that can be expressed as a constraint relative to a special zero timepoint
variable, z  X, which represents the start of time. Each temporal difference constraint cij
is of the form xj  xi  bij , where xi and xj are distinct timepoints, and bij  R  {} is a
real number bound on the difference between xj and xi . Often, as notational convenience,
two constraints, cij and cji , are represented as a single constraint using a bound interval of
the form xj  xi  [bji , bij ].
99

fiBoerkoel & Durfee

Ann

Availability
A
RST
 z  [480, 720]
A
RET
 z  [480, 720]
A
TR ST  z  [480, 720]
TR A
ET  z  [480, 720]

Bill

B
RST
 z  [480, 720]
B
RET
 z  [480, 720]
B
WST
 z  [480, 720]
B
WET
 z  [480, 720]

Chris

TP C
ST  z  [480, 720]
TP C
ET  z  [480, 720]
LC
ST  z  [600, 600]
LC
ET  z  [720, 720]

Duration

Ordering

A
A
RET
 RST
 [60, 60]

External
A
B
RST
 RST
 [0, 0]

A
A
RET
 T RST
0
A
TR A
ET  T RST  [90, 120]

A
TP C
ET  T RST  0

B
B
RET
 RST
 [60, 60]
B
B
RET
 WST
0

A
B
RST
 RST
 [0, 0]

C
TP C
ET  LST  0

A
TP C
ET  T RST  0

B
B
WET
 WST
 [60, 180]

C
TP C
ET  T PST  [90, 120]

C
LC
ET  LST  [120, 120]

Table 1: Summary of the constraints for the running example problem.
A schedule is an assignment of specific time values to timepoint variables. An STP is
consistent if it has at least one solution, which is a schedule that respects all constraints.
In Table 1, we formalize the running example (introduced in Section 1) with specific
constraints. Each activity has two timepoint variables representing its start time (ST )
and end time (ET ), respectively. In this example, all activities are to be scheduled in the
morning (8:00-12:00), and so are constrained (Availability column) to take place within 480
to 720 minutes of the zero timepoint z, which in this case represents midnight. Duration
constraints are specified with bounds over the difference between an activitys end time
and start time. Ordering constraints dictate the order in which an agents activities must
take place with respect to each other. Finally, while a formal introduction of external
constraints is deferred until later (Section 3), the last column represents constraints that
span the subproblems of different agents. Figure 1b illustrates a schedule that represents a
solution to this particular problem.
2.2 Simple Temporal Network
To exploit extant graphical algorithms (e.g., shortest path algorithms) and efficiently reason over the constraints of an STP, each STP is associated with a Simple Temporal Network (STN), which can be represented by a weighted, directed graph, G = hV, Ei, called
a distance graph (Dechter & Pearl, 1987). The set of vertices V contains a vertex vi for
each timepoint variable xi  X, and E is a set of directed edges, where, for each constraint
cij of the form xj  xi  bij , a directed edge eij from vi to vj is constructed with an initial
weight wij = bij . As a graphical short-hand, each edge from vi to vj is assumed to be
bi-directional, capturing both edge weights with a single interval label, [wji , wij ], where
100

fiDistributed Reasoning for Multiagent Simple Temporal Problems

vj  vi  [wji , wij ] and wij or wji is initialized to  if there exists no corresponding
constraint cij  C or cji  C, respectively. An STP is consistent if and only if there exist
no negative cycles in the corresponding STN distance graph.
The reference edge, ezi , of a timepoint vi is the edge between vi and the zero timepoint z. As another short-hand, each reference edge ezi is represented graphically as a
self-loop on vi . This self-loop representation underscores how a reference edge eiz can be
thought of as a unary constraint that implicitly defines vi s domain, where wzi and wiz
represent the earliest and latest times that can be assigned to vi , respectively. In this work,
we will assume that z is always included in V and that, during the construction of G, a
reference edge ezi is added from z to every other timepoint variable, vi  Vz .
The graphical STN representation of the example STP given in Table 1 is displayed in
A
Figure 2a. For example, the duration constraint TR A
ET  T RST  [90, 120] is represented
A
graphically with a directed edge from TR A
ST to TR ET with label [90, 120]. Notice that
B
B
the label on the edge from RET to WST has an infinite upper bound, since while Bill
must start work after he ends recreation, there is no corresponding constraint given for
how soon after he ends recreation this must occur. Finally, the constraint LC
ST  z 
,
with
a
label
of
[10:00,10:00],
which
represents
[600, 600] is translated to a unary loop on LC
ST
that Chris is constrained to start the lecture at exactly 600 minutes after midnight (or at
exactly 10:00 AM). Throughout this work, we use both STP and STN notation. The
distinction is that STP notation captures properties of the original problem, such as which
pair of variables are constrained with which bounds, whereas STN notation is a convenient,
graphical representation of STP problems that agents can algorithmically manipulate in
order to find solutions by, for example, capturing implied constraints as new or tightened
edges in the graph.
2.3 Useful Simple Temporal Network Properties
Temporal networks that are minimal and decomposable provide an efficient representation
of an STPs solution space that can be useful to advice-giving scheduling agents.
2.3.1 Minimality
A minimal edge eij is one whose interval bounds, wij and wji , exactly specify the set
of all values for the difference vj  vi  [wji , wij ] that are part of any solution (Dechter
et al., 1991). A temporal network is minimal if and only if all of its edges are minimal. A
minimal network is a representation of the solution space of an STP. For example, Figure 2b
is a minimal STN, whereas Figure 2a is not, since it would allow Ann to start recreation
at, say, 9:31 and Figure 2c also is not, since it does not allow Ann to start at 9:30. More
practically, a scheduling agent can use a minimal representation to exactly and efficiently
suggest scheduling possibilities to users without overlooking options or suggesting options
that will not work.
2.3.2 Decomposability
Decomposability facilitates the maintenance of minimality by capturing constraints that,
if satisfied, will lead to global solutions. A temporal network is decomposable if any
assignment of values to a subset of timepoint variables that is locally consistent (satisfies
101

fiBoerkoel & Durfee

all constraints involving only those variables) can be extended to a solution (Dechter et al.,
1991). An STN is typically made decomposable by explicitly adding otherwise implicit
constraints and/or tightening the weights of existing edges or variable domains so that only
provably consistent values remain. For example, Figure 1b is trivially decomposable, since
assigning a variable a single value within its minimal domain assures that it will be part
of a solution. Figure 2b, on the other hand, is not decomposable, since, for instance, the
A = 10:30 and TR A = 11:00, while self-consistent (because there are no
assignment RET
ET
constraints that directly link these two variables), cannot be extended to a full solution.
A scheduling agent can use a decomposable temporal network to directly propagate any
newly-arriving constraint to any other area of the network in a single-step, backtrack-free
manner.
In sum, an STN that is both minimal and decomposable represents the entire set of
solutions by establishing the tightest bounds on timepoint variables such that (i) no solutions are eliminated and (ii) any self-consistent assignment of specific times to a subset of
timepoint variables that respects these bounds can be extended to a solution in an efficient,
backtrack-free manner.
2.4 Simple Temporal Problem Consistency Algorithms
In this subsection, we highlight various existing algorithms that help establish the STN
solution properties introduced in the previous subsection.
2.4.1 Full Path Consistency
Full Path Consistency
(FPC) establishes minimality and decomposability of an STP

instance in O n3 time (recall n = |V |) by applying an all-pairs-shortest-path algorithm,
such as Floyd-Warshall (1962), to its STN, resulting in a fully-connected distance graph
(Dechter et al., 1991). The Floyd-Warshall algorithm, presented as Algorithm 1, finds the
tightest possible path between every pair of timepoints, vi and vj , in a fully-connected
distance graph, where i, j, k, wij  wik + wkj . This graph is then checked for consistency
by validating that there are no negative cycles, that is, i 6= j, ensuring that wij + wji  0.
An example of the FPC version of Anns STP subproblem is presented in Figure 3a. Note
that an agent using an FPC representation can provide exact bounds over the values that
will lead to solutions for any pair of variables, regardless of whether or not a corresponding
constraint was present in the original STP formulation.
2.4.2 Graph Triangulation
The next two forms of consistency require a triangulated (also called chordal) temporal
network. A triangulated graph is one whose largest non-bisected cycle is a triangle (of
length three). Conceptually, a graph is triangulated by the process of considering vertices
and their adjacent edges, one by one, adding edges between neighbors of the currentlyconsidered vertex if no edge previously existed, and then eliminating that vertex from
further consideration, until all vertices are eliminated (Golumbic, 1980). This basic graph
triangulation process is presented as Algorithm 2. The set of edges that are added during
this process are called fill edges and the order in which timepoints are eliminated from
consideration is referred to as an elimination order , o. Figure 3b shows the topology
102

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 1: Floyd-Warshall
Input: A fully-connected distance graph G = hV, Ei
Output: A FPC distance graph G or inconsistent
1 for k = 1 . . . n do
2
for i = 1 . . . n do
3
for j = 1 . . . n do
4
wij  min(wij , wik + wkj )
5
if wij + wji < 0 then
6
return inconsistent
7

return G

Algorithm 2: Triangulate
Input: A distance graph G = hV, Ei; and elimination order o = (v1 , v2 , . . . , vn1 , vn )
Output: A triangulated distance graph G
1 for k = 1 . . . n do
2
forall i, j > k s.t. eik , ejk  E do
3
E  E  {eij }
4

return G

of a triangulated version of Anns distance graph where timepoints are eliminated in order
A , RA , RA , T RA ).
o = (T RET
ET
ST
ST
The quantity o is the induced graph width of the distance graph relative to o, and is
defined as the maximum, over all vk , of the size of vk s set of not-yet-eliminated neighbors

at the time of its elimination. The triangulate algorithm, then, operates in O n  o2 time.
Elimination orders are often chosen to attempt to find a minimal triangulation, that is to
attempt to minimize the total number of fill edges. While, generally speaking, finding the
minimum triangulation of a graph is an NP-complete problem, heuristics such as minimum
degree (selecting the vertex with fewest edges) and minimum fill (selecting the vertex
that adds fewest fill edges) are used to efficiently find elimination orders that approximate
the minimum triangulation (Kjaerulff, 1990).
2.4.3 Directional Path Consistency
An alternative to FPC for checking STP consistency is to establish Directional Path
Consistency (DPC) on its temporal network. The DPC algorithm (Dechter et al., 1991),
presented as Algorithm 3, takes a triangulated graph and corresponding elimination order,
o, as input. It traverses each timepoint, vk , in order o, tightening edges between each
pair of neighboring timepoints, vi , vj , (where vi and vj are connected to vk via edges eik
and ejk respectively) that appear after vk in order
 o, using the rule wij  min(wij , wik +
wkj ). The time complexity of DPC is O n  o2 , but instead of establishing minimality, it
establishes the property that a solution can be recovered from the DPC distance graph in
a backtrack-free manner if variables are assigned in reverse elimination order. An example
103

fi






[60,150]

[60, 




90,120

9: 00,10: 30

9: 00,10: 30

60,60







[60,150]




[90,180]
90,120

9: 00,10: 30

Boerkoel & Durfee

10: 30,12: 00

Ann

8: 00,9: 30










10: 30,12: 00

(a) Anns FPC STN.




90,120

9: 00,10: 30

8: 00,11: 00

Ann

8: 00,12: 00

60,60







9: 00,10: 30

60,60







[60,150]

[60, 



8: 00,12: 00

Ann

8: 00,9: 30




90,120

9: 00,10: 30




8: 00,12: 00

(b) Anns DPC STN.




90,120

9: 00,10: 30

10: 30,12: 00

(c) Anns PPC STN.

Figure 3: Alternative forms of Ann
consistency applied to Anns STP.
Ann
9: 00,10: 30

8: 00,9: 30




8: 00,9: 30

9: 00,10: 30

60,60

60,60


Algorithm 3: Directional Path Consistency
(DPC)





Input: A triangulated temporal [60,150]
network G = hV, Ei and corresponding elimination
[60,150]
[90,180]
order o = (v1 , v2 , . . . , vn1 , vn )


Output: A DPC distance graph G
inconsistent
or






90,120
1 for k = 1 . . . n do
90,120
9: 00,10: 30
10: 30,12: 00
2
forall i > k s.t. eik  E do
9: 00,10: 30
10: 30,12: 00
3
forall j > i s.t. ejk  E do
4
wij  min(wij , wik + wkj )
5
wji  min(wji , wjk + wki )
Ann
6
if wij + wji < 0 then
9: 00,10: 30
8: 00,9: 30
7
return inconsistent
8

return G




60,60

[60,150]




[90,180]






A , RA , RA , T RA ),
90,120
of a DPC version of Anns problem, using elimination order o = (T RET
ET
ST
ST
is presented in Figure 3b. Basically,
establishing
DPC
is 00
sufficient for establishing whether
9: 00,10:
30
10:
30,12:

consistent schedules exist or not, but limits a scheduling agent to giving useful advice over
only the last variable to be eliminated.
In Section 4.1, we discuss how we combine the reasoning of the DPC and triangulate
algorithms so that their execution can be distributed. Combining this reasoning yields
an example of a bucket-elimination algorithm. Bucket-elimination algorithms (Dechter,
1999) are a general class of algorithms for calculating knowledge compilations, solution
space representations from which solutions can be extracted in a backtrack-free, linear-time
manner. The adaptive consistency algorithm (Dechter & Pearl, 1987; Dechter, 2003)
calculates a knowledge compilation for general constraint satisfaction problems (Dechter,
1999). Adaptive consistency eliminates variables one by one, and for each variable that it
eliminates, reasons over the bucket of constraints the variable is involved with to deduce
new constraints over the remaining non-eliminated variables. Any solution to the remain104

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 4: Plankens Partial Path Consistency (P3 C)
Input: A triangulated temporal network G = hV, Ei and an elimination order
o = (v1 , v2 , . . . , vn1 , vn )
Output: A PPC distance graph G or inconsistent
1 G  DP C(G, o)
2 return inconsistent if DPC does
3 for k = n . . . 1 do
4
forall i, j > k s.t. eik , ejk  E do
5
wik  min(wik , wij + wjk )
6
wkj  min(wkj , wki + wij )
7

return G

ing subproblem can be extended to a solution to the original problem, since the solution
accounts for all constraints entailed by the eliminated variable. DPC is an important step
in establishing Partial Path Consistency, as we discuss next.
2.4.4 Partial Path Consistency
Bliek and Sam-Haroud (1999) demonstrate that Partial Path Consistency (PPC) is
sufficient for establishing minimality on an STP instance by calculating the tightest possible
path for only the subset of edges that exist within a triangulated distance graph. An
example is Xu and Choueirys algorithm 4STP (Xu & Choueiry, 2003), which processes
and updates a queue of all potentially inconsistent triangles (4) from the triangulated
graph. Alternatively, in their algorithm P3 C, Planken et al. (2008) sweep through these
triangles in a systematic order, which gives a guaranteed upper bound on its runtime,
while no such bound is known for 4STP. The P3 C algorithm, included as Algorithm 4
executes the DPC algorithm as a first phase and then executes a reverse traversal of the
DPC algorithm as a second phase, where edge weights are updated
in reverse elimination

order. Thus P3 C achieves the same complexity, O n  o2 , as the DPC algorithm. By
exploiting sparse network topology, PPC-based algorithms
may establish minimality much

faster than FPC algorithms in practice (O n  o2  O n3 ) (Xu & Choueiry, 2003;
Planken et al., 2008). The PPC representation of Anns subproblem using elimination
A , RA , RA , T RA ) is displayed in Figure 3c.
order o = (T RET
ET
ST
ST
PPC only approximates decomposability since only assignments to fully-connected subsets of variables (those that belong to the same clique in the triangulated network) are
guaranteed to be extensible to a full solution. However solutions can still be recovered in a
backtrack-free manner by either requiring constraint propagation between each subsequent
variable assignment or by assigning variables in any reverse simplicial elimination order any elimination order of variables that yields the same triangulated network (that
is, introduces no new fill edges) (Planken et al., 2008). An agent using a PPC representation can offer advice over any pair of variables that share an edge in the triangulated
graphthose that were originally related via a constraint in the original formulation and
those connected by fill edges. While, unlike FPC temporal networks, an agent using a
105

fiBoerkoel & Durfee

PPC network cannot answer queries regarding arbitrary pairs of variables (i.e., those that
do not share an edge), the sparser PPC structure will have important benefits for agents
independent and private reasoning, as discussed in Section 3.1.2.
2.5 The Temporal Decoupling Problem
Hunsberger (2002) formally defined the concept of a temporal decoupling for STPs. A
A
partitioning of an STP Ss variables
V Bff, leads naturally to the

 AintoA fftwo sets,B V 
 and
A
B
definition of two sub-STPs, S = V , C
and S = V , C B , where C A and C B are
the constraints defined exclusively over the variables in V A and V B , respectively. Then S A
and S B form a temporal decoupling of S if:
 S A and S B are consistent STPs; and
 Merging any locally consistent solutions to the problems in S A and S B yields a solution
to S.
Notice that a temporal decoupling exists if and only if the original STP is consistent.
Alternatively, when S A and S B form a temporal decoupling of S, S A and S B are said to be
temporally independent. The Temporal Decoupling Problem (TDP), then, is defined as
A
B , such that if C A and C B are combined
finding two sets of decoupling constraints,


 A A CA ffand CB


ff 
A
B
A
B respectively, then
with S and S to form S = V , C  C and S = V B , C B  C
A and S B form a temporal decoupling of STP S. A minimal decoupling is one where,
S

A or C B is relaxed (increasing the
if the bound of any decoupling constraint in either C

bound so that the constraint is more inclusive) or removed, then S A and S B no longer
form a decoupling. The original TDP algorithm (Hunsberger, 2002) executes centrally
A and C B and propagating
and iterates between proposing new constraints to add to C

these constraints to reestablish FPC on the corresponding global distance graph so that
subsequently proposed decoupling constraints are guaranteed to be consistent. An iteration
occurs for each constraint that spans between S A and S B until all such constraints have
been rendered moot due to new decoupling constraints.
A temporal decoupling trades a complete solution space with possibly messy interdependencies for a partial solution space with nice independence properties. Independent
reasoning, which can be critical in applications that must provide time-critical, unilateral
scheduling advice in environments where communication is costly or uncertain, comes at
the cost of eliminating valid joint solutions. In Section 4.3, we will present various flexibility
metrics that quantify the portion of the solution space that is retained by a given temporal
decoupling, and use these to quantify this trade-off.

3. The Multiagent Simple Temporal Problem
The problem that this paper addresses is that of developing a compact, distributed representation of, and distributed algorithms for finding (a temporal decoupling of) the joint
solution space of, multiagent scheduling problems. This section formally defines the distributed representation in the form of the Multiagent Simple Temporal Problem, extends
the definitions of minimality and decomposability to this representation, and characterizes
independence and privacy within this representation. Sections 4 and 5 will then describe
106

fiDistributed Reasoning for Multiagent Simple Temporal Problems

distributed algorithms that find either complete or temporally decoupled solution spaces
for providing users with flexible and sound scheduling alternatives. These algorithms avoid
unnecessarily centralizing or redistributing agents subproblems and also achieve significant
speedup over current state-of-the-art approaches.
3.1 Multiagent Simple Temporal Problem Formulation
The Multiagent Simple Temporal Problem (MaSTP) is composed of a local STP
subproblems, one for each of the a agents, and a set of constraints CX that establish
relationships between the local subproblems of different agents (Boerkoel & Durfee,

 i 2010,
ff
i
2011; Boerkoel, 2012). An agent is local STP subproblem is defined as SL = VL , CLi 1 ,
where:
 VLi is defined as agent is set of local variables, which is composed of all timepoints
assignable by agent i along with a variable representing agent is reference to z; and
 CLi is defined as agent is set of intra-agent or local constraints, where a local
constraint, cij  CLi , is defined as a bound bij on the difference between two local
variables, vj  vi  bij , where vi , vj  VLi .
In Figure 2a, the variables and constraints entirely within the boxes labeled Chris, Ann, and
Bill represent each persons respective local STP subproblem from the running example.
Notice, the sets VLi partition the set of all (non-reference) timepoint variables, Vz .
CX is the set of inter-agent or external constraints, where an external constraint
is defined as a bound on the difference between two variables that are local to different
agents, vi  VLi and vj  VLj , where i 6= j. However, each agent knows only the subset of
external constraints that involve its local timepoints and, as a by-product of these external
constraints, is also aware of a subset of non-local variables, where:
i is agent is set of external constraints, each of which involves exactly one of
 CX
agent is local timepoint variables (since all constraints are inherently binary); and

 VXi is agent is set of external timepoint variables, which are local to some other
i .
agent j 6= i, but are known to agent i due to an external constraint in CX
S i
The sets
all external variables is
S ofi all external constraints is CX = i CX and set of
VX = i VX . Together, an agent is set of known timepoints is VLi  VXi and its set of known
i . Note that this assumes that each constraint is known by each agent
constraints is CLi  CX
that has at least one variable involved in the constraint. In Figure 2a, external constraints
and variables are denoted with dashed edges.
More
then, an MaSTP,
S formally,
S i M, is defined as the STP M = hVM , CM i where
i
VM = i VL and CM = CX  i CL .
1. Throughout this paper, we will use superscripts to index agents and subscripts to index variables and
constraints/edges.

107

fiBoerkoel & Durfee

Figure 4: High-level overview of the MaSTP structure.

3.1.1 Minimality and Decomposability
Up until this point, we have discussed the MaSTP as a problem formulation. However, we can also discuss properties of the corresponding Multiagent Simple Temporal Network (MaSTN). An MaSTP is converted to an equivalent MaSTN in the same way that an
STP is converted into an STN, where the definition of agent is local and external edges,
i , follows analogously from the definition of C i and C i , respectively. Like in
ELi and EX
X
L
the STN, the MaSTN can be algorithmically manipulated to represent useful information.
Here, we will discuss how STN properties such as minimality and decomposability translate
to the MaSTN.
Because an MaSTN
decentralized one), properties
(a) is an STN (albeit a (b)
(c) such as minimality
and decomposability extend unhindered to multiagent temporal networks. Thus, a minimal
MaSTN is one where all the edges are minimal. Likewise, an MaSTN is decomposable if
any self-consistent assignment of values to a subset of variables can be extended to a full
joint solution.
Calculating an MaSTN that is both minimal and decomposable requires computing a
fully-connected network, which, in a multiagent setting, clobbers all independence between
agents subproblems: each of an agents timepoints will now be connected to every other
timepoint of every other agent. Full connectivity obliterates privacy (since now v  VLi , v 
VXj j 6= i) and requires that every scheduling decision be coordinated among all agents. For
this reason, our work focuses on instead establishing partial path consistency to approximate
decomposability while retaining the loosely-coupled structure that may exist in an MaSTN.
3.1.2 An Algorithm-Centric MaSTP Partitioning

(a)

(b)

The MaSTP formalization just presented naturally captures MaSTPs using an agent-centric
perspective. However, algorithmically, it is often easier to discuss an MaSTP in terms of
which parts of the problem an agent can solve independently, and which parts require shared
effort to solve. Thus, here we introduce some additional terminology that helps improve
the precision and comprehension of both our algorithmic descriptions and our analytical
arguments.
108

fiDistributed Reasoning for Multiagent Simple Temporal Problems

(a)

(a) Each agent has external, interface, and private timepoints.

(b)

(b) Local vs. External.

(c)

(c) Private vs. Shared.

Figure 5: Alternative partitionings of an agents STP.
The natural distribution of the MaSTP representation affords a partitioning of the
MaSTP into independent (private) and interdependent (shared) components. We start by
defining the shared STP, SS = hVS , CS i, which is composed of:
 VS = VX  {z}the set of shared variables is comprised of all variables that are
involved in at least one external constraint; and
 CS = {cij | vi , vj  VS }the set of shared constraints is defined as the constraints
between pairs of shared variables, and includes the entire set of external constraints
CX , but could also include otherwise local constraints that exist between two shared
variables belonging to a single agent.
Notice that, as illustrated in Figure 4, the shared STP overlaps with an agents local
i , into three distinct sets:
(a) each agent is known timepoints, V (b)
subproblem, and thus divides
 VXi agent is set of external variables, defined as before;
 VIi = VLi  VS agent is set of interface variables, which is defined as agent is set
of local variables that are involved in one or more external constraints; and
 VPi = VLi \ VS agent is set of private variables, which is defined as agent is local
variables that are not involved in any external constraints.
These three sets of variables are depicted graphically in Figure 5a. Figure 5 also highlights the two alternate partitionings of an MaSTP into agent-centric local versus external
components (Figure 5b) and algorithm-centric independent (private) versus interdependent
(shared) components (Figure 5c). More formally, this allows us to define agent is private
109

fiBoerkoel & Durfee



ff
subproblem, SPi = VPi , CPi , where agent is set of private constraints, CPi = CLi \ CS ,
is the subset of agent is local constraints that include at least one of its private variables.
The partitioning depicted in Figure 5c is useful algorithmically because it establishes
which parts of an agents subnetwork are independent of other agents (private), and which
parts are inherently interdependent (shared). Notice that agent is local constraints are
included in its private subproblem as long as they include a private variable, even if they
also involve a shared variable. This is because agent i is able to propagate changes to that
constraint, and any other private constraint, without directly affecting a shared timepoint
or constraint. Private edges connected to a shared timepoint appear to hang in Figure 5c
because the shared timepoint is actually part of the shared subproblem.
3.1.3 Independence
Algorithms that use the distributed MaSTN representation to reason over scheduling problems that span multiple agents have strategic (e.g., privacy) and computational (e.g., concurrency) advantages. The extent to which these advantages can be realized largely depends
on the level of independent reasoning that an agent is able to perform over its local problem. We define two timepoints as independent if there is no path that connects them
in the MaSTN, and dependent otherwise. Notice that all dependencies between agents
inherently flow through the set of shared variables, VS . The implication is that each agent
i can independently (and thus concurrently, asynchronously, privately, autonomously, etc.)
reason over its private subproblem SPi independently of SPj j 6= i.
Theorem 1. The only dependencies between agent is local subproblem, SLi , and another
agent js (j 6= i) local subproblem SLj , exist exclusively through the shared STP, SS .
Proof. By contradiction, assume there exist variables vi  VPi and vj  VPj such that vi and
vj are not independent given SS . This implies that there exists a path in the constraint
network between vi and vj that involves some pair of private variables vi0  VPi and vj0  VPj
that are connected via a constraint. However, this is a contradiction, since vi0 and vj0 would,
by definition, belong to VS , and thus SS . Therefore, every pair of variables vi  VPi and
vj  VPj are independent given SS .
3.1.4 Privacy
In our work, we assume that agents are cooperative. However, at the same time, a user
may still wish to avoid the gratuitous revelation of details about his or her schedule to the
agents of other people, to the extent possible. We next look the privacy that is preserved as a
byproduct of both the distributed problem representation and level of independent reasoning
as established in Theorem 1. Obviously, any coordination between agents activities has
some inherent privacy costs. However, we show that these costs are limited to the shared
timepoints and edges between them.
Notice that in Figure 2a the only variable of Anns that Bills agent starts out knowing is
A , due to Bills shared constraint with Ann. However,
her recreational start time variable RST
while Anns agent can establish PPC using a variety of different elimination orderings, Anns
agent alone cannot establish PPC over its timepoints without adding new external edges.
This is because Anns problem contains two different externally constrained timepoints that
110

fiDistributed Reasoning for Multiagent Simple Temporal Problems

share a local constraint path, regardless of which elimination order is used. So, for example
A
if Anns agent were to eliminate both of its private timepoints (TR A
ET and RET ) and then
A , the triangulation process would construct a new external edge between
eliminate RST
A
B
shared timepoints TR A
ST and RST as shown in Figure 2b. This effectively adds TR ST to
A
Bills agents set of external timepoints. Because TR ST was already shared with Chris
A is a shared edge, and the triangulation process
agent, the fill edge between it and RST
allows Bills agent to become aware of these shared components of Anns problem. The
question becomes: can Bills agent continue this process to draw inferences about any of
Anns private timepoints and edges? Theorem 2 shows that, without an exogenous source of
information, an agent will not be able to infer the existence of, the number of, or bounds on
another agents private timepoints, even if they implicitly influence the agents subproblem
through shared constraint paths.
Theorem 2. No agent can infer the existence or bounds of another agents private edges,
or subsequently the existence of private timepoints, solely from the shared STN.
Proof. First, we prove that the existence or bounds of a private edge cannot be inferred
from the shared STN. Assume agent i has a private edge, eik  EPi . By definition, at least
one of vi and vk is private; without loss of generality, assume vi  VPi . For every pair of
edges eij and ejk that are capable of entailing (the bounds of) eik , regardless of whether vj
is shared or private, vi  VPi implies eij  EPi is private. Hence, any pair of edges capable
of implying a private edge must also contain at least one private edge. Therefore, a private
edge cannot be inferred from shared edges alone.
Thus, since an agent cannot extend its view of the shared STN to include another agents
private edges, it cannot infer another agents private timepoints.
Theorem 2 implies that SS (the variables and constraints of which are represented with
dashed lines in Figure 2b) represents the maximum portion of the MaSTP that agents can
infer without an exogenous (or hypothesized) source of information, even if they collude
to reveal the entire shared subnetwork. Hence, given the distribution of an MaSTP M, if
agent i executes a multiagent algorithm that does not reveal any of its private timepoints or
constraints, it can be guaranteed that any agent j 6= i will not be able to infer any private
timepoint in VPi or private constraint in CPi by also executing the multiagent algorithmat
least not without requiring conjecture or ulterior (methods of inferring) information on the
part of agent j. More generally, it is not necessary or inevitable that any one agent knows
or infers the entire shared STP SS .
3.2 Multiagent Temporal Decoupling Problem
We next adapt the original definition of temporal decoupling (Hunsberger, 2002) as described in Section 2.5 to apply to the MaSTP. The set of agents local STP subproblems
{SL1 , SL2 , . . . , SLn } form a temporal decoupling of an MaSTP M if:
 {SL1 , SL2 , . . . , SLn } are consistent STPs; and
 Merging any combination of locally-consistent solutions to each of the problems in
{SL1 , SL2 , . . . , SLn } yields a solution to M.
111

fiBoerkoel & Durfee

(a)

(b)

(a)

(c)

(b)

Figure 6: The temporal decoupling problem calculates new local constraints that render
constraints between agents superfluous.
Alternatively, when {SL1 , SL2 , . . . , SLn } form a temporal decoupling of M, they are said to
be temporally independent. As illustrated in Figure 6, the objective of Multiagent
i for each
Temporal Decoupling Problem
is to find a set of constraints C

 i (MaTDP)
ff
i
i
i
1
2
n
agent i such that if SL+ = VL , CL  C , then {SL+ , SL+ , . . . , SL+ } is a temporal
decoupling of MaSTP M. Note that solving the MaTDP does not mean that the agents
subproblems have somehow become inherently independent from each other (with respect
to the original MaSTP), but rather that the new decoupling constraints provide agents a
way to perform sound reasoning completely independently of each other.
i , depicted with dotted lines in the right-hand side of
Notice that new constraints, C
Figure 6, allow the safe removal of the now superfluous external constraints involving agent
is local variables, and so the external constraints are also removed in the figure. Finally, notice that local variables and edges that were previously part of the shared problem (marked
in the right-hand side of Figure 6 with double edges) can now be treated algorithmically
as private. Figure 1b and Figure 2c both represent temporal decouplings of the example,
where new or tighter unary decoupling constraints, in essence, replace all external edges
(shown faded). A minimal decoupling is one where, if the bound of any decoupling coni for some agent i is relaxed (or removed), then {S 1
2
n
straint c  C
L+ , SL+ , . . . , SL+ } is
no longer a decoupling. Figure 2c is an example of a minimal decoupling whereas the de
facto decoupling formed by a full assignment (such as the one in Figure 1b) is not minimal.

4. Distributed Algorithms for Calculating Partial Path Consistency
In this section, we introduce our Distributed Partial Path Consistency (D4PPC) algorithm
for establishing PPC on an MaSTN. As illustrated in Figure 7, our algorithm works by
solving a+1 subproblems: a private agent subproblems and the one shared STP. Like the
original P3 C algorithm (Algorithm 4 on page 105), each of these subproblems is solved
112

fiDistributed Reasoning for Multiagent Simple Temporal Problems

9: 30,10: 00

90,120







8: 00,11: 00

Ann

8: 00,12: 00

60,60




Bill

8: 00,12: 00













8: 00,12: 00

60,60




[60, )




120,120

10: 00,10: 00

12: 00,12: 00




90,120

8: 00,12: 00

8: 00,10: 30

60,180

8: 00,12: 00




8: 00,12: 00

Phase 2: Eliminate Shared Timepoints
0,0







9: 30,10: 00
Chris




[60, )

8: 00,11: 00

Ann




[60, )

9: 30,10: 30

DPPC (Algorithm 8)

[0, )

Shared STN

8: 00,9: 30
Bill

Phase 3: Reinstate Shared Timepoints
[0,60]

Shared STN







9: 30,10: 00
Chris

0,0
[60,150]

8: 00,9: 30

Ann







[60,150]

9: 30,10: 30

8: 00,9: 30
Bill




Chris

9: 30,10: 00

90,120




Ann

8: 00,9: 30

9: 00,10: 30

60,60







8: 00,9: 30
0,0




Bill

9: 00,10: 30

60,60




60,150


120,120

10: 00,10: 00



12: 00,12: 00




90,120

9: 30,10: 30




11: 00,12: 00




9: 00,11: 00

60,180




10: 00,12: 00

PPC (Algorithm 6)

Phase 4 - Reinstate Private Timepoints
8: 00,8: 30

Figure 7: Our distributed PPC algorithms operate in four distinct phases.

113

DDPC (Algorithm 7)

Chris

8: 00,12: 00

DPC (Algorithm 5)

Phase 1: Eliminate Private Timepoints

fiBoerkoel & Durfee

in two phases: a forward sweep eliminates timepoints to compute implied constraints and
establish DPC and a second, reverse sweep reinstates nodes and updates their incident edges
to establish full minimality and PPC. In order to maximize concurrency and independence in
our distributed version of this algorithm, we carefully partition execution into four phases.
Agents work together to perform the forward (Phase 2, Figure 7) and reverse (Phase 3,
Figure 7) sweeps of the P3 C algorithm respectively on the shared STP, but each agent
independently performs the forward (Phase 1, Figure 7) and reverse (Phase 4, Figure 7)
sweeps of the P3 C algorithm on its private subproblem.
In Section 4.1, we introduce our 4DPC and 4PPC algorithms, which both serve as
subroutines of the D4PPC algorithm to establish DPC and PPC on each agents private
subproblem Our 4DPC algorithm tweaks the original DPC algorithm so that each agent
can independently triangulate and establish DPC over the private portion of its subproblem (Phase 1). Then, later in Phase 4, each agent executes 4PPC to reinstate its private
timepoints and complete the process of establishing PPC on its private subproblem. Next,
in Section 4.2.1, we describe the D4DPC algorithm that carefully distributes the execution
of the 4DPC algorithm so that, after separately establishing DPC on their private STNs,
agents work together to triangulate and update the remaining shared portion of the MaSTN
in a globally consistent manner, thus establishing DPC on the MaSTN as a whole (Phases
1 and 2). Finally, we present our overarching D4PPC algorithm in Section 4.2.2. Each
agent executes D4PPC separately. D4PPC first invokes D4DPC to establish DPC on
the MaSTN (Phases 1 and 2), then executes a distributed version of 4PPCs reverse sweep
for agents to cooperatively establish PPC on the shared portion of their MaSTN (Phase
3), and finally invokes 4PPC to finish PPC establishment on the agents separate private
subproblem (Phase 4). We conclude this section by proving that, despite the distribution
of their execution, these algorithms maintain their correctness (Section 4.2.3), and by empirically demonstrating that, as a result of concurrent execution, our D4PPC algorithm
achieves high levels of speedup over its centralized counterparts (Section 4.3).
4.1 Centralized Partial Path Consistency Revisited
The DPC (Algorithm 3 on page 104) and P3 C (Algorithm 4 on page 105) algorithms both
take a variable-elimination ordering and already triangulated STN as input. However, if
our aim is to decentralize algorithm execution, requiring an already triangulated network
and complete variable elimination order punts on providing a distributed solution to a
critical algorithmic requirement at best, or requires a centralized representation of the entire
network at worst, thus invalidating many of the motivations for distribution in the first
place. Hence, the point of this section is to demonstrate that both the graph triangulation
process and the elimination order construction process can be incorporated into the DPC
algorithm, whose execution we later distribute, with no added computational overhead.
Observe that both the triangulation (Algorithm 2 on page 103) and DPC algorithms end
up traversing graphs in exactly the same order, and so their processing can be combined.
Our 4DPC algorithm (Algorithm 5) is the result of modifying the DPC algorithm based on
two insights: (i) that 4DPC can construct the variable elimination order during execution
by applying the SelectNextTimepoint procedure (line 3), which heuristically chooses
the next timepoint, vk , to eliminate; and (ii) as 4DPC considers the implications of each
114

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 5: Triangulating Directional Path Consistency (4DPC)
Input: An STN G = hV, Ei
Output: A triangulated, DPC STN G and corresponding elimination order
o = (v1 , v2 , . . . , vn1 , vn ), or inconsistent
1 o  ()
2 while |V | > 0 do
3
vk  SelectNextTimepoint(hV, Ei , o)
4
V  V \ {vk }
5
o.append(vk )
6
forall vi , vj  V s.t. eik , ejk  E do
7
E  E  {eij }
8
wij  min(wij , wik + wkj )
9
wji  min(wji , wjk + wki )
10
if wij + wji < 0 then
11
return inconsistent
12

return G, o

pair of temporal difference constraints involving the removed timepoint variable, it necessarily considers the exact fill edges that the triangulation process would have added. 4DPC
requires only the original distance graph representation of the STN, G as input. Line 7 adds,
if necessary, any newly-created fill edges between vk s non-eliminated neighbors and then
proceeds to propagate the implications of the eliminated timepoints constraints forward
in lines 89. Like the DPC algorithm, 4DPC halts as soon as it detects an inconsistency
(line 11). Incorporating the triangulation process into the 4DPC algorithm reduces the
problem of distributing both the DPC and graph triangulation algorithms to that of distributing the execution of the 4DPC algorithm alone. The 4DPC algorithm is an example
of a bucket-elimination algorithm and has the property that the elimination process could
stop at any point, and any solution to the remaining subproblem is guaranteed to be extensible to a full solution involving the eliminated timepoints. Thus, a solution can be derived
from a DPC network by assigning timepoints in reverse elimination order.
As an example, consider Anns agent, which we refer to as agent A, executing the 4DPC
algorithm on Anns subproblem (see Figure 8a). The algorithm starts by using the minimum
A
fill heuristic to select variable TR A
ET for elimination and adds TR ET to its elimination order.
Next agent A loops through each pair of TR A
ET s neighbors. In this case there is only one
such pair of neighboring edges: the unary edge, which is represented algorithmically as an
edge shared with the reference timepoint z, and the edge with TR A
ST . Together, these edges
A
imply a tighter unary constraint on TR ST , lowering the upper bound to 10:30 so that it
is guaranteed to occur at least 90 minutes before the eliminated TR A
ET . By eliminating
A
A
values from TR ST s domain that are inconsistent with TR ET , agent A is guaranteed that if
a solution to the remaining (non-eliminated) subproblem exists, this solution is extensible
to include TR A
ET .
A for elimination. In this case notice that RA is involved in a
Next agent A selects RET
ET
A
A
path from RST to TR ST . So in addition to updating the domains of neighboring timepoints,
115

fiBoerkoel & Durfee

Ann
8: 00,12: 00
Ann 8: 00,12: 00
00
8: 00,12:
00 60,60 8: 00,12:




60,60







90,120






 
8: 00,12:
0090,1208: 00,12:00

8: 00,12: 00

Ann
8: 00,12: 00
Ann

00
8: 00,12:
00 60,60 8: 00,12:




60,60







90,120



8: 00,10:
30




 
8: 00,12:00

8: 00,11: 00 Ann
8: 00,12: 00
8: 00,12: 00
8: 00,11: 00 Ann
8: 00,12:
00 60,60 8: 00,12:
00




60,60





[60, 
[60, 



90,120






90,120


8: 00,10: 3090,1208: 00,12: 00
8: 00,12: 00
8: 00,10: 30
8: 00,12: 00
8: 00,12: 00
8: 00,12: 00
8: 00,12: 00
8:and
00,10:
30 eliminating
8:
00,12:
00
(a)
forward1:
sweep
of 4PPC
(4DPC) works by apply
selecting DPC
each timepoint,
before
it from
(a)ThePhase
Agents
independently
to their
local
subproblem.

further consideration, calculating the constraints implied over its remaining neighbors.

(a) PhaseAnn
1: Agents independentlyAnn
apply DPC to their local
subproblem.
9: 00,10: 30
Ann
8: 00,9: 30

Ann 9: 00,10: 30

30
8: 00,9:
30 60,60 9: 00,10:




60,60


60,150


60,150



90,120




8: 00,9:
30 60,60 9: 00,10:

 30


60,60




[60,150]

8: 00,12: 00
9: 00,10: 30
60,60
8: 00,12: 00
8: 00,9:
30




60,60




[60,150]

[60,150]



[60,150]



8: 00,9: 30

Ann

9: 00,10: 30




90,120





00
11: 00,12:


9: 30,10:
3090,12011: 00,12:
00
9: 30,10: 3090,120
8: 00,12: 00
11: 00,12: 00
9: 30,10: 30
11: 00,12: 00
9:
30,10:
30
8: 00,12:
00
(b) Phase 4: Agents independently apply
PPC

8: 00,9: 30

Ann

90,120








9: 30,10: 3090,1208: 00,12: 00

8: 00,12: 00
9: 30,10:
30 subproblem.
to their
local
(b)
Phase
4: Agents
independently
PPC intoreverse
theirorder,
local
subproblem.
(b)
The
reverse sweep
of 4PPC
works by reinstatingapply
each timepoint
tightening
its incident
edges with respect to the newly-updated, explicit constraints of its neighbors.

Figure 8: Agent A executing the two-part 4PPC on Anns private subproblem. This can
be viewed as the first (a) and last (b) phases of our overall D4PPC algorithm.

A to TR A involving RA . To guarantee that
agent A must also capture the path from RST
ST
ET
A to TR A , with a
the integrity of this path is retained, agent A adds a fill edge from RST
ST
lower bound of 60 and infinite upper bound (as implied by the path). This addition of fill
edges is the reason that the output of 4DPC is a triangulated network. Note that typically
the minimum fill heuristic would select variables that add no fill edges before selecting ones
that do, but as we explain in Section 4.2.1, to improve concurrency, we restrict an agent to
eliminating its private timepoints prior to its shared timepoints.
When 4DPC completes, any remaining, non-eliminated timepoints are involved in external constraints, and will be addressed by the distributed algorithms described shortly
in Section 4.2. Before turning to the distributed algorithms, however, we first describe
4PPC, which each agent will apply to complete the computation of PPC on its private
subproblem in Phase 4. The 4PPC algorithm, included as Algorithm 6, nearly identically
follows the original P3 C algorithm, only replacing DPC with the 4DPC algorithm, dropping the triangulation and elimination order requirements, and adding line 5 to explicitly

116

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 6: Triangulating Partial Path Consistency (4PPC)
Input: An STN G = hV, Ei; or DPC STN G w/ corresponding elimination order o
Output: A triangulated, PPC STN G or inconsistent
1 if o is not provided then
2
G, o  4DP C(G)
3
return inconsistent if 4DPC does

8

for k = n . . . 1 do
V  V  vk
forall i, j > k s.t. eik , ejk  E do
wik  min(wik , wij + wjk )
wkj  min(wkj , wki + wij )

9

return G

4
5
6
7

reinstate each eliminated timepoint during the reverse sweep. The 4PPC algorithm complements the 4DPC algorithm by reinstating eliminated timepoints in reverse elimination
order. However as a timepoint is reinstated, its incident edges are updated with respect to
its previously reinstated neighbors, whose now explicit edges are inductively guaranteed to
be minimal as a property of the 4DPC algorithm.
Figure 8b shows Anns subproblem just as agent A has returned from establishing PPC
over its shared timepoints, as soon described in Section 4.2. To convey the reverse reinstatement order, note that Figure 8b works from right to left. Agent A starts by selecting
A , the last private timepoint that it eliminated, for reinstatement. Then agent A loops
RET
A s neighbors, this time updating each incident edge with respect
through each pair of RET
to the now explicit and minimal third edge (in Figure 8b, there are three incident edges:
A s neighbors and the edge between them). This results in an
the domain of each of RET
A , which occurs exactly 60 minutes after RA , and a
updated domain of [9:00,10:30] for RET
ST
A
new upper bound on the edge shared with TR A
.
TR
is
similarly
reinstated,
completing
ST
ET
the execution of our algorithm. Next, we prove that our 4PPC algorithm is correct and
that it runs in O n  (G + o2 ) time in Theorems 3 and 4 respectively.
Theorem 3. The 4DPC and 4PPC algorithms establish DPC and PPC on an STN,
respectively.
Proof. The difference between Algorithm 6 and the P3 C algorithm (Algorithm 4, page 105),
is the call to 4DPC to obtain an elimination order, to triangulate, and to establish DPC.
The while loop of Algorithm 5 (line 2), along with lines 35, establish a total order, o, of all
vertices. Given o, lines 2, 6, and 7 exactly execute the triangulate algorithm (Algorithm 2,
page 103), which triangulates the graph so that lines 2, 6, 8, and 11 can exactly execute the
DPC algorithm (Algorithm 3, page 104). Thus the 4DPC algorithm establishes DPC. The
remainder of the algorithm exactly follows the P3 C algorithm, which Planken et al. (2008)
prove correctly establishes PPC.

117

fiBoerkoel & Durfee


Theorem 4. 4PPC executes in O n  (G + o2 ) time, where G is the complexity of the
variable selection heuristic (as applied to G) and o is the graph width induced by o.
Proof. Besides the call to 4DPC in
the first line, Algorithm 6 exactly executes P3 C on the

STN G, which requires O n  o2 time, as proven by Planken et al. (2008). Meanwhile,
the outer while loop of Algorithm 5 (line 2) is executed n times. For each iteration, all
operations require constant time other than the SelectNextTimepoint heuristic (line 3),
whose cost G is a function of the size and complexity of G, and the inner for loop (line 6),
which has complexity o2 .
Note that because an elimination order is not provided as input, the costs of the variable selection heuristic become embedded into the algorithm. These costs can range from
constant time (if selection is arbitrary) to NP-hard (if selection is optimal), but are typically polynomial in the number of vertices n and number of constraints m (Kjaerulff, 1990).
Thus, the algorithm internalizes a computational cost that is typically assumed to be part
of the preprocessing. So that our analyses are consistent with convention, and to better
capture only the computation costs associated with directly manipulating the underlying
temporal network, we will not include these G costs in our remaining analyses.
4.2 The Distributed Partial Path Consistency Algorithm
Agents execute the 4DPC and 4PPC algorithms separately on their private STNs (Phases
1 and 4, Figure 7), but correctly solving the shared STN (Phases 2 and 3, Figure 7) requires
cooperation. To accomplish this, we introduce our distributed partial path consistency
algorithm D4PPC. Our presentation parallels that of the previous section, where we begin
with the forward-sweeping D4DPC algorithm for triangulating and establishing DPC on
the MaSTN instance in Section 4.2.1, and follow this with the reverse sweep in the D4PPC
algorithm in Section 4.2.2.
4.2.1 The D4DPC Algorithm
Consider once again the example in Figure 8a. Agent A can successfully and independently
A
execute 4DPC on its two private timepoints TR A
ET and RET . At this point, consider what
A and TR A .
would happen if agent A proceeded with eliminating its other timepoints RET
ST
Each remaining timepoint is connected to portions of the MaSTN that belong to other
agents, and so agent A must now consider how its actions will impact other agents and vice
A , but unbeknownst to
versa. For example, suppose agent A is considering eliminating RST
B . In this case, agent A would
agent A, Bills agent (agent B) has already eliminated RST
A
B
eliminate RST assuming that an edge with RST still exists, when in reality, it does not.
As a result, the computation of agent A could result in superfluous reasoning or reasoning
over stale information, which ultimately could jeopardize the integrity of the output of
the algorithm as a whole. Next, we discuss how our algorithm avoids these problematic
situations.
Our distributed algorithm D4DPC (Algorithm 7) is a novel, distributed implementation
of a bucket-elimination algorithm for establishing DPC in multiagent temporal networks
(Phases 1 and 2, Figure 7). Each agent starts by completing Phase 1 by applying 4DPC on
its private STP (lines 12), as presented in the previous section and illustrated in Figure 8a.
118

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 7: Distributed Directional Path Consistency (D4DPC)


ff
Input: Agent is portion of a distance graph G i = V i , E i
i
Output: Agent is portion of a triangulated, DPC
 distance graph G and
corresponding elimination orders oiP = v1 , . . . vni and oS = (v1 , . . . vnS ), or
P
inconsistent


ff
1 G i , oiP  4DP C( VPi , E i )
2 return inconsistent if 4DPC does
3 oS  ()
4 while |VIi | > 0 do
5
RequestLock(oS )


ff
6
vk  SelectNextTimepoint( VIi , E i , oS )
7
oS .append(vk )
8
ReleaseLock(oS )
9
VIi  VIi \ {vk }
10
forall vi  VXi  oS s.t. eik  E and updates for vi have not yet been received do
11
E i  E i  BlockReceiveUpdatedEdges(Agent(vi ))
12
13
14
15
16
17
18
19
20
21
22

forall vi , vj  VSi s.t. eik , ejk  E do
E  E  {eij }
wij  min(wij , wik + wkj )
wji  min(wji , wjk + wki )
if wij + wji < 0 then
Broadcast(inconsistent)
return inconsistent
else
SendUpdatedEdge(eij , Agent(vi ))
SendUpdatedEdge(eij , Agent(vj ))
return G i , oiP , oiS

The output of 4DPC for an agents private problem (shown in Figure 9a) can be viewed as
a summary of how its private subproblem impacts the shared problem. In other words, any
schedules that are consistent with the remaining shared portion of an agents subproblem
are guaranteed to be extensible to a solution for that agents subproblem. As a result,
rather than jointly reasoning over the entire MaSTN, agents only need to cooperatively
reason over the potentially much smaller shared STN.
Continuing on to Phase 2, each agent is still responsible for eliminating each of its
own timepoints in the shared STN, and does so by securing a lock on the shared timepoint
elimination ordering (line 5), selecting one of its interface timepoints vk to eliminate (line 6),
recording vk in the shared elimination ordering (line 7), and releasing the lock (line 8).
This is done on a first-come, first-served basis. To avoid deadlocks and establish a precise
ordering over all timepoints, if two or more agents request the lock at the same time, the
lock simply goes to the agent with the smallest id. Then, before performing the basic 4DPC
inner loop for this selected timepoint, the agent blocks until it has received updated edge
119

fiBoerkoel & Durfee

Shared STN



Shared STN

9: 30,10: 00

Chris

9: 30,10:
Shared
STN 00
Chris



Shared STN

9: 30,10: 00

Chris

9: 30,10: 00
Shared
STN
Chris



Shared STN

9: 30,10: 00

Chris


[0, )

0,0




[0, ) [60, )




0,0




[60, )

9: 00,10: 30
8: 00,11:
00
8: 00,10:
00




Ann

Bill
8:
00,10:
30


[60, )
[60, )
9: 00,10:
0,0 30
8: 00,10: 30

8: 00,11:
[0, ) 00
Ann



8: 00,10: 00
Bill



 




[60, )
[0, ) [60, )
0,0
9: 30,10: 30
8: 00,11:
00
8: 00,10:
00


 30



Ann 9: 00,10:
Bill

[60, )
[60, )

9: 30,10: 30
0,0
Ann 9: 00,10: 30

8: 00,11:
[0, ) 00




[0, ) [60, )




0,0

8: 00,11:
00




9: 30,10:30

Ann
[60, )

8: 00,10: 00
Bill
[60, )




8: 00,9: 30


Bill
8:
 00
[60, ) 00,10:

8:subproblem.
00,9:
30
8: 00,11:
00
9:
30,10: 30
(a)
D4DPC
portion of
the D4PPC
uses
blocking
communication
9: 30,10:
00
(a)The
Phase
2: Collaboratively
applyalgorithm
DDPC
to shared
Ann and the network is 8:triangulated
Christimepoints are eliminated
Bill 00 in a
00,10:
to ensure that
0,0
STN manner. [0, )
globallyShared
consistent

(a) Phase 2: Collaboratively
apply DDPC to shared subproblem.




Shared STN




[0, ) [60, )

9: 30,10: 00

Chris


8: 00,11:
00


9: 30,10: 00
Shared
STN
Chris

8: 00,11:
[0, )00




Shared STN

9: 30,10: 00

Chris




0,0

9: 30,10:30

Ann
[60, )
Ann



9: 30,10:
0,030


[0, ) [60, )

8: 00,11:
00









0,0

9: 30,10:30

Ann
[60, )

[60, )



[60, )

8: 00,9: 30


Bill


8: 00,9: 30
Bill
60,150


[60, )

30
60,150 8: 00,9:


Bill

[60, )

8: 00,11:
9: 30,10:
8: 00,9: 30
[0,60]00
0,030
Ann
Bill
[0, )
60,150








[0,60] [60, )
[60,150]
0,0
Shared STN
) 30
8:[0,
00,9:
30
8: 00,9:
30
9: 30,10: 00
60,1509: 30,10:





8: 
00,11:
00 Ann 
Chris
Bill



[60, )
[60,150]
8:
00,9:
30
9: 30,10: 30
8: 00,9: 30
9: 30,10:
00
Phase
3: Collaboratively
DPPC
to shared subproblem.
8: 00,11:apply
00 Ann
Chris
Bill
9: 30,10: 00
Shared
STN
Chris

(b)

(b) As agents reinstate timepoints during the reverse sweep of the D4PPC al(b) Phase 3: Collaboratively apply DPPC to shared subproblem.
gorithm, each agent responds to each message it processed during the previous
phase with updated edge information for that edge.

Figure 9: The second and third phases of our D4PPC algorithm require coordination (as
represented by block arrows) between agents to establish PPC on the shared STN.
120

fiDistributed Reasoning for Multiagent Simple Temporal Problems

information with respect to all timepoints that share an edge with vk but appear before it
in the shared elimination ordering (line 11). This involves adding the edge, if necessary,
and then updating its weights to the minimum of its existing and newly received weights.
Once these steps are completed, an agent can safely proceed with lines 1221, which are
identical to the inner loop of the 4DPC algorithm (Algorithm 5) except for lines 2021,
which send updated edge information to each neighboring agent. This continues until an
agent has eliminated all of its interface timepoints.
Returning to our running example, Figure 8a represents the first step of the D4DPC
algorithm. Note, not all agents need to complete eliminating their private timepoints before
an agent can proceed to eliminating its shared timepoints. For example, in Figure 9b, agents
A and C both begin eliminating before agent B does. This can be due to heterogeneity
in either agents capabilities or in their subproblems. In this case, agent C appends its
A
timepoint TP C
ET to the shared elimination order before agent A appends RST . However,
A
because there are no shared edges between TP C
ET and RST , both agents can continue
concurrently (without blocking in line 11). Note that both agents A and C compute a new
domain for TR A
ET ; however, agent A records only its update of [9:00,10:30] locally. It is
not until agent A selects to eliminate TR A
ET , and thus must block to receive updates due
to TP C
,
that
agent
A
records
agent
Cs
tighter
update of [9:30,10:30]. The elimination of
ET
agent As remaining timepoints also leads to messages to agent B. However agent B only
B in the final step. Note that, if agent B had
receives these updates once it eliminates RST
B
attempted to eliminate RST before agent A had completed computing these new edges for
B, then agent B would have been forced to block until it had received the edge updates
A , which would appear before RB in the shared elimination order.
regarding RST
ST
4.2.2 The D4PPC Algorithm
The D4PPC algorithm (Algorithm 8) invokes the preceding algorithms as it accomplishes
all four phases depicted in Figure 7. It starts by invoking the D4DPC algorithm, which
results in a globally DPC and triangulated network (or detection of inconsistency), along
with an elimination order over vertices (lines 12). The new contribution of the D4PPC
algorithm then is in agents cooperatively performing the reverse sweep to establish PPC
over the shared STN (Phase 3 in Figure 7), before invoking the 4PPC algorithm to finish
establishing PPC on the private STN (Phase 4).
To cooperatively establish PPC on the shared STN, the D4PPC algorithm traverses
vertices in reverse order and, instead of calculating or updating a third edge based on its
two neighboring edges as D4DPC does, it updates the neighboring edges based on the
(newly) updated third edge. Thus, to guarantee that these updates are correctly shared
with and correctly incorporate information from other agents, if that third edge is external
to the agent, it must wait until it receives updated edge weights from the agent responsible
for updating it last in line 8. For an edge eij where i < j, agent i will have been the last
agent to update this edge, since its timepoint appears earliest in the elimination order. The
incident edge weights are then updated with respect to this updated third edge (lines 9
12). After performing all updates on an edge, the agent then communicates the updated
weights to any agent that also shares the edge (line 14). This will be any agent that sent
121

fiBoerkoel & Durfee

Algorithm 8: Distributed Partial Path Consistency (D4PPC)


ff
Input: agent is local STP instance G i = V i , E i
Output: agent
is portion
of the PPC network G i or inconsistent


1 G i , oiP = v1 , . . . vni
, oS = (v1 , . . . vnS )  D4DPC (G i )
P

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

return inconsistent if D4DPC does
for k = nS . . . 1 s.t. vk  VIi do
V  V  vk
for i = nS . . . k + 1 s.t. eik  E i do
for j = nS . . . i + 1 s.t. ejk  E i do
i and w , w have not yet been updated then
if eij  EX
ij
ji
wij , wji  BlockReceiveUpdatedEdge(Agent(vi ))
wik  min(wik , wij + wjk )
wki  min(wki , wkj + wji )
wkj  min(wkj , wki + wij )
wjk  min(wjk , wji + wik )
for j = 1 . . . k  1 s.t. eij , ejk  E i  vj  VXi do
SendUpdatedEdge(Agent(vj ), eik )


ff
4PPC ( VPi , E i , oiP )
return G i

an update regarding this edge during D4DPC. Note that the mechanisms that are in place
for guaranteeing correct communication only need to be executed for external edges.
Figure 9b shows this process in action. The first timepoint to be reinstated was also
B . As a property of DPC graphs, RB s domain of [8:00-9:30]
the last to be eliminated: RST
ST
is guaranteed to be minimal, that is, represent the exact set of values that could be part of
some joint solution. The D4PPC algorithm, then, propagates the information captured by
the tighter edges and domains of variables that appear later in the elimination order back
through the MaSTN to the variables that appear earlier. Agent B kicks off the algorithm by
skipping the inner-most for loop (since it has only one neighbor that appears after it: the
de facto z reference point), and sending its updated domain to agent A. Notice that even
B s updated domain to update both of its shared timepoints,
though agent A might need RST
the message is only received once. Then, agent A reinstates Anns variables, one by one,
A and RB to
and uses this information to update the upper bounds on the edges from RST
ST
A , and also the domain of RA . After all shared timepoints have been reinstated, the
TST
ST
algorithm terminates after each agent finishes propagating the shared updates through its
private network (as illustrated in Figure 8b). Notice that in line 15, agent i supplies the
previously computed oP to 4PPC, ensuring that 4DPC will be skipped and that timepoints
will be reinstated in reverse oP order.
Overall, our D4PPC algorithm introduces three points of synchrony during execution:
contention over the shared elimination order and the two calls to BlockReceiveUpdatedEdge. To simplify the understanding and analysis of our algorithms, we have made these
122

fiDistributed Reasoning for Multiagent Simple Temporal Problems

synchronization points explicit. A previous formulation of these algorithms (Boerkoel &
Durfee, 2010) allows agents to proceed optimistically beyond these synchronization points.
For example, an agent could optimistically begin eliminating one of its timepoints before
officially obtaining the shared elimination order lock, as long as it reevaluates whether any
other agents had concurrently eliminated a neighboring timepoint once the lock is obtained.
Likewise, during the reverse-sweep, an agent could begin updating its private edges with
the optimistic assumption that its current view of the network is up-to-date, backjumping its computation when a new edge update arrives that does not fit this assumption.
In both cases, proceeding optimistically never leads to incorrect computation, and while
computation may be wasted if invalidating updates later arrive, the agents would have sat
idle anyway waiting for those updates. Hence, at worst nothing is lost, and at best the
optimistic assumptions hold and agents have gotten a head start on further processing.
4.2.3 Theoretical Analysis
In this section, we prove that our D4PPC algorithm is correct by showing both that it
is deadlock free (Theorem 5) and establishes PPC (Theorem 6), and also prove that the
added communication does not change the underlying runtime complexity (Theorem 7).
Theorem 5. D4PPC is deadlock free.
Proof. We start by proving first that the call to D4DPC is deadlock free. There are two
lines in D4DPC (Algorithm 7) where agents may block on other agents: line 5 and line 11.
In line 5, there is only one lock (on the shared elimination order), and requests for the lock
are granted on a first-come, first-served basis, with ties being broken according to agent
id. Further, once an agent has a lock on the elimination order, oS , it executes two local
operations that select a variable to append to oS before releasing the lock again in line 8.
SelectNextTimepoint is an independent, local decision requiring only locally known
information. Hence, a deadlock cannot occur as a result of contention over oS .
This leaves line 11. Assume, by way of contradiction, that line 11 causes a deadlock.
This implies that there are two (or more) agents, i and j, where i 6= j such that each agent
is simultaneously waiting for communication from the other in line 11. Thus, there exists a
timepoint vxj  VXi  VLj for which agent i is waiting to receive updated edges from agent j,
while there is also a vyi  VXj  VLi for which agent j is waiting to receive updated edges from
agent i. Notice that vyj must appear before vxi in agent is copy of oS because, otherwise,
by the time vyj appeared in oS , agent i would have already sent agent j all edge updates
pertaining to vxi (lines 2021) in the previous loop iteration in which vxi was eliminated
(and added to oS in line 7). However, for the same reason, vxi must appear before vyj in
agent js copy of oS . But this is a contradiction, because there is only one shared elimination
order and agents can only append to it after being granted mutually-exclusive access. This
argument extends inductively to three or more agents, and so line 11 can also not be the
cause of a deadlock, which presents a contradiction.
Therefore the D4DPC algorithm is deadlock free. Because after its call to D4DPC,
the D4PPC algorithm traverses the MaSTN in the exact opposite order of the D4DPC
algorithm, the proof that the remainder of the D4PPC algorithm is deadlock free follows,
mutandis mutatis.

123

fiBoerkoel & Durfee

Theorem 6. D4PPC correctly establishes PPC on the MaSTN.
Proof (Sketch). 2 The algorithm starts by correctly establishing DPC on the MaSTN. Since,
by definition, none of an agents private timepoints share any edges with private timepoints
of any other agent, each agent can apply 4DPC to its private subproblem independently
(lines 12). Given the nature of the 4DPC algorithm as a bucket-elimination algorithm
(solutions to the remaining subproblem are guaranteed extensible to the eliminated variables), each agent will have computed constraints over its interface variables that capture
the impact its private subproblem has on the shared subproblem. The remaining algorithm
applies the 4DPC algorithm on an agents shared timepoints. Lines 58 guarantee that
a globally consistent elimination ordering of all shared timepoints is established. Finally,
lines 11 and 2021 guarantee that the edge weights used by an agent are not stale and are
consistent among all agents involved.
Then, the algorithm executes the same operations as the second phase of the 4PPC
algorithm, but in a distributed fashion, using blocking communication to guarantee that all
computation is performed using only the correctly updated edge weights.

Theorem 7. D4PPC executes in O (nP + nS )  o2 time, where nP = maxi |VPi |, nS =
|VS |, and o is the graph width induced by o.
Proof. Beyond the lines added for communication, the D4PPC algorithm exactly executes
4PPC with the caveat that the elimination order is restricted to eliminating all private
timepoints prior to all shared timepoints. The lines of code added for communication only
increase work by a constant factor within the 4PPC algorithm (for each edge update that is
performed by 4PPC, at most one message is sent or received). However, when agents must
block on edge updates from other agents, an agent may need to wait for some other agent
to complete some local computation. In the worst case, the elimination and subsequent
revisiting of all shared timepoints must be done completely sequentially, which puts
 the
algorithm in the same complexity class as the 4PPC algorithm, O (nP + nS )  o2 .
Note Theorem 7 provides a worst-case analysis. In the best case, complete concurrency
is possible, putting the runtime closer to O nL  o2 , where nL = |VL | is less than or equal
to nP + nS . That is, in the best case, no blocking occurs (and agents can execute 100%
concurrently), leading to only the costs of agents executing 4DPC on their subproblems.
Note that this best case is likely only realized when, for instance, there are no external
constraints. In our empirical evaluations, which we present next, we find that run times fall
somewhere between these two extremes in practice and closer to the best case for looselycoupled problems.
4.3 Empirical Evaluation
In this section, we empirically evaluate our distributed algorithm for solving the MaSTP.
The performance of our distributed algorithm relies on the size of each agents private
subproblem relative to the size and complexity of the collective shared subproblem. The
greater the portion of the problem that is private to an agent, rather than shared, the
2. The full version of this, and all remaining proof sketches, are available in the online appendix associated
with this publication.

124

fiDistributed Reasoning for Multiagent Simple Temporal Problems

greater the level of independent reasoning and concurrency, and thus the faster the overall
solve time of our distributed algorithm compared to a centralized algorithm.
4.3.1 Experimental Setup
Random Problem Generator. We evaluate our algorithms for solving MaSTPs using
the random problem generator described by Hunsberger (2002), which we adapt so that
it generates multiagent STP instances. Each problem instance has A agents that each
have start timepoints and end timepoints for 10 activities. Each activity is constrained
to occur within the time interval [0,600] relative to a global zero reference timepoint, z.
Each activitys duration is constrained by a lower bound, lb, chosen uniformly from interval
[0,60] and an upper bound chosen uniformly from the interval [lb, lb + 60]. In addition to
these constraints, the generator adds 50 additional local constraints for each agent and X
total external constraints. Each of these additional constraints, eij , has a bound that is
chosen uniformly from the interval [wij  t  (wij + wji ), wij ], where vi and vj are chosen,
with replacement, from the set of all timepoints with uniform probability, and t  [0, 1] is a
tightness parameter that dictates the maximum portion that an interval can be tightened,
and whose default value is set to t = 1 in these experiments. This particular problem
generator provides us the upside of directly and systematically controlling the number of
external constraints relative to the number of (and size/complexity of) agent subproblems.
Factory Scheduling Problem Benchmark. A second source of problems is from a
multiagent factory scheduling domain (Boerkoel, Planken, Wilcox, & Shah, 2013). These
randomly generated MaSTPs simulate A agents working together to complete T tasks in
the construction of a large structural workpiece in a manufacturing environment, using realistic task duration, wait, and deadline constraint settings. This publicly available benchmark (Boerkoel et al., 2013) distinguishes between structural constraintsthe existence
of which provide the underlying structure of the temporal network (e.g., all task durations are between some minimum and maximum durations and must complete before some
makespan), and refinement constraintsthose that instantiate constraints bounds with
particular weights (e.g., task-specific deadline and duration constraints). The former encode
general task structure and knowledge, while the later encode particular domain knowledge
of a factory manager to refine the space of schedules so that only feasible schedules remain.
In our experiments, we process all constraints equally, whether a structural constraint is refined with a more particular bound or not. We evaluate our approaches on the problem data
available: both as number of agents increases (A  {2, 4, 8, 12, 16, 20}, T = 20A) and also as
the total number of tasks increases (A = 16, T  {80, 160, 240, 320, 400, 480, 560}). Unlike
our Random Problem Generator, this set of problems is distinctive in that the underlying
problem size and (inter)constrainedness of each agent problem is dictated by real-world
factory needs, rather than generated to be uniformly distributed across agents.
Experimental Procedure. To capture expected trends for a particular problem source
(Random or Factory Scheduling) and parameter setting, we evaluate the average performance of an algorithm over 50 independently-generated trials. We include error bars representing one standard deviation for all of our results. Note, in many cases, the error bars
appear smaller than the tick marks used to represent the data points. Our algorithms
were programmed in Java, on a 3 GHz processor using 4 GB of RAM. For the purposes of
125

fiBoerkoel & Durfee

modeling a concurrent, multiagent system, we interrupted each agent after it was given the
opportunity to perform a single edge update or evaluation and also a single communication
(sending or receiving one message), systematically sharing the processor between all agents
involved. All approaches use the minimum fill heuristic (Kjaerulff, 1990). We applied our
approaches to connected networks of agents, although intuitively, the performance of any of
our algorithms would be enhanced by applying them to disparate agent networks independently. Finally, all problem instances were generated to lead to consistent STP instances
to evaluate a full application of each algorithm. While not necessary for our algorithms,
excluding inconsistent STP instances avoids overestimating reductions in execution times
when an algorithm halts as soon as an inconsistency is found. Moreover, unlike previous
approaches, our algorithms do not require input STPs to be triangulated (Xu & Choueiry,
2003; Planken et al., 2008).
Evaluation Metrics. When solving a traditional CSP, one of the primary measures of a
unit of computation is the constraint check. Meisels and Zivan (2007) extend this metric to
a distributed setting by introducing the non-concurrent constraint check (nccc). Note that
agents solving a distributed problem form a partial order over constraint checks based on the
fact that (i) any two constraint checks performed within the same agent must be performed
sequentially and (ii) any constraint check xi performed by agent i prior to sending a message
mij can be ordered before any constraint check xj performed by agent j after receipt of
mij . The nccc metric, then, is simply the length of the longest critical path in this partial
ordering of constraint checks. We generalized the nccc metric to our work by counting the
number of non-concurrent edge updates: the number of cycles it takes to establish PPC on
the MaSTP, where each agent is given an opportunity to update or check the bounds of
a single edge during each cycle of computation, although agents may spend this cycle idly
blocking on updates from other agents.
We report non-concurrent edge updates rather than the simulated algorithm runtime
due to limitations of simulating a distributed system. First, the heterogeneity of agent
capabilities and underlying sources of message latency can vary dramatically across different
real distributed computing systems. There is no systematic, compelling way to estimate
the runtime of such distributed systems in an assumption-free, unbiased manner, so instead
we provide an implementation- and system-independent evaluation. Second, separately
maintaining the state of many agents in a simulated distributed system introduces a large
amount of overhead due to practical issues such as memory swapping, which unduly inhibits
accurate and fair algorithm runtime comparisons. Since D4PPC requires a significant
number of messages (which would typically incur latency), we separately count the number
of computation cycles where at least one agent sends a message, which as described later,
allows for rudimentary runtime projections with latency. We report the total number of
messages sent by D4PPC later in Section 5.3.1, where we compare it against our distributed
temporal decoupling algorithm.
4.3.2 Empirical Comparison
One of the main benefits that we associate with performing more of a computation in a
distributed fashion is that it promotes greater concurrency. In this section, we explore how
well our distributed algorithms can exploit concurrent computation, reporting the number
126

fiDistributed Reasoning for Multiagent Simple Temporal Problems

of non-concurrent edge updates along with the number of computational cycles that require
messages. We compare the following approaches:
 FW  the Floyd-Warshall algorithm executed on a centralized version of the problem;
 4PPC  our 4PPC algorithm executed by a single agent on a centralized version of
the problem;
 D4PPC w/ Latency  our D4PPC algorithm where we assume that every computational cycle that might incur message latency requires an-order-of-magnitude more
time than performing a single edge update; and
 D4PPC w/o Latencyour D4PPC algorithm with no extra message latency
penalties.
Random Problems. Figure 10a compares each algorithms performance on problems
from the Random Problem generator, as the level of coupling, as measured by the number
of external constraints increases. As one would expect, as the density of the overall network increases, the 4PPC approaches the same costs as the fully-connected FW algorithm.
Without latency, our D4PPC algorithm, in expectation, maintains a steady improvement
over the its centralized counterpart, ranging from 12 times speedup (centralized computation/distributed computation) when X = 100 to a nearly 23 times speedup when X = 3200,
which is within 12% of perfect speedup for 25 agents.
Interestingly, even when there are no external constraints (X = 0), our D4PPC algorithm achieves only an 18-fold, rather than 25-fold, improvement over 4PPC. This is due
to heterogeneity in the complexity of individual agent problems. When there are no or
only very few external constraints, there are few opportunities for agents with easy-to-solve
local problems to help other agents out, and thus effectively load-balance. As the number
external constraints increases, so does the overall time complexity of both the 4PPC and
D4PPC w/o Latency approaches; however, opportunities for load-balancing and synchronization points also increase. These results indicate that the increased opportunities for
load-balancing outweigh the costs of increased synchronization as the number of external
constraints increases.
Coordination does, however, introduce overhead. For the D4PPC w/ Latency curve of
Figure 10a, we try to account for message latency by penalizing our D4PPC with the extra
computational cost equivalent to 10 edge updates for every message cycle. In practice, this
represents only a rough estimate of the costs of communication, since (i) not all messages
sent will require an agent to block (i.e., many messages will be received prior to their need)
and (ii) message latency could lead to compounding delays elsewhere in the network. Even
with a penalty for message latency, our distributed approach maintains an advantage over
the centralized approach, albeit a much smaller one (e.g., only a 20% improvement when
X = 100). As the number of external constraints increases, however, the effects of the
additional latency penalty become mitigated since (i) as the network becomes more dense,
each new external constraint is increasingly less likely to spur new messages (i.e., an edge
would have already been created and communicated), and (ii) the extra message latency
costs are amortized across a greater number of agents.
127

fiBoerkoel & Durfee

1e+009
Nonconcurrent Edge Updates

Nonconcurrent Edge Updates

1e+009
1e+008
1e+007
1e+006
100000
FW
PPC
DPPC w/ Latency
DPPC w/o Latency

10000
1000
50

100

200

400

800

FW
PPC
DPPC w/ Latency
DPPC w/o Latency

1e+008
1e+007
1e+006
100000
10000
1000

1600

3200

1

Number of External Constraints (X)

2

4

8

16

32

Number of Agents (A)

(a) Non-concurrent edge updates as the number of
external constraints X increases.

(b) Non-concurrent edge updates as the number of
agents A increases.

Figure 10: Empirical comparison of D4PPC on the randomly generated problem set.
Figure 10b shows the number of non-concurrent edge updates as the number of agents
grows. The number of non-concurrent edge updates grows linearly with the number of
agents across all approaches. Interestingly, here when we estimate the effects of message
latency, there is a cross-over point between 4PPC and D4PPC w/ Latency. A certain level
of message latency can mean that, for a small number of agents, it is faster to centralize
computation using 4PPC than to utilize parallelism with slow communication, but as the
number of agents grows, the centralized problem becomes unwieldy and the distribution
of computation makes D4PPC w/ Latency superior once again. Figure 10b also shows
that the expected runtime of D4PPC increases more slowly than FW or 4PPC, and that
D4PPCs speedup increases with the number of agents as seen by the widening relative gap
between the 4PPC and D4PPC curves. Thus, D4PPC scales with the increasing number
of agents better than 4PPC.
Factory Scheduling Problems. At a high level, Figure 11 for the Factory Scheduling
Problems shows many of the same trends that we saw in Figure 10. There are, however, a
few notable differences. First, we do not plot using a log scale, like in Figure 10, since (i)
the parameters grow on a linear scale, and (ii) we leave out the much more expensive FW
approach so that we can better compare the most similar approaches. Second, the number
of tasks and constraints across agents subproblems are not uniform, like in the Random
Problem case, leading to constraint networks that are based on underlying structure of
realistic problems. As both the number of tasks and agents increase, our D4PPC clearly
scales better than its centralized counter-part, demonstrating a robustness to heterogeneity
across agent problems. When the number of tasks is low (T = 80), D4PPC exhibits 8.5
times speedup over 4PPC which steadily grows to a 12.3 fold speedup as the number tasks
grows (recall problems are distributed across 16 agents). Similarly, the relative gap between
D4PPC and 4PPC grows linearly in the number of agents, settling to within 30% of perfect
speedup. Finally, this set of experiments more clearly demonstrates that the costs of high
message latency can be overcome as the number or complexity of agents local problems
grows sufficiently high.
128

fiDistributed Reasoning for Multiagent Simple Temporal Problems

8000

PPC
DPPC w/ Latency
DPPC w/o Latency

10000

Nonconcurrent Edge Updates

Nonconcurrent Edge Updates

12000

8000
6000
4000
2000
0

PPC
DPPC w/ Latency
DPPC w/o Latency

7000
6000
5000
4000
3000
2000
1000
0

80

160

240

320

400

480

560

2

Number of Tasks (T)

4

6

8

10

12

14

16

18

20

Number of Agents (A)

(a) Non-concurrent edge updates as the number
tasks T increases.

(b) Non-concurrent edge updates as the number of
agents A increases.

Figure 11: Empirical comparison of D4PPC on the factory scheduling problem set.

Total Effort. The minimum-fill variable ordering heuristic myopically selects the timepoint, from a set of timepoints, whose elimination it expects will lead to the fewest added
fill edges. Since the centralized algorithm, 4PPC, places no restrictions on this heuristic,
we expect the heuristic to perform well, adding few fill edges. D4PPC, on the other hand,
both restricts private timepoints to be eliminated prior to shared timepoints, and restricts
each agent to only eliminating its own timepoints. Intuitively, we expect each of these additional restrictions to hurt heuristic performance, that is, to lead to triangulations with more
fill edges. An increase in the overall number of fill edges, in turn, increases the number of
edges, and thus overall number of edge updates required to update the network. To evaluate
how efficiently effort is distributed across agents, we compare 4PPC and D4PPC in the
total number of fill edges added and the total number of edge updates (summed across all
agents). We compute the ratio of D4PPC versus 4PPC effort using both these metrics
across the two parameters that most directly impact constraint density: number of external
constraints X for our randomly generated problems, and the number of tasks T for the
factory scheduling problems. Results for both are displayed in Figure 12.
Overall, for both algorithms, the total number of fill edges and edge updates increases as
X or T increases. Figure 12a displays that relative to 4PPC, the number of fill edges and
edge updates computed by D4PPC increases at a faster rate with low X values, and at a
slower rate for higher X values, leading to a 57% increase in the total edge updates and 11%
increase in total fill edges at its peak. There are multiple trends occurring here. Early on
(X = 0 . . . 200), heterogeneity between individual agents private subproblems dictates that
the elimination of shared timepoints is done by the agent that gets there first, which is not
necessarily correlated with the least-connected shared timepoint. As the number external
constraints increases, however, an agents ability to load balance the elimination of shared
timepoints improveswhen more agents are spending more time competing for the shared
elimination order lock, the one with the least amount of (shared) computation (which is
correlated with fewest added shared fill edges) will get it. This improved load-balancing
is also accentuated by the fact that as X increases, so does the shared network density,
129

fiBoerkoel & Durfee

3

Edge Updates
Fill Edges

1.6

Ratio of Effort: DPPC vs. PPC

Ratio of Effort: DPPC vs. PPC

1.7

1.5
1.4
1.3
1.2
1.1
1

Edge Updates
Fill Edges

2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
1.2
1

50

100

200

400

800

1600

3200

80

Number of External Constraints (X)

160

240

320

400

480

560

Number of Tasks (T)

(a) Relative (to centralized) number of added fill
edges as the number of external constraints X increases for the randomly generated problem set.

(b) Relative (to centralized) number of added fill
edges as the number of tasks T increases for the
factory scheduling set.

Figure 12: Empirical comparison of D4PPC in terms of total effort.

increasing the number of fill edges that must be inevitably added by both approaches, and
thus dampening the relative advantage that 4PPC has over D4PPC.
The results displayed in Figure 12a show the trends as the density of external constraints
increases on the random problems. Figure 12b, on the other hand, shows that when the
overall number of constraints is increased more generally, the relative total effort monotonically decreases on the factory scheduling problems. This validates our previous study
that showed that eliminating private timepoints before shared timepoints actually improves
the performance of variable elimination heuristics by taking advantage of structural knowledge to avoid increasing the connectedness between agents problems (Boerkoel & Durfee,
2009). Thus, as the complexity of local problems grows relative to the complexity of the
shared problem, as is the case for these well-structured, loosely coupled factory scheduling problems, the distributed application of the variable ordering heuristic (as in D4PPC)
improves performance over its centralized application (as in 4PPC). Interestingly, in the
factory scheduling problem set, the variability in the number of shared timepoints per agent
initially causes the relative margin of extra fill edges to be much larger than in our randomly
generated problems, settling after a sufficient number of tasks are added.
4.3.3 Summary
In this section, we presented the D4PPC algorithm, our distributed version of the P3 C algorithm, which exchanges local constraint summaries so that agents can establish minimal,
partially path consistent MaSTNs. The D4PPC algorithm utilizes our D4DPC algorithm
in which an agent independently eliminates its private timepoint variables before coordinating to eliminate shared variables, which has the effect of exactly summarizing the impact
its subproblem has on other agents problems. The remainder of the D4PPC algorithm is
essentially a reverse sweep of the D4DPC algorithm, where care is taken to ensure that
an agent synchronizes its edge weights with those of other agents prior to updating its
local edges. We prove that our algorithm correctly calculates the joint solution space of
130

fiDistributed Reasoning for Multiagent Simple Temporal Problems

an MaSTP without unnecessarily revealing private variables. While D4PPC has the same
worst-case complexity as its centralized counterpart, we show empirically that it exploits
concurrency to outperform centralized approaches in practice. It scales much more slowly
with the number of agents than centralized approaches and achieves within 12% of perfect
speedup as the relative size and complexity of agents local problems grow in the problems
we studied.

5. A Distributed Algorithm for Calculating Temporal Decouplings
Our D4PPC algorithm outputs the space of all possible joint solutions for an MaSTP, as
depicted in Phase 4 of Figure 7. However, as discussed in Section 1, a potential limitation
of this approach is that dependencies between agents problems are maintained. So agents
must coordinate decisions using this full representation to avoid giving inconsistent advice,
such as if Anns agent advises Ann to start recreation at 8:00 while Bills agent advises him
to wait until 9:00. Thus, any and all advice by one agent that could potentially lead to
an update (e.g., Anns decision to start recreation at 8:00) would need to be propagated
before some other agent offers advice that can be guaranteed consistent. This presents a
challenge if updates are frequent while communication is expensive (e.g., slow), uncertain
(e.g., intermittent), or otherwise problematic. Agents must coordinate to re-establish partial
path consistency either by re-executing our D4PPC algorithm or by employing more recent
incremental versions of our algorithm that recognize that updates might only propagate
through a small portion of the MaSTN (Boerkoel et al., 2013). A temporal decoupling,
on the other hand, gives each agent the ability to reason over its users local schedule in a
completely independent manner. This allows agents to can give unilateral, responsive, and
sound, though generally incomplete, advice.
The goal of the Multiagent Temporal Decoupling (MaTD) algorithm is to find a set
of decoupling constraints C that render the MaSTPs external constraints CX moot, and
thus agents subproblems independent (see Section 3.2). This goal can be achieved by
assigning values to all of the variables on the reverse sweep (as in Figure 1b), but doing
so sacrifices all of the flexibility that is provided by maintaining solution spaces. Our
insight is that assigning private timepoints does not matter to independence, and so we can
assign just the shared variables, and then only tighten private variables in response to these
assignments (Section 5.1). We show that, in many cases, assignments to shared variables can
be relaxed to further increase flexibility (Section 5.2). Overall, our algorithms compute a
temporal decoupling while making heuristic choices to reduce flexibility sacrificed. Later in
Section 5.3, we describe ways of measuring flexibility, and empirically evaluate how different
algorithmic choices affect flexibility as well as runtime.
As shown in Figure 13, Phases 1 and 2 of our algorithm, which establish DPC on the
MaSTP, are the same as the D4PPC algorithm (shown in Figure 7). In the new Phase
3, however, agents establish a temporal decoupling by assigning their shared timepoints in
the reverse order that agents eliminated them in Phase 2. Next, in the optional Phase 4,
agents revisit each shared timepoint in original elimination order and relax all decoupling
constraints to the maximum extent possible, resulting in a minimal decoupling. Phase 5,
in which each agent independently establishes PPC on its local problem, is again the same
as the last phase of D4PPC algorithm.
131

fiBoerkoel & Durfee

9: 30,10: 00

90,120







Ann

8: 00,11: 00

8: 00,12: 00

60,60




8: 00,12: 00













Bill

8: 00,12: 00

60,60




[60, 


120,120

10: 00,10: 00



12: 00,12: 00




90,120

8: 00,12: 00

8: 00,10: 30




60,180

8: 00,12: 00

8: 00,12: 00

Phase 2: Eliminate Shared Timepoints

Eliminate



9: 30,10: 00

[0, 

0,0
[60, 




8: 00,11: 00

[60, 







8: 00,9: 30

9: 30,10: 30

Phase 3: Reinstate and Assign Shared Timepoints
Decouple



9: 45,9: 45

 =  
9: 45,9: 45

[60, 




8: 45,8: 45



8: 45,8: 45







8: 45,8: 45



8: 45,8: 45

10: 08,10: 08



10: 08,10: 08

Relax



9: 30,10: 00

[60, 




8: 45,8: 45




8: 45,8: 45
Phase 5: Reinstate Private Timepoints
 =



Chris

9: 30,10: 00

8: 00,8: 30



90,120




8: 45,8: 45



Ann
60,60







10: 00,10: 30

8: 45,8: 45


10: 00  




8: 45,8: 45

9: 45,9: 45

8: 45,8: 45













Bill

9: 45,9: 45

60,60




60,105


120,120

10: 00,10: 00



12: 00,12: 00




90,120

10: 00,10: 30

11: 30,12: 00

9: 45,11: 00

60,135




10: 45,12: 00

PPC (Algorithm 6)

Relaxation
(Algorithm 10)

Phase 4 (optional): Relax Shared Timepoint Domains

MaTD/MaTDR (Algorithm 9)

Chris

DDPC (Algorithm 7)

8: 00,12: 00

DPC (Algorithm 5)

Phase 1: Eliminate Private Timepoints

Figure 13: Our MaTD algorithm replaces the shared D4PPC phase of the D4PPC algorithm (depicted in Figure 7) with a decoupling phase that assigns shared timepoints in
reverse elimination order (Phase 3) and an optional Relaxation phase, in which timepoints
are revisited and flexibility is recovered to the extent possible.

132

fiDistributed Reasoning for Multiagent Simple Temporal Problems

5.1 A Distributed Multiagent Temporal Decoupling Algorithm
The MaTD algorithm, presented as Algorithm 9, starts (lines 1-2) by executing our D4DPC
algorithm (Algorithm 7 on page 119) to triangulate and propagate the constraints, eliminating the shared timepoints VS last. If D4DPC propagates to an inconsistent graph, the
algorithm returns inconsistent; else, as illustrated in the first two phases of Figure 13,
the resulting MaSTN captures the implications of agents private constraints as reflected in
the tighter domains of the shared timepoints. Next, MaTD initializes an empty C (line 3)
and then steps through vertices in reverse elimination order. In our example, this means
B (line 4). However, as an agent reinstates each interface timepoint, it simstarting with RST
ply assigns it rather than updating all incident edges as was the case in the reverse sweep of
the D4PPC algorithm. Typically, before assigning a reinstated timepoint, its domain must
be updated with respect to any previously assigned shared timepoints. However, because
B is the first reinstated variable, MaTD skips over the inner loop (lines 7-11) because
RST
there are no vertices later than it in oS .
B is guaranteed to be minimal, meaning that agent B
At this point, the domain of RST
can assign it any remaining value and still be guaranteed that it will be globally consistent.
The choice of which value to assign is made in line 12 by the function HeuristicAssign.
By default, HeuristicAssign assigns the timepoint to the midpoint of its bounds interval to split remaining flexibility down the middle; later we discuss alternative assignment
heuristics. Notice that if the D4DPC algorithm had not first been executed, agent B would
B to the midpoint of its original domain of 8:00-12:00 rather than its updated doassign RST
main of 8:00-9:30, which would result in an inconsistent assignment (to 10:00). However,
B happens at 8:45 to C (line 14). In line 13,
MaTD instead adds the constraint that RST

B shares external edges with Anns timepoints.
this is sent to agent A, because RST
A . Note, agent A would consider processing this variable right
The next vertex is T RST
away, but the inner loop (lines 7-11) forces agent A to wait for the message from agent B
(line 9). Once the message arrives, agent A updates its edge weights accordingly (lines 10A is at least 60 minutes after RB = 8:45, T RA s domain
11). In this case, given that T RST
ST
ST
is tightened to [9:45, 10:30]. Then in line 12, agent A chooses the decoupling point by
A occurs
splitting the difference, thus adding (and communicating) the constraint that T RST
at 10:08. This same process is repeated until all timepoints in VS have been assigned. The
resulting network and decoupling constraints are shown in Phase 3 of Figure 13.
To avoid inconsistency due to concurrency, before calculating decoupling constraints for
vk , an agent blocks in line 9 until it receives the fresh, newly-computed weights wzj , wjz
i where j > k.
from vj s agent (Agent(vj ), as sent in line 13) for each external edge ejk  EX
While this implies some sequentialization, it also allows for concurrency whenever variables
do not share an external edge. For example, in Phase 3 of Figure 13, because TP C
ET and
A do not share an edge, once agent A has assigned T RA , both agent A and agent C
RST
ST
A and TP C respectively.
can concurrently and independently update and assign RST
ET
Finally, for the moment skipping over optional Phase 4 in Figure 13, each agent establishes PPC in response to its new decoupling constraints by executing 4PPC on its private STN in Phase 5. Our algorithm represents an improvement over previous approaches
(Hunsberger, 2002; Planken, de Weerdt, & Witteveen, 2010a) in that it: (i) is distributed,
rather than centralized; (ii) executes on a sparser, more efficient network representation;
133

fiBoerkoel & Durfee

Algorithm 9: Multiagent Temporal Decoupling (MaTD) with optional Relaxation
(MaTDR)
Input: G i , agent is known portion of the distance graph corresponding an MaSTP
instance M, and a field indicating whether to RELAX decoupling
i , agent is decoupling constraints, and G i , agent is PPC distance graph
Output: C
i
w.r.t. C
1 G i , oiL , oS = (v1 , v2 , . . . vn ) D4DPC (G i )
2 return inconsistent if D4DPC does
i =
3 C
4 for k = n . . . 1 such that vk  VIi do
5
V  V  vk
DP C  w , w DP C  w
6
wzk
zk
kz
kz
7
for j = n . . . k + 1 such that ejk  E i do
i then
8
if ejk  EX
9
wzj , wjz  BlockReceiveUpdatedEdges(Agent(vj ))
10
11
12
13
14

wzk  min(wzk , wzj + wjk )
wkz  min(wkz , wkj + wjz )
wkz , wzk  HeuristicAssign(vk , G)
i
SendUpdatedEdge(wzk , wkz , Agent(vj ) j < k, ejk  EX
i
i
C  C  {(z  vk  [wzk , wkz ])}

16

if RELAX then
i  Relaxation (G i , w DP C )
G i , C

17

i
i
return 4PPC (GL+
, oiL ), C

15

(iii) eliminates the assumption that input graphs must be consistent; and (iv) performs
decoupling as an embedded procedure within the existing D4PPC algorithm, rather than
as an outer-loop.
As mentioned, a simple default for HeuristicAssign is to assign vk to the midpoint
of its directional path consistent domain (which corresponds to using the rules wzk 
wzk  21 (wzk + wkz ); wkz  wzk ). In Section 5.3.2, we explore and evaluate an alternative
HeuristicAssign function. In general, however, narrowing variable assignments to single
values is more constraining than necessary. Fortunately, agents can optionally call a relaxation algorithm (introduced in Section 5.2 and shown in Figure 13 as Phase 4) that replaces
C with a set of minimal decoupling constraints.

Theorem 8. The MaTD algorithm has an overall time complexity of O (nP + nS )  o2 ,
where nP = maxi |VPi | and nS = |VS |, and requires O (mX ) messages, where mX = |EX |.

Proof. The MaTD algorithm calculates DPC and PPC in O (nP + nS )  o2 time. Unary,
decoupling constraints are calculated for each of nS external variables, vk  VS (lines 414), after iterating over each of vk s O (o ) neighbors (lines 7-11). Thus decoupling requires O (nS  o )  O nS  o2 time, and so MaTD has an overall time complexity of
O (nP + nS )  o2 . The MaTD algorithm sends exactly one message for each external
constraint in line 13, for a total of O (mX ) messages.
134

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Theorem 9. The MaTD algorithm is sound.
Proof. Lines 1-2 return inconsistent whenever the input MaSTP M is not consistent. By
contradiction, assume that there exists some external constraint cxy with bound bxy that
is not satisfied when the decoupling constraints cxz and czy , calculated by MaTD, with
bounds b0xz and b0zy respectively, are. In such a case, b0xz + b0zy > bxy . WLOG, let x < y in
the elimination order oS .
Note, by the time vx is visited (in line 4), the following are true:
wxy  bxy ;

(1)

wzy + wyz = 0;

(2)

b0zy = wzy .

(3)

(1) is true since line 1 computes DPC using oS ; (2) is true since line 12 will have
already been executed for vy , and (3) is true by construction of czy in line 14. The only
update to wxz occurs in line 11, and, since exy is external, one of these updates will be
0 = min(w , w
wxz
xz
xy + wyz ), and thus
0
wxz
 wxy + wyz .

(4)

Combining (1), (2), and (4), yields the fact:
0
wxz
+ wzy  wxy  bxy .

(5)

0 may be further updated is in future iterations of line 11 and then possibly
The only time wxz
00 , but both lines 11 and 12 only tighten (never relax). Thus, with
in line 12 to produce wxz
(5) this implies that
00
0
wxz
+ wzy  wxz
+ wzy  wxy  bxy .

(6)

00 ; this fact, along with (3) and (6), implies
In line 14, cxz is constructed such that bxz = wxz
bxz + bzy  bxy . However, this is a contradiction to the assumption that b0xz + b0zy > bxy , so
the constraints C calculated by MaTD form a temporal decoupling of M.

Theorem 10. The MaTD algorithm is complete.
Proof (Sketch). The basic intuition for this proof is provided by the fact that the MaTD
algorithm is simply a more general, distributed version of the basic backtrack-free assignment procedure that can be consistently applied to a DPC distance graph. We show that
when we choose bounds for new, unary decoupling constraints for vk (effectively in line 12),
wzk , wkz are path consistent with respect to all other variables. This is because not only
is the distance graph DPC, but also the updates in lines 10-11 guarantee that wzk , wkz are
path consistent with respect to vk for all j > k (since each such path from vj to vk will be
represented as an edge ejk in the distance graph). So the only proactive edge tightening
that occurs, which happens in line 13 and guarantees that wzk + wkz = 0, is done on pathconsistent edges and thus will never introduce a negative cycle (or empty domain). So if the
MaSTP is consistent, the MaTD algorithm is guaranteed to find a temporal decoupling.
135

fiBoerkoel & Durfee

5.2 A Minimal Temporal Decoupling Relaxation Algorithm
One of the key properties of a minimal MaSTP is that it can represent a space of consistent
joint schedules, which in turn can be used as a hedge against scheduling dynamism. The
larger this space of solutions is, the more flexibility agents have to, e.g., add new constraints,
while mitigating the risk of invalidating all solution schedules. Flexibility , then, is a
measure of the completeness of the solution space. Flexibility is good in that it increases
agents collective abilities to autonomously react to disturbances or new constraints, but
at the same time, flexibility in the joint solution space limits each individual agent from
unilaterally and independently exploiting this autonomy. Our MaTD algorithm explicitly
trades away flexibility for increased independence during the assignment process in line 12.
In this section, we describe an algorithm that attempts to recover as much flexibility as
possible to maximize agents ability to autonomously, yet independently, react to dynamic
scheduling changes.
The goal of the Multiagent Temporal Decoupling with Relaxation (MaTDR), which
is the version of the MaTD that executes the Relaxation algorithm (Algorithm 10) as
a subprocedure, is to replace the set of decoupling constraints produced by the MaTD
0
algorithm, C , with a set of minimal decoupling constraints, C . Recall from page 111
i
that a minimal decoupling is one where, if the bound of any decoupling constraint c  C
1
2
n
for some agent i is relaxed, then {SL+ , SL+ , . . . , SL+ } is no longer guaranteed to form
a decoupling. Clearly the temporal decoupling produced when running MaTD using the
default heuristic on the example problem, as shown in Phase 3 of Figure 13, is not minimal
the decoupling bounds on TP C
ET could be relaxed to include its entire original domain and
A . The basic idea of the Relaxation algorithm is to revisit
still be decoupled from T RST
each external timepoint vk and, while holding the domains of all other external timepoint
variables constant, relax the bounds of vk s decoupling constraints as much as possible.
We describe the execution of the algorithm by describing an execution trace over the
running example problem. As depicted in Phase 4 of Figure 13, Relaxation works in origC
inal oS order, and thus starts with TP C
ET . First, Chris agent removes TP ET s decoupling
constraints and restores TP C
ET s domain to [9:30,10:00] by updating the corresponding edge
weights to their stored DPC values (line 3). Notice that lines 2-19 are similar to the reverse
sweep to the D4PPC algorithm, except that a separate, shadow  bound representation is used and updated only with respect to the original external constraint bounds (not
tightened edge weights) to ensure that the later-constructed decoupling constraints are minimally constraining. Also, in lines 13-18, a decoupling constraint is only constructed when
the bound of the potential, new constraint (e.g., kz ) is tighter than the already implied edge
weight (e.g., when kz < wkz ). For example, the only constraint involving TP C
ET is that
A
A
it should occur before T RST . Therefore because T RST is currently set to occur at 10:08
(=10:08) and TP C
occur before 10:00 (w =10:00),  6< w, no
ET is already constrained to
0
decoupling constraints are added to the set C for TP C
ET (allowing it to retain its original
domain of [9:30,10:00]).
A , whose domain relaxes back to [8:00,9:30]. However,
The next variable to consider is RST
A
B , whose current domain is [8:45,8:45],
since RST shares a synchronization constraint with RST
A  [8:45,8:45].
Anns agent will end up re-adopting the original decoupling constraint of RST
A
Next, agent A recovers T RST s original DPC domain of [9:30,10:30] and tightens it to ensure

136

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Algorithm 10: Relaxation
DP C , w DP C , for each v  V i
Input: G i , and the DPC weights, wzk
k
X
kz
0i
Output: C , agent is minimal decoupling constraints, and G i , agent is PPC distance
0
graph w.r.t. Ci
0i
1 C  
2 for k = 1 . . . n such that vk  VIi do
DP C , w
DP C
3
wzk  wzk
kz  wkz
4
zk  kz  
5
for j = 1 to n such that ejk  E i do
i then
6
if ejk  EX
7
if j < k then wzj , wjz  BlockReceiveUpdatedEdges(Agent(vj ))
8
if cjk exists then zk min(zk , bjk  wjz )
9
if ckj exists then kz min(kz , bkj  wzj )
10
11
12
13
14
15

else if j < k then
wzk  min(wzk , wzj + wjk )
wkz  min(wkz , wkj + wjz )
if kz < wkz then
wkz  kz
0
0
Ci  Ci  {(z  vk  kz )}

18

if zk < wzk then
wzk  zk
0
0
Ci  Ci  {(vk  z  zk )}

19

i
SendUpdatedEdge(wzk , wkz , Agent(vj ) j > k, ejk  EX

16
17

0

20

return G i , Ci

C
that it follows TP C
ET s new domain of [9:30,10:00]. In this case, decoupling from TP ET
requires a new lower bound of 10:00 and results in a more flexible domain of [10:00,10:30]
A . The minimal decoupling constraints and corresponding temporal network that
for T RST
Relaxation calculates for the running example are presented in Phase 4 of Figure 13 for the
shared portion of the network. Similarly, Phase 5 shows the implications of incorporating
the decoupling constraints on each agents PPC subproblem.

The Relaxation algorithm applies two different kinds of updates. When the edge ejk
considered in line 6 is local to agent i, the Relaxation algorithm executes lines 11-12,
which update the actual edge weights wzk and wkz in the same manner as the D4PPC
algorithm, guaranteeing that the values captured within the bounds interval are consistent
with at least some value of vj s domain (so that they will be locally PPC). On the other
hand, if edge ejk is external, the Relaxation algorithm instead executes lines 8-9, which
update the shadow edge weights zk and kz in a way that guarantees they will be consistent
with all values of vj s domain (so that they will be temporally decoupled). Note that while
these updates are more restrictive, they only lead to new decoupling constraints if they are
tighter than vk s current domain.
137

fiBoerkoel & Durfee

Theorem 11. The Relaxation algorithm has an overall time complexity of O (nS  o )
and requires O (mX ) messages.
Proof. Unary, decoupling constraints are recalculated for each of nS shared variables, but
require visiting each of vk  VS s O (o ) neighbors (lines 2-19), after iterating over each of
vk s O (o ) neighbors (lines 5-12). Thus the Relaxation algorithm requires O (nS  o )
time. The Relaxation algorithm sends exactly one message for each external constraint
in line 19, for a total of O (mX ) messages.
Notice that the Relaxation algorithm is called as a subroutine of the MaTD algorithm,

but runs in less time, so the overall MaTDR algorithm runtime is still O (nP + nS )  o2 .
Theorem 12. The local constraints calculated by the Relaxation algorithm form a minimal temporal decoupling of S.
0

Proof (Sketch). The proof that the set C forms a temporal decoupling is roughly analogous
to the proof for Theorem 5.1. By contradiction, we show that if the bound bxz of some
0
0
decoupling constraint cxz  C is relaxed by some small, positive value xz > 0, then C
is no longer a temporal decoupling. This is because lines 8-9 imply that there exists some
y such that either bxz = bxy  bzy , and thus bxz + xz + bzy > bxy (and thus no longer is a
temporal decoupling), or that bzy = bxy  (bxz + xz ) (and so is either not a decoupling or
requires altering bzy in order to maintain the temporal decoupling).
5.3 Evaluation
In the following subsections, we reuse the basic experimental setup from Section 4.3 to
empirically evaluate the performance of the MaTDR algorithms computational effort in
Section 5.3.1, and the impact on flexibility of variations of the MaTDR algorithm in Section 5.3.2. Like the original D4DPC algorithms, our decoupling algorithms performance
depends on the size of each agents private subproblem versus the size of the shared subproblem. As the number of external constraints relative to the number of agents increases, not
only can less reasoning occur independently, but also the resulting decoupled solution spaces
are subject to an increasing number of local constraints, and thus diminish in flexibility.
5.3.1 Evaluation of Computational Effort
We empirically compared:
 MaTDR  the default MaTD algorithm with Relaxation;
 Centralized MaTDR  a single agent that executes MaTDR on a centralized version
of the problem;
 D4PPC  the execution of the D4PPC distributed algorithm for establishing PPC
for an MaSTP (but not a decoupling); and
 TDPour implementation of the fastest variation (the RGB variation) of the (centralized) TDP algorithm as reported by Hunsberger (2002).
138

fiDistributed Reasoning for Multiagent Simple Temporal Problems

To implement the original TDP approach, we use the Floyd-Warshall algorithm to initially
establish FPC, and the incremental update described by Planken (2008) to maintain FPC
as new constraints are posted. We evaluated approaches across two metrics. The nonconcurrent edge update metric, which, as described in Section 4.3, is the number computational cycles during which an edge is updated before all agents in the simulated multiagent
environment have completed their executions of the algorithm. The other metric we report
in this section is the total number of messages exchanged by agents.
Random Problems. We evaluate our algorithms on the same randomly generated and
factory scheduling problem sets as described in Section 4.3.1. The results shown in Figure 14
demonstrate that the MaTDR algorithm clearly dominates the original TDP approach in
terms of execution time, even when the MaTDR algorithm is executed in a centralized
fashion, demonstrating the advantages of exploiting structure by using PPC (versus FPC)
and dividing the problem into local and shared subproblems. The other advantage of
MaTDR over the original TDP approach is that it incorporates the decoupling procedure
within a single execution of constraint propagation rather than introducing decoupling
constraints one at a time as an outer loop that reestablishes FPC after each new decoupling
constraint is added. Additionally, when compared to the centralized version of the MaTDR
algorithm, the distributed version has a speedup that varies between 19.4 and 24.7. This
demonstrates that the structures of the generated problem instances support parallelism
and that the distributed algorithm can exploit this structure to achieve significant amounts
of parallelism.
The MaTDR algorithm also dominates the D4PPC algorithm in terms of both computation and number of messages. This means the MaTDR algorithm can calculate a temporal
decoupling with less computational effort than the D4PPC algorithm can establish PPC
on the MaSTP. While the MaTDR is generally bound by the same runtime complexity as
the D4PPC (due to both applying the D4DPC algorithm), the complexity of the actual
decoupling portion of the procedure is less in practice as argued in Theorem 8. So while the
MaTDR algorithm calculates and communicates new bounds for all unary reference edges,
by doing so it renders all external edges moot and thus does not need to reason over them.
In contrast, the D4PPC algorithm must calculate, communicate, and maintain new bounds
for every shared edge. The total number of messages sent by both algorithms grows linearly with the number of agents and, perhaps less intuitively, sublinearly with the number
of external constraints. This indicates that, as the network becomes more saturated, each
new external constraint is increasingly more likely to be expressed over an edge that would
already require communication, and thus requires no new messages.
Factory Scheduling Problems. When we also evaluated our algorithms using the factory problem set, MaTDR exhibited similarly large speedups over the Centralized MaTDR
approach (ranging from 4 to 5 orders of magnitude depending on the number of tasks). We
excluded the TDP approach in our charts in Figure 15 to zoom in on the differences between
the (Centralized) MaTDR and D4PPC algorithms, which were much more subtle. While
still achieving impressive speedups over its centralized counterpart, the MaTDR algorithms
computational advantages over D4PPC are less pronounced than before (Figure 14) due to
the more loosely-coupled nature of the factory problem set. Similarly, the relative drop in
139

fiBoerkoel & Durfee

1e+009
Nonconcurrent Edge Updates

Nonconcurrent Edge Updates

1e+009
1e+008
1e+007
1e+006
100000
TDP
Centralized MaTDR
DPPC
MaTDR

10000
1000
50

100

200

400

800

TDP
Centralized MaTDR
DPPC
MaTDR

1e+008
1e+007
1e+006
100000
10000
1000

1600

3200

1

2

4

Number of External Constraints (X)

(a) Nonconcurrent computation as the number of
external constraints X increases.

16

32

(b) Nonconcurrent computation as the number of
agents A increases.
1e+008

DPPC
MaTDR

DPPC
MaTDR

1e+007

1e+006
Number of Messages

Total Number of Messages

1e+007

8

Number of Agents (A)

100000

10000

1000

1e+006
100000
10000
1000

100

100
50

100

200

400

800

1600

3200

2

Number of External Constraints (X)

4

8

16

32

Number of Agents (A)

(c) Number of messages as the number of external
constraints X increases.

(d) Number of messages as the number of agents A
increases.

Figure 14: Empirical evaluation of MaTDR on our randomly generated problem set.
external constraints leads, in turn, to fewer shared edges, and thus fewer messages overall,
mitigating the more substantial gap in Figures 14c and 14d.
The fact that the MaTDR algorithm dominates the D4PPC algorithm implies that,
even if the original TDP algorithm were adapted to make use of the state-of-the-art D4PPC
algorithm, the MaTDR algorithm would still outperform the revised TDP approach in terms
of computational effort. Overall, we confirmed that we could exploit the structure of the
MaSTP instances to calculate a temporal decoupling not only more efficiently than before,
but also in a distributed manner, avoiding the (previously-required) centralization costs,
and exploiting parallelism to lead to significant levels of speedup. We next ask whether
the quality of a solution produced by the MaTDR algorithm is competitive in terms of the
solution space completeness.
5.3.2 Evaluation of Completeness (Flexibility)
Hunsberger (2002) introduced two metrics, flexibility (F lex) and conversely rigidity
(Rig), that act as relative measures of the number of total solutions represented by a
140

fiDistributed Reasoning for Multiagent Simple Temporal Problems

4500

Centralized MaTDR
DPPC
MaTDR

6000

Centralized MaTDR
DPPC
MaTDR

4000
Nonconcurrent Edge Updates

Nonconcurrent Edge Updates

7000

5000
4000
3000
2000
1000

3500
3000
2500
2000
1500
1000
500

0

0
80

160

240

320

400

480

560

2

4

Number of Tasks (T)

(a) Nonconcurrent computation as the number of
tasks T increases.

12

16

20

(b) Nonconcurrent computation as the number of
agents A increases.
2000

DPPC
MaTDR

DPPC
1800 MaTDR

2500

1600
Number of Messages

Total Number of Messages

3000

8

Number of Agents (A)

2000
1500
1000

1400
1200
1000
800
600
400

500

200
0

0
80

160

240

320

400

480

560

2

Number of Tasks (T)

4

8

12

16

20

Number of Agents (A)

(c) Number of messages as the number of tasks T
increases.

(d) Number of messages as the number of agents A
increases.

Figure 15: Empirical evaluation of MaTDR on the factory scheduling problem set.
temporal network, allowing the quality or level of completeness of alternative temporal
decouplings to be compared. He defined the flexibility between a pair of timepoints, vi and
vj , as the sum F lex(vi , vj ) = wij + wji which is always non-negative for consistent STPs.
1
The rigidity of a pair of timepoints is defined as Rig(vi , vj ) = 1+F lex(v
, and the rigidity
i ,vj )
over an entire STP is the root mean square (RMS) value over the rigidity values for all
pairs of timepoints:
s
X
2
Rig(G) =
[Rig(vi , vj )]2 .
n  (n + 1)
i<j

This implies that Rig(G)  [0, 1], where Rig(G) = 0 when G has no constraints and
Rig(G) = 1 when G has a single solution (Hunsberger, 2002). Since Rig(G) requires FPC
to calculate, in our work we apply this metric only as a post-processing evaluation technique by centralizing and establishing FPC on the temporal decouplings returned by our
algorithms. There does exist a centralized, polynomial time algorithm for calculating an
optimal temporal decoupling (Planken, de Weerdt, & Witteveen, 2010a), but it requires
141

fiBoerkoel & Durfee

an evaluation metric that is a linear function of distance graph edge weights, which the
aggregate rigidity function, Rig(G), unfortunately, is not.
Our MaTDR algorithms default HeuristicAssign function assigns a timepoint to the
midpoint of its currently-calculated bounds. While the assumption is that assignment to the
midpoint (along with the relaxation) attempts to divide slack evenly, in practice subsequent
assignments are influenced by earlier ones. For example, in Figure 13, TR A
ST is assigned
B . Hence, perhaps
to 10:08AM, rather than 10:00AM, due to the earlier assignment of RST
there are smarter ways to assign timepoints that maintain greater amounts of flexibility.
We evaluate one such alternative heuristic, the locality heuristic, which can be thought of
an application of the least-constraining value heuristic that attempts to be less myopic in its
assignment procedure by considering the loss of flexibility of neighboring timepoints. Here,
an agent assigns vk to the value that reduces the domains of its neighboring timepoints the
least. As described by Hunsberger (2002), the original TDP approach attempts to maximize
flexibility by progressively tightening the reference bounds of timepoints by a fraction of
the total amount that would be required for decoupling.
Evaluation. In this set of experiments, we compare the rigidity of the temporal decouplings calculated by:
 Midpoint  a variant of the MaTD algorithm where HeuristicAssign uses the
(default) midpoint heuristic, but without Relaxation;
 Midpoint+R  the MaTD algorithm using the midpoint heuristic along with the
Relaxation sub-routine;
 Locality  a variant of the MaTD algorithm where HeuristicAssign assigns vk to
the value that reduces the domains of its neighboring, local timepoints the least (no
Relaxation);
 Locality+R  the MaTD algorithm using the locality heuristic along with the Relaxation sub-routine;
 Input  the rigidity of the input MaSTN; and
 TDP  our implementation of Hunsbergers RLF variation of his TDP algorithm
(where r = 0.5 and  = 1.0 which lead to a computational multiplier of approximately
9) that was reported to calculate the least rigid decoupling by Hunsberger (2002)
(rather than the RGB variation used earlier, which was reported to be most efficient).
In our first experiment, we used our Random Problem Generator with parameter settings A = 25 and X = {50, 200, 800}. The left-hand side of Table 2 displays the rigidity
of the temporal decoupling calculated by each approach over these problems. Unsurprisingly, as the number of external constraints increases, so does the level of rigidity across
all approaches, including the inherent rigidity of the input MaSTN. Combing these rigidity
results with our evaluation of computational effort displayed in Figure 14, we can look at
the trade-offs between efficiency and quality of our temporal decoupling approaches. On
average, as compared to Midpoint, the Midpoint+R approach decreases rigidity by 51.0%
while increasing computational effort by 30.2%, and the Locality approach decreases rigidity
142

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Table 2: A comparison of how much four different MaTD variants increase rigidity as compared to the input MaSTN and the previous centralized approach (TDP) across randomly
generated and factory scheduling problem sets.
Randomly Generated Problems

Factory Scheduling Problems

X = 50

X = 200

X = 800

T = 80

T = 320

T = 560

Input

0.418

0.549

0.729

0.274

0.148

0.119

Locality+R

0.508

0.699

0.878

0.756

0.838

0.838

Midpoint+R

0.496

0.699

0.886

0.765

0.853

0.855

Locality

0.621

0.842

0.988

0.965

0.983

0.982

Midpoint

0.628

0.849

0.988

0.968

0.983

0.988

TDP

0.482

0.668

0.865

0.720

0.722

0.706

by 2.0% while increasing computational effort by 146%. The Midpoint+R approach, significantly improves the output decoupling with the least computational overhead. The Locality
heuristic, on the other hand, more than doubles computational overhead while providing no
significant improvement in rigidity. We also explored combining these rigidity-decreasing
heuristics, and while the increase in computational effort tended to be additive (the Locality+R approach increases effort by 172%), the decrease in rigidity did not. In fact, no other
heuristics that we tried led to a statistically significant decrease in rigidity compared to
the original Midpoint+R approach in the cases we investigated. The Locality+R approach
came the closest, decreasing rigidity by 49.9% in expectation.
In our second experiment, we used the Factory Scheduling Problem benchmark with
parameter settings A = 16 and T = {80, 320, 560} and ensured that each problem was set
with a tight makespan; these results appear in the right-hand side of Table 2. Interestingly,
as the number of tasks increases, the level of rigidity of the input MaSTN decreases, while
the rigidity of the calculated decouplings plateaus. This is because adding tasks does not
directly correlate to adding interdependencies: only if the tasks happen to be naturally
dependent on one another and assigned to different agents is the level of coupling affected. So
setting a tight makespan on a set of tasks has the effect of making the critical path through
the workflow rigid, but leaves many other workflow paths flexible. However, decoupling
effectively introduces many additional interim makespan deadlines, which can lead to many
rigid paths in the workflow. Overall, the increased structure of these problems led to
starker differences between approaches. Computing a temporal decoupling results in a 3fold increase in rigidity over the input MaSTP regardless of the decoupling approached
used. Once again, including the relaxation phase decreased rigidity, by an additional 20.2%
margin when using the Midpoint heuristic and by a 21.5% margin when using the Locality
heuristic. The differences in flexibility between the outputs of the Midpoint(+R) and the
computationally more expensive Locality(+R) approaches are insignificant.
This is far from conclusive evidence that there are no other variable assignment heuristics
that would outperform either the default midpoint or locality heuristics. It does, however,
point to the robustness of the MaTDR algorithm at recovering flexible decouplings. While
143

fiBoerkoel & Durfee

there was not a significant difference between the Locality and Midpoint heuristics, nor the
Locality+R and Minimality+R approaches, the addition of the relaxation led to a significant
improvement in both cases. The fact that the MaTDR algorithm alone increases rigidity
less than any other strategy can be attributed to both the structure of an MaSTP and how
rigidity is measured. The MaTDR algorithm improves the distribution of flexibility to the
shared timepoints reactively, instead of proactively trying to guess good values. As the
MaTD algorithm tightens bounds, the general triangulated graph structure formed by the
elimination order branches out the impact of this tightening. For instance, when a timepoint
is assigned, it defers flexibility to its possibly many neighboring timepoints, and then during
Relaxation our algorithm recovers flexibility for each these neighboring timepoints before
recovering flexibility for the originally assigned timepoint.
Notice from Table 2 that the original TDP approach increases the rigidity the least,
averaging 20.6% less rigidity than our Midpoint+R approach for the randomly generated
problem set and 15.9% for the factory scheduling problems. However, this lower rigidity
comes at a significant computational costthe original TDP approach incurs, in expectation, over 10,000 times more computational effort than the Midpoint+R approach. Further,
the original TDP approach has access to edge information involving any pair of timepoints
in the entire, centralized MaSTN, clobbering all privacy, while the MaTDR makes its heuristic decoupling decisions with much more limited information, maintaining greater levels of
agent privacy. While in some scheduling environments the costs of centralization (e.g., privacy) alone would discourage using the original TDP approach, in others the computational
effort may be prohibitive if constraints arise faster than the centralized TDP algorithm can
calculate a temporal decoupling. Further, differences in rigidity between decoupling approaches may become mitigated in scheduling problems where many external constraints
enforce synchronization (e.g., Anns and Bills recreational start time). Because synchronization requires fully assigning timepoints in order to decouple, there may be no flexibility
to recover, which makes our efficient, assignment-based MaTD algorithm the better choice.
5.3.3 Summary
Our MaTDR algorithm combines the reasoning of the D4PPC algorithm with a decoupling
procedure that first assigns, then relaxes, shared timepoints in a way that leads to a minimal
decoupling for an MaSTP. We showed that while this algorithm has the same complexity
as the original D4PPC algorithm, it reduces solve time in expectation. Overall, on the
space of problems that we investigated, the Midpoint+R approach, in expectation, outputs
a high-quality temporal decoupling that approaches the quality (within 20.6% for random
problems and 15.9% for factory scheduling problems) of the state-of-the-art centralized
approach (Hunsberger, 2002), in a distributed, privacy-maintaining manner and multiple
orders-of-magnitude faster than this previous centralized approach.

6. Related Work
In this section, we summarize related approaches and highlight applications that could
benefit from using our MaSTP formulation and algorithms.
144

fiDistributed Reasoning for Multiagent Simple Temporal Problems

6.1 Simple Temporal Problem with Uncertainty
A Simple Temporal Problem with Uncertainty (STPU) (Vidal & Ghallab, 1996; Vidal
& Fargier, 1999) partitions the set of constraints of an STP into a set of requirement links
and a set of contingent links. Requirement and contingent are both temporal difference
constraints, as defined in Section 2.1. The difference is that a requirement link can be
assigned by the agent whereas a contingent link is exogenously assigned outside of the control
of the agent to some value ij  [bji , bij ]. An STPU is checked not only for consistency, but
also for various classes of controllability , including strong, weak, and dynamic variants
(Vidal, 2000; Morris, Muscettola, & Vidal, 2001; Morris & Muscettola, 2005; Lau, Li, &
Yap, 2006; Hunsberger, 2009). Whereas a consistent STP is one where there exist schedules
that satisfy all constraints, a controllable STPU is one where there exist satisfying schedules
regardless of how the contingent links are assigned. Typical strategies for establishing or
maintaining controllability include preemptively constraining requirement timepoints so
that the remaining values are consistent with all possible contingencies. Our work does not
explicitly differentiate between which constraints can be exogenously updated and which
cannot. We instead use the space of all solutions to hedge against dynamic constraints.
However, as a nice parallel to the STPU literature, our MaTD algorithm can be viewed as a
negotiation in which agents preemptively add new decoupling constraints to control against
the uncertainty and contingencies introduced by their interactions with other agents.
6.2 Applications of Multiagent Scheduling
Temporal constraint networks represent spaces of feasible schedules as compact intervals of
time that can be calculated efficiently. For this reason, a temporal network naturally lends
itself to online plan monitoring and dispatchable executionan online approach whereby a
dispatcher uses the flexible times representation to efficiently adapt to scheduling upheavals
by introducing new constraints. The plan monitor can flag when a current schedule has
become infeasible, and a dispatcher can notify agents of the time it assigns to variables
immediately prior to execution (Muscettola, Morris, & Tsamardinos, 1998). Another contribution of our work is to show that these advantages extend to distributed, multiagent
temporal constraint networks, while also introducing a level of independence that agents
can exploit in many ways. Properties of temporal networks such as minimality and decomposability have proven essential in representing the solution space for many centralized
applications such as project scheduling (Cesta, Oddi, & Smith, 2002), medical informatics
(Anselma, Terenziani, Montani, & Bottrighi, 2006), air traffic control (Buzing & Witteveen,
2004), and spacecraft control (Fukunaga, Rabideau, Chien, & Yan, 1997). Unfortunately,
multiagent scheduling applications (Laborie & Ghallab, 1995; Bresina et al., 2005; Castillo
et al., 2006; Smith et al., 2007; Barbulescu et al., 2010) wishing to exploit these properties
have previously relied on either a centralized temporal network representation or, if independence was also needed, completely disjoint, separate agent networks. We now highlight
a few specific example applications that may benefit from the work described in this paper.
The approach originally described by Smith et al. (2007) and extended by Barbulescu
et al. (2010) exploits the flexibility of STNs in a distributed framework. The general framework of the problems they solve contains models of uncertainty over both the durations
and utilities of different activities. Using a greedy heuristic, their scheduler selects a set of
145

fiBoerkoel & Durfee

activities that would maximize agent utility, and extracts duration bounds from the distribution of possible durations for each activity, thus creating an STP for each local agent.
Each agent captures the current state of its local problem in the form of an STN representation of its local solution space, which the agent uses to help hedge against uncertainty. An
agent maintains the current state of the problem as new constraints arise by using efficient,
incremental STN consistency algorithms (e.g., Cesta & Oddi, 1996; Planken, de Weerdt, &
Yorke-Smith, 2010b). Each agent maintains its local STN problem representation until an
improved replacement STN is identified by the scheduler mechanism. This efficient statemaintenance strategy frees agents to spend a greater portion of time exploring alternative
allocations and schedulings of activities between agents. Barbulescu et al.s approach divides
the problem into separate, localized STP instances, requiring a distributed state manager
to react to and communicate local scheduling changes that may affect other agents. To
deal with the challenge of coordination, Barbulescu et al. establish an acceptable time, ,
within which interdependent activities between agents are considered synchronized. As long
as an agent can execute its activities within the prescribed s, it can assume consistency
with other agents. The risk of inconsistencies between agents is mitigated by (i) restricting
synchronization scheduling to a limited time horizon and (ii) allowing agents to abandon a
synchronization as soon as it is determined to be unrealizable.
Shah and Williams (2008), Shah, Conrad, and Williams (2009) and Conrad and Williams
(2011) generalize this idea to multiagent, disjunctive scheduling problems by calculating dispatchable representations. Shah and Williams (2008) recognize that many of the disjunctive scheduling possibilities contain significant redundancy, and so gain representational
efficiency by reusing the redundant portions of existing solution representations as much as
possible. The algorithm propagates each disjunct using a recursive, incremental constraint
compilation technique called dynamic back propagation, keeping a list of the implications
of the disjunct to the temporal network. This centralized compilation leads to not only a
much more compact representation than the previous approach (Tsamardinos, Pollack, &
Ganchev, 2001), but also faster plan monitoring by avoiding the need to simultaneously and
separately update each disparate STP.
Each of these domains offer examples of work that could benefit by putting our approach
into practice. Representing the joint solution space as a multiagent temporal network could
instead offer an agent a more complete view of available scheduling possibilities as well as
an increased understanding of how its problem impacts (or is impacted by) other agents
problems. Further, directly representing and reasoning over the interacting scheduling problem of multiple agents also eliminates the need for agents to execute separate threads of
execution to monitor and communicate state changes. Finally, directly implementing a
multiagent temporal network allows agents to more flexibly and directly strike a trade-off
between representing the complete joint solution space and internalizing decoupling constraints in a just-in-time manner (e.g., using the time-horizon concept of Barbulescu et al.,
2010), rather than having to rely on additional mechanisms manage and coordinate state.
6.3 Distributed Finite-Domain Constraint Reasoning
The Distributed Constraint Satisfaction Problem (DCSP) is a distributed constraint-based
problem formulation where all variables have a discrete, finite domain, rather than temporal,
146

fiDistributed Reasoning for Multiagent Simple Temporal Problems

continuous domain. Two seminal algorithms for solving the DCSP are the Asynchronous
Backtracking (ABT) and the Asynchronous Weak-Commitment (AWC) search algorithms
(Yokoo, Durfee, Ishida, & Kuwabara, 1998). ABT, AWC, and their variants are algorithms
based on asynchronous variable assignment, and use message passing of no-goods to resolve
conflicts. Armstrong and Durfee (1997), Hirayama, Yokoo, and Sycara (2004), and Silaghi
and Faltings (2005) provide variants of these algorithms that perform more intelligent prioritization of agents, order local agent variables dynamically, and aggregate exchanges of
information for agents with complex local problems. However, approaches based on assignment and no-good learning may be less applicable to continuous-domain, constraint-based
scheduling problems, where typical strategies focus on maintaining consistent sets of solutions using inference. Mailler and Lessers Asynchronous Partial Overlay (APO) algorithm
(Mailler & Lesser, 2006) deals with inconsistencies between agents through mediation, which
over time centralizes the view of the problem, and thus may conflict with the privacy goals of
our work. Asynchronous Forward Checking (AFC), proposed and evaluated by Meisels and
Zivan (2007), provides mechanisms for asynchronously increasing consistency across agents;
however the pay-off of this algorithmavoiding expensive backtracking operationsis not
a challenge faced by our MaSTP algorithms.
A Distributed Constraint Optimization Problem (DCOP) is a generalization of the
DCSP with more general utility or cost functions (soft constraints) that assign a utility
or cost to every possible combination of assignments to a particular subset of variables, replacing the set of hard constraints. A DCSP can be translated into a DCOP by converting
constraints into cost functions with infinite cost for inconsistent assignments. As before,
each utility function is known by at least one agent, and a DCOP is solved by finding the
assignment of values to variables that maximizes total utility (or minimizes total cost).
ADOPT (Modi, Shen, Tambe, & Yokoo, 2005) and BnB-ADOPT (Yeoh, Felner, & Koenig,
2010) are both decentralized, complete search algorithms for solving a DCOP using bestfirst and branch-and-bound depth-first principles respectively. The OptAPO algorithm is
an optimization variant of the APO algorithm by Mailler and Lesser (2004). ADOPT and
OptAPO are generalizations of AWC and APO respectively, though in each case, instead
of terminating after the first feasible assignment of values to variables, agents must exhaust
the entire search space to guarantee the assignment they return is the optimal one. The
DPOP algorithm (Petcu & Faltings, 2005), which is also a distributed implementation of
the more general bucket-elimination algorithm for solving DCOPs, is probably the most
similar to our approach. While DPOP requires only a linear number of messages to solve
a DCOP, it suffers from exponentially (in the induced width of the constraint graph) large
message sizes. Our approach, on the other hand, can exploit the relatively simple temporal
constraints to compactly encode the impact of eliminated variables using binary constraints.
6.4 Multiagent Planning
The term planning encompasses many specific problem domains including classical (Fikes
& Nilsson, 1972), Hierarchical Task Networks (Erol, Hendler, & Nau, 1994), and MDPbased planning (Bellman, 1966; Sutton & Barto, 1998). At a high level, planning involves
developing policies or (partially-ordered) sequences of actions that (provably or probabilistically) evolve the state of the world in a way that achieves a set of goals or optimizes
147

fiBoerkoel & Durfee

some objective function. For many types of planning problems, the plan or policy must
also consider types of uncertainty not typically found in scheduling (e.g., uncertainty over
observations, effects of actions, etc.). In comparison, in our work, the events that are to
be scheduled have already been determined and are taken as input. The output, instead
of some sort of general policy or (partial) sequence of actions, is a specification of how
times can be assigned to timepoint variables to satisfy the temporal constraints, where all
such schedules are considered equal. Planning and scheduling, while interrelated (Myers
& Smith, 1999; Garrido & Barber, 2001; Halsey, Long, & Fox, 2004), are often treated
as separate subproblems (e.g., McVey, Atkins, Durfee, & Shin, 1997). Smith, Frank, and
Jonsson (2000), who give a more complete comparison of planning and scheduling, suggest
the difference [between planning and scheduling] is a subtle one: scheduling problems only
involve a small, fixed set of choices, while planning problems often involve cascading sets of
choices that interact in complex ways.
There are many planning approaches, particularly multiagent planning (for a more
complete introduction, see desJardins, Durfee, Ortiz Jr, & Wolverton, 1999; de Weerdt
& Clement, 2009) or decentralized planning (e.g., desJardins & Wolverton, 1999; Bernstein, Zilberstein, & Immerman, 2000) that relate to and inspire our approach for solving
multiagent scheduling problems. First, there is long history of exploiting loosely-coupled
structure in multiagent planning; Witwicki and Durfee (2010) offer one recent example.
Second, a main challenge in multiagent planninghow to interleave local planning and interagent coordinationis also apt for multiagent scheduling problems. In planning, there
have been approaches where agents develop local plans and then work to integrate the plans
(e.g., Georgeff, 1983), approaches that work out interdependencies between agents first and
then build local plans to fit these commitments (e.g., ter Mors, Valk, & Witteveen, 2004),
and approaches that blur this dichotomy by interleaving planning and coordination (e.g.,
Clement, Durfee, & Barrett, 2007 does this by establishing multiple levels of abstraction).
Third, both planning and scheduling involve ordering events and checking for cycles (to
ensure goals/conditions are not clobbered in planning and to ensure consistency in scheduling), which is particularly challenging when cycles are potentially distributed across multiple
agents (Cox, Durfee, & Bartold, 2005).
6.5 Resource and Task Allocation Problems
Allocating tasks or resources to multiple agents has been studied in a variety of settings (e.g.,
Goldberg, Cicirello, Dias, Simmons, Smith, & Stentz, 2003; Nair, Ito, Tambe, & Marsella,
2002; Sandholm, 1993; Simmons, Apfelbaum, Fox, Goldman, Haigh, Musliner, Pelican,
& Thrun, 2000; Wellman, Walsh, Wurman, & MacKie-Mason, 2001; Zlot & Stentz, 2006).
Typically these problems involve either assigning a set of tasks to a limited number of agents
that can perform them or, alternatively, assigning scarce resources to agents that require
these resources. A common approach for handling such task allocation is a market-based
approach (e.g., Nair et al., 2002), where agents place bids on tasks (or subsets of tasks
in combinatorial auctions) according to their calculated costs for performing the tasks,
with tasks then being assigned to the lowest bidder. Other market-based approaches allow
agents to locally exchange tasks in order to quickly respond to a dynamic environment (e.g.,
Sandholm, 1993). While more recent approaches (Goldberg et al., 2003; Zlot & Stentz, 2006)
148

fiDistributed Reasoning for Multiagent Simple Temporal Problems

allow agents to negotiate at various levels of task abstraction/decomposition, the primary
temporal reasoning occurs within an agent, which uses scheduling information to estimate
costs for its bid. Similarly, before bidding on them, agents can append temporal constraints
to tasks, such as time-windows, to help capture/enforce relevant precedence constraints
between tasks of different agents.
Our work assumes that the task and resource allocation problems have already been
solved, or, if necessary, constraints are in place to prevent concurrent, overlapping use
of a resource or duplication of a redundant activity. Additionally, whereas task/resource
allocation is cast as an assignment problem, our work deals largely with reasoning over
bounds so as to support flexible times representations. Finally as noted in greater detail by
Zlot and Stentz (2006), while optimal, centralized approaches for solving this problem exist,
the NP-hard nature of the problem, coupled with the uncertain or dynamic environment,
leads to most recent approaches being both distributed as well as heuristic or approximate
in nature. In contrast, our work assumes deterministic and complete approaches, but does
not explicitly model the relative costs or values of various schedules. Instead, our agents
provide their users with the autonomy to make their own cost/value judgments, and in
turn, provide advice about the implications of their scheduling decisions.
6.6 Operations Research
Our MaSTP representations are capable of representing a particular class of scheduling
problems. The Operations Research (OR) community is also interested in solving (often
NP-hard) scheduling problems. In her overview comparison of the two fields, Gomes (2000,
2001) classifies OR problems as optimization problems, where the utility function tends to
play an important role. Additionally, Gomes notes OR tends to represent problems using
mathematical modeling languages and linear and non-linear inequalities, and uses tools
such as linear programming, mixed-integer linear programming, and non-linear models.
Typically, OR approaches gain traction by exploiting problem structuring. For example
approaches such as timetabling or edge-finding (Laborie, 2003) are both OR techniques
for tightening the bounds over when possible activities can occur. While a full review of
the many OR models (e.g., Traveling Salesperson Problem, Job Shop Scheduling Problem,
Resource Constrained Project Scheduling Problem, Timetabling, etc.) is beyond the scope
of this paper, it is worth pointing out that synergy between the AI scheduling and OR
communities is growing (Baptiste, Le Pape, & Nuijten, 1995; Bartak, 1999; Laborie, 2003;
Baptiste, Laborie, Le Pape, & Nuijten, 2006; Oddi, Rasconi, & Cesta, 2010).

7. Conclusion
The work we presented in this paper builds foundational algorithms for scheduling agents
that assist people in managing their activities, despite distributed information, implicit
constraints, costs in sharing information among agents (e.g., delays, privacy, autonomy,
etc.), and the possibility of new constraints dynamically arising. Agents flexibly combine
shared reasoning about interactions between their schedules with independent reasoning
about their local problems. They do so by externalizing constraints that compactly summarize how their local subproblems affect each other, and internalizing new local constraints
that decouple their problems from one another. This approach is most advantageous for
149

fiBoerkoel & Durfee

problems where interactions between the agents are sparse compared to the complexity of
agents individual scheduling problems, where our algorithms achieve significant computational speedup over the current art.
More particularly, we contributed a formal definition of the Multiagent Simple Temporal Problem and analyzed the benefits that a distributed representation affords. We
presented the D4PPC algorithm, which is the first distributed algorithm for calculating
a decentralized representation of the complete space of joint solutions, and proved that it
correctly establishes partial-path consistency with a worst-case runtime complexity that is
equal to the previous state-of-the-art. We empirically demonstrated that D4PPC scales
to problems containing more agents better than its state-of-the-art centralized counterpart,
and exhibits a steady margin of speedup. Exactly how effectively it load-balances computational effort depends on the number of external constraints coupling agents problems,
where D4PPC achieves up to a 22-fold reduction in nonconcurrent computational effort
over 4PPC for problems with 25 agents in the problems we studied. Additionally, we formally defined the Multiagent Temporal Decoupling Problem along with the first distributed
algorithm for solving it. We proved that the MaTD algorithm is correct, and demonstrated
both analytically and empirically that it calculates a temporal decoupling upwards of four
orders-of-magnitude faster than previous approaches, exploiting sparse structure and parallelism when it exists. Additionally, we introduced the Relaxation algorithm for relaxing
the bounds of existing decoupling constraints to form a minimal temporal decoupling, and
empirically showed that this algorithm can decrease rigidity by upwards of 50% (within 1521% of the state-of-the-art centralized approach) while increasing computational effort by
as little as 20% in the problems we studied. Overall, we have shown that the combination
of the MaTD and MaTDR algorithms can calculate a temporal decoupling faster than the
D4PPC algorithm, and four orders-of-magnitude faster than the previous state-of-the-art
centralized TDP algorithm.
This work leads to many interesting ongoing and future research directions. First, the
use of an MaSTN to monitor plan execution and dispatch scheduling advice to users in
a distributed manner (as was done for centralized dispatching in, e.g., Drake, see Conrad
& Williams, 2011) will rely on an ability to efficiently maintain MaSTNs (Boerkoel et al.,
2013) and a on comprehensive empirical understanding of the inherent trade-offs between
the costs of maintaining PPC versus the loss of flexibility in temporal decouplings. Second,
the MaSTP formulation could be adapted to include models of uncertainty or utility, as
it has for the centralized STP (Rossi, Venable, & Yorke-Smith, 2006). Optimally solving
the temporal decoupling problem for general models of utility is an NP-hard problem,
even in the centralized case (Planken, de Weerdt, & Witteveen, 2010a). Thus, designing
mechanisms that can augment our distributed temporal decoupling algorithm and encourage
agents to solve the MaTDP in both a distributed and globally optimal (rather than just
locally minimal) manner is an open challenge that spans both multiagent scheduling and
algorithmic game theory. Finally, the ideas presented here for MaSTP can be extended
to more-general, disjunctive scheduling problems (Boerkoel & Durfee, 2012, 2013). The
combinatorics of disjunctive problems leads to significant challengesfinding solutions is
NP-hard, and the structure of the underlying temporal network, which for the inherently
binary MaSTP is known a priori, can become conflated or tangled by the combinatorial
number of possible disjunctive choices.
150

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Acknowledgments
This work was supported, in part, by the NSF under grants IIS-0534280 and IIS-0964512,
the AFOSR under Contract No. FA9550-07-1-0262, and the 2011 Rackham Predoctoral
Fellowship. We would like to thank Leon Planken, Michael Wellman, Martha Pollack, Amy
Cohn, Stefan Witwicki, Jason Sleight, Elizabeth Boerkoel, and Julie Shah for their helpful
suggestions.

References
Anselma, L., Terenziani, P., Montani, S., & Bottrighi, A. (2006). Towards a comprehensive
treatment of repetitions, periodicity and temporal constraints in clinical guidelines.
Artificial Intelligence in Medicine, 38 (2), 171195.
Armstrong, A., & Durfee, E. (1997). Dynamic prioritization of complex agents in distributed
constraint satisfaction problems. In Proceedings of International Joint Conference on
Artificial Intelligence (IJCAI-97), pp. 620625.
Baptiste, P., Laborie, P., Le Pape, C., & Nuijten, W. (2006). Constraint-based scheduling
and planning. Foundations of Artificial Intelligence, 2, 761799.
Baptiste, P., Le Pape, C., & Nuijten, W. (1995). Incorporating efficient operations research
algorithms in constraint-based scheduling. In Proceedings of the First International
Joint Workshop on Artificial Intelligence and Operations Research.
Barbulescu, L., Rubinstein, Z. B., Smith, S. F., & Zimmerman, T. L. (2010). Distributed
coordination of mobile agent teams: The advantage of planning ahead. In Proceedings
of the Ninth International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2010), pp. 13311338.
Bartak, R. (1999). Constraint programming: In pursuit of the holy grail. In Proceedings of
the Week of Doctoral Students (WDS99 -invited lecture), pp. 555564.
Bellman, R. (1966). Dynamic programming. Science, 153 (3731), 3437.
Bernstein, D., Zilberstein, S., & Immerman, N. (2000). The complexity of decentralized
control of Markov decision processes. In Proceedings of the Sixteenth Conference on
Uncertainty in Artificial Intelligence, pp. 3237.
Bliek, C., & Sam-Haroud, D. (1999). Path consistency on triangulated constraint graphs. In
Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI99), Vol. 16, pp. 456461.
Boerkoel, J. (2012). Distributed Approaches for Solving Constraint-based Multiagent
Scheduling Problems. Ph.D. thesis, University of Michigan.
Boerkoel, J., & Durfee, E. (2009). Evaluating hybrid constraint tightening for scheduling
agents. In Proceedings of The Eighth International Conference on Autonomous Agents
and Multiagent Systems (AAMAS 2009), pp. 673680.
Boerkoel, J., & Durfee, E. (2010). A comparison of algorithms for solving the multiagent
simple temporal problem. In Proceedings of the Twentieth International Conference
on Automated Planning and Scheduling (ICAPS 2010), pp. 2633.
151

fiBoerkoel & Durfee

Boerkoel, J., & Durfee, E. (2011). Distributed algorithms for solving the multiagent temporal decoupling problem. In Proceedings of the Tenth International Conference on
Autonomous Agents and Multiagent Systems (AAMAS 2011), pp. 141148.
Boerkoel, J., & Durfee, E. (2012). A distributed approach to summarizing spaces of multiagent schedules. In Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12), 17421748.
Boerkoel, J., & Durfee, E. (2013). Decoupling the multiagent disjunctive temporal problem.
In Proceedings of the Twenty-Seventh Conference on Artificial Intelligence (AAAI-13),
To appear.
Boerkoel, J., Planken, L., Wilcox, R., & Shah, J. (2013). Distributed algorithms for
incrementally maintaining multiagent simple temporal networks. Proceedings of
the Twenty-Third International Conference on Automated Planning and Scheduling
(ICAPS 2013), To appear.
Bresina, J., Jonsson, A. K., Morris, P., & Rajan, K. (2005). Activity planning for the
Mars exploration rovers. In Proceedings of the Fifteenth International Conference on
Automated Planning and Scheduling (ICAPS 2005), pp. 4049.
Buzing, P., & Witteveen, C. (2004). Distributed (re) planning with preference information. In Proceedings of the Sixteenth Belgium-Netherlands Conference on Artificial
Intelligence, pp. 155162.
Castillo, L., Fernandez-Olivares, J., & O. Garca-Perez, F. P. (2006). Efficiently handling
temporal knowledge in an HTN planner. In Proceedings of the Sixteenth International
Conference on Automated Planning and Scheduling (ICAPS 2006), pp. 6372.
Cesta, A., & Oddi, A. (1996). Gaining efficiency and flexibility in the simple temporal problem. In Proceedings of the 3rd Workshop on Temporal Representation and Reasoning
(TIME96), pp. 4550.
Cesta, A., Oddi, A., & Smith, S. (2002). A constraint-based method for project scheduling
with time windows. Journal of Heuristics, 8 (1), 109136.
Clement, B., Durfee, E., & Barrett, A. (2007). Abstract reasoning for planning and coordination. Journal of Artificial Intelligence Research, 28 (1), 453515.
Conrad, P., & Williams, B. (2011). Drake: An efficient executive for temporal plans with
choice. Journal of Artificial Intelligence Research, 42 (1), 607659.
Cox, J., Durfee, E., & Bartold, T. (2005). A distributed framework for solving the multiagent plan coordination problem. In Proceedings of the Fourth International Joint
Conference on Autonomous Agents and Multiagent Systems (AAMAS 2005), pp. 821
827.
de Weerdt, M., & Clement, B. (2009). Introduction to planning in multiagent systems.
Multiagent and Grid Systems, 5 (4), 345355.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. In Knowledge
Representation, Vol. 49, pp. 6195.
152

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Dechter, R., & Pearl, J. (1987). Network-based heuristics for constraint-satisfaction problems. Artificial Intelligence, 34 (1), 138.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1-2), 4185.
desJardins, M., & Wolverton, M. (1999). Coordinating a distributed planning system. AI
Magazine, 20 (4), 4553.
desJardins, M. E., Durfee, E. H., Ortiz Jr, C. L., & Wolverton, M. J. (1999). A survey of
research in distributed, continual planning. AI Magazine, 20 (4), 1322.
Erol, K., Hendler, J., & Nau, D. S. (1994). Semantics for hierarchical task-network planning.
Tech. rep., University of Maryland at College Park, College Park, MD, USA.
Fikes, R., & Nilsson, N. (1972). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial intelligence, 2 (3-4), 189208.
Floyd, R. (1962). Algorithm 97: Shortest path. Communications of the ACM, 5 (6), 345.
Fukunaga, A., Rabideau, G., Chien, S., & Yan, D. (1997). Aspen: A framework for automated planning and scheduling of spacecraft control and operations. In Proceedings
International Symposium on AI, Robotics and Automation in Space.
Garrido, A., & Barber, F. (2001). Integrating planning and scheduling. Applied Artificial
Intelligence, 15 (5), 471491.
Georgeff, M. (1983). Communication and interaction in multi-agent planning. In Proceedings
of the Third National Conference on Artificial Intelligence (AAAI-83), pp. 125129.
Goldberg, D., Cicirello, V., Dias, M., Simmons, R., Smith, S., & Stentz, A. (2003). Marketbased multi-robot planning in a distributed layered architecture. In Multi-Robot Systems: From Swarms to Intelligent Automata: Proceedings from the 2003 International
Workshop on Multi-Robot Systems, Vol. 2, pp. 2738.
Golumbic, M. (1980). Algorithmic graph theory and perfect graphs. Academic Press.
Gomes, C. (2000). Artificial intelligence and operations research: Challenges and opportunities in planning and scheduling. The Knowledge Engineering Review, 15 (1), 110.
Gomes, C. (2001). On the intersection of AI and OR. The Knowledge Engineering Review,
16 (1), 14.
Halsey, K., Long, D., & Fox, M. (2004). Crikey-a temporal planner looking at the integration of scheduling and planning. In ICAPS Workshop on Integrating Planning into
Scheduling, pp. 4652.
Hirayama, K., Yokoo, M., & Sycara, K. (2004). An easy-hard-easy cost profile in distributed
constraint satisfaction. Transactions of Information Processing Society of Japan, 45,
22172225.
Hunsberger, L. (2002). Algorithms for a temporal decoupling problem in multi-agent planning. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
(AAAI-02), pp. 468475.
Hunsberger, L. (2009). Fixing the semantics for dynamic controllability and providing a
more practical characterization of dynamic execution strategies. In Procedings of 16th
153

fiBoerkoel & Durfee

International Symposium on Temporal Representation and Reasoning(TIME 2009),
pp. 155162.
Kjaerulff, U. (1990). Triangulation of graphs - algorithms giving small total state space.
Tech. rep., Aalborg University.
Laborie, P. (2003). Algorithms for propagating resource constraints in AI planning and
scheduling: Existing approaches and new results. Artificial Intelligence, 143 (2), 151
188.
Laborie, P., & Ghallab, M. (1995). Planning with sharable resource constraints. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI-95), pp. 16431649.
Lau, H. C., Li, J., & Yap, R. H. (2006). Robust controllability of temporal constraint
networks under uncertainty. In Tools with Artificial Intelligence, 2006. ICTAI06.
18th IEEE International Conference on, pp. 288296.
Mailler, R., & Lesser, V. (2006). Asynchronous partial overlay: A new algorithm for solving
distributed constraint satisfaction problems. Journal of Artificial Intelligence Research, 25 (1), 529576.
Mailler, R., & Lesser, V. (2004). Solving Distributed Constraint Optimization Problems
Using Cooperative Mediation. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2004), pp. 438445.
McVey, C., Atkins, E., Durfee, E., & Shin, K. (1997). Development of iterative real-time
scheduler to planner feedback. In Proceedings of the Fifteenth International Joint
Conference on Artificial Intelligence (IJCAI-97), pp. 12671272.
Meisels, A., & Zivan, R. (2007). Asynchronous forward-checking for DisCSPs. Constraints,
12 (1), 131150.
Modi, P., Shen, W., Tambe, M., & Yokoo, M. (2005). Adopt: Asynchronous distributed
constraint optimization with quality guarantees. Artificial Intelligence, 161 (1-2), 149
180.
Morris, P., & Muscettola, N. (2005). Temporal dynamic controllability revisited. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05),
pp. 11931198.
Morris, P., Muscettola, N., & Vidal, T. (2001). Dynamic control of plans with temporal
uncertainty. In International Joint Conference on Artificial Intelligence (IJCAI-01),
pp. 494502.
Muscettola, N., Morris, P., & Tsamardinos, I. (1998). Reformulating temporal plans for
efficient execution. In Proceedings of the International Conference on Principles of
Knowledge Representation and Reasoning (KR 1998), pp. 444452.
Myers, K., & Smith, S. (1999). Issues in the integration of planning and scheduling for
enterprise control. Proceedings of the DARPA Symposium on Advances in Enterprise
Control., 217223.
154

fiDistributed Reasoning for Multiagent Simple Temporal Problems

Nair, R., Ito, T., Tambe, M., & Marsella, S. (2002). Task allocation in the robocup rescue
simulation domain: A short note. In Proceedings of the RoboCup 2001: Robot Soccer
World Cup V, pp. 751754.
Oddi, A., Rasconi, R., & Cesta, A. (2010). Casting project scheduling with time windows as
a DTP. In Proceedings of the ICAPS Workshop on Constraint Satisfaction Techniques
for Planning and Scheduling Problems (COPLAS 2010), pp. 4249.
Petcu, A., & Faltings, B. (2005). A scalable method for multiagent constraint optimization.
In International Joint Conference on Artificial Intelligence (IJCAI-05), Vol. 19, pp.
266271.
Planken, L. (2008). Incrementally solving the STP by enforcing partial path consistency.
In Proceedings of the Twenty-seventh Workshop of the UK Planning and Scheduling
Special Interest Group (PlanSIG 2008), pp. 8794.
Planken, L., de Weerdt, M., & van der Krogt, R. (2008). P3 C: A new algorithm for the
simple temporal problem. In Proceedings of the Eighteenth International Conference
on Automated Planning and Scheduling (ICAPS 2008), pp. 256263.
Planken, L. R., de Weerdt, M. M., & Witteveen, C. (2010a). Optimal temporal decoupling
in multiagent systems. In Proceedings of the Ninth International Joint Conference on
Autonomous Agents and Multiagent Systems (AAMAS 2010), pp. 789796.
Planken, L. R., de Weerdt, M. M., & Yorke-Smith, N. (2010b). Incrementally solving STNs
by enforcing partial path consistency. In Proceedings of the Twentieth International
Conference on Automated Planning and Scheduling (ICAPS 2010), pp. 129136.
Rossi, F., Venable, K., & Yorke-Smith, N. (2006). Uncertainty in soft temporal constraint
problems: A general framework and controllability algorithms for the fuzzy case. Journal of Artificial Intelligence Research, 27 (1), 617674.
Sandholm, T. (1993). An implementation of the contract net protocol based on marginal
cost calculations. In Proceedings of the National Conference on Artificial Intelligence
(AAAI-93), pp. 256256.
Shah, J., & Williams, B. (2008). Fast dynamic scheduling of disjunctive temporal constraint networks through incremental compilation. In Proceedings of the Eighteenth
International Conference on Automated Planning and Scheduling (ICAPS 2008), pp.
322329.
Shah, J. A., Conrad, P. R., & Williams, B. C. (2009). Fast distributed multi-agent plan execution with dynamic task assignment and scheduling. In Proceedings of the Nineteenth
International Conference on Automated Planning and Scheduling (ICAPS 2009), pp.
289296.
Silaghi, M., & Faltings, B. (2005). Asynchronous aggregation and consistency in distributed
constraint satisfaction. Artificial Intelligence, 161 (1-2), 2553.
Simmons, R., Apfelbaum, D., Fox, D., Goldman, R., Haigh, K., Musliner, D., Pelican, M., &
Thrun, S. (2000). Coordinated deployment of multiple, heterogeneous robots. In Proceedings of the International Conference on Intelligent Robots and Systems (IROS),
pp. 22542260.
155

fiBoerkoel & Durfee

Smith, D., Frank, J., & Jonsson, A. (2000). Bridging the gap between planning and scheduling. The Knowledge Engineering Review, 15 (1), 4783.
Smith, S., Gallagher, A., Zimmerman, T., Barbulescu, L., & Rubinstein, Z. (2007). Distributed management of flexible times schedules. In Proceedings of the Sixth International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2007),
pp. 472479.
Sutton, R., & Barto, A. (1998). Reinforcement learning: An introduction, Vol. 1. Cambridge
Univ Press.
ter Mors, A. W., Valk, J. M., & Witteveen, C. (2004). Coordinating autonomous planners.
In Proceedings of the International Conference on Artificial Intelligence (ICAI04),
pp. 795801.
Tsamardinos, I., Pollack, M., & Ganchev, P. (2001). Flexible dispatch of disjunctive plans.
In Proceedings of the Sixth European Conference in Planning (ECP-06), pp. 417422.
Vidal, T. (2000). Controllability characterization and checking in contingent temporal
constraint networks. In Principles of Knowledge Representation and ReasoningInternational Conference, pp. 559570.
Vidal, T., & Fargier, H. (1999). Handling contingency in temporal constraint networks:
From consistency to controllabilities. Journal of Experimental & Theoretical Artificial
Intelligence, 11 (1), 2345.
Vidal, T., & Ghallab, M. (1996). Dealing with uncertain durations in temporal constraint
networks dedicated to planning. In Proceedings of the Twelfth European Conference
on Artificial Intelligence (ECAI-96), pp. 4854.
Wellman, M., Walsh, W., Wurman, P., & MacKie-Mason, J. (2001). Auction protocols for
decentralized scheduling. Games and Economic Behavior, 35 (1/2), 271303.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction for weaklycoupled Dec-POMDPs. In Proceedings of the Twentieth International Conference on
Automated Planning and Scheduling (ICAPS 2010), pp. 185192.
Xu, L., & Choueiry, B. (2003). A new efficient algorithm for solving the simple temporal
problem. In Proceedings of the Tenth International Symposium on Temporal Representation and Reasoning, 2003 and Fourth International Conference on Temporal
Logic (TIME-ICTL 03), pp. 212222.
Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: An asynchronous branch-andbound DCOP algorithm. Journal of Artificial Intelligence Research, 38, 85133.
Yokoo, M., Durfee, E., Ishida, T., & Kuwabara, K. (1998). The distributed constraint
satisfaction problem: Formalization and algorithms. IEEE Transactions on Knowledge
and Data Engineering, 10 (5), 673685.
Zlot, R., & Stentz, A. (2006). Market-based multirobot coordination for complex tasks.
The International Journal of Robotics Research, 25 (1), 73101.

156

fiJournal of Artificial Intelligence Research 47 (2013) 35-70

Submitted 12/12; published 05/13

Scheduling a Dynamic Aircraft Repair Shop
with Limited Repair Resources
Maliheh Aramon Bajestani
J. Christopher Beck

maramon@mie.utoronto.ca
jcb@mie.utoronto.ca

Department of Mechanical & Industrial Engineering
University of Toronto, Canada

Abstract
We address a dynamic repair shop scheduling problem in the context of military aircraft
fleet management where the goal is to maintain a full complement of aircraft over the longterm. A number of flights, each with a requirement for a specific number and type of
aircraft, are already scheduled over a long horizon. We need to assign aircraft to flights
and schedule repair activities while considering the flights requirements, repair capacity,
and aircraft failures. The number of aircraft awaiting repair dynamically changes over
time due to failures and it is therefore necessary to rebuild the repair schedule online. To
solve the problem, we view the dynamic repair shop as successive static repair scheduling
sub-problems over shorter time periods. We propose a complete approach based on the
logic-based Benders decomposition to solve the static sub-problems, and design different
rescheduling policies to schedule the dynamic repair shop. Computational experiments
demonstrate that the Benders model is able to find and prove optimal solutions on average
four times faster than a mixed integer programming model. The rescheduling approach
having both aspects of scheduling over a longer horizon and quickly adjusting the schedule
increases aircraft available in the long term by 10% compared to the approaches having
either one of the aspects alone.

1. Introduction
The United States Air Force website outlines readiness as one of the vital elements to ensure
the operational effectiveness in defense strategies (Schwartz, 2012). Deficiency in resourcing
for operations and maintenance is stated as the common culprit for poor readiness. Since
air forces are budget-constrained and highly dynamic environments, the optimal allocation
of resources to activities to maintain readiness at an appropriate level is challenging. In this
paper, we study a problem in the context of military aircraft repair shop where readiness is
defined as the ability to effectively carry out the pre-scheduled missions. We need to decide
when each failed aircraft should be repaired to guarantee the availability of aircraft at high
and steady level. However, high frequency of unexpected failures in military aircraft (Safaei,
Banjevic, & Jardine, 2011) and limited repair resources such as workforce, tools, and space
(Kozanidis, Gavranis, & Kostarelou, 2012) constrain consistent aircraft availability.
Motivated by the work of Safaei et al. (2010, 2011), we study the problem of scheduling
a military aircraft repair shop, where a number of flights are planned over a long horizon.
Every flight, also called a wave, has a maximum requirement for a specific number of aircraft
of different types though it can be partially carried out without its maximum complement.
At the beginning of the time horizon, an aircraft is either ready for a pre-flight check or is
c
2013
AI Access Foundation. All rights reserved.

fiAramon Bajestani & Beck

awaiting repair in the repair shop. Aircraft flow over a long horizon is illustrated in Figure 1.
The goal is to determine an assignment of aircraft to waves and a schedule of repair jobs
that will maximize flight coverage, that is, the extent to which the aircraft requirements
of the flights are met. When an aircraft fails during a pre- or post-flight check, it enters
the repair shop to be incorporated into the current repair schedule. Each aircraft failure
requires a set of independent repair activities with known characteristics such as processing
times and resource requirements to be scheduled on repair resources with a limited capacity.

Allocated to Flight

No

Pre-Flight Check

Failure

Do the Flight

Post-Flight Check

Yes
Yes

Repair Shop

Failure

No

Figure 1: Aircraft flow among waves, checks, and the repair shop over a long horizon.
The central idea of our solution approach is to view the dynamic repair shop as successive static sub-problems over shorter time periods. A solution of the static sub-problem
determines an assignment of aircraft to flights and a schedule of repair jobs maximizing
the flight coverage. When a failed aircraft enters the repair shop while the previous repair
schedule is still under execution, we reschedule the repair activities by solving a new static
sub-problem.
After proving the NP-hardness of the static sub-problem, we explore several techniques
to solve the problem: mixed integer programming (MIP); constraint programming (CP);
logic-based Benders decomposition (LBBD) using either MIP or CP; and a dispatching
heuristic motivated by the Apparent Tardiness Cost (ATC) dispatching rule. We then
design three different rescheduling policies based on the length of the scheduling horizon
and how frequently rescheduling is done.
We perform two separate empirical studies. The first indicates that the integration of
the dispatching heuristic and LBBD results in the lowest mean run-time of the techniques
tested to optimally schedule the repair shop. The second experiment demonstrates that
both defining the static scheduling problem over a longer horizon and rescheduling more
frequently provide the flights with 10% higher coverage than either one of them alone.
The remainder of the paper is organized as follows: We define the problem, and provide
an overview of the relevant literature in Section 2. Section 3 proves the NP-hardness of the
static sub-problem, defines a number of solution approaches for it, presents the details of
the proposed policies for rescheduling the dynamic repair shop, and describes our model of
the aircraft failures. The computational results on the performance of different scheduling
techniques and on how and when rescheduling should be done are described in Section 4.
A discussion of our solution approach and results are presented in Section 5. We end with
conclusion and directions for the future work in Section 6.
36

fiScheduling a Dynamic Aircraft Repair Shop

2. Background
In this section, the formal definition of the problem is given and the relevant literature is
reviewed.
2.1 Problem Definition
Figure 2 is a snapshot of the problem at time 0, where circles represent aircraft. A number
of flights (five are shown) and their corresponding pre- and post-flight checks are already
scheduled over a long horizon. It is assumed that the total number of aircraft is constant
over a long horizon. A number of aircraft (three in the diagram) are ready for the pre-flight
check while others are currently in the shop awaiting repair before they can proceed to a
pre-flight check. Failure is only detected during a check and we assume that a check will
always correctly assess the status of an aircraft at negligible cost and that the duration of
a check is incorporated in the length of the corresponding wave.

0
Repair Shop

...
st1
Wave-1

et1 st
2

et2
Wave-2

st3

et3
Wave-3

...

st4

et4
Wave-4

st5

et5
Wave-5

Checks

Figure 2: Snapshot of the problem at time 0 over a long horizon.
The goal is to assign aircraft to waves to maximize coverage while at the same time
creating a feasible repair schedule. The scheduling problem is under the constraints that
the repair shop has limited capacity and the aircraft are subject to breakdown. We assume
that once an aircraft fails, it goes to the repair shop and waits until its repair operations
are performed.
We use the following notation to represent the problem.
 N is the set of aircraft. n is the failure rate of the aircraft n  N denoting the
frequency of failure per time unit. For example if the failure rate is 0.2 per day, it
means that the mean time to aircraft failure is 5 days.
 K is the set of aircraft types. Ik denotes the set of aircraft type k  K where k
aircraft are ready (i.e., not in the repair shop at time 0). Let |Ik | denote the number
of aircraft of type k, |Ik |  k aircraft of type k are then in the repair shop at time 0.
k is the mean failure rate over all aircraft of type k.
 R is the set of repair resources (called trades). The maximum capacity of trade r  R
is Cr .
 W is the set of waves. Each wave, w  W , has a start-time, stw , and an end-time,
etw . Each wave requires at most akw aircraft of type k.
 J is the set of existing jobs in the repair shop. Each job is associated with a specific
aircraft type. Mr is the set of jobs requiring trade r. Each job might require more
37

fiAramon Bajestani & Beck

than one trade to be completed. The processing time of job j on trade r is pjr and
cjr is the capacity of trade r required by job j.
To model the deterioration of an aircraft, each time it flies a wave its failure rate, n ,
increases by  percent, i.e., its failure rate is (1 + 0.01  )n after the flight. If an aircraft
fails, its failure rate after repair returns to what it was just before the failure. In the
other words, as in one of the standard repair models in the maintenance literature, repair
is minimal (Wang, 2002). The probability of diagnosing aircraft n as failed in pre- and
post-flight checks is a function of its failure rate right before the checks denoted as f pre (n )
and f post (n ). The probability of failure detection in pre-flight checks is smaller than the
post-flight checks because an aircraft is either just released from the repair shop or has
already passed a previous post-flight check successfully (Safaei et al., 2011).
To find the probability of failure of an aircraft in pre- and post-flight checks for a specific
wave, we need to track the complete history of the aircraft. For example, if we assume that
a given aircraft is repaired and assigned to the first wave, then there are three paths: the
aircraft fails the pre-flight check; the aircraft passes the pre-flight check, flies the wave, and
fails the post-flight check; or the aircraft passes the pre-flight check, flies the wave, and
passes the post-flight check. Therefore, the availability of the aircraft for the second wave
can be represented as a random variable whose expected value depends on the probability of
these three different paths and the scheduling decisions to repair the failed aircraft before
the second wave. Similarly the availability of the aircraft for subsequent waves depends
on its entire path through the checks, repair shop, and waves. As the number of waves
and aircraft increase, the size of the state space will become prohibitive. Furthermore, the
repair scheduling decisions themselves impact the aircraft histories: the probability that an
aircraft is available for the third wave is different depending on if it was repaired in time
for the first wave or only for the second wave. The details on approximating the failure
probabilities are presented in Section 3.2.1.
As the complexity of the problem has not been shown, we prove that it is NP-hard in
Section 3.1.
2.2 Literature Review
This section provides necessary background on repair shop scheduling problem and the
logic-based Benders decomposition.
2.2.1 Repair Shop Scheduling
Repair shops have been mainly studied as a machine-repairman problem (Haque & Armstrong, 2007; Stecke, 1992) which has a set of workers and a set of machines that are
subject to failures and therefore need repair. Workers and machines respectively correspond to trades and aircraft, in our problem. As the number of workers is less than the
number of machines, it is necessary to allocate the repair jobs to the workers with the goal
of optimizing a given performance measure (e.g., the total expected machine downtime)
over the long-term. Derman, Liberman, and Ross (1980) did the early work on solving the
scheduling problem of a repair shop with a single repairman. They showed that repairing the failed machines in non-decreasing order of failure rate stochastically maximizes the
38

fiScheduling a Dynamic Aircraft Repair Shop

number of working machines. The literature on the scheduling of a repair shop was then
extended by considering multiple repairmen, preemptive and non-preemptive repair, and
different failure and repair distributions. A comprehensive review of the literature on the
scheduling of a repair system is provided by Iravani, Krishnamurthy, and Chao (2007).
The analytical models in the literature are mainly developed using Markov Decision
Processes (dynamic programming) and guarantee the optimality of a given performance
measure in the long-term. These models often do not consider the combinatorics of the
real scheduling problems such as different repair capacity limits, different due dates, and
different resource and processing requirements. Therefore, they typically result in a static
dispatching-type repair policy similar to that found by Derman et al. (1980). However,
in our problem, the waves have different plane requirements and the processing times and
the resource requirements of the repair activities become known when they enter the repair
shop. Therefore, we believe that a better performance can be achieved by dealing directly
with the combinatorics and explicitly scheduling the repair shop to meet the waves. To
handle the uncertain and combinatorial structure of the scheduling problems, we use a
dynamic scheduling approach.
Dynamic scheduling is the methodology developed in the scheduling literature where
the operational uncertainties like machine breakdowns or the unexpected arrival of new
orders prevent the execution of the schedule as planned (Aytug, Lawley, McKay, Mohan,
& Uzsoy, 2005; ODonovan, Uzsoy, & McKay, 1999). The dynamic scheduling problem
is often viewed as a collection of linked static sub-problems. Taking this view makes the
myriad of algorithms developed for the static scheduling problems applicable. The developed
algorithms can deal with the combinatorics of the scheduling problems and can optimize the
quality of the schedule at discrete time points, i.e., for each static sub-problem. However,
as they cannot completely deal with the operational uncertainties, a real-time disruption
requires modification of the schedule to either permit the execution or to improve the
quality of the schedule considering the most recently revealed information. The process
of modifying the previous schedule is called rescheduling (Vieira, Hermann, & Lin, 2003;
Aytug et al., 2005; Bidot, Vidal, Laborie, & Beck, 2009). How to solve static sub-problems
and how to connect them using rescheduling strategies are the main aspects of the dynamic
scheduling research. Although we are not aware of using dynamic scheduling in a repair
system, it has been successfully applied to a variety of scheduling problems including single
machine (Ovacik & Uzsoy, 1994; ODonovan et al., 1999; Cowling & Johansson, 2002),
parallel machines (Vieira, Hermann, & Lin, 2000; Ovacik & Uzsoy, 1995), and job shop
(Sabuncuoglu & Bayiz, 2000; Liu, Ong, & Ng, 2005; Vinod & Sridharan, 2011).
Other areas of literature with similarities to our static problem are the operational
level maintenance scheduling problem, in general, and the flight and maintenance planning
problem of military aircraft, specifically. The former literature addresses the problem of
finding a schedule for given maintenance activities such that the sum of maintenance costs
is minimized. The focus is on the operational level, determining the maintenance activities
performed in each time period (Budai, Huisman, & Dekker, 2006). Starting with the early
work of Wagner, Giglio, and Glaser (1964), this literature was extended through developing
mathematical models and effective solution approaches for a variety of applications (Frost
& Dechter, 1998; Haghani & Shafahi, 2002; Budai et al., 2006; Grigoriev, van de Klundert,
& Spieksma, 2006). The latter literature studies the problem of maintenance planning and
39

fiAramon Bajestani & Beck

mission assignment of military aircraft where the goal is to decide which aircraft to fly and
which one to perform maintenance on, maximizing their long-term availability. Similar to
the maintenance scheduling literature, mathematical programming is the common approach
to solve the problems of this literature. Kozanidis, Gavranis, and Kostarelou (2012) recently
proposed a mixed integer non-linear programming model to optimize the joint flight and
maintenance plan of mission aircraft.
Safaei et al. (2010) modeled the static problem addressed here as an operational level
maintenance scheduling problem using MIP. Their MIP includes an assignment problem and
two network problems: the former assigns the aircraft to the waves and the latter calculates
the expected number of available aircraft for the waves as well as the expected number
of available workers for the repair jobs. They later extended their work by using slightly
different MIP model where the time-indexed approach is used to enforce the workforce
availability constraint and they verified the validity of their model by a number of instances
under different combinations of workforce sizes (Safaei et al., 2011).
The difference between the static problem addressed in this paper and the previous
works on operational maintenance scheduling is that our objective function (flight coverage) depends not only on the scheduling decisions but also on the outcomes of the preand post-flight checks. These two quite different components of the problem motivate the
decomposition approach, logic-based Benders decomposition, reviewed below.
2.2.2 Logic-Based Benders Decomposition
The classical Benders decomposition (Benders, 1962; Geoffrion & Graves, 1974) is a mathematical programming approach for solving large-scale mixed integer programming models.
It partitions the problem into a mixed integer master problem (MP) which is a relaxation
of the global model and a set of linear sub-problems (SPs). Solving a problem by classical
Benders involves iteratively solving the MP to optimality and using the solution to generate the sub-problems. The linear programming dual of the SPs is then solved to derive
the tightest bound on the global cost function. If this bound is greater than or equal to
the current MP solution (assuming a maximization problem), the MP solution and the SP
solutions constitute a globally optimal solution. Otherwise, a constraint, a Benders cut, is
added to the MP to express the violated bound and another iteration is performed.
Logic-based Benders decomposition (Hooker & Yan, 1995; Hooker & Ottosson, 2003) was
developed excluding the necessity that the MP should be a mixed integer model and the SPs
should be linear. Therefore, the inference duals (Hooker, 2005) of the SPs are solved rather
than the linear duals to find the tightest bound on the global cost function from the original
constraints and the current MP solution. Although the logic-based Benders decomposition
has more flexibility in modeling the problems, there is no standard procedure to derive the
Benders cuts.
Representing the relaxation of SPs in the MP, and designing a strong Benders cut are of
great importance in decreasing the computational effort to identify the globally feasible and
optimal solution. The former results in MP solutions which are likely to satisfy the SPs,
and the latter rules out a large number of MP solutions in each iteration (Hooker, 2007).
Logic-based Benders decomposition has been shown to be effective in a wide range
of problems including scheduling (Hooker, 2005, 2007; Beck, 2010), facility and vehicle
40

fiScheduling a Dynamic Aircraft Repair Shop

allocation (Fazel-Zarandi & Beck, 2012), and queue design and control problems (Terekhov,
Beck, & Brown, 2009).

3. Solution Approach
The main idea of our solution approach is to view the dynamic problem as linked successive
static sub-problems. As noted above, this is a common approach in dynamic scheduling.
This view results in a rescheduling strategy based on scheduling static sub-problems over
shorter time periods. Therefore, we have two sub-goals: how to solve and how to connect
the static sub-problems. In this section, we first show that the static problem is NP-hard
and present different solution techniques for solving it. We then define three rescheduling
strategies designed to connect the static sub-problems. Finally, we describe our approach
for modeling the dynamic events, i.e., aircraft failures.
3.1 The Complexity of the Static Repair Shop Problem
We establish the NP-hardness of the static repair shop problem by reduction from a single
machine scheduling problem with the objective ofP
minimizing the weighted number of tardy
jobs and with a common due date, i.e., 1|dj = d| wj Uj 1 where dj , wj , and Uj denote the
due date, the weight, and the variable representing whether a job is tardy or not for job j,
respectively. Note that job j is tardy if its processing is not finished at or before its due date.
This problem is equivalent to a one dimensional knapsack problem with non-uniform profit
which is shown to be NP-complete by reduction from the PARTITION problem (Pinedo,
2002).
Theorem 1. The static problem is NP-hard.
Proof. We show that an instance of the single machine scheduling problem, I, with a common due date and with the objective of minimizing the weighted number of tardy jobs
reduces to the static repair shop problem. In instance I, we assume that there are T jobs,
the jth having a processing time tj , a weight wj , and a common due date d. Without
loss of generality, we further assume that the weights, wj , are in the interval [0, 1]. The
objective is to schedule the jobs such that the sum of the weights of the tardy jobs is minimized or equivalently the sum of the non-tardy job weights is maximized. From instance
I, an instance of the static repair shop problem can be formulated such that there is one
wave, there are T failed aircraft in the repair shop (|N | = T ), there are T aircraft types
(|K| = T ), and there is one repair resource (|R| = 1) with capacity C = 1. The start-time
of the wave is st1 = d, requiring all T aircraft. Each failed aircraft, j, has a different type,
and corresponds to one repair job in the repair shop with the processing time pj1 = tj and
resource requirement cj1 = 1 for the single resource. The probability of failure in the precheck of the wave for aircraft j is (1  wj ). The repaired aircraft j contributes to the flight
coverage if it survives the pre-check with probability wj . Therefore, to maximize the flight
coverage, the goal is to schedule the failed aircraft so that the sum of the probabilities that
1. The notation used for describing a problem in scheduling literature is || where  represents the
machine environment,  describes the processing characteristics and constraints in detail, and  denotes
the objective function (Pinedo, 2002).

41

fiAramon Bajestani & Beck

P
the repaired aircraft survives the pre-check, i.e.,
wj is maximized. The goal is equivalent
to maximizing the sum of the non-tardy job weights in instance I. As the single machine
scheduling problem with the objective
of the weighted number of tardy jobs and with a
P
common due date, i.e., 1|dj = d| wj Uj , is NP-hard (Pinedo, 2002), we conclude that the
static repair shop problem is also NP-hard.
3.2 Scheduling Techniques
We investigate a number of approaches to solve the repair shop scheduling problem including
mixed integer programming, constraint programming, logic-based Benders decomposition,
a dispatch rule, and a simple hybrid approach. Each of the approaches is described in detail
in this section.
3.2.1 Mixed Integer Programming
Mixed integer programming (MIP) is the default solution approach for many scheduling
problems (Heinz & Beck, 2012). In a MIP formulation, the constraints are represented in the
form of linear equalities and/or inequalities and polyhedral theory and linear programming
techniques such as relaxation and cutting planes embedded in the state-of-the-art MIP
solvers are applied to solve the problem (Queyranne & Schulz, 1994; Heinz & Beck, 2012).
We propose a novel mixed integer programming model where the uncertainty in the outcome
of the checks is modeled as expectation. This model is different from and, as we show below
in Section 4.1.2, significantly faster than those of Safaei et al. (2010, 2011). Table 1
summarizes the notation defined in Section 2.1 and defines the decision variables of the
MIP model.
In this section, without loss of generality, we interpret W as the set of waves in the
current static sub-problem and consider the start-times of the waves as due dates to finish
the repair of the aircraft. Therefore, we define D = {di |i = 1, 2, ..., |W |, |W | + 1} to be an
ordered set of due dates consisting of the wave start-times plus a big value, B, sorted in
ascending order. More specifically, di equals to the start-time of the i-th wave, sti . Because
of the limited repair capacity, it is possible that some of the failed aircraft cannot be repaired
in time for any of the waves. In such a case, the due date of the repair job is assigned to
d|W |+1 = B. In our model, B equals the sum of the start-time of the last wave and the
maximum processing times of all the jobs over all the trades, i.e., d|W | + max(pjr ) and we
j,r

do not enforce the repair resource capacity after d|W | .
As explained in Section 2.1, the exact calculation of the aircraft failure probability and
consequently of the expected number of available aircraft is intractable since it depends
on the complete aircraft histories. Therefore, we distinguish aircraft based on their type
and use a recursive equation (Equation 4) to approximate the expected number of available
aircraft. The details of Equation (4) are provided later in this section. For each aircraft type
of k, the average failure rate, k , is used to calculate the probability of failure during preand post-flight checks, respectively: kpre = f pre (k ) and kpost = f post (k ). Furthermore,
the failure rate of each aircraft is assumed to remain constant in the scheduling horizon of
the static problem and to not increase after flying a wave. Therefore, our approximation is
likely to underestimate the number of actual aircraft failures.
42

fiScheduling a Dynamic Aircraft Repair Shop

Notation
N = {1, ..., n, ..., |N |}
K = {1, ..., k, ..., |K|}
R = {1, ..., r, ..., |R|}
W = {1, ..., w, ..., |W |}
J = {1, ..., j, ..., |J|}
n
Ik
k
k

The set of aircraft
The set of aircraft types
The set of trades
The set of waves
The set of repair jobs (failed aircraft) in the repair shop
The failure rate of aircraft n
The set of the aircraft of type k
The number of aircraft of type k at the repair shop at time 0
ThePaverage failure rate over all aircraft of type k being equal

to
pre
k
The
kpost
The
stw
The
etw
The
akw
The
Mr
The
Cr
The
pjr
The
cjr
The
D = {d1 , ..., di , ..., d|W |+1 }The
B
The
Decision Variables
Zkw
xij
stjr
Inferred Variables
Ukw
Ekw
etjr

nIk

n

|Ik |

probability that aircraft type k fails in pre-flight check
probability that aircraft type k fails in post-flight check
start-time of wave w
end-time of wave w
maximum number of aircraft of type k required by wave w
set of the repair jobs requiring trade r
maximum capacity of trade r
processing time of job j on trade r
capacity of trade r required to process job j
set of due dates where di = sti , i  |W | and d|W |+1 = B
big value equal to d|W | + max(pjr )
j,r

The number of aircraft of type k assigned to fly in wave w
xij = 1 if the ith due date is assigned to job j,
and xij = 0 otherwise
The start-time of job j on trade r
The number of aircraft of type k whose repair due date is stw
The expected number of available aircraft of type k for wave w
The end-time of job j on trade r

Table 1: Summary of notation; the decision variables and inferred variables for the MIP
model.

The MIP model is shown in Figure 3 where Zkw , the number of aircraft of type k that
are assigned to fly in wave w, is a true decision variable: we can choose to send fewer aircraft
on a wave than are currently (in expectation) available. In contrast, Ekw is the expected
number of aircraft of type k available for wave w and is based on the probabilistic outcomes
of previous waves and the number of newly repaired aircraft (Ukw ). We refer to this model
as MIP and we rely on the default branch-and-bound search in the IBM ILOG CPLEX
12.3 solver, a state-of-the-art commercial MIP solver to solve it. The details of MIP model
are summarized as follows:
43

fiAramon Bajestani & Beck

Maximize

|W | |K|
X
X

Zkw

(1)

w=1 k=1

Subject to:
X
Ukw =

k, w

(2)

k

(3)

w(w 6= 1), k

(4)

Zkw  akw ,

k, w

(5)

Zkw  Ekw ,

k, w

(6)

j

(7)

j, r

(8)

j, r

(9)

t(t  st|W | ), r

(10)

i, j

(11)

k, w

(12)

j, r

(13)

k, w

(14)

xij ,

jIk , i=w

Ek1 = (k + Uk1 )(1  kpre ),
Ekw = (Ek(w1)  Zk(w1) + Ukw )(1 
X
+
Zkv (1  kpost )(1  kpre ),

kpre )

vVw

|W |+1

X

xij = 1,

i=1

stjr + pjr = etjr ,
|W |+1

etjr 

X

xij di ,

i=1

X

cjr ((t  stjr )  (t < etjr ))  Cr ,

jMr

xij  {0, 1},
0  Ekw  |N |,
+

stjr , etjr  Z  {0},
+

Zkw  Z  {0}, Zkw  |N |,

Figure 3: The global MIP model for the static repair shop scheduling problem.
 The objective function (1) maximizes the number of aircraft assigned to waves. Although we have modeled the uncertain outcome of the flight checks as expectation,
the objective function is not the expected wave coverage because (i) each wave has
specific upper bounds on plane requirements and (ii) the maximum wave coverage for
each wave is 1. If the expected number of available aircraft, Ekw , is more than the
requirement, akw , for a given wave, the extra aircraft do not fly the wave and so do
not contribute to the coverage. By not flying extra planes, we do not decrease the
probability that they will be available for the next wave.
 Equation (2) calculates the number of aircraft of type k whose repair due date is stw .
In the other words, summing the decision variables xij where job j is an aircraft of
type k and where the i-th due date corresponds to the start-time of wave w gives the
44

fiScheduling a Dynamic Aircraft Repair Shop

number of aircraft type k leaving the repair shop right before the pre-flight check of
wave w.
 Equation (3) calculates the expected number of available aircraft of type k for the
first wave.
 Equation (4) calculates the expected number of available aircraft of type k for the other
waves. The first term includes those aircraft available but not used for the previous
wave, i.e., (Ek(w1)  Zk(w1) ), and those newly arrived from the repair shop, i.e.,
Ukw . The second term sums over all aircraft that become available because they have
completed waves since the previous wave started where Vw = {v|v  W, stw1 < etv 
stw }.
 Constraints (5) and (6) ensure that the number of aircraft that are assigned to fly in
each wave is less than or equal to the number of aircraft required and the expected
number available.
 Constraint (7) assigns exactly one due date to each job.
 Equation (8) calculates the end-time of the jobs.
 Constraint (9) guarantees that the end-time of each job is less than or equal to its
assigned due date.
 Constraint (10) is a logical-and constraint enforcing the capacity limit of trade r by
summing over the capacity required by the set of jobs under repair at time t. Since
the jobs after the start-time of the last wave do not contribute to the coverage, the
capacity constraint is enforced only until the start-time of the last wave, i.e., st|W | .
The logical  constraint evaluates to 1 if and only if its two component constraints
both evaluate to 1 and to 0 otherwise. A logical inequality evaluates to 1 if and
only if it is true and 0 otherwise. For example, if job j is under repair at time t,
both logical inequalities, (stjr  t) and (t < etjr ) evaluate to 1 and the logical-and
constraint, therefore, evaluates to 1. To linearize this constraint, we rely on the default
approaches in IBM ILOG CPLEX for handling logical constraints. These approaches
translate the logical constraints into their equivalent linear counterparts by creation
of new variables and constraints (CPLEX, 2011).
 Constraints (11) to (14) define the domains of the decision variables.
3.2.2 Constraint Programming
Constraint programming is a paradigm for solving combinatorial optimization problems.
The success of constraint programming (CP) in solving a wide variety of scheduling problems
is well established in the literature (Beck, Davenport, Davis, & Fox, 1998; Baptiste, Pape,
& Nuijten, 2001). The scheduling problems are usually defined as one or several instances
of the constraints satisfaction problem (CSP) (Baptiste et al., 2001). An instance of CSP
can be formally described as a triple of (V, D, C) where V = {V1 , V2 , ..., Vn } is a set of
n variables, D = {D1 , D2 , ..., Dn } is a set of the variable domains, Di corresponding to
the possible values that Vi can take, and C = {C1 , C2 , ..., Cm } is a set of m constraints,
45

fiAramon Bajestani & Beck

each defined over a subset of variables. A constraint Ck = {Vi , ..., Vj } is defined on the
Cartesian product of the domains of the variables in its scope Di  ...  Dj and is satisfied
if the assignment of the variables in its scope corresponds to one of the value tuples in the
constraint relation (Beck, 1999). Representing the scheduling problems using CSPs results
in more modeling flexibility compared to mixed integer programming models as there is no
restriction on the type of decision variables and constraints.
CP solves scheduling problems by applying three general tools of heuristic search, constraint propagation, and backtracking within a branch-and-bound search tree (Beck, 1999;
Beck & Refalo, 2003). Constraint propagation, one of the key principles contributing to
the success of CP, is exploited by representing the problem as a conjunction of global constraints, each of which embeds efficient inference techniques that reduce the solution space
within a branch-and-bound search tree (Baptiste et al., 2001). The possibility of using two
global constraints, cumulative and global cardinality, to formulate the static repair shop
problem motivates the CP model shown in Figure 4.
To formulate the problem using CP, we use the same decision variables as in Table 1.
However, instead of xij , we define Dj corresponding to the assigned due date for job j. The
CP model differs from MIP in several constraints defined below.
The global cardinality constraint (gcc) has the syntax of gcc(card, value, base) where
card, value, and base are arrays of variables, values, and variables, respectively. The gcc
constraint is satisfied if value[i] is taken by card[i] elements of base. In our CP model, for
each aircraft type k, Constraint (15) enforces that Ukw counts the number of times that the
start-time of wave w is assigned as a due date to the jobs associated with a failed aircraft
of type k.
The cumulative constraint has the syntax of cumulative(s, p, c, C) where s = {s1 , s2 , ..., sn },
p = {p1 , p2 , ..., pn }, and c = {c1 , c2 , ..., cn } are vectors of the start-time variables, the processing time values, and the amount of required resource of each job, respectively, and where
C is the total resource capacity value. The cumulative constraint ensures that the total
amount of resource capacity used at any time never exceeds C (Hooker, 2005).
Constraint (17) enforces the time windows: job j on trade r cannot be started later
than (Dj  pjr ). Constraint (18) defines the domain of the decision variables Dj .

Maximize Objective (1)
Subject to:
Constraints (3) to (6), (12), (14)
gcc([Uk1 , Uk2 , ..., Uk|W | ], [st1 , st2 , ..., st|W | ], [DjIk ]),

k

(15)

cumulative([stjr |j  Mr ], [pjr |j  Mr ], [cjr |j  Mr ], Cr ),

r

(16)

j, r

(17)

j

(18)

0  stjr  Dj  pjr ,
Dj  {st1 , st2 , ..., st|W | , B},

Figure 4: The CP model for the static repair shop scheduling problem.
46

fiScheduling a Dynamic Aircraft Repair Shop

We implement this model using IBM ILOG CP Optimizer 12.3 where the default search
is used. The start-time variables, stjr , are defined by IloIntervalVar objects. To implement
the global constraints, we use IloDistribute class for the gcc constraint and IloPulse and
IloAlwaysIn functions for the cumulative constraint. Note that, the cumulative constraint
is implemented for any time point t until st|W | .
3.2.3 Logic-Based Benders Decomposition
As the static problem requires making two different decisions, assigning aircraft to the waves
and scheduling repair jobs for failed aircraft, a decomposition approach may be well suited.
A logic-based Benders decomposition (LBBD) method can be formulated where the master
problem assigns aircraft to waves to maximize wave coverage and the sub-problems create
the repair schedules given the due dates derived from the master problem solution. We
propose four variations: Benders-MIP and Benders-MIP-T, where the master problems are
solved using MIP, the latter with a tighter sub-problem relaxation (T stands for tighter);
and Benders-CP and Benders-CP-T with a constraint programming-based master problem.
All models use CP for the scheduling sub-problems.
The Due-Date Assignment Master Problem (DAMP): MIP Model To formulate
the master problem as a MIP model, we use a binary variable xij for job j and the i-th due
date with the same meaning as in the global MIP model. A MIP formulation of DAMP is
as follows:

Maximize Objective (1)
Subject to:

(19)

Constraints (2) to (7), (11), (12), (14)
|W |+1

X

cjr pjr  Cr max (

jMr

jMr

X

xij di ),

r

(20)

i=1

MIP cuts

(21)

The master problem incorporates a number of the constraints in the global MIP model.
It does not represent the start-times of jobs nor does it fully represent the capacity of the
trades. As is common in Benders decomposition, the master problem includes a relaxation
of the sub-problems (Constraints 20) and Benders cuts (Constraints 21).
The Sub-problem Relaxation Defining the area of job j as the area of a rectangle with height
cjr and width pjr , Constraint (20) is the relaxation of the capacity of a trade, expressing a
limit on the area of jobs that can be executed. The limit is defined using the area bounded
by the capacity of the trade and the time interval [0, M ] where M is the maximum due date
assigned to the jobs on the trade. This relaxation is due to Hooker (2005, 2007).
We tighten the relaxation of sub-problems in the Benders-MIP-T approach by enforcing
an analogous limit on multiple intervals: [0, stw ] for each wave w. For each interval, the
sum of the areas of the jobs whose assigned due date is less than or equal to the end-time
of the interval must be less than or equal to the available area. This relaxation is a special
47

fiAramon Bajestani & Beck

case of the interval relaxation due to Hooker (2005, 2007). Formally, the tighter relaxation
replaces Constraint (20) with:
|W |+1

X
jMr

cjr pjr ((

X

xij di )  stw )  stw Cr ,

r, w

(22)

i=1

P|W |+1
where (( i=1 xij di )  stw ) is a logical inequality evaluating to 1 if and only if the assigned
due date to job j is less than or equal to stw .
The Benders Cuts Before defining the cut formally, we demonstrate the intuition with an
example. Consider a due date set, D = {14, 17, 20, 35}, and, for a given trade with five
jobs, the current master solution: x21 = 1, x12 = 1, x43 = 1, x14 = 1, and x15 = 1. Job 1
is assigned to the second due date, 17, job 2 has the first due date, 14, and so on. If the
current solution is infeasible due to the resource capacity of the trade, then we know that at
least one of the jobs must have a later due date than it has in the current master solution.
We can, therefore, constrain the sum of the consecutive xij up to and including the ones
currently assigned to 1 to be one less than the number of jobs. In our example, the cut
would be:

(x11 + x21 ) + (x12 )+
(x13 + x23 + x33 + x43 ) + (x14 ) + (x15 )  5  1
These variables represent the possible due dates less than or equal to those currently assigned
for all jobs. By constraining these variables to be at most one less than the number of jobs,
at least one job must be assigned a later due date.
Formally, assume that in iteration h, the solution of the DAMP assigns a set, Q, of due
dates to the jobs on trade r. Assume further that there is no feasible solution on trade r
with the assignments in Q. The cut after iteration h is:
X X

xij  |Mr |  1,

r

(23)

jMr iI h
jr

h = {i0 |i0  i, and xh = 1} is the set of due date indices
where for each job j on trade r, Ijr
ij
less than or equal to the due date index assigned to job j in iteration h and |Mr | is the
number of jobs on trade r. The validity of this cut is proved in Section 3.2.6.

The Due-Date Assignment Master Problem: CP Model We also formulate the
DAMP using CP. Let Dj be the variable corresponding to the due date for job j similar to
the global CP model.
48

fiScheduling a Dynamic Aircraft Repair Shop

Maximize Objective (1)
Subject to:
Constraints (3) to (6), (12), (14), (15), (18)
X
cjr pjr  Cr max (Dj ),

r

jMr

jMr

(24)

CP cuts

(25)

The master problem modeled using CP includes several constraints of the global CP
model. Constraint (24) represents the relaxation of repair capacity limit of the trades
guaranteeing that the sum of processing areas for the set of jobs on the same trade does
not exceed the maximum available area.
A tighter relaxation in a CP-based DAMP replaces (24) with the following defining the
Benders-CP-T approach where the logical inequality (Dj  stw ) evaluates to 1 if and only
if the due date of job j, Dj , is less than or equal to the start time of wave w, stw .
X

cjr pjr (Dj  stw )  stw Cr ,

r, w

jMr

The CP cut is based on the same reasoning as the MIP cuts. If the assigned set of due
dates to the jobs on trade r is not a feasible solution for the SP, the cut will guarantee
that in the next iteration at least one of the assigned due dates will have a greater value.
Formally, the cut is:
_

Dj > Djh ,

j  Mr

where Djh is the due date assigned to job j in iteration h,
constraints and Mr is the set of jobs on trade r.

(26)
W

represents the logical-or

Repair Scheduling Sub-problem Given a set of due dates assigned to the jobs on a
trade, the goal of the repair scheduling sub-problem (RSSP) is to assign start-times to the
jobs to satisfy the due dates and the trade capacity. We use a CP formulation where the
RSSP for each trade is modeled by the cumulative constraint.
cumulative([stjr |j  Mr ], [pjr |j  Mr ], [cjr |j  Mr ], Cr ),
0  stjr  Djh  pjr ,

r
j, r

(27)

Recall that [stjr |j  Mr ] is the tuple of the start-time variables of the jobs on trade r,
Djh is the value assigned to the due date for job j in master problem in iteration h. The
parameters pjr , cjr , Cr are as defined in Table 1. Constraint (27) enforces the time windows
similar to constraint (17). It is worth mentioning that in the RSSP, the due date of job j,
Djh , is a value; however, it is a decision variable, Dj , in the global CP model.
Since CP approaches are shown to be significantly more efficient than MIP for simple
scheduling problems with resource capacity constraints (Hooker & Ottosson, 2003; Hooker,
2005, 2007), we do not experiment with MIP formulations of the sub-problems.
49

fiAramon Bajestani & Beck

To implement the master problems in Benders-MIP and Benders-MIP-T, we use IBM
ILOG CPLEX 12.3 solver; while Benders-CP and Benders-CP-T master problems and the
RSSP are implemented in IBM ILOG CP Optimizer 12.3. The details on the implementation
of the global constraints are similar to Section 3.2.2.
3.2.4 A Dispatching Heuristic
Since the static problem is NP-hard, solving it to optimality may be prohibitively expensive.
We therefore investigate a heuristic approach, inspired by the Apparent Tardiness Cost
(ATC) heuristic, a composite dispatching rule that is typically applied to single machine
scheduling problem with the sum of weighted tardiness objective (Pinedo, 2005). The
heuristic computes a ranking index for each job and sorts the jobs in ascending order of
the index. The heuristic then iterates through the jobs, scheduling each job at its earliest
available time. The ranking index we use is as follows:
Ij = ST (kj ) exp(

F Nj
),
F Cj

j

If we let kj denote the type of aircraft j, then ST (kj ) is the start-time of the first wave that
requires an aircraft of type kj . F Nj is the fraction of the total number of aircraft of type
kj required by the first wave that requires kj , and F Cj is the maximum proportion of the
capacity needed by job j over all its required trades, as follows.
F Cj = max(
r

pjr cjr
)
ST (kj )Cr

Intuitively, the earlier the start-time of the first relevant wave, the higher proportion of
aircraft required by that wave, and the lower the proportion of capacity required before the
wave, then the sooner the job will be scheduled. The exponential function is used to place
more weight on the start-time.
In preliminary experiments, three other dispatching heuristics were investigated, with
the chosen heuristic performing best. The first two heuristics rank the jobs with slightly
max(pjr )

max(pjr cjr )

different ranking indices equal to Ij = ST (kj )  rF Nj and Ij = ST (kj )  r F Nj ,
respectively. The third heuristic is a two-stage approach based on a decomposition. The
first stage finds the number of each aircraft type assigned to each wave and the second stage
schedules the jobs in increasing order of max(pjr cjr ) considering the values determined in
j,r

the first stage as upper bounds on the number of jobs required before each wave. Our
preliminary experiments demonstrated that our chosen dispatching rule results in on average
of 6% higher wave coverage compared to the first two heuristics and in the same coverage as
the third heuristic while having the advantage of being easy to understand and implement.
3.2.5 Hybrid Heuristic-Complete Approaches
A hybrid heuristic-complete approach in which the heuristic solution provides a lower bound
for the maximization objective (Equation 1) may improve the performance of the complete
approaches. Therefore, a simple hybrid first runs the dispatching heuristic and then uses
the objective value as a starting lower bound for the complete approaches. Assume that the
50

fiScheduling a Dynamic Aircraft Repair Shop

heuristic finds a solution, S, with f (S) as the number of aircraft assigned to waves. Any of
the complete approaches can now be modified by adding the following constraint:
|W | |K|
X
X

Zkw  f (S)

w=1 k=1

For LBBD variations, the above constraint is added to the master problem.
3.2.6 Theoretical Results
To guarantee the finite convergence of a LBBD model to a globally optimal solution, the
Benders cuts must be valid and the master decision variables must have finite domains. A
Benders cut is valid in a given iteration, h, if and only if (1) it excludes the current globally
infeasible assignment in the master problem without (2) removing any globally optimal
assignments (Chu & Xia, 2004). The former guarantees the finite convergence and the
latter guarantees the optimality. As the decision variables in DAMP have a finite domain,
it is sufficient to prove the satisfaction of the two conditions.
Theorem 2. Cut (23) is valid.
Proof. For condition (1), for the sub-problem in iteration h on trade r, by definition:
X X

xij = |Mr |

jMr iI h
jr

Consequently, cut (23) excludes the current assignment of master problem.
For condition (2), consider a global optimal solution S that does not satisfy cut (23)
as generated in iteration h. As the cut states that at least one job must have a greater
due date than it had in h, to violate the cut, all jobs in S must have equal or lesser due
dates than they had in iteration h. However, because the sub-problem was infeasible in
iteration h, any sub-problem with only equal or lesser due dates must also be infeasible as
the available capacity on the trade is the same or less. Therefore, S must be infeasible and
we contradict the assumption that S is globally optimal.
Therefore, the cut is valid.
An analogous argument holds for cut (26).
3.3 Rescheduling Strategies
The dynamic repair shop problem over the long horizon can be viewed as static scheduling
sub-problems over successive time periods. Lets assume that we start repairing the failed
aircraft and assigning them to the waves based on the computed schedule at time 0. A wave
might start while a repair is under way in the repair shop. If some aircraft fails the pre-flight
check, it goes to the repair shop. Each failed aircraft requires a set of independent repair
activities with known processing times and resource requirements. At the repair shop, some
of the previously failed aircraft might be already repaired, some might be under repair, and
others might be awaiting repair. Once the failed aircraft enters the repair shop, we have
a new static repair scheduling sub-problem where the set of existing jobs (J), the number
51

fiAramon Bajestani & Beck

of aircraft not in the repair shop for each aircraft type (k ), and the failure rates of the
aircraft (n ) are updated. The set of existing jobs includes the recently failed aircraft and
the previously failed aircraft whose repairs are still under way or are not yet started. The
new static sub-problem has an added constraint, namely that the repairs currently under
way cannot be disrupted.
We connect the static sub-problems using three different policies denoted as Pij where i
and j define the length of scheduling horizon and the frequency of rescheduling in number
of waves, respectively. In all three policies, we schedule the repair activities, observe the
aircraft failures, and respond to failures by rescheduling the repair activities.
The three policies discussed here are as follows:
 P11 : In Figure 5, we show that P11 schedules one wave at a time (i = 1) and reschedules
after each wave (j = 1). P11 is a myopic policy aiming at providing the next first
wave with the highest possible coverage.
 P31 : In contrast to P11 , for P31 (Figure 5), the scheduling horizon is three waves but
rescheduling is still done after each wave. P31 with a longer scheduling horizon than
P11 trades-off the coverage among the next three waves. It is worth mentioning that
we have chosen three as the length of the scheduling horizon because three waves are
usually scheduled daily based on the real data (Safaei et al., 2011).
 P33 : This policy has a scheduling horizon with a length of three waves and reschedules
after every third wave (Figure 5). P33 might trade-off the lower coverage of the next
first wave for higher coverages of the second and the third waves; however, it has a
lower frequency of rescheduling.

3.4 Modeling the Aircraft Failures
To model the dynamic events, we simulate the aircraft failures in pre- and post-flight checks.
Every aircraft either passes or fails each check. If the aircraft fails, a new set of repair
activities with known processing times and resource requirements is added to the repair
shop. If the aircraft passes, it flies the wave if required. As mentioned in Section 2.1,
after repair the failure rate of the aircraft returns to what it was before the failure and it
increases by  percent each time it flies a wave due to deterioration. If n is the initial
failure rate of the aircraft n  N , its failure rate after flying w waves without failure will
be n (1 + 0.01  )w .

4. Computational Experiments
In this section, we present two separate empirical studies. The first study compares the
scheduling techniques experimentally and presents insights into each algorithms performance through a deeper analysis of the results. The second study investigates the impact
of using different scheduling techniques and rescheduling policies on the observed wave
coverage.
52

fiScheduling a Dynamic Aircraft Repair Shop

The P11 Policy:
Scheduling Horizon

Scheduling Horizon
0

Scheduling Horizon

Scheduling Horizon

...
st1

et1

st2

Wave-1

et2
st3

Wave-2

et3

st4

Wave-3

et4
Wave-4

Scheduling Horizon

The P31 Policy:

Scheduling Horizon

Scheduling Horizon
0

...
st1

et1

st2

Wave-1

et2
st3

Wave-2

et3

st4

Wave-3

et4
Wave-4

The P33 Policy:
Scheduling Horizon

Scheduling Horizon
0

...
st1

et1

st2

Wave-1

et2
et3

st3

Wave-2

Wave-3

st4

et4
Wave-4

Figure 5: The rescheduling policies.
4.1 Experimental Results on Scheduling Techniques
This sub-section describes the experiment comparing different solution techniques for scheduling the static repair shop.
4.1.1 Experimental Setup
The problem instances have 10 to 30 aircraft (in steps of 1), 3 or 4 trades, and 3 or 4 waves.
Five instances for each combination of parameters are generated, resulting in 420 instances
(21 total aircraft counts by 2 trades counts by 2 waves counts by 5 instances).
Aircraft: The number of aircraft types is equal to |N5 | , where |N | is the number of aircraft.
The aircraft are randomly assigned to different types with uniform probability. The number
of aircraft of type k is |Ik |. The failure rate for each aircraft is randomly chosen from the
uniform distribution [0, 0.5]. The failure rate for aircraft of type k, k , is the mean failure
rate over all aircraft of type k. The functions used to represent aircraft n probability of
failures in pre- and post-flight checks, respectively, are f pre (n ) = (1en ) and f post (n ) =
(1e3n ). It is worth mentioning that the conditions of a reliability function in the extreme
values of failure rates hold true for the functions used. If the failure rate goes to 0, the
probability of failure equals 0, and if the failure rate goes to , the probability of failure
equals 1.
Waves: The plane requirement of each aircraft type for each wave is randomly generated
from the integer uniform distribution [1, |Ik |]. The length of each wave is drawn with uniform
53

fiAramon Bajestani & Beck

probability from [3, 5]. To make an instance loose enough to permit feasible solutions yet
tight enough to be challenging, a lower bound on the length of the scheduling horizon (H)
is needed. The sum of the processing areas of the jobs in each trade, r, divided by the
trade capacity is denoted by Sr . LB = max(Sr ) is a lower bound on the time required to
r
schedule all jobs and we use H = 1.2  LB. The end-time for each wave, etw , is generated
as et|W | = H  rand[0, 3] for the final wave, |W |, and etw = stw+1  rand[0, 3] for w < |W |.
Trades: The capacity limit for each trade is set at Cr = 10.
Repair Jobs: Eighty percent of the aircraft are in the repair shop at the beginning, resulting
in |J| = 0.8|N | repair jobs. The jobs are randomly assigned to the trades with replacement
such that the number of jobs per trade is equal to |J|/2. Each job requires at least one
trade and some require more than one trade. The capacity of trade r used by job j, cjr ,
is drawn from [1, 10] while the processing time, pjr , is drawn from [r, 10r]: jobs on trades
with lower indices have shorter processing times than those on trades with higher indices.
Though our problem instances are generated randomly, the setting of our experiment
includes three numerical examples of Safaei et al. (2011) which are based on the real data.
Furthermore, our setting consists of more instances and results in problem instances which
are one and a half times bigger than the examples used in the literature (Safaei et al., 2011)
where the number of aircraft is 10, 15, or 20; the number of waves is 3 or 4; and the number
of trades and aircraft types is equal to 3 and 2.
All experiments were run with a 7200-second time limit on an AMD 270 CPU with 1
MB cache per core, 4 GB of main memory, running Red Hat Enterprise Linux 4.
4.1.2 Experimental Results
Figure 6 shows scatter-plots of run-times of the six complete approaches. Both axes are
log-scale, and the points below the line y = x indicate a lower run-time for the algorithm
on the y-axis. The numbers in the boxes indicate the number of points below or above the
line. Run-times are counted as equal if they differ less than 10%.
The graphs indicate a benefit for MIP over CP, for Benders-CP over MIP, for BendersMIP over Benders-CP and MIP, for Benders-MIP-T over Benders-MIP, and for BendersCP-T over Benders-CP. Table 2 presents further data, sorted in descending percentage of
problems solved to optimality, for all algorithms. The changes in the results compared to the
previous work (Aramon Bajestani & Beck, 2011b) are due to improved scheduling models,
different solvers, and different test problems. The scheduling algorithms are enhanced by
using a smaller value for B, the resource capacity constraint is enforced in a shorter interval,
i.e., [0, st|W | ], and more efficient formulations for Constraints (5), (6), and (20) are used.
The mean run-time of the MIP model given by Safaei et al. (2011) over eight scenarios
with 10 aircraft and 3 waves is 294.75 seconds. However, our proposed MIP model has
the run-time of 2.64 seconds on average over ten of the instances with the same number of
aircraft and waves, indicating that it is significantly faster than Safaei et al.s model.
MIP vs. CP The MIP approach has a clear superiority over the CP, achieving a lower
run-time on 89% of the problem instances. The CP model outperforms the MIP only
on 5% of the instances where it can solve to optimality within the time limit. A further
investigation of the results shows that the mean quality of CP solution is 0.27% from the
54

fiScheduling a Dynamic Aircraft Repair Shop

MIP vs. CP

10000

Benders-CP vs. MIP

10000

20

35

169
1000

100

100

100

10
1

Benders-MIP

1000

Benders-CP

1000

MIP

Benders-MIP vs. Benders-CP

10000

10
1

10
1

23
0.1

0.1

0.1

220

372
0.01
0.01

1

100

0.01
0.01

10000

1

CP

10000

Benders-MIP-T vs. Benders-MIP

10000

10
1

Benders-CP-T vs. Benders-CP
122

100
10

100

10

0.1

378
1

100

1

1
0.1

0.1

10000

1000

Benders-CP-T

Benders-MIP-T

100

100

Benders-CP

1000

1000

Benders-MIP

1

162

34

0.01
0.01

368
0.01
0.01

10000

MIP

Benders-MIP vs. MIP

10000

100

161

229
0.01
0.01
10000

1

MIP

100

0.01
0.01
10000

1

100

10000

Benders-CP

Benders-MIP

Figure 6: Run-times (seconds) of the six complete models.

Method
Benders-MIP-T-Hybrid
Benders-MIP-T
Benders-MIP
MIP
MIP-Hybrid
Benders-CP
Benders-CP-T
Dispatching Rule
CP

Mean
Time (s)
211.98
213.12
227.94
837.04
924.30
1373.16
1356.70
0
6857.14

Iter.

% MP

% SP

66.63 (8.0)
66.44 (8.0)
64.66 (8.0)
75.72 (15.5)
66.42 (10.0)
-

51.39 (55.05)
51.84 (53.98)
61.75 (67.44)
84.30 (96.96)
85.36 (97.36)
-

48.61 (44.95)
48.16 (46.02)
38.25 (32.56)
15.70 (3.04)
14.64 (2.64)
-

% Solved
to optimality
98.10
97.86
97.62
93.57
91.19
85.24
85.00
9.76
4.76

Table 2: The mean run-time, the mean (the median) number of master problem iterations,
the mean (median) percentage of run-time spent solving the master problem and
the sub-problems, and the percentage of problems solved to optimality for all
approaches.

55

fiAramon Bajestani & Beck

best found solution across all the algorithms. Therefore, the poor performance of CP is due
to its weakness in proving the optimality.
Benders-CP vs. MIP The Benders-CP approach does better than MIP in terms of runtime on 52% of the instances while performing worse on 40%. However, Table 2 favors MIP
in terms of the overall performance, smaller mean run-time, mainly because more instances
are solved to optimality within the time limit.
Benders-MIP vs. Benders-CP The Benders-MIP approach achieves a better runtime than Benders-CP on 88% of the test problems, performing worse on about 8%. The
branching heuristics for Benders-CP often lead to an initial feasible master solution with
tighter due dates than the initial master solution in Benders-MIP. The tighter, globally
infeasible initial solution means that the CP-based master problem model requires more
iterations to find a globally feasible solution.
Benders-MIP vs. MIP The Benders-MIP approach achieves a better run-time than
MIP on 90% of the test problems and a worse run-time on 8%, achieving a lower mean
run-time and solving a higher proportion of the problem instances. When the time horizon
is short, the MIP approach is faster, however, with longer horizons and more jobs, the
number of constraints and variables grows, substantially reducing its performance.
Benders-MIP-T vs. Benders-MIP The tighter relaxation in Benders-MIP-T slightly
speeds up LBBD: Benders-MIP-T has a better run-time than Benders-MIP on 55% of
problems instances and worse on 39%. The mean run-time decreases by 6% and the tighter
relaxation solves one more instance to optimality. Tightening the relaxation of sub-problems
increases the mean number of iterations in spite of our expectation. A closer look to the
results shows that the mean number of iterations of the instances solved to optimality by
both approaches (97.38% of the instances) decreases: 39.15 and 41.24 for Benders-MIP-T
and Benders-MIP, respectively. However, the mean number of iterations of the instances
timed out by both approaches (1.9% of the instances) increases from 1051.38 for BendersMIP to 1259.88 for Benders-MIP-T. Therefore, the increase in the number of iterations
results from the timed-out instances which does not support that the tighter relaxation
requires more iterations to optimality.
Furthermore, the percentage of time solving the master problem decreases compared to
Benders-MIP, while the sub-problem percentage run-time increases. This latter observation
is because the sub-problems for Benders-MIP that can be quickly proved insoluble by the
initial propagation of CP sub-problem model, violate the tighter relaxation in the BendersMIP-T master problem. Therefore, in the tighter model, the sub-problem solver is not called
on these easy sub-problems, increasing the percentage run-time spend on sub-problems.
Benders-CP-T vs. Benders-CP The tighter relaxation in CP-based master problem
results in a slightly lower mean run-time; however, as shown in Figure 6, their performance
comparison is more even.
Incomplete and Hybrid Approaches The dispatching heuristic is fast, finding a feasible solution to all problems. However, it finds (but, of course, does not prove) an optimal
solution in only 9.76% of the instances and Benders-MIP-T finds and proves optimality for
56

fiScheduling a Dynamic Aircraft Repair Shop

these instances in 0.99 seconds on average. It seems that the heuristic can find the optimal solution only when the problem instance is relatively easy. The mean quality of the
heuristic solution is 16% from optimal. In industries with expensive assets, such a reduction
in solution quality can translate to costly under-use of a valuable resource (e.g., a fighter
aircraft costs in the vicinity of 100 million dollars). However, finding a feasible solution almost instantaneously for large problem instances makes the heuristic approach compelling
in situations where the long run-time might delay carrying out the waves. For example, if a
wave starts within a very short time, flying the wave with lower coverage (achieved by the
dispatching heuristic) is better than not carrying it out because of the long solving time of
the complete approaches.
To evaluate the effect of combining the dispatching heuristic with the complete approaches, we examine using the hybrid heuristic-complete approach. A smaller feasible set
is the direct consequence of defining a bound on the cost function. As the MIP model
searches the feasible set, while LBBD methods explore the infeasible space, one intuition is
that the MIP model should benefit more from using the heuristic solution. However, solving
the master problem in LBBD requires searching in a relaxed feasible space and therefore
the heuristic starting solution may also speed solving.
Table 2 shows a very marginal benefit for bounding the Benders-MIP-T approaches with
the dispatching heuristic solution. Bootstrap paired-t tests (Cohen, 1995) also indicate that
there is no significant difference in mean run-time at p  0.01 for either hybrid.
Scalability Figure 7 shows our results as the number of aircraft per wave increases. We
|N |
aggregate results by truncating |W
| and using the instances with three waves and both three
and four trades. Note that each point represents 30 problem instances except x = 3 which
has only 20 problems instances. We omitted x = 10 as we only have 10 problem instances
for that point. The y-axis is log-scale.
The results show that the LBBD variations outperform the other techniques across all
ratios.
Summary The following observations on the performance of scheduling techniques are
supported by this empirical study.
 The LBBD approach combining mixed integer programming and constraint programming outperforms the mixed integer programming model. The mean run-time of
Benders-MIP is almost 4 times lower than MIP. Furthermore, defining the time ratio for a given instance as the MIP run-time divided by Benders-MIP run-time, the
Benders-MIP is almost 32 times faster than MIP, on average, with a geometric mean
time ratio of 31.56. The time ratios range is [0.01, 94132.67] with a median of 38.32.
 A tighter relaxation slightly speeds up LBBD. Benders-MIP-T and Benders-CP-T,
both, have a run-time about 1.2 times faster than Benders-MIP and Benders-CP,
respectively.
 A dispatching heuristic can provide the optimal solution for the easy problem instances. However, the mean percent relative error of heuristic is almost 16% overall,
indicating that the dispatching rule by itself is not effective enough for industries with
high equipment cost.
57

fiAramon Bajestani & Beck

10000

CP
MIP
MIP-Hybrid
Benders-CP
Benders-CP-T
Benders-MIP
Benders-MIP-T
Benders-MIP-T-Hybrid

Mean Run-Time (sec)

1000

100

10

1

0.1

0.01
3

4

5

6

7

8

9

Aircraft/Wave

Figure 7: Mean run-time vs. number of aircraft per wave (|W | = 3).
 The simple hybridization of the complete approaches with the dispatching heuristic
does not result in a statistically significant difference in run-time.
4.2 Experimental Results on Rescheduling Strategies
This sub-section describes experiment investigating the impact of applying different scheduling techniques and rescheduling policies in a dynamic repair shop.
4.2.1 Experimental Setup
For our problem instances, the number of aircraft, the number of trades, and the total
number of waves are set to {10, 15, 20, 25, 30}, {4}, and {30} respectively. Each combination
has 5 instances for a total of 25 instances. Each instance is simulated 20 times. The
parameters of the problem instances are generated as in Section 4.1 with the following
modifications:
Aircraft: The failure rate of an aircraft is increased by  = 5 percent each time it is used.
Repair Jobs: Repair jobs that entered repair shop after time 0 are randomly assigned to
the trades. The probability of assigning a job to each trade is considered as 0.5.
Waves: The start-time of each wave is generated as st1 = rand[ H3 , H2 ] for the first wave,
and stw = etw1 + rand(0, 40) for 1 < w  30. As mentioned earlier the total number of
waves is 30. The value of H is calculated as in Section 4.1.
Dynamic events: To simulate an aircraft failure, we generate a random value from the
uniform distribution [0, 1] for each aircraft at each check. If the random value is less than
the aircrafts probability of failure, the aircraft fails; otherwise, it passes. The aircrafts
probability of failure in pre- and post-flight checks are calculated using (1  en ) and
(1  e3n ), respectively. Recall that, n is the failure rate of aircraft, n  N , which
58

fiScheduling a Dynamic Aircraft Repair Shop

increases by  = 5 percent each time the aircraft flies a wave. Note that passing the
pre-flight check of a wave does not necessarily mean that the aircraft flies the wave. If
the number of available aircraft is more than the requirements, the aircraft that fly are
randomly selected from those that passed the pre-flight check to meet the requirements.
Our latter assumption implies that all aircraft ready at the beginning of a wave are checked
regardless of the wave requirements. Since we have assumed that the pre- and post-flight
checks have negligible cost, our assumption is reasonable to discover the potential aircraft
failures sooner which is likely to increase their availability for the subsequent waves.
We experiment with three techniques including MIP, Benders-MIP-T, and the dispatching rule discussed in Section 3.2. The time-limit to schedule the repair activities at each
decision time point is 600 seconds. We execute the best feasible schedule found before the
time-limit if an algorithm times out. In the case that Benders-MIP-T times out, the schedule created by the dispatching heuristic is executed as Benders-MIP-T does not create a
feasible schedule when it times-out.
As in the static problem, the scheduling uses IBM ILOG CPLEX 12.3 and IBM ILOG
CP Optimizer 12.3. The simulation is implemented in C++.
4.2.2 Experimental Results
In this section, we discuss our results to compare the performance of different scheduling
and rescheduling techniques on the availability of the aircraft in the long run. We further
investigate the effect of modeling the aircraft failures using the expected coverage.
Figures 8, 9, and 10 illustrate the mean observed coverage up to flight w  {1, 2, ..., 28}
for different scheduling and rescheduling techniques. Denoting wpl as
the coverage of flight
Pw
i=1 ipl
w in the l-th simulation of instance p for a given policy, Owpl =
represents the
w
mean observed coverage up to flight w in instance p and in simulation
l. The
mean observed
P
P
P

L

Owpl

coverage up to flight w, shown in the figures, is calculated as Ow = p=1 P l=1
, where P
L
and L are the number of instances and simulations, respectively. Table 3 shows the mean
observed coverage up to flight 28, i.e., O28 and the variance of the observed coverage up
to flight 28 for all scheduling techniques and rescheduling policies. As illustrated, BendersMIP-T using P31 achieves at least a 10% higher mean coverage than any other combinations
of the scheduling and rescheduling techniques and has the lowest variance.
The impact of the scheduling algorithms A complete technique is anticipated to
achieve higher flight coverage because it takes the expected probabilistic information into
Method
Benders-MIP-T
MIP
Dispatching heuristic

P11
0.67 [0.03]
0.52 [0.03]
0.61 [0.04]

O28 [var(.)]
P31
0.77 [0.01]
0.64 [0.03]
0.61 [0.04]

P33
0.70 [0.01]
0.60 [0.02]
0.63 [0.03]

P11
45
34
42

(.)
P31
56
46
44

P33
48
38
47

Table 3: The mean (variance) of observed coverage up to flight 28 (O28 [var(.)]) and the
mean percentage of available aircraft for the first flight ().

59

fi1

1

0.9

0.9

0.8

0.8

0.7

0.7

Mean Observed Coverage

Mean Observed Coverage

Aramon Bajestani & Beck

0.6
0.5
0.4
0.3
0.2

0.5
0.4
0.3
0.2

Benders-MIP-T: P31
Benders-MIP-T: P11
Benders-MIP-T: P33

0.1

0.6

MIP: P31
MIP: P11
MIP: P33

0.1

0

0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Up to Flight

Up to Flight

Figure 8: Mean observed coverage for three
different policies using BendersMIP-T.

Figure 9: Mean observed coverage for
three different policies using
MIP.

1
0.9

Mean Observed Coverage

0.8
0.7
0.6
0.5
0.4
0.3
0.2

Heuristic: P31
Heuristic: P11
Heuristic: P33

0.1
0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Up to Flight

Figure 10: Mean observed coverage for three
different policies using the dispatching heuristic.

account when creating a repair schedule, while the dispatching heuristic does not have
this property. As shown in Table 3, Benders-MIP-T as a complete technique results in
higher mean observed coverage over all policies when compared to the dispatching heuristic.
However, MIP, incorporating the mean of known information on uncertainty into scheduling
60

fiScheduling a Dynamic Aircraft Repair Shop

the repair activities, results in flights with lower coverage than the dispatching heuristic over
two of the rescheduling policies, P11 and P33 . To understand the MIP performance, we make
two conjectures.
Our first conjecture is that the poor performance of MIP algorithm is because it frequently times out on most of the static sub-problems and the best found feasible solution or
the dispatching heuristic is then used to create the repair schedule. However, our results do
not support the conjecture. MIP algorithm only times out on 13% of the scheduling subproblems and a feasible solution is found in each, implying that the dispatching heuristic is
never used to find a repair schedule.
Our second conjecture is that the low coverage achieved by MIP can be attributed to
its different way of scheduling the repair activities compared to the other two scheduling
techniques. A deeper look into the schedules of the static sub-problems shows that the dispatching heuristic and Benders-MIP-T schedule the repair activities at the earliest possible
time; however, MIP usually does not. Repairing the aircraft earlier makes more aircraft
available which intuitively increases the coverage in the long run, even though the number
of pre-flight checks that aircraft go through increases in expectation. The quick adjustment
of the schedule makes some of the failed planes again available before the start-time of the
next flight. To investigate the impact of making the aircraft available earlier using a given
scheduling technique,
we define the mean percentage of available aircraft for the first flight
P
P
P
P

L

S

kpl

k=1
as (Pij ) = p=1 l=1
where kpl and S denote the percentage of aircraft available
P LS
at the beginning of the first flight of the k-th static sub-problem in the l-th simulation of
instance p and the number of static sub-problems in Pij policy, respectively. For example,
in P31 policy for a given p and l, the first static sub-problem includes flights 1, 2, and 3.
Then, 1pl is the number of aircraft available for flight 1 divided by the total number of
aircraft. The second static sub-problem schedules for flights 2, 3, and 4. Therefore, 2pl is
equal to the number of aircraft available for flight 2 divided by the total number of aircraft.
We follow the same procedure to find kpl for all 28 static sub-problems in P31 policy. We
can find (P11 ) and (P33 ) using the same argument considering that the number of static
sub-problems are 30 and 10, respectively.
Comparing any pair of the scheduling and the rescheduling techniques in Table 3, there
is a positive relationship between making the aircraft available earlier and the wave coverage
in the long term which supports our conjecture: if the mean percentage of available aircraft
for the first flight () increases, the mean observed coverage in the long rum (O28 ) also
increases.

The impact of rescheduling policies As illustrated in Figures 8 and 9, the P11 policy
with either Benders-MIP-T or MIP in the short-term (i.e., for the first three flights) outperforms the other two policies. However, the P31 policy then leads to consistently higher
coverage because it schedules over a longer horizon and adjusts the schedule as soon as
aircraft failures occur. Although P31 with the dispatching heuristic also responds quickly
to the aircraft failures, it does not incorporate the length of the scheduling horizon into the
ranking index for repair activities and always repairs the aircraft for the earliest possible
time, resulting in flights with the same coverage as P11 .
Figure 11 displays the cumulative percentage of the flights with a coverage less than
or equal to  for Benders-MIP-T, the dispatching heuristic and MIP where  denotes the
61

fiAramon Bajestani & Beck

values on the x-axis. The best performing approach will have fewer flights with a low
coverage and more flights with a high coverage. Therefore, its curve will be closer to the
lower right-hand corner. As illustrated, Benders-MIP-T using P31 performs better than any
other combination. The P31 rescheduling policy is computationally more expensive than
the other two policies, its run-time per one static sub-problem, however, is small compared
to the length of scheduling horizon being usually one day in the real applications (Safaei
et al., 2011). The P31 policy using Benders-MIP-T has a run-time of on average 67 seconds
per one static sub-problem and of less than 249 seconds on 90% of static sub-problems.
100

P31-Benders-MIP-T
P11-Benders-MIP-T
P33-Benders-MIP-T
P31-Heuristic
P11-Heuristic
P33-Heuristic
P31-MIP
P11-MIP
P33-MIP

90

Percentage of Flights

80
70
60
50
40
30
20
10
0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Flight Coverage

Figure 11: The percentage of flights with a coverage less than or equal to , where  denotes
the values on the x-axis.
In summary, our analysis of the results identifies the Benders-MIP-T with P31 (BendersMIP-T:P31 ) as the best combination of the scheduling and the rescheduling techniques
providing the flights with a higher mean coverage over the long term. Furthermore, it
has the lowest variance for the observed coverage compared to the other scheduling and
rescheduling techniques.
The impact of modeling the uncertainty in expectation Because of the random
aircraft failures, the coverage achieved by any scheduling algorithm is a random variable.
The ultimate goal is to construct a repair schedule which is optimal for the specific realization of the uncertainty that actually occurs. However, since the complete information on the
aircraft failures is not known and the future uncertainty is dependent on the previous repair
decisions, it is impossible to find a repair schedule which is ideal under any realization of
uncertainty. As discussed earlier, we have modeled the aircraft failures using the expected
value to find the optimal repair schedule. Since treating the uncertainty in the expectation form may be far from optimal for the actual realization of uncertainty, we perform a
sensitivity analysis on the failure rates of the aircraft to investigate how the optimal repair
schedule by Benders-MIP-T:P31 is hedged against various uncertain situations.
62

fiScheduling a Dynamic Aircraft Repair Shop

Using the same problem instances as in Section 4.2.1, two other experiments are set up
where the failure rate of each aircraft (n ) is increased to n +0.05 and n +0.1. Our results
show that while the mean observed coverage up to flight 28 decreases to 0.69 and 0.62, the
variance of observed coverage does not change, indicating that modeling the uncertainty
using the expected probabilistic information is a reasonable approach.
To find a possible upper bound (tighter than 1) on the mean observed coverage up to
flight w under any scheduling algorithm, we define a policy called Relaxed which relaxes
the repair capacity limit and repairs any failed aircraft after its maximum processing times
over all trades. Although the Relaxed policy makes the aircraft available for the waves
earlier than any other repair scheduling policy, we cannot guarantee that it results in the
upper bound on the observed coverage unless all waves have the same requirements. The
optimal decision might be to trade off the immediate low coverage for future higher coverage
when the plane requirements for the waves are different. Applying the Relaxed scheduling
policy on the same instances as in Section 4.2.1, the mean observed coverage up to flight
28, i.e. Ow , is 24% higher than the best identified algorithm, Benders-MIP-T:P31 . More
specifically, the Relaxed policy results in a mean coverage of 0.95 with the variance of 0.002.
The impact of longer scheduling horizon vs. more frequent rescheduling The
P31 policy changes the repair schedule after each flight and trades-off the coverage among
three consecutive flights by scheduling over a longer horizon. In contrast, the P11 policy
schedules for one flight and reacts after each flight while the P33 policy reasons over a longer
term without a quick response to the dynamic events. As already shown in Figures 8 and
9, the P31 policy with any of the complete techniques results in a higher mean coverage.
The P11 policy outperforms the P33 for early waves, but the P33 provides the later waves
with higher coverage.
The superiority of policy P31 indicates that both features of quick response to the dynamic events and long-term reasoning contribute to the overall performance. The contribution of each feature is significantly dependent on several parameters such as the aircraft
failure rates, the plane requirements, and the repair capacity. When the failure rate is high,
the probability of aircraft being diagnosed as failed in pre- and post- flight checks is higher.
Therefore, the arrival rate of the aircraft to the repair shop is higher and the previously
constructed schedule is more likely not to be executed as is. In such a system, frequent
rescheduling is more likely to increase the coverage. When the plane requirements of the
waves are widely varying and the repair capacity limit is tight, trading-off the coverage
among the flights through scheduling over a longer horizon significantly contributes to the
availability of the aircraft in the long term.
Summary The following observations on how and when the scheduling and rescheduling
should be done are supported by the second empirical study:
 Solving the dynamic repair shop problem using the Benders-MIP-T scheduling technique and the P31 rescheduling policy results into an observed coverage with higher
mean and lower variance than any other combination tested.
 There is a positive relationship between making the failed aircraft available as early
as possible and achieving a higher coverage in the long term.
63

fiAramon Bajestani & Beck

 Since the variance of the P31 policy with the Benders-MIP-T is not sensitive to the
small changes in the aircraft failures, modeling the uncertainty with respect to the
mean is a reasonable approach to balance against different uncertain scenarios.

5. Discussion
The experimental results demonstrate that incorporating both probabilistic and execution
time reasoning into the schedule of repair activities results in a better system performance.
We showed that the decomposition technique, LBBD, and the rescheduling policy P31 result
in a 10% higher mean observed coverage in the long term, increasing the utilization of the
valuable resources. The decomposition technique considers the mean of known probabilistic
information about uncertainty over a longer scheduling horizon and repairs the failed aircraft
at the earliest time. The P31 rescheduling policy takes advantage of up-to-date information
more frequently. It is also shown that the variance of the coverage does not increase as the
aircraft failures increase, supporting the core idea of our solution approach: the dynamic
repair shop problem is viewed as a collection of static sub-problems where the uncertainty
on aircraft failures is treated as expectation.
Optimizing with respect to the mean and considering a specific class of scheduling problems are limitations of our solution approach. We address each in detail below and discuss
ideas to deal with them.
Modeling the uncertainty Optimizing with respect to the expected coverage can have
unfavorable consequences: the constructed repair schedule may have a remarkably poor
performance for particular realizations of uncertainty that might happen in actuality (Birge
& Louveaux, 1997). There are a number of other possible approaches for solving the dynamic
problem. We briefly discuss each method below.
Leaving some availability slack on repair resources to make the schedule more robust
and flexible (Branke & Mattfeld, 2002, 2005; Davenport, Gefflot, & Beck, 2001) is the
first approach. For example, Branke and Mattfeld (2002, 2005) propose an anticipatory
scheduling algorithm to predict the future job arrivals in a dynamic scheduling problem. A
secondary objective, called flexibility, is included within each static sub-problem to penalize
the early idleness of the machines. They experimentally show that this approach improves
the system performance. Their conclusion is consistent with our observation in Section
4.2.2 on the positive relationship between repairing the aircraft earlier and achieving a
higher coverage. It would be therefore interesting to adjust the MIP model such that a
flexibility term is added in the objective function to quantify the value of making the repair
resources available as early as possible. However, it appears that none of the existing work
on such slack-based techniques uses analytical reasoning to decide the amount of slack or
level of penalization of early slack that should be used for different levels of stochasticity.
Modeling each static sub-problem as a two-stage stochastic programming is the second
approach (Birge & Louveaux, 1997). The first stage decision corresponds to constructing the
repair schedule and occurs before aircraft failures in pre- and post-flight checks. The second
stage decision, which includes the allocation of aircraft to flights, occurs after the pre-flight
s as the number of aircraft of type k assigned to fly in
checks. One approach is to define Zkw
wave w under scenario s. Each scenario s represents a possible realization of aircraft failures
64

fiScheduling a Dynamic Aircraft Repair Shop

in the horizon of the static sub-problem
p(s). Therefore, the objective
P with probability
P P
s . The main modeling challenge
function (Equation 1) can be written as s p(s) k w Zkw
is then to calculate the probability of each scenario. As already explained in Section 2.1, the
uncertainty in our problem is not exogenous information and is dependent on the first-stage
decisions which is hard to represent it in a closed and tractable form. Computationally,
two-stage stochastic programming models are substantially more challenging than most
discrete optimization problems (Dyer & Stougie, 2003) and therefore the ability to solve
such models of our problem to optimality is doubtful.
The third approach is to use multi-stage dynamic programming for solving the dynamic
repair shop problem (Iravani et al., 2007). The goal is to construct a repair schedule at
each decision epoch, marked by the arrival of newly failed aircraft to the repair shop,
such that the coverage is maximized over the long term. Using the dynamic programming
framework, the state of the repair shop at each decision time point is a tuple of the aircraft
failure rates, the aircraft processing times, and the aircraft resource requirements. The
decision or action is to assign start-times to the failed aircraft in the repair shop. There
are several challenges in modeling the problem as a classical dynamic program. First,
the expected wave coverage as a result of the current state and the action taken cannot
be represented in a closed form expression because of the combinatorics involved in the
scheduling problem. Second, the probabilities with which the repair shop transitions to a
new state at the next decision epoch as a result of the current state, the action taken, and
the revealed uncertainty on the aircraft failures are not known and are hard to calculate
mainly, again, due to the combinatorics of the scheduling decisions and the fact that the
processing times and the resource requirements of the repair operations become known
upon aircraft failure. The challenges indicate that the analytical tools of the classical
dynamic programming methodology cannot be used in modeling our problem. However, AI
techniques which have a broader scope of applicability such as machine learning (Sutton &
Barto, 1998), online stochastic combinatorial optimization (Van Hentenryck & Bent, 2006),
and hindsight optimization (Burns, Benton, Ruml, Yoon, & Do, 2012) can be investigated
as potential approaches in future work.
Extending the scheduling problem Although our results are demonstrated for a specific class of scheduling problems where the only constraint is the repair capacity limit, our
solution approach can be adapted for more complex scheduling problems. More specifically,
the proposed MIP and CP algorithms for the static sub-problem can be easily extended
to handle other types of scheduling constraints such as precedence constraints. However,
modeling the problem via the decomposition approach would require additional effort. The
existence of the precedence constraints among the repair activities of a failed aircraft makes
the scheduling of different repair resources dependent. Therefore, a separate RSSP for each
repair resource cannot be defined. One possible idea is to represent the scheduling problem
as a single sub-problem where appropriate relaxation and Benders cut can be developed.
Taking another perspective, we decomposed the problem into stochastic long-term planning and deterministic short-term scheduling problems in DAMP and RSSP, respectively.
In the long-term plan, we deal with the uncertainty using the known information on the
probability of aircraft failures to trade-off the coverage between the flights. In the short65

fiAramon Bajestani & Beck

term schedule, we construct a feasible repair schedule to achieve the coverage as decided
in the long-term plan. We then designed different rescheduling policies to investigate how
the information revealed over time can be effectively used to adjust the repair schedule and
change the long-term plan. We conclude that changing the long-term plan based on shortterm information which cannot be completely incorporated in the long-term plan from the
beginning significantly increases the utilization of the airplanes.
A typical hierarchical optimization approach does not deal with the interdependency between different levels of decision makings. However a communication technique as designed
in this paper, capable of utilizing the short-term deterministic scheduling in the long-term
stochastic planning is more likely to lead to higher system performance. In the problem
addressed here, the assignment of the aircraft to the flights and the scheduling of the repair
activities in the repair shop represent the long-term and the short-term reasoning, respectively. As a wide range of operational decisions can be viewed as integrated optimization
problems, the pattern of the algorithms designed here might be applicable. Combining
maintenance or inventory planning with job production scheduling is an example of the
integrated operational problem where at the higher level of decision-making, the maintenance or inventory policy is determined whereas at the lower level, the jobs are scheduled
(Terekhov, Dogru, Ozen, & Beck, 2012; Aramon Bajestani, 2013). If the maintenance or
inventory policy leads to an infeasible scheduling problem at the lower level, then the higher
level needs to be informed with this information so that the inventory or maintenance decisions are adjusted. Therefore, the overall approach demonstrated here has applications to
other problems typically solved with a hierarchical optimization approach.

6. Conclusion
In this paper, we address the problem of scheduling a dynamic repair shop in the context
of aircraft fleet management. The goal is to maximize the flight coverage over a longterm by considering the repair capacity and the aircraft failures. The number of failed
aircraft dynamically changes because of aircraft breakdowns. Our proposed solution solves
the dynamic problem as successive static scheduling problems over shorter time periods.
Several scheduling algorithms and different rescheduling policies are proposed to schedule
the repair activities online with dynamic reaction to the aircraft failures. The length of
the scheduling horizon and the frequency of rescheduling are the features defining our three
policies.
The computational results show that an optimization approach using logic-based Benders decomposition, scheduling over a longer horizon, incorporating the mean of known
information on aircraft failures, and adjusting the repair schedule as soon as new jobs enter
the repair shop yield higher mean coverage and is a reasonable approach to balance against
different uncertain scenarios.
Developing richer solution approaches to better handle the uncertainty is a challenging
and interesting topic to pursue in future work. However, within our solution technique,
establishing a formal framework to exactly determine how long into the future we should
plan ahead and how quickly we should change our schedule based on the new information
is also a very interesting direction for the future work. The answer of these two questions
seems to be highly dependent on the arrival rate of the new information and the impact of
66

fiScheduling a Dynamic Aircraft Repair Shop

the new information on the overall system performance. How to quantify these two values
can be the starting point to provide solid responses to these two questions.

Acknowledgments
This research was supported in part by the Natural Sciences and Engineering Research
Council of Canada (NSERC) and the consortium members of Centre for Maintenance Optimization & Reliability Engineering (C-MORE). Thanks to Nima Safaei and Dragan Banjevic
for introducing the problem to us and subsequent discussions. This paper is a combined and
extended version of a conference paper (Aramon Bajestani & Beck, 2011b) and a workshop
paper (Aramon Bajestani & Beck, 2011a) that have already appeared.

References
Aramon Bajestani, M. (2013). Integrating Maintenance Planning and Production Scheduling: Making Operational Decisions with a Strategic Perspective. Ph.D. thesis, Department of Mechanical & Industrial Engineering, University of Toronto, Canada.
Forthcoming.
Aramon Bajestani, M., & Beck, J. C. (2011a). Scheduling a dynamic aircraft repair shop. In
Proceedings of the ICAPS2011 Workshop on Scheduling and Planning Applications.
Aramon Bajestani, M., & Beck, J. C. (2011b). Scheduling an aircraft repair shop. In
Proceedings of the Twenty-First International Conference on Automated Planning and
Scheduling (ICAPS2011), pp. 1017.
Aytug, H., Lawley, M., McKay, K., Mohan, S., & Uzsoy, R. (2005). Executing production
schedules in the face of uncertainties: A review and future directions. European Journal
of Operational Research, 161, 86110.
Baptiste, P., Pape, C. L., & Nuijten, W. (2001). Constraint-based Scheduling. Kluwer
Academic Publishers, Boston/Dordrecht/London.
Beck, J. C. (1999). Texture Measurements as a Basis for Heuristic Commitment Techniques
in Constraint-Directed Scheduling. Ph.D. thesis, Department of Computer Science,
University of Toronto, Canada.
Beck, J. C. (2010). Checking-up on branch-and-check. In Proceedings of the Sixteenth International Conference on Principles and Practice of Constraint Programming (CP2010),
pp. 8498.
Beck, J. C., Davenport, A. J., Davis, E. D., & Fox, M. S. (1998). The ODO project: Toward
a unified basis for constraint-directed scheduling. Journal of Scheduling, 1, 89125.
Beck, J. C., & Refalo, P. (2003). A hybrid approach to scheduling with earliness and
tardiness costs. Annals of Operations Research, 118, 4971.
Benders, J. (1962). Partitioning procedures for solving mixed-variables programming problems. Numerische Mathematik, 4, 238252.
Bidot, J., Vidal, T., Laborie, P., & Beck, J. C. (2009). A theoretical and practical framework
for scheduling in a stochastic environment. Journal of Scheduling, 12, 315344.
67

fiAramon Bajestani & Beck

Birge, J. R., & Louveaux, F. (1997). Introduction to Stochastic Programming. Springer
Verlag, New York, USA.
Branke, J., & Mattfeld, D. C. (2002). Anticipatory scheduling for dynamic job shop problems. In Proceedings of the ICAPS2002 Workshop on On-line Planning and Scheduling, pp. 310.
Branke, J., & Mattfeld, D. C. (2005). Anticipation and flexibility in dynamic scheduling.
International Journal of Production Research, 43, 31033129.
Budai, G., Huisman, D., & Dekker, R. (2006). Scheduling preventive railway maintenance
activities. Journal of the Operational Research Society, 57, 10351044.
Burns, E., Benton, J., Ruml, W., Yoon, S., & Do, M. B. (2012). Anticipatory on-line planning. In Proceedings of the Twenty-Second International Conference on Automated
Planning and Scheduling (ICAPS2012), pp. 333337.
Chu, Y., & Xia, Q. (2004). Generating Benders cuts for a general class of integer programming problems. In Proceedings of the First International Conference on the Integration
of AI and OR Techniques in Constraint Programming (CPAIOR2004), pp. 127136.
Cohen, P. R. (1995). Empirical Methods for Artificial Intelligence. The MIT Press, Cambridge, Mass.
Cowling, P., & Johansson, M. (2002). Using real time information for effective dynamic
scheduling. European Journal of Operational Research, 139, 230244.
CPLEX (2011). IBM ILOG CPLEX 12.3 Users Manual. IBM ILOG.
http://pic.dhe.ibm.com/infocenter/cosinfoc/v12r3/index.jsp.

Available at

Davenport, A. J., Gefflot, C., & Beck, J. C. (2001). Slack-based techniques for robust
schedules. In Proceedings of the Sixth European Conference on Planning (ECP-2001).
Derman, C., Lieberman, G. J., & Ross, S. M. (1980). On the optimal assignment of servers
and a repairman. Journal of Applied Probabilities, 19, 577581.
Dyer, M., & Stougie, L. (2003). Computational complexity of stochastic programming
problems. Spor-report 2003-20, Department of Mathematics and Computer Science,
Eindhoven Technical University, Eindhoven.
Fazel-Zarandi, M. M., & Beck, J. C. (2012). Using logic-based Benders decomposition
to solve the capacity- and distance-constrained plant location problem. INFORMS
Journal on Computing, 24, 399415.
Frost, D., & Dechter, R. (1998). Optimizing with constraints: A case study in scheduling
maintenance of electric power units. Lecture Notes in Computer Science, 1520, 469
488.
Geoffrion, A. M., & Graves, G. W. (1974). Multicommodity distribution system design by
Benders decomposition. Management Science, 20, 822844.
Grigoriev, A., van de Klundert, J., & Spieksma, F. C. R. (2006). Modeling and solving
the periodic maintenance problem. European Journal of Operational Research, 172,
783797.
68

fiScheduling a Dynamic Aircraft Repair Shop

Haghani, A., & Shafahi, Y. (2002). Bus maintenance systems and maintenance scheduling:
Model formulations and solutions. Transportation Research Part A, 36, 453482.
Haque, L., & Armstrong, M. J. (2007). A survey of the machine interference problem.
European Journal of Operational Research, 179, 469482.
Heinz, S., & Beck, J. C. (2012). Reconsidering mixed integer programming and MIPbased hybrids for scheduling. In Proceedings of the Ninth International Conference on
Integration of AI and OR Techniques in Constraint Programming for Combinatorial
Optimization Problems (CPAIOR2012), pp. 211227.
Hooker, J. (2005). A hybrid method for planning and scheduling. Constraints, 10, 385401.
Hooker, J. (2007). Planning and scheduling by logic-based Benders decomposition. Operations Research, 55, 588602.
Hooker, J., & Ottosson, G. (2003). Logic-based Benders decomposition. Mathematical
Programming, 96, 3360.
Hooker, J., & Yan, H. (1995). Logic circuit verification by Benders decomposition. In
Saraswat, V., & Van Hentenryck, P. (Eds.), Principles and Practice of Constraint
Programming: The Newport Papers, chap. 15, pp. 267288. MIT Press.
Iravani, S. M. R., Krishnamurthy, V., & Chao, G. H. (2007). Optimal server scheduling in
nonpreemptive finite-population queueing systems. Queueing System, 55, 95105.
Kozanidis, G., Gavranis, A., & Kostarelou, E. (2012). Mixed integer least squares optimization for flight and maintenance planning of mission aircraft. Naval Research Logistics,
59, 212229.
Liu, S. Q., Ong, H. L., & Ng, K. M. (2005). Metaheuristics for minimizing the makespan of
the dynamic shop scheduling problem. Advances in Engineering Software, 36, 199205.
ODonovan, R., Uzsoy, R., & McKay, K. N. (1999). Predictable scheduling of a single
machine with breakdowns and sensitive jobs. International Journal of Production
Research, 37, 42174233.
Ovacik, I. M., & Uzsoy, R. (1994). Rolling horizon algorithms for a single-machine dynamic
scheduling problem with sequence-dependent setup times. International Journal of
Production Research, 32, 12431263.
Ovacik, I. M., & Uzsoy, R. (1995). Rolling horizon procedures for dynamic parallel machine
scheduling with sequence-dependent setup times. International Journal of Production
Research, 33, 31733192.
Pinedo, M. (2002). Scheduling: Theory, Algorithms, and Systems (2nd edition). Prentice
Hall, New Jersey, USA.
Pinedo, M. (2005). Planning and Scheduling in Manufacturing and Services. Springer Series
in Operations Research. Springer.
Queyranne, M., & Schulz, A. (1994). Polyhedral approaches to machine scheduling problems. Tech. rep. 408/1994, Department of Mathematics, Technische Universitat Berlin,
Germany. revised in 1996.
69

fiAramon Bajestani & Beck

Sabuncuoglu, I., & Bayiz, M. (2000). Analysis of reactive scheduling problems in a job shop
environment. European Journal of Operational Research, 126, 567586.
Safaei, N., Banjevic, D., & Jardine, A. K. S. (2010). Workforce constrained maintenance
scheduling for aircraft fleet: A case study. In Proceedings of Sixteenth ISSAT International Conference on Reliability and Quality in Design, pp. 291297.
Safaei, N., Banjevic, D., & Jardine, A. K. S. (2011). Workforce-constrained maintenance
scheduling for military aircraft fleet: A case study. Annals of Operations Research,
186, 295316.
Schwartz, N. (2012). Balancing risk: Readiness, force structure, and modernization. In
CSAF remarks at 2012 Air Force Reserve Senior Leader Conference. Available at
http://www.af.mil/shared/media/document/AFD-120611-028.pdf.
Stecke, K. E. (1992). Machine Interference: Assignment of Machines to Operators. Handbook
of Industrial Engineering (2nd edition). John Wiley & Sons.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, Mass.
Terekhov, D., Beck, J. C., & Brown, K. N. (2009). A constraint programming approach for
solving a queueing design and control problem. INFORMS Journal on Computing,
21, 546561.
Terekhov, D., Dogru, M. K., Ozen, U., & Beck, J. C. (2012). Solving two-machine assembly scheduling problems with inventory constraints. Computers and Industrial
Engineering, 63, 120134.
Van Hentenryck, P., & Bent, R. (2006). Online Stochastic Combinatorial Optimization.
MIT Press.
Vieira, G. E., Hermann, J. W., & Lin, E. (2000). Predicting the performance of rescheduling
strategies for parallel machine systems. Journal of Manufacturing Systems, 19, 256
266.
Vieira, G. E., Hermann, J. W., & Lin, E. (2003). Rescheduling manufacturing systems: A
framework of strategies, policies and methods. Journal of Scheduling, 6, 3692.
Vinod, V., & Sridharan, R. (2011). Simulation modeling and analysis of due-date assignment
methods and scheduling decision rules in a dynamic job shop production system.
International Journal of Production Economics, 129, 127146.
Wagner, H. M., Giglio, R. J., & Glaser, R. G. (1964). Preventive maintenance scheduling
by mathematical programming. Management Science, 10, 316334.
Wang, H. (2002). A survey of maintenance policies of deteriorating systems. European
Journal of Operational Research, 139, 469489.

70

fi