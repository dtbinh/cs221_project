Journal of Artificial Intelligence Research 19 (2003) 469-512

Submitted 12/02; published 10/03

Temporal Decision Trees:
Model-based Diagnosis of Dynamic Systems On-Board
Luca Console
Claudia Picardi

luca.console@di.unito.it
claudia.picardi@di.unito.it

Dipartimento di Informatica, Universita di Torino,
Corso Svizzera 185, I-10149, Torino, Italy

Daniele Theseider Dupre

dtd@mfn.unipmn.it

Dipartimento di Informatica, Universita del Piemonte Orientale
Spalto Marengo 33, I-15100, Alessandria, Italy

Abstract
The automatic generation of decision trees based on o-line reasoning on models of
a domain is a reasonable compromise between the advantages of using a model-based approach in technical domains and the constraints imposed by embedded applications. In this
paper we extend the approach to deal with temporal information. We introduce a notion
of temporal decision tree, which is designed to make use of relevant information as long as
it is acquired, and we present an algorithm for compiling such trees from a model-based
reasoning system.

1. Introduction
The embedding of software components inside physical systems became widespread in the
last decades due to the convenience of including electronic control into the systems themselves. This phenomenon occurs in several industrial sectors, ranging from large-scale products such as cars to much more expensive systems like aircraft and spacecrafts.
The case of automotive systems is paradigmatic. In fact, the number and complexity
of vehicle subsystems which are managed by software control increased signicantly since
the mid 80s and will further increase in the next decades (see Foresight-Vehicle, 2002), due
to the possibility of introducing, at costs that are acceptable for such wide scale products,
more exibility in the systems, for e.g. increased performance and safety, and reduced
emissions. Systems such as fuel injection control, ABS (to prevent blockage of the wheels
while braking), ASR (to avoid slipping wheels), ESP (controlling the stability of the vehicle),
would not be possible at feasible costs without electronic control.
The software modules are usually installed on dedicated Electronic Control Units (ECUs)
and they play a very important role since they have complete control of a subsystem: human control becomes simply an input to the control system, together with inputs from
appropriate sensors. For example, the position of the accelerator pedal is an input to the
ECU which controls fuel delivery to the injectors.
A serious problem with these systems is that the software must behave properly also in
presence of faults and must guarantee high levels of availability and safety for the controlled
system and for the vehicle. The controlled systems, in fact, are in many cases safety
critical: the braking system is an obvious example. This means that monitoring the systems

c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiConsole, Picardi, & Theseider Dupre

behaviour, detecting and isolating failures, performing the appropriate recovery actions is
a critical task that must be performed by control software. If any problem is detected or
suspected the software must react, modifying the way the system is controlled, with the
primary goal of guaranteeing safety and availability. According to recent estimates, about
75% of the ECU software deals with detecting problems and performing recovery actions,
that is to the tasks of diagnosis and repair (see again Foresight-Vehicle, 2002).
Thus the design of the diagnostic software is a very critical and time consuming activity, which is currently performed manually by expert engineers who use their knowledge
to perform the Failure Mode and Eect Analysis (FMEA) 1 and dene diagnostic and
recovery strategies.
The problem is complex and critical per-se, but it is made even more dicult by a
number of other issues and constraints that have to be taken into account:
 The resources that are available on-board must be limited, in terms of memory and
computing power, to keep costs low. This has to be combined with the problem
that near real time performance is needed, especially in situations that may be safety
critical. For example, for direct injection fuel delivery systems, where fuel is maintaned
at a very high pressure (more than 1000 bar) there are cases where the system must
react to problems within a rotation of the engine (e.g. 15 milliseconds at 4000 rpm),
to prevent serious damage of the engine and danger to passengers. In fact, a fuel
leakage can be very dangerous if it comes from a high pressure line. In this case it
is important to distinguish whether a loss of pressure is due to such a leak, in order
to activate some emergency action (for example, stop the engine), or to some other
failure which can simply be signalled to the user.
 In order to keep costs acceptable for a large scale product, the set of sensors available
on board is usually limited to those necessary for controlling the systems under their
correct behaviour; thus, it is not always easy to gure out the impact that faults may
have on the quantities monitored by the sensors, whose physical, logical and temporal
relation to faults may be not straightforward.
 The devices to be diagnosed are complex from the behavioural point of view: they have
a dynamic and time-varying behaviour; they are embedded in complex systems and
they interact with other subsystems; in some cases the control system automatically
compensates deviations from the nominal behaviour.
These aspects make the design of software modules for control and diagnosis very challenging but also very expensive and time consuming. There is then a signicant need for
improving this activity, making it more reliable, complete and ecient through the use of
automated systems to support and complement the experience of engineers, in order to meet
the growing standards which are required for monitoring, diagnosis and repair strategies.
Model-based reasoning (MBR) proved to be an interesting opportunity for automotive
applications and indeed some applications to real systems have been experimented in the
1. The result of FMEA is a table which lists, for each possible fault of each component of a system, the
eect of the faults on the component and on the system as a whole and the possible strategy to detect the
faults. This table is compiled manually by engineers, based on experience knowledge and on a blueprint
of the system.

470

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

90s (e.g., Cascio & Sanseverino, 1997; Mosterman, Biswas, & Manders, 1998; Sachenbacher,
Malik, & Struss, 1998; Sachenbacher, Struss, & Weber, 2000). The type of models adopted
in MBR are conceptually not too far away from those adopted by engineers. In particular,
the component oriented approach typical of MBR ts quite well with the problem of dealing
with several variants of systems, assembled starting from the same set of basic components.
For a more thorough discussion of the advantages of the MBR approach, see Console and
Dressler (1999).
Most of the applications developed so far, however, concentrated on o-board diagnosis,
that is diagnosis in the workshop, and not on on-board diagnosis. The case of on-board
systems seems to be more complicated since, due to the restrictions on hardware to be
put on-board, it is still questionable if diagnostic systems can be designed to reason on
rst-principle models on-board. For this reason other approaches have been developed
in order to exploit the advantages of MBR also in on-board applications. In particular,
a compilation-based scheme to the design of on-board diagnostic systems for vehicles was
experimented in the Vehicle Model Based Diagnosis (VMBD) BRITE-Euram Project (199699), and applied to a Common-rail fuel injection system (Cascio, Console, Guagliumi, Osella,
Sottano, & Theseider, 1999). In this approach a model-based diagnostic system is used to
generate a compact on-board diagnostic system in the form of a decision tree. Similarly,
automated FMEA reports generated by Model-Based Reasoning in the Autosteve system
can be used to generate diagnostic decision trees (Price, 1999). Yet similar is the idea
proposed by Darwiche (1999), where diagnostic rules are generated from a model in order
to meet resource constraints.
These approaches have interesting advantages. On the one hand, they share most of
the benets of model-based systems, such as relying on a comprehensive representation
of the system behaviour and a well dened characterization of diagnosis. On the other
hand, decision trees and other compact representations make sense for representing onboard diagnostic strategies, being ecient in space and time. Furthermore, algorithms
for synthesizing decision trees from examples are well established in the machine learning
community. In this specic case the examples are the solutions (diagnoses and recovery
actions) generated by a model-based system.
However, the basic notion of decision tree and the approaches for learning such trees
from examples have a major limitation for our kind of applications: they do not cope
properly with the temporal behaviour of the systems to be diagnosed, and, in particular,
with the fact that incremental discrimination of possible faults, leading to a nal decision
on an action to be taken on-board, should be based on observations acquired across time,
thus taking into account temporal patterns.
For such a reason, in the work described in this paper we introduce a new notion of
decision tree, the temporal decision tree, which takes into account the temporal dimension,
and we introduce an algorithm for synthesizing temporal decision trees.
Temporal decision trees extend traditional decision trees in the fact that nodes have a
temporal label which species when a condition should be checked in order to select one
of the branches or to make a decision. As we shall see, this allows taking into account
that in some cases the order and the delay between observable measures inuences the
decision to be made and thus provides important power to improve the decision process.
Waiting, however, is not always possible and thus the generation of the trees includes a
471

fiConsole, Picardi, & Theseider Dupre

notion of deadline for each possible decision. Thus, the temporal decision process supports
the possibility of selecting the best decision, exploiting observations and their temporal
locations (patterns) and taking into account that in some cases at some point a decision
has to be taken anyway to prevent serious problems.
The rest of the paper is organized as follows. In section 2 we summarize some basic
ideas about model-based diagnosis (MBD), the use of decision trees in conjunction with it,
and the temporal dimension in MBD and in decision trees. In section 3 we provide basic
formal denitions about decision trees, which form the basis for their extension to temporal
decision trees in section 4. We then describe (section 5) the problem of synthesizing temporal
decision trees and our solution (section 6).

2. Model-based Diagnosis and Decision Trees
In this section we briey recall the basic notions of model-based diagnosis and we discuss
how decision trees can be used for diagnostic purposes, focusing on how they have been
used in VMBD in conjunction with the model-based approach (Cascio et al., 1999).
2.1 The Atemporal Case
First of all let us sketch the atemporal case and the traditional use of diagnostic decision
trees.
2.1.1 Atemporal model-based diagnosis
The starting point for model-based diagnosis is a model of the structure and behaviour of
the device to be diagnosed. More specically, we assume a component centered approach
in which:
 A model is provided for each component type; a component is characterized by
 A set of variables (with a distinguished set of interface variables);
 A set of modes, including an ok mode (correct behaviour) and possibly a set of
fault modes.
 A set of relations involving component variables and modes, describing the behaviour of the component in such a mode. These relations may model the correct
behaviour of the device and, in some cases, the behaviour in presence of faults
(faulty behaviour).
 A model for the device is given as a list of the component instances and of their
connections (connections between interface variables).
In the Articial Intelligence approach, models are usually qualitative, that is the domain
of each variable is a nite set of values. Such an abstraction has proven to be useful for
diagnostic purposes.
The model can be used for simulating the behaviour of a system and then for computing
diagnoses. In fact, given a set of observations about the system behaviour, diagnoses can
be determined after comparing the behaviour predicted by the model (in normal conditions
or in the presence of single or multiple faults) and the observed behaviour.
472

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

In order for the model to be useful for on-board diagnosis, for each fault mode F (or
combination of fault modes) the model must include the recovery action the control software
should perform in case F occurs. In general these actions have a cost, mainly related to the
resulting reduction of functionality of the system. Moreover, two actions a1 and a2 can be
related in the following sense:
 a1 , the recovery action associated with fault F1 , carries out operations that include
those performed by a2 , the recovery action associated with F2 ;
 a1 can be used as a recovery action also when F2 occurs; it may however carry out
unneeded operations, thus reducing the system functionality more than strictly necessary.
 However, in case we cannot discriminate between F1 and F2 , applying a1 is a rational
choice.
In section 4.3 we will present a model for actions which formalizes this relation.
Thus the main goal of the on-board diagnostic procedure is to decide the best action
to be performed, given the observed malfunction. This type of procedure can be eciently
represented using decision trees.
2.1.2 Decision trees
Decision trees can be used to implement classication problem solving and thus some form
of diagnostic procedure. Each node of the tree corresponds to an observable. In on-board
diagnosis, observables correspond either to direct sensor readings, or to the results of computations carried out by the ECUs on the available measurements. In the following the
word sensor will denote both types of observables; it is worth pointing out that the latter
may require some time to be performed. In this paper we mainly assume that a sensor
reading takes no time; however the apporach we propose deals also with the case in which a
sensor reading is time consuming, as pointed out in section 5.1. A node can have as many
descendants as the number of qualitative values associated with the sensor. The leaves of
the tree correspond to actions that can be performed on board. Thus, given the available
sensor readings, the tree can be very easily used to make a decision on the recovery action
to be performed.
Such decision trees can be generated automatically from a set of examples or cases.
By example here we mean a possible assignment of values to observables and the corresponding diagnosis, or possible alternative diagnoses, and a selected recovery action which
is appropriate for such a set of suspect diagnoses. This set can be produced using a modelbased diagnostic systems, which, given a set of observables can compute the diagnoses and
recovery actions.
In the atemporal case, with nite qualitative domains, the number of possible combinations of observations is nite, and usually small, therefore considering all cases exhaustively
(and not just a sample) is feasible and there are two equivalent ways of building such an
exhaustive set of cases:
1. Simulation approach: for each fault F , we run the model-based system to predict the
observations corresponding to F .
473

fiConsole, Picardi, & Theseider Dupre

2. Diagnostic approach: we run a diagnosis engine on combinations (all relevant combinations) of observations, to compute the candidate diagnoses for each one of these
cases.
In either case, the resulting decision tree contains the same information as the set of cases;
if, once sensors are placed in the system, observations have no further cost, the decision
tree is just a way to save space with respect to a table, and speed up lookup of information.
In this way the advantages of the model-based approach and of the use of compact decision trees can be combined: the model-based engine produces diagnoses based on reusable
component models and can be used as the diagnoser o-board; compact decision trees,
synthesized from cases classied by the model-based engine, can be installed on-board.
2.2 Towards Temporal Decision Trees
In this section we briey recall some basic notions on temporal model-based diagnosis (see
Brusoni, Console, Terenziani, & Theseider Dupre, 1998 for a general discussion on temporal
diagnosis), and we informally introduce temporal decision trees.
2.2.1 Temporal MBD
The basic denition of MBD is conceptually similar to the atemporal case. Let us consider
the main dierences.
As regards the model of each component type we consider a further type of variable:
state variables used to model the dynamic behaviour of the component. The set of relations
describing the behavior of the component (for each mode) is augmented with temporal
information (constraints); we do not make specic assumptions on the model of time, even
though, as we shall see in the following, this has an impact on the cases which can be
considered for the tree generation. As an example, these constraints may specify a delay
between an input and an output or in the change of state of the component.
As regards recovery actions, a deadline for performing the action must be specied; this
represents the maximum time that can elapse between fault detection and the recovery action; this is the amount of time available for the control software to perform discrimination.
This piece of information is specic to each component instance, rather than component
type, because the action and the deadline are related to the potential unacceptable eects
that a fault could have on the overall system; the same fault of the same component type
could be very dangerous for one instance and tolerable for another.
Diagnosis is started when observations indicate that the system is not behaving correctly.
Observations correspond to (possibly qualitative) values of variables across time. In general,
in the temporal case a diagnosis is an assignment of a mode of behaviour to component
instances across time such that the observed behaviour is explained by the assignment given
the model. For details on dierent ways of dening explanation in this case see Brusoni et
al. (1998). For the purposes of this paper we are only interested in the fact that, given a
set of observables, a diagnosis (or a set of candidate diagnoses if no complete discrimination
is possible) can be computed and a recovery action is determined.
This means that the starting point of our approach is a table containing the results of
running the model-based diagnostic system on a set of cases, (almost) independently of the
model-based diagnostic system used for generating the table.
474

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

We already mentioned that in the static case, with nite qualitative domains, an exhaustive set of cases can be considered. In the temporal case, if the model of time is purely
qualitative, a table with temporal information cannot be built by prediction, while it can be
built running the diagnosis engine on a set of cases with quantitative information: diagnoses
which make qualitative predictions that are inconsistent with the quantitative information
can be ruled out. Of course, this cannot in general be done exhaustively, even if observations are assumed to be acquired at discrete times; if it is not, the decision tree generation
will actually be learning from examples.
Thus a simulation approach can only be used in case the temporal constraints in the
model are precise enough to predict at least partial information on the temporal location
of the observations, e.g., in case the model includes quantitative temporal constraints.
The diagnostic approach can be used also in case of weaker (qualitative) temporal constraints in the model.
As regards the observations, we consider the general case where a set of snapshots is
available; each snapshot is labelled with the time of observation and reports the value of the
sensors (observables) at that time. This makes the approach suitable for dierent notions of
time in the underlying model and on the observations (see again the discussion in Brusoni
et al., 1998).
Example 1 The starting point for generating a temporal decision tree is a table like the
one in Figure 1.

sit1
sit2
sit3
sit4
sit5
sit6
sit7
sit8

0
n
h
n
n
h
n
h
h

1
n
h
n
n
h
n
h
h

2
n
h
n
n
h
n
h
h

s1
3 4 5 6 7 0
n h h
l
h
n h h h h l
h h h h
l
h
h
h h h
l
h h h
l
h h h
h

1
v
n
l
l
n
v
l
h

2
v
n
l
l
n
v
n
n

s2
3 4 5 6 7 0
v v v
n
n
l v v v v n
l l l v
n
n
n
z z z
n
n l v
n
n l l
n

1
n
n
n
n
n
n
n
n

2
n
n
n
h
n
n
n
n

s3
3 4 5 6 7 Act Dl
l l l
a
5
b
2
l l l v v b
7
h h h h
c
6
l
c
3
l l v
d
5
l l v
b
5
l v z
c
5

Figure 1: An example of a set of cases for learning temporal decision trees.
Each row of the table corresponds to a situation (case or example in the terminology
of machine learning) and it reports:
 For each sensor si the values that have been observed at each snapshots (in the example
we have 8 snapshots, labelled as 0 to 7); n, l, h and v correspond to the qualitative
values of the sensor measurements; n for normal, l for low, h for high, v for very low
and z for zero.
 The recovery action Act to be performed in that situation.
 The deadline Dl for performing such an action.



A table as the one in the above example represents a set of situations that may be
encountered in case of faults and, as noticed above, it can be generated using either a
475

fiConsole, Picardi, & Theseider Dupre

diagnostic or a simulation approach. In the next section we shall introduce the notion of
temporal decision trees and show how the pieces of information about sensor histories like
those in the table above can be exploited in the generation of such trees.
2.2.2 Introduction to temporal decision trees
Traditional decision trees do not include a notion of time, i.e., the fact that data may
be observable at dierent times or that dierent faults may be distinguished only by the
temporal patterns of data. Thus, neglecting the notion of time may lead to limitations in
the decision process.
For such a reason in this work we introduce a notion of temporal decision tree. Let us
analyse the intuition behind temporal decision trees and the decision process they support.
Formal denitions will be provided later in the paper.
Let us consider, for example, the fault situations sit3 and sit4 in Figure 1, and let
us assume, for the sake of simplicity, that the only available sensor is s2 . The two fault
situations have to be distinguished in the control software because they require dierent
recovery actions. Both of them can be detected by the fact that s2 shows a low value.
Moreover, in both situation after a while s2 starts showing a very low value.
The only way to discriminate these two situations is to make use of temporal information,
that is to exploit the fact that in sit3 value v shows up after 4 time units from fault detection,
while in sit4 the same value shows up after 6 time units.
In order to take into account this in a decision tree, we have to include time into the
tree. In both examples, the best decision procedure is to wait after observing thst s2 = l
(that is, after dectecting that a fault has occurred). After 4 time units we can make a
decision, depending on whether s2 = v or not. This corresponds to the procedure described
by the tree in Figure 2.
s2
...
...

l
s2 after 4
l
sit4

v
sit3

Figure 2: A simple example of temporal decision tree
Obviously, waiting is not always the solution or is not always possible. In many cases, in
fact, safety or other constraints may impose some deadlines for performing recovery actions.
This has to be reected in the generation of the decision procedure. Suppose, in the example
above, that the deadline for sit3 was 3 rather than 6: in this case the two situations would
have been indistinguishable, beacause it would have been infeasible to wait 4 time units.

476

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

Thus, an essential idea for generating small decision trees in the temporal case is to
take advantage of the fact that in some cases there is nothing better than waiting2 , in order
to get a good discrimination, provided that safety and integrity of the physical system are
kept into account, and that deadlines for recovery actions are always met.
More generally, one should exploit all the information about temporal patterns of observables and about deadline, like the ones in Figure 1, to produce an optimal diagnostic
procedure.
Another idea we use in our approach is the integration of incremental discrimination,
which is the basis for the generation and traversal of a decision tree, with the incremental
acquisition of information across time.
In the atemporal case, the decision tree should be generated in order to guide the incremental acquisition of information: dierent subtrees of a node are relative to dierent sets
of faults and therefore may involve dierent measurements: in a subtree T we perform the
measurements that are useful for discriminating the faults compatible with the measurements that made us reach subtree T, starting from the root. For o-board diagnosis, this
allows reducing the average number of measurements to get to a decision (i.e. the average
depth of the tree), which is useful because measurements have a cost - e.g. time for an operator to take them from the system; for on-board diagnosis, even in case all measurements
are simply sensor readings, which have no cost once the sensor has been made part of the
system, we are interested in generating small decision trees to save space.
In the temporal case there is a further issue: the incremental acquisition of information
is naturally constrained by the ow of time. If we do not want to store sensor values across
time  which seems a natural choice since we have memory constraints  information must
be acquired when it is available and it is not possible to read sensors once the choice of
waiting has been made. This issue will be taken into account in the generation of temporal
decision trees.

3. Basic Notions on Decision Trees
Before moving to a formal denition of temporal decision trees, in this section we briey
recall some denitions and algorithms for the atemporal case. In particular, we recall the
standard ID3 algorithm (Quinlan, 1986), which will be the basis for our algorithm for the
temporal case. The denitions in this section are the standard ones (see any textbook on
Articial Intelligence, e.g., Russel & Norvig, 1995).
3.1 Decision Trees
We adopt the following formal denition of decision tree, which will be extended in section
4.1 to temporal decision trees.
Definition 1 Let us consider a decision process P where A is the set of available decisions,
O is the set of tests that can be performed on the external environment, and out(oi ) =
2. A dierent approach would be that of weighing the amount of elapsed time agains the possibility of
better discriminating faults; such an approach is something we are considering in the future work on
temporal decision trees, as outlined in section 7.

477

fiConsole, Picardi, & Theseider Dupre

{v1 , . . . , vki } are the possible outcomes of test oi  O. A decision tree for P is a labelled
tree structure T = r, N, E, L where:
 r, N, E is a tree structure with root r, set of nodes N and set of edges E  N  N ;
N is partitioned in a set of internal nodes NI and a set of leaves NL .
 L is a labelling function dened over N  E.
 If n  NI , L(n)  O; in other words each internal node is labelled with the name of a
test.
 If (n, c)  E then L((n, c))  out(L(n)); that is, an edge directed from n to c is
labelled with a possible outcome of the test associated with n.
 Moreover, if (n, c1 ), (n, c2 )  E and L((n, c1 )) = L((n, c2 )) then c1 = c2 , and for each
v  out(L(n)) there is c such that
(n, c)  E and L((n, c)) = v; that is, n has exactly one outgoing edge for each possible
outcome of test L(n).
 If l  NL , L(l)  A; in other words each leaf is labelled with a decision.



When the decision-making agent uses the tree, it starts from the root. Every time it reaches
an inner node n, the agent performs test L(n), observes its outcome v and follows the vlabelled edge. When the agent reaches a leaf l, it makes decision a = L(l).
3.2 Building Decision Trees
Figure 3 shows a generic recursive algorithm that can be used to build a decision tree
starting from a set of Examples and a set of Tests.
Recursion ends when either the remaining examples do not need further discrimination
because they all correspond to the same decision, or all available observables have been used,
and the values observed match cases with dierent decisions. In the latter case observables
are not enough for getting to the proper decision and if an agent is actually using the tree,
it should take into account this.
In case no terminating condition holds, the algorithm chooses an observable variable
test to become the root label for subtree T. Depending on how ChooseTest is implemented we get dierent specic algorithms and dierent decision trees.
A subtree is built for each possible outcome value of test in a recursive call of
BuildTree, with sets Tests Update and SubExamples as inputs. Tests Update is obtained by removing test from the set of tests, in order to avoid using it again. SubExamples
is the subset of Examples containing only those examples that have value as outcome for
test.
As mentioned before, there are as many specic algorithms, and, in general, results, as
there are implementations of ChooseTest. It is in general desirable to generate a tree
with minimum average depth, for two reasons:
 Minimizing average depth means minimizing the average number of tests and thus
speeding up the decision process.
 In machine learning, a small number of tests also means a higher degree of generalization on the particular examples used in building the tree.
478

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

function BuildTree (set Examples, set Tests)
returns a decision tree T = root, Nodes, Edges, Labels
begin
if all ex  Examples correspond to the same decision then
return BuildLeaf(Examples);
if Tests is empty then
return BuildLeaf(Examples);
test  ChooseTest (Tests, Examples);
root  new node;
Nodes  {root}; Edges  ; Labels(root)  test;
T  root, Nodes, Edges, Labels;
Tests Update  Tests \ {test};
for each possible outcome value of test do begin
SubExamples  {ex  Examples | test has outcome value in ex};
if SubExamples is not empty then begin
SubTree  BuildTree (SubExamples, Tests Update);
Append(T, root, SubTree);
Labels((root, Root(SubTree)))  value;
end;
end;
return T;
end.

Figure 3: Generic algorithm for building a decision tree
3.3 ID3
Unfortunately, nding a decision tree with minimum average depth is an intractable problem; however, there exists a good best-rst heuristic for choosing tests in order to produce
trees that are not too deep. This heuristic was proposed in the ID3 algorithm (Quinlan,
1986), and is base on the concept of entropy from information theory. In the following we
recall this approach in some detail also in order to introduce some notation which will be
used in the rest of the paper.
Definition 2 Given a (discrete) probability distribution P = {p1 , . . . , pn } its entropy E(P)
is:
(1)

E(P) = 

n


pi log2 pi

i=1


Entropy measures the degree of disorder. When we choose a test, we want it to split the
examples with the lowest degree of disorder with respect to the decisions associated with
them.
Given a set of examples E we introduce the sets:
E |a = {e  E | the decision associated with example e is a}.
If the set of available decisions is A = {a1 , . . . , an } then E |A = {E |a1 , . . . , E |an } is a partition of E.
479

fiConsole, Picardi, & Theseider Dupre

Definition 3 For each ai  A, i = 1, . . . , n, we dene the probability of ai with respect to
E as follows:
|(E |ai )|
P (ai ; E) =
|E|


It is worth noting that, if examples are endowed with their a priori probabilities, we can
redene P (ai ; E) in order to take them into account. The basic formulation of ID3 assumes
however that all examples are equiprobable and it computes the probability distribution
from the frequencies of the examples.
The entropy of E is:
E(E) = 

(2)

n


P (ai ; E) log2 P (ai ; E).

i=1

If all decisions are equiprobable, we get E(E) = log2 n, which is the maximum degree of
disorder for n decisions. If all decisions but one have probability equal to 0, then E(E) = 0:
the degree of disorder is minimum.
Entropy is used as follows for test selection. A test o with possible outcomes v1 , . . . , vk ,
splits the set of examples into:
E |ovi = {e  E | test o has value vi in e}.
E |o = {E |ov1 , . . . , E |ovk } is again a partition of E. In particular, if while building the
tree we choose test o, E |ovj is the subset of examples we use in building the subtree for
the child corresponding to vj . The lowest the degree of disorder in E |ovj , the closer we
are to a leaf. Therefore, following equation (2):
E(E |ovj ) = 

n


P (ai ; E |ovj ) log2 P (ai ; E |ovi ).

i=1

Finally, we dene the entropy of a test o as the average entropy on its possible outcomes:
Definition 4 The entropy of a test o with respect to a set of examples E is:
E(o; E) =

(3)

k


P (o  vj )E(E |ovj ),

j=1

where3 P (o  vj ) =

|(E |ovj )|
|E|



.

The ID3 algorithm simply consists of choosing the test with lowest entropy. Figure 4 shows
the implementation of ChooseTest that yields ID3.
3. Again, if examples are endowed with a priori probabilities, this denition can be changed in order to
take them into account

480

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

function ID3ChooseTest (set Tests, set Examples)
returns a test best test
begin
best test  any element in Tests;
min entropy  log2 |Examples|;
for each test  Tests do begin
part  Partition({Examples}, test);
ent  Entropy(part);
if (ent < min entropy) then begin
min entropy  ent;
best test  test;
end;
end;
return best test;
end.

Figure 4: ID3 implementation of ChooseTest

4. Extending Decision Trees
In this section we formally introduce the notion of temporal decision tree, and we show how
timing information can be added to the set of examples used in tree building. Moreover we
introduce a model for recovery action that expresses information needed by the algorithm.
4.1 Temporal Decision Trees
In section 2.2.2 we motivated the monotonicity requirement for temporal decision trees, so
that their traversal requires information relative to increasing time and then no information
has to be stored.
We now discuss how temporal information is actually included in the tree and matched
with temporal information on the observations. The tree has to be used when some abnormal value is detected for some sensor (fault detection). We then intend the time of fault
detection as the reference time for temporal labels of observations in the tree. If we look
for example at the data shown in Figure 1, we see that, for every fault situation, there is
always at least one sensor whose value at time 0 is dierent than nnormal. The reason is
that, for each fault situation, we associate a 0 time label to the rst snapshot in which there
is a sensor showing some deviation from nominal behaviour.
The following denition provides the extension for the temporal dimension in decision
trees.
Definition 5 A temporal decision tree is a decision tree r, NI , NL , E, L endowed with a
time-labelling function T such that:
(1) T : NI  IR+ ; we call T (n) a time label;
(2) if n  NI and there exist n such that (n, n )  E (in other words, n is child of n),

then T (n )  T (n).

481

fiConsole, Picardi, & Theseider Dupre

Since we assume not to store any information, but rather to use information for traversing the tree as dictated by the tree itself, a rst branching for discrimination is provided
depending on which is the sensor that provided such a value. We then assume to have
dierent temporal decision trees, one for each sensor which could possibly provide the rst
abnormal value, or, alternatively, that the root node has no time label, and the edges from
the root are not labelled with dierent values of a single observable, but with dierent
sensors which could provide the rst abnormal observation. Each tree, or subtree in the
second alternative, can be generated independently of the other ones, only using the examples where the sensor providing fault detection is the same. This generation is what will be
described in the rest of the paper.
Let us tree how a temporal decision tree (or forest, in the case of multiple detecting
sensors) can be exploited by an on-board diagnostic agent in order to choose a recovery
action. When the rst abnormal value is detected, the agent activates a time counter and
starts visiting the appropriate tree from the root. When it reaches an inner node n, the
agent retrieves both the associated test s = L(n) and the time label t = T (n). Then it
waits until the time counter reaches t, performs test s and chooses one of the child nodes
depending on the outcome. When the agent reaches a leaf, it performs the corresponding
recovery action.
With respect to the atemporal case, the agent has now the option to wait. From the
point of view of the agent it may seem pointless to wait when it could look at sensor values,
since reading sensor values has no cost. However, from the point of view of the tree things
are quite dierent: we do not want to add a test that makes the tree deeper and at the
same time is not necessary.
Condition (2) states that the agent can only move forward in time. This corresponds
to the assumption that sensor readings are not stored, discussed in section 2.2.2.
Example 2 Let us consider the diagnostic setting described in example 1. Figure 5 shows a
temporal decision tree for such setting, that is a temporal decision tree that uses the sensors
and recovery actions mentioned in Figure 1. If such a tree is run on the fault situations in

the table, a proper recovery action is selected within the deadline.

4.2 Adding Timing Information to the Set of Examples
In order to generate temporal decision trees, we need temporal information in the examples.
We already introduced informally the notion of a set of examples (or fault situations)
with temporal information when describing the table in Figure 1. The following denition
formalizes the same notion.
Definition 6 A temporal example-set (te-set for short) E is a collection of fault situations
sit1 , . . . , sitn characterized by a number of sensors sens1 , . . . , sensm and an ascending sequence of time labels t1 < . . . < tlast representing the instants in time for which sensor
readings are available. In this context we call observation a pair sensi , tj . A te-set is
organized in a table as follows:
(1) The table has n rows, one for each fault situation.
482

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

low
high

s1,0

normal

Action: b

Action: a

normal

s2,1

Actions: b,c
very low

very low

high

s2,0

s2,3

s2,1

Action: c

low
normal

zero

Action: d

high

Action: b

s3,2

high

Action: c

Figure 5: A temporal decision tree for the situations described in Figure 1.
(2) The table has m  last observation columns containing the outcomes of the dierent
observations for each fault situation. We denote by Val(sith , sensi , tj ) the value
measured by sensor sensi at time tj in fault situation sith .
(3) The table has a distinguished column Act containing the recovery action associated
with each fault situation. We denote by Act(sith ) the recovery action associated
with sith .
(4) The table has a second distinguished column Dl containing the deadline for each
fault situation. We denote by Dl(sith ) the deadline for sith , and we have that
Dl(sith )  {t1 , . . . , tlast } for each h = 1, . . . , n. We dene a global deadline for a
te-set S as Dl(S) = min{Dl(sit) | sit  S}.
4 P (sit; E) is associated with each sit  E , such that
We
 moreover assume that a probability


sitE P (sit; E) = 1. For every E  E and for every sit  E we introduce the following


notation: P (E ; E) = sitE P (sit; E) and P (sit; E ) = PP (sit;E)
(E ;E) .

4.3 A Model for Recovery Actions
The algorithms we shall introduce require a more detailed model of recovery actions. In
particular we want to better characterize what happens when it is not possible to uniquely
identify the most appropriate recovery action. Moreover, we want to quantify the loss we
incur in when this happens.
We start with a formal denition:
Definition 7 A basic model for recovery actions is a triple A, ,  where:
(1) A = {a1 , . . . , aK } is a nite set of symbols denoting basic recovery actions.
4. P (sit; E) can be computed as a frequency, that is P (sit; E) = 1/n, where n is the number of fault
situations, or it can be known a priori and added to the set of examples.

483

fiConsole, Picardi, & Theseider Dupre

(2)  A  A is a partial strict order relation on A. We say that ai is weaker than
aj , written as ai  aj , if aj produces more recovery eects than ai , in the sense
that aj could be used in place of ai (but not the vice versa). We therefore assume
that there are no drawbacks in actions, that is any action can be performed at any
time with no negative consequences, apart from the cost of the action itself (see
below). This is clearly a limitation and something to be tackled in future work (see
the discussion in section 7).
(3)  : A  IR+ is the cost function, and is such that if ai  aj then (ai ) < (aj ). 
associates a cost with each basic recovery action, expressing possible drawbacks of
the action itself. Recovery actions performed on-board usually imply a performance
limitation or the abortion of some ongoing activity; costs are meant to estimate
monetary losses or inconveniences for the users resulting from these. The requirement of monotonicity with respect to  stems from the following consideration: if
ai  aj and (ai )  (aj ) it would not make any sense to ever perform ai , since aj
could be performed with the same eects at the same (or lower) cost. We moreover
assume that costs are independent from the fault situation (a consequence of the
no-drawbacks assumption mentioned in the previous point).

Example 3 Let us consider again the four recovery actions a, b, c, d that appear in the teset of Figure 1. Figure 6 shows a basic action model for them. The graph expresses the

oredering relation , while costs are shown next to action names.
a - : 100

b - : 20

c - : 50

d - : 10

Figure 6: A basic action model.
We have seen in the previous section that with each fault situation is associated a
recovery action; usually this association depends on the fault, but it may also depend on
the operating conditions in which the fault occurs.
What happens when we cannot discriminate multiple fault situations? In section 3.2,
while outlining the generic algorithm for the atemporal case, we referred the solution to the
decision-making agent. In this case we want to be more precise.
Definition 8 Let A, ,  be a basic model for recovery actions. We dene the function
merge : 2A  2A as follows:
(4)

merge(S) = {ai  S | there is no aj  S such that ai  aj }
484

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

We moreover dene:
(5)

merge-set(A) = {merge(S) | S  2A }  2A

merge-set(A) is the set of compound recovery actions which includes basic recovery actions

in the form of singletons.
The intuition behind merge is that when we cannot discriminate multiple fault situations
we simply merge the corresponding recovery actions. This means that we collect all recovery
actions, and then remove the unnecessary ones (equation (4)). An action in a set becomes
unnecessary when the set contains a stronger action. Thus given a te-set E, we dene:
(6)

Act(E) = merge({Act(sit) | sit  E}).

If we take into account compound actions, we can extend the notion of model for recovery
actions as follows:
Definition 9 Let A = A, ,  be a basic model for recovery actions. An extended model
for A is a triple A, ext , ext  where:
(1) ext  merge-set(A)  merge-set(A) and given A, A  merge-set(A), A ext A if
for every a  A either a  A or there exists a  A such that a  a . Notice that
{ai } ext {aj } if and only if ai  aj .
actions such that for
(2) ext : merge-set(A)  IR+ is a cost function over compound

every A  merge-set(A), maxaA (a)  ext (A)  aA . Moreover, if A ext A
then it must hold that ext (A) < ext (A ).
While ext is uniquely determined by , the same does not hold for ext : for this reason there is more than one extended model for any basic model. The requirement that
maxaA (a)  ext (A) is motivated as follows: if there existed a  A such that ext (A) 
(a) then it would make sense never to perform a, substituting it for A. In fact, {a} ext A
and A would have the same or lower cost. We also ask ext - as we did for basic models to be monotonic with respect to ext .
In the following we shall consider only extended models for recovery actions, thus we
shall drop the ext prex from both  and .
Example 4 Figure 7 shows a possible extension for the basic model in Figure 6. In this
case the cost of the compound action {b, c} is given by the sum of the individual costs of b

and c.

5. The Problem of Building Temporal Decision Trees
In this section we outline the peculiarities of building temporal decision trees, showing the
dierences with respect to the traditional case.

485

fiConsole, Picardi, & Theseider Dupre

{a} - : 100
{b, c} - : 70

{b} - : 20

{c} - : 50

{d} - : 10

Figure 7: An extended action model.
5.1 The Challenge of Temporal Decision Trees
What makes the generation of temporal decision trees more dicult from standard ones is
the requirement that time labels do not decrease when moving from the root to the leaves:
this corresponds to assuming that sensor values cannot be stored; when the decision-making
agent decides to wait it gives up using all values that sensors show while it is waiting.
If we release this restriction we can actually generate temporal decision trees with a
minor variation of ID3, essentially by considering each pair formed by a sensor and a time
label as an individual test. In other words ID3ChooseTest selects a sensor s and a time
label t such that reading s at time t allows for the maximum discrimination among examples.
However in systems as the ones we are considering, that is low-memory real-time systems,
the possibility of performing the diagnostic task without discarding dynamics but also
without having to store sensor values across time is a serious issue to take into account.
For this reason the denition of temporal decision tree includes the requirement that time
labels be not decreasing on root-leaf paths.
Figure 8 shows a generic algorithm for building temporal decision trees that can help us
outline the diculties of the task. Line 8 shows a minor modication aimed at taking into
account deadlines: an observation can be used on a given set of examples only if its time
label does not exceed its global deadline. Violating this condition would result in a tree
that selects a recovery action only after the deadline for the corresponding fault situation
has expired.
The major change with respect to the standard algorithm is however shown in line
15: once we select an observation pair sensor, tlabel we must remove from the set of
observations all those pairs whose time label is lower than tlabel5 .
As a consequence of these operations - ruling out invalid observations and discarding
those that are in the past - the set of observations available when building a child node can
be dierent from the one used for its parent in more than one way:
5. Actually this assumes that reading a sensor and moving downwards the tree accordingly can be done
so swiftly that the qualitative sensor values have no time to change in the meanwhile. If this is not the
case, one can choose to remove also the pairs with time labels equal to tlabel, or more generally those
with time labels lower than tlabel + k where k is the time needed by the diagnostic agent to carry out
tree operations. For the sake of simplicity, however, in the following we will assume that k is 0, since the
choice of k does not aect the approach we propose.

486

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

function BuildTemporalTree (te-set Examples, set Obs, action model ActModel)
returns a temporal decision tree T = root, Nodes, Edges, Labels, TLabels
begin
if for all sit  Examples Act(sit) is the same then
return BuildLeaf(Examples, ActModel);
deadline  Dl(Examples);
UsefulObs  {o  Obs | s1, s2  Examples s.t. Val(s1, o) = Val(s2, o)};
ValidObs  {sensor, tlabel  UsefulObs | tlabel  deadline};
if ValidObs is empty then
return BuildLeaf(Examples, ActModel);
sensor, tlabel  ChooseObs (ValidObs, Examples, ActModel);
root  new node;
Nodes  {root}; Edges  ; Labels(root)  sensor; TLabels(root)  tlabel;
T  root, Nodes, Edges, Labels, TLabels;
Obs Update  {sens, tst  UsefulObs | tst  tlabel};
for each possible measure value of sensor do begin
SubExamples  {sit  Examples | Val(sit, sensor, tlabel) = value};
if SubExamples is not empty then begin
SubTree  BuildTemporalTree (SubExamples, Obs Update, ActModel);
Append(T, root, SubTree);
Labels((root, Root(SubTree)))  value;
end;
end;
return T;
end.

26
27
28
29
30
31
32
33

function BuildLeaf (te-set Examples, action model ActModel)
returns a loose temporal decision tree T = leaf, {leaf}, , Labels, 
begin
all act  {Act(sit) | sit  Examples};
comp act  merge(all act);
leaf  new node; Labels(leaf)  comp act; T  leaf, {leaf}, , Labels, ;
return T;
end.

Figure 8: Generic algorithm for building a temporal decision tree
 Some observations can be invalid for the parent and valid for the child. The recursive
call for the child works on a smaller set of examples; therefore the global deadline may
be further ahead in time, allowing more observations to be used.
 Some observations can become unavailable for the child because they have a time
label lower than that used for the parent.
Of course the problematic issue is the latter: some observations are lost, and among them
there may be some information which is necessary for properly selecting a recovery action.
Let us consider as an example the te-set in Figure 9, with four fault situations, two time
labels (0 and 1) and only one sensor (s). Each fault situation is characterized by a dierent
recovery action, and the te-set obviously allows to discriminate all of them. However the
entropy criterion would rst select the observation s, 1, which is more discriminating.

487

fiConsole, Picardi, & Theseider Dupre

sit1
sit2
sit3
sit4

s, 0
x
x
y
y

s, 1
x
y
y
z

Act
a
b
c
d

Figure 9: A te-set causing some problems to standard ID3 algorithm if used for temporal
decision trees.

The observation s, 0 would then become unavailable, and the resulting tree could never
discriminate sit2 and sit3 .
This shows that there is a relevant dierence between building standard decision trees
and building temporal decision trees. Let us look again at the generic algorithm for standard
decision trees presented in Figure 3: the particular strategy implemented in ChooseTest
does not aect the capability of the tree of selecting the proper recovery action, but only the
size of the tree. Essentially the tree contains the same information as the set of examples at least for what concerns the association between observations and recovery actions. We
can say that the tree has always the same discriminating power as the set of examples,
meaning that the only case when the tree is not capable of deciding between two recovery
actions is when the set of examples contains two fault situations with identical observations
and dierent actions.
If we consider the algorithm in Figure 8 we see that the order in which observations are
selected - that is, the particular implementation of ChooseObs - can aect the discriminating power of the tree, and not only its size. Since from one recursive call to the following
some observations are discarded, we can obtain a tree with less discriminating information
than the original set of examples. Our primary task is then to avoid such a situation, that is
to build a tree which is small, but which does not sacrice relevant information. As a consequence, we cannot exploit the strategy of simply selecting an observation with minimum
entropy.
In the next sections we shall formalize the new requirements for the output tree, and
propose an implementation of ChooseObs which meets them.
5.2 Each Tree Has a Cost
In the previous section we informally introduced the notion of discriminating power. In
this section we shall introduce a more general notion of expected cost of a temporal decision
tree. Intuitively, the expected cost associated with a temporal decision tree is the expected
cost of a recovery action selected by the tree, with respect to the probability distribution
of the fault situations.
Expected cost is a stronger notion than discriminating power: on the one hand if a tree
discriminates better than another, than it has also a lower expected cost (we shall soon
prove this statement). On the other hand expected cost adds something to the notion of
discriminating power, since any two trees are comparable from the point of view of cost,
while they may not be from the point of view of the discrimination they carry out.

488

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

Before dening expected cost, we need to introduce some preliminary denitions.
We shall make use of a function, named examples, that given an initial set of examples
E and a tree T associates to each node of the tree a subset of E. To understand the meaning
of such a function before formally dening it, let us imagine to run the tree on E and at
a certain point of the decision process to reach a node n: examples tells us which is the
subset of fault situations which we have not yet discarded.
Definition 10 Let E be a te-set with sensors s1 , . . . , sm , time labels t1 , . . . , tlast and actions
model A, , . Moreover let T = r, N, E, L, T  be a temporal decision tree such that
for every internal node n  N we have L(n)  {s1 , . . . , sm } and T (n)  {t1 , . . . , tlast }. We
dene a function examples(; E) : N  2E as follows:
(7) examples(r; E) = E

where r is the root of T;

examples(n; E) = {sit  examples(p; E) | Val(sit, L(p), T (p)) = L((p, n))}
where n  N , n = r and (p, n)  E.
examples is well dened since for any n  N dierent from the root there exists exactly

one p  N such that (p, n)  E.
Notice that, if E is the set of examples used for building T, examples(n; E) corresponds to
the subset of examples used while creating node n.
Example 5 Let us consider Figure 10: it shows the same tree as Figure 5, but for every
node n we can also see the set of fault situations examples(n; E), where E is the te-set of

Figure 1.

low
high

Action: b
sit7

very low

s1,0
sit1,sit3,sit4
sit6,sit7

normal

sit1
sit6

high
normal

Actions: b,c

very low
s2,3

s2,2
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

s2,1
sit1,sit3
sit4,sit6

s2,1
sit2,sit5
sit8

high
Action: c

sit2,sit5

sit8

low

zero

normal

s3,2

high

sit3,sit4

Action: a

Action: d

Action: b

Action: c

sit1

sit6

sit3

sit4

Figure 10: A temporal decision tree showing the value of example(n, E).

489

fiConsole, Picardi, & Theseider Dupre

In the following when using function examples we shall omit the second argument,
denoting the initial te-set, when there is no ambiguity about which te-set is considered.
Not every tree can be used on a given set of examples: actually we need some compatibility between the two, which is characterized by the following denition.
Definition 11 Let E be as in previous denition. We say that a temporal decision tree
T = r, N, E, L, T  is compatible with E if:
 For every internal node n  N , L(n)  {s1 , . . . , sm }, T (n)  {t1 , . . . , tlast } and T (n) 
Dl(examples(n; E)).
 For every leaf l  N , L(l)  merge(A) and L(l) = merge(Act(examples(l; E))).
It is straightforward to see that a tree is compatible with the set of examples used in building
it.
We have the following property6 :
Proposition 12 Let T = r, N, E, L, T  be a temporal decision tree compatible with a teset E. Let l1 , . . . , lf  N denote the leaves of T . Then examples(l1 ), . . . , examples(lf ) is a
partition of E.
For each sit  E we then denote by leafT (sit) the unique leaf l of T such that sit 
examples(l). We are now ready to formalize the notion of discriminating power.
Definition 13 Let T = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU  denote two temporal decision trees compatible with the same te-set E. Let moreover A, ,  be the
recovery action model used in building T and U. We say that T is more discriminating
than U with respect to E if:
(1) for every sit  E either LT (leafT (sit))  LU (leafU (sit)) or LT (leafT (sit)) =
LU (leafU (sit));
(2) there exists sit  E such that LT (leafT (sit))  LU (leafU (sit)).



Notice that the second condition makes sure that the two trees are not equal (in which
case they would be equally discriminating), something that the rst condition alone cannot
guarantee.
Example 6 Let us consider the tree in Figure 10 above and the tree in Figure 11 below.
The former is more discriminating than the latter. In fact, the two trees associate the same
actions to sit1 , sit2 , sit3 , sit4 , sit5 and sit6 . However the former associates action b with sit7
and action c with sit8 , while the latter associates to both sit7 and sit8 the compound action

{b, c}.
Unfortunately we cannot easily use discriminating power - as dened above - as a preference criterion for decision trees. The reason is that it does not dene a total order on
decision trees, but only a partial one. The following situations may arise:
6. For the sake of readability, all proofs are collected in a separate appendix at the end of the paper.

490

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

 For some sit, LT (leafT (sit))  LU (leafU (sit)); for some other sit, LU (leafU (sit)) 
LT (leafT (sit)).
 For a given sit, LT (leafT (sit))  LU (leafU (sit)), nor LU (leafU (sit))  LT (leafT (sit)).
From the point of view of discriminating power alone, it is reasonable for T and U not to be
comparable in the above cases. Nonetheless, there may be a reason for preferring one over
the other, and this reason is cost. For example if we consider the second situation, even if
LT (sit) and LU (sit) are not directly comparable from the point of view of their strength,
one of the two may be cheaper than the other and thus preferable.
We therefore introduce the notion of expected cost of a tree.
Definition 14 Let T = r, N, E, L, T  be a temporal decision tree compatible with a teset E and an action model A = A, , . We inductively dene an expected cost function
XE,A : N  IR+ on tree nodes as follows:


if l  N is a leaf;
(L(l))

(8)
XE,A (n) =
P (L(n)  L((n, c)))  XE,A (c) if n  N is an inner node.


c:(n,c)E

where P (L(n)  L((n, c))) is the probability of sensor L(n) showing a value v = L((n, c))
and is given by:
P (L(n)  L((n, c))) =

P (examples(c); E)
= P (examples(c); examples(n)).
P (examples(n); E)

The expected cost of T with respect to E and A, denoted by XE,A (T) , is then dened as:
XE,A (T) = XE,A (r) where r is the root of T

(9)


The above denition states that:
 The expected cost of a tree leaf l is simply the cost of its recovery action L(l);
 The expected cost of an inner node n is given by the weighted sum of its childrens
expected costs; weight for child c is given by the probability P (L(n)  L((n, c))).
 The expected cost of a temporal decision tree T is the expected cost of its root.
The following proposition states that the weighted sum for computing the expected cost
of the root can be performed directly on tree leaves.
Proposition 15 Let T = r, N, E, L, T  denote a temporal decision tree, and let l1 , . . . , lu
be its leaves. Then
(10)

XE,A (T) =

u


(L(li ))  P (examples(l); E)

i=1

491

fiConsole, Picardi, & Theseider Dupre

very low

very low

s2,5
sit1
sit6

s2,0

low

sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

high

zero

very low

s3,6
sit3
sit4

high

Action: a

Action: d

Actions: b

Action: c

sit1

sit6

sit3

sit4

Actions: b
sit2,sit5,sit7,sit8

Figure 11: A temporal decision tree less discriminating than the one in Figure 10.
The next proposition shows that expected cost is monotonic with respect to the better
discrimination relation, and therefore it is a good preference criterion for temporal decision
trees, since a tree with the lowest possible expected cost is the most discriminating one,
and moreover it is the cheapest among equally discriminating trees.
Proposition 16 Let T = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU  be two temporal decision trees compatible with the same te-set E and the same actions model A. If T is
more discriminating than U then XE,A (T) < XE,A (U).
Example 7 Let us compute the expected cost of tree T1 in Figure 10 and of tree T2 in
Figure 11, with respect to the te-set E in Figure 1 and the action model A in Figure 7. We
shall assume that all fault situations are equiprobable, that is each of them has probability
1/8. By exploiting proposition 15 we obtain:
XE,A (T1 ) = P (sit1 )(a) + P (sit7 )(b) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) +
+P ({sit2 , sit5 })({b, c}) + P (sit8 )(c) =
1
1
1
1
1
1
1
 100 +  20 +  10 +  20 +  50 +  70 +  50 =
=
8
8
8
8
8
4
8
= 12.5 + 2.5 + 1.25 + 2.5 + 6.25 + 17.5 + 6.25 = 48.75
XE,A (T2 ) = P (sit1 )(a) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) +
+P ({sit2 , sit5 , sit7 , sit8 })({b, c}) =
1
1
1
1
1
 100 +  10 +  20 +  50 +  70 =
=
8
8
8
8
2
= 12.5 + 1.25 + 2.5 + 6.25 + 35 = 57.5
We can see that the less discriminating tree, that is T2 , has a higher expected cost.



5.3 Restating the Problem
In the previous section we introduced expected cost as a preference criterion for decision
trees. Given this notion, we can restate the problem of building temporal decision tree as
492

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

that of building a tree with minimum possible expected cost. This section formally shows
that the notion of minimum possible expected cost is well dened, and more precisely
it corresponds to the cost of a tree that exploits all observations in the te-set. The goal
can then be expressed as nding a reasonably small tree among those whose expected cost
is minimum.
In this section, as well as formalizing the above mentioned notions, we introduce some
formal machinery that will be useful in proving the correctness of our algorithm.
Definition 17 Let E denote a te-set. Moreover, let t1 , . . . , tlast denote the time labels of
E. We say that siti , sitj  E are pairwise indistinguishable, and we write siti  sitj , if
for all ti < min{Dl(siti ), Dl(sitj )} and for all sensors s we have that Val(siti , s, ti ) =

Val(sitj , s, ti ).
As a relation,  is obviously reexive and symmetric, but it is not transitive. If we consider
a sitk with a particularly strict deadline, it might well be that siti  sitk , sitk  sitj , but
siti  sitj . We now introduce a new relation  which is the transitive closure of .
Definition 18 Let E denote a te-set. We say that siti , sitj  E are indistinguishable, and
we write siti  sitj , if there exists a nite sequence sitk1 , . . . , sitku  E such that
 sitk1 = siti ;
 sitku = sitj ;
 for every g = 1, . . . , u  1, sitkg  sitkg+1 .



 is an equivalence relation over E, and we denote by E/ the corresponding quotient set.
We have the following denition:
Definition 19 Let E be a te-set, with actions model A. The expected cost of E, denoted
by X E,A , is dened as:

X E,A =
(merge({Act(sit) | sit  }))  P (; E)
(11)
E/


Example 8 Let us consider the te-set E in Figure 1 and the action model A in Figure 7.
The only two indistinguishable fault situations in E are sit2 and sit5 . Thus we have:
X E,A = P (sit1 )(a) + P ({sit2 , sit5 })({b, c}) + P (sit3 )(b) + P (sit4 )(c) +
+P (sit6 )(d) + P (sit7 )(b) + P (sit8 )(c) =
1
1
1
1
1
1
1
 100 +  70 +  20 +  50 +  10 +  20 +  50 =
=
8
4
8
8
8
8
8
= 12.5 + 17.5 + 2.5 + 6.25 + 1.25 + 2.5 + 6.25 = 48.75
Notice that the tree in Figure 10 has the same cost as the te-set, thus its cost is the minimum
possible, as we show below. Of course we may still be able to build another smaller tree with

the same cost.
493

fiConsole, Picardi, & Theseider Dupre

Now we need to show that X E,A is actually the minimum possible expected cost for a
temporal decision tree compatible with E.
Theorem 20 Let E be a te-set with actions model A. We have that:
(i) There exists a decision tree T compatible with E such that XE,A (T) = X E,A .
(ii) For every temporal decision tree T compatible with E, X E,A  XE,A (T).



Now we can state more precisely the problem of building a temporal decision tree:
Given a te-set E with actions model A, we want to build a temporal decision
tree T over E, such that XE,A (T) = X E,A . Moreover, we want to keep the tree
reasonably small by exploiting entropy.

6. The Algorithm
In this section we describe in detail our proposal for building temporal decision trees from a
given te-set and action model. We also discuss the complexity of the algorithm we introduce,
and give an example of how the algorithm works.
6.1 Preconditions
Our goal is now to dene an implementation of function ChooseObs such that, once
plugged into function BuildTemporalTree, yields a solution to the problem of building
temporal decision trees as stated in section 5.3. First however we shall analyze some properties of BuildTemporalTree as dened in Figure 8: this will lead us smoothly to the
solution and will help us prove formally its correctness. In order to accomplish this task we
need to introduce some notation that allows us to speak about algorithm properties.
Let E be a te-set with fault situations {sit1 , . . . , sitn }, sensors {s1 , . . . , sm }, time labels
{t1 , . . . , tlast } and action model A. We aim at computing our tree T by executing:
(12)

T  BuildTemporalTree({sit1 , . . . , sitn }, {s1 , . . . , sm }  {t1 , . . . , tlast }, A)

Each execution of BuildTemporalTree comprises several recursive calls to the same
function; given two recursive calls c, c we shall write c  c when c occurs immediately
inside c . Moreover we shall denote by c0 the initial call. Finally, we shall call terminal a
recursive call which does not have any further inner call.
For a given call c, we shall denote by [[Example]]c , [[Obs]]c , [[ActModel]]c the actual values
of the formal parameters in c. With a slight abuse of notation, we shall also write [[var]]c
to denote the value of those variables var in c that, once set, never change their value
(deadline, UsefulObs, ValidObs, sensor, tlabel, Obs Update). Finally, we shall denote
by [[T]]c the tree returned by call c.
Each recursive call c works on a dierent te-set, which is dened by [[Examples]]c
and [[Obs]]c . The actions model however is always the same, since for c  c we have
[[ActModel]]c = [[ActModel]]c . We shall denote by Ec the te-set used in call c, which is
determined by its input parameters.
494

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

The following proposition is critical for proving the correctness of our approach. It
states that we can obtain a tree with minimum expected cost if and only if we guarantee
that there is no increase in the expected cost of the te-set when passing from the set of
observations Obs to the set Obs Update:
Proposition 21 Let us consider an execution of BuildTemporalTree starting with a
main call c0 . The initial te-set, which we want to build a tree over, is E = Ec0 with
A = [[ActModel]]c0 . For any recursive call c, let us denote by Ec the te-set determined by
[[Examples]]c and [[Obs Update]]c . Then:
(1) XE,A ([[T]]c0 )  X E,A
(2) XE,A ([[T]]c0 ) = X E,A if and only if for every non terminal7 recursive call c generated
by c0 it holds that X Ec ,A = X Ec ,A
6.2 Implementing ChooseObs
Proposition 21 suggests that we need to provide an implementation for ChooseObs such
that at each recursive call c, X Ec ,A = X Ec ,A . Let us examine in more detail the relations
between [[Obs]]c and [[Obs Update]]c .
As a rst step, [[UsefulObs]]c is obtained by removing from [[Obs]]c those observations
that do not help in discriminating fault situations. This has no eect on the expected cost
of the te-set, since it does not aect the relation of indistinguishability.
Then [[Obs Update]]c is obtained from [[UsefulObs]]c by removing those observations
whose time label precedes the chosen one. The expected cost of the resulting te-set thus
depends on the time label selected by function ChooseObs. We have the following properties:
Proposition 22 Let c, d denote two independent calls to BuildTemporalTree with the
same input arguments but with dierent implementations of ChooseObs. If [[tlabel]]c 
[[tlabel]]d then X Ec ,A  X Ed ,A .
Proposition 23 Let c be a call to BuildTemporalTree. If [[tlabel]]c = tminc = min{t |
t, s  [[ValidObs]]c } then X Ec ,A  X Ec ,A .
Now we dene the notion of a safe time label:
Definition 24 Let c denote a call to BuildTemporalTree. A time label t is said to be
safe with respect to c if [[tlabel]]c = t implies X Ec ,A = X Ec ,A .
An immediate consequence of propositions 22 and 23 is the following:
Proposition 25 For any call c to BuildTemporalTree there exist a time label tmaxc
such that the safe time labels are all and only those t with tminc  t  tmaxc , where tminc is
as in proposition 23.
7. We exclude terminal calls because they do not even compute Obs Update.

495

fiConsole, Picardi, & Theseider Dupre

Figure 12 describes ID3ChooseSafeObs, the implementation of ChooseObs we propose. It exploits the properties we have proved in this and the previous section in order to
achieve the desired task in an ecient way. Let us examine it in more detail.
ID3ChooseSafeObs (Figure 12) computes the set of safe observations (line 4) and
then chooses among them one with minimum entropy (line 5). For what we have proved
up to now, such an implementation yields a temporal decision tree with minimum expected
cost, and at the same time exploits entropy in order to keep the tree small.
Let us now see how FindSafeObs (also in Figure 12) computes the set of safe observations. Proposition 23 shows that the notion of safeness is tied to time labels rather than
to individual observations. First of all FindSafeObs determines the range of valid time
labels for the current set of examples (line 12); the lower bound tlow is the lowest time label
in Obs, and is stored in variable t low, while the upper bound tup is given by the global
deadline for Examples, and is stored in variable t up.
Then the idea is to nd the maximum safe label tmax (variable t max) which allows us
to easily build the set of safe observations (line 21).
In order to accomplish this task the following steps have to be performed:
 Given the initial te-set E, dened by Examples, Obs and ActModel, compute X E,A .
 For each time label t in the range delimited by tlow and tup , consider the te-set Et
dened by Examples and by those observations with time label equal or greater than
t. Then compute X Et ,A .
 As soon as we nd a time label t with X Et ,A > X E,A , we know that tmax is the time
label immediately preceding t.
Here the most critical operation (in terms of eciency) is that of computing the expected
cost of each Et , because this involves nding the quotient set Et /. In fact, in order
to obtain the quotient set, we need to repeatedly partition the te-set with respect to all
observations available for it.
QuotientSet function (Figure 12) performs precisely this task. It takes in input the
current time label tlabel, an initial partition (possibly made of a single block with the
entire te-set) and the set of all observations, from which it will select valid ones.
First of all it partitions the input te-set with respect to observations with the current
time label (lines 2831). Then it executes iteratively the following operations:
 For each partition block it checks whether the deadline has moved further in time
(lines 3638).
 If so, it partitions again the block and stores the resulting sub-blocks for further
examination (lines 3941).
 If not, the block is part of the Final partition that will be returned (line 42).
In order to simplify the task, we introduce as a data type the extended partition, where each
partition block is stored together with the highest time label used in building it. In this way
we can easily check if the deadline for the block allows us to exploit more observations or
not. Using extended partitions instead of standard ones we need to dene a new function,
ExtPartition, which works in the same way as the Partition function used in Figure 4,
but also records with each block the highest time label used for it.
496

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47

function ID3ChooseSafeObs (set Obs, te-set Examples, action model ActModel)
returns an observation o = sensor, tlabel
begin
SafeObs  FindSafeObs(Obs, Examples, ActModel);
o  ID3ChooseTest(SafeObs, Examples);
return o;
end.
function FindSafeObs (set Obs, te-set Examples, action model ActModel)
returns a set of observations SafeObs
begin

cost  sitExamples Act(sit);
t up  Dl(Examples); t low  min{t | s, t  Obs};
t max  t up; part  {Examples, t up};
for every time label tx starting from t up down to t low do begin
part  QuotientSet(part, Obs, tx);
newcost  ExpectedCost(part);
if newcost < cost then begin
cost  newcost; t max  tx;
end;
end;
SafeObs  {s, t | t  t max};
return SafeObs;
end.
function QuotientSet (partition Initial, set Obs, time label tlabel)
returns a rened partition Final
begin
part  Initial;
for all s, t with t = tlabel do begin
part  ExtPartition(part, s, t);
ObsCurr  ObsCurr  {s, t};
end;
Final  ;
while part =  do
tmp part  ;
for each block, ty  part do begin
newdl  Dl(block);
single  {block, ty};
if newdl > ty then begin
for each s, t with ty < t  newdl do
single  ExtPartition(single, s, t);
tmp part  tmp part  single;
end else Final  Final  start;
end;
part  tmp part;
end;
return Final;
end.

Figure 12: ID3ChooseSafeObs is an implementation of ChooseObs yielding a tree with
minimum expected cost

497

fiConsole, Picardi, & Theseider Dupre

Notice that QuotientSet needs the whole set of observations (and not only valid ones)
to properly compute the result; therefore when BuildTemporalTree calls ID3ChooseSafeObs
it must pass as rst argument UsefulObs instead of ValidObs.
FindSafeObs exploits QuotientSet to nd all quotient sets for all Et , but does so
using an ecient approach which we call backward strategy.
First of all, we can notice that the order in which observations are considered does not
matter while building a quotient set. Moreover, if t < t , Et / is a renement of Et /; in
other words we can obtain it from Et / by simply rening the partition with additional
observations.
Thus, we can compute all quotient sets at the same time as we compute E/.
FindSafeObs does exactly so: it computes all quotient sets and their expected cost
starting from the last time label tup . Each quotient set is not built from scratch, but as a
renement of the previous one. This is the reason why QuotientSet (and ExtPartition
as well) takes as rst argument not a single set, but a partition. In this way, all quotient sets
are computed with the same operations8 needed to build E/. The next section analyzes
in further detail complexity issues.
6.3 Complexity
In this section we aim at showing that the additional computations needed in building
temporal decision trees do not lead to a higher asymptotical complexity than that we
would get by using the standard ID3 algorithm on the same set of examples (we discussed
in section 5.1 the circumstances that could make such an approach feasible).
Essentially the dierence between the two cases lies in the presence of FindSafeObs function. Wherever BuildTree calls ID3ChooseTest, BuildTemporalTree calls
ID3ChooseSafeObs, which in turn calls both FindSafeObs and ID3ChooseTest.
Let us compare FindSafeObs and ID3ChooseTest, which are similar in many ways.
The former repeatedly partitions the input te-set with respect to every available observation;
then it computes entropy for each partition built in such a way. FindSafeObs builds
just one partition by exploiting all available observations; in other words instead of using
each observation to partition the initial te-set, it exploits it in order to rene an existing
partition of the same te-set. Moreover, at each time label it computes the expected cost
of the partition built so far. Essentially, if we denote by NS the number of sensors and
with NT the number of time labels in the initial partition, we have roughly the following
comparison:
 NS  NT (number of observations) entropy computations for ID3ChooseTest vs.
NT expected cost computations for FindSafeObs.
 NS  NT partitions of the initial te-set for ID3ChooseTest vs. NS  NT renements
of existing partitions of the initial te-set for FindSafeObs.
Entropy and expected cost can be computed with roughly the same eort: both require
retrieving some information for each element of each partition block, and to combine this
information in some quite straightforward way. The complexity of this task depends only
on the overall number of elements, and not on how they are distributed between dierent
8. There is a slight overhead due to the need to nd which observations should be used at each step.

498

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

block of the partition. So even if expected cost is computed most of the times on ner
partitions than entropy, the only thing that matters is that both are partitions of the same
set, thus involving the same elements.
Now let us examine the problem of creating a partition. This involves retrieving a value
for each element of each block of the initial partition (which again depends only on the
number of elements, and not on the number of blocks of the initial partition) and to properly
assign the element to a new partition block depending on the original block and on the new
value. The main dierence in this case between starting with the whole te-set (creation) or
with an initial partition (renement), is the size of the new blocks that are being created,
which are smaller in the second case. Dependent on how we implement the partition data
type, this may make no dierence, or may take less time for the renement case. However,
it never happens that renement (corresponding to FindSafeObs function) requires more
time than the creation (corresponding to ID3ChooseTest function) of a partition.
Therefore we can claim that FindSafeObs function has the same asymptotic complexity as function ID3ChooseTest. Thus also ID3ChooseSafeObs has the same asymptotic complexity as ID3ChooseTest, and we can conclude that BuildTree has the same
asymptotic complexity as BuildTemporalTree.
6.4 An Example
In this section we shall show how our algorithm operates on the te-set in Figure 1 with
respect to the action model in Figure 7.
Let us summarize the information the algorithm receives. Eight fault situations are
involved; moreover we can exploit three sensors, each of which can show ve dierent
qualitative values: h - high, n - normal, l - low, v - very low, z - zero. Time labels
correspond to natural numbers ranging from 0 to 7, and we assume they correspond to
times measured by an internal clock which is started at the time of fault detection. There
are four basic recovery actions a, b, c, d, such that d  b  a and d  c  a. The set of
compound recovery actions is thus A = {{a}, {b}, {c}, {b, c}, {d}}; the ordering relation is
pictured in 7, together with action costs.
BuildTemporalTree is rst called on the whole te-set. None of the two terminating conditions is met (notice however that there are two observations that are not useful, since they do not discriminate: s3 , 0 and s3 , 1). Then the main function calls
ID3ChooseSafeObs and consequently FindSafeObs. Since the global deadline is 2 we
must check time labels 0, 1 and 2, starting from the last one.
Exploiting only observations with time label 2 we obtain the following partition:
{{sit1 , sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}
However in order to nd the expected cost we still have to check if for some partition block
the deadline has changed; this happens for {sit1 , sit6 } as well as for {sit3 } and {sit4 }. For
the last two blocks it does not change anything - they already contain only one element. As
to the rst block, the deadline is now 5 and thus it is possible to further split the partition.
Therefore we obtain that the partition for time label 2 is:
Pt=2 = {{sit1 }, {sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}
499

fiConsole, Picardi, & Theseider Dupre

After nding the partition, the algorithm computes the expected cost, which turns out
to be:
1
+ (Act(sit6 )) 
8
1
+ (Act(sit3 ))  + (Act(sit4 )) 
8
1
1
1
1
= 100  + 10  + 70  + 20 
8
8
2
8
= 57.5

XE,t=2 = (Act(sit1 )) 

1
1
+ (Act({sit2 , sit5 , sit7 , sit8 })) 
8
2
1
8
1
+ 50 
8

Then the algorithm moves to time label 1; starting from Pt=2 it adds observations with
time label 1, obtaining a new partition:
Pt=1 = {{sit1 }, {sit6 }, {sit2 , sit5 }, {sit7 }, {sit8 }, {sit3 }, {sit4 }}
Deadlines move for {sit7 } and {sit8 }, but since these are singletons the new observations
cannot further split the partition. The expected cost is now:
1
+ (Act(sit6 )) 
8
1
+ (Act(sit8 ))  + (Act(sit3 )) 
8
1
1
1
1
= 100  + 10  + 70  + 20 
8
8
4
8
= 48.75

XE,t=1 = (Act(sit1 )) 

1
+ (Act({sit2 , sit5 })) 
8
1
1
+ (Act(sit4 )) 
8
8
1
1
+ +50  + 20  + 50 
8
8

1
1
+ (Act(sit7 )) 
4
8
1
8

Since XE,t=1 < XE,t=2 we can conclude that observations with time label 2 are not safe. We
now move to time label 0, and we immediately realize that the new observations do not
change the partition. Thus XE,t=0 = XE,t=1 , and safe observations are those with time label
equal either to 0 or to 1.
The algorithm now calls function ID3ChooseTest which selects the observation with
minimum entropy. Figure 13 shows the entropies of the dierent observations at this stage,
from which we deduce that the best choice is s2 , 1.
Entropies for {sit1 , sit2 , sit3 , sit4 , sit5 , sit6 , sit7 , sit8 }
s1 , 0 1.5
s1 , 1 1.5
s2 , 0 1.451
s2 , 1 0.844

Figure 13: Entropies for safe observations at the initial call
Figure 15.(a) shows the tree at this point; function BuildTemporalTree recursively
invokes itself four times, yielding:
 a call c1 on Ev = {sit1 , sit6 };
 a call c2 on En = {sit2 , sit5 };
 a call c3 on El = {sit3 , sit4 , sit7 };
 a call c4 on Eh = {sit8 }.
500

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

Let us focus on call c1 : again, none of the terminating conditions is met, therefore the
algorithm invokes ID3ChooseSafeObs and thus FindSafeObs. Notice however that on
this subset only a few observations are in UsefulObs: s1 , 3, s2 , 3, s2 , 4, s2 , 5 and
s3 , 5. The global deadline is 5.
First we nd the partition (and the expected cost) for time label 5:
Pt=5 = {{sit1 }, {sit6 }}
1
1
1
1
XEv ,t=5 = (Act(sit1 ))  + (Act(sit6 ))  = 100  + 10  = 55
2
2
2
2
No additional observations can split further this partition and lower the cost; therefore we
nd that all valid observations are also safe. It is moreover obvious that all these observations have the same entropy, which is 0. Therefore the algorithm can non-deterministically
select one of them; a reasonable criterion would be to select any of the earliest ones, for
example s1 , 3.
Since the initial te-set for call c1 is now split in two, there are two more recursive calls.
However we can notice that if BuildTemporalTree is called on a te-set with a single
element, the rst terminating condition is trivially met (all the fault situations have the
same recovery action). The function simply returns a tree leaf with the name of the proper
recovery action. Figure 15.(b) shows the tree after c1 has been completed.
Now let us examine c2 : the algorithm eliminates non-discriminating observations, and
nds out that the set of useful observations is empty. Thus it builds a leaf with recovery
action {b, c}. Let us pass to call c3 . In this case none of the terminating conditions is met:
Entropies for {sit3 , sit4 , sit7 }
s1 , 1 0.667 s1 , 2 0.667 s1 , 3 0.667
s2 , 2 0.667 s2 , 3 0.667 s2 , 4 0.667
s2 , 5
0
s3 , 1
0
s3 , 2
0
s3 , 3
0
s3 , 4
0

Figure 14: Entropies for safe observations at call c3
the algorithm must then look for safe observations. The global deadline is 5, so we start
examining time label 5, and we nd:
Pt=5 = {{sit3 }, {sit4 }, {sit7 }}
1
1
1
XEl ,t=5 = (Act(sit3 ))  + (Act(sit4 ))  + (Act(sit7 )) 
3
3
3
1
1
1
= 20  + 50  + 20  = 30
3
3
3
Much as happened for c1 , no additional observation can further split the partition, so we
can conclude that all valid observations are also safe. Figure 14 shows entropy for all valid
observations; the earliest one with minimum entropy is s3 , 2 and this the algorithm selects.
The two recursive sub-calls that are generated immediately terminate: {sit4 } is a singleton,
and in {sit3 , sit7 } both fault situations correspond to the same recovery action.
The last recursive call, c4 , has in input a singleton and thus immediately terminates.
The nal decision tree T is pictured in Figure 15.(c).
501

fiConsole, Picardi, & Theseider Dupre

very low

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

very low

low

low

normal

normal
high

?

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

sit1,sit6

s2,3

normal

?
sit3,sit4,sit7

?

?

sit2,sit5

sit8

high

sit1
sit6

high

?

a

d

sit1

sit6

(a)

?
sit3,sit4,sit7

sit2,sit5

?
sit8

(b)
very low

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

normal
normal

s2,3
sit1
sit6

low

high

high

normal

s3,2
sit3,sit4

high

sit7

a

d

sit1

sit6

b,c

c

sit2,sit5

sit8

b

c

sit3,sit7

sit4

(c)

Figure 15: The output tree at dierent stages. (c) shows the nal tree.
Let us check if the expected cost of T is really equal to the expected cost for E. Figure 16
shows for each tree leaf l the corresponding set of fault situations examples(l), its probability
P (examples(l); E) and its cost (L(l)). Leaves are numbered from 1 to 6 going left to right
in Figure 15.(c).
leaf
l1
l2
l3
l4
l5
l6

examples
sit1
sit6
sit2 , sit5
sit8
sit3 , sit7
sit4

P
1/8
1/8
1/4
1/8
1/4
1/8


100
10
70
50
20
50

Figure 16: Fault situations, probabilities and costs for the leaves of the tree in Figure 15.(c)

502

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

Thus the expected cost of the tree is:
XE,A (T) =

6


(L(li ))  P (examples(l); E)

i=1

1
1
1
1
1
1
 100 +  10 +  70 +  50 +  20 +  50
8
8
4
8
4
8
= 48.75

=

If we look back at example 8 we see that XE,A = 48.75, thus T has the minimum possible
expected cost. Moreover, we can compare T with the tree T1 of example 5. Also T1 has
the minimum possible expected cost, but T is more compact.

7. Conclusions
In this paper we introduced a new notion of diagnostic decision tree that takes into account
temporal information on the observations and temporal constraints on the recovery actions
to be performed. In this way we can take advantage of the discriminatory power that is
available in the model of a dynamic system. We presented an algorithm that generates
temporal diagnostic decision trees from a set of examples, discussing also how an optimal
tree can be generated.
The automatic compilation of decision trees seems to be a promising approach for reconciling the advantages of model-based reasoning and the constraints imposed by the on-board
hardware and software environment. It is worth noting that this is not only true in the
automotive domain and indeed the idea of compiling diagnostic rules from a model has been
investigated also in other approaches (see e.g., Darwiche, 1999; Dvorak & Kuipers, 1989).
Darwiche (1999), in particular, discusses how rules can be generated for those platforms
where constrained resources do not allow a direct use of a model-based diagnostic system.
What is new in our approach is the possibility of compiling also information concerning
the system temporal behaviour, obtaining in this way more accurate decision procedures.
To the best of our knowledge, temporal decision trees are a new notion in the diagnostic
literature. However, there are works in other elds that have some relation to ours, since
they are aimed at learnig rules or associations that take into account time.
Geurts and Whenkel (1998) propose a notion of temporal tree to be used for early
prediction of faults. This topic is closely related to diagnosis, albeit dierent in some
ways: the idea is that the device under examination has not failed yet, but by observing
its behaviour it is possible to predict that a fault is about to occur. Geurts and Wehenkel
propose to learn the relation between observed behavioural patterns and consequent failures
by inducing a temporal tree.
The notion of temporal tree introduced by Geurts and Wehenkel is dierent than our
temporal decision trees, reecting the dierent purpose it has been introduced for. Rather
than sensor readings, it consider a more general notion of test, and the tree does not
species the time to wait before performing the tests, but rather the agent running the tree
is supposed to wait until one of the tests associated to a tree node becomes true.
Also the notion of optimality is quite dierent: in the situation described by Geurts and
Wehenkel the size of the resulting tree is not a concern. The tree-building algorithms aims
503

fiConsole, Picardi, & Theseider Dupre

then at minimizing the time at which the nal decision is taken. In our algorithm, size is
the primary concern, while from the point of view of time it suces that diagnosis is carried
out within certain deadlines. From the point of view of time alone, the apporach by Geurts
and Wehenkel is probably more general than ours; the problem of considering the trade-o
between diagnostic capability and time needed for diagnosis is one of the major extensions
we are considering for future work on this topic (see below).
Finally, the algorithm proposed by Geurts and Wehenkel works in a quite dierent
way than ours: it rst builds the tree greedily, using an evaluation function that weighs
discriminability power agains time needed to reach a result, and selecting at each step the
texts that optimizes such function. Then it prunes the tree in order to avoid overtting. On
the other hand, our approach aim at optimizing the tree from the point of view of cost, and
at the same time tries to keep the tree small with the entropy heuristic. We think that, since
optimization can be carried out at no additional cost9 with respect to the minimization of
entropy, our approach can obtain better results, at least in those cases where one can dene
a notion of deadline.
The process of learning association rules involving time has also been studied in other
areas, such as machine learning (see for example Bischof & Caelli, 2001, where the authors
propose a technique to learn movements) and data mining. While the specic diagnostic
tailoring of our approach makes it dicult to compare it with more generic learning algorithms, the connections with data mining may be stronger. Our proposal in fact aims
essentially at extracting from series of observations those patterns in time that allow to
correctly diagnose a fault: this process can be regarded as a form of temporal classication.
A preliminary investigation of papers in this area (see Antunes & Oliveira, 2001 for an
overview) seems to suggest that, whereas the analysis of temporal sequences of data has
received much interest in the last years, not much work has been done in the direction of
data classication, where temporal decision trees could be exploited.
This suggests an interesting development for our work, in particular as concerns its
applicability in other areas. However, we believe that the algorithm we presented needs to
be extended in order to be exploited in other contexts. In particular we are investigating
the following extensions:
 Deadlines could be turned from hard to soft. Soft deadlines do not have to be met,
but rather dene a cost associated to not meeting them. Thus not meeting a deadline
becomes an option that can be taken into account when it is less expensive than
performing a recovery action when the diagnosis is not complete. One could even
dene a cost that increases as the time passes from the expiration of the deadline.
Such an extension would allow to model also the trade-o between discriminability
power and time needed by the decision process, which we believe is the key to making
our work applicable in other areas.
 Actions could be assumed to have a dierent cost depending on the fault situation; for
example the action associated to a fault could become dangerous and thus extremely
expensive if performed in presence of another fault.
9. From the point of view of asymptotical complexity.

504

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

On the long term, future work on this topic will be aimed at widening its areas of
applicability, and investigating in deeper details its connections with other elds, such as
fault prevention and data mining.

8. Acknowledgements
This work was partially supported by the EU under the grant GRD-1999-0058, project IDD
(Integrated Diagnosis and Design), whose partners are: Centro Ricerche Fiat, DaimlerChrysler, Magneti Marelli, OCCM, PSA, Renault, Technische Universitat Munchen, Universite Paris XIII, Universita di Torino.

Appendix A. Proofs
This section contains the proofs of all propositions, lemmas and theorems in the paper.
Proposition 12. Let T = r, N, E, L, T  be a temporal decision tree compatible with
a te-set E. Let l1 , . . . , lf  N denote the leaves of T . Then examples(l1 ), . . . , examples(lf )
is a partition of E.
Proof. Follows immediately by the denition of examples (10), by noticing that if
n1 , . . . , nk are the children of p then {examples(n1 ), . . . , examples(nk )} is a partition of

examples(p).
Proposition 15 Let T = r, N, E, L, T  denote a temporal decision tree, and let
l1 , . . . , lu be its leaves. Then
XE,A (T) =

u


(L(li ))  P (examples(l); E)

i=1

Proof. By induction on the depth of T. If T has depth 0 then it consists of a single
leaf l and 10 holds trivially since examples(l) must be equal to E and P (E; E) = 1.
If T has depth > 0 then let T1 , . . . , Tk denote its direct subtrees and c1 , . . . , ck denote
their roots. We can regard each Ti as an autonomous temporal decision tree compatible
with te-set Ei = examples(ci ). By induction hypothesis we have that:
(13)


P (examples(l); E)
.
(L(l))  P (examples(l); Ei ) =
(L(l)) 
XEi ,A (Ti ) =
P (Ei ; E)
l leaf of Ti

l leaf of Ti

Moreover by denition of expected cost:
(14) XE,A (T) =

k


P (L(r)  L((r, ci )))  XEi ,A (Ti ) with

i=1

P (L(r)  L((r, ci ))) = P (examples(ci ); examples(r)) = P (Ei ; E).

505

fiConsole, Picardi, & Theseider Dupre

From (13) and (14) we thus obtain:
(15) XE,A (T) =

k




P (Ei ; E)  (L(l)) 

i=1 l leaf of Ti

=

P (examples(l); E)
P (Ei ; E)
k




(L(l))  P (examples(l); E)

i=1 l leaf of Ti

Since the leaves of T are all and only the leaves of T1 , . . . , Tk , (15) is equivalent to the

thesis.
Proposition 16 Let T = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU  be two
temporal decision trees compatible with the same te-set E and the same actions model A. If
T is more discriminating than U then XE,A (T) < XE,A (U).
Proof. Rewriting equation (10) we obtain:
(16) XE,A (T) =

r


(LT (li ))  P (examples(l); E) =

(17) XE,A (U) =

(LT (leafT (sit)))P (sit; E)

sitE

i=1





(LU (leafU (sit)))P (sit; E).

sitE

Since T is more discriminating than U, we have that for all sit  E:
LT (leafT (sit))  LU (leafU (sit)) or

LT (leafT (sit)) = LU (leafU (sit))

with at least one sit satisfying the rst relation. By denition of  it follows that for all sit:
(LT (leafT (sit)))  (LU (leafU (sit)))
and for at least one sit:
(LT (leafT (sit))) = (LU (leafU (sit)))
Therefore if we compare the individual elements of the two sums in (16) and (17) we observe
there exists at least one sit for which:
(LT (leafT (sit)))P (sit; E) < (LU (leafU (sit)))P (sit; E)
and for all other sit
(LT (leafT (sit)))P (sit; E)  (LU (leafU (sit)))P (sit; E)
which concludes the proof.
Theorem 20 Let E be a te-set with actions model A. We have that:



(i) There exists a decision tree T compatible with E such that XE,A (T) = X E,A .
(ii) For every temporal decision tree T compatible with E, X E,A  XE,A (T).
506



fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

In order to prove this theorem we introduce some lemmas.
Lemma 26 Let T be a temporal decision tree compatible with a te-set E. Then siti  sitj
implies leafT (siti ) = leafT (sitj ).
Proof.
We prove that siti  sitj implies leafT (siti ) = leafT (sitj ), from which the
lemma easily follows. Let us suppose that leafT (siti ) = leafT (sitj ). This means that
there is a common ancestor n of the two leaves such that siti , sitj  examples(n) and
Val(siti , L(n), T (n)) = Val(sitj , L(n), T (n)). Since siti  sitj this is possible only if
T (n) > min{Dl(siti ), Dl(sitj )}. But since T is compatible with E it must hold that T (n) 

Dl(examples(n))  min{Dl(siti ), Dl(sitj )}, which contradicts the previous statement.
Lemma 27 Let E be a te-set with sensors s1 , . . . , sm , time labels t1 , . . . , tlast and actions
model A. There exists a temporal decision tree T = r, N , E, L, T  such that XE,A (T) =
X E,A .
Proof. In order to prove the thesis we construct a tree T with the same expected cost
as the te-set.
Let us dene a total order on observations in E as follows: s, t < s , t  if either t < t
or t = t and s precedes s in a lexicographic ordering. Let us denote by o1 , . . . , omax the
ordered sequence of observations thus obtained. We shall dene T level by level (starting
from the root, at level 1) giving the value of L and T for nodes at level h.
T has a maximum of max + 1 levels, where max is the number of observations. New
levels are added until all nodes in a level are leaves (which as we shall see happens at most
at level max + 1). Let thus n be a node at level h , and let sih , tih  = oh if h  max. We
have:

a leaf
if h = max + 1, or Dl(examples(n)) < tih ;
n is
an internal node otherwise.

merge({Act(sit | sit  examples(n)}) if n is a leaf;
L(n) =
if n is an internal node.
sih
T (n) = tih

if n is an internal node.

A decision-making agent running such a tree would essentially take into account all sensor
measurement at all time labels until either there are no more available observations or it
must perform a recovery action because a deadline is about to expire.
Now we need to show that XE,A (T) = X E,A .
Let l1 , . . . , lu denote the leaves of T. We shall rst of all prove that siti  sitj if and
only if leafT (siti ) = leafT (sitj ), or equivalently that
{examples(l1 ), . . . , examples(lu )} = E/.
This, together with equations (10) and (11) yields the thesis.
We already know from lemma 26 that siti  sitj implies leafT (siti ) = leafT (sitj ); we
need to show that the opposite is also true. Let us thus assume that siti , sitj  examples(l)
507

fiConsole, Picardi, & Theseider Dupre

for some l  {l1 , . . . , lu }. Let r = n1 , n2 , . . . , nH , nH+1 = l be the path from the root to
l. We know from the denition of T that for all h  H, L(nh ), T (nh ) = oh , and that
o1 , . . . , oH are all observations s, t of E such that t  Dl(examples(l)). Moreover since
siti , sitj  examples(l) we have that for all h = 1, . . . , H, Val(siti , oh ) = Val(sitj , oh ).
Now there are two possibilities: either Dl(examples(l)) = min{Dl(siti ), Dl(sitj )}, or
Dl(examples(l)) < min{Dl(siti ), Dl(sitj )}.
In the rst case we immediately obtain that siti  sitj and thus siti  sitj .
In the second case, there must be sitk  examples(l) such that Dl(examples(l)) =
Dl(sitk ). Moreover, Dl(sitk ) = min{Dl(siti ), Dl(sitk )} = min{Dl(sitj ), Dl(sitk )}. Since all
considerations above apply also to sitk we thus have that siti  sitk and sitk  sitj ; therefore

by transitivity siti  sitj .
Lemma 28 Let T = r, N, E, L, T  be a decision tree compatible with a te-set E with
actions model A. Then X E,A  XE,A (T ).
Proof. Let T be as dened in the proof of lemma 27. In order to prove the thesis it
suces to show that T is either equally10 or more discriminating than T (see proposition
16). Actually we shall show that given sit  E either L(leafT (sit))  L(leafT (sit)) or
L(leafT (sit)) = L(leafT (sit)).
We know that examples(leafT (sit))  examples(leafT (sit)). In fact, let sit be an
element of examples(leafT (sit)) dierent from sit itself: by construction of T we have that
sit  sit , and by lemma 26 it follows that leafT (sit) = leafT (sit ).
Let:
A = {Act(s) | s  examples(leafT (sit))}, A = {Act(s) | s  examples(leafT (sit))}.
Since A  A, by denition of merge:
merge(A)  merge(A) or

merge(A) = merge(A)

Thus having L(leafT (sit)) = merge(A) and L(leafT (sit)) = merge(A) we obtain that either

the action selected by T is weaker than that selected by T, or it is the same.
Now we can prove theorem 20.
Proof. Point (i) is proved by lemma 27, while point (ii) corresponds to lemma 28. 
Proposition 21 Let us consider an execution of BuildTemporalTree starting with
a main call c0 . The initial te-set, which we want to build a tree over, is E = Ec0 with
A = [[ActModel]]c0 . For any recursive call c, let us denote by Ec the te-set determined by
[[Examples]]c and [[Obs Update]]c . Then:
(1) XE,A ([[T]]c0 )  X E,A
(2) XE,A ([[T]]c0 ) = X E,A if and only if for every non terminal11 recursive call c generated
by c0 it holds that X Ec ,A = X Ec ,A
10. Rather intuitively, two trees are equally discriminating if they associate to each fault situation the same
recovery action.
11. We exclude terminal calls because they do not even compute Obs Update.

508

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

Proof. We shall prove (1) and (2) for every recursive call c (rather than only for c0 ).
The proof is by induction on the depth of the recursion starting from c.
depth = 0. Then c is terminal, and we only have to prove that XE,A ([[T]]c ) = X E,A . There
are two reasons why c may be terminal: either (i) all fault situations in [[Examples]]c
are associated with the same recovery action A, or (ii) [[ValidObs]]c = .
(i) From denition 19 we have:

X E,A =
(merge({Act(sit) | sit  }))  P (; Ec )
Ec /

=



(A)  P (; Ec )

Ec /

= (A)



P (; Ec ) = (A).

Ec /

Since [[T]]c is made of a single leaf l with L(l) = A we also have that XEc ,A ([[T]]c ) =
(A), which proves the thesis.
(ii) If [[ValidObs]]c =  then for all s, t  [[Examples]]c , t > Dl([[Examples]]c ).
Let sit  Ec be such that Dl(sit) = Dl([[Examples]]c ). Then by denition of
indistinguishability for any sit  Ec we have that sit  sit . This proves that
Ec / is made of a single equivalence class which coincides with Ec itself. Thus
X Ec ,A = (merge({Act(sit) | sit  Ec })). Since [[T]]c is made of a single leaf l
with L(l) = merge({Act(sit) | sit  Ec }), it follows that XEc ,A ([[T]]c ) = X Ec ,A .
depth > 0. Then c is not terminal and ChooseObs selects an observation o = s, t.
Let v1 , . . . , vk be the possible values for o: then c has k inner recursive calls to
BuildTemporalTree, which we shall denote respectively by c1 , . . . , ck . We have
that {Ec1 , . . . , Eck } is a partition of Ec .
By denition of expected cost (14) we have that:
XEc ,A ([[T]]c ) =

k


P (Ec |ovi ; Ec )  XEc |ov

i

i=1

,A ([[T]]ci )

Ec |ovi and Eci dier only in the set of observations, which is [[Obs]]c for the former
and [[Obs Update]]c for the latter. However we have that:
 P (Ec |ovi ; Ec ) = P (Eci ; Ec ) since probabilities depend only on the fault situations in a te-set, and not on the observations.
 XEc |ov ,A ([[T]]ci ) = XEci ,A ([[T]]ci ): expected cost depends also on the observai
tions, but [[T]]ci by construction can contain as labels only those observations in
Eci .
Moreover, since also Ec and Ec dier only in the observations P (Eci ; Ec ) = P (Eci ; Ec ).
Therefore we can write:
XEc ,A ([[T]]c ) =

k


P (Eci ; Ec )  XEci ,A ([[T]]ci )

i=1

509

fiConsole, Picardi, & Theseider Dupre

In order to prove (1), we can apply the induction hypothesis XEci ,A ([[T]]ci )  X Eci ,A )
and obtain:

XEc ,A ([[T]]c ) 

(18)

k


P (Eci ; Ec )  X Eci ,A

i=1

Now let us work on the right-hand side expression in 18:
k


P (Eci ; Ec )  X Eci ,A =

i=1

k


P (Eci ; Ec )

i=1

=



(Act())  P (; Eci )

Eci /

k



(Act())  P (; Eci )  P (Eci ; Ec )

i=1 Eci /

=

k



(Act())  P (; Ec )

i=1 Eci /

Notice however that {Eci /} is a partition of Ec /; in other words each   Ec /
belongs to exactly one set Eci /. In fact, splitting examples according to the value
of one observation cannot split a class of undistinguishable observations. Thus the
above equality becomes:
k


P (Eci ; Ec )  X Eci ,A =

i=1



(Act())  P (; Ec ) = X Ec ,A

Ec /

This, together with 18, yields:
(19)

XEc ,A ([[T]]c )  X Ec ,A

As mentioned above, the only dierence between Ec and Ec is that the former has
fewer observations. This implies that, if sit  sit in Ec , then sit  sit in Ec as well.
This means that Ec / is a sub-partition12 of Ec / in the following sense: we can
partition every   Ec / in () = {1 , . . . , k  } such that for each j there exists
exactly one j  Ec / containing exactly the same fault situations as j . This yields:

X Ec ,A =
(Act())  P (; Ec )
Ec /

Since each j has the same fault situations as the corresponding j , and necessarily
for each j there is a  containing it, we have:
 
X Ec ,A =
(Act(  ))  P (; Ec )
Ec /  ()

12. Ec / is not a sub-partition of Ec / in the ordinary sense because they do not have the same set of
observations.

510

fiTemporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board

If    () the fault situations in   are a subset of those in ; thus (Act(  )) 
(Act()). Then we obtain:
 
X Ec ,A 
(Act())  P (  ; Ec )
Ec /  ()

=



(Act()) 

Ec /

=





P (  ; Ec )

 ()

(Act())  P (; Ec ) = X Ec ,A

Ec /

Together with equation 19, this proves (1):
XEc ,A ([[T]]c )  X Ec ,A
Now let us prove (2). The induction hypothesis changes 18, and thus 19, into equalities, thus yielding:
(20)

XEc ,A ([[T]]c ) = X Ec ,A

Since by hypothesis in (2) we have that X Ec ,A = X Ec ,A , we immediately obtain:
XEc ,A ([[T]]c ) = X Ec ,A
which concludes the proof.

Proposition 22 Let c, d denote two independent calls to BuildTemporalTree
with the same input arguments but with dierent implementations of ChooseObs. If
[[tlabel]]c  [[tlabel]]d then X Ec ,A  X Ed ,A .

Proof. Follows immediately from [[Obs Update]]c  [[Obs Update]]d .
Proposition 23 Let c be a call to BuildTemporalTree. If [[tlabel]]c = tminc =
min{t | t, s  [[ValidObs]]c } then X Ec ,A  X Ec ,A .
Proof. In this case [[Obs Update]]c = [[UsefulObs]]c , thus the only removed observa
tions are non discriminating ones.
Proposition 25 For any call c to BuildTemporalTree there exist a time label tmaxc
such that the safe time labels are all and only those t with tminc  t  tmaxc , where tminc is
as in proposition 23.

Proof. Straightforward.

References
Antunes, C., & Oliveira, A. (2001). Temporal data mining: An overview. In KDD Workshop
on Temporal Data Mining, San Francisco.
Bischof, W., & Caelli, T. (2001). Learning spatio-temporal relational structures. Journal
of Applied Intelligence, 15, 707722.
Brusoni, V., Console, L., Terenziani, P., & Theseider Dupre, D. (1998). A spectrum of
denitions for temporal model-based diagnosis. Articial Intelligence, 102 (1), 3979.
511

fiConsole, Picardi, & Theseider Dupre

Cascio, F., Console, L., Guagliumi, M., Osella, M., Sottano, S., & Theseider, D. (1999).
Generating on-board diagnostics of dynamic automotive systems based on qualitative
deviations.. AI Communications, 12 (1), 3344.
Cascio, F., & Sanseverino, M. (1997). IDEA (Integrated Diagnostic Expert Assistant)
model-based diagnosis in the the car repair centers. IEEE Expert, 12 (6).
Console, L., & Dressler, O. (1999). Model-based diagnosis in the real world: lessons learned
and challenges remaining. In Proc. 16th IJCAI, pp. 13931400, Stockholm.
Darwiche, A. (1999). On compiling system descriptions into diagnostic rules. In Proc. 10th
Int. Work. on Principles of Diagnosis, pp. 5967.
Dvorak, D., & Kuipers, B. (1989). Model-based monitoring of dynamic systems. In Proc.
11th IJCAI, pp. 12381243, Detroit.
Foresight-Vehicle (2002).
Foresight vehicle automotive roadmap,technology
and research directions for future road vehicles.
Tech. rep.,
http://www.foresightvehicle.org.uk/initiatives/init01/init01-report.asp.
Geurts, P., & Wehenkel, L. (1998). Early prediction of electric power system blackouts
by temporal machine learning. In Proceedings of the ICML98/AAAI98 Workshop on
Predicting the future: AI Approaches to time series analysis, Madison, July 24-26.
Mosterman, P., Biswas, G., & Manders, E. (1998). A comprehensive framework for model
based diagnosis. In Proc. 9th Int. Work. on Principles of Diagnosis, pp. 8693.
Price, C. (1999). Computer-Based Diagnostic Systems. Springer.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81106.
Russel, S., & Norvig, P. (1995). Articial Intelligence: A Modern Approach. Prentice Hall.
Sachenbacher, M., Malik, A., & Struss, P. (1998). From electrics to emissions: experiences
in applying model-based diagnosis to real problems in real cars. In Proc. 9th Int.
Work. on Principles of Diagnosis, pp. 246253.
Sachenbacher, M., Struss, P., & Weber, R. (2000). Advances in design and implementation of
obd functions for diesel injection systems based on a qualitative approach to diagnosis.
In SAE 2000 World Congress.

512

fiJournal of Artificial Intelligence Research 19 (2003) 631-657

Submitted 10/02; published 12/03

AltAltp : Online Parallelization of Plans with
Heuristic State Search
Romeo Sanchez Nigenda
Subbarao Kambhampati

rsanchez@asu.edu
rao@asu.edu

Department of Computer Science and Engineering,
Arizona State University, Tempe AZ 85287-5406

Abstract
Despite their near dominance, heuristic state search planners still lag behind disjunctive
planners in the generation of parallel plans in classical planning. The reason is that directly
searching for parallel solutions in state space planners would require the planners to branch
on all possible subsets of parallel actions, thus increasing the branching factor exponentially.
We present a variant of our heuristic state search planner AltAlt called AltAltp which
generates parallel plans by using greedy online parallelization of partial plans. The greedy
approach is significantly informed by the use of novel distance heuristics that AltAltp
derives from a graphplan-style planning graph for the problem. While this approach is
not guaranteed to provide optimal parallel plans, empirical results show that AltAltp is
capable of generating good quality parallel plans at a fraction of the cost incurred by the
disjunctive planners.

1. Introduction
Heuristic state space search planning has proved to be one of the most efficient planning
frameworks for solving large deterministic planning problems (Bonet, Loerincs, & Geffner,
1997; Bonet & Geffner, 1999; Bacchus, 2001). Despite its near dominance, its one achilles
heel remains generation of parallel plans (Haslum & Geffner, 2000). Parallel plans allow
concurrent execution of multiple actions in each time step. Such concurrency is likely
to be more important as we progress to temporal domains. While disjunctive planners
such as Graphplan (Blum & Furst, 1997) SATPLAN (Kautz & Selman, 1996) and GPCSP (Do & Kambhampati, 2000) seem to have no trouble generating such parallel plans,
planners that search in the space of states are overwhelmed by this task. The main reason
is that straightforward methods for generation of parallel plans would involve progression
or regression over sets of actions. This increases the branching factor of the search space
exponentially. Given n actions, the branching factor of a simple progression or regression
search is bounded by n, while that of progression or regression search for parallel plans will
be bounded by 2n .
The inability of state search planners in producing parallel plans has been noted in the
literature previously. Past attempts to overcome this limitation have not been very successful. Indeed, Haslum and Geffner (2000) consider the problem of generating parallel
plans using a regression search in the space of states. They note that the resulting planner, HSP*p, scales significantly worse than Graphplan. They present TP4 in (Haslum &
Geffner, 2001), which in addition to being aimed at actions with durations, also improves
c
2003
AI Access Foundation. All rights reserved.

fiSanchez & Kambhampati

the branching scheme of HSP*p, by making it incremental along the lines of Graphplan.
Empirical studies reported by Haslum and Geffner (2001), however indicate that even this
new approach, unfortunately, scales quite poorly compared to Graphplan variants. Informally, this achilles heel of heuristic state search planners has been interpreted as a sort of
last stand of the disjunctive planners  only they are capable of generating parallel plans
efficiently.
Given that the only way of efficiently generating optimal parallel plans involves using
disjunctive planners, we might want to consider ways of generating near-optimal parallel
plans using state search planners. One obvious approach is to post-process the sequential
plans generated by the state search planners to make them parallel. While this can easily
be done - using approaches such as those explored by Backstrom (1998), one drawback
is that such approaches are limited to transforming the sequential plan given as input.
Parallelization of sequential plans often results in plans that are not close to optimal parallel
plans.1
An alternative, that we explore in this paper, involves incremental online parallelization.
Specifically, our planner AltAltp , which is a variant of the AltAlt planner (Sanchez, Nguyen,
& Kambhampati, 2000; Nguyen, Kambhampati, & Sanchez, 2002), starts its search in the
space of regression over single actions. Once the most promising single action to regress
is selected, AltAltp then attempts to parallelize (fatten) the selected search branch with
other independent actions. This parallelization is done in a greedy incremental fashion actions are considered for addition to the current search branch based on the heuristic cost
of the subgoals they promise to achieve. The parallelization continues to the next step
only if the state resulting from the addition of the new action has a better heuristic cost.
The sub-optimality introduced by the greedy nature of the parallelization is offset to some
extent by a plan-compression procedure called Pushup that tries to rearrange the evolving
parallel plans by pushing up actions to higher levels in the search branch (i.e. later stages
of execution) in the plan.
Despite the seeming simplicity of our approach, it has proven to be quite robust in
practice. In fact, our experimental comparison with five competing planners - STAN (Long
& Fox, 1999), LPG (Gerevini & Serina, 2002), Blackbox (Kautz & Selman, 1996), SAPA (Do
& Kambhampati, 2001) and TP4 (Haslum & Geffner, 2001) - shows that AltAltp is a viable
and scalable alternative for generating parallel plans in several domains. For many problems,
AltAltp is able to generate parallel plans that are close to optimal in makespan. It also
seems to retain the efficiency advantages of heuristic state search over disjunctive planners,
producing plans in a fraction of the time taken by the disjunctive planners in many cases.
AltAltp has also been found to be superior to post-processing approaches. Specifically,
we compared AltAltp to an approach that involves post-processing the sequential plans
generated by AltAlt using the techniques from Backstrom (1998). We found that AltAltp is
able to generate shorter parallel plans in many cases. Finally, we show that AltAltp incurs
very little additional overhead compared to AltAlt.
In the rest of this paper, we discuss the implementation and evaluation of our approach
to generate parallel plans with AltAltp . Section 2 starts by providing a review of the
AltAlt planning system, on which AltAltp is based. Section 3 describes the generation of
1. We will empirically demonstrate this later; curious readers may refer to the plots in Figure 15.

632

fiOnline Parallelization of Plans with Heuristic State Search

Action Templates

Serial Planning
Graph

Graphplan
Plan Extension Phase
(based on STAN)

Problem Spec
(Init, Goal state)

Extraction of
Heuristics

Actions in the
Last Level

AltAlt

Heuristics

Regression Planner
(based on HSP-R)

Solution Plan

Figure 1: Architecture of AltAlt
parallel plans in AltAltp . Section 4 presents extensive empirical evaluation of AltAltp . The
evaluation includes both comparison and ablation studies. Finally, Section 5 discusses some
related work in classical as well as metric temporal planning. Section 6 summarizes our
contributions.

2. AltAlt Background Architecture and Heuristics
The AltAlt planning system is based on a combination of Graphplan (Blum & Furst, 1997;
Long & Fox, 1999; Kautz & Selman, 1999) and heuristic state space search (Bonet et al.,
1997; Bonet & Geffner, 1999; McDermott, 1999) technology. AltAlt extracts powerful
heuristics from a planning graph data structure to guide a regression search in the space of
states. The high level architecture of AltAlt is shown in Figure 1. The problem specification
and the action template description are first fed to a Graphplan-style planner (in our case,
STAN from Long & Fox, 1999), which constructs a planning graph for that problem in
polynomial time (we assume the reader is familiar with the Graphplan algorithm of Blum
& Furst, 1997). This planning graph structure is then fed to a heuristic extractor module
that is capable of extracting a variety of effective heuristics (Nguyen & Kambhampati,
2000; Nguyen et al., 2002). These heuristics, along with the problem specification, and the
set of ground actions in the final action level of the planning graph structure are fed to a
regression state-search planner.
To explain the operation of AltAlt at a more detailed level, we need to provide some
further background on its various components. We shall start with the regression search
module. Regression search is a process of searching in the space of potential plan suffixes. The suffixes are generated by starting with the goal state and regressing it over
the set of relevant action instances from the domain. The resulting states are then (nondeterministically) regressed again over relevant action instances, and this process is repeated
until we reach a state (set of subgoals) which is satisfied by the initial state. A state S in our
framework is a set of (conjunction of) literals that can be seen as subgoals that need to be
made true on the way to achieving the top level goals. An action instance a is considered
relevant to a state S if the effects of a give at least one element of S and do not delete
633

fiSanchez & Kambhampati

any element of S. The result of regressing S over a is then (S\ef f (a))  prec(a) - which is
essentially the set of goals that still need to be achieved before the application of a, such
that everything in S would have been achieved once a is applied. For each relevant action
a, a separate search branch is generated, with the result of regressing S over that action as
the new fringe in that branch. Search terminates with success at a node if every literal in
the state corresponding to that node is present in the initial state of the problem.
The crux of controlling the regression search involves providing a heuristic function that
can estimate the relative goodness of the states on the fringe of the current search tree and
guide the search in the most promising directions. The heuristic function needs to evaluate
the cost of achieving the set S of subgoals (comprising a regressed state) from the initial
state. In other words, the heuristic computes the length of the plan needed to achieve the
subgoals from the initial state. We now discuss how such a heuristic can be computed from
the planning graph, which, provides optimistic reachability estimates.
Normally, the planning graph data structure supports parallel plans - i.e., plans where
at each step more than one action may be executed simultaneously. Since we want the planning graph to provide heuristics to the regression search module of AltAlt, which generates
sequential solutions, we first make a modification to the algorithm so that it generates a
serial planning graph. A serial planning graph is a planning graph in which, in addition
to the normal mutex relations, every pair of non-noop actions at the same level are marked
mutex. These additional action mutexes propagate to give additional propositional mutexes. Finally, a planning graph is said to level off when there is no change in the action,
proposition and mutex lists between two consecutive levels.
We will assume for now that given a problem, the Graphplan module of AltAlt is used
to generate and expand a serial planning graph until it levels off. As discussed by Sanchez
et al. (2000), we can relax the requirement of growing the planning graph to level-off, if we
can tolerate a graded loss of informedness of heuristics derived from the planning graph.
We will start with the notion of level of a set of propositions:
Definition 1 (Level) Given a set S of propositions, lev(S) is the index of the first level
in the leveled serial planning graph in which all propositions in S appear and are non-mutex
with one another. If S is a singleton, then lev(S) is just the index of the first level where
the singleton element occurs. If no such level exists, then lev(S) =  if the planning graph
has been grown to level-off.
The intuition behind this definition is that the level of a literal p in the serial planning
graph provides a lower bound on the length of the plan (which, for a serial planning graph,
is equal to the number of actions in the plan) to achieve p from the initial state. Using this
insight, a simple way of estimating the cost of a set of subgoals will be to sum their levels.
Heuristic 1 (Sum heuristic) hsum (S) :=

P

pS

lev({p})

The sum heuristic is very similar to the greedy regression heuristic used in UNPOP (McDermott, 1999) and the heuristic used in the HSP planner (Bonet et al., 1997). Its main
limitation is that the heuristic makes the implicit assumption that all the subgoals (elements
of S) are independent. The hsum heuristic is neither admissible nor particularly informed
as it ignores the interactions between the subgoals. To develop more effective heuristics,
634

fiOnline Parallelization of Plans with Heuristic State Search

we need to consider both positive and negative interactions among subgoals in a limited
fashion.
In (Nguyen et al., 2002), we discuss a variety of ways of using the planning graph to
incorporate negative and positive interactions into the heuristic estimate, and discuss their
relative tradeoffs. One of the best heuristics according to that analysis was a heuristic called
hAdjSum2M . We adopted this heuristic as the default heuristic in AltAlt. The basic idea
of hAdjSum2M is to adjust the sum heuristic to take positive and negative interactions into
account. This heuristic approximates the cost of achieving the subgoals in some set S as the
sum of the cost of achieving S, while considering positive interactions and ignoring negative
interactions, plus a penalty for ignoring the negative interactions. The first component
RP (S) can be computed as the length of a relaxed plan for supporting S, which is
extracted by ignoring all the mutex relations. To approximate the penalty induced by the
negative interactions alone, we proceed with the following argument. Consider any pair of
subgoals p, q  S. If there are no negative interactions between p and q, then lev({p, q}),
the level at which p and q are present together, is exactly the maximum of lev(p) and lev(q).
The degree of negative interaction between p and q can thus be quantified by:
(p, q) = lev({p, q})  max (lev(p), lev(q))
We now want to use the  - values to characterize the amount of negative interactions
present among the subgoals of a given set S. If all subgoals in S are pair-wise independent,
clearly, all  values will be zero, otherwise each pair of subgoals in S will have a different
value. The largest such  value among any pair of subgoals in S is used as a measure of the
negative interactions present in S in the heuristic hAdjSum2M . In summary, we have
Heuristic 2 (Adjusted 2M) hAdjSum2M (S) := length(RP (S)) + maxp,qS (p, q)
The analysis by Nguyen et al. (2002) shows that this is one of the more robust heuristics
in terms of both solution time and quality. This is thus the default heuristic used in AltAlt
(as well as AltAltp ; see below).

3. Generation of Parallel Plans Using AltAltp
The obvious way to make AltAlt produce parallel plans would involve regressing over subsets
of (non interfering) actions. Unfortunately, this increases the branching factor exponentially
and is infeasible in practice. Instead, AltAltp uses a greedy depth-first approach that makes
use of its heuristics to regress single actions, and incrementally parallelizes the partial plan
at each step, rearranging the partial plan later if necessary.
The high level architecture of AltAltp is shown in Figure 2. Notice that the heuristic
extraction phase of AltAltp is very similar to that of AltAlt, but with one important modification. In contrast to AltAlt which uses a serial planning graph as the basis for its
heuristic (see Section 2), AltAltp uses the standard parallel planning graph. This makes
sense given that AltAltp is interested in parallel plans while AltAlt was aimed at generating
sequential plans. The regression state-search engine for AltAltp is also different from the
search module in AltAlt. AltAltp augments the search engine of AltAlt with 1) a fattening
step and 2) a plan compression procedure (Pushup). The details of these procedures are
discussed below.
635

fiSanchez & Kambhampati

Action Templates

Parallel
Planning
Graph

Graphplan
Plan Extension Phase
(based on STAN)

Extraction of
Heuristics

Actions in the
Last Level

AltAltp

Heuristics

Problem Spec
(Init, Goal state)

Node
Expansion
(Fattening)

Node Ordering
and Selection

Plan
Compression
Algorithm
(PushUp)

Solution Plan

Figure 2: Architecture of AltAltp

S

A={a 1 ,a 2 ,...,a p ,...a m }

a1

a2

S1

S2

ap
. ..

am
Sp

Figure 3: AltAltp Notation

636

. ..

Sm

fiOnline Parallelization of Plans with Heuristic State Search

parexpand(S)
A  get set of applicable actions for current state S
forall ai  A
Si  Regress(S,ai )
CHILDREN(S)  CHILDREN(S) + Si
Sp  The state among Children(S) with minimum
hadjsum2M value
ap  the action that regresses to Sp from S
/**Fattening process
O  { ap }
forall g  S ranked in the decreasing order of level(g)
Find an action ag  A supporting g such that ag 6 O and
ai is pairwise independent with each action in O.
If there are multiple such actions, pick the one which has
minimum hadjsum (Regress(S, O + ag )) among all ag  A
If hadjsum2M (S, O + ai ) < hadjsum2M (S, O)
O  O + ag
Spar  Regress(S, O)
CHILDREN(S)  CHILDREN(S) + Spar
return CHILDREN
END;
Figure 4: Node Expansion Procedure
The general idea in AltAltp is to select a fringe action ap from among those actions A
used to regress a particular state S during any stage of the search (see Figure 3). Then,
the pivot branch given by the action ap is fattened by adding more actions from A,
generating a new state that is a consequence of regression over multiple parallel actions.
The candidate actions used for fattening the pivot branch must (a) come from the sibling
branches of the pivot branch and (b) be pairwise independent with all the other actions
currently in the pivot branch. We use the standard definition of action independence: two
actions a1 and a2 are considered independent if the state S  resulting after regressing both
actions simultaneously is the same as that obtained by applying a1 and a2 sequentially with
any of their possible linearizations. A sufficient condition for this is that the preconditions
and effects of the actions do not interfere:
((|prec(a1 )|  |ef f (a1 )|)  (|prec(a2 )|  |ef f (a2 )|)) = 
where |L| refers to the non-negated versions of the literals in the set L. We now discuss
the details of how the pivot branch is selected in the first place, and how the branch is
incrementally fattened.
Selecting the Pivot Branch: Figure 4 shows the procedure used to select and parallelize
the pivot branch. The procedure first identifies the set of regressable actions A for the
current node S, and regresses each of them, computing the new children states. Next, the
action leading to the child state with the lowest heuristic cost among the new children is
selected as the pivot action ap , and the corresponding branch becomes the pivot branch.
637

fiSanchez & Kambhampati

S
At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)

H=23
am: Unload(pack4,airp2,Home)

ap: Unload(pack1,airp1,ASU)

a1: Unload(pack1,airp2,ASU)

Sp
H=21
Pivot
Unload(pack2,airp1,ASU)
Unload(pack2,airp2,ASU)
Unload(pack3,airp1,ASU)
Unload(pack3,airp2,ASU)
Unload(pack4,airp2,HOME)

S1
H=21

...

Sm
H=22

Possible Pairwise
Independent Actions

Figure 5: After the regression of a state, we can identify the P ivot and the related set of
pairwise independent actions.

The heuristic cost of the states is computed with the hadjsum2M heuristic from AltAlt,
based on a parallel planning graph. Specifically, in the context of the discussion of the
hadjsum2M heuristic at the end of Section 2, we compute the (p, q) values, which in turn
depend on the level(p), level(q) and level(p, q) in terms of the levels in the parallel planning
graph rather than the serial planning graph. It is easy to show that the level of a set of
conditions on the parallel planning graph will be less than or equal to the level on the serial
planning graph. The length of the relaxed plan is still computed in terms of number of
actions. We show later (see Figure 19(a)) that this change does improve the quality of the
parallel plans produced by AltAltp .
The search algorithm used in AltAltp is similar to that used in HSPr (Bonet & Geffner,
1999) - it is a hybrid between greedy depth first and a weighted A* search. It goes depthfirst as long as the heuristic cost of any of the children states is lower than that of the
current state. Otherwise, the algorithm resorts to a weighted A* search to select the next
node to expand. In this latter case, the evaluation function used to rank the nodes is
f (S) = g(S) + w  h(S) where g(S) is the length of the current partial plan in terms of
number of steps, h(S) is our estimated cost given by the heuristic function (e.g. hAdjSum2M ),
and w is the weight given to the heuristic function. w is set to 5 based on our empirical
experience.2
Breaking Ties: In case of a tie in selecting the pivot branch, i.e., more than one branch
leads to a state with the lowest heuristic cost, we break the tie by choosing the action that
2. For the role of w in Best-First search see (Korf, 1993).

638

fiOnline Parallelization of Plans with Heuristic State Search

supports subgoals that are harder to achieve. Here, the hardness of a literal l is measured
in terms of the level in the planning graph at which l first appears. The standard rationale
for this decision (c.f. Kambhampati & Sanchez, 2000) is that we want to fail faster by
considering the most difficult subgoals first. We have an additional justification in our case,
we also know that a subgoal with a higher level value requires more steps and actions for
its achievement because it appeared later into the planning graph. So, by supporting it
first, we may be able to achieve other easier subgoals along the way and thereby reduce the
number of parallel steps in our partial plan.
Fattening the Pivot Branch: Next the procedure needs to decide which subset O  A
of the sibling actions of the pivot action ap will be used to fatten the pivot branch. The
obvious first idea would be to fatten the pivot branch maximally by adding all pairwise
independent actions found during that search stage. The problem with this idea is that
it may add redundant and heuristically inferior actions to the branch, and satisfying their
preconditions may lead to an increase of the number of parallel steps.
So, in order to avoid fattening the pivot branch with such irrelevant actions, before
adding any action a to O, we require that the heuristic cost of the state S  that results
from regressing S over O + a be strictly lower than that of S. This is in addition to the
requirement that a be pairwise independent with the current set of actions in O. This
simple check also ensures that we do not add more than one action for supporting the same
set of subgoals in S.
The overall procedure for fattening the pivot branch thus involves picking the next
hardest subgoal g in S (with hardness measured in terms of the level of the subgoal in the
planning graph), and finding the action ag  A achieving g, which is pair-wise independent
of all actions in O and which, when added to O and used to regress S, leads to a state S 
with the lowest heuristic cost, which in consequence should be lower than the cost of S.
Once found, ag is then added to O, and the procedure is repeated. If there is more than one
action that can be ag , then we break ties by considering the degree of overlap between the
preconditions of action ag and the set of actions currently in O. The degree of precondition
overlap between a and O is defined as |prec(a)  {oO prec(o)}|. The action a with higher
degree of overlap is preferred as this will reduce the amount of additional work we will need
to do to establish its preconditions. Notice that because of the fattening process, a search
node may have multiple actions leading to it from its parent, and multiple actions leading
from it to each of its children.
Example: Figure 5 illustrates the use of this node expansion procedure for a problem
from the logistics domain (Bacchus, 2001). In this example we have four packages pack1,
pack2, pack3 and pack4. Our goal is to place the first three of them at ASU and the
remaining one at home. There are two planes airp1 and airp2 to carry out the plans. The
figure shows the first level of the search after S has been regressed. It also shows the pivot
action ap given by unload(pack1,airp1,ASU), and a candidate set of pairwise independent
actions with respect to ap . Finally, we can see in Figure 6 the generation of the parallel
branch. Notice that each node can be seen as a partial regressed plan. As described in the
paragraphs above, only actions regressing to lower heuristic estimates are considered in apar
to fatten the pivot branch. Notice that the action unload(pack4,airp2,Home) has been
discarded because it leads to a state with higher cost, even though it is not inconsistent
639

fiSanchez & Kambhampati

S
At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)
apar: Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

H=23
am: Unload(pack4,airp2,Home)

a1: Unload(pack1,airp2,ASU)
ap: Unload(pack1,airp1,ASU)

Spar
H=19

Sp

S1
H=21

H=21
Pivot

...

Sm
H=22

O
Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack2,airp2,ASU)
Unload(pack3,airp1,ASU)
Unload(pack3,airp2,ASU)
Unload(pack4,airp2,HOME)

Figure 6: Spar is the result of incrementally fattening the P ivot branch with the pairwise
independent actions in O

with the rest of the actions chosen to fatten the pivot branch. Furthermore, we can also
see that we have preferred actions using the plane airp1, since they overlap more with the
pivot action ap .
Offsetting the Greediness of Fattening: The fattening procedure is greedy, since it
insists that the state resulting after fattening have a strictly better heuristic value. While
useful in avoiding the addition of irrelevant actions to the plan, this procedure can also
sometimes preclude actions that are ultimately relevant but were discarded because the
heuristic is not perfect. These actions may then become part of the plan at later stages
during search (i.e., earlier parts of the execution of the eventual solution plan; since we are
searching in the space of plan suffixes). When this happens, the length of the parallel plan
is likely to be greater, since more steps that may be needed to support the preconditions
of such actions would be forced to come at even later stages of search (earlier parts of the
plan). Had the action been allowed into the partial plan earlier in the search (i.e., closer to
the end of the eventual solution plan), its preconditions could probably have been achieved
in parallel to the other subgoals in the plan, thus improving the number of steps.
In order to offset this negative effect of greediness, AltAltp re-arranges the partial plan
to promote such actions higher up the search branch (i.e., later parts of the execution of
the eventual solution plan). Specifically, before expanding a given node S, AltAltp checks
to see if any of the actions in As leading to S from its parent node (i.e., Figure 6 shows
that Apar leads to Spar ) can be pushed up to higher levels in the search branch. This online
640

fiOnline Parallelization of Plans with Heuristic State Search

pushUP(S)
As  get actions leading to S
forall a  As
x0
Sx  get parent node of S
/** Getting highest ancestor for each action
Loop
Ax  get actions leading to Sx
If (parallel(a, Ax ))
x  x+1
Sx  get parent node of Sx1
Else
aj  get action conflicting with a from Ax
If (Secondary Optimizations)
Remove a and aj from branch
Include anew if necessary
Else
Ax1  Ax1 + a
As  As  a
break
End Loop
/**Adjusting the partial plan
Sx  get highest ancestor x in history
createN ewBranchF rom(Sx )
while x > 0
Snew  regress Sx with Ax1
Sx  Snew
x  x1
END;

Figure 7: Pushup Procedure

re-arrangement of the plan is done by the Pushup procedure, which is shown in Figure 7.
The Pushup procedure is called each time before a node gets expanded, and it will try to
compress the partial plan. For each of the actions a  As we find the highest ancestor node
Sx of S in the search branch to which the action can be applied (i.e., it gives some literal in
Sx without deleting any other literals in Sx , and it is pairwise independent of all the actions
Ax currently leading out of Sx , in other words the condition parallel(a, Ax ) is satisfied).
Once Sx is found, a is then removed from the set of actions As leading to S and introduced
into the set of actions leading out of Sx (to its child in the current search branch). Next, the
states in the search branch below Sx are adjusted to reflect this change. The adjustment
involves recomputing the regressions of all the search nodes below Sx . At first glance, this
might seem like a transformation of questionable utility since the preconditions of a (and
their regressions) just become part of the descendants of Sx , and this does not necessarily
reduce the length of the plan. We however expect a length reduction because actions
supporting the preconditions of a will get pushed up eventually during later expansions.
641

fiSanchez & Kambhampati

S
At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)
Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

H=23
Unload(pack4,airp2,Home)

Unload(pack1,airp2,ASU)
Unload(pack1,airp1,ASU)

Spar

Sp

H=19

...

S1

H=21

H=21

Sm
H=22

Unload(pack4,airp2,Home)
fly(airp1,LocX,ASU)
Spar

Sp

...

Sm

H=18
Pivot

Unload(pack4,airp2,Home)
fly(airp1,LocX,ASU)

H=18

H=16

(a) Finding the highest ancestor node to which an action can be
pushed up.
S

Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)
Unload(pack4,airp2,HOME)

At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)

H=23
Unload(pack4,airp2,Home)

Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

Unload(pack1,airp2,ASU)

Unload(pack1,airp1,ASU)
Spar

Snew

Sp

H=19

S1

H=21

Sm

H=21

fly(airp1,LocX,ASU)

fly(airp1,LocX,ASU)

...

Snew

Spar

Sp

H=16

fly(airp1,LocX,ASU)
Unload(pack4,airp2,Home)

H=18
Pivot

H=22

Unload(pack4,airp2,Home)

...

Sm
H=18

H=16

(b) The Pushup procedure generates a new search branch.

Figure 8: Rearranging of the Partial Plan
642

fiOnline Parallelization of Plans with Heuristic State Search

Rather than doctor the existing branch, in the current implementation, we just add a
new branch below Sx that reflects the changes made by the Pushup procedure.3 The new
branch then becomes the active search branch, and its leaf node is expanded next.
Aggressive Variation of Pushup: The Pushup procedure, as described above, is not
expensive as it only affects the current search branch, and the only operations involved are
recomputing the regressions in the branch. Of course, it is possible to be more aggressive
in manipulating the search branch. For example, after applying an action a to its ancestor
Sx the set of literals in the child state, say Snew changes, and thus additional actions may
become relevant for expanding Snew . In principle, we could re-expand Snew in light of the
new information. We decided not to go with the re-expansion option, as it typically does
not seem to be worth the cost. In Section 4.3, we do compare our default version of Pushup
procedure with a variant that re-expands all nodes in the search branch, and the results of
those studies support our decision to avoid re-expansion. Finally, although we introduced
the Pushup procedure as an add-on to the fattening step, it can also be used independent of
the latter, in which case the net effect would be an incremental parallelization of a sequential
plan.
Example: In Figure 8(a), we have two actions leading to the node Spar (at depth two),
these two actions are Unload(pack4,airp2,Home) and fly(airp1,LocX,ASU). So, before
expanding Spar we check if any of the two actions leading to it can be pushed up. While the
second action is not pushable since it interacts with the actions in its ancestor node, the first
one is. We find the highest ancestor in the partial plan that interacts with our pushable
action. In our example the root node is such an ancestor. So, we insert our pushable
action Unload(pack4,airp2,Home) directly below the root node. We then re-adjust the
state Spar to Snew at depth 1, as shown in Figure 8(b), adding a new branch, and reflecting
the changes in the states below. Notice that the action Unload(pack4,airp2,Home) was
initially discarded by the greediness of the fattening procedure (see Figure 6), but we have
offset this negative effect with our plan compression algorithm. We can see also that we
have not re-expanded the state Snew at depth 1, we have only made the adjustments to the
partial plan using the actions already presented in the search trace.4

4. Evaluating the Performance of AltAltp
We implemented AltAltp on top of AltAlt. We have tested our implementation on a suite
of problems that were used in the 2000 and 2002 AIPS competition (Bacchus, 2001; Long
& Fox, 2002), as well as other benchmark problems (McDermott, 2000). Our experiments
are broadly divided into three sets, each aimed at comparing the performance of AltAltp
under different scenarios:
1. Comparing the performance of AltAltp to other planning systems capable of producing
parallel plans.
3. Because of the way our data structures are set up, adding the new branch turns out to be a more robust
option than manipulating the existing search branch.
4. Instead, the aggressive Pushup modification would expand Snew at depth 1, generating similar states to
those generated by the expansion of Spar at the same depth.

643

fiSanchez & Kambhampati

80

70

60

35

Gripper AIPS-98

AltAlt-p
STAN
TP4
Blackbox
LPG 2nd

Elevator AIPS-00
AltAlt-p
STAN

30

Blackbox
LPG 2nd

25

50

Steps

20

Steps

40

15

30
10

20

5

10

0

0
1

2

3

4

5

Problems

6

7

1

8

3

5

7

9

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49

Problems

(b)

(a)

Figure 9: Performance on the Gripper (AIPS-98) and the Elevator (AIPS-00) Domains.
12

1400

AltAlt-p
STAN
Blackbox

10

1200

1000

Steps

8

Time

800

6

600

4
400

2
200

0

0

1

4

7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70

1

Problems

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70

Problems

(b)

(a)

Figure 10: Performance on the Schedule domain (AIPS-00)
2. Comparing our incremental parallelization technique to AltAlt + Post-Processing.
3. Ablation studies to analyze the effect of the different parts of the AltAltp approach
on its overall performance.
Our experiments were all done on a Sun Blade-100 workstation, running SunOS 5.8
with 1GB RAM. Unless noted otherwise, AltAltp was run with the hadjsum2M heuristic
644

fiOnline Parallelization of Plans with Heuristic State Search

90

450

Altalt-p
STAN
TP4
Blackbox
LPG 2nd

80

70

350
300

50

Time

Steps

60

400

250

40

200

30

150

20

100
50

10

0

0
1

3

5

7

1

9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

(b)

Figure 11: Performance on the Logistics domain(AIPS-00)

35

800

AltAlt-p
STAN
TP4
Blackbox
LPG 2nd
Sapa

30

25

700

600

400

Time

Steps

500

20

15

300

10
200

5
100

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

0

15

1

Problems

2

3

4

(a)

5

6

7

8

9

Problems

10

11

12

13

14

15

(b)

Figure 12: Performance on the DriverLog domain(AIPS-02)
described in section 2 of this paper, and with a parallel planning graph grown until the first
level where the top-level goals are present without being mutex. All times are in seconds.
4.1 Comparing AltAltp with Competing Approaches
In the first set of experiments we have compared the performance of our planner with
the results obtained by running STAN (Long & Fox, 1999), Blackbox (Kautz & Selman,
645

fiSanchez & Kambhampati

1999), TP4 (Haslum & Geffner, 2001), LPG (Gerevini & Serina, 2002) and SAPA (Do &
Kambhampati, 2001). Unless noted otherwise, every planner has been run with its default
settings. Some of the planners could not be run in some domains due to parsing problems
or memory allocation errors. In such cases, we just omit that planner from consideration
for those particular domains.
4.1.1 Planners Used in the Comparison Studies
STAN is a disjunctive planner, which is an optimized version of the Graphplan algorithm
that reasons with invariants and symmetries to reduce the size of the search space. Blackbox
is also based on the Graphplan algorithm but it works by converting planning problems
specified in STRIPS (Fikes & Nilsson, 1971) notation into boolean satisfiability problems
and solving it using a SAT solver (the version we used defaults to SATZ).5 LPG (Gerevini
& Serina, 2002) was judged the best performing planner at the 3rd International Planning
Competition (Long & Fox, 2002), and it is a planner based on planning graphs and local
search inspired by the Walksat approach. LPG was run with its default heuristics and
settings. Since LPG employs an iterative improvement algorithm, the quality of the plans
produced by it can be improved by running it for multiple iterations (thus increasing the
running time). To make the comparisons meaningful, we decided to run LPG for two
iterations (n=2), since beyond that, the running time of LPG was generally worse than
that of AltAltp . Finally, we have also chosen two metric temporal planners, which are able
to represent parallel plans because of their representation of time and durative actions.
TP4 (Haslum & Geffner, 2001) is a temporal planner based on HSP*p (Haslum & Geffner,
2000), which is an optimal parallel state space planner with an IDA* search algorithm.
The last planner in our list is SAPA (Do & Kambhampati, 2001). SAPA is a powerful
domain-independent heuristic forward chaining planner for metric temporal domains that
employs distance-based heuristics (Kambhampati & Sanchez, 2000) to control its search.
4.1.2 Comparison Results in Different Domains
We have run the planners in the Gripper domain from the International Planning and
Scheduling competition from 1998 (McDermott, 2000), as well as three different domains
(Logistics, Scheduling, and Elevator-miconic-strips) from 2000 (Bacchus, 2001), and three
more from the 2002 competition (Long & Fox, 2002) - DriverLog, ZenoTravel, and Satellite.
In cases where there were multiple versions of a domain, we used the STRIPS Untyped
versions.6 . We discuss the results of each of the domains below.
Gripper: In Figure 9(a), we compare the performance of AltAltp on the Gripper domain (McDermott, 2000) to the rest of the planners excluding SAPA. The plot shows the
results in terms of number of (parallel) steps. We can see that for even this simplistic domain, AltAltp and LPG are the only planners capable of scaling up and generating parallel
5. We have not chosen IPP (Koehler, 1999), which is also an optimized Graphplan planning system because
results reported by Haslum and Geffner (2001) show that it is already less efficient than STAN.
6. Since SAPA does not read the STRIPS file format, we have run the SAPA planner on equivalent problems
with unit-duration actions from Long and Fox (2002).

646

fiOnline Parallelization of Plans with Heuristic State Search

5000

40

AltAlt-p
STAN
LPG 2nd
Sapa

35

4500
4000

30
3500

25

3000

Steps

Time

20

15

2500
2000
1500

10
1000

5

500
0

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

1

15

Problems

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Problems

(b)

(a)

Figure 13: Performance on the ZenoTravel domain (AIPS-02)
plans. None of the other approaches is able to solve more than four problems.7 AltAltp is
able to scale up without any difficulty to problems involving 30 balls. Furthermore, AltAltp
returns better plans than LPG.
Elevator: In Figure 9(b), we compare AltAltp to STAN, Blackbox and LPG in the Elevator
domain (Miconic Strips) (Bacchus, 2001).8 AltAltp approached the quality of the solutions
produced by the optimal approaches (e.g. Blackbox and STAN). Notice that Blackbox can
only solve around half of the problems solved by AltAltp in this domain.
Scheduling: Results from the Scheduling domain are shown in Figure 10. Only Blackbox
and STAN are considered for comparison.9 AltAltp seems to reasonably approximate the
optimal parallel plans for many problems (around 50 of them), but does produce significantly
suboptimal plans for some. However, it is again able to solve more problems than the other
two approaches and in a fraction of the time.
Logistics: The plots corresponding to the Logistics domain from Bacchus (2001) are shown
in Figure 11.10 For some of the most difficult problems AltAltp outputs lower quality
solutions than the optimal approaches. However, only AltAltp and LPG are able to scale
up to more complex problems, and we can easily see that AltAltp provides better quality
solutions than LPG. AltAltp also seems to be more efficient than any of the other approaches.
7. Although STAN is supposed to be able to generate optimal step-length plans, in a handful of cases it
seems to have produced nonoptimal solutions for the Gripper Domain. We have no explanation for this
behavior, but have informed the authors of the code.
8. we did not include the traces from TP4 because the pre-processor of the planner was not able to read
the domain.
9. The TP4 pre-processor cannot read this domain, LPG runs out of memory, and SAPA has parsing
problems.
10. Only SAPA is excluded due to parsing problems.

647

fiSanchez & Kambhampati

5000

60

AltAlt-p
STAN
TP4
Blackbox

50

4500
4000

LPG 2nd
Sapa

40

3500
3000

Steps

Time

30

2500
2000

20
1500
1000

10

500

0

0

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Problems

Problems

(b)

(a)

Figure 14: Performance on the Satellite domain(AIPS-02)
The LPG solutions for problems 49 to 61 are obtained doing only one iteration, since LPG
was not able to complete the second iteration in a reasonable amount of time. This explains
the low time taken for LPG, and the lower quality of its solutions.
DriverLog: We see in Figure 12(a) that AltAltp does reasonably well in terms of quality
with respect to the other approaches in the DriverLog domain. Every planner is considered
this time. AltAltp is one of the two planners able to scale up. Figure 12(b) shows also that
AltAltp is more efficient than any of the other planners.
Zeno-Travel: Only AltAltp , SAPA, and LPG are able to solve most of the problems in this
domain.11 AltAltp solves them very efficiently (Figure 13(b)) providing very good solution
quality (Figure 13(a)) compared to the temporal metric planners.
Satellite: The results from the Satellite domain are shown in Figure 14. Although every
planner is considered, only AltAltp , SAPA, and LPG can solve most of the problems. SAPA
solves all problems but produces lower quality solutions for many of them. AltAltp produces
better solution quality than SAPA, and is also more efficient. However, AltAltp produces
lower quality solutions than LPG in four problems. LPG cannot solve one of the problems
and produces lower quality solutions in 5 of them.
Summary: In summary, we note that AltAltp is significantly superior in the elevator and
gripper domains. It also performs very well in the DriverLog, ZenoTravel, and Satellite
domains from the 2002 competition (Long & Fox, 2002). The performance of all planners
is similar in the Schedule domain. In the Logistics domain, the quality of AltAltp plans
are second only to those of Blackbox for the problems that this optimal planner can solve.
However, it scales up along with LPG to bigger size problems, returning very good step11. Blackbox and TP4 are not able to parse this domain.

648

fiOnline Parallelization of Plans with Heuristic State Search

250

900

AltAlt-PostProc
AltAlt-p
AltAlt

800

200

700
600

150

Time

Steps

500
400

100
300
200

50

100
0

0

1

4

7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

(b)

Figure 15: AltAlt and Post-Processing vs. AltAltp (Logistics domain)

450

60

AltAlt
AltAlt-PostProc
AltAlt-p

50

400
350
300

Time

40

Steps

30

250
200
150

20

100
10
50
0

0
1

2

3

4

5

6

7

8

Problems

9

10

11

12

13

14

1

15

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Problems

(a)

(b)

Figure 16: AltAlt and Post-Processing vs. AltAltp (Zenotravel domain)

length quality plans. TP4, the only other heuristic state search regression planner capable
of producing parallel plans is not able to scale up in most of the domains. SAPA, a heuristic
search progression planner, while competitive, is still outperformed by AltAltp in planning
time and solution quality.
649

fiSanchez & Kambhampati

SOLUTION: solution found (length = 9)
Time 1: load-truck(obj13,tru1,pos1) Level: 1
Time 1: load-truck(obj12,tru1,pos1) Level: 1
Time 1: load-truck(obj11,tru1,pos1) Level: 1
Time 2: drive-truck(tru1,pos1,apt1,cit1) Level: 1
Time 3: unload-truck(obj12,tru1,apt1) Level: 3
Time 3: fly-airplane(apn1,apt2,apt1) Level: 1
Time 3: unload-truck(obj11,tru1,apt1) Level: 3
Time 4: load-airplane(obj12,apn1,apt1) Level: 4
Time 4: load-airplane(obj11,apn1,apt1) Level: 4
Time 5: load-truck(obj21,tru2,pos2) Level: 1
Time 5: fly-airplane(apn1,apt1,apt2) Level: 2
Time 6: drive-truck(tru2,pos2,apt2,cit2) Level: 1
Time 6: unload-airplane(obj11,apn1,apt2) Level: 6
Time 7: load-truck(obj11,tru2,apt2) Level: 7
Time 7: unload-truck(obj21,tru2,apt2) Level: 3
Time 8: drive-truck(tru2,apt2,pos2,cit2) Level: 2
Time 9: unload-airplane(obj12,apn1,apt2) Level: 6
Time 9: unload-truck(obj13,tru1,apt1) Level: 3
Time 9: unload-truck(obj11,tru2,pos2) Level: 9
Total Number of actions in Plan: 19

POST PROCESSED PLAN ...
Time: 1 : load-truck(obj13,tru1,pos1)
Time: 1 : load-truck(obj12,tru1,pos1)
Time: 1 : load-truck(obj11,tru1,pos1)
Time: 1 : fly-airplane(apn1,apt2,apt1)
Time: 1 : load-truck(obj21,tru2,pos2)
Time: 2 : drive-truck(tru1,pos1,apt1,cit1)
Time: 2 : drive-truck(tru2,pos2,apt2,cit2)
Time: 3 : unload-truck(obj12,tru1,apt1)
Time: 3 : unload-truck(obj11,tru1,apt1)
Time: 3 : unload-truck(obj21,tru2,apt2)
Time: 3 : unload-truck(obj13,tru1,apt1)
Time: 4 : load-airplane(obj12,apn1,apt1)
Time: 4 : load-airplane(obj11,apn1,apt1)
Time: 5 : fly-airplane(apn1,apt1,apt2)
Time: 6 : unload-airplane(obj11,apn1,apt2)
Time: 6 : unload-airplane(obj12,apn1,apt2)
Time: 7 : load-truck(obj11,tru2,apt2)
Time: 8 : drive-truck(tru2,apt2,pos2,cit2)
Time: 9 : unload-truck(obj11,tru2,pos2)
END OF POST PROCESSING: Actions= 19 Length: 9

(a) AltAltp Solution

(b) AltAltp plus Post-processing

Figure 17: Plots showing that AltAltp solutions cannot be improved anymore by Postprocessing.

4.2 Comparison to Post-Processing Approaches
As we mentioned earlier (see Section 1), one way of producing parallel plans that has been
studied previously in the literature is to post-process sequential plans (Backstrom, 1998). To
compare online parallelization to post-processing, we have implemented Backstrom (1998)s
Minimal De-ordering Algorithm, and used it to post-process the sequential plans produced
by AltAlt (running with its default heuristic hAdjSum2M using a serial planning graph). In
this section we will compare our online parallelization procedure to this offline method.
The first set of experiments is on the Logistics domain (Bacchus, 2001). The results are
shown in Figure 15. As expected, the original AltAlt has the longest plans since it allows
only one action per time step. The plot shows that post-processing techniques do help in
reducing the makespan of the plans generated by AltAlt. However, we also notice that
AltAltp outputs plans with better makespan than either AltAlt or AltAlt followed by postprocessing. This shows that online parallelization is a better approach than post-processing
sequential plans. Moreover, the plot in Figure 15(b) shows that the time taken by AltAltp
is largely comparable to that taken by the other two approaches. In fact, there is not much
additional cost overhead in our procedure.
Figure 16 repeats these experiments in the ZenoTravel domain (Long & Fox, 2002). Once
again, we see that AltAltp produces better makespan than post-processing the sequential
plans of AltAlt. Notice that this time, AltAlt plus post-processing is clearly less efficient
650

fiOnline Parallelization of Plans with Heuristic State Search

140

Logistics AIPS-00

600

AltAlt-p
AltAlt-p NoPush
AltAlt-p AGR

120

Logistics AIPS-00

500

100

Time

Steps

400

80

300

60
200

40
100

20

0

0

1

4

7

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

100

(b)

Satellite AIPS-02

250

Satellite AIPS-02

90

AltAlt-p
AltAlt-NoPush
AltAlt-p AGR

80

200

60

150

Time

Steps

70

50
40

100

30
20

50

10
0

0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20

Problems

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

Problems

(c)

(d)

Figure 18: Analyzing the effect of the Pushup procedure

than either of the other two approaches. In summary, the results of this section demonstrate
that AltAltp is superior to AltAlt plus post-processing.
One might wonder if the plans generated by AltAltp can also benefit from the postprocessing phase. We have investigated this issue and found that the specific post-processing
routines that we used do not produce any further improvements. The main reason for
this behavior is that the Pushup procedure already tries to exploit any opportunity for
shortening the plan length by promoting actions up in the partial plan. As an illustrative
example, we show, in Figure 17, the parallel plan output by AltAltp for a problem from the
logistics domain (logistics-4-1 from Bacchus, 2001), and the result of post-processing
651

fiSanchez & Kambhampati

80

140

Logistics AIPS-00

BlocksWorld AIPS-00

Serial PG
Parallel PG

70

120

60

AltAlt-p
AltAlt

100

50

Time

Steps

80

40

60

30
40

20
20

10

0

0
1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

1

3

5

7

9

11

13

Problems

15

17

19

21

Problems

23

25

27

29

31

33

35

(b) Solving a Serial domain

(a) Utility of using Parallel Planning Graphs

Figure 19: Plots showing the utility of using parallel planning graphs in computing the
heuristics, and characterizing the overhead incurred by AltAltp in serial domains.

this solution. Although the two solutions differ in terms of step contents, we notice that
they have the same step length. The difference in step contents can be explained by the fact
that the de-ordering algorithm relaxes the ordering relations in the plan, allowing for some
actions to come earlier, while Pushup always moves actions towards the end of the plan.
We have run more comprehensive studies in three different domains (Logistics, Satellite and
Zenotravel), and found that in no case is the step length of a plan produced by AltAltp
improved by the post-processing routine (we omit the comparison plots since they essentially
show the curves corresponding to AltAltp and AltAltp with post-processing coincident).12
4.3 Ablation Studies
This section attempts to analyze the impact of the different parts of AltAltp on its performance.
Utility of the Pushup Procedure: Figure 18 shows the effects of running AltAltp with
and without the Pushup procedure (but with the fattening procedure), as well as running
it with a more aggressive version of Pushup, which as described in Section 3, re-expands all
the nodes in the search branch, after an action has been pushed up. We can see that running
AltAltp with the Pushup and fattening procedures is better than just the latter. Comparison
of results in Figure 15(a) and Figure 18(a) shows that even just the fattening procedure
performs better than the original AltAlt. In Figure 18(b) we can see that although the
Pushup procedure does not add much overhead, the aggressive version of Pushup does get
quite expensive. We also notice that only around 20 problems are solved within time limits
12. We also verified this with at least one problem in all the other domains.

652

fiOnline Parallelization of Plans with Heuristic State Search

with aggressive Pushup. The plots in Figure 18(c) and Figure 18(d) show the results of the
same experiments in the Satellite domain. We see that the situation is quite similar in this
domain. We can conclude then that the Pushup procedure, used to offset the greediness of
the algorithm, achieves its purpose.
Utility of basing heuristics on Parallel Planning Graphs: We can see in Figure 19(a) that using parallel planning graph as the basis for deriving heuristic estimates
in AltAltp is a winning idea. The serial planning graph overestimates the heuristic values
in terms of steps, producing somewhat longer parallel solutions. The fact that the version
using serial planning graph runs out of time in many problems also demonstrates that the
running times are also improved by the use of parallel planning graphs.
Comparison to AltAlt: One final concern would be how much of an extra computational
hit is taken by the AltAltp algorithm in serial domains (e.g. Blocks-World from Bacchus,
2001). We expect it to be negligible and to confirm our intuitions, we ran AltAltp on a set
of problems from the sequential Blocks-World domain. We see from the plot 19(b) that the
time performance between AltAlt and AltAltp are equivalent for almost all of the problems.

5. Related work
The idea of partial exploration of parallelizable sets of actions is not new (Kabanza, 1997;
Godefroid & Kabanza, 1991; Do & Kambhampati, 2001). It has been studied in the area
of concurrent and reactive planning, where one of the main goals is to approximate optimal
parallelism. However, most of the research there has been focused on forward chaining
planners (Kabanza, 1997), where the state of the world is completely known. It has been
implied that backward-search methods are not suitable for this kind of analysis (Godefroid
& Kabanza, 1991) because the search nodes correspond to partial states. We have shown
that backward-search methods can also be used to approximate parallel plans in the context
of classical planning.
Optimization of plans according to different criteria (e.g. execution time, quality, etc)
has also been done as a post-processing step. The post-processing computation of a given
plan to maximize its parallelism has been discussed by Backstrom (1998). Reordering and
de-ordering techniques are used to maximize the parallelism of the plan. In de-ordering
techniques ordering relations can only be removed, not added. In reordering, arbitrary
modifications to the plan are allowed. In the general case this problem is NP-Hard and
it is difficult to approximate (Backstrom, 1998). Furthermore, as discussed in Section 1
and 4, post-processing techniques are just concerned with modifying the order between the
existing actions of a given plan. Our approach not only considers modifying such orderings
but also inserting new actions online which can minimize the possible number of parallel
steps of the overall problem.
We have already discussed Graphplan based planners (Long & Fox, 1999; Kautz &
Selman, 1999), which return optimal plans based on the number of time steps. Graphplan
uses IDA* to include the greatest number of parallel actions at each time step of the
search. However, this iterative procedure is very time consuming and it does not provide
any guarantee on the number of actions in its final plans. There have been a few attempts
to minimize the number of actions in these planners (Huang, Selman, & Kautz, 1999) by
653

fiSanchez & Kambhampati

using some domain control knowledge based on the generation of rules for each specific
planning domain. The Graphplan algorithm tries to maximize its parallelism by satisfying
most of the subgoals at each time step, if the search fails then it backtracks and reduces
the set of parallel actions being considered one level before. AltAltp does the opposite, it
tries to guess initial parallel nodes given the heuristics, and iteratively adds more actions
to these nodes as possible with the Pushup procedure later during search.
More recently, there has been some work on generalizing forward state search to handle action concurrency in metric temporal domains. Of particular relevance to this work
are the Temporal TLPlan (Bacchus & Ady, 2001) and SAPA (Do & Kambhampati, 2001).
Both these planners are designed specifically for handling metric temporal domains, and use
similar search strategies. The main difference between them being that Temporal TLPlan
depends on hand-coded search control knowledge to guide its search, while SAPA (like
AltAltp ) uses heuristics derived from (temporal) planning graphs. As such, both these
planners can be co-opted to produce parallel plans in classical domains. Both these planners do a forward chaining search, and like AltAltp , both of them achieve concurrency
incrementally, without projecting sets of actions, in the following way. Normal forward
search planners start with the initial state S0 , corresponding to time t0 , consider all actions
that apply to S0 , and choose one, say a1 apply it to S0 , getting S1 . They simultaneously
progress the system clock from t0 to t1 . In order to allow for concurrency, the planners
by Bacchus and Ady (2001), and Do and Kambhampati (2001) essentially decouple the
action application and clock progression. At every point in the search, there is a nondeterministic choice - between progressing the clock, or applying (additional) actions at the
current time point. From the point of view of these planners, AltAltp can be seen as providing heuristic guidance for this non-deterministic choice (modulo the difference that AltAltp
does regression search). The results of empirical comparisons between AltAltp and SAPA,
which show that AltAltp outperforms SAPA, suggest that the heuristic strategies employed
in AltAltp including the incremental fattening, and the pushup procedure, can be gainfully
adapted to these planners to increase the concurrency in the solution plans. Finally, HSP*,
and TP4, its extension to temporal domains, are both heuristic state search planners using
regression that are capable of producing parallel plans (Haslum & Geffner, 2000). TP4 can
be seen as the regression version of the approach used in SAPA and temporal TLPlan. Our
experiments however demonstrate that neither of these planners scales well in comparison
to AltAltp .
The Pushup procedure can be seen as a plan compression procedure. As such, it is similar
to other plan compression procedures such as double-back optimization (Crawford, 1996).
One difference is that double-back is used in the context of a local search, while Pushup
is used in the context of a systematic search. Double-back could be also applied to any
finished plan or schedule, but as any other post-processing approach its outcome would
depend highly on the plan given as input.

6. Concluding Remarks
Motivated by the acknowledged inability of heuristic search planners to generate parallel
plans, we have developed and presented an approach to generate parallel plans in the
context of AltAlt, a heuristic state space planner. This is a challenging problem because of
654

fiOnline Parallelization of Plans with Heuristic State Search

the exponential branching factor incurred by naive methods. Our approach tries to avoid
the branching factor blow up by greedy and online parallelization of the evolving partial
plans. A plan compression procedure called Pushup is used to offset the ill effects of the
greedy search. Our empirical results show that in comparison to other planners capable of
producing parallel plans, AltAltp is able to provide reasonable quality parallel plans in a
fraction of the time of competing approaches. Our approach also seems to provide better
quality plans than can be achieved by post-processing sequential plans. Our results show
that AltAltp provides an attractive tradeoff between quality and efficiency in the generation
of parallel plans. In the future, we plan to adapt the AltAltp approach to metric temporal
domains, where the need for concurrency is more pressing. One idea is to adapt some of
the sources of strength in AltAltp to SAPA, a metric temporal planner being developed in
our group (Do & Kambhampati, 2001).

Acknowledgments
We thank Minh B. Do and XuanLong Nguyen for helpful discussions and feedback. We also
thank David Smith and the JAIR reviewers for many constructive comments. This research
is supported in part by the NASA grants NAG2-1461 and NCC-1225, and the NSF grant
IRI-9801676.

References
Bacchus, F. (2001). The AIPS00 planning competition. AI Magazine, 22 (3), 4756.
Bacchus, F., & Ady, M. (2001). Planning with resources and concurrency: a forward
chaining approach. In Proceedings of IJCAI-01, pp. 417424.
Backstrom, C. (1998). Computational aspects of reordering plans. Journal of Artificial
Intelligence Research, 9, 99137.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: new results. In Proceedings
of ECP-99.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism
for planning. In Proceedings of AAAI-97, pp. 714719. AAAI Press.
Crawford, J. (1996). An approach to resource-constrained project scheduling. In Proceedings
of the 1996 Artificial Intelligence and Manufacturing Research Planning Workshop.
AAAI Press.
Do, M. B., & Kambhampati, S. (2000). Solving planning graph by compiling it into a CSP.
In Proceedings of AIPS-00, pp. 8291.
Do, M. B., & Kambhampati, S. (2001). SAPA: a domain-independent heuristic metric
temporal planner. In Proceedings of ECP-01.
655

fiSanchez & Kambhampati

Fikes, R., & Nilsson, N. (1971). Strips: a new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (3-4), 189208.
Gerevini, A., & Serina, I. (2002). LPG: A planner based on local search for planning graphs.
In Proceedings of AIPS-02. AAAI Press.
Godefroid, P., & Kabanza, F. (1991). An efficient reactive planner for synthesizing reactive
plans. In Proceedings of AAAI-91, Vol. 2, pp. 640645. AAAI Press/MIT Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings
of AIPS-00, pp. 140149.
Haslum, P., & Geffner, H. (2001). Heuristic planning with time and resources. In Proceedings
of ECP-01. Springer.
Huang, Y., Selman, B., & Kautz, H. (1999). Control knowledge in planning: benefits and
tradeoffs. In Proceedings of AAAI/IAAI-99, pp. 511517.
Kabanza, F. (1997). Planning and verifying reactive plans (position paper). In Proceedings
of AAAI-97 Workshop on Immobots: Theories of Action, Planning and Control.
Kambhampati, S., & Sanchez, R. (2000). Distance based goal ordering heuristics for graphplan. In Proceedings of AIPS-00, pp. 315322.
Kautz, H., & Selman, B. (1996). Pushing the envelope: planning, propositional logic, and
stochastic search. In Proceedings of AAAI-96, pp. 11941201. AAAI Press.
Kautz, H., & Selman, B. (1999). Blackbox: unifying sat-based and graph-based planning.
In Proceedings of IJCAI-99.
Koehler, J. (1999). RIFO within IPP. Tech. rep. 126, University of Freiburg.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Long, D., & Fox, M. (1999). Efficient implementation of the plan graph in STAN. Journal
of Artificial Intelligence Research, 10, 87115.
Long, D., & Fox, M. (2002). The 3rd international planning competition: results and
analysis. To appear in JAIR.
McDermott, D. (1999). Using regression-match graphs to control search in planning. Artificial Intelligence, 109 (1-2), 111159.
McDermott, D. (2000). The 1998 AI planning systems competition. AI Magazine, 21 (2),
3555.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective and admissible heuristics from
the planning graph. In Proceedings of AAAI/IAAI-00, pp. 798805.
Nguyen, X., Kambhampati, S., & Sanchez, R. (2002). Planning graph as the basis for deriving heuristics for plan synthesis by state space and CSP search. Artificial Intelligence,
135 (1-2), 73123.
656

fiOnline Parallelization of Plans with Heuristic State Search

Sanchez, R., Nguyen, X., & Kambhampati, S. (2000). AltAlt: combining the advantages of
graphplan and heuristics state search. In Proceedings of KBCS-00. Bombay, India.

657

fi
	ff
fi 
			 ! #"$ % 
'&)( *,+.-//0213( 0*54'( 6
7

89:;< =>*2?
/-!@A:	%&=CB2?
/0

DFEHGJILKNMPO3ERQLSUTRS>VXW2EHI

Y[Z$\]^ _a`Pb,cd^ea^'`gf

hjilknmpoqrsRhjtvuwxay{z}|lr~yr$

Lc\]Z$#jcv`

tvknm}sRhjtvuwxay{z}|lr~yr$

}
 jU 'XR j<

2
 
2  9'v2nn9U 'UvC)2{v

>a3v
5.g.2<<! 9nga{)a5<gaHja{.XpCa

.a~ff 9a
~~2 
Ua!UvR#9>vaU55a
!g52!{vnd,!#dj2a5dj5aa<apa{9R
.55{~Xj 5a{>ff93.~59 52!R !5{~a59va
 <{#U}.)9v29!a2R255a{n
C{J9a5aa

5a{9a$<.
>a95{<2ag>5a{ap>2n599X
5n<{!aja>aa#$9{ffH25
g.a595)5<{ 95
55ffan5Lvaa9{5a.9J 5vaH9
5a{!C!59!!5a,5R !C5593{.a9p
<<
L25a<a>9 a
3Jn <,jv 
Hff}$XJdpC$XlffU$	,ff
fi$vp)2$lp$>
Jl

$llJdpp>2p,ffv,Jd>J22J
 2$)ff"!ld$2#

l$2%
ff2&pdp<'
2$})('2*+
<dl-,X$.!/10#v*23354+6798-.
""$$*
.p:

vXp+2}2!
2pR22$Jld$*+2$RpC$!$2$jv


'
RJ)$RJ2JllJ22;2)p,"d
 $2 2 $~J 2 2 $
 $2l
 >>
 R?
 ; 2#@Xj$;2?!$2$;2$*
< Rp=)
2p292 pRff$l2~

 ff$$p
 2 !@2 p)p
 U
 2 %AB$
 J$
 ,
! $2 $,$)X
 
pX!&)
 $ff2D$
C E(F
0  G/IHB$
 p2*J23KL+Md
N l
 X)
 $*O233P)
6 $
 
(
$




$




2

,




>


T

$


*

2

3

L
+
L
.
M
0
,




U



ff
V


*

2

3
V
K
W
2
7
6
X

A
R
W


C


$




d


*
$
ff

$

$






p







2






J




2
p

d

$

,

ff


$






Y









<
Q+ROS
$2}
! 2 Z([\p
 2}=
 H
 T$*]2333^M
_ $$JR
 $*`aaa=6-$
 2!b2 $@>
c 2 p)
d $V$
 9$3p
 "!
ff2b
! l
 
!  $
 ff$ef = 9;g)
 $2 JE$
C v
 }$
 2$
 *2 $; J2 pd$) "\ 
Jdp$
 $&2 $v$2}
 l
 $
 $; 9$2 l
 <#

 932X$
 " *V)
 $9@.l
  
)$2 92 $)
 bC
 hVp
 W:
 )J2
! l22?!
 ff$$i2 $>
 
C$
 l!jp
 $Xj$
 	,
 YX
 .$
 $<
 JX"2 +Xk^$
 +2 $2 
< $R$

.2
 pX
! ffl$Jdp]el
 = 9; ffl$2p
 2 g $
 k^p
 *2 pCJ2 pd$]
! $,l
 

 2
 Bm)p
  nC$
 * 
 $
 ;2 l
 * 2 $
 ff"
! l
 ~$2}
 l
 $
 RpC $
 $ $oX
 >
 
aVp
4 $
 aVrq^ < p>J2 pd$Ol
 "$
 $-
 d$
 gg2 p,lff"2 $2 g
fi.l
 2 
 
V2 pYp
 >
 "*5R
 $
X	,
 jX
 .l
 ps(Fg
t 2u/v
d ff$X*`aa2WH
6 jp
 $X:X
 .l
 ps(fiw +
 
! =H
 
 $*J`aa`67
t ;2 x,
g
 ff $i
 J
 $p
 
* p	,
 v
* bX
 .$
 l!:2 $)ff"
! l
 d$2l
 l
 2 %
 jff2 &p
 dp
\2
 pyp
 >
 "
 $22 $)R
8 z Dp
 	 2ff9p$Wb
 d$
 z2 p; +2 
 
 l
 ?$
 !
 $X$
p
  	,
 gX
 .l
 $)E2 pLlff"2 $2 z
F.$
 2 
 GR
 $zC
 zX
 .$
 $R{2 pJff"
! ?$
 
EX
 p9$|p
 @2 pXj
}2 p|>
c $;~  9$p
 b2 $j>
 pp
 b^{~
t 2?
$
 u
d ff$XE(`aa`67

  -//0v %% !=5
=}C2
Y3
!fi{;
9v:!	%&%		a&2%l%?2 =	

fi oxorlzGir








$!p2&2-'pJlg
d2lff<$ff"!$$vp#o('C$	6 e52?!!JOtg$!+#$p
 v$22X$
p
  Z*Y}
 "p
 dpb ?	'( =6bH 2?!Dpdp$+
$2
 &2 $
! + ;
 lff I(fi67*g'( W6 < ,
  ff"
! $
 :p
 v$J$
 +2p&2$!+;
lff I'(  6HBp
 2 pp\
  lff o
  2.
! up
 dpff"
! l
 Mx2 p
2
 p-p
 dp$
 2D+h2 $R
 lff -
 
! } oF$	

ef2$ff"2}>C$2+<H2pvL|X.l$l!2)$L$<"2L2ff;#
$2XyX
X$2l!2?|2p$2-2$U2}?$;,
g
pdpX"&yX.$|2p
ff"!?$

@2!=LLX22$l

 	)
 
! +2 p
  2 phX
 .$
 $$ff2s)
 $2 
< p^p
"p2.$$?! ?$;9e2
2$$2d.p2Jl2W:$$C~ff"!lx.+23$
="!Jue JX"2= 2$
~ &2 $@
 $
 $
! $
  ff$$2Wd
 2 E 9.$
 k^p
  ff2
p@."$$p$2-w L$2X@ $>
 Wv
 Hffn2 pX
 .$
 $2l
! 2 
 $
! 2 $l
C ;
w py2pU2"l$<'p22 "
  "
* p
 
 $C
 d
 ''
 ^l
 "bp
 XL
 ff$$p
 2  $
 k^p
 
X
~$2l!2
 ld
 !b2 $$
X
 !b.$
 ff2 
 X
 x2 p?2 
efsNv2E`D,
 d
 l?\2 $b
! p
 j2 pv
YX
 .$
 $2}
! 2 *OR
 $ p2$X
 
J2
! $2 $$
 uv
N 2 \^ielnd
N 2 D4b>
 2pW>2 pjX
 $dp
 U
%2 p$
! 2 $ 
 
WL
 $l
 D*)
 $2<|v
N 2 &qj2 pU2"$
 2 ff2pW-b
 2U2"2 }p
 >
 "d
 '
 ?$
 $*
nv
N 2 nP^*.>
 $
 $
 $$
 $$
 2>X22$l
 Rl2 lx
 :
fi"$
 22ff9
y \g  j{ I X  O

L3   i 

 $
>
 pG[ff"WZp>" $Cps+{2pX=2.(TT69 pX$p+ 2 

pdp-   < pl2ll$&|C$h2pgp,ffJ"2;2 C$2X"2$
 (Ti6i>


  (T  6

0lff\)pPp+2;2pffff!$R$2ll$

(f2W6
W


)pv$u('pp^T67*>2$Z!p2$L2k^$22pff$2

2-+lX
	C
B2p

d$=2$+;2

tg)lH$Cp2p 2H
>lffg$vp*J U *hX@2$y$vp-2$lff9  
2
2p2;C
 
%2 pjp
 >
 "Ym)pD2*.
$"2$*V2pj.$
]2$)$vpCX+2
2$
 +2 
 
 ;
 p
  ff"
! $
 %p
 dp* R
 $d
 l
 $!&2 $ ffff
! $
 %p
 v$R$2 2$

 \2 $
.$
 E(fi)
 $9}2 p@
[ ff"
 WD}
 U
 
967
Ux



W?+ ;

ff	

 T 

(`6

fix |lr.u>k  i.$impo  r

Nvi
#?$;2$i$!p:2	egJpOp2ff"*pW>*2$?pCpg2$!+glff92
 >)2+l
 X
 gd

 2 p'p
 dp* F$ 
* l
 2p'2 p>lff  ff2V*.Op
 #
 ?$
 $
 p
< pjX
^u}(F
 $!$
 2|2  67}H "
 =
 $
 l$2 D
 :2 p2;
 
'2 p2 p"
 x2 $-2 p2~ 
'2 p
2 g"2  l*i?'uu F$ l
* U2Xp
 
! D $d
 R
 J2 2 l
 
NV$
 $X-2 lY>
 jVp
 WF
 Llffff2 $
 ffC;2 J
! 
]2 plff9 p
 v$*'  ?	*2 p
 J 2 !
2pl
 $$2 $
 X$; $.2 
 
9  
! $\ U C;
 (   5  ?6'


   ?6
 (     F$ 
  F$  F$

(6

< $gd$22O2}xJU2l}?,"<2px[ff"Wyl.]
2px.$

X<$G F$  < +lx2pL"$]^$@Xff2L$pX$$=C
'2p 2;R
'2p@p,ff$
uX
 g
 .$
 $2 p:^
 v
 }$2 $
 p!&2 $"$
 2 
 :^
 =
 2 $RJ2 l$;2  
J2
 pdop
N ff9$3
* $
X>
 &>
 $
 GVp
 W2 pp
 
 $; $.2 
 	@2 $lff p
 dp*J>
 

 $
  g2 pff"
! $
 X$l
 l
 
 
9
 (  6i



 (    F$   ?6  (  ?6
 ?   U   F$   F$

(fi4+6

8B
"".$$*H!p)> pph^p	2p $;9$2  (  U6$2p
2G>
.p
 J   (  67|w *#$W>*<;2p"2 l2X"2
}2pffff!$
$"
 $2 *$
 J$p
 $X,$
 	,
 xX
 .l
 $i
 C;2  
'  i$$;2$*php$X
X.$
   
 
 2|X
 j
 $
 h+
 J.2 !
 (   6i

5V

+   ? 1



   ?6^x(  U6
 (   7  F$ 
  ?   U   F$   F$

(q6

.$$@2p$"+2@  U   ? x(  ?6al
}(  ?6y2 < p$"$2

  ?   ?
x(  U
6 2X$
 $' u?=X
2UlffJ9-(F.$$i2p!$$;+276lb
2py>
 ;
 n'( p2B2 pJ5p
 $
 )
6 ff"
! $
 ]ffp
 
 9  mB^d$
 2$X
* $
9x(  ?6

(



?



}
6

2
p




$
$

X



x








$






2

2



2






l
$

}



|


2
p






d




B

ff




p


.
*


.





D

!




p








(



?



6
C
..^p
 	:
 
 
$
 D>
 $Wg 5p
 ~ WR3X22$l
 R$" $2 $
 mjp
 
 2!l
 $~ 9k^$
 2 \J
q 
2J}$
 ff)$2
! 9!G( S 
6 $2}
 R
 $2 Ujffffl
 :w $2 2ffk+l
 2 
 uCl
>
 W>
 :X
 .$
 $ J (   6
8 . p
R
 	>
 $$
 
 $
 l
 p+ 
 2  X
 &x(  ?6>
 $
! +
 $+ E2 p
k^$
 2  
 	\c} "p
 $Xb2 $>
 2. J Zp
 $X$
  W>
 
 $
 $s2 p
ff"
! ?$
 ~l

 2 p%$
 vp]2 $bl%R
 W >
 >
 }$
 ,
.2 $O^$
 W)
 .
! 	ow U="2 $
!  
p
 2 l
 ,}

 v2 l

 ffff
! $
 ^p
 v$O>
 }
 $
 pff9$
 2 
 U.]2 p2Y2 l
 ff2U".l
 2 

i ? XmB+d$
 $*+R
 pp
 >
 jC$
 2.
 J. h
 $
 $>n%
  )
 p2%
 %  ? ( *
 uX
>
 "p
 2j2 l

x(  U6'}(   T6Y  (   6
(P6
  ?   ? 
 
$2ff$
Y2$W>x$ < pHk+l2$Uff2jp2$?!JB2lVL++2
x(  ?6B)${>W

$lpJ{2ph S #$l
EXk+l2sq^ < pX2
 
  2$)ff2"l2
'UR9p C $ !&`- 
  )V$"+2

fi)T	'TfFffT	ffTf9iJr	9T	9FW 
 Tx
		;ffTY7j;"Tr'ffiff		T	5
	)l  
JpT
  




ff=

fi oxorlzGir




$!p2`^ < pff2 R$x(  (  - 6$;2p	:Ix(aa=6Jl$2$!2$
2p $;9$2Dp9$~ =6 < p ^v h2p WCl } < p
$ff"
 Llp
  2p	 pW 2 $^9 sX2;s^ $l!ff9Ly#
 DX
 .$
 l>)$
 ; +2 )?h2 pp
 ffUl2
! ?L
! $2l
 u%6 < $>2ff$
 $2 

  X$d
 pp < pnl
 " ?p
 2p	2 plp
 &)
 p2u2 $u
fi$
 2 

6 U$
 ;2 +:
 2 $Clffff2 $
 ffU$l
 D
    (  / G29  (  - 6}(  (  - C
efz2 $B>
 Wg
* 
 2 
  G+\
 $
 $HJff*3$2l!b\*
2p2^yC$$?!-2!=X$$%
92p)ffff!$ < $$2d2bXU2X&
jl2
<

 ff"
! l
 p
 v$},
 l2 u.+2 3+"
! $
 LC2$

"!$#

&%\('*)l\nZ,+-+ ^'`.)

bC^'`gcvZ$\

efDU
;2$lff$ffU$2!!*9k^$2nqDXv$2

0/i13 2 6
4  55 4 
5

5

)pj2pjnff9lUff2pCp

4

1

a87:9D4  

=; <

(TL6

1x(  U 6

(K6



(36


   U6'  (  V  ?6
 (    F 5
  F$  F $
  (   6
9?>?@ (  (  B A=D  A=? 6 ?6
 > 
; ?

C @ 
C   (   6

E

 
 BF  U        
  DG

(f2a=6

)p
@ (   DA=  ?6\ 2IH>2$"2
2$G$vpoX21pdp22$ ?PFff2
k^$
  < ^$}2pl&$>
]2$J9 2X$hE  UW*2pW)U
K9[$ ; 2p
$
 ; 9=2g(
%)$o,l@`- 
  x
\ 
   U67 < pp?!
%2p2	)C
L9
ff
M

fix |lr.u>k  i.$impo  r

 R
! $
 }2 p2Jg
X2 < $l;+2$:x(  ?6>2p$hX
; U22ff=U
p$~uX$9N9[$ ;  +2k^$9!
$


  U

x(  U6x2

C

$


  U

x(  ?6}

C
2

(f22W6

 2 4  22X$$  (   F67 < p@p!2$ 
   (   6C\
.lD+
< p5d@$ 
)1O
$2?! C 13
 2 4  x2$QP2$g
F.$2

"!RTSVU ^ + K Wc8XZY~Z,+ K Wc
e
!p2!&pp2*  / *%2$lff9
2 2$2;
x2p|p,"+G>2p
 p
p
 $
 *  ( l
   - w \>+o J.nX.$$2pff"!$  (  / {2W67 Nd$D>
pEp
 |^p
 	2 pl; $ 2  Wo2plffopdp*Y,lu $2pC2$l
$"
 $2 $
 }#j9k^$
 2 @q^ < $2
 ',
 >= v.l
 %2 $i
 lffJ9Ox(  (  - 6 < $'!
]p
 Ws HJ  92 p>$
 @$
  $
 5p
 @l
 F
2 pX
fi.l
 2 @
(
G29  (  - 6Vx(  (  - 6
 (-   /
ef
 !p
 2Eff
` >
  2pW ?12 p2 $Jl
 2$
 2 p l(fi2 p ff"
!  ^
 9 
6 )
 $912 p
$"
 $2 x(  (  - C
6 x(aa=U
6 J}$2 $y
! $n+
 p
 ?+$
 }2 $g2 p22 p-n?p
 
w  l
* $W>
 *'2 pff9J  '( 
! |$
 vp	6)
 $
 $  (  ( 
6 $
   ( - 6
D2 pJ$l
 D < pJ2; 9)2 pJl{
 $!p
 bff
` h
Fp
 "2 $3
* 2$
 y
 $
 ;2 $
 n'( 2
9k^$
 2 \P6

w

x(  ( G2W69




x(  ( {2[A  - 6ix(;2a=6.\x(;22W6}

l
plpX$p+l;+2*))$9Fff2

pW:

 (  ( G2W6

(f2`6

$!p2`fflp?2p

^

mB^d$2$*^$$?!2lR?
92\2pJl\)$9G}(  (  - 6RWH2;9
2l,
 2p	:
 |
 !p
 2-`5Jefb2 pH2JC.!$
 2>
 R$WR$p|l
 ffLp*)l2X$
2$)l$
 ')
 p2-2 pQ
 P2 $B
fi$
 2 
 ,$
 ;2 +i
H ;2 $
 $ff9$
 ff$2
! !j v
)pJ$ $
 2"$
 ffb2 pH5d
 @$
  $
 &2 pR$
 $
  
2 $i
fi.l
 2 b2 ^$
 iX
 .$
 l!)2 p
ff"
! ?$
   (  / {2W6
 `cb~g
]_^oi a

< $
$2}Dw

 

edE

n

D`

X

0f

ff"}$ff"Gp>"E>|Cl;pp&2p


XU@>|R'$|
2p

9p2

fE(hgG69ji+U)B"kRlT-mgn7

  

kl "kl5

 omgqp

(f26

)po"kRl#g2$$;y?xz{ F$ $  p$!bpdp < $9p$2
2p;2  l
B"
 kl 6gs*i)
 l J2L2Lp$2pJ.22Z2Jze
x>
$*
 Up
 Jl
* g` 6 
 2 p)5e !y
! h
 $!p
 2b
2 2ff$
 !l
 l
 ff"|p
 dpd
* $
 $2 p
B
C.!$
 2 L
 
 !p
 2)2 ->
 $
 @X
 >?$
 $
 p0
 fj'C
 $2?
! g` 2 ]2 pUC.!p
 92 $
 
D
 $!p
 &2V
 $
 &
 F
 2 $:p
 
 pW:
 = &fj*.xp
 :
 $!$
 2&2 V}
* 2?$
 )2 $)
 lff9 
2
 X
 $p?
 ff"
! $
 @( p! 
 $!$
 2&2  67ief
 2 p-,
 l
 fEh( g{C
6 }2 p
 <

 
?}
 Y)
 $9{UuVu F U
 9p
 C
 ff"
! Y2 $r
 g*^,
 $
,
 H$2?
! :p
 dp)9>
 $
 
222
 $X
 .$
 $ffff
ff(s

fi oxorlzGir

Jf)'w 
(TT6$   (F6$?$$2~Y2$ pR$|~2X2$$
X
 .$
 Wz$"2-,"2+)$\{X.$$	
"}2
9(p!$!p
 2|2
 $
 67Yef
 + ;- tP
+)
 l2l
 l
 $2 l
 *.2 p@X
 .$
 $
 ff"!l	"l22
 .$
 @X
 J  2J}$G+ ff$
 .
! Wb2 pff"
! ?$
 j

  *l2lB>$W$
 |X
 .$
 $Ul
 Dp
 2 p2Jn$
 
w

j)lJ -X.l$9
UJ
})$9|2phXB
$L}ff

2ffRJJffb
 
 
AR
 E
 2$
 $ 
g> $W
 

uv!$#jw0x c8y]acv\Zp]n^Rzc{%HZ$\]
pjVUZ|}fZ>CX"
9

 $CJU2';%,U'py2pfi9P$ ;  
'2p S #l2lD*
2
2l&2$pX$$$$?L$2p JX.$$j
R$Z%
 ) ?	
 p
 x S # $l
  +d$
 2
 g U 	 < p)+$
 @X
 j
}'( p
 k^$$6
< p)^$X>
Jffffl>
$;9=2)Uk^$ 
  ~

` ~  
t
(f24+6
~
~


?







F


m)$2p S #$2l

Bpu2$
ff*,@"hJl2WX.$$}2$Hff2$Cp	

X
jR$U2lff

IH

< pJ}ffzC.!$2b2

$

*
$;2l*g

p+2*B2p|$ X)pGX.ln>J$LW2H"2lJ$)2p

1   Xk+l 2 s3n$2 ff$$$bffp < $E>J |2pop$$X$ W>
X.$$}
:2l|   ^$d!b2p S #$l)C5d$~$$$~'e
2p@p

.$
 hffp
 Jl2Wx2$gX.$*^,;2$*p2$")j>llp$
 2C2X u.+2 X=ff
! $
  C2pJiel
 $)2$2l9>
< $$ 2 $$v.$
pC$
 |X
 .l
 :X
 .
! =ff
! v
 B2 p22 $ J$WJ+R
 
fi p
 $ 92 
Ux2
 $
 p
 9=iel
%2 $Upl}
 )
 $
 $*V2 p$
! 9$2 $U; }
uv!R '+ K g]aZp]^'}`#Z,W  '*+ L WcYg^]
 S #$2l<$-|2p2!l$ ""$$J&2p)$9"p2 < pJ 22$}Jlp$

"$$2lUU2p	:h&XX$^$(Fd$l$*O23L367YcY.)J2ff2=2$*V

J;C
 $92 $2l
 92 pj^$
 @
 Cg

  2 $
 xp
 $>) Lp
 ffCo2 pB^$
 @X
 >

ffffl
 ))2 p^$
 @X
 )j

 $
 " +2 * R
 $$)2y( < v$J*`aa`67jef
 9$ 2 
 
 l$2
 ?
! 2 Jp
 l o
 X"
 9J*R
 $)2JX2 u2 $H
 2
! $
 
2pJ=2 
Y2 pJ 
 9 $
  < $2
 2 $ d
 X  J.2 2 $
 <Jld
 $n
 
$d
! p
 j S # $l
  Oc
?= *V)
 p2 92 p)^$
 X
 >g

 $
 ; 9=2  < p"$
 
l
 ff2 2 z2 J pX$
 $Rpd$
 \2 p lff"2 l
 ffRl2l
 D* H
 u
! p
 9 S #
$2}
 jp
 . D l
 
X2 $$
 2$
 $
}ffffl
 $
  $
 ; ?=2 ff2J2$
 }
  < $H
 
2pl2+ J2 pdD 2 l
 g
 Rlff :)
 $2 ZU)
 <

 J2ffU2$~ 
 "n  L
p
 |\V>
 Xp
  R$X>
 ) S # $2}
 X
 p
 X
 ]2 pXp
 l#$
 p
 9
 %2 p
	,
 YX
 .$
 %)
A  B2 $,
 ?b2 $ 2 *+,
 Hp@p
 Xp
 |  $.
! :2 pH _
 9 $
 & 
 pCp
 :2 pQ
 P2 $
F.$
 2 p
* +d$
 $
 $:nff"
; 
 2Xk+$ 2 E2aV < pg o1  *2 $R
)$2    < p2
 2*>
  d
 X
 o 2 J2 2 $
 Jlp
 $
 O

    - "'?= 

|p
 3$2 
! p
 $
 ; oR
 W k+l
 "d
 $Z2 p$
! $2 l =ff
! J2$
 $
 $ 
;2
  iel2 pjp
 V>
 22 $
* p	,
 *>
 $p292 $9 lRo2 pBl
 2,#

 @ 	
 $l
 D
 $
 p-2 $>2 *^,
 H2ffff&2 $Y)
 p2 pB S # $2l
  Yp
 Y 2 l
 d
* p
 
<  l
$C
 d
 )W
 R+
 $
 " +2 )p
 $$
* lWv
 .!b2 p$
%
! ;2 
! 2X
 .$
 $

ffW

fix |lr.u>k  i.$impo  r

1

Mean

0.5

Exact
|S
|=1
mar
|S
|=2
mar
| Smar | = 3

0

0.5

1
/w

1


w

50

100

150

200

0

1

Ring position

$!p2^-Hc>$;~.9$p9!
J`aapdp<?$$2~)2b2p22$$<$b>$!+2
R2$W:
 u2 p W>
 Hlff"
 
'2 p@C.!p
 2id
 H
 J$
 Hff2}; ->
 g-2 p
2"$
 $2 
x2 poX
 .$
  l2l
! 2  $
! 2 $D < p^$
 @
 
:p
 dp $
 $
 $ 
#
 $
 $ h2 p
! $
 J




^ * ~` 


< b!RJ?="$2.x2pX"
9$
]2$$!$2$ ,2L!u(pff"!
v#
6 
J`aagl
 ?$
 ff"yp
 v$%R
A dpjff2R.p
  jy2 p-,
c ;~.$$)$2Jj>$!+
! !o2 pL
! 
 $ p v$$o2 p22${)w $! +2 $ z2 p2p$Rff2ynff"d 
 2p	: 2
 poX
 ;  

 $!$
 2h^zp
 b2 $2J}$2}
  >
  J  2 $p
 
2.
! |p
 dpff"
! $
  < p2ff2l;  L p
 ?
 $!p
 2h^Gw 92 poX
 .$
 
$2}
! 2 
 $
! 2 $2 $2x2 J*R
 p2x>
 xnffy2 p5p
 @l
 I+$
 @X
 <
$
 vp<?$
 $
 p
;2*x;  
` l
 p;v^bef <,
 p|>
 lff9 p
 dp
2pj,
 op
 $
! +
 9,
i < ^$
 }2 pj2 $22X$
 h f  ` 0  ,
* f  ` 7  $
 {f  ` 6  
H >>ffY
 2 
H
 $!p
 2)@2 p2Jl;j
 $2&C $
 p
 +Y
 $
 $i
 }2 p(P$

x2
 pop
 v$up
 Jff"
! op
 
! 2 $o,
 $
! +2 <
* p	,
 *92 $9pp
 L
 "
 =uH
H 	)
 !
ff"
! }ff ff$
 J$2	-2 $2ff$
 $Bel
 )2 pLJ2 2 z2 C
 |
 
$
 l - 

!$# ^ Y~c%Z'}^'`]aeZl`.  '`z}cv\()}cv`c
elRUX2$l)p$g2$CdX+
'2pgX.$$2l!2$!$2l $$^2

,
c ;
~ .
 
! $
,
 B2 
  2
! :n$
 :
 C>
 $
! +2 )(  ,
6 $
 $
 -
 >2p2p$B(h+67
 #2
! @p
 v$ff"
! l
 ff2 k+l
 g
 "$
 $
 >
 " B
[ 2	
 p  o2 $R;d J "
 $X)l
p
 	,
 BX
 .$
 $U\2 p 2
! g$
 vpR2p$
 DX
 p+2  < $)J}}2 $-
 B2 p

y%5	-Yffiff		TlJpTj:=LLT	r7ZffWTff7;
ff


fi oxorlzGir

1

1
1
0.1
0.01

0.8

1.6

0.5

0.5

0.99

1.4
1.2

threshold

threshold

0.9

1.8

1

0

0.8

0.99

0.7
0.6
0.999

0
0.999

0.9

0.4
0.9

0.6

0.3

0.5

0.5

0.2

0.4

0.1

0.2
1
2

0.5

1

0
weight

1

1
2

0

2

1

0
weight

1

2





$!p2j4 < pJ2"l$)
nc,;~. 9$pJ9!
U ff"l$9ff"\^$@
xpdpbHH
2p22p$RlG,$!+2$W&2pJ2Jbffpgefz2p
fiR}p2py!ff 2p	:
 ,

 Z2 php
 $X $
  W>
 X
 .l
  
x2 pJ 
2 php
 dp < p$!9$2$
="
! >d
 Xp
 =2 $ 2 pC $
 X
 .$
 $?!-ffp
 ,jv
 ,$j
 j)$
 
! 
 )2 p$;2 l
  $2 ;d
 J. 2 yffp
 H$ff2 $
 % 8.*J)
 $2 )2 p
< +l
+$
 @X
 U<

 $ 2 $
  < plff  2p	:
 D2 $$
! +U
 lp
 

CdX+Y2$g
W)?! $$}
)=J"
 (   6'

5.

     1 

+ 

(       (    ( 6Vx(    (    ( 6

(f2q6

.$$x2p$;?=2
x(    ( 6x
x(    ( x
6 
x(    ( 6x

 (    ( 6i
6 
  (  ( i
  (    ( 6i

x(    ( 6x

J (    ( 6i

 (   6
  (  T6

(f2P6

  ( 6
O (  T6

(f2K6

(f2WL6
(f236

$ff9$b
:2pW>:X.l < p$$2$$2l}$D9k^$22qJx!$u+

 (       (    ( 6id@(h  

\ )

 (  

\ )

  ( 6

(`a=6

p2 2pJk^$2$p pb2pbCd?=)
  (   6H$   (   67bej"p7$H*
p	,
 *X2$J$L 2b p?p2$Cd X= 
2pDplL$ 	,bX.$ 
2pJp
* p$
   (    2W6 C   (    C 2W6j$   (    2W6 C  (    C 2W6ic}Y2$,$}p
HX)
 D2 p$$
 l)"$
 $2 Ux2 
! ="p
 )$
 +"!$ X
efn
 $!p
 j
4 &2 p $IHX2$
 gX
 >
 \2 pp
 $X$
 W>
 :X.lu2pJ$))2p	:

&nff9$
 9p
)2 pD>
 $
! +
 $
 2 $22pJH
H &>
 2WX
 
 2D2 $
! +&X
 .$
 $LX
 
.2
 $
 
 2}>
 $
! +2 $
 2J)
 $
 ff"
! *YL
 X2$2 $D>
 $
! +2 *XR
 p2|$
 
! 2 $
>$
! =2 j2"$
 
 ?L
 2 pjXv*.
 "2 p
 V#T 9$v+X
 .l
 $Oelj
 2$$
 X
 J=2 p
 
* p	,
 W*
2$'2 p2>"$
 $2 222l
 @ j2 $pU
 2 pU2;<
 $
 ; x("f U
2 l
 Dr?`6
ff(

fix |lr.u>k  i.$impo  r


x2p)$l2l!2$!2$D < pBX.$$>2&XH$2Wo^$ff"!!2p
$
 " x>
 2W D
 $!p
 ^
 - b2 $BC.v
C
 X+92 p)X
 .$
 $2l!2!$2$ +"!dXp=2$ < p
$"2
 $
 ) &2 p;d
 J. 2 )ffp
 DX
 )
 ; 
   *)
 p2t Y2pg^$@XU
$92$
$
 aPRD^$
 @X
 -X
 >
 {~ 2$
 $
 L?$
 $2 .!&2 p+"
! $
 X < p j 
p
  *^2 p2	,
 :2 p$
! $2 $ +"
! XelD
 $!$
 2)4{
  C2$W:h
 x2 p2Jj>
 $
! +2 
$
 G2 p2p$ge
 ffj2 $
 ff"
! ,
 $
! +2 l
 .$
 L2W>
 +"
! $
  < l
 ))
 $
>h
! $
 
 $l
 l
 2 92 l)
 ff2 p ?$
 ;2 H2	 p	:D2 p+"
! $
 

G v `


` 

~ 

 ~<

< 22i2p:k^$$
2pxX.$ $2}!2$!2$I
jJ29$!?!jp>"d*=>
J.
 \X
 .l
 $
 C
c W
 2Ep,ffZ(TNv2Eq^2W67*2lHJ$$nl?ff"2
C$2+2
 2 $g

 2>
 $2}
 Div
N $
 $$*^>
 H2$W12 $Y2 p-X
 .l
 $2}
! 2 
J2
 pd 
 z$
  C$
 P=2 $
! =hX
 .$
 l&
  ff"
! z
e 2!s
!  (Tv
N 2 q^r` $
 
C$
 $3
* Ev
N 2 Eq^r^*O>
 2p	 2"$
 2 )
 hl
 $# lff"2 $ 
! fflp
 g2 p2&p
 >
 "d
 *O>
 b)
 
$.
 l2$
 2X
 .$
  $l
! 2   22 )
 2 E2 p$
 ; @nff2  2 pv(F
d ff$X
/w 
! $
 ff *`aa`6x)
 $9>p
 
%2 pjX
 ; )ff$$2Wd
 2 
 J2 pd$}
 
! p
 
! ff}$
w pp
 )X2$l
 *^,
 2pW>2 pp
 R
 ff"
! ?$
 

"!$#jw0x
c

S WZl\
+c]'l\nf

 hHHff
p>" 0 $$$2Z$>")l  2$2+2$2$
R2C

< p
cC2p>"EeCL$!$$ p2$X+c>l Jx(f23K36LLz$>"

J$
  
! l2 =2  + $
 2$ff2he $2;2
:LDpdp
x,*2p2
p"2
$
 H2#p
 
! \ pp
 
 J2 2 $
 Belz
 $!p
 2bq&2 $@
! ff}$Jv$gH2p	:
m)p
 :2 ;h>
 : p p
 j2 p
! gp
 dpffff
! $
 ?
 l
 $
 

 +
 vp$
 X$
 R9
pdpz2 pp
 
 ff"
! ?$
 )l2l
 l
 $2 b
 $2 "2  ff2 2p	:P
 $$~ +2 Hp
  < p
 $
 zX
 ;  
92 pL22 .
! ff2.$
 z2 p2Lp
 H$
 $ 2 pyp
 $X$
 W>
 jX
 .$
 $
2X2
 $|p
 2 $jp
 ,
 "{>
 |$
 2fE(`
q A"aaa=6 7 &e
Y>
 
 |2 l$*]>
  2
p
 2 $|2 p?$
 
[ ff"
 	l
 ,
 

 2
! Bp
 dpHff"
! $
  < $H$
! 9$2 $ ="
! 

 X
 
 2$
 ?+ 
w 2b2 $)2 pbX
 .$
 $Hz2 p	,
 
fi
 ffp
 ff2bff
 2 D2 $Jd
 gffp
 @w 
s
! $+"$
 $2 GR
 ={2 $$ff$X$
 umBl
 2"&2 $yp
 dp
L  $2 	:  lG2 p
2Rg

 lff9 Y
 xp
 dp-2- bP^ < +l
 ,222 $
 X
 ,
 o2 p2Bp
 dp>$
 2 pR;,
 

2pbp
 ,
 "
 ff22 p2
! Gp
 dp&L+*R
 $J$
 )2 lB2 $B$
 "2 +
 $
 $
 pX$
 $Rz2 p
.$
 ff2 =
 
ip
 dpL+ < p; Hlff$X$
 x |X
 gff
 2X'( $
 2p-2 $R
 H&
! n'
 76
$
 D2 $2
 g2 pff"
! ?$
 X
 .$
 $Y
 :p
 dpg2j 
P ff2j"h2 $
! +
 .$
 $~?g2 pYp
 $X3$
! +#
 ff$
 
* @2 p,2 pl$
 ff
* ff2Xk^$
 $ Xv < $~glff"2 $
< $iX
.p
 - y2 pH$$
 2$
 
J2 p.p
 2 l
 X2 $2Hl
 2 $B^$
 @
 >g

 ;2  C9
O2 p)p
 dp>$
ef @l
 l
 2 Z2 $ $  ff"
! ;2  lJ2E
 J2>$
 ; 9 < p2
 2*'>
 
.p
 U
 J  )
 $
 $
 2 C

 @l
 $2 l-p
 v$*+R
 $
! p
 $
 $Y y>
 
 X
 .$
 $

W*5K	5ffX;7t	W;J5			- TT	ffr7t]lfi7c]ffpT7Bff
,h,$Rh$(
 y
 xF"T
	 JpT_F 9 7
	7 W	 ffrpr	lT;ffy
	 T57 7	x

ff T W5	[		i	7		JpT
7f5FTT%"T'5	 7. B
	 F"Tr(fiff T7JT	x7TTxf75	xr(Y
 rffc7 l]f5FTlTf
ff[

fi oxorlzGir

21. pulmembolus

12. minvolset

34. pap

19. fio2

13. ventmach

14. venttube

11. disconnect

22. shunt

9. intubation

35. press

10. kinkedtube

16. ventalv

15. ventlung

33. minvol

17. artco2

32. expco2

8. errlowoutput

27. hr

29. errcauter

28. hrbp

30. hrekg

31. hrsat

20. pvsat

24. anaphylaxis

23. sao2

18. insuffanesth

1. lvfailure

3. hypovolemia

25. tpr

26. catechol

2. history

4. lvedvolume

7. strokevolume

5. cvp

6. pcwp

36. co

37. bp

$!p2q^ < ppHff"!$@(fi2$"$$~+2g$76>$u2$gX.$$(2!76x
B2p
ffp
 ,
 "ov
N b
 $
 $Hff2D2 $
! +j2 $g2 $J22 
! jp
 jv2$l
 @w 
 {fE(`
$
q A"aaa=6

ff(

fix |lr.u>k  i.$impo  r

0.6
5
0.5
10
0.4

15
20

0.3

25
0.2
30
0.1

35
40
5

10

15

20

25

30

35

40

$!p2P^ < pBl$o)2z(Fp$XC^$,W>x$ 6
gupdp@4a5V4aye
2!!
2pW-^j2
 p9}
 "Vp
 2#
2 p,ffk+$
 ff < pjpff"!$$$]XjJ 

 <"$
 9ffff
! Yp
 ,
 ff  < p>p
 2p	s2 $XX
 .$
 lff"))
 p2Y2 pYl
 $
 @)2
 -aV29w $
>
 2{fE(`qaa=6

Nv$l$*2p,d$2ll$B2lff2>
l-
 2$gl$$$O
%2pY$$2}!2
$
! 2 $D < $
 $
* J
 

 $2l
 }
 $2 >ff2g&~2$p
 2C%)
 $>
 x
! p
 
  < pCffC2JUlff"2 <2 pxp,ff@2$ff$ff
< ll"p
J. D(F$
 gp
 22ff$
 JX2$l
 	6YR
 $z$
 "l
 $
 $WJ2Jp$
 J; 7$
 "p
 2L$
  
ff"
! |'( vUR
6 ;2  lym)2 plff"2 GX
 yX
 .$
 p\"z,
 T)
A  b2 lj2 $)p
 >
 "
$
 2$&X
 X
 $p
 "
 ff"
! jp
 ,
 "o2 $
 C+ 2 l
 B
 +
 p
 
 J2 pd
i ?n2 $
 *O2 p@X
 .l
 $l
! 2 
 $
! $2 l l
 nX
 y$
 z hC$
 2ff)2ff$
 $2 
N l
d
 n2 p J2 pd l
 
 dRJ 2 2 $
 *-2 pd
 "2 $
 

  ff"
! "p
 2.$
 $.!
 ,
p
 "
 $)J;:$
 
 p
 l
 

w

"!RTS

() 8ye^`)q\n^

b>Z$\ }c

L2n># $J$$Je
2.!Jdp*R$)22!$ff-!u)$2V#

 2 l
p
 X
 >
 3l
$vp2$Rff2 $2$$!=Xp9iw y$|!
4a&^h4a
llff"gpdp < pCX=2]>2p9:
2 ).$$
[$;2jX,b~$ p
ef
 + ;})
 $2cC2hX
p,ffvg(FAR*233`692pRl2ll$L$;9$2L	x2p
.$
 JX&
e 2!g
! 9Xp
 C
  2 $&p
 $~ *^X2 $jlY$
  $
 2k+p
 l
 X
 }2 p
$
! 2 $D
ff(

fi oxorlzGir

1
0

1
0

$!p2@L+-Hl$#lff"2$Dp," 
)>+pdpJ 9 Wj9lpdpE(fiX;

 )
6 $:2 p2}ff2=2Ru2$g$9}ff2=Bpdp$)d2$n2p2
$p2Jj
0 2l
 l
 $-2 l
 >
 2l
 p$H
 $
 $2 $~ :
 2F:.$
 $
  $; 9$2 J
 dp,b2 p-
 ;  W
 >ff2HJXb L"2 ?
 ;2   < pX
 .$
 
< p)2lp|p
$2l
! 2 
 $
! 2 $ n)
 $2 8f(`qaa=)
6 l
 +"
! n
L $
 $xp
 9
 v$2 $b2 $"
p
 p$~ +2 <$
 HpWI2 pd
 
 ff"
! $
 *2 $b  $
 GX
 ; 

%2
 $22 
! ffg2 pp
 lU$
 W>
 X
 .$
 $}
 .$
 J

 2Y
e 2!z
p
! $LJ 2 !z2 pp
 
 ff"
! ly2l*XJJv b2p
!)ff"!-2lXB`q|^n`q&2pd$!9$2$ 
').po2pvXp+2g!

2
 pJ2 2 $
 ,lp
 $ < p
 $
  $2}
! 2 !$2$D*s2p2pJ$$*
$
 $ pX$
 lL2 pd; ff$
 "$
 2Z'(  X2 p2~ 
)2 pz
[ ff"
 	l
 2 

 p
 dp	6
$
 2 +$
  2 $
 ff$nR
 $2 {2 po+$
 @X
 
p
 dphw 2  4a5V4a\5e 2?!u
! E)
 $2 
ll
 ff"p
 dp<2ff$
 X
 	)$
 Lb2 $x
 $
 l2l
! 2 
 
! $2 $Dp
 i2 l'p
 >
 "
2pp
 
 $
! $2 $ ,
 $
 2k+l
 2" 
! ffl
 <

 R
 "-` 7( 2O+$
 @X
 *)
 $2
X.$
 $l
! 2  +"
!   X
 @L^
2 ?+ G)
 $9{2 J
 J 
 $
 $H
2pff"
! $
 >	H'2Paaop
 dp
efE
 $!p
 2|Po>
 2$W2 p|4a5^4an
! G)
 pb2 pbl
 ff^p
 
}2 p"k^$
 ff22X$
 G 
2pl
 $
 n)
 2 
92 p2?
! p
 dp ff"
! $
  < $:l
 $
 n)
 2 )pC p
 B2 p $IHX2$
 
X>
 2 $|p
 $X$
  W>
 @X
 .l
   p
 | \2 p&'
 2 $ ff"
! l
 <$2l
 }
 $2 "l
  
p
 *2 p>
 Dl
 $
 \)
 .2 $
 )2 pyl
 $
 ffffup
 p
 2$
 Hff2Jp+2 Bw J ff$
 JJ
2
! l
 y2 p;2 @(fi2 pl
 "
 ,ffV 76
 i)
 $yX
 .$
  $2}
! 2 
 $J2J)$$
 $
 $2 
[ ;U

 
O2 pffff
! $
 d
* p	,
 }
* ff2)
 $
 pok+$
  ->
 <
 J2)2 $nL
q  l@}
 $
 o)
 2 
22
 $uaV2

"!uw0x

%

\nZ$ x

cJ^ HZ$\]^]ac

Hl$#lff"2$o!ffl L\$>" $;2!
:, WL
-pdp*)$22$o W
$lp $
 Z2 phX
 ; W&v2$l < p$$ .p2$J?2pDp,ff ff2D
22p
 
 n( }ff2=yp
 dp76jz2$X; n('9$spdp767ENV$ ff$"p2ff$Xff
"
 2J}*
 2.\)
 2  '$
Ypdp$
U =92lbJ.&2p
ff"
! ?$
 Jp
 2 $)
 p
 v$$
 Jl2+ H }
 # lff"2 $ D
! 9ffl X
 u.$
 $  
$
 h( X+2 
! p	:=C
 ff$67H 2+<
 d
 J}C
2 px$
C 9;
 ]2 $ S h( $2d.$
 
 l

 d
 X"2 76

2>H
 + (f23336])
 $2 $22 Xp
 v$ < $x )$
 ffg
[ $)
 2l
 ^c( J6Jp
 >
 "j
 2
N +,
d
 L
 i(f233V2W
6 o
! vvp
 Jly
 l2 Gl
 # }ff"2 $ 
! 9ffl
 )=2 $\2 p2y>
 2


fix |lr.u>k  i.$impo  r

900

150

800
700
600

0.1 < 
0.01 <  < 0.1
0 <  < 0.01

100

500
400
300

50

200
100
0
0

0.2

0.4



$!p2K^-H



0.6

l#}ff"2$Gp>"
p$DC$22=

0.8

0
0

1

0.2

0.4





Z2p$2$1pdp

"s2l

lpdp



0.6

0.8

1

DC2F$d#

< p
fiJ$;!

2pWRb2p

l$ER2$
.lE
@2p2.!|pdpffff!$
)9$JXEpdp < p
$
 $2l
! 2 
 $
! $2 l =ff
! 
 
 .-L
q ^ U$
 \$
 }fE(`qaa=6
! +
 $" 
!  2p	)
 2 $22 $X22 
 
]2 $ff$$2Wd
 2 D)
 2 $o2 p
< p$
$
 $J
! 
! -H
H ff$$2Wd
 2 $
 :,
 2yX
 >
 z2 pyp
 $XR$
 	,
 )X
 .$
 *
$2
 p
! D2 lUp
 $
 22ff|2 p2

$
$*
2333^M][\p2l+$*'233367 < p2
2,J$WJpzD$2pl#lffff2$@p,ff
ff9$$
 "p
 2 & ;2 p
 $$2l!2$!$2l 
w 2 }
 # lff"2 $ hp
 ,
 "d
 LR

  k^$
 }+l
 X 
(fillff"{n?p 6pv$ZX2
W
 gj
 9 $\$
 vp $Hd
 2 \2 plff=2 $
  lff2+R
 X+2 B p
 2 $\2 p2
lp2J < $R.p
 2 $
 Y,
 2$Rl
 p$ < pH$
 $$2 $
 ll2l
 l
 $2 l
 jc(  S.Q  76
pl!|2
 $ l2l
 l
 $
 
92 $L9$\
! $ $2 Hlff=2 )C
 $
 $2 $~ uR
 $2 \$
 $
  9$
 p
^$
 X
 xX
 >
 D~ 2$
 p
  < $HlC$2l
 }
 $&2 l
 }
 2 plff=2 x>
 2l
 $2 $~ 
2ff$
ef
 $!p
 2GLz,
 2p	 {l
 $# lff"2 $ up
 ,
 ff)
 $2 ,
 +$
 vp  W
 *})
 $9
;2
 Hp
 
!   J 2 p p
 
 ff"
! ?$
  < p ff2W)
 2pW 2 p lff2+f# l
22
 $
  <  "2 $ $ dpl
 oC
 J$=  (fi2 p2$p$
 vp67Zw 92 p
X.$
  $2l
! 2  $
! 2 $ R
 $2 -fE(`qaa=6 l
  29p =ff
! $
 s
L l
 $\$
 
"gp
 dpi>
 ,pWs2 pjff"
! ?$
 W<$2 O>
 R;2   < pi2 $ff
 p
 J2 pjp
 
 ff"
! $
 *	2 p
$
 |X
 ;  
J2 pR22 
! ff2.$
 $j
 l
 $ :2 $-p
 $Xjl
 	,
 YX
 .$
 JX2 $$
el
 <ff'2 $'
 J;<
 
2 pxp
 v$2 pUJ yX
 .$
 lffC2j2 $
! ='2 l%2 $@
! $J;
2pp
 ffp
 &$
 "{p
 dpn( p
 .g,
 +6g2 pb
! ffX
 ,
 sp
 $X$
  	,
 
 $
  
22
 $
 p
 j2 $
2 pj)
 p
! 
 ,
 "
 2p	:
 ?n
 $!p
 2b
L )2~p
 
! z oX
  2 p
 2 $ < $)$
 l
 B$
 
< $@p
pW2 p uX
 .l
 $} 
! 2 p))
 $2 n2 pp
 
 ff"
! $
  < p
 $
 $2}
! 2 
$
! 2 $D}
* $W>
 3
* \X
 l
 n
 @$
 9ff"
! Hff9$$ "$
 2 < $2
 >
  2 l
 #
lff"2
  @
! ff} ->
 J$\X
 
 2* :2 $-2 J@R
 $2 2 $$
 2$
 zp
 dp
 9 W
 H-
! 
>$=  dp$
 o "E2 $p
 dpE2 $X
 ;  W
  < p|X
 .l
  $2}
! 2 
Jff$$29pbslff$$2Wd2$k^pb
&2$bp,ff( +s/



fi oxorlzGir

$!2$

$2!tfE(`qaa=6+"!LXiLqH^%ely$!p2xKff)>CpW

$;!9


i2p@l$\)2l(Fp$XR+l)	,BX.$ 6Y
.$n
B2pL2!pdpffff!$:


.$JXo$vpXCff$*
2p(P

]2$ff"!l}"h2$!+x$$Cff2g
.$
H $2 p
H
! p
 
 J.2 2 $
 ff2
 2$}
 g
 j2 $)p
 >
 "~
* p
 J ;2 ]$
 2Lffl$2^#
2
  J2 pd$) uC$
  "2  
Y2 p2.
! bp
 v$Lff"
! $
  < $2
 *J,
 J9{2 p
$
 " &nff92 
 J2 $v>c( ,)L
6 p2$X
  (F
d ff$X/ w 
! $
 ff *`aa`67p
 o2 p
$
 " :p
 $u^8
 Lg* ,
  2Jl| v
 Jp
 dp2 -
 :)
 $ +2 #pC$
 Y$
 
"y.$
 @p
 dpX>
 UJ y2 $>2 $CX2$2 
 
2 pUff$$p
 2 
 %2 $C
! 
 dpff"
p
! ?$
 V)
 $2 $?2 p:
 $
 $H
! + "nT
* pp
  b^0
   < ^$
 K|1
a l
 $ 
 ffl$2^#
2
 J
 k+l
 . 2 $R	,
 X
 $
 
* $
 2ff$
 | 2x
 92 $-p
 $X'X
 .$
 J]H
H $2 p
! |2 p2
g$
 +d$
 2s2 pff$l2p
   ff"
! $
 2p$
 G
 $
 2$2 $&X
 .$
 $.
! ?= "ff*
$g"$
 ffp
  @2 pG,
 hH l; 
!  C

 '2 p2 $X2$2 $
 pW: E
 $!p
 K5J*
)p@>
  2l$Bp
 \2 $@
F$
 #$; 
!  + o2 p2 lff"2 X
*  2
 !& "2 
 2
! 
 

l$
 GR
 2 $e
 ff"ff
 l
 y2 $j2 p2lffL
Y2 p$" 
!  pvgp
 jffff"
 $
 E
! $
2po2 $
! +"p
 2 
2 poX
 .$
 $uR
H $$p
 2 $
 @ $
 s G
 E2 $$$*%@
 LXl
 $
 
).2 $Y2 p2ff2ff$$2Wd
 2 $
 U &2 p
! 
m)$
 
 ff"!p
 2 l->
 ff2 Jlff?
! ff$}x ff9}
* 2?$
 g>
  
 2$
 $2W@2 p
2"l
 $2 
X
 2 J$
! $2 l*)
 $>$
 p
 
 ff'R
 $Ljff2Jlff}
  < $"2 $
 
ffp
 *O
 $
 g$
 
 p
 + =2 G 
 lffG p $
 R
 $ 2 pv )2 p
X;@R
H $$p
 2 
 $
 GX
 .$
 l
! J2 $v$)X
 2  $Wb2 p	:GX
 p
 C.2 gw $2+ 
 $!p

 2xK5y )
! $U
 ;<
 JC="l
 $2 
 X
 2 pC"$
 ^$
 @
 9Oef
! $
 V,
 Y
 .$
 @2 $
ff$$p
 2 $
 Hff2y$
 "$
 $R
 $2 $D2 $@X
 .$
 l
! + "ff*)
 p
 2 2 z2 JL:
 .
2
! l$u2 pJ2J < $Rpv~
* p	,
 W*]p
 
 
 b2 pbX
 .l
 $R22ff=ym)z2 pJ= ffff*
p
 H$
 &$
 22 pX
 .$
 l!)n?p
 >gff$2 yR
 p2 p,ffl$2p
 2 $
 ff2-
! d,
 j2 $ )}
$
C p$
 + "ff

w

_d

ffpjffff 

U$U2$W:2$%X.$l2l!2LRJl,$!$2$

)$2 "p$2?!$!vv 2ff$$2
el>X"
,pJ22lCvlff"2C
]2pjp>"$
 olxff
O2$)$;#
2+2}2$}$?!,!ief2lY>W$CCffllB jp
 $XC$
 W>
 xX
 .$
 $,
+ff"!?$gl2ll$
,2
X$vpH\2pL= 92
 l
 bp
 >
 "@}p
 2=2 $h>
 
ppX.lp;2$|)$9|p,ff$2X"2jff2RX$
 2$l
 Y
 Y2 p:2 $
! +"p
 2,
J2 p-X
 .$
 $

.$ef$!pBP^*
>l;2$*V,2WF2$l<
O>
 H2"$
 2 ,
 L 
J2 $
! +XX
 .$
 l]el
^v$2$px}y$Lpl2
 <$2ip
 
! =X
 p
 $dff
* 2$
 }X
 .$
  $2}
! 2 
X"
dg22$Hn2p@p
 ,
 ff w L$Wp
 p
 
* pW>
 *)
 +o2 $$
 $
ff2U'2pd2>x
.l@2
 $D]w x 9y )C$
 L222 yR
 $2 2 pU2$~ C
2 px>
 $
! +2 
(29$2!j2pBp>
 "
 X +2 X >
c $;~ .
 $; 9$2 3
6 Y)
 $2 |2 pRd.
 ff$
 " 2 

2pp
 ,
 ff'( lD
! 2);2  	67*
 l
 up
 
 d
 l?D2 pk^$
 $
 
2 p
 $
 $UD 9

2
 pgk^$
 +2 $2 iR
 2g>
 X$
 R
 Xuk+$
 ;2 
 lJ}d
  
2 p)$
! 2 $ $
 $L
 p ?p
 b^2 $);2  R2l
< $)J2 2 $

2
 $>}ff ip
 dpp
 x$
 *)
 $L
 2 pU$
! 2 $ $
 "$
 $2 }
 i
 $d$
 .$
  
 ,
p
 "d
 U"$
 9x,
c ;
~ .
 9$p
 'R
A "2 p*2 p2>Lff"
! 
! g

 ff9$$ "p
 

)
 ${X
 .$
 l2l
! 2   2\X
 p$
 hNV$
 9p
 ,
 ffv
 @v
 l
 $W$ 

M

fix |lr.u>k  i.$impo  r

^$X
U.$2$@pv$&)$9

(P

@2p $


}2p[ff"WE}2

p!u&Xj2lg
2p$$2l!2$!$2$u

w jC
 =} $2$
 2jp
 JX"2 +,
 $ff
 
J2 $BX
 .l
 $l
! 2 
 $
! $2l>$
p$p2)X
2elG2$Hffff2y,y
.$p
x?	)+z9k^$22^  pyD2$
$j2
 plff $
 vpY)
 l$C
 d
 }X
 H?2 $)$
 $
! =X
 p
 9pd
''w $*$p	,W*
$
 F2{
 p   9p2Z ? elv
N 2 >`Z>
  ;2  >2 $
 "
 =
 $
 l$2 
 
 n2 p
! $2 $  92 l
 b2 ${UnVn F$ 
 2p
 
!   p p
 
J.2
 2 $
 yH 2b
! p
 *O
 ;2 "
 =
 $
 l$2  B2 $g>
 2pl
 9pvo?
"$
 9n2 l  (TV;U
6 nX
  J 
 +2 $X
* 2$
 g2 lR2 pk+$
 +2 $D,
 yp
 '( 
9k^$
 2 u3679e
]2 pgp
 >
 "
 ; ff$
 "$
 2$
 2p:2 pffJlff D^u?)
 u
 )
 9$; 
 
P".$ 2  2})
 $2 H2p
 
! @ 2})
 .2 *W>
 >L
 J  X2 $>l
 $$2 $
 $2l
 }
 $2 
 $
x2
 po+$
 @X
 
$
 vp$
 2^Z?J
 " ff"
! \$
 Lff2  ff$ "p
 22 p
lff9
 R
 
 D2 $R
 gX
 2X$
 
! Ym)$
 u2 $.
 <

 &p
 ,
 "
 $
 ;2 J
! 

D^$
 X
 
Y 2 l
 tP".$
 2 \ j2 $
 ff2p
  { 9 2 pg^
 +l
 X
 

xp
 dphmjp
 2pl
 GX
 C
 ff2
}2 pJX"2 +
 $IHX2$
 bX
 >
 s2 pJ2 pd .2 p
 
p2$
 D2 p2 pv<

 $
 $$2 l
 !D(F#
0 ff9*]23KK67*)
 $9pvp
 R
 .
 2l
 9w $|p
 
2k^$
 2D2 $lff"
 
j2 $np
 >
 "2 $
 J2 pu2 plff {'(  ,
 p
 dp&p
 
 ?	*
  BF$6Y &X
 ) 2 l
  < pX
 $2l
  } p
 
 +)
 $
! $2 l  bC$
 
"$
 2 l
 lff 9l
* 2l
 j2 pgp
 dp
 "l
 \U
 ff2j
! p
 
 l2D)
 $$
 $
 $X2 $<ff"2 }>
 C$
 } 2L
 B
 
 Jx,
 l<
 .
 p
 ff$2
! ! < $
< $
Dl
C ,

 ff2 $
 ;2 p}!bH-+ J$WJ+2  S # $d
! $
! 2 $
$2
 $
 
 $
 $
 92 p>2"l
 $2 gl2+  ?2 $#ff"2 ]w l
 
! p
 Y2 $
 J2Uff$
 
 S # J2 pd$$
 &
 $
 C.'
 I2 p'
 92 $92 p) $
 9 &Xk+l
 2 u2
a jlff'H}>
 ;
$$}

 
2 p+ ff2:~ 2 < jX
 J$22O2 p)l
 ; +2 jl
 .$
 +L
 lffff2 $
 ffX%
 
ff2Up
 2 $&`-%
  j
 2	)
 y2 p 0
 9)
 2 p
 2 $&`- U X$
 V#T~ 2+ *+2 +$
 <ld!
y
 2
 
 
}2 C 2   
[ WX
* 2 pgp
 V#T~ J= Uff2p
 i
 $
 $*V2 p2
  ~ 2X
$2.!|
F"p
 2y
  S # g
* 2?$
 y2 p$
! $2 lj DX
 J"$
 2 l
 
 }ff
J.2
 !u(FR
H \/
[ 
! $p*J233a=67


  ,8`   b]


} 

< $C2ff9)"p$X"h^h2p < .p!up.$$28 Q* *lffll2$pd2

 $
! J
%2p@[$""
jpjH H3
 & $ D2 pj 9.p !

G

   l

  

HH*]A@$*i/ [!$$*%A@}(f233a=67{0ff,?pff$!.!sCv
ffp
 2$?l;2=2Jel
lJqL	45^qK`^

$$2

J;

	ff
fifi  !""#$&%(')fi*!,+.-/1023+.04+*

c>$*Je$*NpJ$.*$*iC$~*$*/ >X*<x(f23K367 < $HBtOHZ-[ J$!
"v
 ; uH L;"$
 .D)
 $2 D,
 $2}
 l"2
2$g9.$k^p-
BX
p>"d
ef
.v
N $.
! ;# <!*
c 
>

657-80++9;:<%=!>+@? A7-*+9BC')D%2+2-+20+<FE=GAHI+J"08KL+
s

3M

fi oxorlzGir

A57-804+J+9BK;:B7&%7!>+ON@ff!>=ff!,+.-4B!"PBQR')B8%2+2-+20+
B(EG-4!" S02PQ1NA+.-Q1NA+2!"T-UD #M
^V* < $*X/$*9[-(f233367VMff2$$2ll;2
2$$2p61W9X Q
$>" YB-4QZ&%=E=-.!" S[02PQ1ff!,+2QKQ\]:+.04+G^_+48+9-0.=
> *)`b5a *`3V27^``^

H=J*n+(f233367V02d.$2~
vdX"2ef
* <O2*l$JJ27^P^

dff$X*@$*O/vw!$ff *wX(`aa`67|HpW,$2
2
 pvJ%ef

29pJb
g2pl;jnff92

cEd10+4@FNA+.-Q18%2-4fi/B!""Be57-80+.J;:$#
.!,+.fiA7*#M<TJ24*pJell22
d$9$$*t9(f23L367HX$Vp$!$2l[??pff$2!!('?0-$22 67fff<UgQh 

E=U b+2fiiKPjN<Ukff#B^ *@?b?Dl *@2a3W 2a3P^ 9.!2$22-Nv	vn[2p2
 d*#<M ?$J@`aV*O23V2#ff2354
t~2* [$*/dff$X*@%(`aa2W67jH 2$!+)X.$D
-!9ffl$#Jv$=NA+.-Q[')fim
!3 !"" *Z`Bn (367*`V24=3W^`V2WL^2
t~2*[$*J/dff$X* @i(`aa`67[$*3222$$z$$-el
c ff
>
 *)N$*/ $pl*x,j(F,$ 67*
7* <O24$
* lJ4=qqW=4=P`^*U
 $
 9!*[\H [ne < 02



;9* <

~$*

EdB0+.opNA+.-Qq8%.B-fi/B!"Pr57-80+.J4K;:

ff
g.!,+.fiA M

[\p2l+*%d&$*]w *%_$*i/ l*%[(f233367st~v^EX
U$2}!2Z
Jff$$p
?
 $
 OHB Jl9<;"$|el
* <X2q^*#$J
4=PL=4+Lq^

tsu04+.-.!3ff!"
cIE=-4!" S08"Qu!,+.QQvh:;+20+ M

#EG-4!" S02PQff!,+.QQvh:;+20+*wbx*L^27 22^
0ff**.O(f23KK67y5y-&;z9bz2KQ\{.!""0A-9+9.#;:(KoK!,+.QQvh:;+2ff!O4
4!,+.fiA*$$J`a45^`V2aVx[ff!nd
U#
.u09.l2$*=ef$O|h8}  a5#354=PV2	#lL	#lL+
AR*
V(f233`67 C.p2$;#ff7$!

^$>"v

0*X)$*'/
 >
$
 "v
 

HB$p9*x(f23KL67H J
 C$s2p"
* * 33qW 2
a 23^

A')fiQ~+9$4
4!,+.fiA u`

ffff$?!!$2$
y$p

0j
UffV* < %
 (f23KV2W67>+"!$J$l$2
i2p Q=ROS k^$2\
)2pLCl$#!2!
2lh!2)Jv$
* *]23L^27 23LK^

7Y-4Q&%=5[>;
g4"02=EHIB!>+2fi !""0QZ1:;+.L+.-Q Z` w

Nd$T*+ti$*+V* < $*/$*[EJ(f233P679[hC$2p"
U2$!|X
Jp>"d
=* * PV27VLP^

Y*-4QZ%=E=-.!" S08"BQff!,+.QQvh:;+20+^q+42+J-0.> Ll

Nd+,*[$*[$l*	cB$*R"J*  $*Rp*[$*5R"d$;~*$*t~$*	@$*/1CX*
y)(f233V2W67 
0 2}
 l
 "2 $!p
 2|$2?!  
$2
j2p  Q    Q

Vp
 	)
 
! g}
 
* 5*`54.27^`qq^

|

HI+8!>8A%=8%2-fi/B!"PcHI+J"08KL+ n;a
0J*Kgx(f23LL67ZNv2
* ](67*q3W^Pa2

< pl2*  $*'HB$p*]0%$*'/
!2$

[57>bKQ\K.ff>;"0QRHI8:bg.1+ n;w

| W8;


jnlJdpj
Rl

HB!>+2fi/B!""0Q57-9: -&fiifii;:*ub`*

< v$J*^[(`aa`67 < $R+&
'2U
g$ffj$2!9!
4.2WL=4=P^
w



+$!+*[E$* ^V* <

$*/w;^* H@(`aa`67Hp

2H
9$$X-X.$lz2p

su04+.-.!3ff!"
cE=-4!" S08"Q!,+.QQvh:;+20+*#M<O2K^

!lff"2$2h
fi.l2el

_$lV*.$*V$2*Vw$*/

w

,X$.!* A@$*/0*



6Ed10+4

*=_yJ(`aaa=67jHp~X
l2l!2]el

cN<+2-&QL8%.B-fi/B!"Po5y-&804+4;:F#
4!,+2fiA7*#M<]2^*$$JPK3W^P3q^
(f23354+67jH

2Jlffl$2\DcC2Gp,ffJ22$:ef

5y-&D0+J+J;:@&%G!>+/1+.ff!>o')B Pj')B8%2+2-+20+E=-.!" S[02PQff!,+2QKQ\h:;+.10+*$J2WL^27


2WLK^

fiJournal of Artificial Intelligence Research 19 (2003) 315-354

Submitted 12//02; published 10/03

Learning When Training Data are Costly: The Effect of Class
Distribution on Tree Induction
Gary M. Weiss

GMWEISS@ATT.COM

AT&T Labs, 30 Knightsbridge Road
Piscataway, NJ 08854 USA

Foster Provost

FPROVOST@STERN.NYU.EDU

New York University, Stern School of Business
44 W. 4th St., New York, NY 10012 USA

Abstract
For large, real-world inductive learning problems, the number of training examples often
must be limited due to the costs associated with procuring, preparing, and storing the training
examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only n training examples can be selected,
in what proportion should the classes be represented? In this article we help to answer this
question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data.
We study twenty-six data sets and, for each, determine the best class distribution for learning.
The naturally occurring class distribution is shown to generally perform well when classifier
performance is evaluated using undifferentiated error rate (0/1 loss). However, when the area
under the ROC curve is used to evaluate classifier performance, a balanced distribution is
shown to perform well. Since neither of these choices for class distribution always generates
the best-performing classifier, we introduce a budget-sensitive progressive sampling algorithm for selecting training examples based on the class associated with each example. An
empirical analysis of this algorithm shows that the class distribution of the resulting training
set yields classifiers with good (nearly-optimal) classification performance.

1. Introduction
In many real-world situations the number of training examples must be limited because obtaining
examples in a form suitable for learning may be costly and/or learning from these examples may
be costly. These costs include the cost of obtaining the raw data, cleaning the data, storing the
data, and transforming the data into a representation suitable for learning, as well as the cost of
computer hardware, the cost associated with the time it takes to learn from the data, and the opportunity cost associated with suboptimal learning from extremely large data sets due to limited
computational resources (Turney, 2000). When these costs make it necessary to limit the amount
of training data, an important question is: in what proportion should the classes be represented in
the training data? In answering this question, this article makes two main contributions. It addresses (for classification-tree induction) the practical problem of how to select the class distribution of the training data when the amount of training data must be limited, and, by providing a
detailed empirical study of the effect of class distribution on classifier performance, it provides a
better understanding of the role of class distribution in learning.
2003 AI Access Foundation and Morgan Kaufmann Publishers. All Rights Reserved.

fiWeiss & Provost

Some practitioners believe that the naturally occurring marginal class distribution should be
used for learning, so that new examples will be classified using a model built from the same underlying distribution. Other practitioners believe that the training set should contain an increased
percentage of minority-class examples, because otherwise the induced classifier will not classify
minority-class examples well. This latter viewpoint is expressed by the statement, if the sample
size is fixed, a balanced sample will usually produce more accurate predictions than an unbalanced 5%/95% split (SAS, 2001). However, we are aware of no thorough prior empirical study
of the relationship between the class distribution of the training data and classifier performance,
so neither of these views has been validated and the choice of class distribution often is made
arbitrarilyand with little understanding of the consequences. In this article we provide a thorough study of the relationship between class distribution and classifier performance and provide
guidelinesas well as a progressive sampling algorithmfor determining a good class distribution to use for learning.
There are two situations where the research described in this article is of direct practical use.
When the training data must be limited due to the cost of learning from the data, then our resultsand the guidelines we establishcan help to determine the class distribution that should
be used for the training data. In this case, these guidelines determine how many examples of
each class to omit from the training set so that the cost of learning is acceptable. The second
scenario is when training examples are costly to procure so that the number of training examples
must be limited. In this case the research presented in this article can be used to determine the
proportion of training examples belonging to each class that should be procured in order to
maximize classifier performance. Note that this assumes that one can select examples belonging
to a specific class. This situation occurs in a variety of situations, such as when the examples
belonging to each class are produced or stored separately or when the main cost is due to transforming the raw data into a form suitable for learning rather than the cost of obtaining the raw,
labeled, data.
Fraud detection (Fawcett & Provost, 1997) provides one example where training instances belonging to each class come from different sources and may be procured independently by class.
Typically, after a bill has been paid, any transactions credited as being fraudulent are stored
separately from legitimate transactions. Furthermore transactions credited to a customer as being
fraudulent may in fact have been legitimate, and so these transactions must undergo a verification
process before being used as training data.
In other situations, labeled raw data can be obtained very cheaply, but it is the process of
forming usable training examples from the raw data that is expensive. As an example, consider
the phone data set, one of the twenty-six data sets analyzed in this article. This data set is used to
learn to classify whether a phone line is associated with a business or a residential customer. The
data set is constructed from low-level call-detail records that describe a phone call, where each
record includes the originating and terminating phone numbers, the time the call was made, and
the day of week and duration of the call. There may be hundreds or even thousands of call-detail
records associated with a given phone line, all of which must be summarized into a single training example. Billions of call-detail records, covering hundreds of millions of phone lines, potentially are available for learning. Because of the effort associated with loading data from dozens
of computer tapes, disk-space limitations and the enormous processing time required to summarize the raw data, it is not feasible to construct a data set using all available raw data. Consequently, the number of usable training examples must be limited. In this case this was done
based on the class associated with each phone linewhich is known. The phone data set was
316

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

limited to include approximately 650,000 training examples, which were generated from approximately 600 million call-detail records. Because huge transaction-oriented databases are
now routinely being used for learning, we expect that the number of training examples will also
need to be limited in many of these cases.
The remainder of this article is organized as follows. Section 2 introduces terminology that
will be used throughout this article. Section 3 describes how to adjust a classifier to compensate
for changes made to the class distribution of the training set, so that the generated classifier is not
improperly biased. The experimental methodology and the twenty-six benchmark data sets analyzed in this article are described in Section 4. In Section 5 the performance of the classifiers
induced from the twenty-six naturally unbalanced data sets is analyzed, in order to show how
class distribution affects the behavior and performance of the induced classifiers. Section 6,
which includes our main empirical results, analyzes how varying the class distribution of the
training data affects classifier performance. Section 7 then describes a progressive sampling
algorithm for selecting training examples, such that the resulting class distribution yields classifiers that perform well. Related research is described in Section 8 and limitations of our research
and future research directions are discussed in Section 9. The main lessons learned from our
research are summarized in Section 10.

2. Background and Notation
Let x be an instance drawn from some fixed distribution D. Every instance x is mapped (perhaps
probabilistically) to a class C  {p, n} by the function c, where c represents the true, but unknown, classification function.1  /HW  EH WKH PDUJLQDO SUREDELOLW\ RI PHPEHUVKLS RI x in the
positive class and 1   WKH PDUJLQDO SURbability of membership in the negative class. These
marginal probabilities sometimes are referred to as the class priors or the base rate.
A classifier t is a mapping from instances x to classes {p, n} and is an approximation of c.
For notational convenience, let t(x)  {P, N} so that it is always clear whether a class value is an
actual (lower case) or predicted (upper case) value. The expected accuracy of a classifier t, t, is
GHILQHGDV t = Pr(t(x) = c(x)), or, equivalently as:
 t    Pr(t(x) = P | c(x) = p) + (1  fiPr(t(x) = N | c(x) = n)

[1]

Many classifiers produce not only a classification, but also estimates of the probability that x
will take on each class value. Let Postt(x) be classifier ts estimated (posterior) probability that
for instance x, c(x) = p. Classifiers that produce class-membership probabilities produce a classification by applying a numeric threshold to the posterior probabilities. For example, a threshold
value of .5 may be used so that t(x) = P iff Postt (x) > .5; otherwise t(x) = N.
A variety of classifiers function by partitioning the input space into a set L of disjoint regions
(a region being defined by a set of potential instances). For example, for a classification tree, the
regions are described by conjoining the conditions leading to the leaves of the tree. Each region
L  L ZLOOFRQWDLQVRPHQXPEHURIWUDLQLQJLQVWDQFHV L/HW LpDQG Ln be the numbers of
positiYHDQGQHJDWLYHWUDLQLQJLQVWDQFHVLQUHJLRQ/VXFKWKDW L  Lp + Ln. Such classifiers
1. This paper addresses binary classification; the positive class always corresponds to the minority class and the negative class to the majority class.

317

fiWeiss & Provost

often estimate Postt(x | x  /fiDV Lpff Lp+ Ln) and assign a classification for all instances x  L
based on this estimate and a numeric threshold, as described earlier. Now, let LP and LN be the
sets of regions that predict the positive and negative classes, respectively, such that LP  LN = L.
For each region L  L, tKDVDQDVVRFLDWHGDFFXUDF\ L = Pr(c(x) = t(x) | x  Lfi/HW LP represent the expected accuracy for x  LPDQG LN the expected accuracy for x  LN.2 As we shall see
LQ6HFWLRQZHH[SHFW LP  LN when  .5.

3. Correcting for Changes to the Class Distribution of the Training Set
Many classifier induction algorithms assume that the training and test data are drawn from the
same fixed, underlying, distribution D. In particular, these algorithms assume that rtrain and rtest,
the fractions of positive examples in the training and test sets, approximDWH  WKH WUXH SULRU
probability of encountering a positive example. These induction algorithms use the estimated
class priors based on rtrain, either implicitly or explicitly, to construct a model and to assign classifications. If the estimated value of the class priors is not accurate, then the posterior probabilities of the model will be improperly biased. Specifically, increasing the prior probability of a
class increases the posterior probability of the class, moving the classification boundary for that
class so that more cases are classified into the class (SAS, 2001). Thus, if the training-set data
are selected so that rtrain GRHVQRWDSSUR[LPDWH WKHQWKHSRVWHULRUSUREDELOLWLHVVKRXOGEHDdjusted based on the differences beWZHHQ DQGrtrain. If such a correction is not performed, then
the resulting bias will cause the classifier to classify the preferentially sampled class more accurately, but the overall accuracy of the classifier will almost always suffer (we discuss this further
in Section 4 and provide the supporting evidence in Appendix A).3
In the majority of experiments described in this article the class distribution of the training set
is purposefully altered so that rtrainGRHVQRWDSSUR[LPDWH 7KHSXUSRVHIRUPRGLIying the class
distribution of the training set is to evaluate how this change affects the overall performance of
the classifierand whether it can produce better-performing classifiers. However, we do not
want the biased posterior probability estimates to affect the results. In this section we describe a
method for adjusting the posterior probabilities to account for the difference between rtrainDQG 
This method (Weiss & Provost, 2001) is justified informally, using a simple, intuitive, argument.
Elkan (2001) presents an equivalent method for adjusting the posterior probabilities, including a
formal derivation.
When learning classification trees, differences between rtrainDQG QRUPDOO\UHVXOWLQEiased
posterior class-probability estimates at the leaves. To remove this bias, we adjust the probability
estimates to take these differences into account. Two simple, common probability estimation
IRUPXODV DUH OLVWHG LQ 7DEOH   )RU HDFK OHW Lp ff Ln) represent the number of minority-class
(majority-class) training examples at a leaf L of a decision tree (or, more generally, within any
region L). The uncorrected estimates, which are based on the assumption that the training and
test sets are drawn from D and approximate , estimate the probability of seeing a minority-class
(positive) example in L. The uncorrected frequency-based estimate is straightforward and requires no explanation. However, this estimate does not perform well when the sample size,
Lp Ln, is smalland is not even defined when the sample size is 0. For these reasons the
2. For notational convenience we treat LP and LN as the union of the sets of instances in the corresponding regions.
3. In situations where it is more costly to misclassify minority-class examples than majority-class examples, practitioners sometimes introduce this bias on purpose.

318

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

Laplace estimate often is used instead. We consider a version based on the Laplace law of succession (Good, 1965). This probability estimate will always be closer to 0.5 than the frequencybased estimate, but the difference between the two estimates will be negligible for large sample
sizes.
Estimate Name
Frequency-Based
Laplace (law of succession)

Uncorrected
Lp/( Lp+ Ln)

ff Lp+1)/( Lp+ Ln+2)

Corrected
 Lpff Lp+o Ln)
ff Lpfiff Lp+o Ln+2)

Table 1: Probability Estimates for Observing a Minority-Class Example
The corrected versions of the estimates in Table 1 account for differences between rtrainDQG 
by factoring in the over-sampling ratio o, which measures the degree to which the minority class
is over-sampled in the training set relative to the naturally occurring distribution. The value of o
is computed as the ratio of minority-class examples to majority-class examples in the training set
divided by the same ratio in the naturally occurring class distribution. If the ratio of minority to
majority examples were 1:2 in the training set and 1:6 in the naturally occurring distribution,
then o would be 3. A learner can account properly for differences between rtrainDQG E\XVLQJ
the corrected estimates to calculate the posterior probabilities at L.
As an example, if the ratio of minority-class examples to majority-class examples in the naturally occurring class distribution is 1:5 but the training distribution is modified so that the ratio is
1:1, then o is 1.0/0.2, or 5. For L to be labeled with the minority class the probability must be
greater than 0.5, so, using the corrected frequency-EDVHG HVWLPDWH Lpff Lp Ln) > 0.5, or,
Lp! Ln. Thus, L is labeled with the minority class only if it covers o times as many minorityclass examples as majority-class examples. Note that in calculating o above we use the class
ratios and not the fraction of examples belonging to the minority class (if we mistakenly used the
latter in the above example, then o would be one-half divided by one-sixth, or 3). Using the class
ratios substantially simplifies the formulas and leads to more easily understood estimates. Elkan
(2001) provides a more complex, but equivalent, formula that uses fractions instead of ratios. In
this discussion we assume that a good approximation of the true base rate is known. In some
real-world situations this is not true and different methods are required to compensate for
changes to the training set (Provost & Fawcett, 2001; Saerens et al., 2002).
In order to demonstrate the importance of using the corrected estimates, Appendix A presents
results comparing decision trees labeled using the uncorrected frequency-based estimate with
trees using the corrected frequency-based estimate. This comparison shows that for a particular
modification of the class distribution of the training sets (they are modified so that the classes are
balanced), using the corrected estimates yields classifiers that substantially outperform classifiers
labeled using the uncorrected estimate. In particular, over the twenty-six data sets used in our
study, the corrected frequency-based estimate yields a relative reduction in error rate of 10.6%.
Furthermore, for only one of the twenty-six data sets does the corrected estimate perform worse.
Consequently it is critical to take the differences in the class distributions into account when
labeling the leaves. Previous work on modifying the class distribution of the training set (Catlett,
1991; Chan & Stolfo, 1998; Japkowicz, 2002) did not take these differences into account and this
undoubtedly affected the results.
319

fiWeiss & Provost

4. Experimental Setup
In this section we describe the data sets analyzed in this article, the sampling strategy used to
alter the class distribution of the training data, the classifier induction program used, and, finally,
the metrics for evaluating the performance of the induced classifiers.

4.1 The Data Sets and the Method for Generating the Training Data
The twenty-six data sets used throughout this article are described in Table 2. This collection
includes twenty data sets from the UCI repository (Blake & Merz, 1998), five data sets, identified with a +, from previously published work by researchers at AT&T (Cohen & Singer,
1999), and one new data set, the phone data set, generated by the authors. The data sets in Table
2 are listed in order of decreasing class imbalance, a convention used throughout this article.

Dataset
letter-a*
pendigits*
abalone*
sick-euthyroid
connect-4*
optdigits*
covertype*
solar-flare*
phone
letter-vowel*
contraceptive*
adult
splice-junction*

% Minority Dataset
Examples
Size
# Dataset
3.9
20,000 14 network2
8.3
13,821 15 yeast*
8.7
4,177 16 network1+
9.3
3,163 17 car*
9.5
11,258 18 german
9.9
5,620 19 breast-wisc
14.8
581,102 20 blackjack+
15.7
1,389 21 weather+
18.2
652,557 22 bands
19.4
20,000 23 market1+
22.6
1,473 24 crx
23.9
48,842 25 kr-vs-kp
24.1
3,175 26 move+

% Minority Dataset
Examples
Size
27.9
3,826
28.9
1,484
29.2
3,577
30.0
1,728
30.0
1,000
34.5
699
35.6
15,000
40.1
5,597
42.2
538
43.0
3,181
44.5
690
47.8
3,196
49.9
3,029

Table 2: Description of Data Sets
In order to simplify the presentation and the analysis of our results, data sets with more than
two classes were mapped to two-class problems. This was accomplished by designating one of
the original classes, typically the least frequently occurring class, as the minority class and then
mapping the remaining classes into the majority class. The data sets that originally contained
more than 2 classes are identified with an asterisk (*) in Table 2. The letter-a data set was created from the letter-recognition data set by assigning the examples labeled with the letter a to
the minority class; the letter-vowel data set was created by assigning the examples labeled with
any vowel to the minority class.
We generated training sets with different class distributions as follows. For each experimental run, first the test set is formed by randomly selecting 25% of the minority-class examples and
25% of the majority-class examples from the original data set, without replacement (the resulting
test set therefore conforms to the original class distribution). The remaining data are available
for training. To ensure that all experiments for a given data set have the same training-set size
no matter what the class distribution of the training setthe training-set size, S, is made equal to
the total number of minority-class examples still available for training (i.e., 75% of the original
320

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

number). This makes it possible, without replicating any examples, to generate any class distribution for training-set size S. Each training set is then formed by random sampling from the
remaining data, without replacement, such that the desired class distribution is achieved. For the
experiments described in this article, the class distribution of the training set is varied so that the
minority class accounts for between 2% and 95% of the training data.

4.2 C4.5 and Pruning
The experiments in this article use C4.5, a program for inducing classification trees from labeled
examples (Quinlan, 1993). C4.5 uses the uncorrected frequency-based estimate to label the
leaves of the decision tree, since it assumes that the training data approximate the true, underlying distribution. Given that we modify the class distribution of the training set, it is essential that
we use the corrected estimates to re-label the leaves of the induced tree. The results presented in
the body of this article are based on the use of the corrected versions of the frequency-based and
Laplace estimates (described in Table 1), using a probability threshold of .5 to label the leaves of
the induced decision trees.
C4.5 does not factor in differences between the class distributions of the training and test
setswe adjust for this as a post-processing step. If C4.5s pruning strategy, which attempts to
minimize error rate, were allowed to execute, it would prune based on a false assumption (viz.,
that the test distribution matches the training distribution). Since this may negatively affect the
generated classifier, except where otherwise indicated all results are based on C4.5 without pruning. This decision is supported by recent research, which indicates that when target misclassification costs (or class distributions) are unknown then standard pruning does not improve, and
may degrade, generalization performance (Provost & Domingos, 2001; Zadrozny & Elkan, 2001;
Bradford et al., 1998; Bauer & Kohavi, 1999). Indeed, Bradford et al. (1998) found that even if
the pruning strategy is adapted to take misclassification costs and class distribution into account,
this does not generally improve the performance of the classifier. Nonetheless, in order to justify
using C4.5 without pruning, we also present the results of C4.5 with pruning when the training
set uses the natural distribution. In this situation C4.5s assumption about rtrainDSSUR[LPDWLQJ 
is valid and hence its pruning strategy will perform properly. Looking ahead, these results show
that C4.5 without pruning indeed performs competitively with C4.5 with pruning.

4.3 Evaluating Classifier Performance
A variety of metrics for assessing classifier performance are based on the terms listed in the confusion matrix shown below.

c(x) Actual Positive
Actual Negative

t(x)
Positive Prediction Negative Prediction
tp (true positive)
fn (false negative)
fp (false positive)
tn (true negative)

Table 3 summarizes eight such metrics. The metrics described in the first two rows measure
the ability of a classifier to classify positive and negative examples correctly, while the metrics
described in the last two rows measure the effectiveness of the predictions made by a classifier.
For example, the positive predictive value (PPV), or precision, of a classifier measures the fraction of positive predictions that are correctly classified. The metrics described in the last two
321

fiWeiss & Provost

rows of Table 3 are used throughout this article to evaluate how various training-set class distributions affect the predictions made by the induced classifiers. Finally, the metrics in the second
column of Table 3 are complements of the corresponding metrics in the first column, and can
alternatively be computed by subtracting the value in the first column from 1. More specifically,
proceeding from row 1 through 4, the metrics in column 1 (column 2) represent: 1) the accuracy
(error rate) when classifying positive/minority examples, 2) the accuracy (error rate) when classifying negative/minority examples, 3) the accuracy (error rate) of the positive/minority predictions, and 4) the accuracy (error rate) of the negative/majority predictions.


tp
tp + fn

True Positive Rate FN = Pr(N|p)
(recall or sensitivity)



fn
tp + fn

False Negative Rate

TN = Pr(N|n) 

tn
tn + fp

True Negative Rate FP = Pr(P|n)
(specificity)



fp
tn + fp

False Positive Rate

PPV = Pr(p|P) 

tp
tp + fp

NPV = Pr(n|N) 

tn
tn + fn

TP = Pr(P|p)

fp
Positive Predictive Value PPV = Pr(n|P) 
(precision)
tp + fp
Negative Predictive Value NPV =Pr(y|N) 

fn
tn + fn

Table 3: Classifier Performance Metrics
We use two performance measures to gauge the overall performance of a classifier: classification accuracy and the area under the ROC curve (Bradley, 1997). Classification accuracy is (tp +
fp)/(tp + fp + tn + fn). This formula, which represents the fraction of examples that are correctly
FODVVLILHGLVDQHVWLPDWHRIWKHH[SHFWHGDFFXUDF\ t, defined earlier in equation 1. Throughout
this article we specify classification accuracy in terms of error rate, which is 1  accuracy.
We consider classification accuracy in part because it is the most common evaluation metric
in machine-learning research. However, using accuracy as a performance measure assumes that
the target (marginal) class distribution is known and unchanging and, more importantly, that the
error coststhe costs of a false positive and false negativeare equal. These assumptions are
unrealistic in many domains (Provost et al., 1998). Furthermore, highly unbalanced data sets
typically have highly non-uniform error costs that favor the minority class, which, as in the case
of medical diagnosis and fraud detection, is the class of primary interest. The use of accuracy in
these cases is particularly suspect since, as we discuss in Section 5.2, it is heavily biased to favor
the majority class and therefore will sometimes generate classifiers that never predict the minority class. In such cases, Receiver Operating Characteristic (ROC) analysis is more appropriate
(Swets et al., 2000; Bradley, 1997; Provost & Fawcett, 2001). When producing the ROC curves
we use the Laplace estimate to estimate the probabilities at the leaves, since it has been shown to
yield consistent improvements (Provost & Domingos, 2001). To assess the overall quality of a
classifier we measure the fraction of the total area that falls under the ROC curve (AUC), which
is equivalent to several other statistical measures for evaluating classification and ranking models
(Hand, 1997). Larger AUC values indicate generally better classifier performance and, in particular, indicate a better ability to rank cases by likelihood of class membership.

322

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

5. Learning from Unbalanced Data Sets
We now analyze the classifiers induced from the twenty-six naturally unbalanced data sets described in Table 2, focusing on the differences in performance for the minority and majority
classes. We do not alter the class distribution of the training data in this section, so the classifiers need not be adjusted using the method described in Section 3. However, so that these experiments are consistent with those in Section 6 that use the natural distribution, the size of the
training set is reduced, as described in Section 4.1.
Before addressing these differences, it is important to discuss an issue that may lead to confusion if left untreated. Practitioners have noted that learning performance often is unsatisfactory
when learning from data sets where the minority class is substantially underrepresented. In particular, they observe that there is a large error rate for the minority class. As should be clear
from Table 3 and the associated discussion, there are two different notions of error rate for the
minority class: the minority-class predictions could have a high error rate (large PPV ) or the
minority-class test examples could have a high error rate (large FN). When practitioners observe
that the error rate is unsatisfactory for the minority class, they are usually referring to the fact
that the minority-class examples have a high error rate (large FN). The analysis in this section
will show that the error rate associated with the minority-class predictions ( PPV ) and the minority-class test examples (FN) both are much larger than their majority-class counterparts ( NPV and
FP, respectively). We discuss several explanations for these observed differences.

5.1 Experimental Results
The performances of the classifiers induced from the twenty-six unbalanced data sets are described in Table 4. This table warrants some explanation. The first column specifies the data set
name while the second column, which for convenience has been copied from Table 2, specifies
the percentage of minority-class examples in the natural class distribution. The third column
specifies the percentage of the total test errors that can be attributed to the test examples that
belong to the minority class. By comparing the values in columns two and three we see that in
all cases a disproportionately large percentage of the errors come from the minority-class examples. For instance, minority-class examples make up only 3.9% of the letter-a data set but contribute 58.3% of the errors. Furthermore, for 22 of 26 data sets a majority of the errors can be
attributed to minority-class examples.
The fourth column specifies the number of leaves labeled with the minority and majority
classes and shows that in all but two cases there are fewer leaves labeled with the minority class
than with the majority class. The fifth column, Coverage, specifies the average number of
training examples that each minority-labeled or majority-labeled leaf classifies (covers). These
results indicate that the leaves labeled with the minority class are formed from far fewer training
examples than those labeled with the majority class.
The Prediction ER column specifies the error rates associated with the minority-class and
majority-class predictions, based on the performance of these predictions at classifying the test
examples. The Actuals ER column specifies the classification error rates for the minority and
majority class examples, again based on the test set. These last two columns are also labeled
using the terms defined in Section 2 ( PPV , NPV , FN, and FP). As an example, these columns
show that for the letter-a data set the minority-labeled predictions have an error rate of 32.5%
while the majority-labeled predictions have an error rate of only 1.7%, and that the minorityclass test examples have a classification error rate of 41.5% while the majority-class test exam323

fiWeiss & Provost

ples have an error rate of only 1.2%. In each of the last two columns we underline the higher
error rate.

Dataset
letter-a
pendigits
abalone
sick-euthyroid
connect-4
optdigits
covertype
solar-flare
phone
letter-vowel
contraceptive
adult
splice-junction
network2
yeast
network1
car
german
breast-wisc
blackjack
weather
bands
market1
crx
kr-vs-kp
move
Average
Median

% Minority % Errors
Examples from Min.
3.9
8.3
8.7
9.3
9.5
9.9
14.8
15.7
18.2
19.4
22.6
23.9
24.1
27.9
28.9
29.2
30.0
30.0
34.5
35.6
40.1
42.2
43.0
44.5
47.8
49.9
25.8
26.0

58.3
32.4
68.9
51.2
51.4
73.0
16.7
64.4
64.4
61.8
48.7
57.5
58.9
57.1
58.9
57.1
58.6
55.4
45.7
81.5
50.7
91.2
50.3
51.0
54.0
61.4
56.9
57.3

Leaves
Min. Maj.

Coverage
Min. Maj.

Prediction ER
Min.
Maj.

Actuals ER
Min.
Maj.

11
6
5
4
47
15
350
12
1008
233
31
627
26
50
8
42
38
34
5
13
134
52
87
28
23
235
120
33

2.2
4.3
16.8 109.3
2.8 35.5
7.1 26.9
1.7
5.8
2.9
2.4
27.3 123.2
1.7
3.1
13.0 62.7
2.4
0.9
1.8
2.8
3.1
1.6
5.5
9.6
4.0 10.3
14.4 26.1
5.1 12.8
3.1
6.6
2.0
2.0
12.6 26.0
57.7 188.0
5.0
7.2
1.4
0.3
5.1
2.7
3.9
2.1
24.0 41.2
2.4
0.6
8.8 27.4
3.9
6.9

(PPV)
32.5
25.8
69.8
22.5
55.8
18.0
23.1
67.8
30.8
27.0
69.8
34.3
15.1
48.2
45.6
46.2
14.0
57.1
11.4
28.9
41.0
17.8
30.9
23.2
1.2
24.4
33.9
29.9

(FN)
41.5
14.3
84.4
24.7
57.6
36.7
5.7
78.9
44.6
37.5
68.3
41.5
20.3
55.5
55.0
53.9
18.6
62.4
9.8
64.4
41.7
69.8
31.2
24.1
1.4
33.9
41.4
41.5

138
8
8
9
128
173
446
48
1220
2547
70
4118
46
61
12
49
42
81
5
19
142
389
227
65
15
1025
426
67

(NPV)
1.7
1.3
7.7
2.5
6.0
3.9
1.0
13.7
9.5
8.7
20.1
12.6
6.3
20.4
20.9
21.0
7.7
25.4
5.1
27.9
27.7
34.8
23.4
18.9
1.3
29.9
13.8
11.1

(FP)
1.2
2.7
3.6
2.4
5.7
1.5
4.9
8.1
5.5
5.6
21.1
9.6
4.5
16.2
15.6
16.7
5.6
21.5
6.1
8.1
27.1
4.9
23.3
18.5
1.1
21.2
10.1
5.9

Table 4: Behavior of Classifiers Induced from Unbalanced Data Sets
The results in Table 4 clearly demonstrate that the minority-class predictions perform much
worse than the majority-class predictions and that the minority-class examples are misclassified
much more frequently than majority-class examples. Over the twenty-six data sets, the minority
predictions have an average error rate ( PPV ) of 33.9% while the majority-class predictions have
an average error rate ( NPV ) of only 13.8%. Furthermore, for only three of the twenty-six data
sets do the majority-class predictions have a higher error rateand for these three data sets the
class distributions are only slightly unbalanced. Table 4 also shows us that the average error rate
for the minority-class test examples (FN) is 41.4% whereas for the majority-class test examples
the error rate (FP) is only 10.1%. In every one of the twenty-six cases the minority-class test
examples have a higher error rate than the majority-class test examples.
324

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

5.2 Discussion
Why do the minority-class predictions have a higher error rate ( PPV ) than the majority-class
predictions ( NPV )? There are at least two reasons. First, consider a classifier trandom where the
partitions L are chosen randomly and the assignment of each L  L to LP and LN is also made
randomly (recall that LP and LN represent the regions labeled with the positive and negative
classes). For a two-class learning problem WKH H[SHFWHG RYHUDOO DFFXUDF\ t, of this randomly
generated and labeled classifier must be 0.5. However, the expected accuracy of the regions in
the positive partiWLRQ LPZLOOEH ZKLOHWKHH[SHFWHGDFFXUDF\RIWKHUHJLRQVLQWKHQHJDWLYH
partition, LN, will be 1  )RUDKLJKO\XQEDODQFHGFODVVGLVWULEXWLRQZKHUH  LP = .01
DQG LN = .99. Thus, in such a scenario the negative/majority predictions will be much more
accurate. While this test distribution effect will be small for a well-learned concept with a
low Bayes error rate (and non-existent for a perfectly learned concept with a Bayes error rate of
0), many learning problems are quite hard and have high Bayes error rates.4
The results in Table 4 suggest a second explanation for why the minority-class predictions are
so error prone. According to the coverage results, minority-labeled predictions tend to be formed
from fewer training examples than majority-labeled predictions. Small disjuncts, which are the
components of disjunctive concepts (i.e., classification rules, decision-tree leaves, etc.) that cover
few training examples, have been shown to have a much higher error rate than large disjuncts
(Holte, et al., 1989; Weiss & Hirsh, 2000). Consequently, the rules/leaves labeled with the minority class have a higher error rate partly because they suffer more from this problem of small
disjuncts.
Next, why are minority-class examples classified incorrectly much more often than majorityclass examples (FN > FP)a phenomenon that has also been observed by others (Japkowicz &
Stephen, 2002)? Consider the estimated accuracy, at, of a classifier t, where the test set is drawn
from the true, underlying distribution D:
at = TP  rtest + TN  (1  rtest)

[2]

Since the positive class corresponds to the minority class, rtest < .5, and for highly unbalanced
data sets rtest << .5. Therefore, false-positive errors are more damaging to classification accuracy
than false negative errors are. A classifier that is induced using an induction algorithm geared
toward maximizing accuracy therefore should prefer false-negative errors over false-positive
errors. This will cause negative/majority examples to be predicted more often and hence will
lead to a higher error rate for minority-class examples. One straightforward example of how
learning algorithms exhibit this behavior is provided by the common-sense rule: if there is no
evidence favoring one classification over another, then predict the majority class. More generally, induction algorithms that maximize accuracy should be biased to perform better at classifying majority-class examples than minority-class examples, since the former component is
weighted more heavily when calculating accuracy. This also explains why, when learning from
data sets with a high degree of class imbalance, classifiers rarely predict the minority class.
A second reason why minority-class examples are misclassified more often than majorityclass examples is that fewer minority-class examples are likely to be sampled from the distribu4. The (optimal) Bayes error rate, using the terminology from Section 2, occurs when t(.)=c(.). Because c(.) may be
probabilistic (e.g., when noise is present), the Bayes error rate for a well-learned concept may not always be low.
The test distribution effect will be small when the concept is well learned and the Bayes error rate is low.

325

fiWeiss & Provost

tion D. Therefore, the training data are less likely to include (enough) instances of all of the
minority-class subconcepts in the concept space, and the learner may not have the opportunity to
represent all truly positive regions in LP. Because of this, some minority-class test examples will
be mistakenly classified as belonging to the majority class.
Finally, it is worth noting that PPV > NPV does not imply that FN > FP. That is, having
more error-prone minority predictions does not imply that the minority-class examples will be
misclassified more often than majority-class examples. Indeed, a higher error rate for minority
predictions means more majority-class test examples will be misclassified. The reason we generally observe a lower error rate for the majority-class test examples (FN > FP) is because the
majority class is predicted far more often than the minority class.

6. The Effect of Training-Set Class Distribution on Classifier Performance
We now turn to the central questions of our study: how do different training-set class distributions affect the performance of the induced classifiers and which class distributions lead to the
best classifiers? We begin by describing the methodology for determining which class distribution performs best. Then, in the next two sections, we evaluate and analyze classifier performance for the twenty-six data sets using a variety of class distributions. We use error rate as the
performance metric in Section 6.2 and AUC as the performance metric in Section 6.3.

6.1 Methodology for Determining the Optimum Training Class Distribution(s)
In order to evaluate the effect of class distribution on classifier performance, we vary the training-set class distributions for the twenty-six data sets using the methodology described in Section
4.1. We evaluate the following twelve class distributions (expressed as the percentage of minority-class examples): 2%, 5%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, and 95%. For
each data set we also evaluate the performance using the naturally occurring class distribution.
Before we try to determine the best class distribution for a training set, there are several issues that must be addressed. First, because we do not evaluate every possible class distribution,
we can only determine the best distribution among the 13 evaluated distributions. Beyond this
concern, however, is the issue of statistical significance and, because we generate classifiers for
13 training distributions, the issue of multiple comparisons (Jensen & Cohen, 2000). Because of
these issues we cannot always conclude that the distribution that yields the best performing classifiers is truly the best one for training.
We take several steps to address the issues of statistical significance and multiple comparisons. To enhance our ability to identify true differences in classifier performance with respect to
changes in class distribution, all results presented in this section are based on 30 runs, rather than
the 10 runs employed in Section 5. Also, rather than trying to determine the best class distribution, we adopt a more conservative approach, and instead identify an optimal range of class
distributionsa range in which we are confident the best distribution lies. To identify the optimal range of class distributions, we begin by identifying, for each data set, the class distribution
that yields the classifiers that perform best over the 30 runs. We then perform t-tests to compare
the performance of these 30 classifiers with the 30 classifiers generated using each of the other
twelve class distributions (i.e., 12 t-tests each with n=30 data points). If a t-test yields a probability  .10 then we conclude that the best distribution is different from the other distribution
(i.e., we are at least 90% confident of this); otherwise we cannot conclude that the class distributions truly perform differently and therefore group the distributions together. These grouped
326

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

distributions collectively form the optimal range of class distributions. As Tables 5 and 6 will
show, in 50 of 52 cases the optimal ranges are contiguous, assuaging concerns that our conclusions are due to problems of multiple comparisons.

6.2 The Relationship between Class Distribution and Classification Error Rate
Table 5 displays the error rates of the classifiers induced for each of the twenty-six data sets.
The first column in Table 5 specifies the name of the data set and the next two columns specify
the error rates that result from using the natural distribution, with and then without pruning. The
next 12 columns present the error rate values for the 12 fixed class distributions (without pruning). For each data set, the best distribution (i.e., the one with the lowest error rate) is highlighted by underlining it and displaying it in boldface. The relative position of the natural
distribution within the range of evaluated class distributions is denoted by the use of a vertical
bar between columns. For example, for the letter-a data set the vertical bar indicates that the
natural distribution falls between the 2% and 5% distributions (from Table 2 we see it is 3.9%).
Error Rate when using Specified Training Distribution
(training distribution expressed as % minority)

Dataset
Nat-Prune Nat
letter-a

2

5

10

20

30

40

50

60

70

Relative %
Improvement
80

90

95 best vs. nat best vs. bal

2.80 x 2.78

2.86 2.75 2.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.10 26.14

6.8

51.9

pendigits

3.65 + 3.74

5.77 3.95 3.63 3.45 3.70 3.64 4.02 4.48 4.98 5.73 8.83 13.36

7.8

14.2

abalone

10.68 x 10.46

9.04 9.61 10.64 13.19 15.33 20.76 22.97 24.09 26.44 27.70 27.73 33.91

13.6

60.6

4.46 x 4.10

5.78 4.82 4.69 4.25 5.79 6.54 6.85 9.73 12.89 17.28 28.84 40.34

0.0

40.1

10.68 x 10.56

7.65 8.66 10.80 15.09 19.31 23.18 27.57 33.09 39.45 47.24 59.73 72.08

27.6

72.3

4.94 x 4.68

8.91 7.01 4.05 3.05 2.83 2.79 3.41 3.87 5.15 5.75 9.72 12.87

40.4

18.2

5.12 x 5.03

22.6

sick-euthyroid
connect-4
optdigits

5.54 5.04 5.00 5.26 5.64 5.95 6.46 7.23 8.50 10.18 13.03 16.27

0.6

solar-flare

19.16 + 19.98 16.54 17.52 18.96 21.45 23.03 25.49 29.12 30.73 33.74 38.31 44.72 52.22

17.2

43.2

phone

12.63 x 12.62 13.45 12.87 12.32 12.68 13.25 13.94 14.81 15.97 17.32 18.73 20.24 21.07

2.4

16.8

letter-vowel

11.76 x 11.63 15.87 14.24 12.53 11.67 12.00 12.69 14.16 16.00 18.68 23.47 32.20 41.81

0.0

17.9

contraceptive

31.71 x 30.47 24.09 24.57 25.94 30.03 32.43 35.45 39.65 43.20 47.57 54.44 62.31 67.07

20.9

39.2

adult

17.42 x 17.25 18.47 17.26 16.85 17.09 17.78 18.85 20.05 21.79 24.08 27.11 33.00 39.75

2.3

16.0

8.30 + 8.37 20.00 13.95 10.72 8.68 8.50 8.15 8.74 9.86 9.85 12.08 16.25 21.18

2.6

6.8

27.13 x 26.67 27.37 25.91 25.71 25.66 26.94 28.65 29.96 32.27 34.25 37.73 40.76 37.72

3.8

14.4

yeast

26.98 x 26.59 29.08 28.61 27.51 26.35 26.93 27.10 28.80 29.82 30.91 35.42 35.79 36.33

0.9

8.5

network1

27.57 + 27.59 27.90 27.43 26.78 26.58 27.45 28.61 30.99 32.65 34.26 37.30 39.39 41.09

3.7

14.2

covertype

splice-junction
network2

car
german

9.51 x 8.85 23.22 18.58 14.90 10.94 8.63 8.31 7.92 7.35 7.79 8.78 10.18 12.86

16.9

7.2

33.76 x 33.41 30.17 30.39 31.01 32.59 33.08 34.15 37.09 40.55 44.04 48.36 55.07 60.99

9.7

18.7
0.0

7.41 x 6.82 20.65 14.04 11.00 8.12 7.49 6.82 6.74 7.30 6.94 7.53 10.02 10.56

1.2

blackjack

28.14 + 28.40 30.74 30.66 29.81 28.67 28.56 28.45 28.71 28.91 29.78 31.02 32.67 33.87

0.0

1.1

weather

33.68 + 33.69 38.41 36.89 35.25 33.68 33.11 33.43 34.61 36.69 38.36 41.68 47.23 51.69

1.7

4.3

breast-wisc

bands

32.26 + 32.53 38.72 35.87 35.71 34.76 33.33 32.16 32.68 33.91 34.64 39.88 40.98 40.80

1.1

1.6

market1

26.71 x 26.16 34.26 32.50 29.54 26.95 26.13 26.05 25.77 26.86 29.53 31.69 36.72 39.90

1.5

0.0

crx

20.99 x 20.39 35.99 30.86 27.68 23.61 20.84 20.82 21.48 21.64 22.20 23.98 28.09 32.85

0.0

5.1

1.25 + 1.39 12.18 6.50 3.20 2.33 1.73 1.16 1.22 1.34 1.53 2.55 3.66 6.04

16.5

4.9

27.54 + 28.57 46.13 42.10 38.34 33.48 30.80 28.36 28.24 29.33 30.21 31.80 36.08 40.95

1.2

0.0

kr-vs-kp
move

Table 5: Effect of Training Set Class Distribution on Error Rate
327

fiWeiss & Provost

The error rate values that are not significantly different, statistically, from the lowest error
rate (i.e., the comparison yields a t-test value > .10) are shaded. Thus, for the letter-a data set,
the optimum range includes those class distributions that include between 2% and 10% minorityclass exampleswhich includes the natural distribution. The last two columns in Table 5 show
the relative improvement in error rate achieved by using the best distribution instead of the natural and balanced distributions. When this improvement is statistically significant (i.e., is associated with a t-test value  .10) then the value is displayed in bold.
The results in Table 5 show that for 9 of the 26 data sets we are confident that the natural distribution is not within the optimal range. For most of these 9 data sets, using the best distribution
rather than the natural distribution yields a remarkably large relative reduction in error rate. We
feel that this is sufficient evidence to conclude that for accuracy, when the training-set size must
be limited, it is not appropriate simply to assume that the natural distribution should be used.
Inspection of the error-rate results in Table 5 also shows that the best distribution does not differ
from the natural distribution in any consistent mannersometimes it includes more minorityclass examples (e.g., optdigits, car) and sometimes fewer (e.g., connect-4, solar-flare). However,
it is clear that for data sets with a substantial amount of class imbalance (the ones in the top half
of the table), a balanced class distribution also is not the best class distribution for training, to
minimize undifferentiated error rate. More specifically, none of the top-12 most skewed data
sets have the balanced class distribution within their respective optimal ranges, and for these data
sets the relative improvements over the balanced distributions are striking.
Let us now consider the error-rate values for the remaining 17 data sets for which the t-test results do not permit us to conclude that the best observed distribution truly outperforms the natural distribution. In these cases we see that the error rate values for the 12 training-set class
distributions usually form a unimodal, or nearly unimodal, distribution. This is the distribution
one would expect if the accuracy of a classifier progressively degrades the further it deviates
from the best distribution. This suggests that adjacent class distributions may indeed produce
classifiers that perform differently, but that our statistical testing is not sufficiently sensitive to
identify these differences. Based on this, we suspect that many of the observed improvements
shown in the last column of Table 5 that are not deemed to be significant statistically are nonetheless meaningful.
Figure 1 shows the behavior of the learned classifiers for the adult, phone, covertype, and letter-a data sets in a graphical form. In this figure the natural distribution is denoted by the X
tick mark and the associated error rate is noted above the marker. The error rate for the best
distribution is underlined and displayed below the corresponding data point (for these four data
sets the best distribution happens to include 10% minority-class examples). Two of the curves
are associated with data sets (adult, phone) for which we are >90% confident that the best distribution performs better than the natural distribution, while for the other two curves (covertype,
letter-a) we are not. Note that all four curves are perfectly unimodal. It is also clear that near the
distribution that minimizes error rate, changes to the class distribution yield only modest changes
in the error ratefar more dramatic changes occur elsewhere. This is also evident for most data
sets in Table 5. This is a convenient property given the common goal of minimizing error rate.
This property would be far less evident if the correction described in Section 3 were not performed, since then classifiers induced from class distributions deviating from the naturally occurring distribution would be improperly biased.

328

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

25

adult

Error Rate (%)

20
17.25
16.85

15
12.62
12.32

phone

10

covertype
5.03

5

2.75 5.00
2.59

letter-a

0
0

10

20

30

40

50

60

70

80

% Minority Class
Figure 1: Effect of Class Distribution on Error Rate for Select Data Sets

Finally, to assess whether pruning would have improved performance, consider the second
column in Table 5, which displays the error rates that result from using C4.5 with pruning on the
natural distribution (recall from Section 4.2 that this is the only case when C4.5s pruning strategy will give unbiased results). A +/x in the second column indicates that C4.5 with pruning
outperforms/underperforms C4.5 without pruning, when learning from the natural distribution.
Note that C4.5 with pruning underperforms C4.5 without pruning for 17 of the 26 data sets,
which leads us to conclude that C4.5 without pruning is a reasonable learner. Furthermore, in no
case does C4.5 with pruning generate a classifier within the optimal range when C4.5 without
pruning does not also generate a classifier within this range.

6.3 The Relationship between Class Distribution and AUC
The performance of the induced classifiers, using AUC as the performance measure, is displayed
in Table 6. When viewing these results, recall that for AUC larger values indicate improved
performance. The relative improvement in classifier performance is again specified in the last
two columns, but now the relative improvement in performance is calculated in terms of the area
above the ROC curve (i.e., 1  AUC). We use the area above the ROC curve because it better
reflects the relative improvementjust as in Table 5 relative improvement is specified in terms
of the change in error rate instead of the change in accuracy. As before, the relative improvements are shown in bold only if we are more than 90% confident that they reflect a true improvement in performance (i.e., t-test value fi
In general, the optimum ranges appear to be centered near, but slightly to the right, of the balanced class distribution. For 12 of the 26 data sets the optimum range does not include the natural distribution (i.e., the third column is not shaded). Note that for these data sets, with the
exception of the solar-flare data set, the class distributions within the optimal range contain more
minority-class examples than the natural class distribution. Based on these results we conclude
even more strongly for AUC (i.e., for cost-sensitive classification and for ranking) than for accu329

fiWeiss & Provost

racy that it is not appropriate simply to choose the natural class distribution for training. Table 6
also shows that, unlike for accuracy, a balanced class distribution generally performs very well,
although it does not always perform optimally. In particular, we see that for 19 of the 26 data
sets the balanced distribution is within the optimal range. This result is not too surprising since
AUC, unlike error rate, is unaffected by the class distribution of the test set, and effectively factors in classifier performance over all class distributions.
AUC when using Specified Training Distribution
(training distribution expressed as % minority)

Dataset
Nat-prune Nat

2

5

10

20

30

40

50

60

70

80

Relative %
Improv. (1-AUC)
90

95 best vs. nat best vs. bal

letter-a

.500 x

.772

.711 .799 .865 .891 .911 .938 .937 .944 .951 .954 .952 .940

79.8

pendigits

.962 x

.967

.892 .958 .971 .976 .978 .979 .979 .978 .977 .976 .966 .957

36.4

27.0
0.0

abalone

.590 x

.711

.572 .667 .710 .751 .771 .775 .776 .778 .768 .733 .694 .687

25.8

0.9

sick-euthyroid

.937 x

.940

.892 .908 .933 .943 .944 .949 .952 .951 .955 .945 .942 .921

25.0

6.3

connect-4

.658 x

.731

.664 .702 .724 .759 .763 .777 .783 .793 .793 .789 .772 .730

23.1

4.6

optdigits

.659 x

.803

.599 .653 .833 .900 .924 .943 .948 .959 .967 .965 .970 .965

84.8

42.3
20.0

covertype

.982 x

.984

.970 .980 .984 .984 .983 .982 .980 .978 .976 .973 .968 .960

0.0

solar-flare

.515 x

.627

.614 .611 .646 .627 .635 .636 .632 .650 .662 .652 .653 .623

9.4

8.2

phone

.850 x

.851

.843 .850 .852 .851 .850 .850 .849 .848 .848 .850 .853 .850

1.3

2.6

letter-vowel

.806 +

.793

.635 .673 .744 .799 .819 .842 .849 .861 .868 .868 .858 .833

36.2

12.6

contraceptive

.539 x

.611

.567 .613 .617 .616 .622 .640 .635 .635 .640 .641 .627 .613

7.7

1.6

adult

.853 +

.839

.816 .821 .829 .836 .842 .846 .851 .854 .858 .861 .861 .855

13.7

6.7

splice-junction

.932 +

.905

.814 .820 .852 .908 .915 .925 .936 .938 .944 .950 .944 .944

47.4

21.9

network2

.712 +

.708

.634 .696 .703 .708 .705 .704 .705 .702 .706 .710 .719 .683

3.8

4.7

yeast

.702 x

.705

.547 .588 .650 .696 .727 .714 .720 .723 .715 .699 .659 .621

10.9

2.5

network1

.707 +

.705

.626 .676 .697 .709 .709 .706 .702 .704 .708 .713 .709 .696

2.7

3.7

car

.931 +

.879

.754 .757 .787 .851 .884 .892 .916 .932 .931 .936 .930 .915

47.1

23.8

german

.660 +

.646

.573 .600 .632 .615 .635 .654 .645 .640 .650 .645 .643 .613

2.3

2.5

breast-wisc

.951 x

.958

.876 .916 .940 .958 .963 .968 .966 .963 .963 .964 .949 .948

23.8

5.9

blackjack

.682 x

.700

.593 .596 .628 .678 .688 .712 .713 .715 .700 .678 .604 .558

5.0

0.7
1.5

weather

.748 +

.736

.694 .715 .728 .737 .738 .740 .736 .730 .736 .722 .718 .702

1.5

bands

.604 x

.623

.522 .559 .564 .575 .599 .620 .618 .604 .601 .530 .526 .536

0.0

1.3

market1

.815 +

.811

.724 .767 .785 .801 .810 .808 .816 .817 .812 .805 .795 .781

3.2

0.5

crx

.889 +

.852

.804 .799 .805 .817 .834 .843 .853 .845 .857 .848 .853 .866

9.5

8.8

kr-vs-kp

.996 x

.997

.937 .970 .991 .994 .997 .998 .998 .998 .997 .994 .988 .982

33.3

0.0

move

.762 +

.734

.574 .606 .632 .671 .698 .726 .735 .738 .742 .736 .711 .672

3.0

2.6

Table 6: Effect of Training Set Class Distribution on AUC
If we look at the results with pruning, we see that for 15 of the 26 data sets C4.5 with pruning
underperforms C4.5 without pruning. Thus, with respect to AUC, C4.5 without pruning is a
reasonable learner. However, note that for the car data set the natural distribution with pruning
falls into the optimum range, whereas without pruning it does not.
Figure 2 shows how class distribution affects AUC for the adult, covertype, and letter-a data
sets (the phone data set is not displayed as it was in Figure 1 because it would obscure the adult
data set). Again, the natural distribution is denoted by the X tick mark. The AUC for the best
distribution is underlined and displayed below the corresponding data point. In this case we also
see that near the optimal class distribution the AUC curves tend to be flatter, and hence less sensitive to changes in class distribution.
330

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

1.0

covertype

.984
.984

.984
.954

letter-a
AUC

0.9
.839

.861

.861

80

90

adult

0.8
.772

0.7
0

10

20

30

40

50

60

70

% Minority Class
Figure 2: Effect of Class Distribution on AUC for Select Data Sets
Figure 3 shows several ROC curves associated with the letter-vowel data set. These curves each
were generated from a single run of C4.5 (which is why the AUC values do not exactly match the
values in Table 6). In ROC space, the point (0,0) corresponds to the strategy of never making a
positive/minority prediction and the point (1,1) to always predicting the positive/minority class.
Points to the northwest indicate improved performance.
1.0

50% Minority
True Positive Rate

0.8

0.6

90% Minority

19% Minority (Natural)
% Minority
2%
19%
50%
90%

0.4

0.2

2% Minority

AUC
.599
.795
.862
.855

0.0
0.0

0.2

0.4

0.6

0.8

False Positive Rate
Figure 3: ROC Curves for the Letter-Vowel Data set
331

1.0

fiWeiss & Provost

Observe that different training distributions perform better in different areas of ROC space.
Specifically note that the classifier trained with 90% minority-class examples performs substantially better than the classifier trained with the natural distribution for high true-positive rates and
that the classifier training with 2% minority-class examples performs fairly well for low truepositive rates. Why? With only a small sample of minority-class examples (2%) a classifier can
identify only a few minority-labeled rules with high confidence. However, with a much larger
sample of minority-class examples (90%) it can identify many more such minority-labeled rules.
However, for this data set a balanced distribution has the largest AUC and performs best overall.
Note that the curve generated using the balanced class distribution almost always outperforms
the curve associated with the natural distribution (for low false-positive rates the natural distribution performs slightly better).

7. Forming a Good Class Distribution with Sensitivity to Procurement Costs
The results from the previous section demonstrate that some marginal class distributions yield
classifiers that perform substantially better than the classifiers produced by other training distributions. Unfortunately, in order to determine the best class distribution for training, forming all
thirteen training sets of size n, each with a different class distribution, requires nearly 2n examples. When it is costly to obtain training examples in a form suitable for learning, then this approach is self-defeating. Ideally, given a budget that allows for n training examples, one would
select a total of n training examples all of which would be used in the final training setand the
associated class distribution would yield classifiers that perform better than those generated from
any other class distribution (given n training examples). In this section we describe and evaluate
a heuristic, budget-sensitive, progressive sampling algorithm that approximates this ideal.
In order to evaluate this progressive sampling algorithm, it is necessary to measure how class
distribution affects classifier performance for a variety of different training-set sizes. These
measurements are summarized in Section 7.1 (the detailed results are included in Appendix B).
The algorithm for selecting training data is then described in Section 7.2 and its performance
evaluated in Section 7.3, using the measurements included in Appendix B.
7.1

The Effect of Class Distribution and Training-Set Size on Classifier Performance

Experiments were run to establish the relationship between class distribution, training-set size
and classifier performance. In order to ensure that the training sets contain a sufficient number
of training examples to provide meaningful results when the training-set size is dramatically reduced, only the data sets that yield relatively large training sets are used (this is determined based
on the size of the data set and the fraction of minority-class examples in the data set). Based on
this criterion, the following seven data sets were selected for analysis: phone, adult, covertype,
blackjack, kr-vs-kp, letter-a, and weather. The detailed results associated with these experiments
are contained in Appendix B.
The results for one of these data sets, the adult data set, are shown graphically in Figure 4 and
Figure 5, which show classifier performance using error rate and AUC, respectively. Each of the
nine performance curves in these figures is associated with a specific training-set size, which
contains between 1/128 and all of the training data available for learning (using the methodology
described in Section 4.1). Because the performance curves always improve with increasing dataset size, only the curves corresponding to the smallest and largest training-set sizes are explicitly
labeled.
332

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

Error Rate

35

30

Natural

1/128

25

20
1 (all available training data)
15
0

10

20

30

40

50

60

70

80

% Minority Examples in Training Set
Figure 4: Effect of Class Distribution and Training-set Size on Error Rate (Adult Data Set)

0.9
1 (all available training data)

AUC

0.8

0.7
1/128
0.6
Natural
0.5
0

10

20

30

40

50

60

70

80

90

100

% Minority Class Examples in Training Set
Figure 5: Effect of Class Distribution and Training-set Size on AUC (Adult Data Set)
333

fiWeiss & Provost

Figure 4 and Figure 5 show several important things. First, while a change in training-set size
shifts the performance curves, the relative rank of each point on each performance curve remains
roughly the same. Thus, while the class distribution that yields the best performance occasionally varies with training-set size, these variations are relatively rare and when they occur, they are
small. For example, Figure 5 (and the supporting details in Appendix B) indicates that for the
adult data set, the class distribution that yields the best AUC typically contains 80% minorityclass examples, although there is occasionally a small deviation (with 1/8 the training data 70%
minority-class examples does best). This gives support to the notion that there may be a best
marginal class distribution for a learning task and suggests that a progressive sampling algorithm
may be useful in locating the class distribution that yields the best, or nearly best, classifier performance.
The results also indicate that, for any fixed class distribution, increasing the size of the training set always leads to improved classifier performance. Also note that the performance curves
tend to flatten out as the size of the data set grows, indicating that the choice of class distribution may become less important as the training-set size grows. Nonetheless, even when all of the
available training data are used, the choice of class distribution does make a difference. This is
significant because if a plateau had been reached (i.e., learning had stopped), then it would be
possible to reduce the size of the training set without degrading classifier performance. In that
case it would not be necessary to select the class distribution of the training data carefully.
The results in Figure 4 and Figure 5 also show that by carefully selecting the class distribution, one can sometimes achieve improved performance while using fewer training examples. To
see this, consider the dashed horizontal line in Figure 4, which intersects the curve associated
with  of the training data at its lowest error rate, when the class distribution includes 10% minority-class examples. When this horizontal line is below the curve associated with all available
training data, then the training set with  of the data outperforms the full training set. In this
case we see that  of the training data with a 10% class distribution outperforms the natural class
distribution using all of the available training data. The two horizontal lines in Figure 5 highlight
just some of the cases where one can achieve improved AUC using fewer training data (because
larger AUC values indicate improved performance, compare the horizontal lines with the curves
that lie above them). For example, Figure 5 shows that the training set with a class distribution
that contains 80% minority-class examples and 1/128th of the total training data outperforms a
training set with twice the training data when its class distribution contains less than or equal to
40% minority-class examples (and outperforms a training set with four times the data if its class
distribution contains less than or equal to 10% minority-class examples). The results in Appendix B show that all of the trends noted for the adult data set hold for the other data sets and that
one can often achieve improved performance using less training data.
7.2

A Budget-Sensitive Progressive sampling Algorithm for Selecting Training Data

As discussed above, the size of the training set sometimes must be limited due to costs associated
with procuring usable training examples. For simplicity, assume that there is a budget n, which
permits one to procure exactly n training examples. Further assume that the number of training
examples that potentially can be procured is sufficiently large so that a training set of size n can
be formed with any desired marginal class distribution. We would like a sampling strategy that
selects x minority-class examples and y majority-class examples, where x + y = n, such that the

334

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

resulting class distribution yields the best possible classification performance for a training set of
size n.
The sampling strategy relies on several assumptions. First, we assume that the cost of executing the learning algorithm is negligible compared to the cost of procuring examples, so that the
learning algorithm may be run multiple times. This certainly will be true when training data are
costly. We further assume that the cost of procuring examples is the same for each class and
hence the budget n represents the number of examples that can be procured as well as the total
cost. This assumption will hold for many, but not all, domains. For example, for the phone data
set described in Section 1 the cost of procuring business and consumer examples is equal,
while for the telephone fraud domain the cost of procuring fraudulent examples may be substantially higher than the cost of procuring non-fraudulent examples. The algorithm described in this
section can be extended to handle non-uniform procurement costs.
The sampling algorithm selects minority-class and majority-class training examples such that
the resulting class distribution will yield classifiers that tend to perform well. The algorithm
begins with a small amount of training data and progressively adds training examples using a
geometric sampling schedule (Provost, Jensen & Oates, 1999). The proportion of minority-class
examples and majority-class examples added in each iteration of the algorithm is determined
empirically by forming several class distributions from the currently available training data,
evaluating the classification performance of the resulting classifiers, and then determining the
class distribution that performs best. The algorithm implements a beam-search through the space
of possible class distributions, where the beam narrows as the budget is exhausted.
We say that the sampling algorithm is budget-efficient if all examples selected during any iteration of the algorithm are used in the final training set, which has the heuristically determined
class distribution. The key is to constrain the search through the space of class distributions so
that budget-efficiency is either guaranteed, or very likely. As we will show, the algorithm described in this section is guaranteed to be budget-efficient. Note, however, that the class distribution of the final training set, that is heuristically determined, is not guaranteed to be the best
class distribution; however, as we will show, it performs well in practice.
The algorithm is outlined in Table 7, using pseudo-code, followed by a line-by-line explanation (a complete example is provided in Appendix C). The algorithm takes three user-specified
LQSXWSDUDPHWHUV WKHJHRPHWULFIDFWRUXVHGWRGHWHUPLQHWKHUDWHDWZKLFKWKHWUDLQLQJ-set size
grows; n, the budget; and cmin, the minimum fraction of minority-class examples and majorityclass examples that are assumed to appear in the final training set in order for the budgetefficiency guarantee to hold.5)RUWKHUHVXOWVSUHVHQWHGLQWKLVVHFWLRQ LVVHWWRVRWKDWWKH
training-set size doubles every iteration of the algorithm, and cmin is set to 1/32.
The algorithm begins by initializing the values for the minority and majority variables, which
represent the total number of minority-class examples and majority-class examples requested by
the algorithm. Then, in line 2, the number of iterations of the algorithm is determined, such that
the initial training-set size, which is subsequently set in line 5, will be at most cmin  n. This
will allow all possible class distributions to be formed using at most cmin minority-class examples and cmin majority-FODVVH[DPSOHV)RUH[DPSOHJLYHQWKDW LVDQGcmin is 1/32, in line 2
variable K will be set to 5 and in line 5 the initial training-set size will be set to 1/32 n.
5. Consider the degenerate case where the algorithm determines that the best class distribution contains no minorityclass examples or no majority-class examples. If the algorithm begins with even a single example of this class, then
it will not be budget-efficient.

335

fiWeiss & Provost

1. minority = majority = 0;


2.

K = log



 1 


 c min 



# current number minority/majority examples in hand

;

# number of iterations is K+1

3. for (j = 0; j .M Mfi
4. {
5.
size = nff fiK-j
6.
7.
8.
9.
10.
11.
12.

# for each iteration (e.g., j = 0, 1,2,3,4,5)
# set training-set size for iteration j

if (j = 0)
beam_bottom = 0; beam_top = 1;
# initialize beam for first iteration
else if (j = K)
beam_bottom = best; beam_top = best; # last iteration only evaluate previous best
else
min(best,1 best )
beam_radius =
 +1  1
beam_bottom = best  beam_radius; beam_top = best + beam_radius;

13.
14.

min_needed = size  beam_top;
# number minority examples needed
maj_needed = size  (1.0  beam_bottom); # number majority examples needed

15.
16.
17.
18.

if (min_needed > minority)
request (min_needed - minority) additional minority-class examples;
if (maj_needed > majority)
request (maj_needed - majority) additional majority-class examples;

19.
20.

evaluate(beam_bottom, beam_top, size); # evaluate distributions in the beam; set best
}
Table 7: The Algorithm for Selecting Training Data

Next, in lines 6-12, the algorithm determines the class distributions to be considered in each
iteration by setting the boundaries of the beam. For the first iteration, all class distributions are
considered (i.e., the fraction of minority-class examples in the training set may vary between 0
and 1) and for the very last iteration, only the best-performing class distribution from the previous iteration is evaluated. In all other iterations, the beam is centered on the class distribution
that performed best in the previous iteration and the radius of the beam is set (in line 11) such
that the ratio beam_top/beam_bottom wiOOHTXDO )RUH[DPSOHLI LVDQGbest is .15, then
beam_radius is .05 and the beam will span from .10 to .20which difIHUE\DIDFWRURIffLH fi
In lines 13 and 14 the algorithm computes the number of minority-class examples and majority-class examples needed to form the class distributions that fall within the beam. These values
are determined from the class distributions at the boundaries of the beam. In lines 15-18 additional examples are requested, if required. In line 19 an evaluation procedure is called to form
336

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

the class distributions within the beam and then to induce and to evaluate the classifiers. At a
minimum this procedure will evaluate the class distributions at the endpoints and at the midpoint
of the beam; however, this procedure may be implemented to evaluate additional class distributions within the beam. The procedure will set the variable best to the class distribution that performs best. If the best performance is achieved by several class distributions, then a resolution
procedure is needed. For example, the class distribution for which the surrounding class distributions perform best may be chosen; if this still does not yield a unique value, then the bestperforming class distribution closest to the center of the beam may be chosen. In any event, for
the last iteration, only one class distribution is evaluatedthe previous best. To ensure budgetefficiency, only one class distribution can be evaluated in the final iteration.
This algorithm is guaranteed to request only examples that are subsequently used in the final
training set, which will have the heuristically determined class distribution. This guarantee can
be verified inductively. First, the base case. The calculation for K in line 2 ensures that the initial training set will contain cmin  n training examples. Since we assume that the final training
set will have at least cmin minority-class examples and cmin majority-class examples, all examples used to form the initial training set are guaranteed to be included in the final training set.
Note that cmin may be set arbitrarily smallthe smaller cmin the larger K and the smaller the
size of the initial training set.
The inductive step is based on the observation that because the radius of the beam in line 11 is
sHWVRWKDWWKHEHDPVSDQVDWPRVWDIDFWRURI DOOH[DPSOHVUHTXHVWHGLQHDFKLWHUDWLRQDUHJXDranteed to be used in the final training set. To see this, we will work backward from the final
iteration, rather than working forward as is the case in most inductive proofs. Assume that the
result of the algorithm is that the fraction of minority-class examples in the final training set is p,
so that there are p  n minority-class examples in the final training set. This means that p was the
best distribution from the previous iteration. Since p must fall somewhere within the beam for
the previous iteration and the beam must span a factor ZHFDQVD\WKHIROORZLQJWKHIUDFWLRQ
of minority-class examples in the previous iteration could range from p ffLIp was at the top of
WKH SUHYLRXV EHDPfi WR  p (if p was at the bottom of the previous beam). Since the previous
iteration contains n/ H[DPSOHVGXHWRWKHJHRPHWULFVDPSOLQJVFKHPHWKHQWKHSUHYLRXVLWHUaWLRQKDVDWPRVWff p)  n RUp  n, minority-class examples. Thus, in all possible cases all
minority-class examples from the previous iteration can be used in the final interaction. This
argument applies similarly to the majority-class examples and can be extended backwards to
previous iterations.6 Thus, because of the bound on the initial training-set size and the restriction
on the width of the beam not to exceed the geometULFIDFWRU WKHDOJRULWKPJXDUDQWHHVWKDWDOO
examples requested during the execution of the algorithm will be used in the final training set.
A complete, detailed, iteration-by-iteration example describing the sampling algorithm as it is
applied to the phone data set is provided in Appendix C, Table C1. In that example error rate is
used to evaluate classifier performance. The description specifies the class distributions that are
evaluated during the execution of the algorithm. This trajectory is graphically depicted in
Figure 6, narrowing in on the final class distribution. At each iteration, the algorithm considers
the beam of class distributions bounded by the two curves.

6. The only exception is for the first iteration of the algorithm, since in this situation the beam is unconditionally set to
span all class distributions. This is the reason why the cmin value is required to provide the efficiency guarantee.

337

fiWeiss & Provost

% Minority Class Examples

100

80

60

beam_top

40

10% (best)
beam_bottom

20

0
0

1

2

3

4

5

Iteration

Figure 6: The Trajectory of the Algorithm through the Space of Class Distributions.
7.3 Results for the Sampling Algorithm
The budget-sensitive progressive sampling algorithm was applied to the phone, adult, covertype,
kr-vs-kp, weather, letter-a and blackjack data sets using both error rate and AUC to measure classifier performance. However, the method for setting the beam (described in lines 6-12 in Table 7)
was modified so that the results from the experiments described in Section 7.1 (and detailed in
Appendix B), which evaluate only the 13 listed class distributions, could be used. Specifically,
at each iteration the low end (high end) of the beam is set to the class distribution specified in
Appendix B that is just below (above) the best performing class distribution from the prior iteration. For example, if in one iteration the best performing class distribution contains 30% minority-class examples, then in the next iteration the bottom of the beam is set to include 20%
minority-class examples and the top of the beam to include 40% minority-class examples (of the
13 sampled class distributions, these are the closest to the 30% class distribution). Although this
will someWLPHVDOORZWKHEHDPWRVSDQDUDQJHJUHDWHUWKDQ fffiLQSUDFWLFHWKLVGRHVQRWUHVXOW
in a problemfor the seven data sets all examples requested by the algorithm are included in the
final training set. In addition, a slight improvement was made to the algorithm. Specifically, for
any iteration, if the number of examples already in hand (procured in previous iterations) is sufficient to evaluate additional class distributions, then the beam is widened to include these additional class distributions (this can happen because during the first iteration the beam is set very
wide).
The performance of this sampling algorithm is summarized in Table 8, along with the performance of two other strategies for selecting the class distribution of the training data. The first
of the two additional strategies, the Pick Natural/Balanced Strategy, is based on the guidelines
suggested by the empirical results from Section 6. This strategy selects the natural distribution
when error rate is the performance metric and the balanced class distribution when AUC is the
338

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

performance metric. The Pick Best strategy selects the class distribution that performs best
over the 13 evaluated class distributions (see Tables 5 and 6). Given that we only consider the
13 class distributions, this strategy will always yield the best results but, as we shall see, is more
costly than the other strategies. The value representing the best, budget-efficient performance
(lowest error rate, highest AUC) is underlined for each data set. A detailed iteration-by-iteration
description of the algorithm, for all seven data sets, is provided in Appendix C, Table C3.
Table 8 also specifies the cost for each strategy, based on the number of training examples requested by the algorithm. This cost is expressed with respect to the budget n (each strategy yields
a final training set with n examples). The Pick Natural/Balanced strategy always requires exactly n examples to be selected and therefore has a cost of n and is budget-efficient. The Pick
Best strategy has a total cost of 1.93n and hence is not budget-efficient (because it evaluates
class distributions with between 2% and 95% minority-class examples, it requires .95n minorityclass examples and .98n majority-class examples). The cost of the sampling algorithm depends
on the performance of the induced classifiers: with the changes to the algorithm described in this
section, it is no longer guaranteed to be budget-efficient. Nonetheless, in all casesfor both
error rate and AUCthe sampling algorithm has a cost of exactly n and hence turns out to be
budget-efficient.

Data Set

phone
adult
covertype
kr-vs-kp
weather
letter-a
blackjack

Sampling Algorithm Pick Natural/Balanced
ER
AUC Cost
ER
AUC Cost

12.3%
17.1%
5.0%
1.2%
33.1%
2.8%
28.4%

.851
.861
.984
.998
.740
.954
.715

n
n
n
n
n
n
n

12.6%
17.3%
5.0%
1.4%
33.7%
2.8%
28.4%

.849
.851
.980
.998
.736
.937
.713

n
n
n
n
n
n
n

ER

Pick Best
AUC Cost

12.3%
16.9%
5.0%
1.2%
33.1%
2.6%
28.4%

.853
.861
.984
.998
.740
.954
.715

1.93n
1.93n
1.93n
1.93n
1.93n
1.93n
1.93n

Table 8: Comparative Performance of the Sampling Algorithm
The results in Table 8 show that by using the budget-sensitive progressive sampling algorithm to choose the training data it is possible to achieve results that are as good as or better than
the strategy of always using the natural distribution for error rate and the balanced distribution
for AUCwithout requiring that any extra examples be procured. In particular, when comparing these two strategies, the progressive sampling strategy has a win-tie-loss record of 10-4-0.
While in some cases these wins do not lead to large improvements in performance, in some cases
they do (e.g., for the kr-vs-kp data set the sampling strategy yields a relative reduction in error
rate of 17%). The results in Table 8 also show that the sampling algorithm performs nearly as
well as the Pick Best strategy (it performs as well in 11 of 14 cases), which is almost twice as
costly. Because the progressive sampling strategy performs nearly as well as the Pick Best
strategy, we conclude that when the progressive sampling strategy does not substantially outperform the Pick Natural/Balanced strategy, it is not because the sampling strategy cannot identify a good (i.e., near-optimal) class distribution for learning, but rather that the optimal class
distribution happens to be near the natural (balanced) distribution for error rate (AUC). Note
that there are some data sets (optdigits, contraceptive, solar-flare, car) for which this is not the
case and hence the Pick Natural/Balanced strategy will perform poorly. Unfortunately, because these data sets would yield relatively small training sets, the progressive sampling algorithm could not be run on them.
339

fiWeiss & Provost

In summary, the sampling algorithm introduced in this section leads to near-optimal results
results that outperform the straw-man strategy of using the natural distribution to minimize error
rate and the balanced distribution to maximize AUC. Based on these results, the budgetsensitive progressive sampling algorithm is attractiveit incurs the minimum possible cost in
terms of procuring examples while permitting the class distribution for training to be selected
using some intelligence.

8. Related Work
Several researchers have considered the question of what class distribution to use for a fixed
training-set size, and/or, more generally, how class distribution affects classifier performance.
Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training
class distribution and classifier performance when the training-set size is held fixed, but focus
most of their attention on other issues. These studies also analyze only a few data sets, which
makes it impossible to draw general conclusions about the relationship between class distribution
and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo
(1998) show that when accuracy is the performance metric, a training set that uses the natural
class distribution yields the best results. These results agree partially with our resultsalthough
we show that the natural distribution does not always maximize accuracy, we show that the optimal distribution generally is close to the natural distribution. Chan & Stolfo also show that
when actual costs are factored in (i.e., the cost of a false positive is not the same as a false negative), the natural distribution does not perform best; rather a training distribution closer to a balanced distribution performs best. They also observe, as we did, that by increasing the percentage
of minority-class examples in the training set, the induced classifier performs better at classifying
minority examples. It is important to note, however, that neither Chan & Stolfo nor Catlett adjusted the induced classifiers to compensate for changes made to the class distribution of the
training set. This means that their results are biased in favor of the natural distribution (when
measuring classification accuracy) and that they could improve the classification performance of
minority class examples simply by changing (implicitly) the decision threshold. As the results in
Appendix A show, compensating for the changed class distribution can affect the performance of
a classifier significantly.
Several researchers have looked at the general question of how to reduce the need for labeled
training data by selecting the data intelligently, but without explicitly considering the class distribution. For example, Cohn et al. (1994) and Lewis and Catlett (1994) use active learning to
add examples to the training set for which the classifier is least certain about the classification.
Saar-Tsechansky and Provost (2001, 2003) provide an overview of such methods and also extend
them to cover AUC and other non-accuracy based performance metrics. The setting where these
methods are applicable is different from the setting we consider. In particular, these methods
assume either that arbitrary examples can be labeled or that the descriptions of a pool of unlabeled examples are available and the critical cost is associated with labeling them (so the algorithms select the examples intelligently rather than randomly). In our typical setting, the cost is
in procuring the descriptions of the examplesthe labels are known beforehand.
There also has been some prior work on progressive sampling strategies. John and Langley
(1996) show how one can use the extrapolation of learning curves to determine when classifier
performance using a subset of available training data comes close to the performance that would
be achieved by using the full data set. Provost et al. (1999) suggest using a geometric sampling
340

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

schedule and show that it is often more efficient than using all of the available training data. The
techniques described by John and Langley (1996) and Provost et al. (1999) do not change the
distribution of examples in the training set, but rather rely on taking random samples from the
available training data. Our progressive sampling routine extends these methods by stratifying
the sampling by class, and using the information acquired during the process to select a good
final class distribution.
There is a considerable amount of research on how to build good classifiers when the class
distribution of the data is highly unbalanced and it is costly to misclassify minority-class examples (Japkowicz et al., 2000). This research is related to our work because a frequent approach
for learning from highly skewed data sets is to modify the class distribution of the training set.
Under these conditions, classifiers that optimize for accuracy are especially inappropriate because they tend to generate trivial models that almost always predict the majority class. A common approach for dealing with highly unbalanced data sets is to reduce the amount of class
imbalance in the training set. This tends to produce classifiers that perform better on the minority class than if the original distribution were used. Note that in this situation the training-set size
is not fixed and the motivation for changing the distribution is simply to produce a better classifiernot to reduce, or minimize, the training-set size.
The two basic methods for reducing class imbalance in training data are under-sampling and
over-sampling. Under-sampling eliminates examples in the majority class while over-sampling
replicates examples in the minority class (Breiman, et al., 1984; Kubat & Matwin, 1997; Japkowicz & Stephen, 2001). Neither approach consistently outperforms the other nor does any specific under-sampling or over-sampling rate consistently yield the best results. Estabrooks and
Japkowicz (2001) address this issue by showing that a mixture-of-experts approach, which combines classifiers built using under-sampling and over-sampling methods with various sampling
rates, can produce consistently good results.
Both under-sampling and over-sampling have known drawbacks. Under-sampling throws out
potentially useful data while over-sampling increases the size of the training set and hence the
time to build a classifier. Furthermore, since most over-sampling methods make exact copies of
minority class examples, overfitting is likely to occurclassification rules may be induced to
cover a single replicated example.7 Recent research has focused on improving these basic methods. Kubat and Matwin (1997) employ an under-sampling strategy that intelligently removes
majority examples by removing only those majority examples that are redundant or that border the minority examplesfiguring they may be the result of noise. Chawla et al. (2000) combine under-sampling and over-sampling methods, and, to avoid the overfitting problem, form new
minority class examples by interpolating between minority-class examples that lie close together.
Chan and Stolfo (1998) take a somewhat different, and innovative, approach. They first run preliminary experiments to determine the best class distribution for learning and then generate multiple training sets with this class distribution. This is typically accomplished by including all
minority-class examples and some of the majority-class examples in each training set. They then
apply a learning algorithm to each training set and then combine the generated classifiers to form
a composite learner. This method ensures that all available training data are used, since each
majority-class example will be found in at least one of the training sets.

7. This is especially true for methods such as C4.5, which stops splitting based on counting examples at the leaves of
the tree.

341

fiWeiss & Provost

The research in this article could properly be viewed as research into under-sampling and its
effect on classifier performance. However, given this perspective, our research performs undersampling in order to reduce the training-set size, whereas in the research relating to skewed data
sets the primary motivation is to improve classifier performance. For example, Kubat and Matwin (1997) motivate the use of under-sampling to handle skewed data sets by saying that adding
examples of the majority class to the training set can have a detrimental effect on the learners
behavior: noisy or otherwise unreliable examples from the majority class can overwhelm the
minority class (p. 179). A consequence of these different motivations is that in our experiments
we under-sample the minority and/or majority classes, while in the research concerned with
learning from skewed distributions it is only the majority class that is under-sampled.
The use of under-sampling for reducing the training-set size (and thereby reducing cost) may
be the more practically useful perspective. Reducing the class imbalance in the training set effectively causes the learner to impose a greater cost for misclassifying minority-class examples
(Breiman et al., 1984). Thus, when the cost of acquiring and learning from the data is not an
issue, cost-sensitive or probabilistic learning methods are a more direct and arguably more appropriate way of dealing with class imbalance, because they do not have the problems, noted
earlier, that are associated with under-sampling and over-sampling. Such approaches have been
shown to outperform under-sampling and over-sampling (Japkowicz & Stephen, 2002). To quote
Drummond and Holte (2000) all of the data available can be used to produce the tree, thus
throwing away no information, and learning speed is not degraded due to duplicate instances (p.
239).

9. Limitations and Future Research
One limitation with the research described in this article is that because all results are based on
the use of a decision-tree learner, our conclusions may hold only for this class of learners. However, there are reasons to believe that our conclusions will hold for other learners as well.
Namely, since the role that class distribution plays in learningand the reasons, discussed in
Section 5.2, for why a classifier will perform worse on the minority classare not specific to
decision-tree learners, one would expect other learners to behave similarly. One class of learners
that may especially warrant further attention, however, are those learners that do not form disjunctive concepts. These learners will not suffer in the same way from the problem of small
disjuncts, which our results indicate is partially responsible for minority-class predictions having a higher error rate than majority-class predictions.8 Thus, it would be informative to extend
this study to include other classes of learners, to determine which results indeed generalize.
The program for inducing decision trees used throughout this article, C4.5, only considers the
class distribution of the training data when generating the decision tree. The differences between
the class distribution of the training data and the test data are accounted for in a post-processing
step by re-computing the probability estimates at the leaves and using these estimates to re-label
the tree. If the induction program had knowledge of the target (i.e., test) distribution during the
tree-building process, then a different decision tree might be constructed. However, research
indicates that this is not a serious limitation. In particular, Drummond and Holte (2000) showed
that there are splitting criteria that are completely insensitive to the class distribution and that
8. However, many learners do form disjunctive concepts or something quite close. For example, Van den Bosch et al.
(1997) showed that instance-based learners can be viewed as forming disjunctive concepts.

342

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

these splitting criteria perform as well or better than methods that factor in the class distribution.
They further showed that C4.5s splitting criterion is relatively insensitive to the class distributionand therefore to changes in class distribution.
We employed C4.5 without pruning in our study because pruning is sensitive to class distribution and C4.5s pruning strategy does not take the changes made to the class distribution of the
training data into account. To justify this choice we showed that C4.5 without pruning performs
competitively with C4.5 with pruning (Sections 6.2 and 6.3). Moreover, other research (Bradford et al., 1998) indicates that classifier performance does not generally improve when pruning
takes class distribution and costs into account. Nevertheless it would be worthwhile to see just
how a cost/distribution-sensitive pruning strategy would affect our results. We know of no
published pruning method that attempts to maximize AUC.
In this article we introduced a budget-sensitive algorithm for selecting training data when it is
costly to obtain usable training examples. It would be interesting to consider the case where it is
more costly to procure examples belonging to one class than to another.

10. Conclusion
In this article we analyze, for a fixed training-set size, the relationship between the class distribution of training data and classifier performance with respect to accuracy and AUC. This analysis
is useful for applications where data procurement is costly and data can be procured independently by class, or where the costs associated with learning from the training data are sufficient to
require that the size of the training set be reduced. Our results indicate that when accuracy is the
performance measure, the best class distribution for learning tends to be near the natural class
distribution, and when AUC is the performance measure, the best class distribution for learning
tends to be near the balanced class distribution. These general guidelines are just that
guidelinesand for a particular data set a different class distribution may lead to substantial
improvements in classifier performance. Nonetheless, if no additional information is provided
and a class distribution must be chosen without any experimentation, our results show that for
accuracy and for AUC maximization, the natural distribution and a balanced distribution (respectively) are reasonable default training distributions.
If it is possible to interleave data procurement and learning, we show that a budget-sensitive
progressive sampling strategy can improve upon the default strategy of using the natural distribution to maximize accuracy and a balanced distribution to maximize the area under the ROC
curvein our experiments the budget-sensitive sampling strategy never did worse. Furthermore,
in our experiments the sampling strategy performs nearly as well as the strategy that evaluates
many different class distributions and chooses the best-performing one (which is optimal in terms
of classification performance but inefficient in terms of the number of examples required).
The results presented in this article also indicate that for many data sets the class distribution
that yields the best-performing classifiers remains relatively constant for different training-set
sizes, supporting the notion that there often is a best marginal class distribution. These results
further show that as the amount of training data increases the differences in performance for
different class distributions lessen (for both error rate and AUC), indicating that as more data
becomes available, the choice of marginal class distribution becomes less and less important
especially in the neighborhood of the optimal distribution.
This article also provides a more comprehensive understanding of how class distribution affects learning and suggests answers to some fundamental questions, such as why classifiers almost always perform worse at classifying minority-class examples. A method for adjusting a
343

fiWeiss & Provost

classifier to compensate for changes made to the class distribution of the training set is described
and this adjustment is shown to substantially improve classifier accuracy (see Appendix A). We
consider this to be particularly significant because previous research on the effect of class distribution on learning has not employed this, or any other, adjustment (Catlett, 1991; Chan & Stolfo,
1998; Japkowicz & Stephen, 2002).
Practitioners often make changes to the class distribution of training data, especially when the
classes are highly unbalanced. These changes are seldom done in a principled manner and the
reasons for changing the distributionand the consequencesare often not fully understood.
We hope this article helps researchers and practitioners better understand the relationship between class distribution and classifier performance and permits them to learn more effectively
when there is a need to limit the amount of training data.

Acknowledgments
We would like to thank Haym Hirsh for the comments and feedback he provided throughout this
research. We would also like to thank the anonymous reviewers for helpful comments, and IBM
for a Faculty Partnership Award.

Appendix A: Impact of Class Distribution Correction on Classifier Performance
Table A1 compares the performance of the decision trees labeled using the uncorrected frequencybased estimate (FB) with those labeled using the corrected frequency-based estimate (CT-FB).
Dataset
letter-a
pendigits
abalone
sick-euthyroid
connect-4
optdigits
covertype
solar-flare
phone
letter-vowel
contraceptive
adult
splice-junction
network2
yeast
network1
car
german
breast-wisc
blackjack
weather
bands
market1
crx
kr-vs-kp
move
Average

Error Rate
FB
CT-FB
9.79
5.38
4.09
4.02
30.45
22.97
9.82
6.85
30.21
27.57
6.17
3.41
6.62
6.46
36.20
29.12
17.85
14.81
18.89
14.16
40.77
39.65
22.69
20.05
9.02
8.74
30.80
29.96
34.01
28.80
31.99
30.99
8.26
7.92
38.37
37.09
6.76
6.74
33.02
28.71
34.62
34.61
32.68
32.68
25.77
25.77
20.84
21.48
1.22
1.22
28.24
28.24
21.89
19.90

% Rel.
Improv.
45.0
1.7
24.6
30.2
8.7
44.7
2.4
19.6
17.0
25.0
2.7
11.6
3.1
2.7
15.3
3.1
4.1
3.3
0.3
13.1
0.0
0.0
0.0
-3.1
0.0
0.0
10.6

% Labels % Errors from Min.
Changed
FB
CT-FB
39.0
2.7
7.2
3.2
5.6
7.8
5.6
8.5
19.1
6.7
8.8
14.6
14.7
8.5
10.4
42.5
6.0
21.2
2.4
7.0
8.5
20.4
19.3
30.7
3.2
25.2
44.4
44.1
15.9
30.2
11.1
20.6
27.6
30.7
19.6
36.8
14.1
20.1
28.4
1.2
32.9
40.1
4.6
29.4
47.0
1.3
32.9
38.2
5.3
25.9
33.8
16.1
30.8
35.8
0.4
38.5
38.7
17.1
42.9
76.2
0.0
40.5
40.5
0.6
90.2
90.2
23.9
46.0
48.6
17.2
46.2
51.4
0.2
58.5
58.5
20.8
52.6
60.7
13.3
28.3
36.4

Table A1: Impact of the Probability Estimates on Error Rate
344

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

The results in the main body of the article were all based on the use of the corrected frequencybased estimate to label the leaves of the induced decision trees, so that the decision trees were
not improperly biased by the changes made to the class distribution of the training set. Thus, the
comparison in Table A1 evaluates the significance of correcting for changes to the class distribution of the training data. This comparison is based on the situation where the class distribution
of the training set is altered to contain an equal number of minority-class and majority-class examples (the test set will still contain the natural class distribution). The results are based on 30
runs and the data sets are listed in order of decreasing class imbalance.
The error rate for the estimates is displayed in the second and third columns in Table A1, and,
for each data set, the lowest error rate is underlined. The fourth column specifies the relative
improvement that results from using the corrected frequency-based estimate. The fifth column
specifies the percentage of the leaves in the decision tree that are assigned a different class label
when the corrected estimate is used. The last two columns specify, for each estimate, the percentage of the total errors that are contributed by the minority-class test examples.
Table A1 shows that by employing the corrected frequency-based estimate instead of the uncorrected frequency-based estimate, there is, on average, a relative 10.6% reduction in error rate.
Furthermore, in only one case does the uncorrected frequency-based estimate outperform the
corrected frequency-based estimate. The correction tends to yield a larger reduction for the most
highly unbalanced data setsin which cases it plays a larger role. If we restrict ourselves to the
first 13 data sets listed in Table 2, for which the minority class makes up less than 25% of the
examples, then the relative improvement over these data sets is 18.2%. Note that because in this
scenario the minority class is over-sampled in the training set, the corrected frequency-based
estimate can only cause minority-labeled leaves to be labeled with the majority-class. Consequently, as the last column in the table demonstrates, the corrected version of the estimate will
cause more of the errors to come from the minority-class test examples.

Appendix B: The Effect of Training-Set Size and Class Distribution on Learning
Experiments were run to establish the joint impact that class distribution and training-set size
have on classifier performance. Classifier performance is reported for the same thirteen class
distributions that were analyzed in Section 6 and for nine different training set sizes. The nine
training set sizes are generated by omitting a portion of the available training data (recall that, as
described in Section 4.1, the amount of available training data equals  of the number of minority-class examples). For these experiments the training set sizes are varied so as to contain the
following fractions of the total available training data: 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 3/4
and 1. In order to ensure that the training sets contain a sufficient number of training examples
to provide meaningful results, the original data set must be relatively large and/or contain a high
proportion of minority-class examples. For this reason, only the following seven data sets were
selected for analysis: phone, adult, covertype, kr-vs-kp, weather, letter-a and blackjack. Because
the last four data sets in this list yield a smaller number of training examples than the first three,
for these data sets the two smallest training-set sizes (1/128 and 1/64) are not evaluated. The
experimental results are summarized in Tables B1a and B1b. An asterisk is used to denote the
natural class distribution for each data set and, for each training-set size, the class distribution
that yields the best performance is displayed in bold and is underlined.

345

fiWeiss & Provost

Data Set
PHONE
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Size Metric
2
5
10
18.2*
20
30
40
50
60
1/128
.641 .737 .784 .793 .792 .791 .791 .789 .788
1/64
.707 .777 .784 .803 .803 .803 .801 .802 .801
1/32
.762 .794 .809 .812 .812 .811 .811 .811 .810
1/16
.784 .813 .816 .823 .823 .824 .818 .821 .822
1/8
AUC .801 .823 .828 .830 .830 .830 .830 .829 .830
1/4
.819 .835 .837 .839 .839 .837 .837 .836 .836
1/2
.832 .843 .846 .846 .845 .845 .843 .843 .843
3/4
.838 .847 .849 .849 .849 .848 .846 .847 .846
1
.843 .850 .852 .851 .851 .850 .850 .849 .848
1/128
17.47 16.42 15.71 16.10 16.25 17.52 18.81 21.21 22.87
1/64
17.01 15.75 15.21 15.12 15.20 16.39 17.59 19.60 22.11
1/32
16.22 15.02 14.52 14.50 14.75 15.41 16.81 18.12 20.02
1/16 Error 15.78 14.59 14.01 14.02 14.18 14.70 16.09 17.50 18.68
1/8
Rate 15.17 14.08 13.46 13.61 13.71 14.27 15.30 16.51 17.66
1/4
14.44 13.55 13.12 13.23 13.27 13.85 14.78 15.85 17.09
1/2
13.84 13.18 12.81 12.83 12.95 13.47 14.38 15.30 16.43
3/4
13.75 13.03 12.60 12.70 12.74 13.35 14.12 15.01 16.17
1
13.45 12.87 12.32 12.62 12.68 13.25 13.94 14.81 15.97

ADULT
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1
1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

COVERTYPE
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1
1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

AUC

Error
Rate

AUC

Error
Rate

2
.571
.621
.638
.690
.735
.774
.795
.811
.816
23.80
23.32
22.95
22.66
21.65
20.56
19.51
18.82
18.47

5
.586
.630
.674
.721
.753
.779
.803
.814
.821
23.64
22.68
22.09
21.34
20.15
19.08
18.10
17.70
17.26

10
.633
.657
.711
.733
.768
.793
.812
.823
.829
23.10
22.21
21.12
20.29
19.13
18.20
17.54
17.17
16.85

20
.674
.702
.735
.760
.785
.804
.822
.830
.836
23.44
21.77
20.77
19.90
18.87
18.42
17.54
17.32
17.09

23.9*
.680
.714
.742
.762
.787
.809
.825
.833
.839
23.68
21.80
20.97
20.07
19.30
18.70
17.85
17.46
17.25

70
.786
.798
.812
.821
.829
.836
.843
.847
.848
26.40
24.80
21.77
20.70
19.66
18.94
17.88
17.33
17.32

80
.785
.799
.811
.822
.831
.838
.844
.848
.850
30.43
27.34
24.86
22.46
21.26
20.43
19.57
18.82
18.73

90
.774
.788
.805
.817
.828
.836
.846
.851
.853
33.26
30.21
25.31
24.15
23.23
22.28
21.68
20.43
20.24

95
.731
.744
.778
.805
.818
.832
.844
.848
.850
37.27
26.86
28.74
24.52
23.33
22.90
21.68
21.24
21.07

30
.694
.711
.751
.778
.793
.813
.829
.837
.842
23.90
23.08
21.11
20.37
19.67
19.12
18.39
18.07
17.78

40
.701
.722
.755
.787
.799
.820
.834
.843
.846
25.22
24.38
22.37
21.43
20.86
20.10
19.38
18.96
18.85

50
.704
.732
.766
.791
.809
.827
.838
.845
.851
26.94
26.29
24.41
23.18
22.33
21.39
20.83
20.40
20.05

60
.723
.739
.762
.794
.812
.831
.841
.849
.854
29.50
28.07
27.08
25.27
24.56
23.48
22.81
22.13
21.79

70
.727
.746
.765
.787
.816
.832
.847
.853
.858
33.08
31.45
30.27
28.67
27.14
25.78
24.88
24.32
24.08

80
.728
.755
.772
.785
.813
.834
.849
.856
.861
37.85
36.41
34.04
33.41
31.06
29.54
28.15
27.59
27.11

90
.722
.752
.766
.780
.803
.824
.847
.855
.861
46.13
43.64
42.40
40.65
38.35
36.17
34.71
33.92
33.00

95
.708
.732
.759
.771
.797
.811
.834
.848
.855
48.34
47.52
47.20
46.68
45.83
43.93
41.24
40.47
39.75

2
5
10
14.8*
20
30
.767 .852 .898 .909 .916 .913
.836 .900 .924 .932 .937 .935
.886 .925 .942 .947 .950 .947
.920 .944 .953 .957 .959 .959
.941 .955 .963 .965 .967 .968
.953 .965 .970 .973 .975 .976
.963 .972 .979 .981 .981 .980
.968 .976 .982 .982 .983 .982
.970 .980 .984 .984 .984 .983
10.44 10.56 10.96 11.86 13.50 16.16
9.67 9.29 10.23 11.04 12.29 14.55
8.87 8.66 9.44 10.35 11.29 13.59
8.19 7.92 8.93 9.67 10.37 11.93
7.59 7.32 7.87 8.65 9.26 10.31
6.87 6.44 7.04 7.49 8.01 9.05
6.04 5.71 5.97 6.45 6.66 7.14
5.81 5.31 5.48 5.75 5.87 6.25
5.54 5.04 5.00 5.03 5.26 5.64

40
.916
.936
.948
.959
.969
.975
.978
.980
.982
18.26
16.52
15.34
13.51
11.63
9.86
7.53
6.57
5.95

50
.916
.932
.948
.957
.968
.973
.977
.979
.980
20.50
18.58
17.30
15.35
13.06
10.56
8.03
6.89
6.46

60
.909
.928
.944
.955
.967
.972
.975
.976
.978
23.44
21.40
19.31
17.42
14.68
11.45
8.80
7.58
7.23

70
.901
.922
.939
.951
.963
.970
.972
.975
.976
26.95
24.78
21.82
19.40
16.39
12.28
9.94
8.72
8.50

80
.882
.913
.930
.945
.957
.965
.970
.971
.973
31.39
27.65
24.86
22.30
18.28
14.36
11.44
10.69
10.18

90
.854
.885
.908
.929
.948
.956
.961
.966
.968
37.92
34.12
28.37
25.74
22.50
18.05
14.85
13.92
13.03

95
.817
.851
.876
.906
.929
.943
.953
.958
.960
44.54
41.67
33.91
28.36
26.87
22.59
18.37
16.29
16.27

Table B1a: The Effect of Training-Set Size and Class Distribution on Classifier Performance

346

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

Data Set
KR-VS-KP
|
|
|
|
|
|
|
|
|
|
|
|
|
WEATHER
|
|
|
|
|
|
|
|
|
|
|
|
|

Size
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

LETTER-A
|
|
|
|
|
|
|
|
|
|
|
|
|

1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

BLACKJACK
|
|
|
|
|
|
|
|
|
|
|
|
|

1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

Metric

AUC

Error
Rate

AUC

Error
Rate

AUC

Error
Rate

AUC

Error
Rate

2
.567
.618
.647
.768
.886
.922
.937
42.61
37.99
35.16
26.33
17.11
13.37
12.18

5
.637
.681
.809
.888
.946
.966
.970
36.35
33.02
22.73
15.74
11.07
7.49
6.50

10
20
30
40
47.8*
50
60
70
80
90
.680 .742 .803 .852 .894 .894 .897 .854 .797 .695
.800 .888 .920 .942 .951 .952 .951 .945 .929 .839
.893 .947 .960 .976 .976 .976 .975 .974 .967 .936
.938 .980 .984 .987 .989 .989 .989 .985 .982 .973
.981 .992 .994 .995 .995 .995 .995 .994 .990 .982
.987 .994 .995 .996 .995 .996 .996 .995 .994 .986
.991 .994 .997 .998 .997 .998 .998 .997 .994 .988
33.49 27.44 21.92 17.82 14.08 14.06 17.17 21.18 26.31 33.10
22.76 15.49 12.66 10.46 10.14 9.74 10.08 11.53 13.97 22.14
15.30 10.51 8.66 7.10 6.45 6.63 6.91 7.44 9.24 13.21
11.26 6.16 5.46 4.59 4.24 4.32 4.23 5.27 5.97 8.54
6.00 3.71 2.72 2.38 2.05 2.11 2.32 2.66 4.16 5.61
4.10 2.75 2.12 1.60 1.64 1.55 1.55 1.93 2.88 5.05
3.20 2.33 1.73 1.16 1.39 1.22 1.34 1.53 2.55 3.66

95
.637
.724
.807
.947
.974
.980
.982
38.82
30.95
23.97
12.45
8.66
7.03
6.04

2
.535
.535
.535
.563
.578
.582
.694
40.76
39.56
39.27
39.00
38.62
38.56
38.41

5
.535
.533
.565
.606
.626
.657
.715
40.76
39.56
38.70
38.11
37.66
37.23
36.89

10
.535
.562
.591
.627
.682
.698
.728
40.76
38.85
37.95
36.72
35.89
35.38
35.25

95
.529
.540
.555
.600
.629
.642
.702
53.77
53.55
53.46
53.32
53.19
52.53
51.69

2
.532
.552
.603
.637
.677
.702
.711
7.86
5.19
4.60
4.38
3.63
3.22
2.86

3.9*
.532
.601
.622
.654
.724
.745
.772
7.86
6.04
4.58
4.36
3.49
3.08
2.78

5
.532
.601
.642
.692
.734
.776
.799
7.86
6.04
4.84
4.77
3.47
3.07
2.75

2
.545
.556
.579
.584
.587
.593
.593
34.26
34.09
32.83
31.84
31.11
30.80
30.74

5
.575
.589
.592
.594
.596
.596
.596
33.48
32.96
31.90
30.78
30.70
30.68
30.66

10
.593
.603
.604
.612
.621
.622
.628
32.43
31.27
30.70
30.60
30.30
29.93
29.81

20
.557
.588
.617
.680
.690
.700
.737
41.06
38.87
36.45
35.40
35.32
35.23
33.68

30
.559
.593
.632
.678
.705
.715
.738
41.55
39.96
37.01
35.84
34.39
34.14
33.11

40
.571
.595
.651
.670
.712
.720
.740
41.91
39.81
37.68
36.98
35.62
34.25
33.43

40.1*
.570
.595
.651
.670
.712
.720
.736
41.91
39.81
37.68
36.98
35.62
35.21
33.69

50
.570
.600
.642
.671
.707
.713
.736
41.91
41.19
39.29
37.79
36.47
36.08
34.61

60
.563
.617
.619
.672
.700
.711
.730
45.40
41.08
41.49
38.37
37.62
37.35
36.69

70
.536
.603
.617
.675
.690
.699
.736
49.73
43.25
42.84
40.47
40.07
39.91
38.36

80
.556
.597
.615
.644
.679
.700
.722
48.59
46.42
46.32
45.47
44.11
43.55
41.68

90
.529
.562
.583
.615
.664
.661
.718
52.77
52.73
51.34
50.68
49.80
49.46
47.23

10
20
30
40
50
60
70
80
90
.558 .637 .699 .724 .775 .765 .769 .745 .747
.639 .704 .726 .798 .804 .828 .833 .830 .799
.645 .758 .798 .826 .841 .860 .861 .871 .854
.743 .793 .845 .865 .878 .893 .899 .904 .900
.790 .868 .893 .912 .916 .921 .926 .933 .927
.841 .890 .908 .917 .930 .935 .941 .948 .939
.865 .891 .911 .938 .937 .944 .951 .954 .952
8.81 11.11 12.58 12.31 15.72 19.66 22.55 32.06 42.38
7.38 8.05 9.23 10.48 14.44 16.40 20.84 27.38 40.64
5.22 6.76 8.19 10.03 12.32 13.67 16.74 24.00 35.44
5.25 6.12 6.87 7.90 9.66 12.21 14.33 18.69 30.22
3.97 4.27 5.32 6.08 7.03 9.02 10.33 15.65 22.76
3.05 3.60 4.04 5.23 5.99 7.31 9.86 12.93 20.60
2.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.10
20
.607
.613
.639
.652
.675
.675
.678
32.30
30.41
29.63
29.61
28.96
28.73
28.67

30
.620
.629
.651
.672
.688
.688
.688
31.97
30.57
29.71
29.25
28.73
28.56
28.56

35.6*
.621
.636
.657
.673
.692
.699
.700
32.44
30.91
30.02
29.34
28.60
28.44
28.40

40
.624
.643
.657
.677
.697
.703
.712
32.84
30.97
30.30
29.64
29.03
28.50
28.45

50
.619
.651
.665
.686
.703
.710
.713
33.48
31.82
30.66
29.62
29.33
28.77
28.71

60
.618
.648
.665
.686
.704
.710
.715
34.89
32.12
31.34
30.40
29.32
28.99
28.91

70
.609
.634
.659
.680
.690
.699
.700
36.05
33.61
32.05
30.86
30.10
29.95
29.78

80
.600
.622
.630
.650
.670
.677
.678
38.04
35.55
32.44
31.33
31.32
31.17
31.02

90
.580
.594
.603
.603
.603
.604
.604
38.31
38.19
35.11
33.02
32.80
32.75
32.67

95
.724
.780
.824
.876
.910
.927
.940
48.52
47.61
45.09
43.12
35.93
29.62
26.14
95
.532
.551
.553
.554
.556
.558
.558
43.65
37.86
37.73
35.09
34.46
34.18
33.87

Table B1b: The Effect of Training-Set Size and Class Distribution on Classifier Performance

347

fiWeiss & Provost

The benefit of selecting the class distribution of the training data is demonstrated using several examples. Table B1a highlights six cases (by using a line to connect pairs of data points)
where competitive or improved performance is achieved from fewer training examples. In each
of these six cases, the data point corresponding to the smaller data-set size performs as well or
better than the data point that corresponds to the larger data-set size (the latter being either the
natural distribution or a balanced one).

Appendix C: Detailed Results for the Budget-Sensitive Sampling Algorithm
This appendix describes the execution of the progressive sampling algorithm that was described
in Table 7. The execution of the algorithm is evaluated using the detailed results from Appendix
B. First, in Table C1, a detailed iteration-by-iteration description of the sampling algorithm is
presented as it is applied to the phone data set using error rate to measure classifier performance.
Table C2 then provides a more compact version of this description, by reporting only the key
variables as they change value from iteration to iteration. Finally, in Table C3a and Table C3b,
this compact description is used to describe the execution of the sampling algorithm for the
phone, adult, covertype, kr-vs-kp, weather and blackjack data sets, using both error rate and AUC
to measure performance. Note that for each of these tables, the column labeled budget refers
to the budget used, or cost incurredand that in no case is the budget exceeded, which means
that all examples requested during the execution of the algorithm are used in the final training
set, with the heuristically-determined class distribution (i.e., the algorithm is budget-efficient).
The results that are described in this appendix, consistent with the results presented in Section
7, are baVHGRQDJHRPHWULFIDFWRU RIDQGDYDOXHRIcmin of 1/32. The total budget available for procuring training examples is n. Based on these values, the value of K, which determines the number of iterations of the algorithm and is computed on line 2 of Table 7, is set to 5.
Note that the value of n is different for each data set and, given the methodology for altering the
class distribution specified in Section 4.1, if the training set size in Table 2 is S and the fraction
of minority-class examples is f, then n = Sf.
Below is the description of the sampling algorithm, as it is applied to the phone data set with
error rate as the performance measure:

j = 0 Training-set size = 1/32 n. Form 13 data sets, which will contain between 2% and 95%
minority-class examples. This requires .0297n (95% of 1/32 n) minority-class examples
and .0306n (100%-2% = 98% of 1/32 n) majority-class examples. Induce and then
evaluate the resulting classifiers. Based on the results in Table 7, the natural distribution, which contains 18.2% minority-class examples, performs best. Total Budget:
.0603n (.0297n minority, .0306n majority).
j=1

Training-set size = 1/16 n. Form data sets corresponding to the best-performing class
distribution form the previous iteration (18.2% minority) and the adjoining class distributions used in the beam search, which contain 10% and 20% minority-class examples.
This requires .0250n (20% of 1/16 n) minority-class examples and .0563n (90% of 1/16
n) majority-class examples. Since .0297n minority-class examples were previously obtained, class distributions containing 30% and 40% minority-class examples can also be
348

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

formed without requesting additional examples. This iteration requires .0257n additional
majority-class examples. The best-performing distribution contains 10% minority-class
examples. Total Budget: .0860 n (.0297n minority, .0563n majority).
j=2

Training-set size = 1/8 n. Since the 10% distribution performed best, the beam search
evaluates the 5%, 10%, and 18.2% minority-class distributions. The 20% class distribution is also evaluated since this requires only .0250n of the .0297n previously obtained
minority-class examples. A total of .1188n (95% of 1/8 n) majority-class examples are
required. The best performing distribution contains 10% minority-class examples. This
iteration requires .0625n additional majority-class examples. Total Budget: .1485n
(.0297n minority, .1188n majority).

j=3

Training-set size = 1/4 n. The distributions to be evaluated are 5%, 10%, and 18.2%.
There are no extra minority-class examples available to evaluate additional class distributions. This iteration requires .0455n (18.2% of 1/4 n) minority-class examples and
.2375n (95% of 1/4 n) majority-class examples. The best-performing class distribution
contains 10% minority-class examples. Total Budget: .2830n (.0455n minority, .2375n
majority)

j=4

Training-set size = 1/2 n. The 5%, 10%, and 18.2% class distributions are evaluated.
This iteration requires .0910n (18.2% of 1/2 n) minority-class examples and .4750n (95%
of 1/2 n) majority-class examples. The best-performing distribution contains 10% minority-class examples. Total Budget: .5660n (.0910n minority, .4750n majority).

j=5

Training-set size = n. For this last iteration only the best class distribution from the previous iteration is evaluated. Thus, a data set of size n is formed, containing .1n minorityclass examples and .9n majority-class examples. Thus .0090n additional minority-class
examples and .4250n additional majority-class examples are required. Since all the previously obtained examples are used, there is no waste and the budget is not exceeded.
Total Budget: 1.0n (.1000n minority, .9000n majority)

Table C1: A Detailed Example of the Sampling Algorithm (Phone Data Set using Error Rate)

j size
class-distr
0 1/32 n all
1 1/16 n 10, 18.2 , 20, 30, 40
2 1/8 n 5, 10 , 18.2, 20
3 1/4 n 5, 10 , 18.2
4 1/2 n 5, 10 , 18.2
1 n 10
5

Expressed as a fraction on n
best min-need maj-need minority majority budget
18.2%
.0297
.0306
.0297
.0306 .0603
10%
.0250
.0563
.0297
.0563 .0860
10%
.0250
.1188
.0297
.1188 .1485
10%
.0455
.2375
.0455
.2375 .2830
10%
.0910
.4750
.0910
.4750 .5660
.1000
.9000
.1000
.9000 1.0000

Table C2: Compact Description of the Results in Table B1a for the Phone Data Set

349

fiWeiss & Provost

Data set
Phone

Metric j
ER
0
1
2
3
4
5
Phone
AUC
0
1
2
3
4
5
Adult
ER
0
1
2
3
4
5
Adult
AUC
0
1
2
3
4
5
Covertype ER
0
1
2
3
4
5
Covertype AUC
0
1
2
3
4
5
Kr-vs-kp ER
0
1
2
3
4
5
Kr-vs-kp AUC
0
1
2
3
4
5

size
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n

class-distr
all
10, 18.2 , 20, 30, 40
5, 10 , 18.2, 20
5, 10 , 18.2
5, 10 , 18.2
10
all
18.2, 20 , 30, 40
20, 30 , 40
20, 3 0, 40
18.2, 20 , 30
18.2
all
10, 20 , 23.9, 30, 40
10, 20 , 23.9
10, 20 , 23.9
5, 10 , 20
20
all
60, 70, 80 , 90
60, 70 , 80
60, 70 , 80
70, 80 , 90
80
all
2, 5 , 10, 20, 30, 40
2, 5 , 10, 20
2, 5 , 10
2, 5 , 10
5
all
14.8, 20 , 30, 40
20, 30 , 40
30, 40 , 50
20, 30 , 40
20
all
47.8, 50, 60
47.8, 50, 60
40, 47.8 , 50
40, 47.8 , 50
50
all
50, 60, 70
47.8, 50, 60
47.8, 50, 60
47.8, 50, 60
50

Expressed as a fraction of n
best min-need maj-need minority majority budget
18.2
.0297
.0306
.0297
.0306 .0603
10
.0250
.0563
.0297
.0563 .0860
10
.0250
.1188
.0297
.1188 .1485
10
.0455
.2375
.0455
.2375 .2830
10
.0910
.4750
.0910
.4750 .5660
.1000
.9000
.1000
.9000
1.0
20
.0297
.0306
.0297
.0306 .0603
30
.0250
.0511
.0297
.0511 .0808
30
.0500
.1000
.0500
.1000 .1500
20
.1000
.2000
.1000
.2000 .3000
18.2
.1500
.4090
.1500
.4090 .5590
.1820
.8180
.1820
.8180
1.0
20
.0297
.0306
.0297
.0306 .0603
20
.0250
.0563
.0297
.0563 .0860
20
.0299
.1125
.0299
.1125 .1424
10
.0598
.2250
.0598
.2250 .2848
20
.1000
.4750
.1000
.4750 .5750
.2000
.8000
.2000
.8000
1.0
80
.0297
.0306
.0297
.0306 .0603
70
.0563
.0250
.0563
.0306 .0869
70
.1000
.0500
.1000
.0500 .1500
80
.2000
.1000
.2000
.1000 .3000
80
.4500
.1500
.4500
.1500 .6000
.8000
.2000
.8000
.2000
1.0
5
.0297
.0306
.0297
.0306 .0603
5
.0250
.0613
.0297
.0613 .0910
5
.0250
.1225
.0297
.1225 .1522
5
.0250
.2450
.0297
.2450 .2747
5
.0500
.4900
.0500
.4900 .5400
.0500
.9500
.0500
.9500
1.0
20
.0297
.0306
.0297
.0306 .0603
30
.0250
.0533
.0297
.0533 .0830
40
.0500
.1000
.0500
.1000 .1500
30
.1250
.1750
.1250
.1750 .3000
20
.2000
.4000
.2000
.4000 .6000
.2000
.8000
.2000
.8000
1.0
50
.0297
.0306
.0297
.0306 .0603
50
.0375
.0327
.0375
.0327 .0702
47.8
.0750
.0653
.0750
.0653 .1403
47.8
.1250
.1500
.1250
.1500 .2750
50
.2500
.3000
.2500
.3000 .5500
.5000
.5000
.5000
.5000
1.0
60
.0297
.0306
.0297
.0306 .0603
50
.0438
.0313
.0438
.0313 .0751
50
.0750
.0653
.0750
.0653 .1403
50
.1500
.1305
.1500
.1305 .2805
50
.3000
.2610
.3000
.2610 .5610
.5000
.5000
.5000
.5000
1.0

Table C3a: Summary Results for the Sampling Algorithm (phone, adult, covertype, kr-vs-kp)

350

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

Data set
Weather

Metric j size
ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Weather AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Letter-a
ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Letter-a
AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Blackjack ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Blackjack AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n

class-distr
all
2,5 ,10,20,30,40, 40.1
5, 10, 20
10, 20 , 30
10, 20 , 30
30
all
30, 40 , 50
40, 50 , 60
30, 40, 50
40, 50 , 60
40
all
2, 3.9 , 5, 10, 20, 30, 40
2 , 3.9, 5, 10, 20
2, 3.9 , 5, 10
2, 3.9, 5
5
all
40, 50 , 60
50, 60 , 70
60, 70 , 80
70, 80 , 90
all
20, 30, 35.6, 40
10, 20, 30
10, 20, 30
20, 30, 35.6
35.6
all
20, 35.6, 40, 50
40, 50, 60
40, 50, 60
40, 50, 60
60

Expressed as a fraction of n
best min-need maj-need minority majority budget
5
.0297
.0306
.0297
.0316 .0613
10
.0250
.0613
.0297
.0613 .0910
20
.0250
.1188
.0297
.1188 .1485
20
.0750
.2250
.0750
.2250 .3000
30
.1500
.4500
.1500
.4500 .6000
.3000
.7000
.3000
.7000
1.0
40
.0297
.0306
.0297
.0316 .0613
50
.0313
.0438
.0313
.0438 .0751
40
.0750
.0750
.0750
.0750 .1500
50
.1250
.1750
.1250
.1750 .3000
40
.3000
.3000
.3000
.3000 .6000
.4000
.6000
.4000
.6000
1.0
3.9
.0297
.0306
.0297
.0306 .0603
2
.0250
.0613
.0297
.0613 .0910
3.9
.0250
.1125
.0297
.1225 .1522
3.9
.0250
.2450
.0297
.2450 .2747
5
.0250
.4900
.0250
.4900 .5150
.0500
.9500
.0500
.9500
1.0
50
.0297
.0306
.0297
.0306 .0603
60
.0375
.0375
.0375
.0375 .0750
70
.0875
.0625
.0875
.0625 .1500
80
.2000
.1000
.2000
.1000 .3000
80
.4500
.1500
.4500
.1500 .6000
.8000
.2000
.8000
.2000
1.0
30
.0297
.0306
.0297
.0316 .0613
20
.0250
.0500
.0297
.0500 .0797
20
.0375
.1125
.0375
.1125 .1500
30
.0750
.2250
.0750
.2250 .3000
35.6
.1780
.4000
.1780
.4000 .5780
.3560
.6440
.3560
.6440
1.0
40
.0297
.0306
.0297
.0316 .0613
50
.0313
.0403
.0313
.0403 .0716
50
.0750
.0750
.0750
.0750 .1500
50
.1500
.1500
.1500
.1500 .3000
60
.3000
.3000
.3000
.3000 .6000
.6000
.4000
.6000
.4000
1.0

Table C3b: Summary Results for the Sampling Algorithm (weather, letter-a, blackjack)

References
Bauer, E., & Kohavi, R. (1999). An empirical comparison of voting classification algorithms:
bagging, boosting, and variants. Machine Learning, 36, 105-139.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and Regression Trees. Belmont, CA: Wadsworth International Group.
Blake, C., & Merz, C. (1998).
UCI Repository of Machine Learning Databases,
(http://www.ics.uci.edu/~mlearn/MLRepository.html), Department of Computer Science,
University of California.
Bradley, A. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30(7), 1145-1159.
351

fiWeiss & Provost

Bradford, J.P., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. E. (1998). Pruning decision trees
with misclassification costs. In Proceedings of the European Conference on Machine
Learning, pp. 131-136.
Catlett, J. (1991). Megainduction: machine learning on very large databases. Ph.D. thesis, Department of Computer Science, University of Sydney.
Chan, P., & Stolfo, S. (1998). Toward scalable learning with non-uniform class and cost distributions: a case study in credit card fraud detection. In Proceedings of the Fourth International
Conference on Knowledge Discovery and Data Mining, pp. 164-168, Menlo Park, CA:
AAAI Press.
Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, W. P. (2000). SMOTE: synthetic minority
over-sampling technique. In International Conference on Knowledge Based Computer Systems.
Cohen, W., & Singer, Y. (1999). A simple, fast, and effective rule learner. In Proceedings of the
Sixteenth National Conference on Artificial Intelligence, pp. 335-342, Menlo Park, CA:
AAAI Press.
Cohn, D., Atlas, L. and Ladner, R. (1994). Improved generalization with active learning. Machine Learning, 15:201-221.
Drummond, C., & Holte, R.C. (2000). Exploiting the cost (in)sensitivity of decision tree splitting
criteria. In Proceedings of the Seventeenth International Conference on Machine Learning,
pp. 239-246.
Elkan, C. (2001). The foundations of cost-sensitive learning. In Proceedings of the Seventeenth
International Joint Conference on Artificial Intelligence, pp. 973-978.
Estabrooks, A., & Japkowicz, N. (2001). A Mixture-of-Experts Framework for ConceptLearning from Imbalanced Data Sets. In Proceedings of the 2001 Intelligent Data Analysis
Conference.
Fawcett, T. & Provost, F. (1997). Adaptive Fraud Detection. Data Mining and Knowledge Discovery 1(3): 291-316.
Good, I. J. (1965). The Estimation of Probabilities. Cambridge, MA: M.I.T. Press.
Hand, D. J. (1997). Construction and Assessment of Classification Rules. Chichester, UK: John
Wiley and Sons.
Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning and the problem of small
disjuncts. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp. 813-818. San Mateo, CA: Morgan Kaufmann.
Japkowicz, N. & Stephen, S. (2002). The Class Imbalance Problem: A Systematic Study. Intelligent Data Analysis Journal, 6(5).
Japkowicz, N., Holte, R. C., Ling, C. X., & Matwin S. (Eds.) (2000). In Papers from the AAAI
Workshop on Learning from Imbalanced Data Sets. Tech, rep. WS-00-05, Menlo Park, CA:
AAAI Press.

352

fiLearning When Training Data are Costly: The Effect of Class Distribution on Tree Induction

Jensen, D. D., & Cohen, P. R. (2000). Multiple comparisons in induction algorithms. Machine
Learning, 38(3): 309-338.
John, G. H., & Langley, P. (1996). Static versus dynamic sampling for data mining. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pp. 367-370. Menlo Park, CA. AAAI Press.
Kubat, M., & Matwin, S. (1997). Addressing the curse of imbalanced training sets: one-sided
selection. In Proceedings of the Fourteenth International Conference on Machine Learning, pp. 179-186.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International Conference on Machine Learning, pp.148-156.
Provost, F., Fawcett, T., & Kohavi, R. (1998). The case against accuracy estimation for comparing classifiers. In Proceedings of the Fifteenth International Conference on Machine
Learning. San Francisco, CA: Morgan Kaufmann.
Provost, F., Jensen, D., & Oates, T. (1999). Efficient progressive sampling. In Proceedings of
the Fifth International Conference on Knowledge Discovery and Data Mining. ACM Press.
Provost, F., & Fawcett, T (2001). Robust classification for imprecise environments. Machine
Learning, 42, 203-231.
Provost, F., & Domingos, P. (2001). Well-trained PETs: improving probability estimation trees.
CeDER Working Paper #IS-00-04, Stern School of Business, New York University, New
York, NY.
Quinlan, J.R. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.
Saar-Tsechansky, M., & Provost, F. (2001). Active learning for class probability estimation and
ranking. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, Seattle, WA.
Saar-Tsechansky, M. and F. Provost (2003). Active Sampling for Class Probability Estimation
and Ranking. To appear in Machine Learning.
SAS Institute (2001). Getting Started With SAS Enterprise Miner. Cary, NC: SAS Institute Inc.
Saerens, M., Latinne, P., & Decaestecker, C. (2002). Adjusting the outputs of a classifier to new
a priori probabilities: a simple procedure. Neural Computation, 14:21-41.
Swets, J., Dawes, R., & Monahan, J. (2000). Better decisions through science. Scientific American, October 2000: 82-87.
Turney P. (2000). Types of cost in inductive learning. In Workshop on Cost-Sensitive Learning
at the Seventeenth International Conference on Machine Learning, 15-21, Stanford, CA.
Van den Bosch A., Weijters, A., Van den Herik, H.J., & Daelemans, W. (1997). When small
disjuncts abound, try lazy learning: a case study. In Proceedings of the Seventh BelgianDutch Conference on Machine Learning, 109-118.

353

fiWeiss & Provost

Weiss, G.M., & Hirsh, H. (2000). A quantitative study of small disjuncts, In Proceedings of the
Seventeenth National Conference on Artificial Intelligence, 665-670. Menlo Park, CA:
AAAI Press.
Weiss, G. M., & Provost, F (2001). The effect of class distribution on classifier learning: an
empirical study. Tech rep. ML-TR-44, Department of Computer Science, Rutgers University, New Jersey.
Zadrozny, B., & Elkan, C. (2001). Learning and making decisions when costs and probabilities
are both unknown. Tech. rep. CS2001-0664, Department of Computer Science and Engineering, University of California, San Diego.

354

fiJournal of Artificial Intelligence Research 19 (2003) 513-567

Submitted 1/03; published 11/03

Decentralized Supply Chain Formation: A Market Protocol and
Competitive Equilibrium Analysis
William E. Walsh

WWALSH 1@ US . IBM . COM

IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532 USA

Michael P. Wellman

WELLMAN @ UMICH . EDU

University of Michigan AI Laboratory
1101 Beal Avenue
Ann Arbor, MI 48109-2110 USA

Abstract
Supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel, multiagent production activity. We present a simple model of
supply chains, highlighting two characteristic features: hierarchical subtask decomposition, and
resource contention. To decentralize the formation process, we introduce a market price system
over the resources produced along the chain. In a competitive equilibrium for this system, agents
choose locally optimal allocations with respect to prices, and outcomes are optimal overall. To determine prices, we define a market protocol based on distributed, progressive auctions, and myopic,
non-strategic agent bidding policies. In the presence of resource contention, this protocol produces
better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature. The protocol often converges to high-value supply chains, and when competitive
equilibria exist, typically to approximate competitive equilibria. However, complementarities in
agent production technologies can cause the protocol to wastefully allocate inputs to agents that do
not produce their outputs. A subsequent decommitment phase recovers a significant fraction of the
lost surplus.

1. Introduction
Electronic commerce technology can provide significant improvements in existing modes of commercial interaction, through increased speed, convenience, quality, and reduced costs. Yet some
have proposed more radical visions of how business may be transformed. Exponential increases
in communications bandwidth and computational ability have the potential to qualitatively decrease
the friction in business interactions. With this as a premise, Malone and Laubauchers treatise on the
emerging E-Lance Economy (1998) puts forth the view that, in the not-too-distant future, business
relationships will lose much of their current persistent character. Indeed, Malone and Laubaucher
propose that large companies as we know them will cease to exist, and rather be dynamically formed
by electronically connected freelancers (e-lancers) for the purpose of producing particular goods
and services, and then dissolved when projects are completed. Others employ the evocative term
virtual corporation (Davidow, 1992) to describe groups of agile organizations forming temporary
confederations for ad hoc purposes.
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWALSH & W ELLMAN

Whether or not one accepts the full extent of this vision of virtual corporations, several business
trends provide evidence that we are moving in this direction. Software companies are time-shifting
development between the U.S. and India, and Sun Microsystems now allows freelance programmers to bid to fix customers software problems (Borenstein & Saloner, 2001). Large, traditional
manufacturing companies, exemplified by major automotive manufacturers, increasingly outsource
the production of various components. Ford and General Motors (GM) have spun off parts manufacturing into separate companies (Lucking-Reily & Spulber, 2001). Start-ups and other small
companies form partnerships to compete with larger, more established companies. Application
service providers supplant in-house provision of standard operations, information, and technology
services.
We study this phenomenon in the guise of supply chains, a common form of coordinated commercial interaction. For our purposes, a supply chain is a network of production and exchange
relationships that spans multiple levels of production or task decomposition. Whenever we have a
producer that buys inputs and sells outputs, we have a supply chain. Although typically used to refer
to multi-business structures in manufacturing industries, any service or contracting relationship that
spans multiple levels can be viewed as a supply chain.
Supply chain formation is the process of determining the participants in the supply chain, who
will exchange what with whom, and the terms of the exchanges. Traditionally, supply chains have
been formed and maintained over long periods of time by means of extensive human interactions.
But the acceleration of commercial decision making is creating a need for more advanced support.
Companies ranging from auto makers to computer manufacturers are basing their business models
on rapid development, build-to-order, and customized products to satisfy ever-changing consumer
demand. And fluctuations in resource costs and availability mean that companies must respond
rapidly to maintain production capabilities and profits. As these changes increasingly occur at
speeds, scales, and complexity unmanageable by humans, the need for automated supply chain
formation becomes acute.
Because the agents are autonomous in an electronic commerce setting, we must generally assume that they have specialized knowledge about their own capabilities but limited knowledge about
other individuals and the large-scale structure of the problem. Because agents are self-interested,
they will participate with the goal of maximizing their own benefit. Additionally, we may have
cause to control the allocation of each resource individually if, for instance, global optimization is
infeasible or if no one entity has global allocative authority. For such environments where information, decision making, and control are inherently decentralized, we seek to engineer the process
of bottom-up supply chain formation. This problem is complicated if the structure of resource
contention precludes the use of simple greedy allocation strategies.
We present a decentralized, asynchronous market protocol for supply chain formation under
conditions of resource scarcity. The protocol allows agents to negotiate the formation of supply
chains in a bottom-up fashion, requiring only local knowledge and communication. In the market
protocol, agents decisions are coordinated by the price system, with the price for each resource
determined through an ascending auction.
The remainder of the paper describes our market protocol, and characterizes its behavior theoretically and empirically.1 We begin in Section 2 with a formal definition of the supply chain
formation problem, and an illustrating application to the automotive industry. In Section 3, we
1. Further details may be found in the first authors dissertation (Walsh, 2001).

514

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

show how typical greedy top-down approaches to supply chain formation can fail in the presence
of resource contention. We define a price system and analyze static properties of price equilibria in
Section 4. In Section 5, we introduce a price-based market protocol for supply chain formation and
analyze its convergence properties. We present the results of an empirical study of the protocol in
Section 6. In Section 7, we discuss relevant results and issues in price-based analysis and auction
theory, as well as some related work in supply chain formation. We conclude in Section 8 and
suggest extensions and future work. Throughout, we defer proofs to Appendix A.

2. The Supply Chain Formation Problem
Agents in the supply chain are characterized in terms of their capabilities to perform tasks, and their
interests in having tasks accomplished. A central feature of our model of the problem is hierarchical
task decomposition: in order to perform a particular task, an agent may need to achieve some
subtasks, which may be delegated to other agents. These may in turn have subtasks that may be
delegated, forming a supply chain through a decomposition of task achievement. Constraints on the
task assignment arise from resource contention, where agents require a common resource (e.g., a
task achievement, or something tangible such as a piece of equipment) to accomplish their tasks.
Tasks are performed on behalf of particular agents; if two agents need a task then it would have
to be performed twice to satisfy them both. In this way, tasks are the same as any other discrete, rival
resource. Hence, we make no distinction in our model, and use the term good to refer to any task
or resource provided or needed by agents. The assumption that goods cannot be shared or reused
(i.e., have limited available quantities) is necessary for much of our analysis. Goods that can be
replicated at little or no marginal cost, such as software and information, provide many interesting
challenges to economic analysis (Shapiro & Varian, 1999), not addressed in this work.
2.1 Example: Automotive Supply Chain Formation
We illustrate our model of supply chain formation with an application to a stylized, hypothetical
example from the automotive industry. Traditionally, automotive supply chains span many tiers,
formed and maintained over long periods of time through extensive human negotiations. Some
automation is emerging, for example through Covisint2 , a company formed by GM, Ford, and
DaimlerChrysler to mediate the negotiation and exchange of parts, as well as other supply chain
interactions. Currently the focus in such efforts is on a particular exchange relationship within a
single level of production. We consider the broader problem of assembling combinations of relationships across multiple levels to form complete, feasible supply chains.
In the example presented in Figure 1, Ford and GM need to acquire contracts for transmissions
in order to produce particular models of cars. Ford can produce the transmissions in its own factories or acquire them from an independent transmission producer. GM currently does not have the
capacity to produce the desired transmissions, and must outsource. The independent transmission
producer has capacity to provide transmissions to either Ford or GM, but not both. Ford and the
independent factory both require the services of a job shop for metal-working tasks, but the job shop
does not have capacity to serve them simultaneously. Contracts with the job shop and with the two
transmission factories are the scarce goods to be allocated.
2. http://www.covisint.com

515

fiWALSH & W ELLMAN

Ford Auto
Assembly

Ford
Distribution

Job Shop

Independent
Transmission
Factory

Ford
Transmission
Factory

GM Auto
Assembly

GM
Distribution

Figure 1: An automotive supply chain formation problem.
The limited capacity of the job shop entails certain constraints on feasible supply chains. Ford
cannot acquire transmissions from the independent factory, because the job shop cannot serve the
independent factory and Ford simultaneously. Additionally, Ford and GM cannot simultaneously
be satisfied.
2.2 Problem Specification
We provide a formal description of the supply chain formation problem in terms of bipartite graphs.
The two types of nodes represent goods and agents, respectively. A task dependency network is a
directed, acyclic graph, (V, E), with vertices V = G  A, where:
G = the set of goods,
A = C  , the set of agents,
C = the set of consumers,
 = the set of producers,
and a set of edges E connecting agents with goods they can use or produce. There exists an edge
hg, ai from g  G to a  A when agent a can make use of one unit of g, and an edge ha, gi when
a can provide one unit of g. If an agent requires multiple units of a good as input, then we treat
each unit as a separate edge, distinguishing them by subscripts. (Edges without explicit subscripts
are interpreted as implicitly subscripted by 1.) For instance, if agent a requires two units of g as
input, then its input edges are hg, ai1 and hg, ai2 .
The various agent types are characterized by their position in the task dependency network.
Each consumer, c  C, wishes to acquire one unit of one good from its set of consumable goods,
Gc  G, where hg, ci  E iff g  Gc .
A producer can produce a single unit of an output good conditional on acquiring some input
goods. With each producer    we associate:
1. an input set, I  G, such that g  I iff there are edges hg, ik  E for one or more k, and
516

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Worked
Metal

Ford Transmission
Subcontractor

Ford
Cars

Ford
Distribution
$25,000

Ford
Transmissions

$20
Job Shop
$20

Ford Auto
Assembler
$20,000

Ford
Transmission
Factory
$100

Independent
Transmission
Factory
$50
GM Transmission
Subcontractor
$60

GM
Trasmissions

GM Auto
Assembler
$25,000

GM
Cars

GM
Distribution
$30,000

Figure 2: Network auto: A task dependency network for the automotive supply chain depicted in
Figure 1.

2. a single output, g  G \ I , such that h, g i  E.
A producers input goods are complementary in that the agent must acquire all of them in order
to produce its output; it cannot accomplish anything with only a partial set. Alternate producers
with the same output indicate different ways that a good can be produced.
Task dependency networks are constrained to be acyclic, that is, no agent produces goods that
could be used to assemble its inputs through any chain of production. Although we might broadly
view all global commerce as one large cycle of production and consumption, in practice, negotiations tend to be clustered within more limited scopes of concern, often referred to as industries.
The resulting supply chains are typically acyclic.
Figure 2 shows an example task dependency network for the automotive supply chain problem
of Figure 1. Here the goods are indicated by circles, and agents by boxes. Producers with inputs
are represented by curved boxes. The numbers under agent boxes represent production costs and
consumption values, explained below. An arrow from an agent to a good indicates that the agent can
provide that good, and an arrow from a good to an agent indicates that the agent can make use of
the good. For instance, the producer labeled Ford Auto Assembly requires Worked Metal and Ford
Transmissions in order to produce cars. Since the transmissions produced by the Ford Transmission
Factory can be used only by Ford, we need to distinguish Ford and GM transmissions as separate
goods. This in turn requires that we introduce Ford and GM Transmission Subcontractor producers
to model the fact that the Independent Transmission Factory can be used to produce either type.
An allocation is a subgraph (V 0 , E 0 )  (V, E). For a  A and g  G, an edge ha, gi  E 0 means
that agent a provides g, and hg, ai  E 0 means a acquires g. An allocations vertices are the agents
and goods incident on its edges:
1. An agent is in an allocation graph iff it acquires or provides a good:
For a  A, we have a  V 0 iff hg, ai  E 0 or ha, gi  E 0 .
517

fiWALSH & W ELLMAN

Worked
Metal

Ford Transmission
Subcontractor

Ford
Cars

Ford
Distribution
$25,000

Ford
Transmissions

$20
Job Shop
$20

Ford Auto
Assembler
$20,000

Ford
Transmission
Factory
$100

Independent
Transmission
Factory
$50
GM Transmission
Subcontractor
$60

GM
Trasmissions

GM Auto
Assembler
$25,000

GM
Cars

GM
Distribution
$30,000

Figure 3: A solution to Network auto.
2. A good is in an allocation graph iff it is acquired or provided:
For g  G, we have g  V 0 iff hg, ai  E 0 or ha, gi  E 0 .
A producer  is active iff it provides its output. A producer is feasible iff it is inactive or
acquires all its inputs. Consumers are always feasible.
Good g is in material balance in (V 0 , E 0 ) iff the number of edges in equals the number out:
fi
fi
fi fi
fi{(a, k) | ha, gik  E 0 }fi = fi{(a, k) | hg, aik  E 0 }fi .

An allocation is feasible iff all agents are feasible and all goods are in material balance. A
solution is a feasible allocation that forms a partial ordering of feasible production, culminating in
consumption. That is, some consumer acquires a good it desires:
There exists a hg, ci  E 0 such that c  C V 0 .

A solution may involve multiple consumers. If consumer c is in a solution (V 0 , E 0 ) then we say that
(V 0 , E 0 ) is a solution for c.
Figure 3 shows a solution allocation for the task dependency network of Figure 2. Shaded
agents and solid arrows are part of the solution, with unshaded agents and dashed arrows indicating
elements of the problem not part of the solution. Note that the Ford Auto Assembler wins an input,
but is inactive. However, recall that inactive producers are feasible, hence the solution properties
are met. We refer to the configuration of an inactive producer acquiring an input in an allocation as
a dead end.
Each producer  has some production cost  for providing a unit of its output. The cost might
represent the value  could obtain from engaging in some other activity (i.e., its opportunity cost),
or some direct cost incurred in producing its output (but not including input costs). Since a producer
provides at most one unit of one good, the total production cost to , with output g, for allocation
E 0 , is  if h, gi  E 0 and 0 otherwise.
We assume that a consumer has preferences over different possible goods, but wishes to obtain
only a single unit of one good. Thus, a consumer c obtains value vc (g) for obtaining a single unit
518

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

of good g, and, for allocation E 0 , obtains value vc ((V 0 , E 0 ))  maxhg,ciE 0 vc (g). In depicting task
dependency networks, we display costs and values below the corresponding agent boxes.
Definition 1 (value of an allocation) The value of allocation (V 0 , E 0 ) is:
value((V 0 , E 0 )) 

 vc ((V 0, E 0))    ((V 0, E 0)).


cC

Definition 2 (efficient allocations) The set of efficient allocations contains all feasible allocations
(V  , E  ) such that:
value((V  , E  )) =

max

(value((V 0 , E 0 )) | (V 0 , E 0 ) is feasible).

(V 0 ,E 0 )(V,E)

Task dependency networks describe the supply chain formation problem from a global perspective. In a decentralized approach to formation, we would generally not assume that an agent, or any
other entity, has perfect or complete knowledge of the entire network. We generally do assume that
all agents have perfect knowledge of their own costs, values, and goods of interest. When mediators
facilitate the negotiations for goods (as in protocols described below), each agent knows of relevant
mediators for its goods of interest. This knowledge includes all rules enforced by the mediators.
Likewise, mediators know of the existence of all agents interested in their respective goods. Beyond
that, a mediator knows only what the agents reveal through communication during negotiation. A
mediator does not know the agents true costs or valuations, nor is it aware of agents preferences
for goods outside of its direct scope of facilitation. We do not address in detail how agents and
mediators achieve mutual awareness (i.e., how connections originate), but assume that it can be
accomplished via some unspecified search, notification, or broadcast protocol.

3. Resource Contention
One natural candidate approach to supply chain formation is the CONTRACT NET protocol (Davis
& Smith, 1983), the most widely studied algorithm for forming task performance relations among
distributed agents. C ONTRACT NET does indeed apply to our framework, as it employs local negotiation to achieve a hierarchical task decomposition. Although definitive characterization is difficult
due to the many variants on CONTRACT NET in the literature (Baker, 1996; Davis & Smith, 1983;
Dellarocas et al., 2000; Sandholm, 1993), it is fair to say that, generally, request for quotes proceed top down from the root task (right-to-left from consumers, in our network terminology), and
contracting proceeds bottom-up (left-to-right towards consumers), selecting at each level among
candidate bids received. (Variants of the protocol are primarily distinguished by the form of bids
and selection criteria employed.) As a consequence, choices are made greedily, without reflecting
ramifications upstream in the evolving chain.
This approach can form satisficing supply chains when there are sufficient resources to support the greedy selection. However, the basic CONTRACT NET protocol does not explicitly address
resource scarcity or contention among multiple agents. Producers accept bids on inputs before it
can be established whether this might cause infeasibility further upstream. Without lookahead or
backtracking, CONTRACT NET might construct infeasible supply chains when there are limited resources.
For instance, a greedy protocol would not produce a solution for the network shown in Figure 4.
Here, if all producers bid according to a common function monotone in cost, the output bid of
519

fiWALSH & W ELLMAN

a1
5

1

a2

2

a5
0
5

1
a3

3

1
a4

a7
0

6

cons
15

a6
0

4

1

Figure 4: Network greedy-bad: A network for which greedy protocols can produce infeasible
allocations.

producer a6 would be preferred to that of a5, because a6 can acquire its inputs cheaper. But since
a7 must acquire the one available unit of good 4 to feasibly participate in a solution, a6 cannot be a
part of the solution.
The issue of resource contention motivates our adoption of a market-based approach. The key
idea is that prices can signal resource value and scarcity up and down the chain, enabling local
decision making while avoiding the pitfalls of greedy one-pass selection or communication of global
structure information.

4. Price Systems
A price system p assigns to each good g, a nonnegative number p(g) as its price. Prices are anonymous (i.e., not agent dependent) and linear in the quantity of goods. Intuitively, prices indicate the
relative value of the goods, and agents use the prices to guide their local decision making.
We assume agents have quasilinear utility functions, defined by money holdings plus the
value (or minus cost) associated with the allocation of goods. Agents wish to maximize their surplus
with respect to prevailing prices.
Definition 3 (surplus) The surplus, (a, (V 0 , E 0 ), p), of agent a with allocation (V 0 , E 0 ) at prices
p, is given by:
 va ((V 0 , E 0 ))  hg,aiE 0 p(g), if a  C
 ha,giE 0 p(g)  hg,aiE 0 p(g)   ((V 0 , E 0 )), if a  .
4.1 Price Equilibrium
Generally, an allocation (V 0 , E 0 ) is a competitive equilibrium at prices p if (V 0 , E 0 ) is feasible and
assigns to each agent an allocation that optimizes the agents surplus at p. For our model, this means
specifically:
520

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

 A producers optimal choice is to be either active and feasible, or to acquire no goods. Hence,
a producer in the allocation obtains nonnegative surplus by being active, and a producer not
in the allocation would obtain nonpositive surplus by being active.
   V 0 ,



p(g) 

   \V 0 ,



p(g) 



p(g)    0



p(g)    0

hg,iE

h,giE

h,giE

hg,iE

 Because a consumer receives value for obtaining at most one good, a consumers optimal
choice is to obtain the good that gives it maximum nonnegative surplus, and to obtain no other
goods at a positive price. Furthermore, a consumer not in the allocation (i.e., not obtaining
any goods) would obtain nonpositive surplus from any good.
c  C V 0 , hg, ci  E 0 , g = arg max
vc (g0 )  p(g0 )
0
g G

 vc (g)  p(g)  0
 hg0 , Ei, g0 6= g, p(g0 ) = 0
c  C \V 0 , g  G,
vc (g)  p(g)  0
Figure 5 shows an example of a competitive equilibrium for Network greedy-bad. The prices
are shown under their respective goods.
a1
5

1

a2

2

1

1

a3

3

1

0

a4

4

1

6

5

a5
0
5
7

a7
0

6

cons

14

15

a6
0

Figure 5: A competitive equilibrium for Network greedy-bad.

A competitive equilibrium allocation is stable in the sense that no agent would want a different
allocation at the equilibrium prices. Moreover, from equilibrium there is no way to reallocate the resources (including money transfers) so that some agent has greater surplus, without degrading some
other agents surplus. This absence of further gains from trade is referred to as Pareto optimality.
Given quasilinear utility, price equilibria have been shown to be efficient under fairly general conditions (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999; Ygge, 1998). This also holds for
the particular case of task dependency networks, as stated in Corollary 4.
521

fiWALSH & W ELLMAN

p(1) >
_5
a1
5
a2
1
a3
1
a4
1

1
p(2) >
_1
2
p(3) <
_1
3
p(4) >
_4

a5
0

9_
> p(6) >
_ 10

p(5) >
_6
5

a7
0

6

cons
9

a6
0

4

Figure 6: Network greedy-bad with costs and values that do not support competitive equilibrium.

4.2 Existence of Competitive Equilibrium
Not all task dependency networks have competitive equilibria. Consider again Network greedybad but with vcons = 9, as shown in Figure 6. The allocation shown is the only efficient allocation,
hence any equilibrium must support it. Recall that in equilibrium, active agents must obtain nonnegative surplus, and inactive producers must not be able to obtain positive surplus. The price inequalities under the goods follow from constraints on the surplus associated with agent activity. The lower
bounds on the prices of goods 1, 2, and 5 ensure that producers a1, a2, and a5, respectively receiv5e
nonnegative surplus. The upper bound on 3 ensures that a3 could not obtain positive surplus. The
lower bound on 4 ensures that a6 would receive nonpositive surplus. Propagating these bounds to
6, we see that p(6)  10 to give a7 positive surplus, but also that p(6)  9 to give cons nonnegative
surplus. Since this is impossible, a competitive equilibrium cannot exist.
Technically, non-existence of equilibrium is due to complementarity of inputs for producers with
discrete-quantity goods. In fact, complementarities are necessary to preclude competitive equilibrium in task dependency networks. A network has no input complementarities when all producers
have at most one input.
Theorem 1 Competitive equilibria exist for any network with no input complementarities.
We defer the proof of this and subsequent theorems to Appendix A.
Consider again Figure 6. The multiple undirected paths between 1 and 4 give rise to the lower
bound on the price of good 6. It turns out that these undirected cycles are also necessary to preclude
competitive equilibrium.
A polytree is a graph in which there is at most one undirected path from any vertex to another.
Recall that in task dependency networks, if a producer uses multiple units of a good, then each unit
is represented by a separate edge. It follows that an allocation is a polytree iff no more than one unit
of a good is used to produce another given good, or used in multiple ways to produce a good.
Theorem 2 Competitive equilibria exist for any polytree.
522

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

4.3 Approximate Price Equilibrium
We should generally expect that market protocols based on discrete price adjustments (such as the
SAMP-SB protocol we describe in Section 5) would overshoot exact equilibria by at least a small
amount. Therefore, our analysis emphasizes approximate equilibrium concepts (Demange et al.,
1986; Wellman et al., 2001a). We introduce a particular type of approximation, --competitive
equilibrium, defined in terms of parameters that bound the degree to which agents acquire suboptimal surplus. Intuitively, b bounds the suboptimality of a consumers surplus, s bounds the
suboptimality of a producers surplus attributable to its output, and g bounds the suboptimality of
a producer s surplus attributable to input g. As described in Section 5, these parameters also have
special interpretation in our market protocol as applied to task dependency networks.
Denote as Ha (p) the maximum surplus that agent a can obtain in (V, E), at prices p, subject to
feasibility. That is,
Ha (p) 
max (a, (V 0 , E 0 ), p)
(V 0 ,E 0 )(V,E)

such that a is feasible at (V 0 , E 0 ).
Definition 4 (--competitive equilibrium) Given the parameters:
 b , s  0,
  for all    and all g  G,
g

an allocation (V 0 , E 0 ) is in --competitive equilibrium at prices p iff:
1. For all a  A, (a, (V 0 , E 0 ), p)  0.
2. For all c  C, (c, (V 0 , E 0 ), p)  Hc (p)  b .
3. For all   , (, (V 0 , E 0 ), p)  H (p)  (hg,iE g + s ), and  is feasible at (V 0 , E 0 ).
4. All goods are in material balance.
Consider Network greedy-bad with the same prices shown in Figure 5 except that p(5) = 8.
This does not constitute an exact competitive equilibrium because a6, though inactive, could make
a positive profit. However, if 2a6 + 3a6 + 4a6 + s  1, then since Ha6 (p) = 1, a6 obeys Condition 3
and the allocation is a --competitive equilibrium at the specified prices.
Theorem 3 If (V 0 , E 0 ) is a --competitive equilibrium for (V, E) at some prices p, then (V 0 , E 0 ) is
a feasible allocation with a nonnegative value that differs from the value of an efficient allocation
by at most  [hg,iE g + s ] + |C|b .
A --competitive equilibrium corresponds to the standard notion of competitive equilibrium
g
when b = s = 0, and  = 0 for all  and g.
Corollary 4 (to Theorem 3) A competitive equilibrium allocation is efficient.
As noted in Section 4.1, this is consistent with previously established results.

523

fiWALSH & W ELLMAN

4.4 Valid Solutions
In the following sections we show that --competitive equilibria can be a useful concept for analyzing decentralized market protocols. However, such protocols do not always reach --competitive
equilibria for all networks. Hence we also consider weaker constraints on prices, consistent with a
lesser degree of agent optimization in a solution allocation.
We say that a solution (V 0 , E 0 ) is valid with respect to prices p if:
1. Each consumer in the solution pays no more than its value for a single good. That is, for all
c  C V 0 , there exists a single hg, ci  E 0 such that
p(g)  vc (g),
and p(g0 ) = 0 for all g0 6= g such that hg0 , ci  E 0 .
2. None of the active producers are unprofitable. For all    V 0 where h, g i  E 0 we have
(, (V 0 , E 0 ), p)  0. Note that solution validity does not preclude an inactive producer from
being unprofitable (i.e., it admits dead ends).
Note that (1) effectively states that consumers do not obtain negative utility, which is weaker than
the competitive equilibrium conditions in that it does not require consumers to receive their optimal
allocation. Similarly, (2) does not require producers to optimize, as in competitive equilibrium, but
only requires nonnegative utility for active producers.
a1
5

1

a2

2

1

2

a3

3

1

1

a4

4

1

5

5

a5
0
5
8

a7
0

6

cons

13

15

a6
0

Figure 7: A valid solution for Network greedy-bad.
Figure 7 shows an example valid solution, with the same underlying costs and values as in
Figure 5. Because it allows dead ends, validity does not directly provide useful bounds on the
inefficiency of an allocation.

5. SAMP-SB Protocol
The preceding section introduces some static properties of price configurations and allocations.
Here we address the problem of how prices might be obtained. To compute prices and allocations,
we must elicit information bearing on the relative value of goods, through some systematic communication process. Mechanisms that determine market-based exchanges based on messages from
agents are called auctions (McAfee & McMillan, 1987).
524

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

The agents bidding policies represent their strategies for interacting with the auctions. Whereas
the auction mechanism may be designed by a central authority, bidding policies are generally determined by individual agents. To understand the implications of the auction design requires an
analysis of the market protocol that arises from the combination the auction mechanism and the
agent bidding policies.
The space of potential auctions is expansive (Wurman et al., 2001), and definitive theoretical
results are currently known only for fairly limited classes of problems (Bikhchandani & Mamer,
1997; Demange & Gale, 1985; Gul & Stacchetti, 2000; Klemperer, 1999; McAfee & McMillan,
1987). Complementarities with discrete goods, which can cause nonexistence of price equilibria,
also greatly complicate auction design and analysis of auctions (Milgrom, 2000).
For our supply-chain domain, we have investigated a particular protocol, called SAMP-SB (Simultaneous Ascending (M+1)st Price with Simple Bidding). As demonstrated below, SAMP-SB
can produce good allocations which, in some cases, are consistent with competitive price equilibrium theory.
5.1 Auction Mechanism
The SAMP-SB mechanism comprises a set of auctions, one for each good. Auctions run simultaneously, asynchronously, and independently, without direct coordination. Agents interact with the
auctions by submitting bids for goods they wish to buy or sell. A bid is of the form: ((q1 p1 ) . . . (qn
pn )). Each pair (qi pi ) indicates an offer to buy or sell the good, with qi indicating the quantity of the
offer and pi indicating the price. If qi > 0, it is an offer to buy qi units of the good for no more than
pi per unit, and we refer to it as a buy offer. If qi < 0, it is an offer to sell qi units for no less than
pi per unit, and we refer to it as a sell offer. Because no agent both buys and sells the same good
in a task dependency network, a bid contains either all positive or all negative quantity offers. Bids
possess what is sometimes called additive-OR semanticsthe offers are treated exactly as if they
came from separate bids, hence the auction can match any of the individual offers independently.
Without loss of generality, we henceforth impose the restriction |qi | = 1 for all offers in all bids,
continuing to allow that agents may submit multiple offers in a bid.
When an auction receives a new bid, it sends each of its bidders a price quote specifying the
price that would result if the auction ended in the current bid state. Price quotes are not issued
until all initial bids are received, but are subsequently issued immediately on receipt of new bids.
Because some offers may be tied at the current price, this information alone is not sufficient for an
agent to tell whether it is winning an offer placed at that price. To clarify this ambiguity, the price
quote also reports to each bidder the quantity it would buy or sell in the current state. The same
prices are sent to all bidders, but the reported winning state is specific to the recipient. Agents may
then choose to revise their bids in response to the notifications (if an agent does not wish to change
its bid, inaction leaves its previous bid standing in the auction).
We assume that communication is reliable but asynchronous.3 That is, all messages sent eventually reach their recipients, although we impose no bound on the delays. Agents and auctions use
message IDs to ensure that they handle messages in the appropriate order. Note that even if all auctions and agents have deterministic behaviors, an overall run of SAMP-SB may be nondeterministic
due to this asynchrony.
3. Technically, we adopt the model of asynchronous reliable message passing systems (Fagin et al., 1995).

525

fiWALSH & W ELLMAN

Under asynchrony, it is helpful for the auction to send the ID of the most recent bid received
from the agent with its price quote. An agent responds only to a price quote that reflects its most
recent bid sent. Without this device, an agent can have difficulty establishing feasibility, as its
understanding of its input and output bid states may be based on nonuniformly delayed reports.
Bidding continues until quiescence, a state where all messages have been received, no agent
chooses to revise its bids, and no auction changes its prices, ask prices, or allocation. At this point,
the auctions clear; each bidder is notified of the final prices and how many units it transacts in each
good. Note that a quiescent system is not necessarily in a solution state or (approximate) equilibrium
state.
Although detecting quiescence is straightforward in a centralized system, in a decentralized,
asynchronous system we need to perform the operation using only local message passing. In previous work (Wellman & Walsh, 2000), we described a protocol for detecting quiescence in general
distributed negotiations, based on a well-known termination-detection algorithm.
Each auction runs according to (M+1)st-price rules (Satterthwaite & Williams, 1989, 1993;
Wurman et al., 1998). The (M+1)st price auction is a variant of the (second-price) Vickrey auction (Vickrey, 1961), generalized to allow for the exchange of multiple units of a good. Given a set
of offers including M units offered for sale, the (M+1)st-price auction sets a price equal to the price
of the (M+1)st highest offer over all of the offers. The price can be said to separate the winners
from the losers, in that the winners include all sell offers strictly below the price and all buy offers
strictly above the price. Some agents that offer at the (M+1)st price also win; in case of ties, offers
submitted earlier have precedence. Winning buy and sell offers are matched one-to-one, and pay
(or get paid) the (M+1)st price.
When issuing price quotes, the auction reports both the price (i.e., the current going price, or
(M+1)st price), p(g) and the ask price, (g) of the good g. The ask price specifies the amount above
which a buyer would have to offer in order to buy the good, given the current set of offers. The ask
price is determined by the price of the Mth highest of all offers in the auction, hence (g)  p(g).
For instance, if we have buy bids 12, 10, and 6 and sell bids 15, 11, and 8, p(g) = 10, (g) = 11,
and if the auction is in quiescence, the buy bids 12 and 10 would match the sell bids 15 and 11 and
trade at p(g) = 10.
Because a producer has complementary inputs, ensuring feasibility is a challenging problem,
requiring careful design. The auctions run simultaneously, and each auction requires that the prices
of an agents successive buy offers increase by no less than some (generally small) positive number
b and the prices of successive sell offers increase by no less than s .4 An auction can enforce the
ascending rule by simply rejecting an agents offer if the price does not increase by b or s . By
constraining the direction of price changes, this design gives producers a more accurate indication of
the relative prices for inputs and outputs than if prices were allowed to fluctuate in both directions.
The ascending bid restriction ensures ascending auction prices, with one technicality. Due to
asynchrony and immediate issuance of price quotes, if the initial bid from an agent arrives after a
higher bid, the price quote could decrease. This can be handled simply at the auction by issuing no
price quotes until some specified period of time after the auction opens. After the first price quote
is issued, the auction accepts new bids only from agents that had previously placed bids.
It is common in auction literature and practice to place an ascending restriction on buy-offer
prices. It may seem counterintuitiveand is in fact atypicalto place the same restriction on
4. These rules differ from those of a more typical simultaneous ascending auction (Demange et al., 1986; Milgrom,
2000), which specify that agents must submit offer prices that are at least an increment above the current price.

526

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

sell-offer prices. However, such an ascending offer price restriction ensures that price quotes rise
monotonically as the auctions progress. Section 5.4 shows how an ascendingoffer-price restriction for both buy and sell offers serves a key role in establishing the relationships between system
quiescence and solution convergence of the system.
5.2 Bidding Policies
Although designers of negotiation mechanisms do not generally have control over the agents behaviors, any conclusions about the outcome of a mechanism must be based on some assumptions
about these behaviors. A typical assumption in economics is that agents are rational in some sense,
for example that they play policies that form a Bayes-Nash equilibrium. However, as discussed in
Section 7.1, the complexity of supply chain formation markets is beyond the current state-of-the-art
in analyzing Bayes-Nash equilibria with simultaneous ascending auctions. Instead, our analysis assumes that the agents follow a simple, non-strategic bidding policy, described in this section. Other
variations may be reasonable, or perhaps better in some respects than the policies we describe.
Rather than explore the range of possibilities, we chose in this work to investigate a particular set of
policies in depth. Our chosen policies obey the ascending offer restriction enforced by the auction,
respect the locality of information in that they require no knowledge of other agents in the system,
and are myopic in that they use only information provided by the current price quotes, without
forecasting future prices.
Recall that a consumer wishes to acquire a single good that maximizes its surplus at the given
prices. We assume that a consumer initially offers zero for each good of interest. So long as it is
winning a good, it does not change its offer. Whenever it is not winning a good, it offers p(g ) + b
for good g = arg maxgG (vc (g)  p(g)  b ) if vc (g )  p(g )  b  0, otherwise it stops bidding.
A producers objective is much more complex, namely to maximize the difference between the
price it receives for its output and the total price it pays for its inputs, while remaining feasible. We
assume that a producer initially offers zero for each of its input goods, and gradually increases these
offers to ensure feasibility. It raises its offer price for an input good by b if and only if the price
quotes indicate that it is losing that good but winning its output.
We assume that producer  bids for its output good g in an effort to recover its production cost
and the perceived costs of its inputs. The producer places its first output offer only after receiving
the first price quotes for all its inputs, and subsequently updates its output offer whenever it receives
a new price quote on any input. For simplicity, consider the case in which  has one offer (each at
quantity one) for each input. If  is currently winning an input g, its perceived cost, p (g) of g is
simply p(g). When  is not currently winning g with a particular offer, p (g) = max((g), p(g) +
b ). If  is the price of the previous offer made by  for g , then when its perceived costs increase,
 offers max( + s , hg,iE p (g)) for its output g . If  has multiple offers for a good g, then
it assumes a separate perceived cost with respect to each offer, and bids for its output accordingly.
Figure 8 shows how a producer would bid next as a function of the current prices and its current
offers, when b = 1 and s  2.
Note that throughout the negotiation, a producer places bids for its output goods before it has
received commitments on its input goods. Producers counteract potential risk by continually updating their bids based on price changes and feasibility status. A producer reduces exposure to dead
ends by incrementing its offer prices on inputs by minimal amounts and only when necessary.
527

fiWALSH & W ELLMAN

Current offer price for A = 2
A
p(A) = 1
(A) = 2

C
3

B
p(B) = 2
(B) = 4

Good

Next Offer
Price

A
B
C

hold at 2
2
5

Current offer price for C = 3

Current offer price for B = 1

Figure 8: A producers next offers, according to SAMP-SB, when b = 1 and s  2. The dashed
arrow from good B indicates that the producer is currently losing B. The solid arrows
indicate that the producer is currently winning goods A and C.

5.3 Bidding with General Preferences
The task dependency network model represents fairly simple production capabilities and consumer
utility. Here we discuss some natural potential extensions of the bidding policies to a broader class
of capabilities and preferences.
A producer capable of variable-unit production could bid exactly as if it were multiple identical
producers. Such a producer would maintain separate offers in its bids for each unit, and update
the separate offers independently. Similarly, a consumer with additive value for multiple goods, or
multiple units of a good, could bid for each unit of each good as if it were a separate consumer.
A producer with alternatives on some input, independent of other inputs, can switch its bidding
to the currently cheapest option. Subtle issues can arise for a producer that has alternative input sets,
particularly when it is tentatively winning parts of the sets. One option would be to focus bidding on
the set with the lowest perceived cost, which may include a premium for goods not in the tentatively
winning set. Alternatively, the producer could assume that it will definitely win its tentatively won
goods and effectively treat them as sunk costs. Fractional accounting of sunk costs may also be
reasonable. Similar considerations arise for extensions presenting complex consumption choices.
5.4 Properties of SAMP-SB
In this section we describe a number of theoretical properties of SAMP-SB. In Section 5.4.1 we
describe properties relating to convergence to quiesence, in Section 5.4.2 we present properties relating to efficiency and convergence to price equilibrium, and in Section 5.4.3 we present properties
relating to solution convergence.
5.4.1 C ONVERGENCE

TO

Q UIESCENCE

The SAMP-SB auctions and bidding policies guarantee that the system will always reach quiescence.
Theorem 5 SAMP-SB reaches quiescence after a finite number of bids have been placed.
However, convergence can take a long time.
Observation 6 In an asynchronous environment, it is possible that a run of the protocol may require
a number of bids that is exponential in the network size, and not a function of the consumer value.
528

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

1-1

2-1

1-A

0
0

1-3

start
2
0

1-B

3-A

0

1

2-3

0
1-2

3-1

2-A

0
2

3-3

0
2-2

2-B

0

0
3-2

3
consumer
1

3-B

0

Figure 9: Network exponential: A network that may require an exponential number of bids to
reach quiescence.

Figure 9 shows Network exponential, which illustrates this observation. The agent named
start places a one-time bid to sell one unit of good 0 for $2. Since (0) = 2, and producers 1-1 and
1-2 are initially losing their input bids, these agents each offer a price of 2 for their output goods.
Producer 1-3 will receive the new price quotes for goods 1-A and 1-B asynchronously, hence may
update its bid for good 1 twice, offering a price of 2 the first time and a price of 4 the second
time. Continuing with this process, we see that producer 3-3 updates its bid for good 3 up to eight
times. If we extend this network and maintain labeling consistent with Figure 9, then producer n-3
would place O(2n ) bids for good n. Note however, that if bids and price quotes are propagated
synchronously, the exponential growth would not occur.
In the example above, most of the bids are actually superfluous in that they do not meaningfully
affect the outcome of the protocol. This appears often true of situations exhibiting the worst-case
behavior described. To capture the distinction between relevant and irrelevant bidding, we introduce
the notion of quasi-quiescence, a persistent state from which all subsequent bids effectively do not
matter for solution convergence. SAMP-SB convergence to quasi-quiescence requires a number of
meaningful bids that can be bounded by the size of the network and the value of the maximum
consumer value.
Definition 5 (quasi-quiescent) A run of SAMP-SB is in a quasi-quiescent state when, for any consumer or active producer , all bids by  have been received and  would not change its bids in
response to any price quotes already received or transmitted by auctions.
Clearly, the requirements of quasi-quiescence are subset of the requirements for quiescence.
Observation 7 A quiescent state is a quasi-quiescent state.
Theorem 8 If a run of SAMP-SB reaches a quasi-quiescent state, then it remains in a quasiquiescent state. Furthermore, neither the allocation nor the prices p subsequently change.
This theorem means that, once quasi-quiescence is reached, all subsequent bids effectively do
not matter in terms of equilibrium and solution convergence.
Corollary 9 (to Theorem 8) The quiescent state of SAMP-SB is a --equilibrium or valid solution
iff the first quasi-quiescent state reached is a --equilibrium or valid solution, respectively.
The following theorem establishes a bound on the number of relevant bids necessary to reach
quasi-quiescence.
529

fiWALSH & W ELLMAN

Theorem 10 SAMP-SB reaches a quasi-quiescent state after a number of bids bounded by a polynomial of the size of the network and the value of the maximum consumer value have been placed
by consumers and active producers.
Our previously mentioned quiescence-detection protocol (Wellman & Walsh, 2000) can also detect
quasi-quiescence, and thus terminate negotiations when it is reached.
5.4.2 E FFICIENCY

AND

C ONVERGENCE

TO

P RICE E QUILIBRIUM

We intentionally use b and s , to parametrize both SAMP-SB and our concept of --competitive
equilibrium. With an interpretation of g in terms of prices and ask prices, we can specify necessary and sufficient conditions for which the result of SAMP-SB corresponds to a --competitive
equilibrium.
Theorem 11 The prices and allocation determined in quiescence by the SAMP-SB protocol is a
--competitive equilibrium, with g = max((g)  p(g), b ), iff no inactive producer buys any
positive-price input.
From Theorems 3 and 11, we can establish bounds on the inefficiency of a --competitive equilibrium, parametrized by g = max((g)  p(g), b ) for each good. In some cases, the difference
between (g) and p(g) may be quite high. However, we can actually establish a tighter bound.
Theorem 12 If (V 0 , E 0 ) is a --competitive equilibrium computed by SAMP-SB, then (V 0 , E 0 ) has
a nonnegative value that differs from the value of an efficient allocation by at most  (|{hg, i 
E}| b + s ) + |C|b .
Note that the theorem replaces  from Theorem 3 with b in the bound.
A network is a tree if it is a polytree with no more than one consumer.
g

Theorem 13 The quiescent state of SAMP-SB is a --competitive equilibrium for a tree.
We are unaware of other general network structures for which SAMP-SB is guaranteed to converge to a --competitive equilibrium. However, Theorem 11 implies that we can improve allocations if we modify SAMP-SB to avoid dead ends. We say that a bidding policy is safe for a producer
if the producer cannot obtain a negative surplus in quiescence. It is clear that if a protocol is safe for
all producers, then it will converge to --competitive equilibrium.
In SAMP-SB we have assumed that a producer updates buy and sell offers simultaneously in
response to price quotes. This policy is not safe, even for single-input producers, because the
producer bids for its input based on the state of its standing offer for its output, rather than the offer
it is about to place. The producer would get negative surplus if it does not win its new output offer
but gets stuck winning its new input offer. However, a slight variant of the bidding policy, which we
call safe SAMP-SB, is safe for any single-input producer. With this protocol, a producer updates
its input bids only when it would not update, and it currently winning, its most recent output offer.
Clearly, safe SAMP-SB has the same static properties as SAMP-SB, hence Theorem 12 applies to
safe SAMP-SB.
Theorem 14 The quiescent state of safe SAMP-SB is a --competitive equilibrium for a network
with no input complementarities.
530

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Safe SAMP-SB is not guaranteed to be safe for producers with multiple inputs in arbitrary
networks, nor do we know of any safe producer bidding policy that ensures safety for producers in
any arbitrary network (other than degenerate policies such as not bidding).
Safe SAMP-SB may take longer to reach quiescence than regular SAMP-SB. With safe SAMP-SB,
a producer must always wait for notification of the results of pending output offers before increasing input offers. For a producer to win an output offer may require propagations of many messages
through various paths in the network before buyers of the output good would increase their buy offer
prices for that good. The resulting delay would be greater than the local delay in communicating
with the output good auction.
That non---competitive equilibrium runs of SAMP-SB result in dead ends suggests a potential
source of significant efficiency loss. For example, Figure 7 shows the result of a run of SAMP-SB
on Network greedy-bad. This valid solution has a dead end at producer a6. Since producer a3
incurs its cost of $1 to provide good 3 to a6, but does not contribute to any value in the system, this
dead end is pure waste from a global efficiency perspective. The allocation is undesirable directly
for producer a6 because it is committed to pay $1 for an input it cannot use. With large networks or
costs, dead ends can result in significant efficiency losses and negative profits to individual agents.
We propose a contract decommitment protocol to remove dead ends after SAMP-SB reaches
quiescence. According to the decommitment protocol, each inactive producer can decommit from
its contracts for its inputs for which it would pay a positive price. The protocol is applied recursively
to the producers that lose their outputs due to decommitment. When the decommitment process
terminates, agents exchange goods as specified by the remaining contracts. We refer to SAMP-SB
with decommitment as SAMP-SB-D.
In Figure 7, producer a6 would decommit from its contract with a3. Clearly, Theorem 11 implies
that no agent decommits iff SAMP-SB produced a --competitive equilibrium. Moreover, if we
remove from consideration all producers that decommit, the remaining agents are in --competitive
equilibrium.
Decommitment has the benefit that, whereas some producers can lose money in the SAMP-SB
protocol, no agent receives a negative surplus from participating in SAMP-SB-D. However, this
is achieved by making the auction allocations non-binding, which is undesirable to the producers
who lose their output sales to decommitments. It also begs the question of how to enforce the
requirement that inactive producers be the only agents that decommit.
In addition to dead ends, efficiency can also be lost if SAMP-SB fails to find a solution when
a positive value solution exists, or if SAMP-SB forms a solution with value inferior to an efficient solution (dead ends are not necessarily mutually exclusive of these two cases). In Section 6
we describe an experimental analysis of the efficiency, the source of inefficiency, and equilibrium
attainment of SAMP-SB in a set of networks.
5.4.3 S OLUTION C ONVERGENCE
Recall that SAMP-SB always converges to a valid solution (specifically a --competitive equilibrium) for networks with tree structures, and the safe variant converges for networks with no input
complementarities. The following theorem shows that, with sufficiently high consumer value, regular SAMP-SB can always converge to a (possibly non-equilibrium) valid solution for polytrees.

531

fiWALSH & W ELLMAN

Theorem 15 If (V, E) is a polytree with a solution that assigns good g to consumer c, then given
all other costs and values, there exists a value vc (g) such that SAMP-SB is guaranteed to converge
to a valid solution (V 0 , E 0 ) for c.
Because dead ends may result, we cannot usefully bound the inefficiency of the solution reached by
SAMP-SB in a polytree.
For general network structures, the prices of all sell offers for all consumers goods could rise
above their values, in which case the system will necessarily reach quasi-quiescence in a nonsolution state. If, however, quasi-quiescence is reached before the price of some consumers good
reaches its value for the good, we have a valid solution.
Theorem 16 If SAMP-SB reaches quasi-quiescence with p(g) < vc (g) for some hg, ci  E, c  C,
then the systems state represents a valid solution.
The next theorem establishes conditions under which a valid solution state will immediately
lead to quasi-quiescence.
Theorem 17 If a run of SAMP-SB in (V, E) is in a valid solution state such that:
 each consumer c is either winning an offer or p(g) + b > vc (g) for all hg, ci  E,
 all agents have correct beliefs about which goods they are currently winning,
 all bids from consumers and active producers have been received in response to the current
price quotes,
 and no sell offers are lost due to tie breaking,
then after the subsequent price quote from each auction, the system will be in a quasi-quiescent
state with a valid solution.
Although SAMP-SB is not guaranteed to converge to a solution, the fact that the problem of finding a solution is NP-Complete (Walsh et al., 2003) should lead us to expect that there are problems
for which SAMP-SB would converge to a solution only after an exponential number of meaningful
bids. Since the number of meaningful bids is bounded by a polynomial of the maximum consumer
value, we should further expect that there exist networks for which SAMP-SB can converge to a
solution only with a exponential consumer values. In practice we find that we can construct problems for which the consumer value must be exponential in order for SAMP-SB to converge to a
solution (Walsh et al., 2003). However, we have run many simulations for which the required value
is much more reasonable (Walsh et al., 2003).
For some networks, costs, and values, SAMP-SB cannot converge to a valid solution with some
values of b and s , no matter how high the consumer value. One example (the simplest we have
been able to construct) is Network no-converge, shown in Figure 10. Observe that a solution
must include agent a8, but cannot include a7. Agent a6 always offers a price of at least p(2) + 20
for good 4, hence a8 cannot win two units of good 4 for less than p(2) + 20 each. Thus agent a8 will
always offer a price of at least 2p(2) + 40 for good 5. Since agent a7 will never offer a price more
than 2p(2) + 22a7 for good 5, agent a8 could only win good 5 if 2a7  20. But, for this to occur,
we must have b  20. A more thorough analysis, taking into account the dynamics of SAMP-SB,
shows we must have b  40 and s = 0 to obtain a valid solution in quiescence, and then only for
certain patterns of asynchrony.
532

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

a2
a1

1

0

2

a7

20

0

a3

5

0
a6
3

cons

a8

4

0

0

a4

a5

20

20

Figure 10: Network no-converge: A network for which SAMP-SB cannot converge to a solution
for certain values of b and s .

6. Empirical Performance of SAMP-SB
Whereas our analytic results provide some insight into SAMP-SB and its variants, they do not support a comprehensive characterization of performance, except for certain special-case network structures. In order to gain further understanding of the effectiveness of SAMP-SB and SAMP-SB-D, we
performed an empirical study based on protocol simulations on sample task dependency networks.
6.1 Setup
Our investigation focuses on a small set of networks exhibiting a variety of structural properties:
simple (Figure 11), unbalanced (Figure 12), two-cons (Figure 13), bigger (Figure 14), and
many-cons (Figure 15). We also also studied Network greedy-bad (Figure 4).
a1

1

a3

3

c1
1.216

a2

2

a4

Figure 11: Network simple.
We ran experiments on multiple instances of each network. For each instance we randomly
chose producer costs uniformly from [0, 1], but for each consumer in a network, we calculated
a fixed value so that, excluding all other consumers, there exists a positive-surplus solution for
this consumer with 0.9 probability. We determined consumer values via simulation, assuming the
specified distributions of producer costs. We discarded all instances whose efficient solutions had
value zero. We set b = s = .01.
To test the effect of competitive equilibrium existence on the performance of the protocols,
we generated instances of unbalanced, two-cons, and greedy-bad with costs that admit
competitive equilibrium and with costs that do not. Because simple and many-cons are polytrees,
533

fiWALSH & W ELLMAN

1

a1

a8
8

a2

a9

2

a13
a3

3
9

a10
a4

4

a5

5

a6

6

a7

7

15

c1
3.73

a14

a11

a15
a12

10

Figure 12: Network unbalanced.

a3
a1

3

1

c1
1.23

a4
4

c2
2.17

a2

2
a5

Figure 13: Network two-cons.

a1

1

a17

a9

a2

2

a3

3

a10

a18

a11
10
a12

a4

4

a5

5

a13

a6

6

a14

a7

7

a15

a25
13

9

a19

a26
a27
14
a28

a20

17
11

a21

a29
15

a22

a30

a23

a31
16

12
a8

8

a16

a24

a32

Figure 14: Network bigger.

534

c1
3.51

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

a1

1

a2

2

a3

3

a4

4

a5

5

a6

6

a13

13
a19

19

c1
4.42

a14

14

a20

a15

15
20

a7

7

3.89
a16

a8

8

a9

9

a10

10

a11

11

a12

12

c2

16

a21
a17

17
a22

21

c3
4.44

a18

18

Figure 15: Network many-cons.
we know from Theorem 2 that all instances thereof have competitive equilibria. We were not able
to generate no-equilibrium instances of bigger with the given cost distributions.
To generate an instance with a desired type of cost structure (equilibrium or no-equilibrium)
we repeatedly chose sets of producer costs randomly from the uniform distribution until the desired
property was met. In the experiments, we determined whether competitive equilibrium existed
given complete information about the network structure, values, and costsusing the following
procedure. Recall that a competitive equilibrium is always efficient (Corollary 4). Hence, given
an optimal allocation (V  , E  ), we attempt to solve the system of linear equations that characterize
a competitive equilibrium, as described in Section 4.1. If a solution to the equations exists, the
resulting prices constitute a competitive equilibrium, otherwise no equilibrium exists. We used
CPLEX, a commercial mixed-integer-linear programming package, to find the efficient allocation
and to solve the corresponding equilibrium equations.
For each type of cost structure in each network, we tested 100 random instances, with the exception of simple, for which we tested 3220 instances.5 For each instance and each protocol, we measured the efficiencythe fraction of the efficient valueattained by SAMP-SB and SAMP-SB-D.
We also measured the percentage of available surplus (i.e., percentage of the value of an optimal
solutions) obtained by the producers.
6.2 Results
We classify the efficiency of a run of the protocols in one of four ways: Negative, Zero, Suboptimal
(but positive), and Optimal efficiency. Table 1 shows the distribution of the efficiency classes in our
experiments. Note that SAMP-SB-D cannot produce negative efficiency, by construction.
5. We tested more instances of simple as a part of a broader study (Walsh et al., 2000).

535

fiWALSH & W ELLMAN

Network
simple
unbalanced, case:
 equilibrium exists
 no equilibrium exists
two-cons, case:
 equilibrium exists
 no equilibrium exists
bigger
many-cons
greedy-bad, case:
 equilibrium exists
 no equilibrium exists

SAMP-SB
% of instances
Neg Zero Sub Opt
0.0
0.3
0.0 99.7

SAMP-SB-D
% of instances
Zero Sub
Opt
0.3
0.0
99.7

5.0
100.0

1.0
0.0

7.0
0.0

87.0
0.0

1.0
100.0

1.0
0.0

98.0
0.0

11.0
18.0
0.0
27.0

0.0
0.0
0.0
0.0

6.0
78.0
4.0
56.0

83.0
4.0
96.0
17.0

0.0
1.0
0.0
0.0

3.0
95.0
0.0
2.0

97.0
4.0
100.0
98.0

4.0
100.0

0.0
0.0

21.0
0.0

75.0
0.0

1.0
100.0

0.0
0.0

99.0
0.0

Table 1: Distribution of efficiency classes from SAMP-SB and SAMP-SB-D. Efficiency classes:
Negative (Neg), Zero, Suboptimal (Sub), and Optimal (Opt).

Recall (from Section 5.4.2) that efficiency loss in SAMP-SB can be attributable to any of three,
not necessarily exclusive, causes: dead ends, failure to form a solution when a positive-valued
solution exists, and finding a suboptimal solution. We can infer the percentage of instances exhibiting dead-end suboptimality in SAMP-SB by examining the differences between SAMP-SB-D and
SAMP-SB totaled over the Negative, Zero, and Suboptimal columns in Table 1. Decommitment
does not affect the contribution of no-solution and suboptimal-solution losses, but helps reveal them
by eliminating dead-end suboptimality. Hence, we can infer the percentage of instances exhibiting no-solution and suboptimal-solution suboptimality in SAMP-SB by examining the Zero and
Suboptimal columns of SAMP-SB-D, respectively.
Table 2 shows the average efficiency attained by the protocols, factored by network and equilibrium existence (where relevant). We see, from the difference between the SAMP-SB-D and
SAMP-SB columns, that dead ends are a significant source of inefficiency. Additionally, existence
of competitive equilibrium has a significant effect on the performance of the protocols. In these networks, SAMP-SB-D produces nearly perfect efficiency when competitive equilibrium exists (recall
that all studied instances of simple, bigger, and many-cons have equilibria), but is is much less
effective when equilibrium does not exist, in fact failing to find any solutions in the no-equilibrium
cases of unbalanced and greedy-bad.
To check whether these differences in performance are significant, we performed Students tTests for each protocol, comparing the mean efficiencies of instances that admit competitive equilibrium with the means of those instances that do not admit competitive equilibrium. Table 3 shows
the results, indicating the p-values that the means of equilibrium and no-equilibrium instances came
from the same underlying population. In typical analyses, the null hypothesis that the means are
equal is rejected if the p-value is below 0.05. With this threshold, it seems we can safely reject the
hypothesis that the mean efficiencies of equilibrium and non-equilibrium instances are the same for
536

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Network
simple
unbalanced
 equilibrium exists
 no equilibrium exists
two-cons, case:
 equilibrium exists
 no equilibrium exists
bigger
many-cons
greedy-bad, case:
 equilibrium exists:
 no equilibrium exists:

SAMP-SB
0.997

SAMP-SB-D
0.997

0.867
20.080

0.990
0.000

0.733
0.268
1.000
0.120

0.986
0.686
1.000
0.996

5.320
18.230

0.990
0.000

Table 2: Average efficiency in each network for the protocols.

Network
unbalanced
two-cons
greedy-bad

SAMP-SB

SAMP-SB-D

6.27  1030
5.15  107
1.41  101

8.23  10101
1.43  1022
8.04  10101

Table 3: P-values computed with the Students t-Test. The t-Test compared the means efficiencies
of instances that admit competitive equilibrium and those that do not admit competitive
equilibrium.

537

fiWALSH & W ELLMAN

Networks unbalanced and greedy-bad. Inspection of the data further supports this conclusion, as SAMP-SB-D essentially always produces zero efficiency, but produces perfect efficiency in
many of the instances that do admit competitive equilibrium.
On the face of it, the high SAMP-SB/greedy-bad p-value suggests that we cannot safely
reject the hypothesis that the mean efficiencies differ between the equilibrium and no-equilibrium
instances of the network. However, inspection of the data indicates that this high probability results from one outlying equilibrium instance with a large negative efficiency. Indeed, the fact that
SAMP-SB-D always produces essentially optimal results in instances that admit competitive equilibrium, but predominantly produces suboptimal results in the instances without such equilibria,
suggests that it is unlikely that the equilibrium and no-equilibrium means are the same for SAMP-SB
in Network greedy-bad.
Network
simple
unbalanced
 equilibrium exists
 no equilibrium exists
two-cons, case:
 equilibrium exists
 no equilibrium exists
bigger
many-cons
greedy-bad, case:
 equilibrium exists
 no equilibrium exists

% --Competitive
Equilibrium
100
88
0
83
2
96
17
75
0

Table 4: Percentage of instances in which SAMP-SB attained --competitive equilibrium.
Table 4 shows the percentage of instances for which SAMP-SB attained --competitive equilibrium in each network. It is straightforward to determine whether --competitive equilibrium
is attained by observing whether there are any dead ends (Theorem 11). Again, we see a strong
connection with the existence of competitive equilibrium. One notable exception is many-cons
(which always admits a competitive equilibrium), for which SAMP-SB frequently produced dead
ends. We do see that --competitive equilibria form in a small percentage of the no-equilibrium
two-cons instances, although this is not a prevalent phenomenon with the b and s parameters
we chose.
Table 5 shows the average efficiency, factored by --competitive equilibrium attainment
(SAMP-SB and SAMP-SB-D produce the same results when --competitive equilibrium is attained). We must be careful in drawing conclusions from these statistics because, for any given
network case, there were relatively few or many --competitive equilibrium instances (Table 4).
Still, we note certain salient trends. The --competitive equilibrium runs produce near perfect efficiency, with smaller degrees of inefficiency than specified by the bounds in Theorem 12. Because
an allocation produced by SAMP-SB is a --competitive equilibrium iff there are no dead ends, we
should expect that a significant portion of efficiency loss in non---competitive equilibrium pro538

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Network
simple
unbalanced
 equilibrium exists
 no equilibrium exists
two-cons, case:
 equilibrium exists
 no equilibrium exists
bigger
many-cons
greedy-bad, case:
 equilibrium exists
 no equilibrium exists

--Equilibrium
Not Found
SAMP-SB SAMP-SB-D
N/A
N/A

--Equilibrium
Found
SAMP-SB/ SAMP-SB-D
0.997

0.248
20.08

0.989
0.000

0.998
N/A

0.570
0.130
0.997
0.060

0.920
0.707
1.000
0.995

1.000
1.000
1.000
1.000

24.28
18.22

0.960
0.000

1.000
N/A

Table 5: Average efficiency in each network for the protocols, factored by --competitive equilibrium attainment.

ducing runs of SAMP-SB would be attributable to negative surplus incurred from dead ends. The
significant differences between the efficiency of SAMP-SB-D and SAMP-SB shown in the non--competitive equilibrium column provides evidence for this hypothesis. Indeed, it appears that
surplus lost to dead ends (as opposed to suboptimal solution attainment) is the dominant cause of
inefficiency when --competitive equilibrium is not attained. In all instances, improvement from
decommitment is greater than the difference between the efficiency of SAMP-SB-D and optimal
efficiency.
Table 6 shows the average fraction of available surplus obtained by producers, respectively, in
each network. Perhaps surprisingly, in some networks the producers can gain significant surplus
with the SAMP-SB-D protocol, even though they are bidding to obtain zero surplus. The reason
for this is that a producers output offer indicates the minimum amount it is willing to accept in
exchange for its output. But rising buy offers can cause the price to rise above the producers output
offer. This could happen in cases when it is necessary to block out certain agents to have a feasible
allocation in quiescence. Note however, that the decommitment step is needed for the producers to
obtain high average surplus. Without decommitment, the average producer surplus can be highly
negative, as shown in the SAMP-SB column.

7. Related Literature
In this section we discuss literature related to our present work. In Section 7.1 we discuss related
literature on price-based analysis and auction theory, and in Section 7.2 we discuss related literature
on supply chain formation.
539

fiWALSH & W ELLMAN

Network
simple
unbalanced, case:
 equilibrium exists
 no equilibrium exists
two-cons, case:
 equilibrium exists
 no equilibrium exists
bigger
many-cons
greedy-bad, case:
 equilibrium exists
 no equilibrium exists

SAMP-SB
0.000

SAMP-SB-D
0.000

0.041
20.09

0.082
0.000

0.210
0.137
0.001
0.517

0.464
0.555
0.001
0.359

6.08
18.11

0.137
0.000

Table 6: Average fraction of available surplus obtained by producers in each network for the protocols.

7.1 Price-Based Analysis and Auction Theory
We have shown some special cases for which competitive equilibria exist in task dependency networks (polytree and single-input-producer networks), that SAMP-SB always finds --competitive
equilibrium in trees, and that a minor variant always finds --competitive equilibria in single-inputproducer networks. A review of the results in price equilibrium and auction theory reveals that such
limited positive results are typical.
It is well-known that given arbitrarily divisible goods and convex utility, cost, and production functions, competitive equilibrium prices exist. If additionally, the gross substitutes condition
(which is a generalization of no-complementarities) is met, the classic tatonnement procedure finds
competitive equilibrium in a distributed manner.6
When goods are discrete, competitive equilibria exist in exchange (non-production) economies
if the gross substitutes conditions are met (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999;
Kelso & Crawford, 1982). Milgrom (2000) showed that the existence of a single complementarity
can be sufficient to preclude equilibrium in exchange economies. Bickhchandani and Mamer (1997)
also show existence under a variety of other conditions, which do not appear to have natural interpretations in task dependency networks. In exchange economies, the gross substitutes condition
also ensures convergence to (approximately) competitive equilibria with simultaneous ascending
auctions (Demange et al., 1986; Gul & Stacchetti, 2000).
That distributed price-based auction protocols can leave agents with undesired goods when their
preferences are complementary (e.g., dead ends in a task dependency network), is a widely recognized problem. An alternative approach is to use a combinatorial auction, which mediates negotiation at a single location, performing global matching of combinations of goods based on indivisible
bids. This general approach has received much attention in the AI community as of late, motivated
6. The reader may consult a standard microeconomic textbook (Mas-Colell et al., 1995) for details on these results.

540

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

in part by techniques for quickly performing the necessary global optimization (Andersson et al.,
2000; Leyton-Brown et al., 2000; Sandholm & Suri, 2000).
Currently, some results on combinatorial equilibria and auctions have been established for onesided (i.e., buyer only) bidding. Bikhchandani and Ostroy (2002) and Wurman and Wellman (2000),
using different combinatorial frameworks, provide positive results on equilibrium existence, and
properties thereof. Wurman and Wellman describe a combinatorial auction for their framework.
Parkes and Ungar (2000) describe a combinatorial auction that is guaranteed to converge to an
efficient allocation if agents follow myopic best-response strategies. By adding an extend-andadjust phase, these authors are able to obtain this allocation in ex post Nash equilibrium (Parkes
& Ungar, 2002). Ausubel and Milgrom (2002) present a proxy-auction mechanism that obtains
efficient allocations with straightforward bidding in equilibrium when goods are substitutes.
In the present work we consider only simple, local, myopic bidding policies. These policies
are non-strategic, in that agents do not reason about their effect on the negotiations in an attempt
to extract greater surplus. The assumption of non-strategic behavior is plausible when there are a
large number of agents. In networks with many agents bidding for individual goods, many parallel
branches, or many agents in sequence, the potential contribution of any one agent to the value of a
solution is relatively small and there is little to gain by strategic behavior.
Our experiments have shown that, even when producers bid to obtain zero surplus with the specified policy, they can obtain positive surplus in some networks. Nevertheless, in smaller networks,
the potential for strategic improvement is more pronounced, and our non-strategic assumption becomes less plausible. A widely studied concept used for analyzing strategic behavior is Bayes-Nash
equilibrium.7 Informally, a set of strategies constitutes a Bayes-Nash equilibrium if no single agent
has an incentive to deviate from its strategy, given that all other agents play their Bayes-Nash equilibrium strategies. McAfee and McMillan (1987) and Klemperer (1999) survey the state of knowledge of strategic analysis of auctions in exchange economies. Milgrom (2000) provides insights
on some of the fundamental challenges to understanding the agent behavior with complementary
preferences. However, definitive results are known only for quite restrictive market structures, and
do not encompass two-sided markets with complementarities, never mind the multi-level characteristic of negotiation in task dependency networks. The problem of even specifying the information
structure of the extensive form game of simultaneous ascending (M+1)st price auctions in task dependency networks, prerequisite to computing Bayes-Nash equilibria, is well beyond the current
state of the art in game-theoretic analysis.
As auction theory currently fails to provide satisfactory guidance for understanding strategic
behavior in even moderately complicated domains, some have used tournaments as a framework
for developing and evaluating candidate agent strategies. The Santa Fe Double Auction Tournament (Rust et al., 1994) provided some unexpected insights into effective strategies in continuous
double auctions, and the recent TAC series of trading agent competitions (Wellman et al., 2001b,
2003) encouraged the development of sophisticated agent strategies (Greenwald, 2003; Stone &
Greenwald, 2000) for a complex market game.
The Vickrey-Clarke-Groves mechanism (Clarke, 1971; Groves, 1973; Vickrey, 1961), also
called the Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), is a direct revelation approach, where agents report their valuations on allocations, and the auction computes a
lump-sum payment. For the GVA, the solution is the optimal allocation based on the reports, and
7. For a foundational reference, Chapter 7 of Fudenberg and Tiroles game theory text (1998) provides a formal treatment of the strategic issues in auction mechanism design and analysis.

541

fiWALSH & W ELLMAN

the payment function is such that it is a dominant strategy for agents to report their true utility.
Because of this incentive compatibility and perfect efficiency, the GVA may seem ideal from an
economic perspective (although the computation can be intractable). However, the GVA is not budget balancedwhen both buyers and sellers bid, the GVA can pay out more money than it takes
in. Unfortunately this is unavoidable, as it is impossible to simultaneously ensure efficiency, budget
balance, and individual rationality (no agent achieves negative surplus) (Myerson & Satterthwaite,
1983). Recently, Babaioff et al. (Babaioff & Nisan, 2001; Babaioff & Walsh, 2003) described
distributed auction mechanisms, based on McAfees trade reduction auction (1992), that obtain incentive compatibility and budget balance in linear supply chains, at the expense of perfect efficiency.
Recent work by Parkes, Kalagnanam, and Eso (2001) explores methods to minimize the deviation
from efficiency while maintaining budget balance in two-sided GVA-like mechanisms.
7.2 Supply Chain Formation
Supply chain managementthe problem of accurately forecasting and planning production and deliveries to meet demand and minimize inventoryis an active field of study in operations research.
The problem of management differs from supply chain formation in that the exchange partners in
the supply chain are pre-established, and it is assumed that information can be gathered from all
agents to effectively optimize global production across the supply chain. In contrast, in this work
we approached the problem of automating the process of determining supply chain participants dynamically, under the assumption that information and decision making is decentralized. Readers
interested in supply chain management may refer to Kjenstad (1998) for an extensive review.
Relatively less effort has been explicitly devoted to the problem we cast as supply chain formation, despite the rhetorical appeals to decentralized and dynamic relation-building commonplace in
the popular literature. Nevertheless, as we point out in Section 3, some venerable AI methodsin
particular the widely-known CONTRACT NET protocolcan in principle be applied to supply chain
formation. As discussed in Section 3, the standard CONTRACT NET does not have mechanisms to
resolve nontrivial resource contention, precluding a systematic comparison with SAMP-SB on general network structures. We can, however, compare the protocols on network structures for which
a resource contention mechanism is not necessary for CONTRACT NET. It is clear that if agents
bid their true costs, then CONTRACT NET with greedy allocation will converge to optimal allocations for trees. The same holds for tree structures relaxed to allow multiple-unit input bids. As we
have shown, SAMP-SB is guaranteed to converge to approximately efficient allocations for trees.
However, it can be shown that it may not converge to good solutions if multiple unit input bids are
allowed. In the latter case, competitive equilibrium may not exist, and and we have observed that
equilibrium non-existence can substantially hurt efficiency in SAMP-SB allocations. In contrast,
producers may receive different prices for the same good with CONTRACT NET. This discriminatory pricing mechanism makes CONTRACT NET robust in the presence of complementarities, for
this class of network structures.
Sandholm (1993) examines a specialization of CONTRACT NET for a generalization of Task Oriented Domains (TODs) (Rosenschein & Zlotkin, 1994). Agents begin with an initial allocation of
tasks and negotiate task exchanges until there are no mutually beneficial trades. Sandholms model
allows local constraints on task achievement in that an agent can perform only certain combinations of tasks. However, there is no dependency structurean agent does not rely on other agents
task achievement in order to accomplish its own tasks. Thus, every locally feasible trade results in
542

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

a globally feasible allocation, and can be executed immediately and independently of other trades.
We cannot generally apply an incremental trading protocol to our task allocation model with subtask
dependencies. A local exchange may require reallocation throughout the entire network to maintain
production feasibility.
Andersson and Sandholm (1998) find that decommitment protocols increase the quality of the
resulting allocations in variants of TODs. With incremental trading, decommitment gives agents the
opportunity to engage in other more cost-effective contracts. Andersson and Sandholm also consider decommitment penalties to provide friction in reallocation and to compensate agents whose
contracts are broken. We expect that such penalties would be an appropriate extension to the
SAMP-SB-D protocol.
Veeramani et al. (Veeramani et al., 1999; Joshi et al., 1999) consider issues arising from simultaneous negotiation of multiple subtasking issues at various levels of a supply chain. In their
asynchronous model, agents may have the opportunity to finalize a contract while other negotiations
are still pending. This uncertainty induces a complex decision problem for agents that do not wish
to overextend their commitments.
Hunsberger and Grosz (2000) study the problem of assigning task performance roles to agents
in the SharedPlans collaborative planning framework. The model is based on recipes, which describe the precedence constraints on the execution time across the various sub-tasks that constitute
a complex task. Contention for shared resources is not modeled explicitly in the recipe, but individual agents may have additional cost, timing, or other constraints, potentially arising from their
individual resource limitations. Hunsberger and Grosz use a combinatorial auction to assign tasks to
agents, given the constraints, to produce a high-valued shared plan. They find that limiting task assignment to certain combinations in roles can effect a tradeoff between computational and allocative
efficiency.
In other work (Walsh et al., 2000; Walsh, 2001), we have studied strategic behavior of agents
bidding in a particular one-shot combinatorial auction within the task dependency network model.
We empirically compared the performance of SAMP-SB, SAMP-SB-D, and the combinatorial auction (with strategic bidding). The combinatorial auction eliminates the problem of dead ends by allocating inputs and outputs to producers on an all-or-nothing basis. This advantage notwithstanding,
combinatorial auctions may not always be an appropriate mechanism. Since finding any feasible
supply chain solution is NP-hard (Walsh et al., 2003), sufficiently large problems will be intractable,
even for advanced optimization procedures. Even when the computation is tractable, social factors
may limit the authority of any one entity to compute allocations over the entire supply chain.

8. Extensions and Future Work
The task dependency network model we propose provides a basis for beginning to understand the
automation of supply chain formation. We have discussed some ways to extend the bidding policies in our market protocol to accommodate more general production capabilities and consumer
preferences. With these extensions we can model capabilities and preferences on multi-attribute
goods (e.g., goods with multiple features such as quality and delivery time, in addition to price and
quantity) by simply representing each configuration as a distinct good in the network. However, it
is clear that this can explode the number of goods with just a few attributes. To effectively handle
greater numbers of attributes would require multiattribute auctions (Bichler, 2001), where multiple
inseparable features of an exchange are negotiated simultaneously.
543

fiWALSH & W ELLMAN

In more realistic scenarios, producers may also have to solve complex internal scheduling, planning, forecasting, or other complex problems in order to evaluate their costs and feasible options.
These types of extensions would increase the fidelity of the model, but would have implications
for agent bidding policies and computation and convergence speed in market protocols. Despite
the best efforts of agents to forecast and plan, agents cannot predict with certainty their operation
within a formed supply chain. Sophisticated agents would employ probabilistic reasoning techniques in evaluating their options during negotiation. When unexpected events do occur that impair
the operation of a formed supply chain, agents would need protocols for repairing or reforming the
supply chain.

We assumed a simple set of non-strategic, myopic bidding policies for the simultaneous ascending auction. Because agents must coordinate input and output bids in a dynamic auction mechanism,
understanding strategic bidding behavior is a challenging and unsolved, albeit important problem for
future work. It seems likely that significant developments in game-theoretic methodology would be
necessary to analytically solve, or even realistically specify, the extensive form games of incomplete
information corresponding to asynchronous iterative auctions. In the meantime, to make progress in
understanding the performance of auctions, we should consider alternate approaches to developing
good bidding policies. Tournaments have proven to be effective ways to both encourage smart
people to design smart trading policies and to evaluate their relative qualities (Rust et al., 1994;
Wellman et al., 2001b). Axelrod (1987) used an evolutionary approach to evaluate populations of
strategies, with fixed types, for the iterated prisoners dilemma. A major challenge in applying an
evolutionary approach to the supply chain formation problem is to develop a sufficiently rich, yet
reasonably searchable set of agent bidding policies.

We suggested decommitment as a solution to the problem of dead ends in SAMP-SB, and a
strategic analysis of the protocol would have to take this phase into account. Because producers
could not lose money when decommitment is allowed, we should expect that producers would be
more willing to participate, and would also be more aggressive in their bidding. Allowing decommitment begs the question of how to enforce that producers decommit only when they are in dead
ends, and also does not address the fact that unilateral decisions for decommitment can potentially
break the (possibly desirable) contracts of many other downstream producers. To reduce aggressive
bidding and mitigate the potential problems, we could charge penalties to producers who initiate
decommitment (Andersson & Sandholm, 1998), perhaps paid to the producers whose output contracts get decommitted. This would reduce spurious decommitments while still allowing an out for
producers stuck in costly dead ends.

Finally, we note that the market configuration studied hereseparate auctions for each good
represents just one possible partition of the scope of negotiations on the supply chain. At the other
extreme, production activity could be mediated by one combinatorial auction mechanism covering
the entire supply chain (Walsh et al., 2000). This avoids some coordination pitfalls of the separate
auction approach, but imposes other disadvantages associated with imposing a mechanism with
global scope. Intermediate configurations, involving multiple auctions for clusters of highly related
goods, represent a promising alternative for further investigation.
544

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

ACKNOWLEDGMENTS
This paper includes material previously presented at the Sixteenth International Joint Conference
on Artificial Intelligence (IJCAI-99) (Walsh & Wellman, 1999). This work was supported in part
by NSF grant IIS-0205435.

Appendix A. Proofs
This appendix provides proofs of our theorems. For convenience, we restate the theorems before
the proofs.
In our proofs, it is sometimes useful to index the position of a producer in a network. The Clevel of a producer with output g is the maximum distance from it to any consumer, formally stated
as follows: one if no producer, but some consumer, has g as input, and k + 1 if the maximum level
of any producer with input g is k. The S-level of a producer is defined similarly, but with respect
to the distance to any producer with no input, and with a basis of zero for producers that have no
inputs themselves. Both the C-level and S-level are well defined, by acyclicity.
A.1 Proof of Theorem 1
Let (V, E) be a network with no input complementarities, that is all producers have at most one
input, and let (V  , E  ) be the optimal allocation for (V, E). For convenience, we partition producers
 into sets 1 , the producers with a single input, and 0 , the producers with no inputs.
Procedure No Input Complementarities Equilibrium constructs prices that support a
competitive equilibrium for (V  , E  ).
No Input Complementarities Equilibrium:
1. Initialize all prices to zero.
2. Perform any of the following until no price changes are made:
(a) If for some c  C \V  , we have vc (g) > p(g), where hg, ci  E \ E  ,
p(g)  vc (g).
(b) If for some c  C V  , we have vc (g0 )  p(g0 ) > vc (g)  p(g)  0,
where hg, ci  E  and hg0 , ci  E \ E  ,
p(g0 )  vc (g0 )  (vc (g)  p(g)).
(c) If for some   0 V  , we have p(g ) <  , where h, g i  E  ,
p(g )   .
(d) If for some   1 V  , we have p(g ) < p(g) + 
where h, g i  E  and hg, i  E  ,
p(g )  p(g) +  .
(e) If for some   1 \V  , we have p(g ) > p(g) +  ,
where h, g i  E \ E  and hg, i  E \ E  ,
p(g)  p(g )   .
In network (V, E) (with no input complementarities) at prices p, a closed, reverse-surplus sequence is a directly connected sequence of agents and goods such that every agent would be better
off by reversing its allocation. Formally, it is a sequence (n1 , . . . , nk ) of vertices ni  V , such that:
545

fiWALSH & W ELLMAN

1. hni , ni+1 i  E or hni+1 , ni i  E for all i where 1  i  k  1.
2. nk  G.
3. n1  (C \V  )  (0 V  ).
(a) If n1  C \ V  , then hn2 , n1 i  E \ E  and n1 would obtain nonnegative surplus at p
from obtaining n2 . If 1 = k  1, then n1 would obtain strictly positive surplus at p from
obtaining n2 .
(b) If n1  0  V  , then hn1 , n2 i  E  and n1 would obtain nonpositive surplus at p from
being active. If k = 2, then n1 would obtain strictly negative surplus at p from being
active.
4. For i  2, if ni  A then ni  1  (C V  ).
(a) If ni  1  V  , then hni1 , ni i  E  , hni , ni+1 i  E  , and ni would obtain nonpositive
surplus at p from being active. If i = k  1, then ni would obtain strictly negative surplus
at p by being active.
(b) If ni  1 \ V  , then hni+1 , ni i  E  , hni , ni1 i  E  , and ni would obtain nonnegative
surplus at p from being active. If i = k  1, then ni would obtain strictly positive surplus
at p by being active.
(c) If ni  C V  , then hni1 , ni i  E  , hni+1 , ni  E \E  , and ni would obtain no less surplus
from ni+1 than from ni1 at p. If i = k  1, then ni would obtain strictly better surplus
from ni+1 than from ni1 .
An open reverse-surplus sequence is the same as a closed, reverse-surplus sequence except
that, instead of Condition 3, we have n1  G and n2  1  (C  V  ) as with Condition 4. Clearly
any closed, reverse-surplus sequence of length greater than two contains an open, reverse-surplus
sequence.
Lemma 18 Procedure No Input Complementarities Equilibrium does not reach a state such
that there is an open, reverse-surplus sequence K = (n1 , . . . , nk ) constituting a cycle with n1 = nk
and k  3.
Proof. Assume, contrary to which we wish to prove, that there is such a cycle K at prices p.
Moreover, let the cycle be the smallest, in that it contains no other such cycle.
We show how to create an alternate, feasible solution (V 0 , E 0 ) with a higher value than (V  , E  ),
giving us a contradiction. Initialize (V 0 , E 0 ) = (V  , E  ). For all n j , where 1  j < k, if hn j , n j+1 i 
E  , remove the edge from E 0 , and if the edge is in E \ E  , add the edge to E 0 . Also, add and remove
vertices as necessary to be consistent with the added and removed edges.
Each producer in (V 0 , E 0 ) is feasible because it was feasible in (V  , E  ) and if it has an input,
either both its input and output are added, both are removed, or neither is changed. Consider a good
n j  G, with 1 < j < k. Since j  1 > 1, it must be that agents n j1 and n j+1 are in 1  (C V  ).
By inspecting Conditions 4(a)4(b) of the definition of a closed, reverse-surplus sequence (which
also apply to an open reverse-surplus sequence), we see that edges incident on n j are added or
removed in such a way that n j is in material balance. Similarly, considering agents nk1 , n2 , and
546

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

good n1 = nk , we have material balance for good n1 = nk . Since goods are in material balance and
producers are feasible, (V 0 , E 0 ) is feasible.
The surpluses of agents not in K are unaffected by the transformation. By definition of an open,
reverse-surplus sequence, every agent in K obtains no lower surplus at p after the transformation,
and agent nk1 obtains strictly higher surplus at p. Because the value of a feasible allocation is the
sum of agent surpluses at any particular prices (Lemma 22), we must then have value((V 0 , E 0 )) >
value((V  , E  )). But this contradicts the optimality of (V  , E  ), so the assumption that K exists
must be false. 2
Lemma 19 If the price of good nk increases in Procedure No Input Complementarities
Equilibrium, then there there exists a finite closed, reverse-surplus sequence (n1 , . . . , nk ) at prices
p just before the price increase.
Proof.
We show how to construct the desired closed, reverse-surplus sequence, referring to the
conditions in the definition, and to the steps in Procedure No Input Complementarities
Equilibrium. The price increase of nk occurred in one of Steps 2(a)2(e), triggered by agent
nk1 . Since the step was triggered, nk1 would obtain strictly better surplus by reversing its allocation at p, as specified in the conditions 3(a), 3(b), or 4(a)4(c). If the price of nk was increased in Step 2(a) or 2(c), then we have the desired closed, reverse-surplus sequence, with
nk1  (C \V  )  (0  V  ) and k  1 = 1. Otherwise, the price or nk was increased in Step 2(b),
2(d), or 2(e), with nk1  1  (C V  ) and k  1 > 2. In this case, we let nk2 be the good that also
matched the condition of the step.
If the price of nk2 increased, then Procedure No Input Complementarities Equilibrium
ensures that we can find an agent nk3 matching one of the Conditions 3(a), 3(b), or 4(a)4(c). If, on
the other hand, p(nk2 ) = 0, then because producers have positive costs and consumers have positive
values, we can also find such an agent nk3 . If we find an agent that corresponds to condition 3(a)
or 3(b), then k  3 = 1 and we are done. Otherwise, we can find a good nk4 , as we did nk2 , and
continue in the same manner.
Clearly, this process constructs an open, reverse-surplus sequence. Now, we must show that this
process of selecting vertices eventually selects an element n1  (C \V  )  (0  V  ). Since (V, E)
is finite, and since by Lemma 18 there can be no cycles in any open, reverse-surplus sequence, we
must eventually find a n1  (C \V  )  (0 V  ) to give us a closed, reverse-surplus sequence. 2
Lemma 20 Procedure No Input Complementarities Equilibrium terminates.
Proof. Assume, contrary to which we wish to prove, that the procedure does not terminate and that
the price of good g increases an infinite number of times. Consider a cycle K = (n1 = g, . . . , nk = g)
of vertices ni  V , k  3 such that:
1. hni , ni+1 i  E or hni+1 , ni i  E for all i  {1, . . . , k  1}.
2. For all i  {2, . . . , k  1}, ni 6= g.
3. For all i  {3, . . . , k}, if ni  G, the price increase of good ni occurred in one of the Steps 2(b),
2(d), or 2(e) in the procedure, and agent ni1 and good ni2 also matched the condition in that
step. Furthermore, the price increase of ni2 , triggered by agent ni3 and good ni4 , caused
the need for the price increase of good ni .
547

fiWALSH & W ELLMAN

Because the price of g increases an infinite number of times, such a cycle must exist.
Let p be prices such that p(n1 ) is as it was when n1 and agent n2 triggered the price increase of
n3 , and for all ni  G and 1 < i < k, p(ni ) is as it was just after the it was increased, as triggered by
agent ni1 and good ni2 . The price of all other goods is an arbitrary nonnegative number.
By the way we constructed p, and by the way prices are increased in the procedure, K must
be an open, reverse-surplus sequence. But by Lemma 18, such a K cannot exist. Therefore, the
procedure terminates. 2
Theorem 1 Competitive equilibria exist for any network with no input complementarities.
Proof. We show that Procedure No Input Complementarities Equilibrium terminates at
prices p with every agent obtaining its maximum surplus according to (V  , E  ) . Since (V  , E  )
is efficient, it is also feasible, giving us a competitive equilibrium at prices p.
By Lemma 20, the procedure terminates. Clearly, when the procedure terminates, agents in
1  (0  V  )  (C \ V  ) optimize according to (V  , E  ). It remains to show the same for (C 
V  )  (0 \V  ). Assume, contrary to which we wish to prove, that some a  (C V  )  (0 \V  )
does not optimize according to (V  , E  ).
Consider the case where a  (C  V  ) and hg, ai  E  . Since the algorithm guarantees that a
does not prefer any other good g0 to g at prices p, it must be that p(g) > vc (g). Let p0 be the
prices immediately before the price of g rose above vc (g) and p00 be the prices immediately after.
By Lemma 19, there is a closed, reverse-surplus sequence (n1 , . . . , nk = g) at prices p0 . At p00 , the
conditions of the closed, reverse-surplus sequence hold, except that the surplus condition in 4(a),
4(b), or 4(c) that applies to nk1 becomes non-strict. However, a obtains a strictly negative surplus
at p00 . Denote a as nk+1 .
We can create an alternate, feasible solution (V 0 , E 0 ) as in the proof of Lemma 18 by adding
edges hni , ni+1 i that are in E \ E  , and removing such edges that are in E  , for all i  {1, . . . , k}.
The surpluses of agents not in K are unaffected by the transformation. Every agent in (a1 , . . . , nk1 )
obtains no lower surplus at p00 after the transformation. Agent a = nk obtains zero surplus after
the transformation, which is higher than the negative surplus it had before. Because the value of a
feasible allocation is the sum of agent surpluses at any particular prices (Lemma 22), we must have
value((V 0 , E 0 )) > value((V  , E  )). But this contradicts the optimality of (V  , E  ), so it must be that
p(g)  vc (g) and a is obtaining its maximum surplus at p in (V  , E  ).
If, on the other hand, a  (0 \V  ), and ha, gi  E. It must be that a < p(g). We can use the
same line of proof as the case of C  V  to show that (V  , E  ) has a suboptimal value, providing a
contradiction. Thus a must optimize according to (V  , E  ) at p.
Thus we have shown that the algorithm terminates with all agents optimizing according to
(V  , E  ) at p. Thus p supports a competitive equilibrium for allocation (V  , E  ). 2
A.2 Proof of Theorem 2
Given a polytree (V, E) and an efficient allocation (V  , E  ), we present Procedure Polytree
Equilibrium that constructs lower bounds p (g) and upper bounds p (g) on the prices of all
goods g, and in turn uses these bounds to construct prices p for all goods. Then we prove that the
resulting prices are in fact competitive equilibrium prices that support (V  , E  ).
548

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Observe that, for the purposes of competitive equilibrium pricing, we can treat a consumer c
that wishes to obtain one good from the set Gc as a consumer that desires a single good gc with
value vc (gc ) = vc = maxgGc vc (g), along with additional producers. For each g  Gc we create a
producer  with output gc , input g, and with  = vc  vc (g). Thus, without loss of generality, we
consider only consumers with preferences for single goods. We denote as gc the good that consumer
c desires and denote as vc the value c has for gc .
We refer to all n0  V such that either hn, n0 i  E or hn0 , ni  E as the neighbors of a vertex
n  V . We use  to refer to a null vertex that is not a neighbor of any other vertex.
Polytree Equilibrium:
1. For each g  G, p (g)  0 and p (g)  .
2. For each connected subgraph (V , E)  (V, E), select a g  G  V arbitrarily:
Perform Set Bounds(g, ).
p(g)  p (g).
Set Bounds recursively visits the vertices, updating the price bounds in postorder (i.e., as the
recursion unwinds) and setting prices to either the lower or upper bounds. Because (V, E) is a
polytree, the procedure sets the price for each good exactly once.
In Set Bounds(n, r), if n  A, then r  G and the procedure either updates p (r) or p (r)
after the bounds for all neighbors of n, other than r, have been fixed. If it updates p (r), it does so
in such a way that if n 
/ V  then n, if active, would get a nonpositive surplus for any p(r)  p (r),
given the bounds on the other neighbors of n, and if n  V  then n, if active, would get a nonnegative surplus for any p(r)  p (r), given the bounds on the other neighbors of n. Since p (r) only
increases (Steps 2, 4(b), and 5(c)), this property is maintained. Similarly, if Set Bounds(n, r)
updates p (r), it does so in such a way that if n 
/ V  then n, if active, would get a nonpositive
surplus for any p(r)  p (r), given the bounds on the other neighbors of n, and if n  V  then
n, if active, would get a nonnegative surplus for any p(r)  p (r), given the bounds on the other
neighbors of n. Since p (r) only decreases (Steps 3, 4(c), and 5(b)), this property is maintained.
Set Bounds(n, r):
1. For each neighbor z of n such that z 6= r, perform Set Bounds(z, n).
2. If n  C \V  ,
p (r)  max(vn , p (r)).
3. Else if n  C V  ,
p (r)  min(vn , p (r)).
4. Else if n   \V  then,
(a) For each neighbor g of n such that g 6= r
If g is an input of n
p(g)  p (g).
Else g is the output of n,
p(g)  p (g).
549

fiWALSH & W ELLMAN

(b) If r is an input of n,
and with the output, gn , of n
p (r)  max(p (r), p (gn )  hg,niE, g6=r p (g)  n ).
(c) Else r is the output of n,
p (r)  min(p (r), hg,niE p (g) + n ).
5. Else if n   V  then,
(a) For each neighbor g of n such that g 6= r,
If g is an input of n,
p(g)  p (g).
If g is the output of n,
p(g)  p (g).
(b) If r is an input of n,
and with the output, gn , of n),
p (r)  min(p (r), p (gn )  hg,niE, g6=r p (g)  n ).
(c) Else r is the output of n,
p (r)  max(p (r), hg,niE p (g) + n )

Lemma 21 Procedure Polytree Equilibrium computes price bounds such that p (g)  p (g)
for all goods g  G.
Proof. Assume, contrary to which we wish to prove, that at some state there is some g  G such that
p (g) > p (g). Assume further that g is the first such good visited.
We say that agent a constrained p (g) if Set Bounds(a, g) was the last to change p (g).
Similarly, we say that agent a constrained p (g) if Set Bounds(a, g) was the last to change
p (g).
Recall from Lemma 22 that the value of any feasible allocation is equal to the sum of the agent
surpluses at any particular prices. We show how to transform (V  , E  ) to an alternate feasible
allocation (V 0 , E 0 ) and compute alternate prices p to show that the sum of surpluses in (V 0 , E 0 ) is
greater than in (V  , E  ).
First, initialize (V 0 , E 0 ) = (V  , E  ) and for each good g  G initialize p(g) = 0. Next, set p(g) =
p (g). Then we recursively change prices and the allocation for a portion of the subtree rooted
at g. Perform Lower Bound(a, g) for the agent a that constrained p (g) and perform Upper
Bound(a, g) for the agent that constrained p (g).
Throughout the transformation, we perform Lower Bound(a, g) iff we visit g and agent a
constrained p (g). Similarly, we perform Upper Bound(a, g) iff we visit g and agent a constrained p (g). The following describes these portions of the transformation.
Lower Bound(a, g):
1. If a   \V  , it must be that g is an input of a (because a constrained p (g)).
For each neighbor g0 6= g of a:
550

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

(a) If g0 is an input of a,
p(g0 )  p (g0 ),
perform Upper Bound(a0 , g0 ) for the agent a0 that constrained p (g0 ).
(b) Else (g0 is an output of a),
p(g0 )  p (g0 ),
perform Lower Bound(a0 , g0 ) for the agent a0 that constrained p (g0 ).
2. Else if a   V  , it must be that g is an output of a (because a constrained p (g)).
For each input g0 of a:
p(g0 )  p (g0 ),
perform Lower Bound(a0 , g0 ) for the agent a0 that constrained p (g0 ).
3. If a  V  ,
remove a and all incident edges from (V 0 , E 0 ).
4. Else if a  V \V  ,
add a and all incident edges to (V 0 , E 0 ).
Upper Bound(a, g):
1. If a   \V  , it must be that g is an output of a (because a constrained p (g)).
For each input g0 of a:
p(g0 )  p (g0 ),
perform Upper Bound(a0 , g0 ) on the agent a0 that constrained p (g0 ).
2. If a   V  , it must be that g is an input of a (because a constrained p (g)).
For each neighbor g0 6= g of a:
(a) If g0 is an input of a,
p(g0 )  p (g0 ),
perform Lower Bound(a0 , g0 ) for the agent a0 that constrained p (g0 ).
(b) Else (g0 is an output of a),
p(g0 )  p (g0 ),
perform Upper Bound(a0 , g0 ) for the agent a0 that constrained p (g0 ).
3. If a  V  ,
remove a and all incident edges from (V 0 , E 0 ).
4. Else if a  V \V  ,
add a and all incident edges to (V 0 , E 0 ).
Observe that, because (V, E) is a polytree, a vertex can be visited at most once by either Upper
Bound or Lower Bound.
Now we show that (V 0 , E 0 ) is feasible. Consumers are always feasible. Producers are feasible
because we add or remove all incident edges when we add or remove a producer, respectively. We
now prove that every g  G is in material balance in (V 0 , E 0 ).
551

fiWALSH & W ELLMAN

Consider good g for which p (g) > p (g). Lower Bound(a, g) is performed only if agent
a constrained p (g), which occurred in 2, 4(b), or 5(c) of Set Bounds(a, g). Therefore Lower
Bound(a, g) either adds hg, ai  E \ E  or else removes ha, gi  E  . Upper Bound(a, g) is
performed only if a constrained p (g), which occurred in 3, 4(c), or 5(b) of Set Bounds(a, g).
Therefore Upper Bound(a, g) either adds ha, gi  E \ E  or else removes hg, ai  E  . For any
possible combination, material balance is maintained for g.
Now consider any other good g 6= g. If g is visited by Lower Bound(a, g), then p(g) was set
to p (g) in one of the following ways, immediately prior:
1. p(g) was set to p (g) by 1(b) of Lower Bound(a, g), for some other agent a   \V  and
other good g. In this case, g is an output of a so ha, gi  E \ E  was added to (V 0 , E 0 ) in 4 of
Lower Bound(a, g).
2. p(g) was set to p (g) by 2 of Lower Bound(a, g), for some other agent a    V  and
other good g. In this case g is an input of a so hg, ai  E  was removed from (V 0 , E 0 ) in 3 of
Lower Bound(a, g).
3. p(g) was set to p (g) by 2(a) of Upper Bound(a, g), for some other agent a   V  and
other good g. In this case case g is an input of a so hg, ai  E  was removed from (V 0 , E 0 ) in
3 of Upper Bound(a, g).
One of the following operations occurred in Lower Bound(a, g):
1. If a constrained p (g) in 2 or 4(b) of Set Bounds(a, g), then hg, ai  E \ E  is added to
(V 0 , E 0 ) in 4 of Lower Bound(a, g).
2. If agent a constrained p (g) in 5(c) of Set Bounds(a, g), then ha, gi  E  is removed from
(V 0 , E 0 ) in 3 of Lower Bound(a, g).
For any possible combination of additions or removals of edges incident on g prior to, and in Lower
Bound(a, g), material balance is maintained for g. We can show a similar result if g is visited by
Upper Bound(a, g). Hence we have established feasibility of (V 0 , E 0 ).
Now we show that for any agent a  A, (a, (V 0 , E 0 ), p)  (a, (V  , E  ), p), and there is some
agent a0  A such that (a0 , (V 0 , E 0 ), p) > (a0 , (V  , E  ), p).
For any agent a not visited in the construction of (V 0 , E 0 ), (a, (V 0 , E 0 ), p) = (a, (V  , E  ), p),
because a has the same allocation as in (V  , E  ).
Consider a0 visited by Upper Bound(a0 , g). Because a0 was thus visited, a0 constrained

p (g). Upper Bound(a0 , g) sets the prices of all other neighbor goods g 6= g to the prices used
to compute p (g) in Set Bounds(a0 , g). The prices of these neighboring goods were computed
such that if a0  V  , a0 would get negative surplus at any price for g above p (g) and if a0  V \V 
it would get a positive surplus for at any price for g above p (g). But, in the alternate prices we
computed, p(g) = p (g), and we assume p (g) > p (g). Since a0 is in V 0 if and only if it is not in
V  , we have (a0 , (V 0 , E 0 ), p) > (a0 , (V  , E  ), p).
Now consider any other a  A, a 6= a0 , visited in the construction of (V 0 , E 0 ). If a is visited
by Upper Bound(a, g), then p(g) = p (g) and a must have constrained p (g). If a  C, then
Set Bounds(a, g) set p (g) such that va  p (g) = 0. If a  , Upper Bound(a, g) sets the
prices of the other goods neighboring a to the prices used to compute p (g) in Set Bounds(a, g).
These neighboring prices are such that if a were active and feasible, it would get zero surplus when
552

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

p(g) = p (p). Thus (a, (V 0 , E 0 ), p) = (a, (V  , E  ), p). If, on the other hand, a is visited by Lower
Bound, then p(g) = p (g) and a must have constrained p (g). By a similar argument we used with
Upper Bound, a gets a zero surplus when p(g) = p (g). Again, this gives (a, (V 0 , E 0 ), p) =
(a, (V  , E  ), p).
We have shown that for any agent a  A, (a, (V 0 , E 0 ), p)  (a, (V  , E  ), p), and there is
some agent a0  A such that (a0 , (V 0 , E 0 ), p) > (a0 , (V  , E  ), p). But then value((V 0 , E 0 )) >
value((V  , E  )), which is a contradiction. Hence, the initial assumption that p (g) > p (g) must
be false. But then p (g)  p (g) for all goods g.
2

Theorem 2 Competitive equilibria exist for any polytree.
Proof. We show that agents optimize according to (V  , E  ) at the prices p computed by procedure Polytree Equilibrium. Since (V  , E  ) is feasible by definition, the resulting prices and
allocation constitute a competitive equilibrium for (V, E).
Because the construction of p (g) ensures that it never decreases, Step 2 of Set Bounds ensures that every consumer c  C \V  optimizes if p(gc )  p (gc ). Because p (gc )  p(c)  p (gc )
(by construction of p and by Lemma 21), c then optimizes according to (V  , E  ). By a similar argument, every c  C V  optimizes according to (V  , E  ).
Consider a producer    \ V  , visited by Set Bounds(, g). If good g is an input of ,
then 4(a) of Set Bounds(, g) sets the price of every other neighbor good g0 6= g of  to the price
bounds used to compute p (g) in Step 4(b) of Set Bounds(, g). Moreover, p (g) is set to the
smallest price such that  could get a maximum surplus of zero, given the specified bounds of the
other neighbor goods. Since p (g) could only increase subsequently, since p (g)  p(g)  p(g)
(by the construction of p and by Lemma 21), and since the price of each good is set only once
(because (V, E) is a polytree)  cannot get a positive surplus at the prices set by Set Bounds(,
g). By a similar argument, if g is an output of , then in Step 4(c) p (g) is set to the largest price
such that  would get at a maximum surplus of zero, given the prices set on the neighbor goods.
Since again, p (g)  p(g)  p (g), p (g) only increases subsequently, and the price of each good
is set only once, it must be that  can get at most zero surplus. Thus  optimizes according to
(V  , E  ). Symmetrically, we can see that every    V  optimizes according to (V  , E  ).
We have shown that all agents optimize according to (V  , E  ) at p, hence we have shown that a
competitive equilibrium exists for polytree (V, E). 2

A.3 Proof of Theorem 3
Lemma 22 The value of a feasible allocation (V 0 , E 0 ), at any prices p, can be expressed as:

value((V 0 , E 0 )) =

 (a, (V 0, E 0), p).

aA

553

(1)

fiWALSH & W ELLMAN

Proof. Equation (1) expands to:
value((V 0 , E 0 )) =



vc ((V 0 , E 0 )) 

+







p(g)



p(g)   ((V , E )) .

hg,ciE 0

cC



p(g) 

h,giE 0

!
0

hg,iE 0

0

!

Since all goods are in material balance in a feasible allocation, all the price terms cancel out. We
are left with

 vc ((V 0, E 0))   s ((V 0 , E 0)),


cC

which is the original formula for the value of a solution (Definition 1). 2
Theorem 3 If (V 0 , E 0 ) is a --competitive equilibrium for (V, E) at some prices p, then (V 0 , E 0 ) is
a feasible allocation with a nonnegative value that differs from the value of an efficient allocation
by at most  [hg,iE g + s ] + |C|b .
Proof. We refer to the four conditions for a --competitive equilibrium (Definition 4). Let (V  , E  )
be an efficient allocation for (V, E).
Conditions (3) and (4) imply that (V 0 , E 0 ) is feasible. Recall the formula for the value of a
feasible allocation from Equation (1). Since (V 0 , E 0 ) and (V  , E  ) are both feasible, we can express
their values as
value((V 0 , E 0 )) =

 (a, (V 0 , E 0), p),

(2)

 (a, (V  , E ), p).

(3)

aA

value((V  , E  )) =

aA

For all c  C, by Condition (2), (c, (V 0 , E 0 ), p)  Hc (p)  b . Because no allocation is any
better for an agent than its optimal allocation, (c, (V  , E  ), p)  Hc (p). Thus,
(c, (V  , E  ), p)  (c, (V 0 , E 0 ), p)  b .

(4)

For all   , by Condition (3), (, (V 0 , E 0 ), p)  H (p)  (hg,iE g + s ). Because no
allocation is any better for an agent than its optimal allocation, (, (V  , E  ), p)  H (p). Thus,
(, (V  , E  ), p)  (, (V 0 , E 0 ), p) 



g + s .

(5)

hg,iE

Equations (2)(5) together imply that value((V  , E  ))  value((V 0 , E 0 ))   [hg,iE  +
s ] + |C|b . Condition (1) implies that each sum term in Equation (2) is nonnegative, hence
value((V 0 , E 0 ))  0. As noted, (V 0 , E 0 ) is feasible.
2
g

554

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

A.4 Proof of Theorem 5
In proving the theorem, we refer to the C-level and S-level of producers in a network, as defined in
the beginning of Section A.
A task dependency network (V, E) is characterized by the following parameters:
 : the maximum C-level of any producer in the network,
 : the maximum number of input goods for any producer,
 R: the maximum consumer value, maxhg,ciE|cC vc (g).
Lemma 23 In a run of SAMP-SB for network (V, E), no agent places a buy offer above R + 2b .
Proof. Consumers never offer above their valuation, which is bounded by R. We prove by induction
on the producer C-level that no producer at C-level k places a buy offer above R + 2kb .
Suppose that a producer  at C-level one places an offer to buy an input g at price  > R + 2b .
Since it always increments buy offers by b , this means at some previous time it submitted a buy
offer for g at price R + b < 0  R + 2b . At that time, it must have been losing g, else it would not
be bidding. But then the ask quote of g must have been greater than R, and  then offered greater
than R for its output. Since only a consumer will offer to buy for the output of a producer at C-level
one,  must lose its output. Because offers are nondecreasing, this situation is permanent, and hence
 never again raises an input offer, contrary to our supposition. Thus a C-level one producer will
never place a buy offer above R + 2b .
For the inductive step, we assume that no producer at any C-level i, where i < k, places a buy
offer above R + 2ib . Thus, no producer at C-level k can win its output offer for more than R +
2(k  1)b . Applying reasoning analogous to the base case (C-level one), we see that no producer
at C-level k places a buy offer above R + 2kb . Because k   for all producers, the lemma follows
immediately. 2
Lemma 24 No agent places more than (R + 2b )/b +  buy offers.
Proof. Since consumers offer at most R and increase offers by at least b , they place offers at
most R/b times. A producer initially places at most  buy offers for its inputs. According to
Lemma 23 and the producer bidding policy, a producer subsequently offers no higher than R + 2b
in increments of b for each of a maximum of  inputs. 2
Theorem 5 SAMP-SB reaches quiescence after a finite number of bids have been placed.
Proof. By Lemma 24, a finite number of buy offers are placed. We need show only that producers place a finite number of sell (output) offers to establish that a finite number of total bids are
placed.
A producer will change its output offer only if: 1) the price of an input changes, 2) the ask
price of an input changes, or 2) it loses an an offer for a good that it was previously winning.
An unchanged input offer can switch between winning and losing at most once without the price
changing. Similarly, an unchanged input offer can switch winning state at most once without the ask
price changing. Hence, it is sufficient to show that the price and ask price of each of a producers
555

fiWALSH & W ELLMAN

input goods change at most a finite number of times. We prove by induction on the producer S-level
that the price and ask quotes of an input good to a producer at S-level k changes a finite number of
times.
Only a producer with no input places an output offer for a input good g to a producer at S-level
one, and producers with no input place only one offer each. Hence, the price or ask price of g
change only in response to a change in a buy offer for g. But by Lemma 24, the number of buy offer
changes for g is finite.
Now assume that all producers at all S-levels less than k place a finite number of output offers.
For a good g which is an input for a producer  at S-level k, the number of output offer changes is
finite. Again the number of input offers for g must be finite. Since the number of input and output
offers for g is finite,  places a finite number of output offers. 2
A.5 Proof of Theorem 8
In proving the theorem, we refer to the C-level of producers in a network, as defined in the beginning
of Section A. For reference, quasi-quiescence is described in Definition 5.
Lemma 25 If a run of SAMP-SB is in a quasi-quiescent state during the time interval [t, t 0 ] then no
inactive producer changes an offer for an input good in the time interval [t, t 0 + ], where  is the
smallest period of time an agent requires to update a bid in response to a price quote.
Proof. By definition of quasi-quiescence, during the interval [t, t 0 ], no consumer or active producer
changes any offer. Thus, a simple induction on the C-level of the inactive producers shows that any
producer that is inactive at time t would not win its output during [t,t 0 ], hence inactive producers
remain inactive during this interval. But a producer that is inactive during [t, t 0 ] would not change
its input offer during [t, t 0 + ]. 2
Lemma 26 If a run of SAMP-SB is in a quasi-quiescent during the time interval [t, t 0 ], then it is
quasi-quiescent during the time interval [t, t 0 + ], where  is the smallest period of time an agent
requires to update a bid in response to a price quote.
Proof. Assume, contrary to that we wish to prove, that a run of SAMP-SB is quasi-quiescent during
[t, t 0 ] but not during time [t 0 ,t 0 + ]. Let a be a consumer or active producer that will change an offer
in [t 0 , t 0 + ].
If a is a consumer, then a would only change an offer if it lost some offer it was previously
winning in quasi-quiescence. If a is a producer, it must be feasible, otherwise it would change its
input offer (because it is active) violating quasi-quiescence. Since a is feasible, it would change an
offer only if it loses an input it was previously winning, or the price of one of its inputs increases.
In any of these cases, a either loses a buy offer it was previously winning, or the price of one of
its buy offers increased. For one of these to occur, it must be that at time t 00  [t,t 0 ], some agent
either 1) changed its own winning output offer or 2) changed its input offer. But the definition of
quasi-quiescence precludes #1, and Lemma 25 and the definition of quasi-quiescence preclude #2.
This gives us a contradiction, proving the lemma. 2
Lemma 27 If a run of SAMP-SB is in a quasi-quiescent state at time t, then it is quasi-quiescent at
all times t 0 > t.
556

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Proof. By Lemma 26 we can conclude that we have quasi-quiescence in the interval [t, t + ], then
further extend that interval by , and so on indefinitely. 2
Lemma 28 If a run of SAMP-SB is in a quasi-quiescent at time t, then a producer that is inactive
at time t is inactive at time t + , and a producer that is active at time t is also inactive at time t + ,
where  is the smallest period of time an agent requires to update a bid in response to a price quote.
Furthermore, p(g) does not change for any good g at time t + 
Proof. Since an agent cannot lower its offers, the only way for an inactive producer  to become
active is for some other agent to raise its buy offer. By Lemma 27 and the definition of quasiquiescence, only inactive producers will change any offers after t, and by Lemma 25 no inactive
producer will change its input offers. But then  remains inactive.
Since offers do not decrease, an active producer  can become inactive only by increasing its
offer for its output. But  will do this only if the prices on its inputs increase. Since we have a
quasi-quiescent state, this can happen only if an inactive producer 0 changes its offer for its output
g. But since 0 is inactive, a change to its offer for g can cause only (g) to change. Since active
producers are feasible (otherwise they would want to change their bids, violating quasi-quiescence),
 is not losing a buy offer for g at time t. Therefore,  does not respond to changes in (g), hence
does not change its output offer and will remain active. 2
Theorem 8 If a run of SAMP-SB reaches a quasi-quiescent state, then it remains in a quasiquiescent state. Furthermore, neither the allocation nor the prices p subsequently change.
Proof. The theorem follows directly from Lemmas 27 and 28. 2
A.6 Proof of Theorem 10
In proving the theorem, we refer to the C-level of producers in a network, as defined in the beginning
of Section A.
A given run of SAMP-SB in network (V, E) is characterized by the following parameters:
 : the maximum C-level of any producer in the network,
 : the maximum number of input goods for any producer,
 R: the maximum consumer value, maxhg,ciE|cC vc (g).
Theorem 10 SAMP-SB reaches a quasi-quiescent state after a number of bids bounded by a polynomial of the size of the network and the value of the maximum consumer value have been placed
by consumers and active producers.
Proof. SAMP-SB is guaranteed to reach a quasi-quiescent state (Theorem 5 and Observation 7).
By Lemma 24, the number of buy offers is bounded by a polynomial in the value of R, hence we
need only be concerned with the number of sell offers placed. Since the prices of buy offers increase
by at least b , a producers perceived cost for any good must rise by at least b , so will increase its
557

fiWALSH & W ELLMAN

sell offer by no less than b . Also, a producer will increase its sell offer by no less than s , as required by the auction. Hence, Lemma 23 implies that an active producer will become permanently
inactive after it places at most (R + 2b )/[max(b , s )] output offers. 2
A.7 Proof of Theorem 11
In proving the theorem, we refer to the conditions for a --competitive equilibrium (Definition 4).
Lemma 29 When SAMP-SB reaches quiescence in network (V, E) then each consumer obeys the
--competitive equilibrium conditions (Conditions (1) and (2)).
Proof. Since each consumer maintains at most a single winning offer for a good that gives it nonnegative surplus, it obeys Condition (1).
Let the final prices and allocation be p and (V 0 , E 0 ), respectively. Assume, contrary to Condition (2), that (c, (V 0 , E 0 ), p) < Hc (p)  b for some consumer c. Let g be a surplus-maximizing
good for c at p.
If c does not buy a good, then p(g ) + b < vc (g ) (otherwise it would have placed and won an
offer for g ) and (c, (V 0 , E 0 ), p) = 0. Noting also that Hc (p) = vc (g )  p(g ), algebraic manipulation gives us (c, (V 0 , E 0 ), p) > Hc (p)  b , which is a contradiction.
Thus, c buys one good g0 such that
vc (g0 )  p(g0 ) < vc (g )  p(g )  b .

(6)

Let p(g ) and p(g0 ) be the prices for g and g0 when c placed its final offer for g0 . Since c offers
p(g0 ) + b for g0 , and since c won this offer at p(g0 ), we have
p(g0 ) + b  p(g0 ).

(7)

p(g )  p(g ).

(8)

Since prices do not decrease, we have

Substituting Equations (7) and (8) into the left and right sides, respectively, of Equation (6) gives us
vc (g0 )  ( p(g0 ) + b ) < vc (g )  ( p(g ) + b ).
But the consumer bidding policy specifies that c would have bid for g , rather than g0 at prices p,
which is a contradiction. Thus each consumer obeys Condition (2). 2
Lemma 30 If SAMP-SB reaches quiescence in network (V, E) such that no inactive producer buys
a positive-price input, then each producer obeys the --competitive equilibrium conditions (Cong
ditions (1) and (3)), with  = max((g)  p(g), b ).
Proof. The bidding policy ensures that each producer  sells its output g only at a nonnegative
surplus, and the lemma conditions directly imply that  has zero surplus if it does not sell . Thus
 obeys Condition (1).
The producer bidding policy guarantees that  is feasible in quiescence.
558

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Let the final prices be p and allocation be (V 0 , E 0 ). If H (p) > hg,iE  + s in quiescence,
then H (p) = p(g )  hg,iE p(g) > hg,iE g + s . Thus p(g ) > hg,iE p(g) + hg,iE g +
s . The producer bidding policy ensures that  offers no more than hg,iE p(g) + hg,iE g + s
for g , so it must be winning g at a profit. Thus (, (V 0 , E 0 ), p) = H (p) and Condition (3) holds.
If instead H (p)  hg,iE g + s , then since (, (V 0 , E 0 ), p)  0, Condition (3) holds. 2
g

Theorem 11 The prices and allocation determined in quiescence by the SAMP-SB protocol is a
--competitive equilibrium, with g = max((g)  p(g), b ), iff no inactive producer buys any
positive-price input.
Proof. Case only if: Condition (1) of --competitive equilibrium (Definition 4) fails if an inactive producer buys any positive-price input.
Case if: Lemmas 29 and 30 show that the consumers and producers, respectively, obey the -competitive equilibrium conditions (Conditions (1)(3)). The (M+1)st-price auction rules ensure
Condition (4). All conditions of --competitive equilibrium are met. 2
A.8 Proof of Theorem 12
In proving the theorem, we refer to the conditions for a --competitive equilibrium (Definition 4).
Lemma 31 If (g)  p(g) > b for any good g in a quiescent state of SAMP-SB for network (V, E),
then no agent wins an offer for g.
Proof. Assume, contrary to which we wish to prove, that (g)  p(g) > b and some agent is
winning an offer for g, in quiescence of SAMP-SB. Either a buy offer or a sell offer sets (g).
Case 1: An agent sets (g) with a buy offer. According to the SAMP-SB bidding policies, an
agent will increase a buy offer only if it is losing that offer. An agent will win any offer for g at a
price above p(g). A producer increases its buy offer in increments of b and a consumer offers at
most p(g) + b . In either case, an agent will place a buy offer no higher than p(g) + b for g. But
then (g)  p(g) + b , which is a contradiction.
Case 2: An agent sets (g) with a sell offer. As with Case 1, there are no buy offers higher than
p(g) + b , hence every buy offer is strictly below (g). Recall that, if there are M sell offers, the
Mth highest offer determines (g). Then since there are no buy offers at or above (g), it must be
that all sell offers are at or above (g). But then all sell offers are strictly above all buy offers and
no agent wins an offer for g, which is a contradiction.
Since each case gives us a contradiction, it must be the case that no agent wins an offer for g
when (g)  p(g) > b . 2
Lemma 32 If (V 0 , E 0 ) is in --competitive equilibrium at prices p, in quiescence of SAMP-SB for
network (V, E), then there exist prices p0 such that (V 0 , E 0 ) is also in --competitive equilibrium at
p0 , with g = b for all producers  and goods g.
Proof. We specify p0 as follows: if (g) > p(g) + b , then p0 (g) = (g), otherwise p0 (g) = p(g).
We will show that all the conditions of --competitive equilibrium hold with g = b . Because we
are considering the same allocation, the goods are still in material balance so Condition 4 still holds
at p0 .
559

fiWALSH & W ELLMAN

Consider an agent a such that p0 (g) = p(g) for all adjacent goods g. Clearly, Ha (p0 ) = Ha (p)
and (a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p). We then have (a, (V 0 , E 0 ), p0 )  0 (Condition 1), and the
surplus bound is met for consumers (Condition 2) since these hold at p. If a is a producer, then
for any input g, (g)  p(g)  b since p0 (g) = p(g). Hence, we have the following bound on
its perceived cost for g: pa (g)  p(g) + b . As a result, the producer bidding policies imply that
(a, (V 0 , E 0 ), p)  H (p)  (hg,iE b + s ). Therefore (a, (V 0 , E 0 ), p0 )  H (p0 )  (hg,aiE ga +
0

s ) and and the producer surplus bound (Condition 3) holds with ga for all inputs g0 .
Now consider an agent a adjacent to a good g with p0 (g) = (g). By Lemma 31, a does not win
an offer for g, so (a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p), implying (a, (V 0 , E 0 ), p0 )  0 (Condition 1).
If a is a consumer, since p0 (g)  p(g), and since a does not win g, we have Ha (p0 ) = Ha (p), so the
surplus bound is met for consumers (Condition 2).
If a is a producer, then since it did not win g, it must not have won any good (according to
the --competitive equilibrium conditions and Theorem 11), implying (a, (V 0 , E 0 ), p0 ) = 0. The
producer bidding policy specifies that a offered a price at most  = a + hg0 ,aiE max((g0 ), p(g0 )+
b ) + s for its output ga . Since a did not win ga , it must be that (ga )  . But by the way
p0 is constructed, p0 (ga )  (ga ) and p0 (g0 ) + b  max((g0 ), p(g0 ) + b ), giving us p0 (ga ) 
(ga )    a + hg0 ,aiE (p0 (g0 ) + b ) + s . If a would optimize at p0 by being active, we have
Ha (p0 ) = p0 (ga )  a  hg0 ,aiE p0 (g0 )  hg0 ,aiE b + s . But since (a, (V 0 , E 0 ), p0 ) = 0 it follows
that (a, (V 0 , E 0 ), p0 )  Ha (p0 )  (hg0 ,aiE b + s ). If, on the other hand, a would optimize at p0 by
being inactive at, (a, (V 0 , E 0 ), p0 ) = Ha (p0 ). In either case, the surplus bound is met for producers
0
(Condition 3) with ga = b for all inputs g0 . 2
Theorem 12 If (V 0 , E 0 ) is a --competitive equilibrium computed by SAMP-SB then (V 0 , E 0 ) has
a nonnegative value that differs from the value of an efficient allocation by at most  (|{hg, i 
E}| b + s ) + |C|b .
Proof. By Lemma 32, there is a --competitive equilibrium for (V 0 , E 0 ) with  = b for all prog
ducers  and goods g. With b substituted for  in the equation from Theorem 3, we have proved
the present theorem. 2
g

A.9 Proof of Theorem 13
In proving the theorem, we refer to the S-level of producers in a network, as defined in the beginning
of Section A.
Theorem 13 The quiescent state of SAMP-SB is a --competitive equilibrium for a tree.
Proof. We prove, by induction on the S-level of producers, that no producer changes its initial
output offer. Since buy offers never decrease, it follows that, if a producer is winning its output, it
will not lose its output at any successive state of the run of the protocol. Since a producer bids for
its inputs only when winning its output, no inactive producer will buy any positive-price output and
the present theorem follows from Theorem 11.
Basis case: The bidding policy specifies that a producer at S-level zero never changes its initial
output offer.
560

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Inductive case: Assume that no producer at S-level less than k changes its initial output offer to
show that a producer  at S-level k never changes its initial output offer. Consider input good g with
M sell offers and the lowest sell offer . Since the network is a tree,  is the only agent that places
buy offers for g. Producer  initially offers zero for g, and so long as it offers less than  it loses
its offer, and p (g) = (g). While this holds, (g), defined as the Mth highest price, is the lowest
sell offer, hence p (g) = . As soon as  offers  or greater for g it will win its offer, and then
p (g) = p(g). When this holds, p(g), defined as the M + 1st highest price, is the lowest sell offer,
hence p (g) = . We conclude that p (g) never changes for any input g, hence  never changes its
initial output offer.
We have proven that no producer changes its initial output offer, and by the argument above, the
theorem is proven. 2
A.10 Proof of Theorem 14
Theorem 14 The quiescent state of safe SAMP-SB is a --competitive equilibrium for a network
with no input complementarities.
Proof. We will show that that no inactive producer buys its input at a positive price in quiescence of
safe SAMP-SB. Since the properties of safe SAMP-SB in quiescence are the same as in SAMP-SB,
the present theorem then follows from Theorem 11. Assume, to the contrary, that, in quiescence,
producer  wins its input g at a positive price but loses its offer for output g .
Let  be the price of the final offer by  for g, p(g) > 0 be the final price of g, and p (g) be the
final perceived cost of g to . Since  wins g in quiescence, p (g) = p(g). Let 0 be the price of the
second to last offer from . Immediately before  places offer , let p0 (g) be the perceived price
of g to  and p0 (g) be the price component from the price quote for g. According to the bidding
policy,  = 0 + b . Since  offers  only if it loses g with offer 0 , it must be that 0  p0 (g), hence
  p0 (g) + b . Furthermore, since  loses with offer 0 , we have p0 (g)  p0 (g) + b . Because we
assume that  wins g in quiescence, it must be that p(g)  , hence p(g)  p0 (g) + b . It follows
that, since p (g) = p(g) and p0 (g)  p0 (g) + b , we have p (g)  p0 (g).
According to the safe SAMP-SB bidding policies,  offers  for g only if it is first winning g
with offer price p0 (g). Since p (g)  p0 (g),  its offer for g is the same in quiescence as when it
had placed  for g. But since no offers from any agent decrease,  must continue to win its final offer
for g , contradicting the assumption that  loses g in quiescence. Thus,  does not win its input
at a positive price if it is inactive, and the quiescent state of safe SAMP-SB is a --competitive
equilibrium. 2
A.11 Proof of Theorem 15
In proving the theorem, we refer to the C-level and S-level of producers in a network, as defined in
the beginning of Section A.
Theorem 15 If (V, E) is a polytree with a solution that assigns good g to consumer c, then given all
other costs and values, there exists a value vc (g) such that SAMP-SB is guaranteed to converge to
a valid solution (V 0 , E 0 ) for c.

561

fiWALSH & W ELLMAN


Proof. For convenience, denote max V 0  , maxc0 C, hg0 ,c0 i6=hg,ci vc0 (g0 ) as . We show that
the theorem holds for:
vc (g) = [ + (2b + s )||] || + b .
We need to show that SAMP-SB cannot reach a state in which p(g) > vc (g)b and c is not winning
g, because then c would stop bidding for g and the desired solution would not form.
First, observe that for any consumer c0 and any good g0 such that hg0 , ai =
6 hg, ci, c0 will not offer
0
above  for g , by construction.
Now, consider a producer  such that there is no directed path from  to g through the output
of . We show, by induction on the C-level of producers, that no such producer offers higher than
 + b d , where d is the C-level of , for one of its inputs. For the basis case, such a producer  at
C-level one cannot win an output offer above  (by the definition of ).  increases its input offers in
increments of b , so to offer 0 >  + b , on any input g0 , it must first offer , where  <    + b
for that input.  will only offer 0 if it is losing  but winning its output offer. But if  is losing
, we must have p(g0 )  , so  must be offering more than  for its output. But then it cannot be
winning its output, hence would not offer 0 for g. Thus  at C-level one does not offer more than
 + b for any input, establishing the base case. Now, assume that the property holds for every such
producer at C-level less than k to show that it holds for producer  at C-level k. Given the inductive
assumption, it must be that  cannot win its output for more than  + b (k  1). By an argument
similar to the basis case,  does not offer above  + b k for its input, proving the inductive case.
Since d  ||, then no such producer offers higher than  + b || for its input.
For any producer    V 0 , denote as I the maximum number of producers, other than , in
the subgraph of (V, E), rooted at . Now we show by induction on the S-level, that a producer  on
a directed path to g offers no more than [ + b (|| + d ) + s (d  1)]I + s for its output, where
d is the S-level of . For the basis case, consider such a producer  at S-level one, offering to buy
some g0 . No consumer offers above  for g0 . Because (V, E) is a polytree, any other producer 0
that offers to buy g0 is not on a directed path to g, hence offers at most  + b || to buy g0 . Any
producer that offers to sell g0 must have no inputs, hence offers no more than  for g0 . Hence 0
can successfully buy g0 with a offer no higher than  + b (|| + 1), thus will offer no higher than
this amount for g0 . Since the the number of inputs to  is equal to I , it will offer no more than
( + b (|| + 1))I + s for its output, and the basis case is proven. Now, assume that the property
holds for every such producer at S-level less than k to prove that it holds for producer  offering to
buy some g0 at S-level k. By the inductive assumption, no producer  offers to sell g0 for more than
( + b (|| + k  1) + s (k  2))I + s . As in the basis case, no consumer offers more than  for
g0 and any producer other than  will offer no more than  + b || to buy g0 . Hence,  will offer at
most ( + b (|| + k  1) + s (k  2))I + s + b to buy g0 , and for its output will offer at most
"



h,g0 ipE | hg0 ,iE

#

( + b (|| + k  1) + s (k  2))I + s + b + s 
[ + b (|| + k) + s(k  1)]I + s ,

proving the inductive case. Since I  || and d  ||, then no such producer offers higher than
[ + 2b || + s (||  1)]|| + s  [ + (2b + s )||] || = vc (g)  b .
We have shown that no agent a 6= c places a buy offer as high as vc (g)  b for g and no producer
on a directed path to g places a sell offer as high as vc (g)  b for g. Hence c is the only agent that
562

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

could possibly offer as high as vc (g)  b for g. But c will offer this high, if necessary, to win g,
and will win g if it offers vc (g)  b or higher. It follows that c will win g at a price below vc (g) in
quiescence. By Observation 7 and Theorem 16, the state must be a valid solution. 2
A.12 Proof of Theorem 16
Theorem 16 If SAMP-SB reaches quasi-quiescence with p(g) < vc (g) for some hg, ci  E, c  C,
then the systems state represents a valid solution.
Proof. Because the definition of quasi-quiescence requires that active producers do not change
their bids, they must be feasible. All other agents are feasible by definition. The price of an active
producers output good must be no less than the total price of its input goods, otherwise it would
increase its output offer, violating quasi-quiescence.
Because p(g) < vc (g), consumer c must have won its offer for g. A consumer bids in such a way
that it wins only one unit of one good, and consumers do not change their bids in quasi-quiescence.
Finally, the auction guarantees that there is a one-to-one mapping between successful buy offers
and successful sell offers for any good, ensuring material balance.
Thus, each of the constraints for a valid solution is satisfied. 2
A.13 Proof of Theorem 17
Theorem 17 If a run of SAMP-SB in (V, E) is in a valid solution state such that:
 each consumer c is either winning an offer or p(g) + b > vc (g) for all hg, ci  E,
 all agents have correct beliefs about which goods they are currently winning,
 all bids from consumers and active producers have been received in response to the current
price quotes,
 and no sell offers are lost due to tie breaking,
then after the subsequent price quote from each auction, the system will be in a quasi-quiescent
state with a valid solution.
Proof. Let the current prices be p. The consumer bidding policy dictates that the consumers do
not change their offers under the specified conditions. Because we have a valid solution, each producer is feasible and thus will not raise any of its buy offers for inputs. Therefore, no agent changes
any buy offers.
An active producer  is feasible in a valid solution. Since  is winning all of its inputs, it only
raises its offer for output g if p(g) changes for once of its inputs g, and will place an offer for its
output a price no higher than the sum of its input good prices. By the definition of a valid solution,
if  is active, then the current price of its output good is no less than sum of the current prices of its
inputs. But since  won its offer for g , it must have offered no higher than p(g ) for g . Because
the previous offer price by  for g is no higher than p(g ), and because the sum of the current
prices of its inputs are no higher than p(g),  will offer no higher than p(g ) for g .
563

fiWALSH & W ELLMAN

We have established that no agent changes any buy offers, and no currently active producer
places a sell offer above p(g) for any good g. We show this implies that, at the next price quotes with
prices p0 , we have p0 (g) = p(g). Assume the contrary. Since offers do not decrease, p0 (g) > p(g).
Since no buy offer and no winning sell offer changed, the price increase is due an updated losing
sell offer at price , such that  = p0 (g). But if the agent was losing with a previous offer price
of 0 , it must be that 0 was at least as high as the (M + 1)st highest offer. Thus , being higher,
must be strictly higher than the (M + 1)st highest offer, hence cannot raise the price of g. Hence
p0 (g) = p(g).
Since prices do not change, the temporal-precedence tie breaking ensures that the set of winning
buy offers does not change. Additionally, since no winning seller offers above p(g) and no sell
offers are currently lost to tie breaking, the set of winning sell offers does not change. Since prices
and allocations do not change, no consumer or active producer will change its bids. Furthermore,
because the system is in a valid solution state based on the current price quotes, it must be in a valid
solution state based on the next price quotes. 2
We note that temporal-precedence tie-breaking itself (without the requirement that no tied sell
offers are lost) is not sufficient to ensure that the allocation to sellers does not change. If some tied
sell offers are lost, it is possible that an active producer could increase its next sell offer price up
to the price of its output good. If this occurs, then that producer would lose the tie breaking of its
output at the next quote, and the system would not be in quasi-quiescence.

References
Andersson, A., Tenhunen, M., & Ygge, F. (2000). Integer programming for combinatorial auction
winner determination. In Fourth International Conference on Multi-Agent Systems, pp. 39
46.
Andersson, M. R., & Sandholm, T. W. (1998). Leveled commitment contracting among myopic
individually rational agents. In Third International Conference on Multi-Agent Systems, pp.
2633.
Ausubel, L. M., & Milgrom, P. R. (2002). Ascending auctions with package bidding. Frontiers of
Theoretical Economics, 1(1).
Axelrod, R. (1987). The evolution of strategies in the iterated prisoners dilemma. In Davis, L.
(Ed.), Genetic Algorithms and Simulated Annealing, chap. 3, pp. 3241. Morgan Kaufmann.
Babaioff, M., & Nisan, N. (2001). Concurrent auctions across the supply chain. In Third ACM
Conference on Electronic Commerce, pp. 110.
Babaioff, M., & Walsh, W. E. (2003). Incentive-compatible, budget-balanced, yet highly efficient
auctions for supply chain formation. In Fourth ACM Conference on Electronic Commerce,
pp. 6475.
Baker, A. D. (1996). Metaphor or reality: A case study where agents bid with actual costs to
schedule a factory. In Clearwater (Clearwater, 1996).
Bichler, M. (2001). The Future of e-Markets: Multidimensional Market Mechanisms. Cambridge
University Press.
Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium in an exchange economy with
indivisibilities. Journal of Economic Theory, 74, 385413.
564

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Bikhchandani, S., & Ostroy, J. M. (2002). The package assignment model. Journal of Economic
Theory, 107, 377406.
Borenstein, S., & Saloner, G. (2001). Economics and electronic commerce. Journal of Economic
Perspectives, 15(1), 312.
Clarke, E. H. (1971). Multipart pricing of public goods. Public Choice, 11, 1733.
Clearwater, S. (Ed.). (1996). Market-Based Control: A Paradigm for Distributed Resource Allocation. World Scientific.
Davidow, W. H. (1992). The Virtual Corporation: Structuring and Revitalizing the Corporation for
the 21st Century. HarperCollins Publishers.
Davis, R., & Smith, R. G. (1983). Negotiation as a metaphor for distributed problem solving.
Artificial Intelligence, 20, 63109.
Dellarocas, C., Klein, M., & Rodriguez-Aguilar, J. A. (2000). An exception-handling architecture
for open electronic marketplaces of Contract Net software agents. In Second ACM Conference
on Electronic Commerce, pp. 225232.
Demange, G., & Gale, D. (1985). The strategy structure of two-sided matching markets. Econometrica, 53(4), 873888.
Demange, G., Gale, D., & Sotomayor, M. (1986). Multi-item auctions. Journal of Political Economy, 94(4), 863872.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning about Knowledge. MIT
Press.
Friedman, D., & Rust, J. (Eds.). (1993). The Double Auction Market: Institutions, Theories, and
Evidence. Addison-Wesley.
Fudenberg, D., & Tirole, J. (1998). Game Theory. The MIT Press.
Greenwald, A. (2003). The 2002 trading agent competition: An overview of agent strategies. AI
Magazine, 24(1), 7782.
Groves, T. (1973). Incentives in teams. Econometrica, 41(4), 617631.
Gul, F., & Stacchetti, E. (2000). English and double auctions with differentiated commodities.
Journal of Economic Theory, 92, 6695.
Gul, F., & Stacchetti, E. (1999). Walrasian equilibrium with gross substitutes. Journal of Economic
Theory, 87, 95124.
Hunsberger, L., & Grosz, B. J. (2000). A combinatorial auction for collaborative planning. In
Fourth International Conference on MultiAgent Systems, pp. 151158.
Joshi, P., Oke, M., Sharma, V., & Veeramani, D. (1999). Issues in dynamic and highly-distributed
configuration of supply webs. In First IAC Workshop on Internet-Based Negotiation Technologies.
Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, and gross substitutes.
Econometrica, 50(6), 14831504.
Kjenstad, D. (1998). Coordinated Supply Chain Scheduling. Ph.D. thesis, Norwegian University of
Science and Technology.
565

fiWALSH & W ELLMAN

Klemperer, P. (1999). Auction theory: A guide to the literature. Journal of Economic Surveys, 13(3),
227286.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards a universal test suite for combinatorial auction algorithrms. In Second ACM Conference on Electronic Commerce, pp. 6676.
Lucking-Reily, D., & Spulber, D. F. (2001). Business-to-business electronic commerce. Journal of
Economic Perspectives, 15(1), 5568.
MacKie-Mason, J. K., & Varian, H. R. (1994). Generalized Vickrey auctions. Tech. rep., Dept. of
Economics, Univ. of Michigan.
Malone, T. W., & Laubacher, R. J. (1998). The dawn of the e-lance economy. Harvard Business
Review, 145152.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University
Press, New York.
McAfee, R. P. (1992). A dominant strategy double auction. Journal of Economic Theory, 56,
434450.
McAfee, R. P., & McMillan, J. (1987). Auctions and bidding. Journal of Economic Literature, 25,
699738.
Milgrom, P. (2000). Putting auction theory to work: The simultaneous ascending auction. The
Journal of Political Economy, 108(2), 245272.
Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms for bilateral trading. Journal
of Economic Theory, 29, 265281.
Parkes, D. C., Kalagnanam, J., & Eso, M. (2001). Achieving budget-balance with Vickrey-based
payment schemes in exchanges. In Seventeenth International Joint Conference on Artificial
Intelligence, pp. 11611168.
Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory and practice. In
Seventeenth National Conference on Artificial Intelligence, pp. 7481.
Parkes, D. C., & Ungar, L. H. (2002). An ascending-price generalized Vickrey auction. In Stanford
Institute for Theoretical Economics Summer Workshop on The Economics of the Internet.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter. MIT Press.
Rust, J., Miller, J. H., & Palmer, R. (1994). Characterizing effective trading strategies: Insights from
a computerized double auction tournament. Journal of Economic Dynamics and Control, 18,
6196.
Sandholm, T., & Suri, S. (2000). Improved algorithms for optimal winner determination in combinatorial auctions and generalizations. In Seventeenth National Conference on Artificial
Intelligence, pp. 9097.
Sandholm, T. W. (1993). An implementation of the CONTRACT NET protocol based on marginal
cost calculations. In Eleventh National Conference on Artificial Intelligence, pp. 256262.
Satterthwaite, M. A., & Williams, S. R. (1989). Bilateral trade with the sealed bid k-double auction:
Existence and efficiency. Journal of Economic Theory, 48, 107133.
Satterthwaite, M. A., & Williams, S. R. (1993). The Bayesian theory of the k-double auction. In
Friedman, & Rust (Friedman & Rust, 1993), chap. 4, pp. 99123.
566

fiD ECENTRALIZED S UPPLY C HAIN F ORMATION

Shapiro, C., & Varian, H. R. (1999). Information Rules. Harvard Business School Press.
Stone, P., & Greenwald, A. (2000). The first international trading agent competition: Autonomous
bidding agents. Journal of Electronic Commerce Research, to appear.
Veeramani, D., Joshi, P., & Sharma, V. (1999). Critical research issues in agent-based manufacturing
supply webs. In Agents-99 Workshop on Agents for Electronic Commerce and Managing the
Internet-Enabled Supply Chain.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. Journal of Finance, 16, 837.
Walsh, W. E. (2001). Market Protocols for Decentralized Supply Chain Formation. Ph.D. thesis,
University of Michigan.
Walsh, W. E., & Wellman, M. P. (1999). Efficiency and equilibrium in task allocation economies
with hierarchical dependencies. In Sixteenth International Joint Conference on Artificial Intelligence, pp. 520526.
Walsh, W. E., Wellman, M. P., & Ygge, F. (2000). Combinatorial auctions for supply chain formation. In Second ACM Conference on Electronic Commerce, pp. 260269.
Walsh, W. E., Yokoo, M., Hirayama, K., & Wellman, M. P. (2003). On market-inspired approaches
to propositional satisfiability. Artificial Intelligence, 144, 125156.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003). The 2001 trading agent competition. Electronic Markets, 13(1), 412.
Wellman, M. P., & Walsh, W. E. (2000). Distributed quiescence detection in multiagent negotiation.
In Fourth International Conference on Multi-Agent Systems, pp. 317324.
Wellman, M. P., Walsh, W. E., Wurman, P. R., & MacKie-Mason, J. K. (2001a). Auction protocols
for decentralized scheduling. Games and Economic Behavior, 35(1/2), 271303.
Wellman, M. P., Wurman, P. R., OMalley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh, W. E.
(2001b). Designing the market game for a trading agent competition. IEEE Internet Computing, 5(2), 4351.
Wurman, P. R., Walsh, W. E., & Wellman, M. P. (1998). Flexible double auctions for electronic
commerce: Theory and implementation. Decision Support Systems, 24, 1727.
Wurman, P. R., & Wellman, M. P. (2000). AkBA: A progressive, anonymous-price combinatorial
auction. In Second ACM Conference on Electronic Commerce, pp. 2129.
Wurman, P. R., Wellman, M. P., & Walsh, W. E. (2001). A parametrization of the auction design
space. Games and Economic Behavior, 35(1/2), 304338.
Ygge, F. (1998). Market-Oriented Programming and its Application to Power Load Management.
Ph.D. thesis, Lund University.

567

fiJournal of Artificial Intelligence Research 19 (2003) 279-314

Submitted 09/02; published 10/03

Compiling Causal Theories to Successor State Axioms and
STRIPS-Like Systems
Fangzhen Lin

flin@cs.ust.hk

Department of Computer Science
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong

Abstract
We describe a system for specifying the effects of actions. Unlike those commonly
used in AI planning, our system uses an action description language that allows one to
specify the effects of actions using domain rules, which are state constraints that can
entail new action effects from old ones. Declaratively, an action domain in our language
corresponds to a nonmonotonic causal theory in the situation calculus. Procedurally, such
an action domain is compiled into a set of logical theories, one for each action in the domain,
from which fully instantiated successor state-like axioms and STRIPS-like systems are then
generated. We expect the system to be a useful tool for knowledge engineers writing action
specifications for classical AI planning systems, GOLOG systems, and other systems where
formal specifications of actions are needed.

1. Introduction
We describe a system for generating action effect specifications from a set of domain rules
and direct action effect axioms, among other things. We expect the system to be a useful
tool for knowledge engineers writing action specifications for classical AI planning systems,
GOLOG systems (Levesque et al., 1997), and other systems where formal specifications of
actions are needed.
To motivate, consider the language used by STRIPS (Fikes & Nilsson, 1971) for describing the effects of actions. Briefly speaking, an action is described in this language by
a first-order formula, called its precondition that describes the condition under which the
action is executable, an add list that enumerates the propositions that the action will make
true when successfully executed in a situation, and a delete list that enumerates the propositions that the action will make false when successfully executed in a situation. While the
original STRIPS allowed the precondition and the elements of the two lists to be complex
formulas, STRIPS actions now refer only to those whose precondition is given by a conjunction of atomic formulas and whose add and delete lists are lists of atomic formulas.
It is widely acknowledged that this language is inadequate for describing actions in the
real world. One of the limitations, the one that we address in this paper, is that with the
language, one has to enumerate all possible effects of an action, a difficult if not impossible
task for complex domains. For example, given a large C program, it is hard to figure out the
effects of changing the value of a pointer on the values of all other pointers in the program.
However, the underlying principle is very simple: when the value of a pointer changes, the
values of all other pointers that point to the same memory location change as well. Put
another way, the direct effect of the action of changing the value of a pointer to x is that the
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLin

value of the pointer will be x. The indirect or side effects of this action are those derived
from the constraint which says that if two pointers point to a common location, then their
values must be the same.
This idea of specifying the effects of actions using domain constraints is like engineering
from first principle, and has many advantages. First of all, constraints are action independent, and work on all actions. Secondly, if the effects of actions derived from domain
constraints agree with ones expectation, then this will be a good indication that one has
axiomatized the domain correctly. Finally, domain constraints can be used for other purposes as well. For instance, they can be used to check the consistency of the initial situation
database. In general, when a set of sentences violates a domain constraint, we know that no
legal situation can satisfy this set of sentences. This idea can and has been used in planning
to prune impossible states. Recently, there have even been efforts at reverse engineering
domain constraints from STRIPS-like systems to speed up planners (e.g. Zhang & Foo,
1997; Gerevini & Schubert, 1998; Fox & Long, 1998, and others).
While it is appealing to use domain constraints to derive the indirect effects of actions,
making the idea work formally turned out to be a challenge. The problem is commonly
known as the ramification problem, and various proposals have been made to solve it. Until
recently, however, these proposals were at best of theoretical interest only because of their
high computational complexity. The situation has since changed substantially due to the
use of causality in representing domain constraints (Lin, 1995, 1996; McCain & Turner,
1995, 1997; Thielscher, 1995, 1997; Baral, 1995; Lifschitz, 1997, and others). What we will
describe in this paper is an implemented system that builds on this recent work on causalitybased approaches to the ramification problem. Specifically, our system takes as input an
action domain description where actions are described by their precondition axioms and
direct effect axioms, and domain constraints are represented by what we call domain rules.
The system returns as output a complete action specification both in STRIPS-like format
and as a set of fully instantiated successor state axioms (Reiter, 1991).
This paper is organized as follows. We begin by introducing our action description language. We then propose a procedure to compile an action domain specified in this language
into a complete set of successor state axioms from which a STRIPS-like description is then
extracted. We then show the soundness of this procedure with respect to a translation from
action domain descriptions to situation calculus causal theories of Lin (1995). We next
describe an implementation of this procedure, and present some experimental results. As
one will see, one of the limitations of our system is that it is essentially propositional. While
effect axioms and domain rules can have variables, they need to be fully instantiated during
the compilation process. To partially overcome this limitation, we show some results that
allow one to generalize the propositional output to the first-order case for certain classes of
action domain descriptions. We then discuss some related work, and conclude this paper
with some pointers for future work.

2. An Action Description Language
We assume a first-order language with equality. We shall call those predicates whose extensions may be changed by actions fluents, and those whose extensions are not changed by
any actions static relations. We also call unary static relations types. By fluent atoms we
280

fiFrom Causal Theories to STRIPS-Like Systems

mean those atomic formulas formed by fluents. An equality atom is one of the form u = v,
where u and v are variables or constants, and an inequality constraint is one of the form
u 6= v. Actions are represented by functions, and they are assumed to be the only functions
with positive arities in the language.
Our action description language includes the following components.
2.1 Type Definitions
A type definition is specified by expressions of the following form:
Domain(p, {a1 , ..., an }),
where p is a type, and a1 , ..., an are constants. The intuitive meaning of this expression is
that the domain (extension) of the type p is the set {a1 , ..., an }. For instance, in the blocks
world, we may have a type called block, and have, say, five blocks named numerically:
Domain(block, {1, 2, 3, 4, 5}). In a logistics domain, we may have a type called loc for
locations, and have, say, 3 locations l1 , l2 , and l3 : Domain(loc, {l1 , l2 , l3 }).
2.2 Primitive Fluent Definitions
Primitive fluents are defined by expressions of the following form:
Fluent(f (x1 , ..., xn ), p1 (x1 )      pn (xn )  e1      em ),
where f is an n-ary fluent, each pi , 1  i  n, a type, and each ei , 1  i  m, an
inequality constraint of the form xj 6= xk , for some 1  j < k  n. The intuitive meaning
of this expression is that f (x1 , ..., xn ) is a legal fluent atom if the second argument is true.
For instance, in the blocks world, given the type definition Domain(block, {1, 2, 3}), the
following fluent specification:
Fluent(on(x, y), block(x)  block(y)  x 6= y)
would generate the following six legal fluent atoms:
on(1, 2), on(1, 3), on(2, 1), on(2, 3), on(3, 1), on(3, 2).
Clearly, there should be exactly one fluent definition for each fluent.
2.3 Complex Fluent Definitions
Given a set of primitive fluents, one may want to define some new ones. For instance,
in the blocks world, given the primitive fluent on, we can define clear in terms of on as:
(x)clear(x)  (y)on(y, x).
To specify complex fluents like clear, we first define fluent formulas as follows:
 t1 = t2 is a fluent formula, where t1 and t2 are terms, i.e. either a constant in the
domain of a type or a variable.
 f (t1 , ..., tn ) is a fluent formula, where t1 , ..., tn are terms, and f is either an n-ary
primitive fluent, a complex fluent, or a static relation.
281

fiLin

 If  and 0 are fluent formula, then ,   0 ,   0 ,   0 , and   0 are also
fluent formulas.
 If  is a fluent formula, x a variable, and p a type, then (x, p) (for all x of type p,
 holds) and (x, p) (for some x of type p,  holds) are fluent formulas. Notice that
we require types to have finite domains, so these quantifications are really shorthands:
If the domain of p is {a1 , ..., an }, then (x, p) stands for
(x/a1 )      (x/an ),
and (x, p) stands for
(x/a1 )      (x/an ).
A complex fluent is then specified in our language by a pair of expressions of the following
form:
Complex(f (x1 , ..., xn ), p1 (x1 )      pn (xn )  e1     em ),
Defined(f (x1 , ..., xn ), ),
where pi s and ei s are the same as in primitive fluent definitions, and  a fluent formula
that does not mention any complex fluents and whose free variables are among x1 , ..., xn .
The first expression specifies the syntax and the second the semantics of the complex fluent.
For instance, the complex fluent clear in the blocks world can be specified as:
Complex(clear(x), block(x)),
Defined(clear(x), (y, block)on(y, x)).
As we mentioned above, quantifiers here are just shorthands because each type must have
a finite domain. For instance, given the following specification:
Domain(block, {1, 2, 3}),
Fluent(on(x, y), block(x)  block(y))
the above fluent definition for clear will be expanded to:
Defined(clear(1), (on(1, 1)  on(2, 1)  on(3, 1))),
Defined(clear(2), (on(1, 2)  on(2, 2)  on(3, 2))),
Defined(clear(3), (on(1, 3)  on(2, 3)  on(3, 3))).
2.4 Static Relation Definitions
As we mentioned, a static relation is one that is not changed by any action in the domain.
For instance, in the robot navigation domain, we may have a proposition connected(d, r1, r2)
meaning that door d connects rooms r1 and r2. The truth value of this proposition cannot
be changed by the navigating robot that just rolls from room to room.
In our language, a static relation is defined by an expression of the following form:
Static(g(x1 , ..., xn ), p1 (x1 )      pn (xn )  e1      em ),
where g is an n-ary predicate, and pi s and ei s are the same as in primitive fluent definitions.
The meaning of this expression is similar to a fluent definition, and there should be exactly
one definition for each static relation.
282

fiFrom Causal Theories to STRIPS-Like Systems

2.5 Domain Axioms
Domain axioms are constraints on static relations. For instance, for the static proposition
connected(d, r1, r2), we may want to impose the following constraint: connected(d, r1, r2) 
connected(d, r2, r1). In our language, domain axioms are specified by expressions of the
form:
Axiom(),
where  is a fluent formula that does not mention any fluents, i.e. it mentions only static
relations and equality. For instance, the above constraint on connected is written as:
Axiom((d, door)(r1 , room)(r2 , room)connected(d, r1 , r2 ) 
connected(d, r2 , r1 )),
where door and room are types.
2.6 Action Definitions
Actions are defined by expressions of the following form:
Action(a(x1 , ..., xn ), p1 (x1 )      pn (xn )  e1      em ),
where a is an n-ary action, and pi s and ei s are the same as in primitive fluent definitions.
For instance, in the blocks world, given the type definition Domain(block, {1, 2, 3}), the
following action specification:
Action(stack(x, y), block(x)  block(y)  x 6= y)
would generate the following six action instances:
stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2).
There should be exactly one action definition for each action.
2.7 Action Precondition Definitions
Action precondition definitions are specified by expressions of the following form:
P recond(a(x1 , ..., xn ), ),
where a is an n-ary action,  is a fluent formula whose free variables are among x1 , ..., xn .
There should be exactly one precondition definition for each action. For instance, in the
blocks world, we may have:
P recond(stack(x, y), clear(x)  clear(y)  ontable(x)),
which says that for the action stack(x, y) to be executable in a situation, clear(x), clear(y),
and ontable(x) must be true in it.
283

fiLin

2.8 Action Effect Specifications
Action effects are specified by expressions of the following form:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
or of the form:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
where  is a fluent formula, and f a primitive fluent. The intuitive meaning of these
expressions is that if  is true in the initial situation, then action a(x1 , ..., xn ) will cause
f (y1 , ..., yk ) to be true (false). For instance, in the blocks world, action stack(x, y) causes
x to be on y:
Effect(stack(x, y), true, on(x, y)).
For an example of a context dependent effect, consider action drop(x) that breaks an object
only if it is fragile:
Effect(drop(x), f ragile(x), broken(x)).
Notice here that the fluent formula  in the action effect specifications can have variables that are not in x1 , ..., xn , y1 , ..., yk . Informally, all the variables are supposed to be
universally quantified. More precisely, when these expressions are instantiated, one can
substitute any objects for these variables, provided the resulting formulas are well-formed.
For instance, given the action effect specification Effect(move(x), g(x1 )  q(x1 , x2 ), f (y)),
one can instantiate it to: Effect(move(a), g(b)  q(b, c), f (d)), as long as move(a) is a legal
action (according to the action definition for move) and g(b), q(b, c), and f (d) are legal
fluent atoms (according to the fluent definitions for g, q, and f ).
2.9 Domain Rules
Domain rules are specified by expressions of the following form:
Causes(, f (x1 , ..., xn )),
or of the following form:
Causes(, f (x1 , ..., xn )),
where  is a fluent formula, and f a primitive fluent. Like action effect specifications, 
here can have variables that are not in x1 , ..., xn . The intuitive meaning of a domain rule
is that in any situation, if  holds, then the fluent atom f (x1 , ..., xn ) will be caused to be
true. A domain rule is stronger than material implication. Its formal semantics is given
by mapping it to a causal rule of Lin (1995) (see Section 4), thus the name causes. For
instance, in the blocks world, a block can be on only one other block:
Causes(on(x, y)  y 6= z, on(x, z)).
In a logistics domain, one may want to say that if a package is inside a truck which is at
location l, then the package is at location l as well:
Causes(in(x, y)  at(y, l), at(x, l)).
284

fiFrom Causal Theories to STRIPS-Like Systems

2.10 Action Domain Descriptions
The following definition sums up our action description language:
Definition 1 An action domain description is a set of type definitions, primitive fluent
definitions, complex fluent definitions, static proposition definitions, domain axioms, action
definitions, action precondition definitions, action effect specifications, and domain rules.
Example 1 The following action domain description defines a blocks world with three
blocks:
Domain(block, {1, 2, 3}),
Fluent(on(x, y), block(x)  block(y)),
Fluent(ontable(x), block(x)),
Complex(clear(x), block(x)),
Defined(clear(x), (y, block)on(y, x)),
Causes(on(x, y)  x 6= z, on(z, y)),
Causes(on(x, y)  y 6= z, on(x, z)),
Causes(on(x, y), ontable(x)),
Causes(ontable(x), on(x, y)),
Action(stack(x, y), block(x)  block(y)  x 6= y),
P recond(stack(x, y), ontable(x)  clear(x)  clear(y)),
Effect(stack(x, y), true, on(x, y)),
Action(unstack(x, y), block(x)  block(y)  x 6= y),
P recond(unstack(x, y), clear(x)  on(x, y)),
Effect(unstack(x, y), true, ontable(x)),
Action(move(x, y, z), block(x)  block(y)  block(z)  x 6= y  x 6= z  y 6= z),
P recond(move(x, y, z), on(x, y)  clear(x)  clear(z)),
Effect(move(x, y, z), true, on(x, z)).

3. A Procedural Semantics
Given an action domain description D, we use the following procedure called CCP (Causal
Completion Procedure) to generate a complete action effect specification:
1. Use primitive and complex fluent definitions to generate all legal fluent atoms. In the
following let F be the set of fluent atoms so generated.
2. Use action definitions to generate all legal action instances, and for each such action
instance A do the following.
285

fiLin

2.1. For each primitive fluent atom F  F, collect all ground instances1 of As positive
effects:
Effect(A, 1 , F ),    , Effect(A, n , F ),
all ground instances of As negative effects:
Effect(A, 1 , F ),    , Effect(A, m , F ),
all ground instances of positive domain rules:
Causes(01 , F ),    , Causes(0k , F ),
all ground instances of negative domain rules:
Causes(01 , F ),    , Causes(0l , F ),
and generate the following pseudo successor state axiom;
succ(F )  init(1 )      init(n )  succ(01 )      succ(0l ) 
init(F )  [init(1 )      init(m ) 
succ(01 )      succ(0k )],

(1)

where for any fluent formula , init() is the formula obtained from  by replacing every fluent atom f in  by init(f ), and similarly succ() is the formula
obtained from  by replacing every fluent atom f in  by succ(f ). Intuitively,
init(f ) means that f is true in the initial situation, and succ(f ) that f is true
in the successor situation of performing action A in the initial situation.
2.2. Let Succ be the set of pseudo successor state axioms, one for each primitive
fluent F , generated by the last step, Succ1 the following set of axioms:
Succ1 = {succ(F )  succ() | Defined(F, ) is a complex fluent definition},
and Init the following set of axioms:
Init = { | Axiom() is a domain axiom} 
{init()  init(F ) | Causes(, F ) is a domain rule} 
{init()  init(F ) | Causes(, F ) is a domain rule} 
{init(F )  init() | Defined(F, ) is a complex fluent definition} 
{init(A ) | P recond(A, A ) is the precondition definition for A}.
For each fluent atom F , if there is a formula F such that
Init  Succ  Succ1 |= succ(F )  F ,
and F does not mention propositions of the form succ(f ), then output the
axiom
succ(F )  F .
1. When generating ground instances, all shorthands like (x, p) are expanded. See the definition of fluent
formulas in the last section.

286

fiFrom Causal Theories to STRIPS-Like Systems

Otherwise, the action As effect on F is indeterminate. In this case, output two
axioms:
succ(F )  F ,
F  succ(F ),
where F is a strongest formula satisfying the first implication, and F a weakest
formula satisfying the second implication. In the following, to be explicit about
the action A for which we are computing its effects, we will write the axioms as
SuccA , InitA , and Succ1A .
Conceptually, Step 2.1 in the above procedure is most significant. In the next section,
we shall prove that this step is provably correct under a translation to the situation calculus
causal theories of Lin (1995). Computationally, Step 2.2 is the most expensive. We shall
describe the strategies that our system uses to implement it in Section 5.
For this procedure to work properly, the action domain description should satisfy the
following conditions.
1. We require that all fluent atoms in Init, Succ, and Succ1 be among those generated
in Step 1. This would rule out cases like
Fluent(on(x, y), block(x)  block(y)  x 6= y)
together with Defined(clear(x), (y, block)on(y, x)), as the latter would generate
fluent atoms of the form on(x, x) which are ruled out by the fluent definition for on.
Here one could either drop the inequality constraint in the definition of on or change
the complex fluent definition into Defined(clear(x), (y, block)(on(y, x)  x 6= y)).
We could have built in a test in our procedure above to reject an action domain
description with incoherent fluent definitions like this. One easy way of making sure
this does not happen is not to use inequality constraints in the definition of fluents.
2. As we mentioned above, for each action there should be exactly one action precondition that captures exactly the conditions under which the action is executable.
When the action precondition is given explicitly like this, one needs to be careful
in writing action effect axioms and domain rules so that no contradictory effects
would be generated. For instance, given P recond(A, true), the action effect axioms
Effect(A, true, F ) and Effect(A, true, F ) are clearly not realizable simultaneously.
Similarly, if Causes(true, F ) is given as a domain rule, then one should not write the
effect axiom Effect(A, true, F ). Had we not insisted that A be always executable, we
could simply conclude that A is not executable when its effect axioms are in contradiction or when some of its effect axioms contradict domain rules. It remains future
work to extend our procedure to allow for automatic generation of these implicitly
given action preconditions. For now, we shall assume that the given action domain
specification is consistent in the sense that for each action instance A generated in
Step 1, the following theory
Init  Succ  Succ1  {init()  succ(F ) |
Effect(A, , F ) is a ground instance of effect axiom}
287

fiLin

is consistent.
3. On a related point, our procedure assumes that information about the initial situation
is given by Init. In particular, action effect axioms should not entail any information about the initial situation. For instance, given Causes(q, p), Effect(A, true, p),
and P recond(A, true), it must be that in the initial situation, q cannot be true, for
otherwise, it will persist into the next situation, causing p to be false, which contradicts the action effect. Formally, this means that given any set I of atoms of the
form init(f ), where f is a primitive fluent atom, if I  I   Init is consistent, then
I I  SuccSucc1 is also consistent, where I  , the complement of I, is the following
set:
{init(f ) | init(f ) 6 I and
f is a primitive fluent atom generated by Step 1}
Notice that for a similar reason, Reiter needed what he called the consistency assumption in order for his completion procedure to be sound and complete for generating
successor state axioms (Reiter, 1991).
While our action domain descriptions are clearly targeted at specifying deterministic actions,
some indeterminate effects can sometimes arise from cyclic domain rules. For instance,
consider the following action domain description:
Causes(p, p),
P recond(A, true)
For action A, Init is a tautology, Succ1 is empty, and Succ consists of the following pseudosuccessor state axiom for p:
succ(p)  succ(p)  init(p),
which is equivalent to init(p)  succ(p). So if initially p is true, then after A is performed,
we know that p will continue to be true. But if p is initially false, then after A is performed,
we do not know if p is true or not.
Example 2 Consider the blocks world description in Example 1. The set of fluent atoms
generated by Step 1 is:
F = {on(1, 1), on(1, 2), on(1, 3), on(2, 1), on(2, 2), on(2, 3),
on(3, 1), on(3, 2), on(3, 3), clear(1), clear(2), clear(3),
ontable(1), ontable(2), ontable(3)}.
Step 2 generates the following action instances:
stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2),
unstack(1, 2), unstack(1, 3), unstack(2, 1), unstack(2, 3), unstack(3, 1),
unstack(3, 2), move(1, 2, 3), move(1, 3, 2), move(2, 1, 3), move(2, 3, 1),
move(3, 1, 2), move(3, 2, 1)
288

fiFrom Causal Theories to STRIPS-Like Systems

For each of these action instances, we need to go through Steps 2.1 and 2.2. For instance,
for stack(1, 2), there is only one effect axiom about on(1, 2):
Effect(stack(1, 2), true, on(1, 2)),
and the following causal rules about on(1, 2):
Causes(on(1, 1), on(1, 2)),
Causes(on(1, 3), on(1, 2)),
Causes(on(2, 2), on(1, 2)),
Causes(on(3, 2), on(1, 2)),
Causes(ontable(1), on(1, 2)).
Therefore Step 2.1 generates the following pseudo-successor state axiom for on(1, 2):
succ(on(1, 2))  true 
init(on(1, 2))  [succ(on(1, 1))  succ(on(1, 3)) 
succ(on(2, 2))  succ(on(3, 2)  succ(ontable(1))].
We can similarly generate the following pseudo-successor state axioms for the other primitive
fluent atoms:
succ(on(1, 1))  init(on(1, 1))  [succ(on(1, 2))  succ(on(1, 3)) 
succ(on(2, 1))  succ(on(3, 1)  succ(ontable(1))],
succ(on(1, 3))  init(on(1, 3))  [succ(on(1, 1))  succ(on(1, 2)) 
succ(on(2, 3))  succ(on(3, 3))  succ(ontable(1))],
succ(on(2, 1))  init(on(2, 1))  [succ(on(1, 1))  succ(on(3, 1)) 
succ(on(2, 2))  succ(on(2, 3))  succ(ontable(2))],
succ(on(2, 2))  init(on(2, 2))  [succ(on(1, 2))  succ(on(1, 3)) 
succ(on(2, 1)  succ(on(2, 3))  succ(ontable(2))],
succ(on(2, 3))  init(on(2, 3))  [succ(on(1, 3))  succ(on(3, 3)) 
succ(on(2, 1)  succ(on(2, 2))  succ(ontable(2))],
succ(on(3, 1))  init(on(3, 1))  [succ(on(3, 2))  succ(on(3, 3)) 
succ(on(1, 1)  succ(on(1, 3))  succ(ontable(1))],
succ(on(3, 2))  init(on(3, 2))  [succ(on(3, 1))  succ(on(3, 3)) 
succ(on(1, 2)  succ(on(2, 2))  succ(ontable(2))],
succ(on(3, 3))  init(on(3, 3))  [succ(on(3, 1))  succ(on(3, 2)) 
succ(on(1, 3)  succ(on(2, 3))  succ(ontable(3))],
succ(ontable(1)) 
init(ontable(1))  [succ(on(1, 2))  succ(on(1, 1))  succ(on(1, 3))],
succ(ontable(2)) 
init(ontable(2))  [succ(on(2, 1))  succ(on(2, 2))  succ(on(2, 3))],
289

fiLin

succ(ontable(3)) 
init(ontable(3))  [succ(on(3, 1))  succ(on(3, 2))  succ(on(3, 3))].
For the complex fluent clear, its definition yields the following axioms:
succ(clear(1))  succ(on(1, 1))  succ(on(2, 1))  succ(on(3, 1)),
succ(clear(2))  succ(on(1, 2))  succ(on(2, 2))  succ(on(3, 2)),
succ(clear(3))  succ(on(1, 3))  succ(on(2, 3))  succ(on(3, 3)).
We can then solve these pseudo-successor state axioms and generate the following successor state axioms:
succ(on(1, 1))  f alse
succ(on(1, 3))  f alse
succ(on(2, 2))  f alse
succ(on(3, 1))  f alse
succ(on(3, 3))  init(on(3, 3))
succ(ontable(2))  init(ontable(2))
succ(clear(1))  init(clear(1))
succ(clear(3))  init(clear(3))

succ(on(1, 2))  true
succ(on(2, 1))  f alse
succ(on(2, 3))  init(on(2, 3))
succ(on(3, 2))  f alse
succ(ontable(1))  f alse
succ(ontable(3))  init(ontable(3))
succ(clear(2))  f alse

Once we have a set of these fully instantiated successor state axioms, we then generate
STRIPS-like descriptions like the following:
stack(1, 2)
Preconditions:
Add list:
Delete list:
Cond. effects:
Indet. effects:

ontable(1), clear(1), clear(2).
on(1, 2).
ontable(1), clear(2).
none.
none.

stack(1, 3)
Preconditions:
Add list:
Delete list:
Cond. effects:
Indet. effects:

ontable(1), clear(1), clear(3).
on(1,3).
ontable(1), clear(3).
none.
none.

290

fiFrom Causal Theories to STRIPS-Like Systems

We have the following remarks:
 Although we generate the axiom succ(on(1, 3))  f alse for stack(1, 2), we do not put
on(1, 3) into its delete list. This is because we can deduce init(on(1, 3))  f alse from
Init as well. A fluent atom is put into the add or the delete list of an action only if
this fluent atoms truth value is definitely changed by the action. See Section 5 for
more details about how a STRIPS-like description is generated from successor state
axioms.
 As one can see, our CCP procedure crucially depends on the fact that each type has
a finite domain so that all reasoning can be done in propositional logic. This is a
limitation of our current system, and this limitation is not as bad as one might think.
First of all, typical planning problems all assume finite domains, and changing the
domain of a type in an action description is easy - all one needs to do is to change the
corresponding type definition. More significantly, a generic action domain description
can often be obtained from one that assumes a finite domain. In our blocks world
example, the numbers 1, 2, and 3 are generic names, and can be replaced by
parameters. For instance, if we replace 1 by x and 2 by y in the above STRIPSlike description of stack(1, 2), we will get a STRIPS-like description for stack(x, y)
that works for any x and y. We have found that this is a strategy that often works in
planning domains.

4. Formal Semantics
The formal semantics of an action domain description is defined by translating it into a
situation calculus causal theory of Lin (1995). We shall show that the procedure CCP given
above is sound under this semantics.
This section is mainly for those who are interested in nonmonotonic action theories. For
those who are interested only in using our action description language for describing action
domains, this section can be safely skipped.
We first briefly review the language of the situation calculus.
4.1 Situation Calculus
The language of the situation calculus is a many sorted first-order language. We assume the
following sorts: situation for situations, action for actions, fluent for propositional fluents,
truth-value for truth values true and f alse, and object for everything else.
We use the following domain independent predicates and functions:
 Binary function do - for any action a and any situation s, do(a, s) is the situation
resulting from performing a in s.
 Binary predicate H - for any p and any situation s, H(p, s) is true if p holds in s.
 Binary predicate P oss - for any action a and any situation s, P oss(a, s) is true if a is
possible (executable) in s.
 Ternary predicate Caused - for any fluent atom p, any truth value v, and any situation
s, Caused(p, v, s) is true if the fluent atom p is caused (by something unspecified) to
have the truth value v in the situation s.
291

fiLin

In the last section, we introduced fluent formulas. We now extend H to these formulas:
for any fluent formula  and situation s, H(, s) is defined as follows:
 H(t1 = t2 , s) is t1 = t2 .
 if P is a static proposition, then H(P, s) is P .
 inductively, H(, s) is H(, s), H(  0 , s) is H(, s)  H(0 , s), and similarly for
other connectives.
 inductively, H((x, p), s) is x.[p(x)  H(, s)] and H((x, p), s) is x.[p(x) 
H(, s)].
According to this definition, H(, s) will be expanded to a situation calculus formula where
H is applied to fluents.
4.2 A Translation to the Situation Calculus
Given a first-order language L for writing action domain descriptions, we assume that there
will be a corresponding language L0 for the situation calculus such that constants in L
will be constants of sort object in L0 , types in L will be types (unary predicates) in L0 ,
static relations will be predicates of the same arities in L0 , fluents in L will be functions
of sort f luent in L0 , and actions in L will be functions of sort action in L0 . Under these
conventions, the following translation will map an action domain description to a situation
calculus theory.
Let D be an action domain description. The translation of D into a situation calculus
theory is defined as follows:
 a type definition Domain(p, {a1 , ..., ak }) is translated to:
(x).p(x)  (x = a1      x = ak ),
a1 6= a2 6=    6= ak .
 a primitive fluent definition
Fluent(f (x1 , ..., xn ), p1 (x1 )      pn (xn )  e1      em )
is translated to
(x1 , ..., xn ).Fluent(f (x1 , ..., xn ))  p1 (x1 )      pn (xn )  e1      em .
 a complex fluent definition
Complex(f (x1 , ..., xn ), p1 (x1 )      pn (x)  e1     em ),
Defined(f (x1 , ..., xn ), ),
is translated to
(x1 , ..., xn ).Fluent(f (x1 , ..., xn ))  p1 (x1 )      pn (x)  e1     em ,
(x1 , ..., xn , s).Fluent(f (x1 , ..., xn ))  [H(f (x1 , ..., xn ), s)  H(, s)].
292

fiFrom Causal Theories to STRIPS-Like Systems

 a domain axiom about static propositions:
Axiom()
is translated to  with quantifiers in it treated as shorthands:
(x, p) = x.p(x)  ,
(x, p) = x.p(x)  .
 an action definition
Action(a(x1 , ..., xn ), p1 (x1 )      pn (xn )  e1      em )
is translated to
(x1 , ..., xn ).Action(a(x1 , ..., xn ))  p1 (x1 )      pn (xn )  e1      em .
We assume here that the domain description has only one action definition for each
action.
 An action precondition axiom
P recond(a(x1 , ..., xn ), )
is translated to
~ s).Action(a(x1 , ..., xn ))  [P oss(a(x1 , ..., xn ), s)  H(, s)],
(,
where ~ is the list of all free variables in a(x1 , ..., xn ) and . We mentioned earlier
that one of the limitations of our current system is that action preconditions have to
be given explicitly. This is reflected in the above translation.
 An action effect axiom:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
is translated to
~ s).Action(a(x1 , ..., xn ))  Fluent(f (y1 , ..., yk ))  P oss(a(x1 , ..., xn ), s) 
(,
{H(, s)  Caused(f (y1 , ..., yk ), true, do(a(x1 , ..., xn ), s))},
where ~ is the list of all free variables in a(x1 , ..., xn ), f (y1 , ..., yk ), and .
Similarly, an effect axiom
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
is translated to
~ s).Action(a(x1 , ..., xn ))  Fluent(f (y1 , ..., yk ))  P oss(a(x1 , ..., xn ), s) 
(,
{H(, s)  Caused(f (y1 , ..., yk ), f alse, do(a(x1 , ..., xn ), s))}.
293

fiLin

 A domain rule of the form
Causes(, f (x1 , ..., xn ))
is translated to
~
().Fluent(f
(x1 , ..., xn ))  (s){H(, s)  Caused(f (x1 , ..., xn ), true, s)},
where ~ is the list of all free variables in f (x1 , ..., xn ) and . Similarly, a domain rule
of the form
Causes(, f (x1 , ..., xn ))
is translated to
~
().Fluent(f
(x1 , ..., xn ))  (s){H(, s)  Caused(f (x1 , ..., xn ), f alse, s)}.
Now given an action domain description D, let T be its translation in the situation
calculus. The semantics of T is then determined by its completion comp(T ) that is defined
as the set of following sentences:
1. The circumscription of Caused in T with all other predicates fixed.
2. The following basic axioms about Caused that says that if a fluent atom is caused to
be true (false), then it is true (false):
Caused(p, true, s)  Holds(p, s),

(2)

Caused(p, f alse, s)  Holds(p, s).

(3)

3. For the truth values, the following unique names and domain closure axiom:
true 6= f alse  (v)(v = true  v = f alse).

(4)

4. The unique names assumptions for fluents and actions. Specifically, if F1 ,..., Fn are
all the fluents, then we have:
Fi (~x) 6= Fj (~y ), i and j are different,
Fi (~x) = Fi (~y )  ~x = ~y .
Similarly for actions.
5. For each primitive fluent atom F , the following generic successor state axiom:
a, s.P oss(a, s)  H(F, do(a, s)) 

(5)

[Caused(F, true, do(a, s))  H(F, s)  Caused(F, f alse, do(a, s))].
6. The foundational axioms (Lin & Reiter, 1994b) for the discrete situation calculus.
These axioms characterize the structure of the space of situations. For the purpose
of this paper, it is enough to mention that they include the following unique names
axioms for situations:
s 6= do(a, s),
do(a, s) = do(a0 , s0 )  (a = a0  s = s0 ).
294

fiFrom Causal Theories to STRIPS-Like Systems

The following theorem shows that the procedural semantics given in the previous section
is sound with respect to the semantics given here.
Theorem 1 Let D be an action domain description, and T its translation in the situation
calculus. Let A be any ground action instance, s a situation variable, and (s) a situation
calculus formula that satisfies the following two conditions (1) it contains at most the two
situation terms s and do(A, s); and (2) it does not mention any predicate other than H,
equality, and static relations. Let  be obtained from  by replacing each H(f, s) in it by
init(f ), and each H(f, do(A, s)) in it by succ(f ). Then
comp(T ) |= s.P oss(A, s)  (s)

(6)

Init  Succ  Succ1 |= ,

(7)

if
where Init, Succ, and Succ1 are the sets of axioms generated for A according to the procedure in Section 3
Proof: Suppose S is a situation and M is a model of comp(T )  {P oss(A, S)}. Construct
MS,A as follows:
 the domain of MS,A is the object domain of M .
 the interpretations of non-situational function and predicate symbols in MS,A are the
same as those in M .
 for any fluent atom f , MS,A |= init(f ) iff M |= H(f, S) and MS,A |= succ(f ) iff
M |= H(f, do(A, S)).
Clearly, M |= (S) iff MS,A |= . We show below that MS,A is a model of the left hand
side of (7). From this, we see that if (7), then (6).
Notice first that a ground fluent atom F is generated by the procedure CCP iff Fluent(F )
is true in M . Notice also that all fluent atoms in Init  Succ  Succ1 must be generated by
the procedure.
We show first that MS,A is a model of Init:
1. If Axiom() is a domain axiom, then  is in T . Thus M satisfies . Since  has no
fluent symbols in it, MS,A satisfies it too.
2. If Causes(, f (x1 , ..., xn )) is a domain rule, then
~
().Fluent(f
(x1 , ..., xn ))  (s){H(, s)  Caused(f (x1 , ..., xn ), true, s)}
is in T . Thus M satisfies
~
().Fluent(f
(x1 , ..., xn ))  (s){H(, s)  H(f (x1 , ..., xn ), s)}.
Thus if init( 0 )  init(F ) is a corresponding formula in Init, where  0 and F is
some ground instantiation of  and f (x1 , ..., xn ), respectively, then Fluent(F ) must
be true in M (otherwise the above formula would not be in Init), so M satisfies
H( 0 , S)  H(F, S). By the construction of MS,A , it satisfies init( 0 )  init(F ). The
case for Causes(, f ) is similar.
295

fiLin

3. Suppose Defined(F, ) is an instantiation of a complex fluent definition such that
F   is in Init. For it to be in Init, Fluent(F ) must be true. Thus M must satisfy
H(F, S)  H(, S). By the construction of MS,A , it satisfies init(F )  init(, S).
4. Suppose P recond(A, A ) is the precondition axiom for A. Since M satisfies P oss(A, S)
and Action(A) (because A is one of the action instances generated by the procedure),
thus M satisfies H(A , S). So MS,A satisfies init(A ).
We now show that MS,A is a model of Succ, that is, for each primitive fluent atom F
generated by the procedure in Step 1, the pseudo-successor state axiom (1) holds. Referring
to the notation in the axiom, we need to show that M satisfies the following formula:
H(F, do(A, S))  H(1 , S)      H(n , S) 
H(01 , do(A, S))      H(0l , do(A, S)) 
H(F, S)  [H(1 , S)      H(m , S) 
H(01 , do(A, S))      H(0k , do(A, S))].
First of all, instantiating the generic successor state axiom (5) over A and S, we get:
P oss(A, S)  H(F, do(A, S)) 
[Caused(F, true, do(A, S))  H(F, S)  Caused(F, f alse, do(A, S))].
Since M is a model of P oss(A, S), we have
H(F, do(A, S)) 

(8)

[Caused(F, true, do(A, S))  H(F, S)  Caused(F, f alse, do(A, S))].
Now consider the circumscription of Caused in T with all other predicates fixed. Notice that
all axioms about Caused in T have the form W  Caused(x, y, z), where W is a formula
that does not mention Caused. Therefore the circumscription of Caused is equivalent to
the predicate completion of Caused. Suppose F is f (t), and that all of the axioms about
Caused(f (x), v, s) in T are as follows:
W1  Caused(f (x), v, s),    , Wi  Caused(f (x), v, s).
Because of the unique names axioms about fluents, the result of predicate completion on
Caused will entail:
Caused(f (x), v, s)  W1      Wi .
Now W1 ,...,Wi are from action effect axioms and domain rules about f . By the way that
(1) is generated, and noting that Action(A), Fluent(F ), and P oss(A, S) are true, one can
see that when the above equivalence is instantiated by replacing t for x, true for v, and S
for s, we will get
Caused(F, true, do(A, S))  H(1 , S)      H(n , S) 
H(01 , do(A, S))      H(0l , do(A, S)).
296

fiFrom Causal Theories to STRIPS-Like Systems

Similarly, we have the following axiom about Caused(F, f alse, do(A, S)):
Caused(F, f alse, do(A, S))  H(1 , S)      H(m , S) 
H(01 , do(A, S))      H(0k , do(A, S)).
From these two axioms and (8), we get:
H(F, do(A, S))  H(1 , S)      H(n , S) 
H(01 , do(A, S))      H(0l , do(A, S)) 
H(F, S)  [H(1 , S)      H(m , S) 
H(01 , do(A, S))      H(0k , do(A, S))].
Since M is a model of Comp(T ), M satisfies the above formula. By the construction of
MS,A , it satisfies the pseudo-successor state axiom (1).
Finally, the fact that MS,A is a model of Succ1 should be apparent.
2
In general, (6) does not imply (7). There are several reasons:
 As we mentioned after the procedure CCP, we assume that all information about the
initial situation is given by Init.
 Our procedure works on actions one at a time. The situation calculus theory T
captures the effects of all actions in a single theory. So it is possible that a bad specification of an action causes the entire theory to become inconsistent. For instance,
if we have Causes(true, p), P recond(A, true), and Effect(A, f alse, p), then the corresponding situation calculus theory will be inconsistent because of action A. But for
our procedure, it will generate an inconsistent theory only on A.

5. An Implementation
Except for Step 2.2, the procedure CCP in Section 3 is straightforward to implement. This
section describes the strategy that our system uses for implementing Step 2.2. The main
idea comes from the work of Lin (2001a) on strongest necessary and weakest sufficient
conditions.
Given a propositional theory T , a proposition q, and a set B of propositions, a formula
 is said to be a sufficient condition of q on B under T if  consists of propositions in B
and T |=   q. It is said to be the weakest sufficient condition if for any such sufficient
condition 0 , we have that T |= 0  . Similarly, a formula  is said to be a necessary
condition of q on B under T if  consists of propositions in B and T |= q  . It is said
to be the strongest necessary condition if for any such necessary condition 0 , we have that
T |=   0 .
It is easy to see that the weakest sufficient condition and the strongest necessary condition are unique up to logical equivalence under the background theory. It was shown (Lin,
2001a) that these two notions are closely related, and can be computed using the technique
of forgetting (Lin & Reiter, 1994a). In particular, for action theories, an effective strategy is to first compute the strongest necessary condition, add it to the background theory,
297

fiLin

and then compute the weakest sufficient condition under the new theory. This strategy is
justified by the following proposition Lin (2001a):
Proposition 1 Let T be a theory, q a proposition, and B a set of propositions. If  is a
necessary condition of q on B under T , and  the weakest sufficient condition of q on B
under T  {}, then    is the weakest sufficient condition of q on B under T .
We can now describe our strategy for implementing Step 2.2 of the procedure CCP. In
the following, given an action instance A, as in Step 2.2, let Succ be the set of pseudosuccessor state axioms for primitive fluent atoms, Succ1 the set of pseudo-successor state
axioms for complex fluent atoms, and Init the set of initial situation axioms derived from
the action precondition axiom for A, domain axioms, domain rules, and complex fluent
definitions. Also in the following, a succ-proposition is one of the form succ(f ), and an
init-proposition is one of the form init(f ).
1. Transform Init into a clausal form and derive from it a set of unit clauses U nit.
2. Use U nit to simplify the axioms in Succ and for each resulting axiom in it:
succ(f )  f ,

(9)

if f does not mention succ-propositions, then delete it from Succ, output it and
replace succ(f ) in the rest of the axioms by f .
3. For each fluent atom f whose pseudo-successor state axiom (9) is in Succ, if f has
the form init(f )  ... (a candidate of a frame axiom), then check to see if succ(f ) can
be derived from Succ, U nit, and init(f ) by unit resolution. If so, delete it from Succ,
output succ(f )  init(f ), and replace succ(f ) in Succ by init(f ).
4. For each fluent atom f whose pseudo-successor state axiom (9) is in Succ, compute
the strongest necessary condition f of succ(f ) on the init-propositions under the
theory Init  Succ, and the weakest sufficient condition f of succ(f ) on the initpropositions under the theory {f }  Init  Succ. If f is a tautology, then delete
(9) from Succ, output succ(f )  f , and replace succ(f ) in Succ by f . If  is not
a tautology, then output succ(f )  f and f  f  succ(f ), but do not delete (9)
from Succ. The correctness of this step follows from Proposition 1.
5. The previous steps solve the equations in Succ, and generate appropriate output for
primitive fluent atoms. For each complex fluent atom F :
Defined(F, ),
if every primitive fluent atom in  has a successor state axiom, then do the following:
(a) if no primitive fluent atoms in  are changed by the action, then this complex
fluent atom is not changed by the action either, so output succ(F )  init(F );
(b) otherwise, output succ(F )  , where  is obtained from succ() by replacing
every succ-proposition in it by the right side of its successor state axiom.
298

fiFrom Causal Theories to STRIPS-Like Systems

Otherwise, if some of the primitive fluent atoms in  do not have a successor state
axiom, which means the action may have an indeterminate effect on them, then this
action may have an indeterminate effect on F as well. Compute the strongest necessary and weakest sufficient conditions of succ(F ) under Init  Succ  Succ1 as in the
last step, and output them.
6. This step will try to generate a STRIPS-like description for the action instance A
based on the results of Steps 4 and 5. For each fluent atom F , do according to one of
the following cases:
(a) if its successor state axiom is succ(F )  true, then put F into the add list unless
init(F ) is entailed by Init;
(b) if its successor state axiom is succ(F )  f alse, then put F into the delete list
unless init(F ) is entailed by Init;
(c) if its successor state axiom is succ(F )  , and  is not the same as true, f alse,
or init(F ), then put F in the conditional effect list and output its successor state
axiom.
(d) If F does not have a successor state axiom, then put it in the list of indeterminate
effects.
Clearly, if F is not put into any of the lists, then its truth value is not affected by A.
Steps 4 and 5 of the above procedure are the bottleneck as in the worst case, computing
the strongest necessary condition of a proposition is coNP-hard. However, it has been our
experience that if action A has a context-free effect on fluent atom F , then its successor
state axiom can be computed without going through Step 4.
We have implemented the procedure CCP using the above strategy in SWI-Prolog 3.2.92 .
The url for this system is as follows:
http://www.cs.ust.hk/~flin/ccp.html
Using the system, we have encoded in our action description language many of the planning
domains that come with the original release of PDDL (McDermott, 1998), and compiled
them to STRIPS-like specifications. Our encodings of the domains and the results returned
by the system are included in the online appendix. In the following, we illustrate some
interesting features of our system using the following two domains: the blocks world and
the monkey and bananas domain.
5.1 The Blocks World
We have used the blocks world as the running example. Here we shall give an alternative specification of the domain using the following better known set of actions: stack,
unstack, pickup, and putdown. We shall use this domain to show that changing slightly
the precondition of one of the actions can result in a very different action specification.
2. SWI-Prolog is developed by Jan Wielemaker at University of Amsterdam

299

fiLin

We begin with a description that corresponds to the standard STRIPS encoding of the
domain.
Fluent(on(x, y), block(x)  block(y)),
Fluent(ontable(x), block(x)),
Fluent(holding(x), block(x)),
Complex(clear(x), block(x),
Defined(clear(x), ((y, block)on(y, x))  holding(x)),
Complex(handempty, true),
Defined(handempty, (x, block)holding(x)),
Causes(on(x, y)  x 6= z, on(z, y)),
Causes(on(x, y)  y 6= z, on(x, z)),
Causes(on(x, y), ontable(x)),
Causes(ontable(x), on(x, y)),
Causes(on(x, y), holding(x)),
Causes(on(x, y), holding(y)),
Causes(holding(x), ontable(x)),
Causes(holding(x), on(x, y)),
Causes(holding(x), on(y, x)),
Causes(holding(x)  y 6= x, holding(y)),
Action(stack(x, y), block(x)  block(y)  x 6= y),
P recond(stack(x, y), holding(x)  clear(y)),
Effect(stack(x, y), true, on(x, y)),
Action(unstack(x, y), block(x)  block(y)  x 6= y),
P recond(unstack(x, y), clear(x)  on(x, y)  handempty),
Effect(unstack(x, y), true, holding(x)),
Action(putdown(x), block(x)),
P recond(putdown(x), holding(x)),
Effect(putdown(x), true, ontable(x)),
Action(pickup(x), block(x)),
P recond(pickup(x), handempty  ontable(x)  clear(x)),
Effect(pickup(x), true, holding(x)).
Notice that compared to the description in Example 1, there are two more fluents, holding
and handempty here. Thus we have a few more domain rules about them, and the definition
of clear is changed to take into account that when a block is held, it is not considered to
be clear.
300

fiFrom Causal Theories to STRIPS-Like Systems

Now assuming a domain with three blocks Domain(block, {1, 2, 3}), our system will
generate 19 fluent atoms, and 18 action instances. For each action instance, it returns
both a complete set of successor state axioms and a STRIPS-like representation. The total
computation time for all actions is 835K inferences and 0.5 seconds.3 This is a pure STRIPS
domain, i.e. all actions are context free. For this type of domains, as we mentioned earlier,
Step 4 in our implementation procedure is not needed, and Step 5 is easy.
The results are as expected. For instance, for action pickup(1), the STRIPS-like representation returned by the system looks like the following: track 1 to track 2), the STRIPSlike representation looks like:
pickup(1):
Preconditions: clear(1), handempty, ontable(1)
Add list: holding(1)
Delete list: ontable(1), clear(1), handempty
Conditional effects:
Indeterminate effects:
The complete output is given in the online appendix. Now let us consider what happen
if we drop ontable(x) from the precondition of pickup(x):
P recond(pickup(x), handempty  clear(x)).
This means that as long as a block is clear, it can be picked up. With this new precondition,
our system returns the following STRIPS-like representation for action pickup(1);
pickup(1):
Preconditions: clear(1), handempty
Add list: holding(1)
Delete list: clear(1), handempty, on(1, 2), on(1, 3), ontable(1)
Conditional effects:
succ(clear(2))<-> - (init(on(2, 2))\/init(on(3, 2)))
succ(clear(3))<-> - (init(on(2, 3))\/init(on(3, 3)))
Indeterminate effects:
Here - is negation, and \/ is disjunction. An ADL-like description for this action
would be something like the following:
pickup(x):
Preconditions: clear(x), handempty
Add list: holding(x),
clear(y) when on(x,y)
Delete list: clear(x), handempty,
on(x,y) when on(x,y)
ontable(x) when ontable(x)
3. All times in this paper refer to CPU times on a Pentium III 1GHz machine with 512MB RAM running
SWI-Prolog 3.2.9 under Linux. The number of inferences is the one reported by SWI-Prolog, and
roughly corresponds to the number of resolution steps carried out by the Prolog interpreter, and is
machine independent.

301

fiLin

5.2 The Monkey and Bananas Domain
This domain is again adapted from McDermotts PDDL library of planning domains, which
attributes it to the University of Washingtons UCPOP collection of action domains, which
in turn attributes it to Prodigy. While some of the action effects generated by our system
are context-dependent, they are all context-free in the other systems. We shall elaborate
on this difference later.
In this domain, there are two types, loc for locations (we assume there are three locations
here), and object for things like monkey, banana, box, etc.:
Domain(loc, {1, 2, 3}),
Domain(object, {monkey, box, banana, knif e, glass, f ountain}).
The following are fluent definitions:
Fluent(onF loor),
Fluent(at(M, X), object(M )  loc(X)),
Fluent(hasknif e),
Fluent(onbox(X), loc(X)),
Fluent(hasbanana),
Fluent(haswater),
Fluent(hasglass).
The following are domain rules about these fluents:
Causes(onbox(X), at(monkey, X)),

(10)

Causes(onbox(X), at(box, X)),

(11)

Causes(onbox(X), onF loor),

(12)

Causes(onF loor, onbox(X)),

(13)

Causes(at(M, X)  X 6= Y, at(M, Y )),

(14)

Causes(hasglass  at(monkey, X), at(glass, X)),

(15)

Causes(hasknif e  at(monkey, X), at(knif e, X)),

(16)

Causes(hasbanana  at(monkey, X), at(banana, X)).

(17)

The following are action definitions along with their respective preconditions and effect
axioms:
 goto(x, y) - the monkey goes to x from y:
Action(goto(X, Y ), loc(X)  loc(Y )  X 6= Y ),
P recond(goto(X, Y ), at(monkey, Y )  onF loor),
Effect(goto(X, Y ), true, at(monkey, X)).
302

fiFrom Causal Theories to STRIPS-Like Systems

 climb(X) - the monkey climbs onto the box at location X:
Action(climb(X), loc(X)),
P recond(climb(X), at(box, X)  onF loor  at(monkey, X)),
Effect(climb(X), true, onbox(X)).
 pushbox(X, Y ) - the monkey pushes the box from Y to X.
Action(pushbox(X, Y ), loc(X)  loc(Y )  X 6= Y ),
P recond(pushbox(X, Y ), at(monkey, Y )  at(box, Y ))  onF loor),
Effect(pushbox(X, Y ), true, at(monkey, X)),
Effect(pushbox(X, Y ), true, at(box, X)).
 getknif e(X) - get knife at location X.
Action(getknif e(X), loc(X)),
P recond(getknif e(X), at(knif e, X)  at(monkey, X)  hasknif e),
Effect(getknif e(X), true, hasknif e).
 getbanana(X) - grab banana at loc X, provided the monkey is on the box.
Action(getbanana(X), loc(X)),
P recond(getbanana(X), onbox(X)  at(banana, X)  hasbanana),
Effect(getbanana(X), true, hasbanana).
 pickglass(X) - pick up glass at loc X.
Action(pickglass(X), loc(X)),
P recond(pickglass(X), at(glass, X)  at(monkey, X)  hasglass),
Effect(pickglass(X), true, hasglass).
 getwater(X) - get water from fountain at loc X, provided the monkey is on the box,
and has a glass in hand.
Action(getwater(X), loc(X)),
P recond(getwater(X), at(f ountain, X)  onbox(X)  hasglass  haswater),
Effect(getwater(X), true, haswater).
This domain has 27 actions and 26 fluent atoms. Again, for each action, our system
generates both a complete set of fully instantiated successor state axioms and a STRIPSlike representation. For instance, for action goto(1, 2), the following is the STRIPS-like
representation generated by the system:
303

fiLin

Action goto(1, 2)
Preconditions: at(monkey, 2), onFloor
Add list: at(monkey, 1)
Delete list: at(monkey, 2)
Conditional effects:
succ(at(banana, 1)) <-> init(hasbanana) \/ init(at(banana, 1))
succ(at(knife, 1)) <-> init(hasknife) \/ init(at(knife, 1))
succ(at(glass, 1)) <-> init(hasglass) \/ init(at(glass, 1))
succ(at(banana, 2)) <-> - init(hasbanana) & init(at(banana, 2))
succ(at(knife, 2)) <-> - init(hasknife) & init(at(knife, 2))
succ(at(glass, 2)) <-> - init(hasglass) & init(at(glass, 2))
The total running time for all actions is 8 seconds while performing 20 million inferences.
About 90 percent of time is spent on Step 4, i.e. on computing the strongest necessary
and weakest sufficient conditions of fluent atoms on which the given action has contextdependent effects. For instance, for action goto(1, 2) above, the majority of time was spent
on generating the above 6 conditional effects.
For this action, actually for all the actions in this domain, we could use an ADL-like
description (Pednault, 1989) for conditional effects:
Add list: at(banana,1) when hasbanana
at(knife,1) when hasknife
at(glass,1) when hasglass
Delete list: at(banana,2) when hasbanana
at(knife,2) when hasknife
at(glass,2) when hasglass
However, it is not clear whether this can always be done in the general case.
We mentioned earlier that the specifications for this domain given in McDermotts collection as well as others are all context-free. For instance, the following is a specification
for action goto in PDDL in McDermotts collection:
(:action GO-TO
:parameters (?x ?y)
:precondition (and (location ?x) (location ?y)
(not (= ?y ?x)) (on-floor) (at monkey ?y))
:effect (and (at monkey ?x) (not (at monkey ?y))))
This corresponds to a context-free action that does not change any other fluent except at.
It is clear that the design of this action does not take into account domain rules (15) - (17).
With this specification, if initially banana is at location 1, then the goal of having banana
at location 2 would not be achievable.
304

fiFrom Causal Theories to STRIPS-Like Systems

5.3 Summary
The other domains that we have experimented including a scheduling domain that includes
Pednaults dictionary and paycheck domain as a special case, the rocket domain, the SRI
robot domain, the machine shop assembling domain, the ferry domain, the grid domain,
the sokoban domain, and the gear domain. They are all included in the online appendix.
We summarize below some of the common features of these domains:
 In all the domains that we tried, it is quite straightforward to decide what effects of
an action should be encoded as direct effects (those given by the predicate Effect) and
what effects as indirect effects (those derived from domain rules).
 The most common domain rules are functional dependency constraints. For instance,
in the blocks world, the fluent atom on(x, y) is functional on both arguments; in
the monkey and banana domain, the fluent atom at(object, loc) is functional on the
second argument (each object can be at only one location). It makes sense then that
we would have a special shorthand for these domain rules, and perhaps a special
procedure for handling them as well. But more significantly, given the prevalence
of these functional dependency constraints in action domains, it is worthwhile to
investigate the possibility of a general purpose planner making good use of these
constraints.
 As we mentioned earlier, our system is propositional. The generated successor state
axioms and STRIPS-like systems are all fully instantiated. However, it is often easy
for the user to generalize these propositional specifications to first-order ones. We
shall investigate the generality of this observation next.

6. Generalizing Propositional STRIPS-Like Systems to Ones With
Parameters
As we mentioned, for many action domain descriptions, the successor state axioms and
STRIPS-like systems generated for a specific domain can be generalized to arbitrary ones.
More precisely, let D be a domain description, and
Domain(p1 , Dp1 ),    , Domain(pk , Dpk )
its type specification. Suppose in D for action A we have that InitA  SuccA |= . Now
suppose D0 is another domain description that is just like D except that it has a different
type specification:
Domain(p1 , Dp0 1 ),    , Domain(pk , Dp0 k ).
The question that we are interested in is this: given any one-to-one mapping from the type
specification of D to that of D0 , will InitA0  SuccA0 |= 0 be true in D0 ? Here A0 (resp. 0 )
is the result of replacing all objects in A (resp. ) according to the mapping.
For instance, if the above is true for the blocks world, then we can generalize the results
for the domain description in Example 1 as follows. As we have shown, for action stack(1, 2),
both succ(on(1, 2)) and succ(on(1, 3)) are true. Now if we change the type specification
to Domain(block, {a, b, c, d, e}), and if we map 1 to a, 2 to c, and 3 to e, in the new domain
305

fiLin

specification, we will have that for action stack(a, c), succ(on(a, c)) and succ(on(a, e)) are
true. Furthermore, by changing the mapping for 3, we see that for any x that is different
from a and c (the mapping needs to be one-to-one), succ(on(a, x)) is true.
Obviously, this is to be expected of the blocks world. We now proceed to show that for
some general classes of domain descriptions, we can do this as well. We first make precise
the mapping from one type specification to another.
Definition 2 Given two type specifications O:
Domain(p1 , Dp1 ),    , Domain(pk , Dpk ),
and O0 :
Domain(p1 , Dp0 1 ),    , Domain(pk , Dp0 k ),
an embedding from O to O0 is a one-to-one mapping  from Dp1   Dpk to Dp0 1   Dp0 k
such that for any 1  i  k, and any a  Dpi , f (a)  Dp0 i .
Clearly, if there is an embedding of O to O0 , then for each type p, the size of the domain
for p in O0 must be at least the size of the domain for p in O. Given such an embedding
 , any expression  (actions, propositions, formulas) in an action domain description D
with O as its type specification can be mapped to  () in the language of D0 : one simply
replaces each object a in  by  (a), where D0 differs from D only in that it uses O0 as its
type specification. Notice that only objects (those in the domain of some type) are to be
replaced, not constants that may occur in the effect axioms or domain rules.
Definition 3 An action domain description belongs to simple-I class if it does not mention
any function of positive arity, does not mention any complex fluents except in complex fluent
definitions, and satisfies the following conditions:
1. If P recond(A, A ) is an action precondition definition, then A has the form
(x, p)...(y, q)W , where W is a fluent formula that does not have any quantifiers.
2. If Effect(A, , F ) or Effect(A, , F ) is an action effect axiom, then  does not have
any quantifiers, and the variables in  and F are among those in A. That is, one
cannot have something like
Effect(explodeAt(x), nearby(y, x), dead(y)).
3. If Causes(, F ) or Causes(, F ) is a domain rule, then  does not have any quantifiers, and all the variables in  must be in F .
Theorem 2 Let D be a simple-I action domain description, and A an action instance in
D. Let D0 be just like D except for the type specification. Then for any formula  that
does not mention any complex fluent and has no quantifiers, and any embedding  from
the type specification of D to that of D0 , we have that if InitA  SuccA |=  in D. then
Init (A)  Succ (A) |=  () in D0 .
306

fiFrom Causal Theories to STRIPS-Like Systems

Proof: Suppose Init (A)  Succ (A) |=  () is not true, and that M1 is a truth assignment
in the language of D0 that satisfies Init (A)  Succ (A) and  (). Now construct a truth
assignment M2 in the language of D as follows: for any proposition P in the language of
D that does not mention any complex fluent, M2 |= P iff M1 |=  (P ) (P is really either
a static proposition, succ(F ),or init(F ), where F is a primitive fluent atom). The truth
values of complex fluent atoms in M2 are defined according to their definitions. Clearly,
M2 |= . We now need to show that M2 also satisfies InitA and SuccA . For InitA , there
are three cases:
1. M2 |= init(F )  init() when Defined(F, ) is a complex fluent definition. This
follows from the construction of M2 .
2. M2 |= init(A ) when P recond(A, A ) is the precondition definition for A. By our
assumption, A has the form (x, p)...(y, q).W , where W is a formula without any
quantifiers. Without loss of generality, let us assume it is (x, p)W . Then this formula
is equivalent to
_
W (x/a)
aDp

under D, where Dp is the domain of type p in D. So M2 |= (x, p)W iff
M2 |=

_

W (x/a)

aDp

iff
M1 |=

_

W (x/ (a)),

aDp

which is true since M1 |= (x, p)W .
3. All other formulas in InitA do not mention complex fluents and have no quantifiers.
They are true in M2 because the corresponding ones are true in M1 .
For SuccA , suppose F is a primitive fluent atom, and its pseudo-successor state axiom F
as constructed according to the procedure CCP given in Section 3 is as follows:
succ(F )  init(1 )      init(n )  succ(01 )      succ(0l ) 
init(F )  [init(1 )      init(m )  succ(01 )      succ(0k )].
Because of the following properties about D:
 each effect axiom Effect(A, , F ) or Effect(A, , F ) has the property that  has no
quantifier, and that the variables in  are also in F ;
 each domain rule of the form Causes(, F ) or Causes(, F ) has the property that
 has no quantifier, and that the variables in  are also in F ;
so the pseudo-successor state axiom for  (succ(F )) under D0 is just  (F ). Thus M2 |= F
since M1 |=  (F ). This proves that M2 is a model of SuccA , thus the theorem.
2

307

fiLin

However, most of the examples that we have in the paper do not belong to this simple-I
class, for two reasons: action preconditions, like those in the blocks world, can mention
complex fluents; and some of the negative domain rules Causes(, F ) may have some
variables not in F . The first problem is not a problem in principle as complex fluents can
be replaced by their definitions. The second problem is more serious, and that leads to a
new type of simple action theories.
Definition 4 An action domain description belongs to simple-II class if it does not mention
any function of positive arity, does not mention any complex fluents except in complex fluent
definitions, and satisfies the following conditions:
1. If P recond(A, A ) is an action precondition definition, then A has the form
(x, p)...(y, q)W , where W is a fluent formula that does not have any quantifiers.
2. If Effect(A, , F ) or Effect(A, , F ) is an action effect axiom, then  does not have
any quantifiers, and the variables in  and F are among those in A.
3. There are no positive domain rules of the form Causes(, F ).
4. If Causes(, F ) is a domain rule, then  must be of the form 1  2 , where 1 is
any formula that does not mention any fluents and 2 is a fluent atom. Notice that
there is no restriction on variables in 2 .
Simple-II class action domain descriptions seem to be very limited in that there can be no
positive domain rules, and the only negative domain rules allowed are binary. Nevertheless,
they still capture many context-free action domains. For instance, both the blocks world
and meet-and-pass domains in this paper belong to this class: for the blocks world, notice
that while it uses the complex fluent clear in some of its action precondition definitions, as in
P recond(stack(x, y), ontable(x)  clear(x)  clear(y)), these definitions can be reformulated
as follows using clears definition:
P recond(stack(x, y),
ontable(x)  (x1 , block)(y1 , block)(on(x1 , x)  on(y1 , y))).
This will then satisfy the condition about P recond in the above definition of simple-II
action domain descriptions. While we have not verified it formally, it seems that all the
context-free action domains in McDermotts PDDL library of action domains, including the
logistics domain, belong to the simple-II class.
Theorem 3 Let D be a simple-II action domain description, and A an action instance in
D. Let D0 be just like D except for the type specification. Then for any formula  that
does not mention any complex fluent and has no quantifiers, and any embedding  from
the type specification of D to that of D0 , we have that if InitA  SuccA |=  in D then
Init (A)  Succ (A) |=  () in D0 .
Proof: Suppose Init (A)  Succ (A) |=  () is not true, and that M1 is a truth assignment
in the language of D0 that satisfies Init (A)  Succ (A) and  (). Now construct a truth
308

fiFrom Causal Theories to STRIPS-Like Systems

assignment M2 in the language of D as follows: for any proposition P in the language of
D that does not mention any complex fluent, M2 |= P iff M1 |=  (P ) (P is really either
a static proposition, succ(F ),or init(F ), where F is a primitive fluent atom). The truth
values of complex fluent atoms in M2 are defined according to their definitions. Clearly,
M2 |= . We now need to show that M2 also satisfies InitA and SuccA . For InitA , there
are three cases:
1. M2 |= init(F )  init() when Defined(F, ) is a complex fluent definition. This
follows from the construction of M2 .
2. M2 |= init(A ) when P recond(A, A ) is the precondition definition for A. By our
assumption, A has the form (x, p)...(y, q).W , where W is a formula without any
quantifiers. Without loss of generality, let us assume it is (x, p1 )W . Then this
formula is equivalent to
W (x/a11 )      W (x/a1n1 )
under D. So M2 |= (x, p1 )W iff
M2 |= W (x/a11 )      W (x/a1n1 )
iff
M1 |= W (x/ (a11 ))      W (x/ (a1n1 )),
which is true since M1 |= (x, p1 )W .
3. All other formulas in InitA do not mention complex fluents and have no quantifiers.
They are true in M2 because the corresponding ones are true in M1 .
For SuccA , suppose F is a primitive fluent atom. Since there is no positive domain rule of
the form Causes(, F ), the pseudo-successor state axiom for F as constructed according to
the procedure CCP given in Section 3 must be of the following form:
succ(F )  init(1 )      init(n ) 
init(F )  [init(1 )      init(m )  succ(01 )      succ(0k )],
where for each 1  i  k, Causes(0i , F ) is an instance of a domain rule in D.
Because in D and D0 , each effect axiom Effect(, F ) or Effect(, F ) has the property
that  has no quantifier, and that the variables in  are also in F , the pseudo-successor
state axiom for  (succ(F )) under D0 must have the form:
succ( (F ))  init( (1 ))      init( (n )) 
init( (F ))  [init( (1 ))      init( (m )) 
succ( (01 ))

  

succ( (0k ))

(18)

 succ()],

where  is a disjunction such that each disjunct  must be such that Causes(,  (F )) is
an instance in D0 and that the fluent atom in  contains an object not in  (A) and  (F ).
There are two cases:
309

fiLin

 Suppose M2 |= succ(F ). Then M1 |= succ( (F )). Since M1 is a model of Succ (A) ,
M1 satisfies the above axiom about succ( (F )). Therefore M1 satisfies the following
formula:
init( (1 ))      init( (n )) 
init( (F ))  [init( (1 ))      init( (m )) 
succ( (01 ))      succ( (0k ))].
Since the above formula does not mention any complex fluents and has no quantifiers,
M2 satisfies the corresponding formula:
init(1 )      init(n ) 

(19)

init(F )  [init(1 )      init(m ) 

succ(01 )

  

succ(0k )],

which is the right side of the equivalence of the pseudo-successor state axiom for
succ(F ) in SuccA .
 Now suppose M2 satisfies (19). Well show that M1 satisfies the right side of (18),
thus M1 |= succ( (F )) so M2 |= succ(F ). There are two cases:
 M2 satisfies the following formula:
init(1 )      init(n ).

(20)

In this case, since the above formula does not mention any complex fluents and
has no quantifier, M1 satisfies the following corresponding formula:
init( (1 ))      init( (n )).

(21)

Thus M1 satisfies the right side of (18).
 M2 does not satisfy (20) but satisfies the following formula:
init(F )  [init(1 )      init(m )  succ(01 )      succ(0k )].
Thus M1 satisfies the following formula:
init( (F ))  [init( (1 ))      init( (m )) 
succ( (01 ))      succ( (0k ))].
So to show that the right side of the equivalence of (18) is satisfied by M1 , we
need to show that M1 |= succ(). Recall that  is a disjunction such that each
disjunct  must correspond to a domain rule of the form Causes(,  (F )), and
that  is of the form 1  G such that 1 does not mention fluents, and G is
a fluent atom that mentions an object that does not occur in  (A). Note that
init()  init( (F )) is an axiom in Succ (A) , which is satisfied by M1 . Thus
M1 |= init(). This means that either 1 or init(G) is false in M1 . If 1
is false, then succ() is false since succ(1 ) is the same as 1 . Suppose that
init(G) is false in M1 . Notice that since there are no positive domain rules, and
310

fiFrom Causal Theories to STRIPS-Like Systems

that G has an object not in  (A) and  (F ), the pseudo-successor state axiom
for G in Succ (A) must be of the form succ(G)  init(G)  . Therefore from
M1 |= init(G) we get M1 |= succ(). Since  is any disjunct of , we have
proved that M1 |= succ(). Therefore M1 |= succ( (F )). Thus M2 |= succ(F ).
2

7. Related Work
In planning, the most closely related work is the causal reasoning module in Wilkinss SIPE
system (Wilkins, 1988). Wilkins writes (page 85, Wilkins, 1988): Use of the STRIPS
assumptions has made operators unacceptably difficult to describe in previous classical
planners... One of the primary reasons for this is that all effects of an action must be explicitly stated... Deductive causal theories are one of the most important mechanisms used
by SIPE to alleviate problems in operator representation caused by the STRIPS assumption. This is certainly one of the motivations for our system as well. In SIPE, domain rules
have triggers, preconditions, conditions, and effects. Informally, when the triggers become
true in the new situation, SIPE would then check in sequence to see if the preconditions
were true in the old situation, and the conditions are true in the new situation. If all these
conditions are true, it will then deduce the effects. For instance, a SIPE causal rule for
on(x, y) in the blocks world would look like:
Causal-rule: Not-on
Arguments: x, y, z;
Trigger: on(x,y);
Precondition: on(x,z);
Effects: not on(x,z);
In comparison, our domain rules are much simpler. For instance, our corresponding rule
for the above SIPE rule is simply: Causes(on(x, y)  y 6= z, on(x, z)). We do not need
procedural directives like triggers. To a large degree, we can see our system as a rational
reconstruction of the causal reasoning module in SIPE. As we have shown in Theorem 1,
the procedure used by our system is sound under a translation to causal theories in the
situation calculus. While Wilkins also gave a translation of his causal rules to formulas in
the situation calculus, he did not specify an underlying logic to reason about such formulas.
In fact, as shown by Lin (1995), such translations would not work.
For those familiar with PDDL, the original version by McDermott and the AIPS-98
Planning Competition Committee allows domain axioms over stratified theories. According
to the manual of PDDL 1.2 (McDermott, 1998), axioms are logical formulas that assert
relationships among propositions that hold within a situation. The format for writing
axioms in PDDL is as follows:
(:axiom
:vars (?x ?y ...)
:context W
:implies P)
311

fiLin

where W is a formula and P a literal. Axioms are treated directionally, from W to P . The
following is the rule and intention for using the axioms according to the manual:
The rule is that action definitions are not allowed to have effects that mention
predicates that occur in the :implies field of an axiom. The intention is that
action definitions mention primitive predicates like on, and that all changes in
truth value of derived predicates like above occur through axioms. Without
axioms, the action definitions will have to describe changes in all predicates that
might be affected by an action, which leads to a complex software engineering
(or domain engineering) problem.
It is clear from this quotation that axioms in PDDL are intended for defining derived
predicates. They are similar to our complex fluent definitions. New versions of PDDL have
extended the original version by allowing actions with durations and continuous changes.
They have not considered using axioms to derive changes to primitive predicates like what
we have done here with domain rules.
Our action domain description language, while having a very different syntax that is
strongly influenced by Prolog syntax, shares much of the same ideas behind action languages
(Gelfond & Lifschitz, 1999). However, unlike action languages, ours does not provide facilities for expressing the truth value of a fluent atom in a particular situation like the initial
situation. Rather, it is aimed at specifying the generic effects of actions. On the other hand,
it has facilities for specifying types and static relations. Most importantly, to date, action
languages are either implemented directly or mapped to a nonmonotonic logic programming
system rather than by compilation into a monotonic system where action effects are given
explicitly, as is done here. For instance, a new SAT-based planning method would have to
be implemented (e.g. McCain & Turner, 1998) for action languages. In comparison, once
an action domain description is compiled to a STRIPS-like description, existing planning
systems such as Blackbox (Selman & Kautz, 1999) or System R (Lin, 2001b) can be directly
called.

8. Concluding Remarks
We have described a system for generating the effects of actions from direct action effect
axioms and domain rules, among other things. We have shown the soundness of the procedure used by the system and tested it successfully in many benchmark action domains used
by current AI planners. For future work, we are considering how to generalize the simple
action theories in Section 6 to include context-dependent action domain descriptions like
the monkey and bananas domain.

Acknowledgments
An extended abstract of part of this paper appeared in Proceedings of AAAI-2000. I would
like to thank the anonymous reviewers for both JAIR and AAAI2000 as well as the associate
editor in charge of this paper for JAIR for their insightful comments on earlier versions of
this paper. This work was supported in part by the Research Grants Council of Hong Kong
under Competitive Earmarked Research Grant HKUST6061/00E.
312

fiFrom Causal Theories to STRIPS-Like Systems

References
Baral, C. (1995). Reasoning about actions: nondeterministic effects, constraints, and qualification. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence (IJCAI95), IJCAI Inc. Distributed by Morgan Kaufmann, San Mateo,
CA., pp. 20172023.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to theorem proving in
problem solving. Artificial Intelligence, 2, 189208.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Artificial Intelligence Research, 9, 367421.
Gelfond, M., & Lifschitz, V. (1999). Action languages. Electronic Transactions on Artificial
Intelligence, http://www.ep.liu.se/ea/cis, Vol 3, nr 016.
Gerevini, A., & Schubert, L. (1998). Inferring state constraints for domain-independent
planning. In Proceedings of the 15th National Conference on Artificial Intelligence
(AAAI98), AAAI Press, Menlo Park, CA.
Levesque, H., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: A logic
programming language for dynamic domains. Journal of Logic Programming, Special
issue on Reasoning about Action and Change, 31, 5984.
Lifschitz, V. (1997). On the logic of causal explanation. Artificial Intelligence, 96, 451465.
Lin, F. (1995). Embracing causality in specifying the indirect effects of actions. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI95), IJCAI Inc. Distributed by Morgan Kaufmann, San Mateo, CA., pp. 1985
1993.
Lin, F. (1996). Embracing causality in specifying the indeterminate effects of actions. In
Proceedings of the 13th National Conference on Artificial Intelligence (AAAI96),
AAAI Press, Menlo Park, CA., pp. 670676.
Lin, F. (2001a). On strongest necessary and weakest sufficient conditions. Artificial Intelligence, 128(1-2), 143159.
Lin, F. (2001b). A planner called R. AI Magazine, 22(3), 7376.
Lin, F., & Reiter, R. (1994a). Forget it!. In Greiner, R., & Subramanian, D. (Eds.),
Working Notes of AAAI Fall Symposium on Relevance, pp. 154159. The American Association for Artificial Intelligence, Menlo Park, CA. Also available at
http://www.cs.toronto.edu/cogrobo/forgetting.ps.Z.
Lin, F., & Reiter, R. (1994b). State constraints revisited. Journal of Logic and Computation,
Special Issue on Actions and Processes, 4(5), 655678.
McCain, N., & Turner, H. (1995). A causal theory of ramifications and qualifications. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI95), IJCAI Inc. Distributed by Morgan Kaufmann, San Mateo, CA., pp. 1978
1984.
McCain, N., & Turner, H. (1997). Causal theories of action and change. In Proceedings
of the 14th National Conference on Artificial Intelligence (AAAI97), AAAI Press,
Menlo Park, CA., pp. 460465.
313

fiLin

McCain, N., & Turner, H. (1998). Satisfiability planning with causal theories. In Proceedings
of the Sixth International Conference on Principles of Knowledge Representation and
Reasoning (KR98), pp. 212221.
McDermott, D. (1998). PDDL  the planning domain definition language. Tech. rep. TR98-003/DCS TR-1165, Yale Center for Computational Vision and Control.
Pednault, E. P. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus. In Proceedings of the First International Conference on Principles of
Knowledge Representation and Reasoning (KR89), pp. 324332. Morgan Kaufmann
Publishers, Inc.
Reiter, R. (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Lifschitz, V. (Ed.), Artificial
Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, pp. 418420. Academic Press, San Diego, CA.
Selman, B., & Kautz, H. (1999). Unifying SAT-based and graph-based planning. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI
99), IJCAI Inc. Distributed by Morgan Kaufmann, San Mateo, CA., pp. 318325.
Thielscher, M. (1995). Computing ramification by post-processing. In Proceedings of the
Fourteenth International Joint Conference on Artificial Intelligence (IJCAI95), IJCAI Inc. Distributed by Morgan Kaufmann, San Mateo, CA., pp. 19942000.
Thielscher, M. (1997). Ramification and causality. Artificial Intelligence, 89, 317364.
Wilkins, D. (1988). Practical planning: extending the classical AI planning paradigm. Morgan Kaufmann, San Mateo, CA.
Zhang, Y., & Foo, N. (1997). Deriving invariants and constraints from action theories.
Fundamenta Informaticae, 30(1), 109123.

314

fiJournal of Artificial Intelligence Research 19 (2003) 205-208

Submitted 12/02; published 9/03

Research Note
Potential-Based Shaping and Q-Value Initialization are
Equivalent
Eric Wiewiora

wiewiora@cs.ucsd.edu

Department of Computer Science and Engineering
University of California, San Diego
La Jolla, CA 92093 0114

Abstract
Shaping has proven to be a powerful but precarious means of improving reinforcement
learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping
algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal
behavior.
In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically,
we prove that a reinforcement learner with initial Q-values based on the shaping algorithms potential function make the same updates throughout learning as a learner receiving
potential-based shaping rewards. We further prove that under a broad category of policies,
the behavior of these two learners are indistinguishable. The comparison provides intuition
on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler
method for capturing the algorithms benefit. In addition, the equivalence raises previously
unaddressed issues concerning the efficiency of learning with potential-based shaping.

1. Potential-Based Shaping
Shaping is a common technique for improving learning performance in reinforcement learning tasks. The idea of shaping is to provide the learner with supplemental rewards that
encourage progress towards highly rewarding states in the environment. If these shaping
rewards are applied arbitrarily, they run the risk of distracting the learner from the intended
goals in the environment. In this case, the learner converges on a policy that is optimal in
the presence of the shaping rewards, but suboptimal in terms of the original task.
Ng, Harada, and Russell (1999) proposed a method for adding shaping rewards in a way
that guarantees the optimal policy maintains its optimality. They model a reinforcement
learning task as a Markov Decision Process (MDP), where the learner tries to find a policy
that maximizes discounted future reward (Sutton & Barto, 1998). They define a potential
function () over the states. The shaping reward for transitioning from state s to s0 is
defined in terms of  as:
F (s, s0 ) = (s0 )  (s),
where  is the MDPs discount rate. This shaping reward is added to the environmental
reward for every state transition the learner experiences. The potential function can be
viewed as defining a topography over the state space. The shaping reward for transitioning from one state to another is therefore the discounted change in this state potential.
Potential-based shaping guarantees that no cycle through a sequence of states yields a net
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWiewiora

benefit from the shaping. In fact, under standard conditions Ng et al. prove that any policy
that is optimal for an MDP augmented with a potential-based shaping reward will also be
optimal for the unaugmented MDP.

2. New Results
Many reinforcement learning algorithms learn an optimal policy by maintaining Q-values.
Q-values are estimates of the expected future reward of taking a given action in a given
state. We show that the effects of potential-based shaping can be achieved by initializing
a learners Q-values with the state potential function. We prove this result only for the
Q-learning algorithm, but the results extend to Sarsa and other TD algorithms as well.
We define two reinforcement learners, L and L0 , that will experience the same changes in
Q-values throughout learning. Let the initial values of Ls Q-table be Q(s, a) = Q0 (s, a). A
potential-based shaping reward F based on the potential function  will be applied during
learning. The other learner, L0 , will have a Q-table initialized to Q00 (s, a) = Q0 (s, a) + (s).
This learner will not receive shaping rewards.
Let an experience be a 4-tuple hs, a, r, s0 i, representing a learner taking action a in state
s, transitioning to state s0 and receiving the reward r. Both learners Q-values are updated
based on an experience using the standard update rule for Q-learning. Q(s, a) is updated
with the potential-based shaping reward, while Q0 (s, a) is updated without the shaping
reward:

Q(s, a)  Q(s, a) +  r + F (s, s0 ) +  max
Q(s0 , a0 )  Q(s, a) ,
0
a
|
{z
}
0

0

0

0

Q(s,a)
0


Q (s, a)  Q (s, a) +  r +  max
Q (s , a )  Q0 (s, a) .
0
a
|
{z
}
Q0 (s,a)

The above equations can be interpreted as updating the Q-values with an error term
scaled by , the learning rate (assume the same  for the learners). We refer to the error
terms as Q(s, a) and Q0 (s, a). We also track the total change in Q and Q0 during learning.
The difference between the original and current values in Q(s, a) and Q0 (s, a) are referred to
as Q(s, a) and Q0 (s, a), respectively. The Q-values for the learners can be represented
as their initial values plus the change in those values that resulted from the updates:
Q(s, a) = Q0 (s, a) + Q(s, a)
Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a).
Theorem 1 Given the same sequence of experiences during learning, Q(s, a) always
equals Q0 (s, a).
Proof: Proof by induction. The base case is when the Q-table entries for s and s0 are still
their initial values. The theorem holds for this case, because the entries in Q and Q0
are both uniformly zero.
For the inductive case, assume that the entries Q(s, a) = Q0 (s, a) for all s and a. We
show that in response to experience hs, a, r, s0 i, the error terms Q(s, a) and Q0 (s, a) are
206

fiPotential-Based Shaping and Q-Value Initialization

equal. First we examine the update performed on Q(s, a) in the presence of the potentialbased shaping reward:

Q(s, a) = r + F (s, s0 ) +  max
Q(s0 , a0 )  Q(s, a)
0
a


0 0
0 0
= r + (s0 )  (s) +  max
Q
(s
,
a
)
+
Q(s
,
a
)
 Q0 (s, a)  Q(s, a)
0
0
a

Now we examine the update performed on Q0 :
Q0 (s0 , a0 )  Q0 (s, a)
Q0 (s, a) = r +  max
0
a


0 0
0
0 0
= r +  max
Q
(s
,
a
)
+
(s
)
+
Q(s
,
a
)
 Q0 (s, a)  (s)  Q(s, a)
0
a0

= r + (s0 )  (s) +  max
Q0 (s0 , a0 ) + Q(s0 , a0 )  Q0 (s, a)  Q(s, a)
0
a

= Q(s, a)
Both Q-tables are updated by the same value, and thus Q() and Q0 () are equal. 2
The implications of the proof can be appreciated when we consider how a learner chooses
actions. Most policies are defined in terms of the learners Q-values. We define an advantagebased policy as a policy that chooses an action in a given state with a probability that is
determined by the differences of the Q-values for that state, not their absolute magnitude.
Thus, if some constant is added to all the the Q-values, the probability distribution of the
next action will not change.
Theorem 2 If L and L0 have learned on the same sequence of experiences and use an
advantage-based policy, they will have an identical probability distribution for their next
action.
Proof: Recall how the Q-values are defined:
Q(s, a) = Q0 (s, a) + Q(s, a)
Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a)
We proved that Q(s, a) and Q0 (s, a) are equal if they have been updated with the same
experiences. Therefore, the only difference between the two Q-tables is the addition of the
state potentials in Q0 . Because this addition is uniform across the actions in a given state,
it does not affect the policy.
2
It turns out that almost all policies used in reinforcement learning are advantage-based.
The most important such policy is the greedy policy. The two most popular exploratory
policies, -greedy and Boltzmann soft-max, are also advantage-based. For any of these
policies, there is no difference in learning between the initialization described above and
potential-based shaping.
207

fiWiewiora

3. Shaping in Goal-Directed Tasks
It has been shown that the initial Q-values have a large influence on the efficiency of
reinforcement learning for goal directed tasks (Koenig & Simmons, 1996). These problems
are characterized by a state-space with some goal region. The agents task is to find a policy
that reaches this goal region as quickly as possible. Clearly an agent must find a goal state
at least once during exploration before an optimal policy can be found. With Q-values
initialized below their optimal value, an agent may require learning time exponential in the
state and action space in order to find a goal state. However, in deterministic environments,
an optimistic initialization of Q-values requires learning time that is polynomial in the stateaction space before a goal is found. See Bertsekas and Tsitsiklis (1996) for further analysis
of reinforcement learning algorithms with various initializations. Because potential-based
shaping is equivalent to Q-value initialization, care must be taken in choosing a potential
function that does not lead to poor learning performance.

4. Conclusion
We have shown that the effects of potential-based shaping can be captured by a particular
initialization of Q-values for agents using Q-learning. These results extend to Sarsa and
other TD methods. In addition, these results extend to the versions of these algorithms
augmented by eligibility traces.
For a discrete-state environment, these results imply that one should simply initialize
the learners Q-values with the potential function rather than alter the learning algorithm
to incorporate shaping rewards. In the case of continuous state-spaces, potential-based
shaping may still offer some benefit. A continuous potential function over the state-space
would be analogous to a continuous initialization of state values. Because potential-based
shaping allows any function defined on the state space to be used as the potential function,
the method may be beneficial to an agent with a restricted representation of state. A careful
analysis of this case would be a fruitful avenue of future research.

Acknowledgements
This research was supported by a grant from Matsushita Electric Industrial Co., Ltd.

References
Bertsekas, D. P., & Tsitsiklis, J. T. (1996). Neuro-dynamic Programming. Athena Scientific.
Koenig, S., & Simmons, R. (1996). The effect of representation and knowledge on goaldirected exploration with reinforcement-learning algorithms. Machine Learning,
22 (1/3), 227  250.
Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations:
theory and application to reward shaping. In Machine Learning, Proceedings of the
Sixteenth International Conference, pp. 278287. Morgan Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT
Press.

208

fiJournal of Artificial Intelligence Research 19 (2003) 399-468

Submitted 1/02; published 10/03

Efficient Solution Algorithms for Factored MDPs
Carlos Guestrin

guestrin@cs.stanford.edu

Computer Science Dept., Stanford University

Daphne Koller

koller@cs.stanford.edu

Computer Science Dept., Stanford University

Ronald Parr

parr@cs.duke.edu

Computer Science Dept., Duke University

Shobha Venkataraman

shobha@cs.cmu.edu

Computer Science Dept., Carnegie Mellon University

Abstract
This paper addresses the problem of planning under uncertainty in large Markov Decision
Processes (MDPs). Factored MDPs represent a complex state space using state variables and
the transition model using a dynamic Bayesian network. This representation often allows an
exponential reduction in the representation size of structured MDPs, but the complexity of exact
solution algorithms for such MDPs can grow exponentially in the representation size. In this paper,
we present two approximate solution algorithms that exploit structure in factored MDPs. Both
use an approximate value function represented as a linear combination of basis functions, where
each basis function involves only a small subset of the domain variables. A key contribution of this
paper is that it shows how the basic operations of both algorithms can be performed efficiently
in closed form, by exploiting both additive and context-specific structure in a factored MDP. A
central element of our algorithms is a novel linear program decomposition technique, analogous to
variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably
equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the
second approximate dynamic programming. Our dynamic programming algorithm is novel in that
it uses an approximation based on max-norm, a technique that more directly minimizes the terms
that appear in error bounds for approximate MDP algorithms. We provide experimental results
on problems with over 1040 states, demonstrating a promising indication of the scalability of our
approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some
problems, exponential gains in computation time.

1. Introduction
Over the last few years, Markov Decision Processes (MDPs) have been used as the basic
semantics for optimal planning for decision theoretic agents in stochastic environments. In
the MDP framework, the system is modeled via a set of states which evolve stochastically.
The main problem with this representation is that, in virtually any real-life domain, the
state space is quite large. However, many large MDPs have significant internal structure,
and can be modeled compactly if the structure is exploited in the representation.
Factored MDPs (Boutilier, Dearden, & Goldszmidt, 2000) are one approach to representing large, structured MDPs compactly. In this framework, a state is implicitly described
by an assignment to some set of state variables. A dynamic Bayesian network (DBN) (Dean
& Kanazawa, 1989) can then allow a compact representation of the transition model, by
exploiting the fact that the transition of a variable often depends only on a small number
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGuestrin, Koller, Parr & Venkataraman

of other variables. Furthermore, the momentary rewards can often also be decomposed as
a sum of rewards related to individual variables or small clusters of variables.
There are two main types of structure that can simultaneously be exploited in factored
MDPs: additive and context-specific structure. Additive structure captures the fact that
typical large-scale systems can often be decomposed into a combination of locally interacting components. For example, consider the management of a large factory with many
production cells. Of course, in the long run, if a cell positioned early in the production line
generates faulty parts, then the whole factory may be affected. However, the quality of the
parts a cell generates depends directly only on the state of this cell and the quality of the
parts it receives from neighboring cells. Such additive structure can also be present in the
reward function. For example, the cost of running the factory depends, among other things,
on the sum of the costs of maintaining each local cell.
Context-specific structure encodes a different type of locality of influence: Although a
part of a large system may, in general, be influenced by the state of every other part of this
system, at any given point in time only a small number of parts may influence it directly.
In our factory example, a cell responsible for anodization may receive parts directly from
any other cell in the factory. However, a work order for a cylindrical part may restrict this
dependency only to cells that have a lathe. Thus, in the context of producing cylindrical
parts, the quality of the anodized parts depends directly only on the state of cells with a
lathe.
Even when a large MDP can be represented compactly, for example, by using a factored
representation, solving it exactly may still be intractable: Typical exact MDP solution algorithms require the manipulation of a value function, whose representation is linear in the
number of states, which is exponential in the number of state variables. One approach is
to approximate the solution using an approximate value function with a compact representation. A common choice is the use of linear value functions as an approximation  value
functions that are a linear combination of potentially non-linear basis functions (Bellman,
Kalaba, & Kotkin, 1963; Sutton, 1988; Tsitsiklis & Van Roy, 1996b). Our work builds on
the ideas of Koller and Parr (1999, 2000), by using factored (linear) value functions, where
each basis function is restricted to some small subset of the domain variables.
This paper presents two new algorithms for computing linear value function approximations for factored MDPs: one that uses approximate dynamic programming and another
that uses approximate linear programming. Both algorithms are based on the use of factored linear value functions, a highly expressive function approximation method. This
representation allows the algorithms to take advantage of both additive and context-specific
structure, in order to produce high-quality approximate solutions very efficiently. The capability to exploit both types of structure distinguishes these algorithms differ from earlier
approaches (Boutilier et al., 2000), which only exploit context-specific structure. We provide
a more detailed discussion of the differences in Section 10.
We show that, for a factored MDP and factored value functions, various critical operations for our planning algorithms can be implemented in closed form without necessarily
enumerating the entire state space. In particular, both our new algorithms build upon a
novel linear programming decomposition technique. This technique reduces structured LPs
with exponentially many constraints to equivalent, polynomially-sized ones. This decomposition follows a procedure analogous to variable elimination that applies both to additively
400

fiEfficient Solution Algorithms for Factored MDPs

structured value functions (Bertele & Brioschi, 1972) and to value functions that also exploit context-specific structure (Zhang & Poole, 1999). Using these basic operations, our
planning algorithms can be implemented efficiently, even though the size of the state space
grows exponentially in the number of variables.
Our first method is based on the approximate linear programming algorithm (Schweitzer
& Seidmann, 1985). This algorithm generates a linear, approximate value function by
solving a single linear program. Unfortunately, the number of constraints in the LP proposed
by Schweitzer and Seidmann grows exponentially in the number of variables. Using our LP
decomposition technique, we exploit structure in factored MDPs to represent exactly the
same optimization problem with exponentially fewer constraints.
In terms of approximate dynamic programming, this paper makes a twofold contribution.
First, we provide a new approach for approximately solving MDPs using a linear value
function. Previous approaches to linear function approximation typically have utilized a
least squares (L2 -norm) approximation to the value function. Least squares approximations
are incompatible with most convergence analyses for MDPs, which are based on max-norm.
We provide the first MDP solution algorithms  both value iteration and policy iteration 
that use a linear max-norm projection to approximate the value function, thereby directly
optimizing the quantity that appears in our provided error bounds. Second, we show how
to exploit the structure of the problem to apply this technique to factored MDPs, by again
leveraging on our LP decomposition technique.
Although approximate dynamic programming currently possesses stronger theoretical
guarantees, our experimental results suggest that approximate linear programming is a
good alternative. Whereas the former tends to generate better policies for the same set of
basis functions, due to the simplicity and computational advantages of approximate linear
programming, we can add more basis functions, obtaining a better policy and still requiring
less computation than the approximate dynamic programming approach.
Finally, we present experimental results comparing our approach to the work of Boutilier
et al. (2000), illustrating some of the tradeoffs between the two methods. In particular, for
problems with significant context-specific structure in the value function, their approach
can be faster due to their efficient handling of their value function representation. However,
there are cases with significant context-specific structure in the problem, rather than in
the value function, in which their algorithm requires an exponentially large value function
representation. In such classes of problems, we demonstrate that by using a value function that exploits both additive and context-specific structure, our algorithm can obtain a
polynomial-time near-optimal approximation of the true value function.
This paper starts with a presentation of factored MDPs and approximate solution algorithms for MDPs. In Section 4, we describe the basic operations used in our algorithms,
including our LP decomposition technique. In Section 5, we present the first of our two
algorithms: the approximate linear programming algorithm for factored MDPs. The second
algorithm, approximate policy iteration with max-norm projection, is presented in Section 6.
Section 7 describes an approach for efficiently computing bounds on policy quality based on
the Bellman error. Section 8 shows how to extend our methods to deal with context-specific
structure. Our paper concludes with an empirical evaluation in Section 9 and a discussion
of related work in Section 10.
401

fiGuestrin, Koller, Parr & Venkataraman

This paper is a greatly expanded version of work that was published before in Guestrin
et al. (2001a), and some of the work presented in Guestrin et al. (2001b, 2002).

2. Factored Markov Decision Processes
A Markov decision process (MDP) is a mathematical framework for sequential decision
problems in stochastic domains. It thus provides an underlying semantics for the task of
planning under uncertainty. We begin with a concise overview of the MDP framework, and
then describe the representation of factored MDPs.
2.1 Markov Decision Processes
We briefly review the MDP framework, referring the reader to the books by Bertsekas and
Tsitsiklis (1996) or Puterman (1994) for a more in-depth review. A Markov Decision Process
(MDP) is defined as a 4-tuple (X, A, R, P ) where: X is a finite set of |X| = N states; A is
a finite set of actions; R is a reward function R : X  A 7 R, such that R(x, a) represents
the reward obtained by the agent in state x after taking action a; and P is a Markovian
transition model where P (x0 | x, a) represents the probability of going from state x to state
x0 with action a. We assume that the rewards are bounded, that is, there exists Rmax such
that Rmax  |R(x, a)| , x, a.
Example 2.1 Consider the problem of optimizing the behavior of a system administrator
(SysAdmin) maintaining a network of m computers. In this network, each machine is
connected to some subset of the other machines. Various possible network topologies can be
defined in this manner (see Figure 1 for some examples). In one simple network, we might
connect the machines in a ring, with machine i connected to machines i + 1 and i  1. (In
this example, we assume addition and subtraction are performed modulo m.)
Each machine is associated with a binary random variable Xi , representing whether it
is working or has failed. At every time step, the SysAdmin receives a certain amount of
money (reward) for each working machine. The job of the SysAdmin is to decide which
machine to reboot; thus, there are m + 1 possible actions at each time step: reboot one of the
m machines or do nothing (only one machine can be rebooted per time step). If a machine
is rebooted, it will be working with high probability at the next time step. Every machine
has a small probability of failing at each time step. However, if a neighboring machine fails,
this probability increases dramatically. These failure probabilities define the transition model
P (x0 | x, a), where x is a particular assignment describing which machines are working or
have failed in the current time step, a is the SysAdmins choice of machine to reboot and x0
is the resulting state in the next time step.
We assume that the MDP has an infinite horizon and that future rewards are discounted
exponentially with a discount factor   [0, 1). A stationary policy  for an MDP is a
mapping  : X 7 A, where (x) is the action the agent takes at state x. In the computer
network problem, for each possible configuration of working and failing machines, the policy
would tell the SysAdmin which machine to reboot. Each policy is associated with a value
function V  RN , where V (x) is the discounted cumulative value that the agent gets if
it starts at state x and follows policy . More precisely, the value V of a state x under
402

fiEfficient Solution Algorithms for Factored MDPs

Server

Server

Star

Bidirectional Ring

Ring and Star

Server

3 Legs

Ring of Rings

Figure 1: Network topologies tested; the status of a machine is influence by the status of
its parent in the network.

policy  is given by:
V (x) = E

" 
X

t



(t)

(t)

 R X , (X

t=0


#

 (0)
) X = x ,


where X(t) is a random variable representing the state of the system after t steps. In our
running example, the value function represents how much money the SysAdmin expects to
collect if she starts acting according to  when the network is at state x. The value function
for a fixed policy is the fixed point of a set of linear equations that define the value of a
state in terms of the value of its possible successor states. More formally, we define:
Definition 2.2 The DP operator, T , for a stationary policy  is:
T V(x) = R(x, (x)) + 

X

P (x0 | x, (x))V(x0 ).

x0

The value function of policy , V , is the fixed point of the T operator: V = T V .
The optimal value function V  describes the optimal value the agent can achieve for
each starting state. V  is also defined by a set of non-linear equations. In this case, the
value of a state must be the maximal expected value achievable by any policy starting at
that state. More precisely, we define:
Definition 2.3 The Bellman operator, T  , is:
T  V(x) = max[R(x, a) + 
a

X

P (x0 | x, a)V(x0 )].

x0

The optimal value function V  is the fixed point of T  : V  = T  V  .
For any value function V, we can define the policy obtained by acting greedily relative
to V. In other words, at each state, the agent takes the action that maximizes the one-step
403

fiGuestrin, Koller, Parr & Venkataraman

utility, assuming that V represents our long-term utility achieved at the next state. More
precisely, we define:
Greedy(V)(x) = arg max[R(x, a) + 
a

X

P (x0 | x, a)V(x0 )].

(1)

x0

The greedy policy relative to the optimal value function V  is the optimal policy   =
Greedy(V  ).
2.2 Factored MDPs
Factored MDPs are a representation language that allows us to exploit problem structure
to represent exponentially large MDPs very compactly. The idea of representing a large
MDP using a factored model was first proposed by Boutilier et al. (1995).
In a factored MDP, the set of states is described via a set of random variables X =
{X1 , . . . , Xn }, where each Xi takes on values in some finite domain Dom(Xi ). A state x
defines a value xi  Dom(Xi ) for each variable Xi . In general, we use upper case letters
(e.g., X) to denote random variables, and lower case (e.g., x) to denote their values. We
use boldface to denote vectors of variables (e.g., X) or their values (x). For an instantiation
y  Dom(Y) and a subset of these variables Z  Y, we use y[Z] to denote the value of the
variables Z in the instantiation y.
In a factored MDP, we define a state transition model  using a dynamic Bayesian
network (DBN) (Dean & Kanazawa, 1989). Let Xi denote the variable Xi at the current
time and Xi0 , the same variable at the next step. The transition graph of a DBN is a
two-layer directed acyclic graph G whose nodes are {X1 , . . . , Xn , X10 , . . . , Xn0 }. We denote
the parents of Xi0 in the graph by Parents (Xi0 ). For simplicity of exposition, we assume
that Parents (Xi0 )  X; thus, all arcs in the DBN are between variables in consecutive
time slices. (This assumption is used for expository purposes only; intra-time slice arcs
are handled by a small modification presented in Section 4.1.) Each node Xi0 is associated
with a conditional probability distribution (CPD) P (Xi0 | Parents (Xi0 )). The transition
probability P (x0 | x) is then defined to be:
P (x0 | x) =

Y

P (x0i | ui ) ,

i

where ui is the value in x of the variables in Parents (Xi0 ).
Example 2.4 Consider an instance of the SysAdmin problem with four computers, labelled
M1 , . . . , M4 , in an unidirectional ring topology as shown in Figure 2(a). Our first task in
modeling this problem as a factored MDP is to define the state space X. Each machine
is associated with a binary random variable Xi , representing whether it is working or has
failed. Thus, our state space is represented by four random variables: {X1 , X2 , X3 , X4 }.
The next task is to define the transition model, represented as a DBN. The parents of the
next time step variables Xi0 depend on the network topology. Specifically, the probability that
machine i will fail at the next time step depends on whether it is working at the current
time step and on the status of its direct neighbors (parents in the topology) in the network
at the current time step. As shown in Figure 2(b), the parents of Xi0 in this example are Xi
and Xi1 . The CPD of Xi0 is such that if Xi = false, then Xi0 = false with high probability;
404

fiEfficient Solution Algorithms for Factored MDPs

X1

X1
R

X2

M1

R

M4

M2

M3

(a)

2

X3
R

X4
R

1

3

P (Xi0 = t | Xi , Xi1 , A):
h
1

h

2

X2

X3
h

3

X4
h

4

(b)

Action is reboot:
machine i other machine

4

Xi1
Xi
Xi1
Xi
Xi1
Xi
Xi1
Xi

=f
=f
=f
=t
=t
=f
=t
=t






1

0.0238

1

0.475

1

0.0475

1

0.95

(c)

Figure 2: Factored MDP example: from a network topology (a) we obtain the factored
MDP representation (b) with the CPDs described in (c).

that is, failures tend to persist. If Xi = true, then Xi0 is a noisy or of its other parents (in
the unidirectional ring topology Xi0 has only one other parent Xi1 ); that is, a failure in any
of its neighbors can independently cause machine i to fail.
We have described how to represent factored the Markovian transition dynamics arising
from an MDP as a DBN, but we have not directly addressed the representation of actions.
Generally, we can define the transition dynamics of an MDP by defining a separate DBN
model a = hGa , Pa i for each action a.
Example 2.5 In our system administrator example, we have an action ai for rebooting
each one of the machines, and a default action d for doing nothing. The transition model
described above corresponds to the do nothing action. The transition model for ai is
different from d only in the transition model for the variable Xi0 , which is now Xi0 = true
with probability one, regardless of the status of the neighboring machines. Figure 2(c) shows
the actual CPD for P (Xi0 = W orking | Xi , Xi1 , A), with one entry for each assignment to
the state variables Xi and Xi1 , and to the action A.
To fully specify an MDP, we also need to provide a compact representation of the reward
function. We assume that the reward function is factored additively into a set of localized
reward functions, each of which only depends on a small set of variables. In our example, we
might have a reward function associated with each machine i, which depends on Xi . That
is, the SysAdmin is paid on a per-machine basis: at every time step, she receives money for
machine i only if it is working. We can formalize this concept of localized functions:
Definition 2.6 A function f has a scope Scope[f ] = C  X if f : Dom(C) 7 R.
If f has scope Y and Y  Z, we use f (z) as shorthand for f (y) where y is the part of the
instantiation z that corresponds to variables in Y.
405

fiGuestrin, Koller, Parr & Venkataraman

We can now characterize the concept of local rewards. Let R1a , . . . , Rra be a set of
functions, where the scope of each Ria is restricted to variable cluster Uai  {X1 , . . . , Xn }.
P
The reward for taking action a at state x is defined to be Ra (x) = ri=1 Ria (Uai )  R. In
our example, we have a reward function Ri associated with each machine i, which depends
only Xi , and does not depend on the action choice. These local rewards are represented
by the diamonds in Figure 2(b), in the usual notation for influence diagrams (Howard &
Matheson, 1984).

3. Approximate Solution Algorithms
There are several algorithms to compute the optimal policy in an MDP. The three most
commonly used are value iteration, policy iteration, and linear programming. A key component in all three algorithms is the computation of value functions, as defined in Section 2.1.
Recall that a value function defines a value for each state x in the state space. With an
explicit representation of the value function as a vector of values for the different states,
the solution algorithms all can be implemented as a series of simple algebraic steps. Thus,
in this case, all three can be implemented very efficiently.
Unfortunately, in the case of factored MDPs, the state space is exponential in the number
of variables in the domain. In the SysAdmin problem, for example, the state x of the system
is an assignment describing which machines are working or have failed; that is, a state x
is an assignment to each random variable Xi . Thus, the number of states is exponential in
the number m of machines in the network (|X| = N = 2m ). Hence, even representing an
explicit value function in problems with more than about ten machines is infeasible. One
might be tempted to believe that factored transition dynamics and rewards would result in
a factored value function, which can thereby be represented compactly. Unfortunately, even
in trivial factored MDPs, there is no guarantee that structure in the model is preserved in
the value function (Koller & Parr, 1999).
In this section, we discuss the use of an approximate value function, that admits a
compact representation. We also describe approximate versions of these exact algorithms,
that use approximate value functions. Our description in this section is somewhat abstract,
and does not specify how the basic operations required by the algorithms can be performed
explicitly. In later sections, we elaborate on these issues, and describe the algorithms in
detail. For brevity, we choose to focus on policy iteration and linear programming; our
techniques easily extend to value iteration.
3.1 Linear Value Functions
A very popular choice for approximating value functions is by using linear regression, as first
proposed by Bellman et al. (1963). Here, we define our space of allowable value functions
V  H  RN via a set of basis functions:
Definition 3.1 A linear value function over a set of basis functions H = {h1 , . . . , hk }
P
is a function V that can be written as V(x) = kj=1 wj hj (x) for some coefficients w =
(w1 , . . . , wk )0 .
We can now define H to be the linear subspace of RN spanned by the basis functions H.
It is useful to define an N  k matrix H whose columns are the k basis functions viewed as
406

fiEfficient Solution Algorithms for Factored MDPs

vectors. In a more compact notation, our approximate value function is then represented
by Hw.
The expressive power of this linear representation is equivalent, for example, to that
of a single layer neural network with features corresponding to the basis functions defining
H. Once the features are defined, we must optimize the coefficients w in order to obtain a
good approximation for the true value function. We can view this approach as separating
the problem of defining a reasonable space of features and the induced space H, from the
problem of searching within the space. The former problem is typically the purview of
domain experts, while the latter is the focus of analysis and algorithmic design. Clearly,
feature selection is an important issue for essentially all areas of learning and approximation.
We offer some simple methods for selecting good features for MDPs in Section 11, but it is
not our goal to address this large and important topic in this paper.
Once we have a chosen a linear value function representation and a set of basis functions,
the problem becomes one of finding values for the weights w such that Hw will yield
a good approximation of the true value function. In this paper, we consider two such
approaches: approximate dynamic programming using policy iteration and approximate
linear programming. In this section, we present these two approaches. In Section 4, we
show how we can exploit problem structure to transform these approaches into practical
algorithms that can deal with exponentially large state spaces.
3.2 Policy Iteration
3.2.1 The Exact Algorithm
The exact policy iteration algorithm iterates over policies, producing an improved policy at
each iteration. Starting with some initial policy  (0) , each iteration consists of two phases.
Value determination computes, for a policy  (t) , the value function V(t) , by finding the
fixed point of the equation T(t) V(t) = V(t) , that is, the unique solution to the set of linear
equations:
X
P (x0 | x,  (t) (x))V(t) (x0 ), x.
V(t) (x) = R(x,  (t) (x)) + 
x0

The policy improvement step defines the next policy as
 (t+1) = Greedy(V(t) ).
It can be shown that this process converges to the optimal policy (Bertsekas & Tsitsiklis,
1996). Furthermore, in practice, the convergence to the optimal policy is often very quick.
3.2.2 Approximate Policy Iteration
The steps in the policy iteration algorithm require a manipulation of both value functions
and policies, both of which often cannot be represented explicitly in large MDPs. To define
a version of the policy iteration algorithm that uses approximate value functions, we use
the following basic idea: We restrict the algorithm to using only value functions within the
provided H; whenever the algorithm takes a step that results in a value function V that is
outside this space, we project the result back into the space by finding the value function
within the space which is closest to V. More precisely:
407

fiGuestrin, Koller, Parr & Venkataraman

Definition 3.2 A projection operator  is a mapping  : RN  H.  is said to be a
projection w.r.t. a norm kk if V = Hw such that w  arg minw kHw  Vk.
That is, V is the linear combination of the basis functions, that is closest to V with respect
to the chosen norm.
Our approximate policy iteration algorithm performs the policy improvement step exactly. In the value determination step, the value function  the value of acting according to
the current policy  (t)  is approximated through a linear combination of basis functions.
We now consider the problem of value determination for a policy  (t) . At this point,
it is useful to introduce some notation: Although the rewards are a function of the state
and action choice, once the policy is fixed, the rewards become a function of the state
only, which we denote as R(t) , where R(t) (x) = R(x,  (t) (x)). Similarly, for the transition
model: P(t) (x0 | x) = P (x0 | x,  (t) (x)). We can now rewrite the value determination step
in terms of matrices and vectors. If we view V(t) and R(t) as N -vectors, and P(t) as an
N  N matrix, we have the equations:
V(t) = R(t) + P(t) V(t) .
This is a system of linear equations with one equation for each state, which can only be
solved exactly for relatively small N . Our goal is to provide an approximate solution, within
H. More precisely, we want to find:
w(t) = arg min kHw  (R(t) + P(t) Hw)k ;
w







= arg min (H  P(t) H) w(t)  R(t)  .
w

Thus, our approximate policy iteration alternates between two steps:
w(t) = arg min kHw  (R(t) + P(t) Hw)k ;
w

 (t+1) = Greedy(Hw(t) ).

(2)
(3)

3.2.3 Max-norm Projection
An approach along the lines described above has been used in various papers, with several
recent theoretical and algorithmic results (Schweitzer & Seidmann, 1985; Tsitsiklis & Van
Roy, 1996b; Van Roy, 1998; Koller & Parr, 1999, 2000). However, these approaches suffer
from a problem that we might call norm incompatibility. When computing the projection,
they utilize the standard Euclidean projection operator with respect to the L2 norm or a
weighted L2 norm.1 On the other hand, most of the convergence and error analyses for MDP
algorithms utilize max-norm (L ). This incompatibility has made it difficult to provide
error guarantees.
We can tie the projection operator more closely to the error bounds through the use
of a projection operator in L norm. The problem of minimizing the L norm has been
studied in the optimization literature as the problem of finding the Chebyshev solution2 to
1. Weighted L2 norm projections are stable and have meaningful error bounds when the weights correspond
to the stationary distribution of a fixed policy under evaluation (value determination) (Van Roy, 1998),
but they are not stable when combined with T  . Averagers (Gordon, 1995) are stable and non-expansive
in L , but require that the mixture weights be determined a priori. Thus, they do not, in general,
minimize L error.
2. The Chebyshev norm is also referred to as max, supremum and L norms and the minimax solution.

408

fiEfficient Solution Algorithms for Factored MDPs

an overdetermined linear system of equations (Cheney, 1982). The problem is defined as
finding w such that:
w  arg min kCw  bk .
(4)
w

We use an algorithm due to Stiefel (1960), that solves this problem by linear programming:
Variables: w1 , . . . , wk ,  ;
Minimize:  ;
P
(5)
Subject to:   kj=1 cij wj  bi and
Pk
  bi  j=1 cij wj , i = 1...N.



P


The constraints in this linear program imply that    kj=1 cij wj  bi  for each i, or
equivalently, that   kCw  bk . The objective of the LP is to minimize . Thus, at the
solution (w ,  ) of this linear program, w is the solution of Equation (4) and  is the L
projection error.
We can use the L projection in the context of the approximate policy iteration in the
obvious way. When implementing the projection operation of Equation (2), we can use
the L projection (as in Equation (4)), where C = (H  P(t) H) and b = R(t) . This
minimization can be solved using the linear program of (5).
A key point is that this LP only has k + 1 variables. However, there are 2N constraints,
which makes it impractical for large state spaces. In the SysAdmin problem, for example,
the number of constraints in this LP is exponential in the number of machines in the network
(a total of 2  2m constraints for m machines). In Section 4, we show that, in factored MDPs
with linear value functions, all the 2N constraints can be represented efficiently, leading to
a tractable algorithm.
3.2.4 Error Analysis
We motivated our use of the max-norm projection within the approximate policy iteration
algorithm via its compatibility with standard error analysis techniques for MDP algorithms.
We now provide a careful analysis of the impact of the L error introduced by the projection step. The analysis provides motivation for the use of a projection step that directly
minimizes this quantity. We acknowledge, however, that the main impact of this analysis
is motivational. In practice, we cannot provide a priori guarantees that an L projection
will outperform other methods.
Our goal is to analyze approximate policy iteration in terms of the amount of error
introduced at each step by the projection operation. If the error is zero, then we are
performing exact value determination, and no error should accrue. If the error is small, we
should get an approximation that is accurate. This result follows from the analysis below.
More precisely, we define the projection error as the error resulting from the approximate
value determination step:








 (t) = Hw(t)  R(t) + P(t) Hw(t)  .


Note that, by using our max-norm projection, we are finding the set of weights w(t) that
exactly minimizes the one-step projection error  (t) . That is, we are choosing the best
409

fiGuestrin, Koller, Parr & Venkataraman

possible weights with respect to this error measure. Furthermore, this is exactly the error
measure that is going to appear in the bounds of our theorem. Thus, we can now make the
bounds for each step as tight as possible.
We first show that the projection error accrued in each step is bounded:
Lemma 3.3 The value determination error is bounded: There exists a constant P  Rmax
such that P   (t) for all iterations t of the algorithm.
Proof: See Appendix A.1.
Due to the contraction property of the Bellman operator, the overall accumulated error
is a decaying average of the projection error incurred throughout all iterations:
Definition 3.4 The discounted value determination error at iteration t is defined as: 
(t1)
(0)
 (t) + 
;  = 0.

(t)

=

Lemma 3.3 implies that the accumulated error remains bounded in approximate policy
(t)
(1 t )
iteration:   P 1
. We can now bound the loss incurred when acting according
to the policy generated by our approximate policy iteration algorithm, as opposed to the
optimal policy:
Theorem 3.5 In the approximate policy iteration algorithm, let  (t) be the policy generated
at iteration t. Furthermore, let V(t) be the actual value of acting according to this policy.
The loss incurred by using policy  (t) as opposed to the optimal policy   with value V  is
bounded by:
(t)
2
t


kV  V(t) k   kV  V(0) k +
.
(6)
(1  )2
Proof: See Appendix A.2.
In words, Equation (6) shows that the difference between our approximation at iteration
t and the optimal value function is bounded by the sum of two terms. The first term is
present in standard policy iteration and goes to zero exponentially fast. The second is the
discounted accumulated projection error and, as Lemma 3.3 shows, is bounded. This second
term can be minimized by choosing w(t) as the one that minimizes:





Hw(t)  R(t) + P(t) Hw(t) 



,

which is exactly the computation performed by the max-norm projection. Therefore, this
theorem motivates the use of max-norm projections to minimize the error term that appears
in our bound.
The bounds we have provided so far may seem fairly trivial, as we have not provided
a strong a priori bound on  (t) . Fortunately, several factors make these bounds interesting despite the lack of a priori guarantees. If approximate policy iteration converges, as
b is the policy
occurred in all of our experiments, we can obtain a much tighter bound: If 
after convergence, then:
 


V  V   2b
,
b
 
(1  )
where b is the one-step max-norm projection error associated with estimating the value
b . Since the max-norm projection operation provides b
of 
 , we can easily obtain an a
410

fiEfficient Solution Algorithms for Factored MDPs

posteriori bound as part of the policy iteration procedure. More details are provided in
Section 7.
One could rewrite the bound in Theorem 3.5 in terms of the worst case projection error P , or the worst projection error in a cycle of policies, if approximate policy iteration
gets stuck in a cycle. These formulations would be closer to the analysis of Bertsekas and
Tsitsiklis (1996, Proposition 6.2, p.276). However, consider the case where most policies
(or most policies in the final cycle) have a low projection error, but there are a few policies
that cannot be approximated well using the projection operation, so that they have a large
one-step projection error. A worst-case bound would be very loose, because it would be
dictated by the error of the most difficult policy to approximate. On the other hand, using
our discounted accumulated error formulation, errors introduced by policies that are hard
to approximate decay very rapidly. Thus, the error bound represents an average case
analysis: a decaying average of the projection errors for policies encountered at the successive iterations of the algorithm. As in the convergent case, this bound can be computed
easily as part of the policy iteration procedure when max-norm projection is used.
The practical benefit of a posteriori bounds is that they can give meaningful feedback on
the impact of the choice of the value function approximation architecture. While we are not
explicitly addressing the difficult and general problem of feature selection in this paper, our
error bounds motivate algorithms that aim to minimize the error given an approximation
architecture and provide feedback that could be useful in future efforts to automatically
discover or improve approximation architectures.
3.3 Approximate Linear Programming
3.3.1 The Exact Algorithm
Linear programming provides an alternative method for solving MDPs. It formulates the
problem of finding a value function as a linear program (LP). Here the LP variables are
V1 , . . . , VN , where Vi represents V(xi ): the value of starting at the ith state of the system.
The LP is given by:
Variables: V1 , . . . , VN ;
P
Minimize:
xi (xi ) Vi ;
P
Subject to: Vi  [R(xi , a) +  j P (xj | xi , a)Vj ] xi  X, a  A,

(7)

where the state relevance weights  are positive. Note that, in this exact case, the solution
obtained is the same for any positive weight vector. It is interesting to note that steps of
the simplex algorithm correspond to policy changes at single states, while steps of policy
iteration can involve policy changes at multiple states. In practice, policy iteration tends
to be faster than the linear programming approach (Puterman, 1994).
3.3.2 Approximate Linear Program
The approximate formulation for the LP approach, first proposed by Schweitzer and Seidmann (1985), restricts the space of allowable value functions to the linear space spanned
by our basis functions. In this approximate formulation, the variables are w1 , . . . , wk : the
weights for our basis functions. The LP is given by:
411

fiGuestrin, Koller, Parr & Venkataraman

Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) i wi hi (x) ;
Px
P
P
0
0
Subject to:
i wi hi (x)  [R(x, a) + 
x0 P (x | x, a)
i wi hi (x )] x  X, a  A.
(8)
In other words, this formulation takes the LP in (7) and substitutes the explicit state
P
value function by a linear value function representation i wi hi (x), or, in our more compact
notation, V is replaced by Hw. This linear program is guaranteed to be feasible if a constant
function  a function with the same constant value for all states  is included in the set
of basis functions.
In this approximate linear programming formulation, the choice of state relevance weights,
, becomes important. Intuitively, not all constraints in this LP are binding; that is, the
constraints are tighter for some states than for others. For each state x, the relevance
weight (x) indicates the relative importance of a tight constraint. Therefore, unlike the
exact case, the solution obtained may differ for different choices of the positive weight vector
. Furthermore, there is, in general, no guarantee as to the quality of the greedy policy
generated from the approximation Hw. However, the recent work of de Farias and Van
Roy (2001a) provides some analysis of the error relative to that of the best possible approximation in the subspace, and some guidance as to selecting  so as to improve the quality
of the approximation. In particular, their analysis shows that this LP provides the best
approximation Hw of the optimal value function V  in a weighted L1 sense subject to the
constraint that Hw  T  Hw , where the weights in the L1 norm are the state relevance
weights .
The transformation from an exact to an approximate problem formulation has the effect of reducing the number of free variables in the LP to k (one for each basis function
coefficient), but the number of constraints remains N  |A|. In our SysAdmin problem, for
example, the number of constraints in the LP in (8) is (m + 1)  2m , where m is the number
of machines in the network. Thus, the process of generating the constraints and solving the
LP still seems unmanageable for more than a few machines. In the next section, we discuss
how we can use the structure of a factored MDP to provide for a compact representation
and an efficient solution to this LP.

4. Factored Value Functions
The linear value function approach, and the algorithms described in Section 3, apply to any
choice of basis functions. In the context of factored MDPs, Koller and Parr (1999) suggest
a particular type of basis function, that is particularly compatible with the structure of a
factored MDP. They suggest that, although the value function is typically not structured,
there are many cases where it might be close to structured. That is, it might be wellapproximated using a linear combination of functions each of which refers only to a small
number of variables. More precisely, we define:
Definition 4.1 A factored (linear) value function is a linear function over the basis set
h1 , . . . , hk , where the scope of each hi is restricted to some subset of variables Ci .
Value functions of this type have a long history in the area of multi-attribute utility theory (Keeney & Raiffa, 1976). In our example, we might have a basis function hi for each
412

fiEfficient Solution Algorithms for Factored MDPs

machine, indicating whether it is working or not. Each basis function has scope restricted
to Xi . These are represented as diamonds in the next time step in Figure 2(b).
Factored value functions provide the key to performing efficient computations over the
exponential-sized state spaces we have in factored MDPs. The main insight is that restricted scope functions (including our basis functions) allow for certain basic operations to
be implemented very efficiently. In the remainder of this section, we show how structure in
factored MDPs can be exploited to perform two crucial operations very efficiently: one-step
lookahead (backprojection), and the representation of exponentially many constraints in
the LPs. Then, we use these basic building blocks to formulate very efficient approximation algorithms for factored MDPs, each presented in its own self-contained section: the
approximate linear programming for factored MDPs in Section 5, and approximate policy
iteration with max-norm projection in Section 6.
4.1 One-step Lookahead
A key step in all of our algorithms is the computation of the one-step lookahead value of
some action a. This is necessary, for example, when computing the greedy policy as in
Equation (1). Lets consider the computation of a Q function, Qa (x), which represents the
expected value the agent obtains after taking action a at the current time step and receiving
a long-term value V thereafter. This Q function can be computed by:
Qa (x) = R(x, a) + 

X

P (x0 | x, a)V(x).

(9)

x0

That is, Qa (x) is given by the current reward plus the discounted expected future value.
Using this notation, we can express the greedy policy as: Greedy(V)(x) = maxa Qa (x).
Recall that we are estimating the long-term value of our policy using a set of basis
P
functions: V(x) = i wi hi (x). Thus, we can rewrite Equation (9) as:
Qa (x) = R(x, a) + 

X

P (x0 | x, a)

X

x0

wi hi (x).

(10)

i

P

The size of the state space is exponential, so that computing the expectation x0 P (x0 |
P
x, a) i wi hi (x) seems infeasible. Fortunately, as discussed by Koller and Parr (1999),
this expectation operation, or backprojection, can be performed efficiently if the transition
model and the value function are both factored appropriately. The linearity of the value
function permits a linear decomposition, where each summand in the expectation can be
viewed as an independent value function and updated in a manner similar to the value
iteration procedure used by Boutilier et al. (2000). We now recap the construction briefly,
by first defining:
Ga (x) =

X
x0

P (x0 | x, a)

X

wi hi (x0 ) =

i

X
i

wi

X

P (x0 | x, a)hi (x0 ).

x0

Thus, we can compute the expectation of each basis function separately:
gia (x) =

X

P (x0 | x, a)hi (x0 ),

x0

413

fiGuestrin, Koller, Parr & Venkataraman

P

and then weight them by wi to obtain the total expectation Ga (x) = i wi gia (x). The
intermediate function gia is called the backprojection of the basis function hi through the
transition model Pa , which we denote by gia = Pa hi . Note that, in factored MDPs, the
transition model Pa is factored (represented as a DBN) and the basis functions hi have
scope restricted to a small set of variables. These two important properties allow us to
compute the backprojections very efficiently.
We now show how some restricted scope function h (such as our basis functions)
can be backprojected through some transition model P represented as a DBN  . Here
h has scope restricted to Y; our goal is to compute g = P h. We define the backprojected scope of Y through  as the set of parents of Y0 in the transition graph G ;
 (Y0 ) = Yi0 Y0 Parents (Yi0 ). If intra-time slice arcs are included, so that Parents (Xi0 ) 
{X1 , . . . , Xn , X10 , . . . , Xn0 }, then the only change in our algorithm is in the definition of backprojected scope of Y through  . The definition now includes not only direct parents of Y 0 ,
but also all variables in {X1 , . . . , Xn } that are ancestors of Y 0 :
 (Y0 ) = {Xj | there exist a directed path from Xj to any Xi0  Y0 }.
Thus, the backprojected scope may become larger, but the functions are still factored.
We can now show that, if h has scope restricted to Y, then its backprojection g has
scope restricted to the parents of Y0 , i.e.,  (Y0 ). Furthermore, each backprojection can
be computed by only enumerating settings of variables in  (Y0 ), rather than settings of
all variables X:
g(x) = (P h)(x);
=

X

P (x0 | x)h(x0 );

x0

=

X

P (x0 | x)h(y0 );

x0

=

X
y0

=

X

X

P (y0 | x)h(y0 )

P (u0 | x);

u0 (x0 y0 )

P (y0 | z)h(y0 );

y0

= g(z);
P

where z is the value of  (Y0 ) in x and the term u0 (x0 y0 ) P (u0 | x) = 1 as it is the
sum of a probability distribution over a complete domain. Therefore, we see that (P h) is a
function whose scope is restricted to  (Y0 ). Note that the cost of the computation depends
linearly on |Dom( (Y0 ))|, which depends on Y (the scope of h) and on the complexity of
the process dynamics. This backprojection procedure is summarized in Figure 3.
Returning to our example, consider a basis function hi that is an indicator of variable Xi :
it takes value 1 if the ith machine is working and 0 otherwise. Each hi has scope restricted to
Xi0 , thus, its backprojection gi has scope restricted to Parents (Xi0 ):  (Xi0 ) = {Xi1 , Xi }.
4.2 Representing Exponentially Many Constraints
As seen in Section 3, both our approximation algorithms require the solution of linear programs: the LP in (5) for approximate policy iteration, and the LP in (8) for the approximate
414

fiEfficient Solution Algorithms for Factored MDPs

Backproja (h) 

where basis function h has scope C.

Define the scope of the backprojection: a (C0 ) = Xi0 C0 Parentsa (Xi0 ).
0
For each assignment
P
Q y  a (C ):0 0
a
g (y) = c0 C0 i|X 0 C0 Pa (c [Xi ] | y)h(c0 ).
i

Return g a .

Figure 3: Backprojection of basis function h.
linear programming algorithm. These LPs have some common characteristics: they have
a small number of free variables (for k basis functions there are k + 1 free variables in approximate policy iteration and k in approximate linear programming), but the number of
constraints is still exponential in the number of state variables. However, in factored MDPs,
these LP constraints have another very useful property: the functionals in the constraints
have restricted scope. This key observation allows us to represent these constraints very
compactly.
First, observe that the constraints in the linear programs are all of the form:


X

wi ci (x)  b(x), x,

(11)

i

where only  and w1 , . . . , wk are free variables in the LP and x ranges over all states. This
general form represents both the type of constraint in the max-norm projection LP in (5)
and the approximate linear programming formulation in (8).3
The first insight in our construction is that we can replace the entire set of constraints
in Equation (11) by one equivalent non-linear constraint:
  max
x

X

wi ci (x)  b(x).

(12)

i

The second insight is that this new non-linear constraint can be implemented by a set of
linear constraints using a construction that follows the structure of variable elimination in
cost networks. This insight allows us to exploit structure in factored MDPs to represent
this constraint compactly.
We tackle the problem of representing the constraint in Equation (12) in two steps:
first, computing the maximum assignment for a fixed set of weights; then, representing the
non-linear constraint by small set of linear constraints, using a construction we call the
factored LP.
4.2.1 Maximizing Over the State Space
The key computation in our algorithms is to represent a non-linear constraint of the form
in Equation (12) efficiently by a small set of linear constraints. Before presenting this construction, lets first consider a simpler problem: Given some fixed weights wi , we would
P
like to compute the maximization:  = maxx i wi ci (x)  b(x), that is, the state x, such
P

3. The complementary constraints in (5),   b(x)  i wi ci (x), can be formulated using an analogous
construction to the one we present in this section by changing the sign of ci (x) and b(x). The approximate
linear programming constraints of (8) can also be formulated in this form, as we show in Section 5.

415

fiGuestrin, Koller, Parr & Venkataraman

P

that the difference between i wi ci (x) and b(x) is maximal. However, we cannot explicitly enumerate the exponential number of states and compute the difference. Fortunately,
structure in factored MDPs allows us to compute this maximum efficiently.
In the case of factored MDPs, our state space is a set of vectors x which are assignments to the state variables X = {X1 , . . . , Xn }. We can view both Cw and b as functions
of these state variables, and hence also their difference. Thus, we can define a function
P
F w (X1 , . . . , Xn ) such that F w (x) = i wi ci (x)  b(x). Note that we have executed a
representation shift; we are viewing F w as a function of the variables X, which is parameterized by w. Recall that the size of the state space is exponential in the number
of variables. Hence, our goal in this section is to compute maxx F w (x) without explicitly
considering each of the exponentially many states. The solution is to use the fact that F w
P
has a factored representation. More precisely, Cw has the form i wi ci (Zi ), where Zi is
a subset of X. For example, we might have c1 (X1 , X2 ) which takes value 1 in states where
X1 = true and X2 = false and 0 otherwise. Similarly, the vector b in our case is also a sum
P
of restricted scope functions. Thus, we can express F w as a sum j fjw (Zj ), where fjw may
or may not depend on w. In the future, we sometimes drop the superscript w when it is
clear from context.
P
Using our more compact notation, our goal here is simply to compute maxx i wi ci (x)
b(x) = maxx F w (x), that is, to find the state x over which F w is maximized. Recall that
P
w
Fw = m
j=1 fj (Zj ). We can maximize such a function, F , without enumerating every state
using non-serial dynamic programming (Bertele & Brioschi, 1972). The idea is virtually
identical to variable elimination in a Bayesian network. We review this construction here,
as it is a central component in our solution LP.
Our goal is to compute
X
max
fj (x[Zj ]).
x1 ,...,xn

j

The main idea is that, rather than summing all functions and then doing the maximization,
we maximize over variables one at a time. When maximizing over xl , only summands
involving xl participate in the maximization.
Example 4.2 Assume
F = f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).
We therefore wish to compute:
max

x1 ,x2 ,x3 ,x4

f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).

We can first compute the maximum over x4 ; the functions f1 and f2 are irrelevant, so we
can push them out. We get
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + max[f3 (x2 , x4 ) + f4 (x3 , x4 )].

x1 ,x2 ,x3

x4

The result of the internal maximization depends on the values of x2 , x3 ; thus, we can introduce a new function e1 (X2 , X3 ) whose value at the point x2 , x3 is the value of the internal
max expression. Our problem now reduces to computing
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + e1 (x2 , x3 ),

x1 ,x2 ,x3

416

fiEfficient Solution Algorithms for Factored MDPs

VariableElimination (F, O)
//F = {f1 , . . . , fm } is the set of functions to be maximized;
//O stores the elimination order.

For i = 1 to number of variables:
//Select the next variable to be eliminated.

Let l = O(i) ;
//Select the relevant functions.

Let e1 , . . . , eL be the functions in F whose scope contains Xl .
//Maximize over current variable Xl .

Define a new function e = maxxl
L
j=1 Scope[ej ]  {Xl }.

PL

j=1 ej

; note that Scope[e] =

//Update set of functions.

Update the set of functions F = F  {e} \ {e1 , . . . , eL }.
//Now, all functions have empty scope
P and their sum is the maximum value of f1 +    + fm .

Return the maximum value

ei F

ei .

Figure 4: Variable elimination procedure for computing the maximum value f1 +    + fm ,
where each fi is a restricted scope function.

having one fewer variable. Next, we eliminate another variable, say X3 , with the resulting
expression reducing to:
max f1 (x1 , x2 ) + e2 (x1 , x2 ),
x1 ,x2

where

e2 (x1 , x2 ) = max[f2 (x1 , x3 ) + e1 (x2 , x3 )].
x3

Finally, we define
e3 = max f1 (x1 , x2 ) + e2 (x1 , x2 ).
x1 ,x2

The result at this point is a number, which is the desired maximum over x1 , . . . , x4 . While
the naive approach of enumerating all states requires 63 arithmetic operations if all variables
are binary, using variable elimination we only need to perform 23 operations.
The general variable elimination algorithm is described in Figure 4. The inputs to
the algorithm are the functions to be maximized F = {f1 , . . . , fm } and an elimination
ordering O on the variables, where O(i) returns the ith variable to be eliminated. As in
the example above, for each variable Xl to be eliminated, we select the relevant functions
e1 , . . . , eL , those whose scope contains Xl . These functions are removed from the set F and
P
we introduce a new function e = maxxl L
j=1 ej . At this point, the scope of the functions in
F no longer depends on Xl , that is, Xl has been eliminated. This procedure is repeated
until all variables have been eliminated. The remaining functions in F thus have empty
scope. The desired maximum is therefore given by the sum of these remaining functions.
The computational cost of this algorithm is linear in the number of new function
values introduced in the elimination process. More precisely, consider the computation of
a new function e whose scope is Z. To compute this function, we need to compute |Dom[Z]|
different values. The cost of the algorithm is linear in the overall number of these values,
introduced throughout the execution. As shown by Dechter (1999), this cost is exponential
417

fiGuestrin, Koller, Parr & Venkataraman

in the induced width of the cost network, the undirected graph defined over the variables
X1 , . . . , Xn , with an edge between Xl and Xm if they appear together in one of the original
functions fj . The complexity of this algorithm is, of course, dependent on the variable
elimination order and the problem structure. Computing the optimal elimination order
is an NP-hard problem (Arnborg, Corneil, & Proskurowski, 1987) and elimination orders
yielding low induced tree width do not exist for some problems. These issues have been
confronted successfully for a large variety of practical problems in the Bayesian network
community, which has benefited from a large variety of good heuristics which have been
developed for the variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff,
1990; Reed, 1992; Becker & Geiger, 2001).
4.2.2 Factored LP
In this section, we present the centerpiece of our planning algorithms: a new, general
approach for compactly representing exponentially large sets of LP constraints in problems
with factored structure  those where the functions in the constraints can be decomposed
as the sum of restricted scope functions. Consider our original problem of representing
the non-linear constraint in Equation (12) compactly. Recall that we wish to represent
P
the non-linear constraint   maxx i wi ci (x)  b(x), or equivalently,   maxx F w (x),
without generating one constraint for each state as in Equation (11). The new, key insight
is that this non-linear constraint can be implemented using a construction that follows the
structure of variable elimination in cost networks.
Consider any function e used within F (including the original fi s), and let Z be its scope.
For any assignment z to Z, we introduce variable uez , whose value represents ez , into the
linear program. For the initial functions fiw , we include the constraint that ufzi = fiw (z). As
fiw is linear in w, this constraint is linear in the LP variables. Now, consider a new function
e introduced into F by eliminating a variable Xl . Let e1 , . . . , eL be the functions extracted
from F, and let Z be the scope of the resulting e. We introduce a set of constraints:
uez



L
X
ej
j=1

u(z,xl )[Zj ]

xl .

(13)

Let en be the last function generated in the elimination, and recall that its scope is empty.
Hence, we have only a single variable uen . We introduce the additional constraint   uen .
The complete algorithm, presented in Figure 5, is divided into three parts: First, we
generate equality constraints for functions that depend on the weights wi (basis functions).
In the second part, we add the equality constraints for functions that do not depend on the
weights (target functions). These equality constraints let us abstract away the differences
between these two types of functions and manage them in a unified fashion in the third
part of the algorithm. This third part follows a procedure similar to variable elimination
described in Figure 4. However, unlike standard variable elimination where we would inP
troduce a new function e, such that e = maxxl L
j=1 ej , in our factored LP procedure we
introduce new LP variables uez . To enforce the definition of e as the maximum over Xl of
PL
j=1 ej , we introduce the new LP constraints in Equation (13).
Example 4.3 To understand this construction, consider our simple example above, and
assume we want to express the fact that   maxx F w (x). We first introduce a set of
418

fiEfficient Solution Algorithms for Factored MDPs

FactoredLP (C, b,O)
// C = {c1 , . . . , ck } is the set of basis functions.
// b = {b1 , . . . , bm } is the set of target functions.
//O stores the elimination order.
P
P
//Return a (polynomial) set of constraints  equivalent to   i wi ci (x) + j bj (x), x .
//Data structure for the constraints in factored LP.

Let  = {} .
//Data structure for the intermediate functions generated in variable elimination.

Let F = {} .
//Generate equality constraint to abstract away basis functions.

For each ci  C:
Let Z = Scope[ci ].
For each assignment z  Z, create a new LP variable ufzi and add a
constraint to :
ufzi = wi ci (z).
Store new function fi to use in variable elimination step: F = F  {fi }.
//Generate equality constraint to abstract away target functions.

For each bj  b:
Let Z = Scope[bj ].
f
For each assignment z  Z, create a new LP variable uzj and add a
constraint to :
f
uzj = bj (z).
Store new function fj to use in variable elimination step: F = F  {fj }.
//Now, F contains all of the functions involved in the LP, our constraints become:  
P
e (x), x , which we represent compactly using a variable elimination procedure.
e F i
i

For i = 1 to number of variables:

//Select the next variable to be eliminated.

Let l = O(i) ;
//Select the relevant functions.

Let e1 , . . . , eL be the functions in F whose scope contains Xl , and let
Zj = Scope[ej ].
//Introduce linear constraints for the maximum over current variable Xl .

Define a new function e with scope Z = L
j=1 Zj  {Xl } to represent
PL
maxxl j=1 ej .
Add constraints to  to enforce maximum: for each assignment z  Z:
uez 

L
X

e

j
u(z,x
l )[Zj ]

xl .

j=1

//Update set of functions.

Update the set of functions F = F  {e} \ {e1 , . . . , eL }.
//Now, all variables have been eliminated and all functions have empty scope.

Add last constraint to :


X

ei .

ei F

Return .

Figure 5: Factored LP algorithm for the compact representation of the exponential set of
P
P
constraints   i wi ci (x) + j bj (x), x.
419

fiGuestrin, Koller, Parr & Venkataraman

variables ufx11 ,x2 for every instantiation of values x1 , x2 to the variables X1 , X2 . Thus, if
X1 and X2 are both binary, we have four such variables. We then introduce a constraint
defining the value of ufx11 ,x2 appropriately. For example, for our f1 above, we have uft,t1 = 0
and uft,f1 = w1 . We have similar variables and constraints for each fj and each value z in
Zj . Note that each of the constraints is a simple equality constraint involving numerical
constants and perhaps the weight variables w.
Next, we introduce variables for each of the intermediate expressions generated by variable elimination. For example, when eliminating X4 , we introduce a set of LP variables
uex12 ,x3 ; for each of them, we have a set of constraints
uex12 ,x3  ufx32 ,x4 + ufx43 ,x4
one for each value x4 of X4 . We have a similar set of constraint for uex21 ,x2 in terms of
ufx21 ,x3 and uex12 ,x3 . Note that each constraint is a simple linear inequality.
We can now prove that our factored LP construction represents the same constraint as
non-linear constraint in Equation (12):
Theorem 4.4 The constraints generated by the factored LP construction are equivalent to
the non-linear constraint in Equation (12). That is, an assignment to (, w) satisfies the
factored LP constraints if and only if it satisfies the constraint in Equation (12).
Proof: See Appendix A.3.
P
Returning to our original formulation, we have that j fjw is Cw  b in the original
set of constraints. Hence our new set of constraints is equivalent to the original set:  
P
maxx i wi ci (x)  b(x) in Equation (12), which in turn is equivalent to the exponential
P
set of constraints   i wi ci (x)  b(x), x in Equation (11). Thus, we can represent this
exponential set of constraints by a new set of constraints and LP variables. The size of
this new set, as in variable elimination, is exponential only in the induced width of the cost
network, rather than in the total number of variables.
In this section, we presented a new, general approach for compactly representing exponentially-large sets of LP constraints in problems with factored structure. In the remainder
of this paper, we exploit this construction to design efficient planning algorithms for factored
MDPs.
4.2.3 Factored Max-norm Projection
We can now use our procedure for representing the exponential number of constraints in
Equation (11) compactly to compute efficient max-norm projections, as in Equation (4):
w  arg min kCw  bk .
w

The max-norm projection is computed by the linear program in (5). There are two sets
P
P
of constraints in this LP:   kj=1 cij wj  bi , i and   bi  kj=1 cij wj , i. Each of
these sets is an instance of the constraints in Equation (11), which we have just addressed
in the previous section. Thus, if each of the k basis functions in C is a restricted scope
function and the target function b is the sum of restricted scope functions, then we can
use our factored LP technique to represent the constraints in the max-norm projection LP
compactly. The correctness of our algorithm is a corollary of Theorem 4.4:
420

fiEfficient Solution Algorithms for Factored MDPs

Corollary 4.5 The solution ( , w ) of a linear program that minimizes  subject to the
constraints in FactoredLP(C, b,O) and FactoredLP(C, b,O), for any elimination
order O satisfies:
w  arg min kCw  bk ,
w

and

 = min kCw  bk .
w

The original max-norm projection LP had k + 1 variables and two constraints for each
state x; thus, the number of constraints is exponential in the number of state variables.
On the other hand, our new factored max-norm projection LP has more variables, but
exponentially fewer constraints. The number of variables and constraints in the new factored
LP is exponential only in the number of state variables in the largest factor in the cost
network, rather than exponential in the total number of state variables. As we show in
Section 9, this exponential gain allows us to compute max-norm projections efficiently when
solving very large factored MDPs.

5. Approximate Linear Programming
We begin with the simplest of our approximate MDP solution algorithms, based on the
approximate linear programming formulation in Section 3.3. Using the basic operations
described in Section 4, we can formulate an algorithm that is both simple and efficient.
5.1 The Algorithm
As discussed in Section 3.3, approximate linear program formulation is based on the linear
programming approach to solving MDPs presented in Section 3.3. However, in this approximate version, we restrict the space of value functions to the linear space defined by
our basis functions. More precisely, in this approximate LP formulation, the variables are
w1 , . . . , wk  the weights for our basis functions. The LP is given by:
Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) i wi hi (x) ;
x
P
P
P
0
0
Subject to:
i wi hi (x)  [R(x, a) + 
x0 P (x | x, a)
i wi hi (x )] x  X, a  A.
(14)
In other words, this formulation takes the LP in (7) and substitutes the explicit state value
P
function with a linear value function representation i wi hi (x). This transformation from
an exact to an approximate problem formulation has the effect of reducing the number
of free variables in the LP to k (one for each basis function coefficient), but the number
of constraints remains |X|  |A|. In our SysAdmin problem, for example, the number of
constraints in the LP in (14) is (m + 1)  2m , where m is the number of machines in the
network. However, using our algorithm for representing exponentially large constraint sets
compactly we are able to compute the solution to this approximate linear programming
algorithm in closed form with an exponentially smaller LP, as in Section 4.2.
P
P
First, consider the objective function x (x) i wi hi (x) of the LP (14). Naively
representing this objective function requires a summation over a exponentially large state
space. However, we can rewrite the objective and obtain a compact representation. We
first reorder the terms:
421

fiGuestrin, Koller, Parr & Venkataraman

FactoredALP (P , R, , H, O, )
//P is the factored transition model.
//R is the set of factored reward functions.
// is the discount factor.
//H is the set of basis functions H = {h1 , . . . , hk }.
//O stores the elimination order.
// are the state relevance weights.
//Return the basis function weights w computed by approximate linear programming.
//Cache the backprojections of the basis functions.

For each basis function hi  H; for each action a:
Let gia = Backproja (hi ).
//Compute factored state relevance weights.

For each basis function hi , compute the factored state relevance weights
i as in Equation (15) .
//Generate approximate linear programming constraints

Let  = {}.
For each action a:
}, Ra , O).
Let  =   FactoredLP({g1a  h1 , . . . , gka  hkP

P

//So far, our constraints guarantee that   R(x, a) +  x0 P (x0 | x, a) i wi hi (x0 ) 
P
w hi (x); to satisfy the approximate linear programming solution in (14) we must add
i i
a final constraint.

Let  =   { = 0}.
//We can now obtain the solution weights by solving an LP.

Let w be the solution of the linear program: minimize
the constraints .
Return w.

P
i

i wi , subject to

Figure 6: Factored approximate linear programming algorithm.

422

fiEfficient Solution Algorithms for Factored MDPs

X

(x)

X

x

wi hi (x) =

X

i

X

wi

(x) hi (x).

x

i

Now, consider the state relevance weights (x) as a distribution over states, so that (x) > 0
P
and x (x) = 1. As in backprojections, we can now write:
i =

X

X

(x) hi (x) =

x

(ci ) hi (ci );

(15)

ci Ci

where (ci ) represents the marginal of the state relevance weights  over the domain
Dom[Ci ] of the basis function hi . For example, if we use uniform state relevance weights as
1
in our experiments  (x) = |X|
 then the marginals become (ci ) = |C1i | . Thus, we can
P
rewrite the objective function as i wi i , where each basis weight i is computed as shown
in Equation (15). If the state relevance weights are represented by marginals, then the cost
of computing each i depends exponentially on the size of the scope of Ci only, rather than
exponentially on the number of state variables. On the other hand, if the state relevance
weights are represented by arbitrary distributions, we need to obtain the marginals over the
Ci s, which may not be an efficient computation. Thus, greatest efficiency is achieved by
using a compact representation, such as a Bayesian network, for the state relevance weights.
Second, note that the right side of the constraints in the LP (14) correspond to the Qa
functions:
X
X
Qa (x) = Ra (x) + 
P (x0 | x, a)
wi hi (x0 ).
x0

i

Using the efficient backprojection operation in factored MDPs described in Section 4.1 we
can rewrite the Qa functions as:
Qa (x) = Ra (x) + 

X

wi gia (x);

i

gia

where
is the backprojection of basis function hi through the transition model Pa . As we
discussed, if hi has scope restricted to Ci , then gia is a restricted scope function of a (C0i ).
We can precompute the backprojections gia and the basis relevance weights i . The
approximate linear programming LP of (14) can be written as:
Variables: w1 , . . . , wk ;
P
Minimize:
 w ;
Pi i i
P
a
a
Subject to:
w
i i hi (x)  [R (x) + 
i wi gi (x)] x  X, a  A.

(16)

Finally, we can rewrite this LP to use constraints of the same form as the one in Equation (12):
Variables: w1 , . . . , wk ;
P
Minimize:
i i wi ;
P
Subject to: 0  maxx {Ra (x) + i wi [gia (x)  hi (x)]} a  A.

(17)

We can now use our factored LP construction in Section 4.2 to represent these non-linear
constraints compactly. Basically, there is one set of factored LP constraints for each action
a. Specifically, we can write the non-linear constraint in the same form as those in Equation (12) by expressing the functions C as: ci (x) = hi (x)gia (x). Each ci (x) is a restricted
423

fiGuestrin, Koller, Parr & Venkataraman

scope function; that is, if hi (x) has scope restricted to Ci , then gia (x) has scope restricted
to a (C0i ), which means that ci (x) has scope restricted to Ci  a (C0i ). Next, the target
function b becomes the reward function Ra (x) which, by assumption, is factored. Finally,
in the constraint in Equation (12),  is a free variable. On the other hand, in the LP in (17)
the maximum in the right hand side must be less than zero. This final condition can be
achieved by adding a constraint  = 0. Thus, our algorithm generates a set of factored
LP constraints, one for each action. The total number of constraints and variables in this
new LP is linear in the number of actions |A| and only exponential in the induced width
of each cost network, rather than in the total number of variables. The complete factored
approximate linear programming algorithm is outlined in Figure 6.
5.2 An Example
We now present a complete example of the operations required by the approximate LP algorithm to solve the factored MDP shown in Figure 2(a). Our presentation follows four steps:
problem representation, basis function selection, backprojections and LP construction.
Problem Representation: First, we must fully specify the factored MDP model for the
problem. The structure of the DBN is shown in Figure 2(b). This structure is maintained
for all action choices. Next, we must define the transition probabilities for each action.
There are 5 actions in this problem: do nothing, or reboot one of the 4 machines in the
network. The CPDs for these actions are shown in Figure 2(c). Finally, we must define the
reward function. We decompose the global reward as the sum of 4 local reward functions,
one for each machine, such that there is a reward if the machine is working. Specifically,
Ri (Xi = true) = 1 and Ri (Xi = false) = 0, breaking symmetry by setting R4 (X4 = true) =
2. We use a discount factor of  = 0.9.
In this simple example, we use five simple basis functions.
Basis Function Selection:
First, we include the constant function h0 = 1. Next, we add indicators for each machine
which take value 1 if the machine is working: hi (Xi = true) = 1 and hi (Xi = false) = 0.
Backprojections:
The first algorithmic step is computing the backprojection of the
basis functions, as defined in Section 4.1. The backprojection of the constant basis is
simple:
g0a =

X

Pa (x0 | x)h0 ;

x0

=

X

Pa (x0 | x) 1 ;

x0

= 1.
Next, we must backproject our indicator basis functions hi :
gia =

X
x0

=

Pa (x0 | x)hi (x0i ) ;

X

Y

Pa (x0j | xj1 , xj )hi (x0i ) ;

x01 ,x02 ,x03 ,x04 j

424

fiEfficient Solution Algorithms for Factored MDPs

=

X
x0i

=

X

X

Pa (x0i | xi1 , xi )hi (x0i )

Y

Pa (x0j | xj1 , xj ) ;

x0 [X0 {Xi0 }] j6=i

Pa (x0i | xi1 , xi )hi (x0i ) ;

x0i

= Pa (Xi0 = true | xi1 , xi ) 1 + Pa (Xi0 = false | xi1 , xi ) 0 ;
= Pa (Xi0 = true | xi1 , xi ) .
Thus, gia is a restricted scope function of {Xi1 , Xi }. We can now use the CPDs in Figure 2(c) to specify gia :
reboot = i

(Xi1 , Xi ) =

reboot 6= i

(Xi1 , Xi ) =

gi

gi

Xi = true Xi = false
Xi1 = true
1
1
;
Xi1 = false
1
1
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.9
0.09
.
0.5
0.05

LP Construction:
To illustrate the factored LPs constructed by our algorithms, we
define the constraints for the approximate linear programming approach presented above.
First, we define the functions cai = gia  hi , as shown in Equation (17). In our example,
these functions are ca0 =   1 = 0.1 for the constant basis, and for the indicator bases:
reboot = i

(Xi1 , Xi ) =

reboot 6= i

(Xi1 , Xi ) =

ci

ci

Xi = true Xi = false
Xi1 = true
0.1
0.9
;
Xi1 = false
0.1
0.9
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.19
0.081
.
0.55
0.045

Using this definition of cai , the approximate linear programming constraints are given by:
0  max
x

X

Ri +

X

i

wj caj , a .

(18)

j

We present the LP construction for one of the 5 actions: reboot = 1. Analogous constructions
can be made for the other actions.
In the first set of constraints, we abstract away the difference between rewards and basis
functions by introducing LP variables u and equality constraints. We begin with the reward
functions:
R1
1
uR
x1 = 1 , ux1 = 0 ;

R2
2
uR
x2 = 1 , ux2 = 0 ;

R3
3
uR
x3 = 1 , ux3 = 0 ;

R4
4
uR
x4 = 2 , ux4 = 0 .

We now represent the equality constraints for the caj functions for the reboot = 1 action. Note
that the appropriate basis function weight from Equation (18) appears in these constraints:

425

fiGuestrin, Koller, Parr & Venkataraman

uc0 = 0.1 w0 ;
ucx11 ,x4 = 0.9 w1 ,
ucx11 ,x4 = 0.1 w1 ,
ucx11 ,x4 = 0.9 w1 ;
ucx11 ,x4 = 0.1 w1 ,
c
c
c
ux21 ,x2 = 0.19 w2 , ux21 ,x2 = 0.55 w2 , ux21 ,x2 = 0.081 w2 , ucx21 ,x2 = 0.045 w2 ;
ucx32 ,x3 = 0.19 w3 , ucx32 ,x3 = 0.55 w3 , ucx32 ,x3 = 0.081 w3 , ucx32 ,x3 = 0.045 w3 ;
ucx43 ,x4 = 0.19 w4 , ucx43 ,x4 = 0.55 w4 , ucx43 ,x4 = 0.081 w4 , ucx43 ,x4 = 0.045 w4 .
Using these new LP variables, our LP constraint from Equation (18) for the reboot = 1 action
becomes:
0

max

X1 ,X2 ,X3 ,X4

4
4
X
X
c
c0
i
uR
+
u
+
uXjj1 ,Xj .
Xi
i=1

j=1

We are now ready for the variable elimination process. We illustrate the elimination of
variable X4 :
0

max

X1 ,X2 ,X3

3
3
h
i
X
X
c
Ri
c1
c4
c0
4
uXi + u +
uXjj1 ,Xj + max uR
X4 + uX1 ,X4 + uX3 ,X4 .
i=1

X4

j=2

h

i

c1
c4
4
We can represent the term maxX4 uR
X4 + uX1 ,X4 + uX3 ,X4 by a set of linear constraints,
one for each assignment of X1 and X3 , using the new LP variables ueX11 ,X3 to represent this
maximum:

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c4
c1
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
 uR
x4 + ux1 ,x4 + ux3 ,x4 .

We have now eliminated variable X4 and our global non-linear constraint becomes:
0

3
3
X
X
c
c0
i
uR
+
u
+
uXjj1 ,Xj + ueX11 ,X3 .
Xi
X1 ,X2 ,X3

max

j=2

i=1

Next, we eliminate variable X3 . The new LP constraints and variables have the form:
c3
e1
3
ueX21 ,X2  uR
X3 + uX2 ,X3 + uX1 ,X3 ,  X1 , X2 , X3 ;

thus, removing X3 from the global non-linear constraint:
2
X
c2
e2
c0
i
uR
Xi + u + uX1 ,X2 + uX1 ,X2 .
X1 ,X2

0  max

i=1

426

fiEfficient Solution Algorithms for Factored MDPs

250000

Number of LP constraints

200000

# explicit constraints =
(n+1) 2 n

Explicit LP
Factored LP
150000

100000

50000

# factored constraints =
12n2 + 5n - 8
0
0

2

4

6
8
10
Number of machines in ring

12

14

16

Figure 7: Number of constraints in the LP generated by the explicit state representation
versus the factored LP construction for the solution of the ring problem with
basis functions over single variables and approximate linear programming as the
solution algorithm.

We can now eliminate X2 , generating the linear constraints:
c2
e2
2
ueX31  uR
X2 + uX1 ,X2 + uX1 ,X2 ,  X1 , X2 .

Now, our global non-linear constraint involves only X1 :
e3
c0
1
0  max uR
X1 + u + uX1 .
X1

As X1 is the last variable to be eliminated, the scope of the new LP variable is empty and
the linear constraints are given by:
e3
1
u e4  u R
X1 + uX1 ,  X1 .

All of the state variables have now been eliminated, turning our global non-linear constraint
into a simple linear constraint:
0  uc0 + ue4 ,
which completes the LP description for the approximate linear programming solution to
the problem in Figure 2.
In this small example with only four state variables, our factored LP technique generates
a total of 89 equality constraints, 115 inequality constraints and 149 LP variables, while
the explicit state representation in Equation (8) generates only 80 inequality constraints
and 5 LP variables. However, as the problem size increases, the number of constraints and
LP variables in our factored LP approach grow as O(n2 ), while the explicit state approach
grows exponentially, at O(n2n ). This scaling effect is illustrated in Figure 7.

6. Approximate Policy Iteration with Max-norm Projection
The factored approximate linear programming approach described in the previous section
is both elegant and easy to implement. However, we cannot, in general, provide strong
427

fiGuestrin, Koller, Parr & Venkataraman

guarantees about the error it achieves. An alternative is to use the approximate policy
iteration described in Section 3.2, which does offer certain bounds on the error. However,
as we shall see, this algorithm is significantly more complicated, and requires that we place
additional restrictions on the factored MDP.
In particular, approximate policy iteration requires a representation of the policy at each
iteration. In order to obtain a compact policy representation, we must make an additional
assumption: each action only affects a small number of state variables. We first state this
assumption formally. Then, we show how to obtain a compact representation of the greedy
policy with respect to a factored value function, under this assumption. Finally, we describe
our factored approximate policy iteration algorithm using max-norm projections.
6.1 Default Action Model
In Section 2.2, we presented the factored MDP model, where each action is associated with
its own factored transition model represented as a DBN and with its own factored reward
function. However, different actions often have very similar transition dynamics, only differing in their effect on some small set of variables. In particular, in many cases a variable
has a default evolution model, which only changes if an action affects it directly (Boutilier
et al., 2000).
This type of structure turns out to be useful for compactly representing policies, a property which is important in our approximate policy iteration algorithm. Thus, in this section
of the paper, we restrict attention to factored MDPs that are defined using a default transition model d = hGd , Pd i (Koller & Parr, 2000). For each action a, we define Effects[a]  X0
to be the variables in the next state whose local probability model is different from d , i.e.,
those variables Xi0 such that Pa (Xi0 | Parentsa (Xi0 )) 6= Pd (Xi0 | Parentsd (Xi0 )).
Example 6.1 In our system administrator example, we have an action ai for rebooting
each one of the machines, and a default action d for doing nothing. The transition model
described above corresponds to the do nothing action, which is also the default transition
model. The transition model for ai is different from d only in the transition model for the
variable Xi0 , which is now Xi0 = true with probability one, regardless of the status of the
neighboring machines. Thus, in this example, Effects[ai ] = Xi0 .
As in the transition dynamics, we can also define the notion of default reward model. In
P
this case, there is a set of reward functions ri=1 Ri (Ui ) associated with the default action
d. In addition, each action a can have a reward function Ra (Ua ). Here, the extra reward of
action a has scope restricted to Rewards[a] = Uai  {X1 , . . . , Xn }. Thus, the total reward
P
associated with action a is given by Ra + ri=1 Ri . Note that Ra can also be factored as a
linear combination of smaller terms for an even more compact representation.
We can now build on this additional assumption to define the complete algorithm.
Recall that the approximate policy iteration algorithm iterates through two steps: policy
improvement and approximate value determination. We now discuss each of these steps.
6.2 Computing Greedy Policies
The policy improvement step computes the greedy policy relative to a value function V (t1) :
 (t) = Greedy(V (t1) ).
428

fiEfficient Solution Algorithms for Factored MDPs

Recall that our value function estimates have the linear form Hw. As we described in
Section 4.1, the greedy policy for this type of value function is given by:
Greedy(Hw)(x) = arg max Qa (x),
a

P

where each Qa can be represented by: Qa (x) = R(x, a) + i wi gia (x).
If we attempt to represent this policy naively, we are again faced with the problem
of exponentially large state spaces. Fortunately, as shown by Koller and Parr (2000), the
greedy policy relative to a factored value function has the form of a decision list. More
precisely, the policy can be written in the form ht1 , a1 i, ht2 , a2 i, . . . , htL , aL i, where each ti
is an assignment of values to some small subset Ti of variables, and each ai is an action.
The greedy action to take in state x is the action aj corresponding to the first event tj in
the list with which x is consistent. For completeness, we now review the construction of
this decision-list policy.
The critical assumption that allows us to represent the policy as a compact decision list
is the default action assumption described in Section 6.1. Under this assumption, the Qa
functions can be written as:
a

Qa (x) = R (x) +

r
X

Ri (x) +

i=1

X

wi gia (x),

i

where Ra has scope restricted to Ua . The Q function for the default action d is just:
P
P
Qd (x) = ri=1 Ri (x) + i wi gid (x).
We now have a set of linear Q-functions which implicitly describes a policy . It is
not immediately obvious that these Q functions result in a compactly expressible policy.
An important insight is that most of the components in the weighted combination are
identical, so that gia is equal to gid for most i. Intuitively, a component gia corresponding
to the backprojection of basis function hi (Ci ) is only different if the action a influences
one of the variables in Ci . More formally, assume that Effects[a]  Ci = . In this case,
all of the variables in Ci have the same transition model in a and d . Thus, we have
that gia (x) = gid (x); in other words, the ith component of the Qa function is irrelevant
when deciding whether action a is better than the default action d. We can define which
components are actually relevant: let Ia be the set of indices i such that Effects[a]  Ci 6= .
These are the indices of those basis functions whose backprojection differs in Pa and Pd .
In our example DBN of Figure 2, actions and basis functions involve single variables, so
Iai = i.
Let us now consider the impact of taking action a over the default action d. We can
define the impact  the difference in value  as:
a (x) = Qa (x)  Qd (x);
= Ra (x) +

X

h

i

wi gia (x)  gid (x) .

(19)

iIa

This analysis shows that a (x) is a function whose scope is restricted to




Ta = Ua  iIa a (C0i ) .
429

(20)

fiGuestrin, Koller, Parr & Venkataraman

DecisionListPolicy (Qa )
//Qa is the set of Q-functions, one for each action;
//Return the decision list policy .
//Initialize decision list.

Let  = {}.
//Compute the bonus functions.

For each action a, other than the default action d:
Compute the bonus for taking action a,
a (x) = Qa (x)  Qd (x);
as in Equation (19). Note that a has scope restricted to Ta , as in
Equation (20).
//Add states with positive bonuses to the (unsorted) decision list.

For each assignment t  Ta :
If a (t) > 0, add branch to decision list:
 =   {ht, a, a (t)i}.
//Add the default action to the (unsorted) decision list.

Let  =   {h, d, 0i}.
//Sort decision list to obtain final policy.

Sort the decision list  in decreasing order on the  element of ht, a, i.
Return .

Figure 8: Method for computing the decision list policy  from the factored representation
of the Qa functions.

In our example DBN, Ta2 = {X1 , X2 }.
Intuitively, we now have a situation where we have a baseline value function Qd (x)
which defines a value for each state x. Each action a changes that baseline by adding or
subtracting an amount from each state. The point is that this amount depends only on Ta ,
so that it is the same for all states in which the variables in Ta take the same values.
We can now define the greedy policy relative to our Q functions. For each action a, define
a set of conditionals ht, a, i, where each t is some assignment of values to the variables Ta ,
and  is a (t). Now, sort the conditionals for all of the actions by order of decreasing :
ht1 , a1 , 1 i, ht2 , a2 , 2 i, . . . , htL , aL , L i.
Consider our optimal action in a state x. We would like to get the largest possible bonus
over the default value. If x is consistent with t1 , we should clearly take action a1 , as it
gives us bonus 1 . If not, then we should try to get 2 ; thus, we should check if x is
consistent with t2 , and if so, take a2 . Using this procedure, we can compute the decisionlist policy associated with our linear estimate of the value function. The complete algorithm
for computing the decision list policy is summarized in Figure 8.
P
Note that the number of conditionals in the list is a |Dom(Ta )|; Ta , in turn, depends
on the set of basis function clusters that intersect with the effects of a. Thus, the size
of the policy depends in a natural way on the interaction between the structure of our
430

fiEfficient Solution Algorithms for Factored MDPs

process description and the structure of our basis functions. In problems where the actions
modify a large number of variables, the policy representation could become unwieldy. The
approximate linear programming approach in Section 5 is more appropriate in such cases,
as it does not require an explicit representation of the policy.
6.3 Value Determination
In the approximate value determination step our algorithm computes:
w(t) = arg min kHw  (R(t) + P(t) Hw)k .
w

By rearranging the expression, we get:
w(t) = arg min k(H  P(t) H) w  R(t) k .
w

This equation is an instance of the optimization in Equation (4). If P(t) is factored, we can
conclude that C = (H  P(t) H) is also a matrix whose columns correspond to restrictedscope functions. More specifically:
(t)

ci (x) = hi (x)  gi (x),
(t)

where gi is the backprojection of the basis function hi through the transition model P(t) ,
as described in Section 4.1. The target b = R(t) corresponds to the reward function, which
for the moment is assumed to be factored. Thus, we can again apply our factored LP in
Section 4.2.3 to estimate the value of the policy  (t) .
Unfortunately, the transition model P(t) is not factored, as a decision list representation for the policy  (t) will, in general, induce a transition model P(t) which cannot be
represented by a compact DBN. Nonetheless, we can still generate a compact LP by exploiting the decision list structure of the policy. The basic idea is to introduce cost networks
corresponding to each branch in the decision list, ensuring, additionally, that only states
consistent with this branch are considered in the cost network maximization. Specifically,
we have a factored LP construction for each branch hti , ai i. The ith cost network only
considers a subset of the states that is consistent with the ith branch of the decision list.
Let Si be the set of states x such that ti is the first event in the decision list for which x
is consistent. That is, for each state x  Si , x is consistent with ti , but it is not consistent
with any tj with j < i.
Recall that, as in Equation (11), our LP construction defines a set of constraints that
P
imply that   i wi ci (x)  b(x) for each state x. Instead, we have a separate set of
constraints for the states in each subset Si . For each state in Si , we know that action ai is
taken. Hence, we can apply our construction above using Pai  a transition model which is
factored by assumption  in place of the non-factored P(t) . Similarly, the reward function
P
becomes Rai (x) + ri=1 Ri (x) for this subset of states.
The only issue is to guarantee that the cost network constraints derived from this transition model are applied only to states in Si . Specifically, we must guarantee that they are
applied only to states consistent with ti , but not to states that are consistent with some
tj for j < i. To guarantee the first condition, we simply instantiate the variables in Ti to
take the values specified in ti . That is, our cost network now considers only the variables in
431

fiGuestrin, Koller, Parr & Venkataraman

FactoredAPI (P , R, , H, O, , tmax )
//P is the factored transition model.
//R is the set of factored reward functions.
// is the discount factor.
//H is the set of basis functions H = {h1 , . . . , hk }.
//O stores the elimination order.
// Bellman error precision.
//tmax maximum number of iterations.
//Return the basis function weights w computed by approximate policy iteration.
//Initialize weights

Let w(0) = 0.
//Cache the backprojections of the basis functions.

For each basis function hi  H; for each action a:
Let gia = Backproja (hi ).
//Main approximate policy iteration loop.

Let t = 0.
Repeat
//Policy improvement part of the loop.
//Compute decision list policy for iteration t weights.

Let (t) = DecisionListPolicy(Ra + 

P

i

(t)

wi gia ).

//Value determination part of the loop.
//Initialize constraints for max-norm projection LP.

Let + = {} and  = {}.
//Initialize indicators.

Let I = {}.
//For every branch of the decision list policy, generate the relevant set of constraints, and
update the indicators to constraint the state space for future branches.

For each branch htj , aj i in the decision list policy (t) :
//Instantiate the variables in Tj to the assignment given in tj .
a

a

Instantiate the set of functions {h1  g1 j , . . . , hk  gk j } with the
partial state assignment tj and store in C.
Instantiate the target functions Raj with the partial state assignment tj and store in b.
Instantiate the indicator functions I with the partial state assignment tj and store in I 0 .
//Generate the factored LP constraints for the current decision list branch.

Let + = +  FactoredLP(C, b + I 0 , O).
Let  =   FactoredLP(C, b + I 0 , O).
//Update the indicator functions.

Let Ij (x) = 1(x = tj ) and update the indicators I = I  Ij .
//We can now obtain the new set of weights by solving an LP, which corresponds to the
max-norm projection.

Let w(t+1) be the solution of the linear program: minimize , subject
to the constraints {+ ,  }.
Let t = t + 1.
Until BellmanErr(Hw(t) )   or t  tmax or w(t1) = w(t) .
Return w(t) .

Figure 9: Factored approximate policy iteration with max-norm projection algorithm.

432

fiEfficient Solution Algorithms for Factored MDPs

{X1 , . . . , Xn }Ti , and computes the maximum only over the states consistent with Ti = ti .
To guarantee the second condition, we ensure that we do not impose any constraints on
states associated with previous decisions. This is achieved by adding indicators Ij for each
previous decision tj , with weight . More specifically, Ij is a function that takes value
 for states consistent with tj and zero for other all assignments of Tj . The constraints
for the ith branch will be of the form:
  R(x, ai ) +

X

wl (gl (x, ai )  h(x)) +

X

1(x = tj ),

x  [ti ],

(21)

j<i

l

where x  [ti ] defines the assignments of X consistent with ti . The introduction of these
indicators causes the constraints associated with ti to be trivially satisfied by states in Sj
for j < i. Note that each of these indicators is a restricted-scope function of Tj and can
be handled in the same fashion as all other terms in the factored LP. Thus, for a decision
list of size L, our factored LP contains constraints from 2L cost networks. The complete
approximate policy iteration with max-norm projection algorithm is outlined in Figure 9.
6.4 Comparisons
It is instructive to compare our max-norm policy iteration algorithm to the L2 -projection
policy iteration algorithm of Koller and Parr (2000) in terms of computational costs per
iteration and implementation complexity. Computing the L2 projection requires (among
other things) a series of dot product operations between basis functions and backprojected
basis functions hhi gj i. These expressions are easy to compute if P refers to the transition
model of a particular action a. However, if the policy  is represented as a decision list, as is
the result of the policy improvement step, then this step becomes much more complicated.
In particular, for every branch of the decision list, for every pair of basis functions i and j,
and for each assignment to the variables in Scope[hi ]  Scope[gja ], it requires the solution of
a counting problem which is ]P -complete in general. Although Koller and Parr show that
this computation can be performed using a Bayesian network (BN) inference, the algorithm
still requires a BN inference for each one of those assignments at each branch of the decision
list. This makes the algorithm very difficult to implement efficiently in practice.
The max-norm projection, on the other hand, relies on solving a linear program at every
iteration. The size of the linear program depends on the cost networks generated. As we
discuss, two cost networks are needed for each point in the decision list. The complexity
of each of these cost networks is approximately the same as only one of the BN inferences
in the counting problem for the L2 projection. Overall, for each branch in the decision
list, we have a total of two of these inferences, as opposed to one for each assignment of
Scope[hi ]  Scope[gja ] for every pair of basis functions i and j. Thus, the max-norm policy
iteration algorithm is substantially less complex computationally than the approach based
on L2 -projection. Furthermore, the use of linear programming allows us to rely on existing
LP packages (such as CPLEX), which are very highly optimized.
It is also interesting to compare the approximate policy iteration algorithm to the approximate linear programming algorithm we presented in Section 5. In the approximate
linear programming algorithm, we never need to compute the decision list policy. The
policy is always represented implicitly by the Qa functions. Thus, this algorithm does not
433

fiGuestrin, Koller, Parr & Venkataraman

require explicit computation or manipulation of the greedy policy. This difference has two
important consequences: one computational and the other in terms of generality.
First, not having to compute or consider the decision lists makes approximate linear
programming faster and easier to implement. In this algorithm, we generate a single LP
with one cost network for each action and never need to compute a decision list policy. On
the other hand, in each iteration, approximate policy iteration needs to generate two LPs
for every branch of the decision list of size L, which is usually significantly longer than |A|,
with a total of 2L cost networks. In terms of representation, we do not require the policies
to be compact; thus, we do not need to make the default action assumption. Therefore, the
approximate linear programming algorithm can deal with a more general class of problems,
where each action can have its own independent DBN transition model. On the other hand,
as described in Section 3.2, approximate policy iteration has stronger guarantees in terms
of error bounds. These differences will be highlighted further in our experimental results
presented in Section 9.

7. Computing Bounds on Policy Quality
We have presented two algorithms for computing approximate solutions to factored MDPs.
b where w
b
All these algorithms generate linear value functions which can be denoted by Hw,
are the resulting basis function weights. In practice, the agent will define its behavior by
b One issue that remains is how this
b = Greedy(Hw).
acting according to the greedy policy 

b compares to the true optimal policy  ; that is, how the actual value Vb
policy 
 of policy
b compares to V  .

In Section 3, we showed some a priori bounds for the quality of the policy. Another
possible procedure is to compute an a posteriori bound. That is, given our resulting weights
b we compute a bound on the loss of acting according to the greedy policy 
b rather than
w,
the optimal policy. This can be achieved by using the Bellman error analysis of Williams
and Baird (1993).
The Bellman error is defined as BellmanErr(V) = kT  V  Vk . Given the greedy
b = Greedy(V), their analysis provides the bound:
policy 
 

V  V   2BellmanErr(V) .
b
 
1

(22)

b to evaluate the quality of our resulting
Thus, we can use the Bellman error BellmanErr(Hw)
greedy policy.
Note that computing the Bellman error involves a maximization over the state space.
Thus, the complexity of this computation grows exponentially with the number of state
variables. Koller and Parr (2000) suggested that structure in the factored MDP can be
exploited to compute the Bellman error efficiently. Here, we show how this error bound can
be computed by a set of cost networks using a similar construction to the one in our maxb that can be represented
norm projection algorithms. This technique can be used for any 
as a decision list and does not depend on the algorithm used to determine the policy. Thus,
we can apply this technique to solutions determined approximate linear programming if the
action descriptions permit a decision list representation of the policy.
b the Bellman error is given by:
For some set of weights w,
434

fiEfficient Solution Algorithms for Factored MDPs

b
FactoredBellmanErr (P , R, , H, O, w)
//P is the factored transition model.
//R is the set of factored reward functions.
// is the discount factor.
//H is the set of basis functions H = {h1 , . . . , hk }.
//O stores the elimination order.
//w
b are the weights for the linear value function.
//Return the Bellman error for the value function Hw.
b
//Cache the backprojections of the basis functions.

For each basis function hi  H; for each action a:
Let gia = Backproja (hi ).
//Compute decision list policy for value function
Hw.
b
a
b = DecisionListPolicy(Ra +  P w
Let 
i bi gi ).
//Initialize indicators.

Let I = {}.
//Initialize Bellman error.

Let  = 0.
//For every branch of the decision list policy, generate the relevant cost networks, solve it with
variable elimination, and update the indicators to constraint the state space for future branches.

b
For each branch htj , aj i in the decision list policy :

//Instantiate the variables in Tj to the assignment given in tj .
a

a

Instantiate the set of functions {w
b1 (h1 g1 j ), . . . , w
bk (hk gk j )} with the
partial state assignment tj and store in C.
Instantiate the target functions Raj with the partial state assignment
tj and store in b.
Instantiate the indicator functions I with the partial state assignment
tj and store in I 0 .
//Use variable elimination to solve first cost network, and update Bellman error, if error
for this branch is larger.

Let  = max (, VariableElimination(C  b + I 0 , O)).
//Use variable elimination to solve second cost network, and update Bellman error, if error
for this branch is larger.

Let  = max (, VariableElimination(C + b + I 0 , O)).
//Update the indicator functions.

Let Ij (x) = 1(x = tj ) and update the indicators I = I  Ij .
Return .
b
Figure 10: Algorithm for computing Bellman error for factored value function Hw.

435

fiGuestrin, Koller, Parr & Venkataraman

b
b  Hwk
b ;
BellmanErr(Hw)
= kT  Hw


= max

P

P

P

maxx i wi hi (x)  Rb (x)   x0 Pb (x0 | x) j wj hj (x0 ) ,
P
P
P
maxx Rb (x) +  x0 Pb (x0 | x) j wj hj (x0 )  i wi hi (x)

!

If the rewards Rb and the transition model Pb are factored appropriately, then we can
compute each one of these two maximizations (maxx ) using variable elimination in a cost
b is a decision list policy and it does not
network as described in Section 4.2.1. However, 
induce a factored transition model. Fortunately, as in the approximate policy iteration
algorithm in Section 6, we can exploit the structure in the decision list to perform such
maximization efficiently. In particular, as in approximate policy iteration, we will generate
two cost networks for each branch in the decision list. To guarantee that our maximization
is performed only over states where this branch is relevant, we include the same type of
indicator functions, which will force irrelevant states to have a value of , thus guaranteeing that at each point of the decision list policy we obtain the corresponding state with
the maximum error. The state with the overall largest Bellman error will be the maximum
over the ones generated for each point the in the decision list policy. The complete factored
algorithm for computing the Bellman error is outlined in Figure 10.
One last interesting note concerns our approximate policy iteration algorithm with maxnorm projection of Section 6. In all our experiments, this algorithm converged, so that
w(t) = w(t+1) after some iterations. If such convergence occurs, then the objective function
(t+1) of the linear program in our last iteration is equal to the Bellman error of the final
policy:
Lemma 7.1 If approximate policy iteration with max-norm projection converges, so that
w(t) = w(t+1) for some iteration t, then the max-norm projection error (t+1) of the last
b = Hw(t) :
iteration is equal to the Bellman error for the final value function estimate Hw
b = (t+1) .
BellmanErr(Hw)

Proof: See Appendix A.4.
Thus, we can bound the loss of acting according to the final policy  (t+1) by substituting
(t+1)

into the Bellman error bound:
Corollary 7.2 If approximate policy iteration with max-norm projection converges after
b associated with a greedy policy 
b =
t iterations to a final value function estimate Hw
b
b instead of the optimal policy   is
Greedy(Hw),
then the loss of acting according to 
bounded by:
(t+1)
 

V  V   2
,
b
 
1

b.
where Vb is the actual value of the policy 

Therefore, when approximate policy iteration converges we can obtain a bound on the
quality of the resulting policy without needing to compute the Bellman error explicitly.
436

.

fiEfficient Solution Algorithms for Factored MDPs

8. Exploiting Context-specific Structure
Thus far, we have presented a suite of algorithms which exploit additive structure in the
reward and basis functions and sparse connectivity in the DBN representing the transition
model. However, there exists another important type of structure that should also be
exploited for efficient decision making: context-specific independence (CSI). For example,
consider an agent responsible for building and maintaining a house, if the painting task can
only be completed after the plumbing and the electrical wiring have been installed, then
the probability that the painting is done is 0, in all contexts where plumbing or electricity
are not done, independently of the agents action. The representation we have used so far in
this paper would use a table to represent this type of function. This table is exponentially
large in the number of variables in the scope of the function, and ignores the context-specific
structure inherent in the problem definition.
Boutilier et al. (Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, &
Hanks, 1999; Boutilier et al., 2000) have developed a set of algorithms which can exploit CSI
in the transition and reward models to perform efficient (approximate) planning. Although
this approach is often successful in problems where the value function contains sufficient
context-specific structure, the approach is not able to exploit the additive structure which
is also often present in real-world problems.
In this section, we extend the factored MDP model to include context-specific structure.
We present a simple, yet effective extension of our algorithms which can exploit both CSI
and additive structure to obtain efficient approximations for factored MDPs. We first extend
the factored MDP representation to include context-specific structure and then show how
the basic operations from Section 4 required by our algorithms can be performed efficiently
in this new representation.
8.1 Factored MDPs with Context-specific and Additive Structure
There are several representations for context-specific functions. The most common are
decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin,
Hu, & Boutilier, 1999), and rules (Zhang & Poole, 1999). We choose to use rules as our
basic representation, for two main reasons. First, the rule-based representation allows a
fairly simple algorithm for variable elimination, which is a key operation in our framework.
Second, rules are not required to be mutually exclusive and exhaustive, a requirement that
can be restrictive if we want to exploit additive independence, where functions can be
represented as a linear combination of a set of non-mutually exclusive functions.
We begin by describing the rule-based representation (along the lines of Zhang and
Pooles presentation (1999)) for the probabilistic transition model, in particular, the CPDs
of our DBN model. Roughly speaking, each rule corresponds to some set of CPD entries
that are all associated with a particular probability value. These entries with the same
value are referred to as consistent contexts:
Definition 8.1 Let C  {X, X0 } and c  Dom(C). We say that c is consistent with
b  Dom(B), for B  {X, X0 }, if c and b have the same assignment for the variables in
C  B.
The probability of these consistent contexts will be represented by probability rules:
437

fiGuestrin, Koller, Parr & Venkataraman

Electrical

Electrical

Done

Not done

Done

Not done

Plumbing

P(Painting) = 0

Plumbing

P(Painting) = 0
Not done

Done

Not done

Painting

P(Painting) = 0

Done

Not done

P(Painting) = 0

P(Painting) = 0.95

P(Painting) = 0

(a)

Done
P(Painting) = 0.9

(b)

4 = hElectrical : 0i
5 = hElectrical  Plumbing : 0i
6 = hElectrical  Plumbing  Painting : 0i
7 = hElectrical  Plumbing  Painting : 0.9i
(d)

1 = hElectrical : 0i
2 = hElectrical   Plumbing : 0i
3 = hElectrical  Plumbing : 0.95i
(c)

Figure 11: Example CPDs for variable the Painting = true represented as decision trees:
(a) when the action is paint; (b) when the action is not paint. The same CPDs
can be represented by probability rules as shown in (c) and (d), respectively.

Definition 8.2 A probability rule  = hc : pi is a function  : {X, X0 } 7 [0, 1], where the
context c  Dom(C) for C  {X, X0 } and p  [0, 1], such that (x, x0 ) = p if (x, x0 ) is
consistent with c and is equal to 1 otherwise.
In this case, it is convenient to require that the rules be mutually exclusive and exhaustive, so that each CPD entry is uniquely defined by its association with a single rule.
Definition 8.3 A rule-based conditional probability distribution (rule CPD) Pa is a function Pa : ({Xi0 }  X) 7 [0, 1], composed of a set of probability rules {1 , 2 , . . . , m } whose
contexts are mutually exclusive and exhaustive. We define:
Pa (x0i | x) = j (x, x0 ),
where j is the unique rule in Pa for which cj is consistent with (x0i , x). We require that,
for all x,
X
Pa (x0i | x) = 1.
x0i

We can define Parentsa (Xi0 ) to be the union of the contexts of the rules in Pa (Xi0 | X). An
example of a CPD represented by a set of probability rules is shown in Figure 11.
Rules can also be used to represent additive functions, such as reward or basis functions.
We represent such context specific value dependencies using value rules:
438

fiEfficient Solution Algorithms for Factored MDPs

Definition 8.4 A value rule  = hc : vi is a function  : X 7 R such that (x) = v when
x is consistent with c and 0 otherwise.
Note that a value rule hc : vi has a scope C.
It is important to note that value rules are not required to be mutually exclusive and
exhaustive. Each value rule represents a (weighted) indicator function, which takes on a
value v in states consistent with some context c, and 0 in all other states. In any given state,
the values of the zero or more rules consistent with that state are simply added together.
Example 8.5 In our construction example, we might have a set of rules:
1 = hPlumbing = done : 100i;
2 = hElectricity = done : 100i;
3 = hPainting = done : 100i;
4 = hAction = plumb : 10i;
..
.
which, when summed together, define the reward function R = 1 + 2 + 3 + 4 +   .
In general, our reward function Ra is represented as a rule-based function:
Definition 8.6 A rule-based function f : X 7 R is composed of a set of rules {1 , . . . , n }
P
such that f (x) = ni=1 i (x).
In the same manner, each one of our basis functions hj is now represented as a rule-based
function.
This notion of a rule-based function is related to the tree-structure functions used by
Boutilier et al. (2000), but is substantially more general. In the tree-structure value functions, the rules corresponding to the different leaves are mutually exclusive and exhaustive.
Thus, the total number of different values represented in the tree is equal to the number
of leaves (or rules). In the rule-based function representation, the rules are not mutually
exclusive, and their values are added to form the overall function value for different settings
of the variables. Different rules are added in different settings, and, in fact, with k rules,
one can easily generate 2k different possible values, as is demonstrated in Section 9. Thus,
the rule-based functions can provide a compact representation for a much richer class of
value functions.
Using this rule-based representation, we can exploit both CSI and additive independence
in the representation of our factored MDP and basis functions. We now show how the basic
operations in Section 4 can be adapted to exploit our rule-based representation.
8.2 Adding, Multiplying and Maximizing Consistent Rules
In our table-based algorithms, we relied on standard sum and product operators applied to
tables. In order to exploit CSI using a rule-based representation, we must redefine these
standard operations. In particular, the algorithms will need to add or multiply rules that
ascribe values to overlapping sets of states.
We will start by defining these operations for rules with the same context:
439

fiGuestrin, Koller, Parr & Venkataraman

Definition 8.7 Let 1 = hc : v1 i and 2 = hc : v2 i be two rules with context c. Define the
rule product as 1  2 = hc : v1  v2 i, and the rule sum as 1 + 2 = hc : v1 + v2 i.
Note that this definition is restricted to rules with the same context. We will address this
issue in a moment. First, we will introduce an additional operation which maximizes a
variable from a set of rules, which otherwise share a common context:
Definition 8.8 Let Y be a variable with Dom[Y ] = {y1 , . . . , yk }, and let i , for each i =
1, . . . , k, be a rule of the form i = hc  Y = yi : vi i. Then for the rule-based function
f = 1 +    + k , define the rule maximization over Y as maxY f = hc : maxi vi i .
After this operation, Y has been maximized out from the scope of the function f .
These three operations we have just described can only be applied to sets of rules that
satisfy very stringent conditions. To make our set of rules amenable to the application
of these operations, we might need to refine some of these rules. We therefore define the
following operation:
Definition 8.9 Let  = hc : vi be a rule, and Y be a variable. Define the rule split
Split(6 Y ) of  on a variable Y as follows: If Y  Scope[C], then Split(6 Y ) = {};
otherwise,
Split(6 Y ) = {hc  Y = yi : vi | yi  Dom[Y ]} .
Thus, if we split a rule  on variable Y that is not in the scope of the context of , then we
generate a new set of rules, with one for each assignment in the domain of Y .
In general, the purpose of rule splitting is to extend the context c of one rule  coincide
with the context c0 of another consistent rule 0 . Naively, we might take all variables in
Scope[C0 ]  Scope[C] and split  recursively on each one of them. However, this process
creates unnecessarily many rules: If Y is a variable in Scope[C0 ]  Scope[C] and we split 
on Y , then only one of the |Dom[Y ]| new rules generated will remain consistent with 0 : the
one which has the same assignment for Y as the one in c0 . Thus, only this consistent rule
needs to be split further. We can now define the recursive splitting procedure that achieves
this more parsimonious representation:
Definition 8.10 Let  = hc : vi be a rule, and b be a context such that b  Dom[B].
Define the recursive rule split Split(6 b) of  on a context b as follows:
1. {}, if c is not consistent with b; else,
2. {}, if Scope[B]  Scope[C]; else,
3. {Split(i 6 b) | i  Split(6 Y )}, for some variable Y  Scope[B]  Scope[C] .

In this definition, each variable Y  Scope[B]  Scope[C] leads to the generation of k =
|Dom(Y )| rules at the step in which it is split. However, only one of these k rules is used
in the next recursive step because only one is consistent with b. Therefore, the size of the
P
split set is simply 1 + Y Scope[B]Scope[C] (|Dom(Y )|  1). This size is independent of the
order in which the variables are split within the operation.
440

fiEfficient Solution Algorithms for Factored MDPs

Note that only one of the rules in Split(6 b) is consistent with b: the one with context
c  b. Thus, if we want to add two consistent rules 1 = hc1 : v1 i and 2 = hc2 : v2 i, then
all we need to do is replace these rules by the set:
Split(1 6 c2 )  Split(2 6 c1 ),
and then simply replace the resulting rules hc1  c2 : v1 i and hc2  c1 : v2 i by their sum
hc1  c2 : v1 + v2 i. Multiplication is performed in an analogous manner.
Example 8.11 Consider adding the following set of consistent rules:
1 = ha  b : 5i,
2 = ha  c  d : 3i.
In these rules, the context c1 of 1 is a  b, and the context c2 of 2 is a  c  d.
Rules 1 and 2 are consistent, therefore, we must split them to perform the addition
operation:


 ha  b  c : 5i,
ha  b  c  d : 5i,
Split(1 6 c2 ) =

 ha  b  c  d : 5i.
Likewise,

(

Split(2 6

c1 ) =

ha  b  c  d : 3i,
ha  b  c  d : 3i.

The result of adding rules 1 and 2 is
ha  b  c : 5i,
ha  b  c  d : 5i,
ha  b  c  d : 8i,
ha  b  c  d : 3i.

8.3 Rule-based One-step Lookahead
Using this compact rule-based representation, we are able to compute a one-step lookahead
plan efficiently for models with significant context-specific or additive independence.
As in Section 4.1 for the table-based case, the rule-based Qa function can be represented
as the sum of the reward function and the discounted expected value of the next state.
Due to our linear approximation of the value function, the expectation term is, in turn,
represented as the linear combination of the backprojections of our basis functions. To
exploit CSI, we are representing the rewards and basis functions as rule-based functions.
To represent Qa as a rule-based function, it is sufficient for us to show how to represent the
backprojection gj of the basis function hj as a rule-based function.
P (h )
Each hj is a rule-based
function,
which can be written as hj (x) = i i j (x), where
D
E
(h )
(h )
(h )
i j has the form ci j : vi j . Each rule is a restricted scope function; thus, we can
simplify the backprojection as:
441

fiGuestrin, Koller, Parr & Venkataraman

RuleBackproja () ,

where  is given by hc : vi, with c  Dom[C].

Let g = {}.
Select the set P of relevant probability rules:
P = {j  P (Xi0 | Parents(Xi0 )) | Xi0  C and c is consistent with cj }.
Remove the X0 assignments from the context of all rules in P.
// Multiply consistent rules:
While there are two consistent rules 1 = hc1 : p1 i and 2 = hc2 : p2 i:
If c1 = c2 , replace these two rules by hc1 : p1 p2 i;
Else replace these two rules by the set: Split(1 6 c2 )  Split(2 6 c1 ).
// Generate value rules:
For each rule i in P:
Update the backprojection g = g  {hci : pi vi}.
Return g.

Figure 12: Rule-based backprojection.
gja (x) =

X

Pa (x0 | x)hj (x0 ) ;

x0

=

X

Pa (x0 | x)

x0

=

XX
i

=

X (hj )

i

(x0 );

i
(hj )

0

Pa (x | x)i

(x0 );

x0

X (hj )

vi

(hj )

Pa (ci

| x);

i
(h )

(h )

where the term vi j Pa (ci j | x) can be written as a rule function. We denote this back(h )
projection operation by RuleBackproja (i j ).
The backprojection procedure, described in Figure 12, follows three steps. First, the
relevant rules are selected: In the CPDs for the variables that appear in the context of ,
we select the rules consistent with this context, as these are the only rules that play a role
in the backprojection computation. Second, we multiply all consistent probability rules to
form a local set of mutually-exclusive rules. This procedure is analogous to the addition
procedure described in Section 8.2. Now that we have represented the probabilities that
can affect  by a mutually-exclusive set, we can simply represent the backprojection of 
by the product of these probabilities with the value of . That is, the backprojection of  is
a rule-based function with one rule for each one of the mutually-exclusive probability rules
i . The context of this new value rule is the same as that of i , and the value is the product
of the probability of i and the value of .
Example 8.12 For example, consider the backprojection of a simple rule,
 = h Painting = done : 100i,
through the CPD in Figure 11(c) for the paint action:
RuleBackprojpaint () =

X

Ppaint (x0 | x)(x0 );

x0

442

fiEfficient Solution Algorithms for Factored MDPs

X

=

Ppaint (Painting0 | x)(Painting0 );

Painting0

= 100

3
Y

i (Painting = done, x) .

i=1

Note that the product of these simple rules is equivalent to the decision tree CPD shown in
Figure 11(a). Hence, this product is equal to 0 in most contexts, for example, when electricity
is not done at time t. The product in non-zero only in one context: in the context associated
with rule 3 . Thus, we can express the result of the backprojection operation by a rule-based
function with a single rule:
RuleBackprojpaint () = hPlumbing  Electrical : 95i.
Similarly, the backprojection of  when the action is not paint can also be represented by a
single rule:
RuleBackprojpaint () = hPlumbing  Electrical  Painting : 90i.
Using this algorithm, we can now write the backprojection of the rule-based basis function hj as:
gja (x) =

X

(hj )

RuleBackproja (i

),

(23)

i

where gja is a sum of rule-based functions, and therefore also a rule-based function. For
simplicity of notation, we use gja = RuleBackproja (hj ) to refer to this definition of backproP
jection. Using this notation, we can write Qa (x) = Ra (x) +  j wj gja (x), which is again a
rule-based function.
8.4 Rule-based Maximization Over the State Space
The second key operation required to extend our planning algorithms to exploit CSI is to
modify the variable elimination algorithm in Section 4.2.1 to handle the rule-based representation. In Section 4.2.1, we showed that the maximization of a linear combination
of table-based functions with restricted scope can be performed efficiently using non-serial
dynamic programming (Bertele & Brioschi, 1972), or variable elimination. To exploit structure in rules, we use an algorithm similar to variable elimination in a Bayesian network with
context-specific independence (Zhang & Poole, 1999).
Intuitively, the algorithm operates by selecting the value rules relevant to the variable
being maximized in the current iteration. Then, a local maximization is performed over
this subset of the rules, generating a new set of rules without the current variable. The
procedure is then repeated recursively until all variables have been eliminated.
More precisely, our algorithm eliminates variables one by one, where the elimination process performs a maximization step over the variables domain. Suppose that we
are eliminating Xi , whose collected value rules lead to a rule function f , and f involves
additional variables in some set B, so that f s scope is B  {Xi }. We need to compute
the maximum value for Xi for each choice of b  Dom[B]. We use MaxOut (f, Xi ) to denote a procedure that takes a rule function f (B, Xi ) and returns a rule function g(B) such
443

fiGuestrin, Koller, Parr & Venkataraman

MaxOut (f, B)
Let g = {}.
Add completing rules to f : hB = bi : 0i, i = 1, . . . , k.
// Summing consistent rules:
While there are two consistent rules 1 = hc1 : v1 i and 2 = hc2 : v2 i:
If c1 = c2 , then replace these two rules by hc1 : v1 + v2 i;
Else replace these two rules by the set: Split(1 6 c2 )  Split(2 6 c1 ).
// Maximizing out variable B:
Repeat until f is empty:
If there are rules hc  B = bi : vi i, bi  Dom(B) :
Then remove these rules from f and add rule hc : maxi vi i to g;
Else select two rules: i = hci  B = bi : vi i and j = hcj  B = bj : vj i
such that ci is consistent with cj , but not identical, and replace
them with Split(i 6 cj )  Split(j 6 ci ) .
Return g.

Figure 13: Maximizing out variable B from rule function f .
that: g(b) = maxxi f (b, xi ). Such a procedure is an extension of the variable elimination
algorithm of Zhang and Poole (Zhang & Poole, 1999).
The rule-based variable elimination algorithm maintains a set F of value rules, initially
containing the set of rules to be maximized. The algorithm then repeats the following steps
for each variable Xi until all variables have been eliminated:
1. Collect all rules which depend on Xi into fi  fi = {hc : vi  F | Xi  C}  and
remove these rules from F.
2. Perform the local maximization step over Xi : gi = MaxOut (fi , Xi );
3. Add the rules in gi to F; now, Xi has been eliminated.
The cost of this algorithm is polynomial in the number of new rules generated in the
maximization operation MaxOut (fi , Xi ). The number of rules is never larger and in many
cases exponentially smaller than the complexity bounds on the table-based maximization in
Section 4.2.1, which, in turn, was exponential only in the induced width of the cost network
graph (Dechter, 1999). However, the computational costs involved in managing sets of rules
usually imply that the computational advantage of the rule-based approach over the tablebased one will only be significant in problems that possess a fair amount of context-specific
structure.
In the remainder of this section, we present the algorithm for computing the local
maximization MaxOut (fi , Xi ). In the next section, we show how these ideas can be applied
to extending the algorithm in Section 4.2.2 to exploit CSI in the LP representation for
planning in factored MDPs.
The procedure, presented in Figure 13, is divided into two parts: first, all consistent
rules are added together as described in Section 8.2; then, variable B is maximized. This
maximization is performed by generating a set of rules, one for each assignment of B, whose
contexts have the same assignment for all variables except for B, as in Definition 8.8. This
set is then substituted by a single rule without a B assignment in its context and with value
equal to the maximum of the values of the rules in the original set. Note that, to simplify
444

fiEfficient Solution Algorithms for Factored MDPs

the algorithm, we initially need to add a set of value rules with 0 value, which guarantee
that our rule function f is complete (i.e., there is at least one rule consistent with every
context).
The correctness of this procedure follows directly from the correctness of the rule-based
variable elimination procedure described by Zhang and Poole, merely by replacing summations with product with max, and products with products with sums. We conclude this
section with a small example to illustrate the algorithm:
Example 8.13 Suppose we are maximizing a for the following set of rules:
1
2
3
4

= ha : 1i,
= ha  b : 2i,
= ha  b  c : 3i,
= ha  b : 1i.

When we add completing rules, we get:
5 = ha : 0i,
6 = ha : 0i.
In the first part of the algorithm, we need to add consistent rules: We add 5 to 1 (which
remains unchanged), combine 1 with 4 , 6 with 2 , and then the split of 6 on the context
of 3 , to get the following inconsistent set of rules:
2
3
7
8
9

= ha  b : 2i,
= ha  b  c : 3i,
= ha  b : 2i,
(from adding 4 to the consistent rule from Split(1 6 b))
= ha  b : 1i,
(from Split(1 6 b))
= ha  b  c : 0i,
(from Split(6 6 a  b  c)).

Note that several rules with value 0 are also generated, but not shown here because they are
added to other rules with consistent contexts. We can move to the second stage (repeat loop)
of MaxOut. We remove 2 , and 8 , and maximize a out of them, to give:
10 = hb : 2i.
We then select rules 3 and 7 and split 7 on c (3 is split on the empty set and is not
changed),
11 = ha  b  c : 2i,
12 = ha  b  c : 2i.
Maximizing out a from rules 12 and 3 , we get:
13 = hb  c : 3i.
We are left with 11 , which maximized over its counterpart 9 gives
12 = hb  c : 2i.
Notice that, throughout this maximization, we have not split on the variable C when b  ci ,
giving us only 6 distinct rules in the final result. This is not possible in a table-based
representation, since our functions would then be over the 3 variables a,b,c, and therefore
must have 8 entries.
445

fiGuestrin, Koller, Parr & Venkataraman

8.5 Rule-based Factored LP
In Section 4.2.2, we showed that the LPs used in our algorithms have exponentially many
P
constraints of the form:   i wi ci (x)  b(x), x, which can be substituted by a single,
P
equivalent, non-linear constraint:   maxx i wi ci (x)  b(x). We then showed that, using
variable elimination, we can represent this non-linear constraint by an equivalent set of
linear constraints in a construction we called the factored LP. The number of constraints in
the factored LP is linear in the size of the largest table generated in the variable elimination
procedure. This table-based algorithm can only exploit additive independence. We now
extend the algorithm in Section 4.2.2 to exploit both additive and context-specific structure,
by using the rule-based variable elimination described in the previous section.
Suppose we wish to enforce the more general constraint 0  maxy F w (y), where F w (y) =
P w
j fj (y) such that each fj is a rule. As in the table-based version, the superscript w means
that fj might depend on w. Specifically, if fj comes from basis function hi , it is multiplied
by the weight wi ; if fj is a rule from the reward function, it is not.
In our rule-based factored linear program, we generate LP variables associated with
contexts; we call these LP rules. An LP rule has the form hc : ui; it is associated with a
context c and a variable u in the linear program. We begin by transforming all our original
rules fjw into LP rules as follows: If rule fj has the form hcj : vj i and comes from basis
function hi , we introduce an LP rule ej = hcj : uj i and the equality constraint uj = wi vj .
If fj has the same form but comes from a reward function, we introduce an LP rule of the
same form, but the equality constraint becomes uj = vj .
P
Now, we have only LP rules and need to represent the constraint: 0  maxy j ej (y).
To represent such a constraint, we follow an algorithm very similar to the variable elimination procedure in Section 8.4. The main difference occurs in the MaxOut (f, B) operation in
Figure 13. Instead of generating new value rules, we generate new LP rules, with associated
new variables and new constraints. The simplest case occurs when computing a split or
adding two LP rules. For example, when we add two value rules in the original algorithm,
we instead perform the following operation on their associated LP rules: If the LP rules
are hc : ui i and hc : uj i, we replace these by a new rule hc : uk i, associated with a new LP
variable uk with context c, whose value should be ui + uj . To enforce this value constraint,
we simply add an additional constraint to the LP: uk = ui + uj . A similar procedure can
be followed when computing the split.
More interesting constraints are generated when we perform a maximization. In the
rule-based variable elimination algorithm in Figure 13, this maximization occurs when we
replace a set of rules:
hc  B = bi : vi i, bi  Dom(B),
by a new rule





c : max vi .
i

Following the same process as in the LP rule summation above, if we are maximizing
ei = hc  B = bi : ui i, bi  Dom(B),
we generate a new LP variable uk associated with the rule ek = hc : uk i. However, we
cannot add the nonlinear constraint uk = maxi ui , but we can add a set of equivalent linear
446

fiEfficient Solution Algorithms for Factored MDPs

constraints
uk  ui , i.
Therefore, using these simple operations, we can exploit structure in the rule functions
P
to represent the nonlinear constraint en  maxy j ej (y), where en is the very last LP
rule we generate. A final constraint un =  implies that we are representing exactly the
constraints in Equation (12), without having to enumerate every state.
The correctness of our rule-based factored LP construction is a corollary of Theorem 4.4
and of the correctness of the rule-based variable elimination algorithm (Zhang & Poole,
1999) .
Corollary 8.14 The constraints generated by the rule-based factored LP construction are
equivalent to the non-linear constraint in Equation (12). That is, an assignment to (, w)
satisfies the rule-based factored LP constraints if and only if it satisfies the constraint in
Equation (12).
The number of variables and constraints in the rule-based factored LP is linear in the
number of rules generated by the variable elimination process. In turn, the number of rules
is no larger, and often exponentially smaller, than the number of entries in the table-based
approach.
To illustrate the generation of LP constraints as just described, we now present a small
example:
Example 8.15 Let e1 , e2 , e3 , and e4 be the set of LP rules which depend on the variable
b being maximized. Here, rule ei is associated with the LP variable ui :
e1
e2
e3
e4

= ha  b : u1 i,
= ha  b  c : u2 i,
= ha  b : u3 i,
= ha  b  c : u4 i.

In this set, note that rules e1 and e2 are consistent. We combine them to generate the
following rules:
e5 = ha  b  c : u5 i,
e6 = ha  b  c : u1 i.
and the constraint u1 + u2 = u5 . Similarly, e6 and e4 may be combined, resulting in:
e7 = ha  b  c : u6 i.
with the constraint u6 = u1 + u4 . Now, we have the following three inconsistent rules for
the maximization:
e3 = ha  b : u3 i,
e5 = ha  b  c : u5 i,
e7 = ha  b  c : u6 i.
Following the maximization procedure, since no pair of rules can be eliminated right away,
we split e3 and e5 to generate the following rules:
e8 = ha  b  c : u3 i,
e9 = ha  b  c : u3 i,
e5 = ha  b  c : u5 i.
447

fiGuestrin, Koller, Parr & Venkataraman

We can now maximize b out from e8 and e5 , resulting in the following rule and constraints
respectively:
e10 = ha  c : u7 i,
u7  u5 ,
u7  u3 .
Likewise, maximizing b out from e9 and e6 , we get:
e11 = ha  c : u8 i,
u8  u3 ,
u8  u6 ;
which completes the elimination of variable b in our rule-based factored LP.
We have presented an algorithm for exploiting both additive and context-specific structure in the LP construction steps of our planning algorithms. This rule-based factored LP
approach can now be applied directly in our approximate linear programming and approximate policy iteration algorithms, which were presented in Sections 5 and 6.
The only additional modification required concerns the manipulation of the decision
list policies presented in Section 6.2. Although approximate linear programming does not
require any explicit policy representation (or the default action model), approximate policy iteration require us to represent such policy. Fortunately, no major modifications are
required in the rule-based case. In particular, the conditionals hti , ai , i i in the decision
list policies are already context-specific rules. Thus, the policy representation algorithm in
Section 6.2 can be applied directly with our new rule-based representation. Therefore, we
now have a complete framework for exploiting both additive and context-specific structure
for efficient planning in factored MDPs.

9. Experimental Results
The factored representation of a value function is most appropriate in certain types of
systems: Systems that involve many variables, but where the strong interactions between
the variables are fairly sparse, so that the decoupling of the influence between variables
does not induce an unacceptable loss in accuracy. As argued by Herbert Simon (1981)
in Architecture of Complexity, many complex systems have a nearly decomposable,
hierarchical structure, with the subsystems interacting only weakly between themselves. To
evaluate our algorithm, we selected problems that we believe exhibit this type of structure.
In this section, we perform various experiments intended to explore the performance
of our algorithms. First, we compare our factored approximate linear programming (LP)
and approximate policy iteration (PI) algorithms. We also compare to the L2 -projection
algorithm of Koller and Parr (2000). Our second evaluation compares a table-based implementation to a rule-based implementation that can exploit CSI. Finally, we present
comparisons between our approach and the algorithms of Boutilier et al. (2000).
9.1 Approximate LP and Approximate PI
In order to compare our approximate LP and approximate PI algorithms, we tested both on
the SysAdmin problem described in detail in Section 2.1. This problem relates to a system
448

fiEfficient Solution Algorithms for Factored MDPs

administrator who has to maintain a network of computers; we experimented with various
network architectures, shown in Figure 1. Machines fail randomly, and a faulty machine
increases the probability that its neighboring machines will fail. At every time step, the
SysAdmin can go to one machine and reboot it, causing it to be working in the next time
step with high probability. Recall that the state space in this problem grows exponentially
in the number of machines in the network, that is, a problem with m machines has 2m states.
Each machine receives a reward of 1 when working (except in the ring, where one machine
receives a reward of 2, to introduce some asymmetry), a zero reward is given to faulty
machines, and the discount factor is  = 0.95. The optimal strategy for rebooting machines
will depend upon the topology, the discount factor, and the status of the machines in the
network. If machine i and machine j are both faulty, the benefit of rebooting i must be
weighed against the expected discounted impact of delaying rebooting j on js successors.
For topologies such as rings, this policy may be a function of the status of every single
machine in the network.
The basis functions used included independent indicators for each machine, with value
1 if it is working and zero otherwise (i.e., each one is a restricted scope function of a single
variable), and the constant basis, whose value is 1 for all states. We selected straightforward
variable elimination orders: for the Star and Three Legs topologies, we first eliminated
the variables corresponding to computers in the legs, and the center computer (server) was
eliminated last; for Ring, we started with an arbitrary computer and followed the ring
order; for Ring and Star, the ring machines were eliminated first and then the center one;
finally, for the Ring of Rings topology, we eliminated the computers in the outer rings
first and then the ones in the inner ring.
We implemented the factored policy iteration and linear programming algorithms in
Matlab, using CPLEX as the LP solver. Experiments were performed on a Sun UltraSPARCII, 359 MHz with 256MB of RAM. To evaluate the complexity of the approximate policy
iteration with max-norm projection algorithm, tests were performed with increasing the
number of states, that is, increasing number of machines on the network. Figure 14 shows
the running time for increasing problem sizes, for various architectures. The simplest one
is the Star, where the backprojection of each basis function has scope restricted to two
variables and the largest factor in the cost network has scope restricted to two variables.
The most difficult one was the Bidirectional Ring, where factors contain five variables.
Note that the number of states is growing exponentially (indicated by the log scale in
Figure 14), but running times increase only logarithmically in the number of states, or
polynomially in the number of variables. We illustrate this behavior in Figure 14(d), where
we fit a 3rd order polynomial to the running times for the unidirectional ring. Note that
the size of the problem description grows quadratically with the number of variables: adding
a machine to the network also adds the possible action of fixing that machine. For this
problem,
the computation
cost of our factored algorithm empirically grows approximately


as O (n  |A|)1.5 , for a problem with n variables, as opposed to the exponential complexity
 poly (2n , |A|)  of the explicit algorithm.
For further evaluation, we measured the error in our approximate value function relative
to the true optimal value function V  . Note that it is only possible to compute V  for small
problems; in our case, we were only able to go up to 10 machines. For comparison, we
also evaluated the error in the approximate value function produced by the L2 -projection
449

fiGuestrin, Koller, Parr & Venkataraman

500

400

Ring
3 Legs

300

Ring of Rings

300
Total Time (minutes)

Total Time (minutes)

400

Star

200

Ring and Star
200

100

100

0

0
1E+00

1E+02

1E+04

1E+06 1E+08 1E+10
number of states

1E+12

1

1E+14

100

10000
1000000
number of states

(a)
1200

500

1000

Fitting a polynomial:

800

time = 0.0184|X| - 0.6655|X| +
9.2499|X| - 31.922

3

Ring:

Total Time (minutes)

Total Time (minutes)

1E+10

(b)

600

400

100000000

Unidirectional
Bidirectional

300
200

2

2

Quality of the fit: R = 0.999
600

400

100

200
0
1E+00

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

0
0

number of state s

(c)

10

20
30
40
number of variables |X|

50

60

(d)

Figure 14: (a)(c) Running times for policy iteration with max-norm projection on variants
of the SysAdmin problem; (d) Fitting a polynomial to the running time for the
Ring topology.

algorithm of Koller and Parr (2000). As we discussed in Section 6.4, the L2 projections in
factored MDPs by Koller and Parr are difficult and time consuming; hence, we were only
able to compare the two algorithms for smaller problems, where an equivalent L2 -projection
can be implemented using an explicit state space formulation. Results for both algorithms
are presented in Figure 15(a), showing the relative error of the approximate solutions to
the true value function for increasing problem sizes. The results indicate that, for larger
problems, the max-norm formulation generates a better approximation of the true optimal
value function V  than the L2 -projection. Here, we used two types of basis functions: the
same single variable functions, and pairwise basis functions. The pairwise basis functions
contain indicators for neighboring pairs of machines (i.e., functions of two variables). As
expected, the use of pairwise basis functions resulted in better approximations.
450

fiEfficient Solution Algorithms for Factored MDPs

0.4

Max norm, single basis
L2, single basis

0.3

Bellman Error / Rmax

Max norm, pair basis
L2, pair basis

Relative error:

0.2

0.1

0
3

4

5

6

7

8

9

10

number of variables

0.3

0.2

Ring
3 Legs

0.1

0
1E+00

Star

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

numbe r of sta tes

(a)

(b)

Figure 15: (a) Relative error to optimal value function V  and comparison to L2 projection
for Ring; (b) For large models, measuring Bellman error after convergence.

For these small problems, we can also compare the actual value of the policy generated
by our algorithm to the value of the optimal policy. Here, the value of the policy generated
by our algorithm is much closer to the value of the optimal policy than the error implied by
the difference between our approximate value function and V  . For example, for the Star
architecture with one server and up to 6 clients, our approximation with single variable
basis functions had relative error of 12%, but the policy we generated had the same value
as the optimal policy. In this case, the same was true for the policy generated by the L2
projection. In a Unidirectional Ring with 8 machines and pairwise basis, the relative
error between our approximation and V  was about 10%, but the resulting policy only had
a 6% loss over the optimal policy. For the same problem, the L2 approximation has a value
function error of 12%, and a true policy loss was 9%. In other words, both methods induce
policies that have lower errors than the errors in the approximate value function (at least
for small problems). However, our algorithm continues to outperform the L2 algorithm,
even with respect to actual policy loss.
For large models, we can no longer compute the correct value function, so we cannot
evaluate our results by computing kV   Hwk . Fortunately, as discussed in Section 7,
the Bellman error can be used to provide a bound on the approximation error and can be
computed efficiently by exploiting problem-specific structure. Figure 15(b) shows that the
Bellman error increases very slowly with the number of states.
It is also valuable to look at the actual decision-list policies generated in our experiments.
First, we noted that the lists tended to be short, the length of the final decision list policy
grew approximately linearly with the number of machines. Furthermore, the policy itself
is often fairly intuitive. In the Ring and Star architecture, for example, the decision list
says: If the server is faulty, fix the server; else, if another machine is faulty, fix it.
Thus far, we have presented scaling results for running times and approximation error for
our approximate PI approach. We now compare this algorithm to the simpler approximate
451

fiGuestrin, Koller, Parr & Venkataraman

400

200

PI single basis
PI single basis

160

LP single basis

140

LP pair basis

120

LP triple basis

Discounted reward of final policy
(averaged over 50 trials of 100 steps)

Total running time (minutes)

180

100
80
60
40
20
0
0

5

10

15

20

25

30

35

numbe r of machine s

LP single basis
LP pair basis

300

LP triple basis

200

100

0
0

10

20

30

40

numbe r of machine s

(a)

(b)

Figure 16: Approximate LP versus approximate PI on the SysAdmin problem with a Ring
topology: (a) running time; (b) estimated value of policy.

LP approach of Section 5. As shown in Figure 16(a), the approximate LP algorithm for
factored MDPs is significantly faster than the approximate PI algorithm. In fact, approximate PI with single-variable basis functions variables is more costly computationally than
the LP approach using basis functions over consecutive triples of variables. As shown in
Figure 16(b), for singleton basis functions, the approximate PI policy obtains slightly better
performance for some problem sizes. However, as we increase the number of basis functions
for the approximate LP formulation, the value of the resulting policy is much better. Thus,
in this problem, our factored approximate linear programming formulation allows us to use
more basis functions and to obtain a resulting policy of higher value, while still maintaining
a faster running time. These results, along with the simpler implementation, suggest that
in practice one may first try to apply the approximate linear programming algorithm before
deciding to move to the more elaborate approximate policy iteration approach.
9.2 Comparing Table-based and Rule-based Implementations
Our next evaluation compares a table-based representation, which exploits only additive
independence, to the rule-based representation presented in Section 8, which can exploit
both additive and context-specific independence. For these experiments, we implemented
our factored approximate linear programming algorithm with table-based and rule-based
representations in C++, using CPLEX as the LP solver. Experiments were performed on
a Sun UltraSPARC-II, 400 MHz with 1GB of RAM.
To evaluate and compare the algorithms, we utilized a more complex extension of the
SysAdmin problem. This problem, dubbed the Process-SysAdmin problem, contains three
state variables for each machine i in the network: Loadi , Statusi and Selectori . Each computer runs processes and receives rewards when the processes terminate. These processes
are represented by the Loadi variable, which takes values in {Idle, Loaded, Success}, and the
computer receives a reward when the assignment of Loadi is Success. The Statusi variable,
452

fitotal running time (minutes)

Efficient Solution Algorithms for Factored MDPs

200
Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100

Rule-based, pair basis

50
0
1E+00

1E+07

1E+14

1E+21

1E+28

1E+35

1E+42

number of states

total running time (minutes)

(a)
250
200

Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100
Rule-based, pair basis
50
0
1E+00 1E+04 1E+08 1E+12 1E+16 1E+20 1E+24 1E+28
number of states

(b)

total running time (minutes)

600
2
(x-1)

(x-1)

y = 7E- 17 x * 18 + 2E- 06 x * 18
+ 0.1124
2
R = 0.995

500

Table-based, single+ basis
Rule-based, single+ basis

400
300

3

2

y = 0.2294x - 4.5415x +
30.974x - 67.851
R 2= 0.9995

200
100
0
0

5

10
number of machines

15

20

(c)
Figure 17: Running time for Process-SysAdmin problem for various topologies: (a) Star;
(b) Ring; (c) Reverse star (with fit function).

453

fiGuestrin, Koller, Parr & Venkataraman

CPLEX time / Total time

1
0.8
Table-based, single+ basis
0.6
Rule-based, single+ basis
0.4
0.2
0
0

5

10

15

20

number of machines

Figure 18: Fraction of total running time spent in CPLEX for the Process-SysAdmin problem with a Ring topology.

representing the status of machine i, takes values in {Good, Faulty, Dead}; if its value is
Faulty, then processes have a smaller probability of terminating and if its value is Dead,
then any running process is lost and Loadi becomes Idle. The status of machine i can become Faulty and eventually Dead at random; however, if machine i receives a packet from
a dead machine, then the probability that Statusi becomes Faulty and then Dead increases.
The Selectori variable represents this communication by selecting one of the neighbors of i
uniformly at random at every time step. The SysAdmin can select at most one computer
to reboot at every time step. If computer i is rebooted, then its status becomes Good
with probability 1, but any running process is lost, i.e., the Loadi variable becomes Idle.
Thus, in this problem, the SysAdmin must balance several conflicting goals: rebooting a
machine kills processes, but not rebooting a machine may cause cascading faults in network.
Furthermore, the SysAdmin can only choose one machine to reboot, which imposes the additional tradeoff of selecting only one of the (potentially many) faulty or dead machines in
the network to reboot.
We experimented with two types of basis functions: single+ includes indicators over
all of the joint assignments of Loadi , Statusi and Selectori , and pair which, in addition,
includes a set of indicators over Statusi , Statusj , and Selectori = j, for each neighbor j
of machine i in the network. The discount factor was  = 0.95. The variable elimination
order eliminated all of the Loadi variables first, and then followed the same patterns as in
the simple SysAdmin problem, eliminating first Statusi and then Selectori when machine i
is eliminated.
Figure 17 compares the running times for the table-based implementation to the ones
for the rule-based representation for three topologies: Star, Ring, and Reverse star.
The Reverse star topology reverses the direction of the influences in the Star: rather
than the central machine influencing all machines in the topology, all machines influence
the central one. These three topologies demonstrate three different levels of CSI: In the
454

fiEfficient Solution Algorithms for Factored MDPs

Star topology, the factors generated by variable elimination are small. Thus, although the
running times are polynomial in the number of state variables for both methods, the tablebased representation is significantly faster than the rule-based one, due to the overhead of
managing the rules. The Ring topology illustrates an intermediate behavior: single+
basis functions induce relatively small variable elimination factors, thus the table-based
approach is faster. However, with pair basis the factors are larger and the rule-based
approach starts to demonstrate faster running times in larger problems. Finally, the Reverse star topology represents the worst-case scenario for the table-based approach. Here,
the scope of the backprojection of a basis function for the central machine will involve all
computers in the network, as all machines can potentially influence the central one in the
next time step. Thus, the size of the factors in the table-based variable elimination approach are exponential in the number of machines in the network, which is illustrated by
the exponential growth in Figure 17(c). The rule-based approach can exploit the CSI in this
problem; for example, the status of the central machine Status0 only depends on machine
j if the value selector is j, i.e., if Selector0 = j. By exploiting CSI, we can solve the same
problem in polynomial time in the number of state variables, as seen in the second curve in
Figure 17(c).
It is also instructive to compare the portion of the total running time spent in CPLEX
for the table-based as compared to the rule-based approach. Figure 18 illustrates this
comparison. Note that amount of time spent in CPLEX is significantly higher for the
table-based approach. There are two reasons for this difference: first, due to CSI, the LPs
generated by the rule-based approach are smaller than the table-based ones; second, rulebased variable elimination is more complex than the table-based one, due to the overhead
introduced by rule management. Interestingly, the proportion of CPLEX time increases as
the problem size increases, indicating that the asymptotic complexity of the LP solution is
higher than that of variable elimination, thus suggesting that, for larger problems, additional
large-scale LP optimization procedures, such as constraint generation, may be helpful.
9.3 Comparison to Apricodd
The most closely related work to ours is a line of research that began with the work of
Boutilier et al. (1995). In particular, the approximate Apricodd algorithm of Hoey et
al. (1999), which uses analytic decision diagrams (ADDs) to represent the value function
is a strong alternative approach for solving factored MDPs. As discussed in detail in Section 10, the Apricodd algorithm can successfully exploit context-specific structure in the
value function, by representing it with the set of mutually-exclusive and exhaustive branches
of the ADD. On the other hand, our approach can exploit both additive and context-specific
structure in the problem, by using a linear combination of non-mutually-exclusive rules. To
better understand this difference, we evaluated both our rule-based approximate linear
programming algorithm and Apricodd in two problems, Linear and Expon, designed by
Boutilier et al. (2000) to illustrate respectively the best-case and the worst-case behavior
of their algorithm. In these experiments, we used the web-distributed version of Apricodd (Hoey, St-Aubin, Hu, & Boutilier, 2002), running it locally on a Linux Pentium III
700MHz with 1GB of RAM.
455

fiGuestrin, Koller, Parr & Venkataraman

500

Rule-based

40
30

3

2

y = 0.1473x - 0.8595x + 2.5006x - 1.5964
2

R = 0.9997

20

Apricodd
2

y = 0.0254x + 0.0363x
+ 0.0725

10

Apricodd

400
Time (in seconds)

Time (in seconds)

50

x

2

x

y = 3E-05 * 2 - 0.0026 * 2 + 5.6737
R2 = 0.9999

300
200

Rule-based
y = 5.275x3 - 29.95x2 +
53.915x - 28.83
R2 = 1

100

2

R = 0.9983

0

0

6

8

10
12
14
16
Number of variables

18

6

20

8

10

12

Number of variables

(a)

(b)

Figure 19: Comparing Apricodd to rule-based approximate linear programming on the (a)
Linear and (b) Expon problems.

These two problems involve n binary variables X1 , . . . , Xn and n deterministic actions
a1 , . . . , an . The reward is 1 when all variables Xk are true, and is 0 otherwise. The problem
is discounted by a factor  = 0.99. The difference between the Linear and the Expon
problems is in the transition probabilities. In the Linear problem, the action ak sets the
variable Xk to true and makes all succeeding variables, Xi for i > k, false. If the state space
of the Linear problem is seen as a binary number, the optimal policy is to set repeatedly the
largest bit (Xk variable) which has all preceding bits set to true. Using an ADD, the optimal
value function for this problem can be represented in linear space, with n+1 leaves (Boutilier
et al., 2000). This is the best-case for Apricodd, and the algorithm can compute this value
function quite efficiently. Figure 19(a) compares the running time of Apricodd to that of
one of our algorithms with indicator basis functions between pairs of consecutive variables.
Note that both algorithms obtain the same policy in polynomial time in the number of
variables. However, in such structured problems, the efficient implementation of the ADD
package used in Apricodd makes it faster in this problem.
On the other hand, the Expon problem illustrates the worst-case for Apricodd. In this
problem, the action ak sets the variable Xk to true, if all preceding variables, Xi for i < k, are
true, and it makes all preceding variables false. If the state space is seen as a binary number,
the optimal policy goes through all binary numbers in sequence, by repeatedly setting the
largest bit (Xk variable) which has all preceding bits set to true. Due to discounting, the
n
optimal value function assigns a value of  2 j1 to the jth binary number, so that the
value function contains exponentially many different values. Using an ADD, the optimal
value function for this problem requires an exponential number of leaves (Boutilier et al.,
2000), which is illustrated by the exponential running time in Figure 19(b). However,
the same value function can be approximated very compactly as a factored linear value
function using n + 1 basis functions: an indicator over each variable Xk and the constant
base. As shown in Figure 19(b), using this representation, our factored approximate linear
programming algorithm computes the value function in polynomial time. Furthermore, the
456

fiEfficient Solution Algorithms for Factored MDPs

30

60

Running time (minutes)

Discounted value of policy
(avg. 50 runs of 100 steps)

Rule-based LP

50

Apricodd
40
30
20
10
0

Rule-based LP

25

Apricodd
20
15
10
5
0

0

2

4
6
8
Number of machines

10

12

0

2

4
6
8
Number of machines

(a)

12

10

12

(b)

50

30

45

Rule-based LP

Rule-based LP

40

Discounted value of policy
(avg. 50 runs of 100 steps)

Running time (minutes)

10

Apricodd

35
30
25
20
15
10

25

Apricodd

20
15
10
5

5
0

0
0

2

4
6
8
Number of machines

10

12

(c)

0

2

4
6
8
Number of machines

(d)

Figure 20: Comparing Apricodd to rule-based approximate linear programming with single+ basis functions on the Process-SysAdmin problem with Ring topology
(a) running time and (b) value of the resulting policy; and with Star topology
(c) running time and (d) value of the resulting policy.

policy obtained by our approach was optimal for this problem. Thus, in this problem, the
ability to exploit additive independence allows an efficient polynomial time solution.
We have also compared Apricodd to our rule-based approximate linear programming
algorithm on the Process-SysAdmin problem. This problem has significant additive structure in the reward function and factorization in the transition model. Although this type of
structure is not exploited directly by Apricodd, the ADD approximation steps performed by
the algorithm can, in principle, allow Apricodd to find approximate solutions to the problem. We spent a significant amount of time attempting to find the best set of parameters
for Apricodd for these problems.4 We settled on the sift method of variable reordering
and the round approximation method with the size (maximum ADD size) criteria. To
4. We are very grateful to Jesse Hoey and Robert St-Aubin for their assistance in selecting the parameters.

457

fiGuestrin, Koller, Parr & Venkataraman

allow the value function representation to scale with the problem size, we set the maximum
ADD size to 4000 + 400n for a network with n machines. (We experimented with a variety
of different growth rates for the maximum ADD size; here, as for the other parameters,
we selected the choice that gave the best results for Apricodd.) We compared Apricodd
with these parameters to our rule-based approximate linear programming algorithm with
single+ basis functions on a Pentium III 700MHz with 1GB of RAM. These results are
summarized in Figure 20.
On very small problems (up to 45 machines), the performance of the two algorithms is
fairly similar in terms of both the running time and the quality of the policies generated.
However, as the problem size grows, the running time of Apricodd increases rapidly, and
becomes significantly higher than that of our algorithm . Furthermore, as the problem size
increases, the quality of the policies generated by Apricodd also deteriorates. This difference
in policy quality is caused by the different value function representation used by the two
algorithms. The ADDs used in Apricodd represent k different values with k leaves; thus,
they are forced to agglomerate many different states and represent them using a single value.
For smaller problems, such agglomeration can still represent good policies. Unfortunately,
as the problem size increases and the state space grows exponentially, Apricodds policy
representation becomes inadequate, and the quality of the policies decreases. On the other
hand, our linear value functions can represent exponentially many values with only k basis
functions, which allows our approach to scale up to significantly larger problems.

10. Related Work
The most closely related work to ours is a line of research that began with the work of
Boutilier et al. (1995). We address this comparison separately below, but we begin this
section with some broader background references.
10.1 Approximate MDP Solutions
The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the
1950s. The importance of value function approximation was recognized at an early stage
by Bellman himself (1963). In the early 1990s the MDP framework was recognized by AI
researchers as a formal framework that could be used to address the problem of planning
under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993).
Within the AI community, value function approximation developed concomitantly with
the notion of value function representations for Markov chains. Suttons seminal paper on
temporal difference learning (1988), which addressed the use of value functions for prediction
but not planning, assumed a very general representation of the value function and noted
the connection to general function approximators such as neural networks. However, the
stability of this combination was not directly addressed at that time.
Several important developments gave the AI community deeper insight into the relationship between function approximation and dynamic programming. Tsitsiklis and Van
Roy (1996a) and, independently, Gordon (1995) popularized the analysis of approximate
MDP methods via the contraction properties of the dynamic programming operator and
function approximator. Tsitsiklis and Van Roy (1996b) later established a general convergence result for linear value function approximators and T D(), and Bertsekas and
458

fiEfficient Solution Algorithms for Factored MDPs

Tsitsiklis (1996) unified a large body of work on approximate dynamic programming under
the name of Neuro-dynamic Programming, also providing many novel and general error
analyses.
Approximate linear programming for MDPs using linear value function approximation
was introduced by Schweitzer and Seidmann (1985), although the approach was somewhat
deprecated until fairly recently due the lack of compelling error analyses and the lack of an
effective method for handling the large number of constraints. Recent work by de Farias
and Van Roy (2001a, 2001b) has started to address these concerns with new error bounds
and constraint sampling methods. Our approach, rather than sampling constraints, utilizes
structure in the model and value function to represent all of the constraints compactly.
10.2 Factored Approaches
Tatman and Shachter (1990) considered the additive decomposition of value nodes in influence diagrams. A number of approaches to factoring of general MDPs have been explored in
the literature. Techniques for exploiting reward functions that decompose additively were
studied by Meuleau et al. (1998), and by Singh and Cohn (1998).
The use of factored representations such as dynamic Bayesian networks was pioneered
by Boutilier et al. (1995) and has developed steadily in recent years. These methods rely
on the use of context-specific structures such as decision trees or analytic decision diagrams
(ADDs) (Hoey et al., 1999) to represent both the transition dynamics of the DBN and
the value function. The algorithms use dynamic programming to partition the state space,
representing the partition using a tree-like structure that branches on state variables and
assigns values at the leaves. The tree is grown dynamically as part of the dynamic programming process and the algorithm creates new leaves as needed: A leaf is split by the
application of a DP operator when two states associated with that leaf turn out to have
different values in the backprojected value function. This process can also be interpreted
as a form of model minimization (Dean & Givan, 1997).
The number of leaves in a tree used to represent a value function determines the computational complexity of the algorithm. It also limits the number of distinct values that can
be assigned to states: since the leaves represent a partitioning of the state space, every state
maps to exactly one leaf. However, as was recognized early on, there are trivial MDPs which
require exponentially large value functions. This observation led to a line of approximation
algorithms aimed at limiting the tree size (Boutilier & Dearden, 1996) and, later, limiting
the ADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim and Dean (2001) also explored
techniques for discovering tree-structured value functions for factored MDPs. While these
methods permit good approximate solutions to some large MDPs, their complexity is still
determined by the number of leaves in the representation and the number of distinct values
than can be assigned to states is still limited as well.
Tadepalli and Ok (1996) were the first to apply linear value function approximation
to Factored MDPs. Linear value function approximation is a potentially more expressive
approximation method because it can assign unique values to every state in an MDP without
requiring storage space that is exponential in the number of state variables. The expressive
power of a tree with k leaves can be captured by a linear function approximator with k basis
functions such that basis function hi is an indicator function that tests if a state belongs
459

fiGuestrin, Koller, Parr & Venkataraman

in the partition of leaf i. Thus, the set of value functions that can be represented by a
tree with k leaves is a subset of the set of value functions that can be represented by a
value function with k basis functions. Our experimental results in Section 9.3 highlight this
difference by showing an example problem that requires exponentially many leaves in the
value function, but that can be approximated well using a linear value function.
The main advantage of tree-based value functions is that their structure is determined
dynamically during the solution of the MDP. In principle, as the value function representation is derived automatically from the model description, this approach requires less insight
from the user. In problems for which the value function can be well approximated by a relatively small number of values, this approach provides an excellent solution to the problem.
Our method of linear value function approximation aims to address what we believe to be
the more common case, where a large range of distinct values is required to achieve a good
approximation.
Finally, we note that Schuurmans and Patrascu (2001), based on our earlier work on
max-norm projection using cost networks and linear programs, independently developed
an alternative approach to approximate linear programming using a cost network. Our
method embeds a cost network inside a single linear program. By contrast, their method
is based on a constraint generation approach, using a cost network to detect constraint
violations. When constraint violations are found, a new constraint is added, repeatedly
generating and attempting to solve LPs until a feasible solution is found. Interestingly,
as the approach of Schuurmans and Patrascu uses multiple calls to variable elimination in
order to speed up the LP solution step, it will be most successful when the time spent
solving the LP is significantly larger than the time required for variable elimination. As
suggested in Section 9.2, the LP solution time is larger for the table-based approach. Thus,
Schuurmans and Patrascus constraint generation method will probably be more successful
in table-based problems than in rule-based ones.

11. Conclusions
In this paper, we presented new algorithms for approximate linear programming and approximate dynamic programming (value and policy iteration) for factored MDPs. Both
of these algorithms leverage on a novel LP decomposition technique, analogous to variable elimination in cost networks, which reduces an exponentially large LP to a provably
equivalent, polynomial-sized one.
Our approximate dynamic programming algorithms are motivated by error analyses
showing the importance of minimizing L error. These algorithms are more efficient and
substantially easier to implement than previous algorithms based on the L2 -projection. Our
experimental results suggest that they also perform better in practice.
Our approximate linear programming algorithm for factored MDPs is simpler, easier to
implement and more general than the dynamic programming approaches. Unlike our policy
iteration algorithm, it does not rely on the default action assumption, which states that
actions only affect a small number of state variables. Although this algorithm does not have
the same theoretical guarantees as max-norm projection approaches, empirically it seems to
be a favorable option. Our experiments suggest that approximate policy iteration tends to
generate better policies for the same set of basis functions. However, due to the computa460

fiEfficient Solution Algorithms for Factored MDPs

tional advantages, we can add more basis functions to the approximate linear programming
algorithm, obtaining a better policy and still maintaining a much faster running time than
approximate policy iteration.
Unlike previous approaches, our algorithms can exploit both additive and contextspecific structure in the factored MDP model. Typical real-world systems possess both
of these types of structure. thus, this feature of our algorithms will increase the applicability of factored MDPs to more practical problems. We demonstrated that exploiting
context-specific independence, by using a rule-based representation instead of the standard
table-based one, can yield exponential improvements in computational time when the problem has significant amounts of CSI. However, the overhead of managing sets of rules make
it less well-suited for simpler problems. We also compared our approach to the work of
Boutilier et al. (2000), which exploits only context-specific structure. For problems with
significant context-specific structure in the value function, their approach can be faster due
to their efficient handling of the ADD representation. However, there are problems with
significant context-specific structure in the problem representation, rather than in the value
function, which require exponentially large ADDs. In some such problems, we demonstrated that by using a linear value function our algorithm can obtain a polynomial-time
near-optimal approximation of the true value function.
The success of our algorithm depends on our ability to capture the most important
structure in the value function using a linear, factored approximation. This ability, in turn,
depends on the choice of the basis functions and on the properties of the domain. The
algorithms currently require the designer to specify the factored basis functions. This is a
limitation compared to the algorithms of Boutilier et al. (2000), which are fully automated.
However, our experiments suggest that a few simple rules can be quite successful for designing a basis. First, we ensure that the reward function is representable by our basis. A
simple basis that, in addition, contained a separate set of indicators for each variable often
did quite well. We can also add indicators over pairs of variables; most simply, we can choose
these according to the DBN transition model, where an indicator is added between variables
Xi and each one of the variables in Parents(Xi ), thus representing one-step influences. This
procedure can be extended, adding more basis functions to represent more influences as
required. Thus, the structure of the DBN gives us indications of how to choose the basis
functions. Other sources of prior knowledge can also be included for further specifying the
basis.
Nonetheless, a general algorithm for choosing good factored basis functions still does
not exist. However, there are some potential approaches: First, in problems with CSI, one
could apply the algorithms of Boutilier et al. for a few iterations to generate partial treestructured solutions. Indicators defined over the variables in backprojection of the leaves
could, in turn, be used to generate a basis set for such problems. Second, the Bellman
error computation, which can be performed efficiently as shown in Section 7, does not only
provide a bound on the quality of the policy, but also the actual state where the error is
largest. This knowledge can be used to create a mechanism to incrementally increase the
basis set, adding new basis functions to tackle states with high Bellman error.
There are many other possible extensions to this work. We have already pursued extensions to collaborative multiagent systems, where multiple agents act simultaneously to
maximize the global reward (Guestrin et al., 2001b), and factored POMDPs, where the
461

fiGuestrin, Koller, Parr & Venkataraman

full state is not observed directly, but indirectly through observation variables (Guestrin,
Koller, & Parr, 2001c). However, there are other settings that remain to be explored. In
particular, we hope to address the problem of learning a factored MDP and planning in a
competitive multiagent system.
Additionally, in this paper we have tackled problems where the induced width of the cost
network is sufficiently low or that possess sufficient context-specific structure to allow for
the exact solution of our factored LPs. Unfortunately, some practical problems may have
prohibitively large induced width. We plan to leverage on ideas from loopy belief propagation algorithms for approximate inference in Bayesian networks (Pearl, 1988; Yedidia,
Freeman, & Weiss, 2001) to address this issue.
We believe that the methods described herein significantly further extend the efficiency,
applicability and general usability of factored models and value functions for the control of
practical dynamic systems.

Acknowledgements
We are very grateful to Craig Boutilier, Dirk Ormoneit and Uri Lerner for many useful
discussions, and to the anonymous reviewers for their detailed and thorough comments. We
also would like to thank Jesse Hoey, Robert St-Aubin, Alan Hu, and Craig Boutilier for
distributing their algorithm and for their very useful assistance in using Apricodd and in
selecting its parameters. This work was supported by the DoD MURI program, administered by the Office of Naval Research under Grant N00014-00-1-0637, by Air Force contract
F30602-00-2-0598 under DARPAs TASK program, and by the Sloan Foundation. The first
author was also supported by a Siebel Scholarship.

Appendix A. Proofs
A.1 Proof of Lemma 3.3
There exists a setting to the weights  the all zero setting  that yields a bounded maxnorm projection error P for any policy (P  Rmax ). Our max-norm projection operator
chooses the set of weights that minimizes the projection error  (t) for each policy  (t) . Thus,
the projection error  (t) must be at least as low as the one given by the zero weights P
(which is bounded). Thus, the error remains bounded for all iterations.
A.2 Proof of Theorem 3.5
First, we need to bound our approximation of V(t) :




V(t)  Hw(t) 












 T(t) Hw(t)  Hw(t) 




+ V(t)  T(t) Hw(t) 








(t)
(t) 
 T(t) Hw  Hw  +  V(t)  Hw(t) 






; (triangle inequality;)

; (T(t) is a contraction.)

Moving the second term to the right hand side and dividing through by 1  , we obtain:




V(t)  Hw(t) 






1 
 (t)


.
T(t) Hw(t)  Hw(t)  =

1
1
462

(24)

fiEfficient Solution Algorithms for Factored MDPs

For the next part of the proof, we adapt a lemma of Bertsekas and Tsitsiklis (1996, Lemma
6.2, p.277) to fit into our framework. After some manipulation, this lemma can be reformulated as:
kV   V(t+1) k   kV   V(t) k +


2 


V(t)  Hw(t)  .

1

(25)

The proof is concluded by substituting Equation (24) into Equation (25) and, finally, induction on t.
A.3 Proof of Theorem 4.4
First, note that the equality constraints represent a simple change of variable. Thus, we
can rewrite Equation (12) in terms of these new LP variables ufzii as:
  max

X

x

ufzii ,

(26)

i

where any assignment to the weights w implies an assignment for each ufzii . After this stage,
we only have LP variables.
It remains to show that the factored LP construction is equivalent to the constraint in
Equation (26). For a system with n variables {X1 , . . . , Xn }, we assume, without loss of
generality, that variables are eliminated starting from Xn down to X1 . We now prove the
equivalence by induction on the number of variables.
The base case is n = 0, so that the functions ci (x) and b(x) in Equation (12) all have
empty scope. In this case, Equation (26) can be written as:


X

uei .

(27)

i

In this case, no transformation is done on the constraint, and equivalence is immediate.
Now, we assume the result holds for systems with i1 variables and prove the equivalence
for a system with i variables. In such a system, the maximization can be decomposed into
two terms: one with the factors that do not depend on Xi , which are irrelevant to the
maximization over Xi , and another term with all the factors that depend on Xi . Using this
decomposition, we can write Equation (26) as:
 



max

X ej

x1 ,...,xi

uzj ;

j

max 

x1 ,...,xi1




X

X

uezll + max
xi

l : xi 6zl

e
uzjj  .

(28)

j : xi zj

At this point we can define new LP variables uez corresponding to the second term on
the right hand side of the constraint. These new LP variables must satisfy the following
constraint:
uez  max
xi

X ej
j=1

463

u(z,xi )[Zj ] .

(29)

fiGuestrin, Koller, Parr & Venkataraman

This new non-linear constraint is again represented in the factored LP construction by a
set of equivalent linear constraints:
uez 

X ej
j=1

u(z,xi )[Zj ] , z, xi .

(30)

The equivalence between the non-linear constraint Equation (29) and the set of linear constraints in Equation (30) can be shown by considering binding constraints. For each new
LP variable created uez , there are |Xi | new constraints created, one for each value xi of Xi .
For any assignment to the LP variables in the right hand side of the constraint in EquaP
ej
tion (30), only one of these |Xi | constraints is relevant. That is, one where `j=1 u(z,x
i )[Zj ]
is maximal, which corresponds to the maximum over Xi . Again, if for each value of z more
than one assignment to Xi achieves the maximum, then any of (and only) the constraints
corresponding to those maximizing assignments could be binding. Thus, Equation (29) and
Equation (30) are equivalent.
Substituting the new LP variables uez into Equation (28), we get:


max

x1 ,...,xi1

X

uezll + uez ,

l : xi 6zl

which does not depend on Xi anymore. Thus, it is equivalent to a system with i1 variables,
concluding the induction step and the proof.
A.4 Proof of Lemma 7.1
First note that at iteration t + 1 the objective function (t+1) of the max-norm projection
LP is given by:








(t+1) = Hw(t+1)  R(t+1) + P(t+1) Hw(t+1)  .


However, by convergence the value function estimates are equal for both iterations:
w(t+1) = w(t) .
So we have that:








(t+1) = Hw(t)  R(t+1) + P(t+1) Hw(t)  .


In operator notation, this term is equivalent to:






(t+1) = Hw(t)  T(t+1) Hw(t)  .


Note that,  (t+1) = Greedy(Hw(t) ) by definition. Thus, we have that:
T(t+1) Hw(t) = T  Hw(t) .
Finally, substituting into the previous expression, we obtain the result:






(t+1) = Hw(t)  T  Hw(t)  .


464

fiEfficient Solution Algorithms for Factored MDPs

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity of finding embeddings
in a K-tree. SIAM Journal of Algebraic and Discrete Methods, 8 (2), 277  284.
Becker, A., & Geiger, D. (2001). A sufficiently fast algorithm for finding close to optimal
clique trees. Artificial Intelligence, 125 (1-2), 317.
Bellman, R., Kalaba, R., & Kotkin, B. (1963). Polynomial approximation  a new computational technique in dynamic programming. Math. Comp., 17 (8), 155161.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, New
Jersey.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, New
York.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, Massachusetts.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 11, 1 
94.
Boutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamic
programming. In Proc. ICML, pp. 5462.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proc. IJCAI, pp. 11041111.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121 (1-2), 49107.
Cheney, E. W. (1982). Approximation Theory (2nd edition). Chelsea Publishing Co., New
York, NY.
de Farias, D., & Van Roy, B. (2001a). The linear programming approach to approximate
dynamic programming. Submitted to Operations Research.
de Farias, D., & Van Roy, B. (2001b). On constraint sampling for the linear programming approach to approximate dynamic programming. To appear in Mathematics of
Operations Research.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in
stochastic domains. In Proceedings of the Eleventh National Conference on Artificial
Intelligence (AAAI-93), pp. 574579, Washington, D.C. AAAI Press.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5 (3), 142150.
Dean, T., & Givan, R. (1997). Model minimization in Markov decision processes. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI97), pp. 106111, Providence, Rhode Island, Oregon. AAAI Press.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
465

fiGuestrin, Koller, Parr & Venkataraman

Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (12), 4185.
Gordon, G. (1995). Stable function approximation in dynamic programming. In Proceedings
of the Twelfth International Conference on Machine Learning, pp. 261268, Tahoe
City, CA. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001a). Max-norm projections for factored MDPs.
In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-01), pp. 673  680, Seattle, Washington. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001b). Multiagent planning with factored MDPs.
In 14th Neural Information Processing Systems (NIPS-14), pp. 15231530, Vancouver,
Canada.
Guestrin, C. E., Koller, D., & Parr, R. (2001c). Solving factored POMDPs with linear value
functions. In Seventeenth International Joint Conference on Artificial Intelligence
(IJCAI-01) workshop on Planning under Uncertainty and Incomplete Information,
pp. 67  75, Seattle, Washington.
Guestrin, C. E., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination and planning with factored MDPs. In The Eighteenth National Conference
on Artificial Intelligence (AAAI-2002), pp. 253259, Edmonton, Canada.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. In Proceedings of the Fifteenth Conference on Uncertainty in
Artificial Intelligence (UAI-99), pp. 279288, Stockholm, Sweden. Morgan Kaufmann.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (2002). Stochastic planning using decision
diagrams  C implementation. http://www.cs.ubc.ca/spider/staubin/Spudd/.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. In Howard, R. A., & Matheson, J. E. (Eds.), Readings on the Principles and Applications of Decision Analysis,
pp. 721762. Strategic Decisions Group, Menlo Park, California.
Keeney, R. L., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and
Value Tradeoffs. Wiley, New York.
Kim, K.-E., & Dean, T. (2001). Solving factored Mdps using non-homogeneous partitioning. In Proceedings of the Seventeenth International Joint Conference on Artificial
Intelligence (IJCAI-01), pp. 683  689, Seattle, Washington. Morgan Kaufmann.
Kjaerulff, U. (1990). Triangulation of graphs  algorithms giving small total state space.
Tech. rep. TR R 90-09, Department of Mathematics and Computer Science, Strandvejen, Aalborg, Denmark.
Koller, D., & Parr, R. (1999). Computing factored value functions for policies in structured
MDPs. In Proceedings of the Sixteenth International Joint Conference on Artificial
Intelligence (IJCAI-99), pp. 1332  1339. Morgan Kaufmann.
Koller, D., & Parr, R. (2000). Policy iteration for factored MDPs. In Proceedings of the
Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI-00), pp. 326 
334, Stanford, California. Morgan Kaufmann.
466

fiEfficient Solution Algorithms for Factored MDPs

Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving very large weakly-coupled Markov decision processes. In Proceedings
of the 15th National Conference on Artificial Intelligence, pp. 165172, Madison, WI.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley, New York.
Reed, B. (1992). Finding approximate separators and computing tree-width quickly. In
24th Annual Symposium on Theory of Computing, pp. 221228. ACM.
Schuurmans, D., & Patrascu, R. (2001). Direct value-approximation for factored MDPs.
In Advances in Neural Information Processing Systems (NIPS-14), pp. 15791586,
Vancouver, Canada.
Schweitzer, P., & Seidmann, A. (1985). Generalized polynomial approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110, 568
 582.
Simon, H. A. (1981). The Sciences of the Artificial (second edition). MIT Press, Cambridge,
Massachusetts.
Singh, S., & Cohn, D. (1998). How to dynamically merge Markov decision processes. In
Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances in Neural Information
Processing Systems, Vol. 10. The MIT Press.
St-Aubin, R., Hoey, J., & Boutilier, C. (2001). APRICODD: Approximate policy construction using decision diagrams. In Advances in Neural Information Processing Systems
13: Proceedings of the 2000 Conference, pp. 10891095, Denver, Colorado. MIT Press.
Stiefel, E. (1960). Note on Jordan elimination, linear programming and Tchebycheff approximation. Numerische Mathematik, 2, 1  17.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine
Learning, 3, 944.
Tadepalli, P., & Ok, D. (1996). Scaling up average reward reinforcmeent learning by approximating the domain models and the value function. In Proceedings of the Thirteenth
International Conference on Machine Learning, Bari, Italy. Morgan Kaufmann.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming and influence diagrams.
IEEE Transactions on Systems, Man and Cybernetics, 20 (2), 365379.
Tsitsiklis, J. N., & Van Roy, B. (1996a). Feature-based methods for large scale dynamic
programming. Machine Learning, 22, 5994.
Tsitsiklis, J. N., & Van Roy, B. (1996b). An analysis of temporal-difference learning with
function approximation. Technical report LIDS-P-2322, Laboratory for Information
and Decision Systems, Massachusetts Institute of Technology.
Van Roy, B. (1998). Learning and Value Function Approximation in Complex Decision
Processes. Ph.D. thesis, Massachusetts Institute of Technology.
467

fiGuestrin, Koller, Parr & Venkataraman

Williams, R. J., & Baird, L. C. I. (1993). Tight performance bounds on greedy policies based
on imperfect value functions. Tech. rep., College of Computer Science, Northeastern
University, Boston, Massachusetts.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. In Advances
in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,
pp. 689695, Denver, Colorado. MIT Press.
Zhang, N., & Poole, D. (1999). On the role of context-specific independence in probabilistic
reasoning. In Proceedings of the Sixteenth International Joint Conference on Artificial
Intelligence (IJCAI-99), pp. 12881293. Morgan Kaufmann.

468

fiJournal of Artificial Intelligence Research 19 (2003) 1-10

Submitted 10/02; published 7/03

Research Note
New Polynomial Classes for Logic-Based Abduction
Bruno Zanuttini

zanutti@info.unicaen.fr

GREYC, Universite de Caen, Boulevard du Marechal Juin
14032 Caen Cedex, France

Abstract
We address the problem of propositional logic-based abduction, i.e., the problem of
searching for a best explanation for a given propositional observation according to a given
propositional knowledge base. We give a general algorithm, based on the notion of projection; then we study restrictions over the representations of the knowledge base and of the
query, and find new polynomial classes of abduction problems.

1. Introduction
Abduction consists in searching for a plausible explanation for a given observation. For
instance, if p |= q then p is a plausible explanation for the observation q. More generally,
abduction is the process of searching for a set of facts (the explanation, here p) that,
conjointly with a given knowledge base (here p  q), imply a given query (q). This process
is also constrained by a set of hypotheses among which the explanations have to be chosen,
and by a preference criterion among them.
The problem of abduction proved its practical interest in many domains. For instance, it
has been used to formalize text interpretation (Hobbs et al., 1993), system (Coste-Marquis
& Marquis, 1998; Stumptner & Wotawa, 2001) or medical diagnosis (Bylander et al., 1989,
Section 6). It is also closely related to configuration problems (Amilhastre et al., 2002),
to the ATMS/CMS (Reiter & de Kleer, 1987), to default reasoning (Selman & Levesque,
1990) and even to induction (Goebel, 1997).
We are interested here in the complexity of propositional logic-based abduction, i.e., we
assume both the knowledge base and the query are represented by propositional formulas.
Even in this framework, many different formalizations have been proposed in the literature,
mainly differing about the definition of an hypothesis and that of a best explanation (Eiter
& Gottlob, 1995). We assume here that the hypotheses are the conjunctions of literals
formed upon a distinguished subset of the variables involved, and that a best explanation
is one no proper subconjunction of which is an explanation (subset-minimality criterion).
Our purpose is to exhibit new polynomial classes of abduction problems. We give a
general algorithm for finding a best explanation in the framework defined above, independently from the syntactic form of the formulas representing the knowledge base and the
query. Then we explore the syntactic forms that allow a polynomial running time for this
algorithm. We find new polynomial classes of abduction problems, among which the one
restricting the knowledge base to be given as a Horn DNF and the query as a positive CNF,
and the one restricting the knowledge base to be given as an affine formula and the query as
a disjunction of linear equations. Our algorithm also unifies several previous such results.
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBruno Zanuttini

The note is organized as follows. We first recall the useful notions of propositional
logic (Section 2), formalize the problem (Section 3) and briefly survey previous work about
the complexity of abduction (Section 4). Then we give our algorithm (Section 5) and
explore polynomial classes for it (Section 6). Finally, we discuss our results and perspectives
(Section 7). For lack of space we cannot detail proofs, but a longer version of this work,
containing detailed proofs and examples, is available (Zanuttini, 2003).

2. Preliminaries
We assume a countable number of propositional variables x1 , x2 . . . and the standard connectives , , , , , . A literal is either a variable xi (positive literal) or its negation
xi (negative literal). A propositional formula is a well-formed formula built on a finite
number of variables and on the connectives; V ar() denotes the set of variables that occur
in the propositional formula . A clause is a finite disjunction of literals, and a propositional formula is in Conjunctive Normal Form (CNF) if it is written as a finite conjunction
of clauses. For instance,  = (x1  x2 )  (x1  x2  x3 ) is in CNF. The dual notions
of clause and CNF are the notions of term (finite conjunction of literals) and Disjunctive
Normal Form (DNF) (finite disjunction of terms).
An assignment to a set of variables V is a set of literals m that contains exactly one
literal per variable in V , and a model of a propositional formula  is an assignment m
to V ar() that satisfies  in the usual way, where m assigns 1 to xi iff xi  m; we also
write m as a tuple, e.g., 0010 for {x1 , x2 , x3 , x4 }. We write m[i] for the value assigned
to xi by m, and M() for the set of all the models of a propositional formula ;  is
said to be satisfiable if M() 6= . A formula  is said to imply a propositional formula 0
(written  |= 0 ) if M()  M(0 ). More generally, we identify sets of models with Boolean
functions, and use the notations M (negation), M  M0 (disjunction) and so on.
The notion of projection is very important for the rest of the paper. For m an assignment
to a set of variables V and A  V , write SelectA (m) for the set of literals in m that are
formed upon A, e.g., Select{x1 ,x2 } (0110) = 01. Projecting a set of assignments onto a subset
A of its variables intuitively consists in replacing each assignment m with SelectA (m); for
sake of simplicity however, we define the projection of a set of models M to be built upon
the same set of variables as M. This yields the following definition.
Definition 1 (projection) Let V = {x1 , . . . , xn } be a set of variables, M a set of assignments to V and A  V . The projection of M onto A is the set of assignments to V
M|A = {m | m0  M, SelectA (m0 ) = SelectA (m)}.
For instance, let M = {0001, 0010, 0111, 1100, 1101} be a set of assignments to V =
{x1 , x2 , x3 , x4 }, and let A = {x1 , x2 }. Then it is easily seen that
M|A = {0000, 0001, 0010, 0011}  {0100, 0101, 0110, 0111}  {1100, 1101, 1110, 1111}
since {SelectA (m) | m  M} = {00, 01, 11}.
Remark that the projection of the set of models of a formula  onto a set of variables
A is the set of models of the most general consequence of  that is independent of all the
variables not in A. Note also that the projection of M() onto A is the set of models of the
formula obtained from  by forgetting its variables not occurring in A. For more details
2

fiLogic-Based Abduction

about variable forgetting and independence we refer the reader to the work by Lang et
al. (Lang et al., 2002).
It is useful to note some straightforward properties of projection. Let M, M0 denote
two sets of assignments to the set of variables V , and let A  V . First, projection is
distributive over disjunction, i.e., (M  M0 )|A = M|A  M0 |A . Now it is distributive over
conjunction when M does not depend on the variables M0 depends on, i.e., when there exist
A, A0  V , A  A0 =  with M|A = M (M does not depend on V \A) and M0 |A0 = M0 ,
(M  M0 )|A = M|A  M0 |A holds; note that this is not true in the general case. Note finally
that in general (M)|A is not the same as M|A .

3. Our Model of Abduction
We now formalize our model; for sake of simplicity, we first define abduction problems and
then the notions of hypothesis and explanation.
Definition 2 (abduction problem) A triple  = (, , A) is called an abduction problem if  and  are satisfiable propositional formulas and A is a set of variables with
V ar(), A  V ar();  is called the knowledge base of ,  its query and A its set
of abducibles.
Definition 3 (hypothesis,explanation) Let  = (, , A) be an abduction problem. An
hypothesis for  is a set of literals formed upon A (seen as their conjunction), and an
hypothesis E for  is an explanation for  if   E is satisfiable and   E |= . If no
proper subconjunction of E is an explanation for , E is called a best explanation for .
Note that this framework does not allow one to specify that a variable must occur unnegated
(resp. negated) in an explanation. We do not think this is a prohibiting restriction, since
abducibles are intuitively meant to represent the variables whose values can be, e.g., modified, observed or repaired, and then no matter their sign in an explanation. But we note that
it is a restriction, and that a more general framework can be defined where the abducibles
are literals and the hypotheses, conjunctions of abducibles (Marquis, 2000).
We are interested in the computational complexity of computing a best explanation for
a given abduction problem, or asserting there is none at all. Following the usual model,
we establish complexities with respect to the size of the representations of  and  and to
the number of abducibles; for hardness results, the following associated decision problem is
usually considered: is there at least one explanation for ? Obviously, if this latter problem
is hard, then the function problem also is.

4. Previous Work
The main general complexity results about propositional logic-based abduction with subsetminimality preference were stated by Eiter and Gottlob (1995). The authors show that
deciding whether a given abduction problem has a solution at all is a P2 -complete problem,
even if A  V ar() = V ar() and  is in CNF. As stated as well by Selman and Levesque
(1990), they also establish that this problem becomes only NP-complete when  is Horn,
and even acyclic Horn. Note that when SAT and deduction are polynomial with  the
problem is obviously in NP.
3

fiBruno Zanuttini

In fact, very few classes of abduction problems are known to be polynomial for the
search for explanations. As far as we know, the only such classes are those defined by the
following restrictions (once again we refer the reader to the references for definitions):
  is in 2CNF and  is in 2DNF (Marquis, 2000, Section 4.2)
  is given as a monotone CNF and  as a clause (Marquis, 2000, Section 4.2)
  is given as a definite Horn CNF and  as a conjunction of positive literals (Selman
& Levesque, 1990; Eiter & Gottlob, 1995)
  is given as an acyclic Horn CNF with pseudo-completion unit-refutable and  is a
variable (Eshghi, 1993)
  has bounded induced kernel width and  is given as a literal (del Val, 2000)
  is represented by its set of characteristics models (with respect to a particular basis)
and  is a variable (Khardon & Roth, 1996); note that a set of characteristic models
is not a propositional formula, but that the result is however similar to the other ones
  is represented by the set of its models, or, equivalently, by a DNF with every variable
occurring in each term, and  is any propositional formula.
The first two classes are proved polynomial with a general method for solving abduction
problems with the notion of prime implicants, the last one is obvious since all the information
is explicitely given in the input, and the four others are exhibited with ad hoc algorithms.
Let us also mention that Amilhastre et al. (2002) study most of the related problems in
the more general framework of multivalued theories instead of propositional formulas, i.e.,
when the domain of the variables is not restricted to be {0, 1}. The authors mainly show,
as far as this note is concerned, that deciding whether there exists an explanation is still a
P2 -complete problem (Amilhastre et al., 2002, Table 1).
Note that not all these results are stated in our exact framework in the papers cited
above, but that they all still hold in it. Let us also mention that the problem of enumerating
all the best explanations for a given abduction problem is of great interest; Eiter and Makino
(2002) provide a discussion and some first results about it, mainly in the case when the
knowledge base is Horn.

5. A General Algorithm
We now give the principle of our algorithm. Let us stress first that, as well as, e.g., Marquis
construction (Marquis, 2000, Section 4.2), its outline matches point by point the definition
of a best explanation; our ideas and Marquis are anyway rather close.
We are first interested in the hypotheses in which every abducible x  A occurs (either
negated or unnegated); let us call them full hypotheses. Note indeed that every explanation
E for an abduction problem is a subconjunction of a full explanation F ; indeed, since E is
by definition such that   E is satisfiable and implies , it suffices to let F be SelectA (m)
for a model m of   E  . Minimization of F will be discussed later on.
4

fiLogic-Based Abduction

Proposition 1 Let  = (, , A) be an abduction problem, and F a full hypothesis of .
Then F is an explanation for  if and only if there exists an assignment m to V ar() with
F = SelectA (m) and m  M()  (M(  ))|A .
Proof Assume first F is an explanation for . Then (i) there exists an assignment m to
V ar() with m |=   F , thus F = SelectA (m) and m  M(), and (ii)   F |= , i.e.,
  F   is unsatisfiable, thus F 
/ {SelectA (m) | m  M(  )}, thus m 
/ (M(  ))|A ,
thus m  (M(  ))|A . Conversely, if m  M()  (M(  ))|A let F = SelectA (m).
Then we have (i) since m  M(),   F is satisfiable, and (ii) since m 
/ (M(  ))|A ,
0
0
there is no m  M(  ) with SelectA (m ) = F , thus   F   is unsatisfiable, thus
  F |= .

Thus we have characterized the full explanations for a given abduction problem. Now
minimizing such an explanation F is not a problem, since the following greedy procedure,
given by Selman and Levesque (1990) reduces F into a best explanation for :
For every literal `  F do
If   F \{`} |=  then F  F \{`} endif;
Endfor;
Note that depending on the order in which the literals `  F are considered the result may
be different, but that in all cases it will be a best explanation for .
Finally, we can give our general algorithm for computing a best explanation for a given
abduction problem  = (, , A); its correctness follows directly from Proposition 1:
0  a propositional formula with M(0 ) = M()  (M(  ))|A ;
If 0 is unsatisfiable then return No explanation;
Else
m  a model of 0 ;
F  SelectA (m);
minimize F ;
return F ;
Endif;

6. Polynomial Classes
We now explore the new polynomial classes of abduction problems that our algorithm allows
to exhibit. Throughout the section, n denotes the number of variables in V ar().
6.1 Affine Formulas
A propositional formula is said to be affine (or in XOR-CNF ) (Schaefer, 1978; Kavvadias &
Sideri, 1998; Zanuttini, 2002) if it is written as a finite conjunction of linear equations over
the two-element field, e.g.,  = (x1  x3 = 1)  (x1  x2  x4 = 0). As can be seen, equations
play the same role in affine formulas as clauses do in CNFs; roughly, affine formulas represent
conjunctions of parity or equivalence constraints. This class proves interesting for knowledge
representation, since on one hand it is tractable for most of the common reasoning tasks, and
5

fiBruno Zanuttini

on the other hand the affine approximations of a knowledge base can be made very small and
are efficiently learnable (Zanuttini, 2002). We show that projecting an affine formula onto
a subset of its variables is quite easy too, enabling our algorithm to run in polynomial time.
The proof of the following lemma is easily obtained with gaussian elimination (Curtis, 1984):
triangulate  with the variables in A put rightmost, and then keep only those equations
formed upon A; full details are given in the technical report version (Zanuttini, 2003).
Lemma 1 Let  be an affine formula containing k equations, and A  V ar(). Then
an affine formula  with M() = (M())|A and containing at most k equations can be
computed in time O(k 2 |V ar()|).
Proposition 2 If  is represented by an affine formula containing k equations and  by
a disjunction of k 0 linear equations, and A is a subset of V ar(), then searching for a best
explanation for  = (, , A) can be done in time O((k + k 0 )((k + 1)2 + |A|(k + k 0 ))n).
Sketch of proof It is easily seen that an affine formula (containing k 0 + k equations and
n variables) for    can be computed in time linear in the size of ; this formula can be
projected onto A in time O((k + k 0 )2 n), and we straightforwardly get a disjunction of at
most k + k 0 linear equations for (M(  ))|A . Then we can use distributivity of  over 
for solving the satisfiability problem of the algorithm; recall that SAT can be solved in time
O(k 2 n) for an affine formula of k equations over n variables by the elimination method of
Gauss (Curtis, 1984). The remaining operations are straightforward.

Note that variables, literals and clauses are special cases of disjunctions of linear equations.
6.2 DNFs
Though the class of DNF formulas has very good computational properties, abduction
remains a hard problem for it as a whole, even with additional restrictions. Recall that the
TAUTOLOGY problem is the one of deciding whether a given DNF formula represents the
identically true function, and that this problem is coNP-complete.
Proposition 3 Deciding whether there is at least one explanation for a given abduction
problem (, , A) is NP-complete when  is given in DNF, even if  is a variable and
A  {} = V ar().
Sketch of proof Membership in NP is obvious, since deduction with DNFs is polynomial;
now it is easily seen that  is tautological if and only if the abduction problem ( 
(x), x, V ar()) has no explanation, where x is a variable not occuring in  (see the DNF
  (x) as the implication   x);   (x) is in DNF, and we get the result.

However, when  is represented by a DNF projecting it onto A is easy; indeed, the properties of projection show that it suffices to cancel its literals that are not formed upon A.
Consequently, if  is such a DNF containing k terms, then a DNF  with M() = (M())|A
and containing at most k terms can be computed in time O(k|V ar()|).
Thus we can show that some subclasses of the class of all DNFs allow polynomial
abduction. We state the first result quite generally, but note that its assumptions are
satisfied by natural classes of DNFs: e.g., that of Horn DNFs, i.e., those DNFs with at
6

fiLogic-Based Abduction

most one positive literal per term; similarly, that of Horn-renamable DNFs, i.e., those
that can be turned into a Horn DNF by replacing some variables with their negation, and
simplifying double negations, everywhere in the formula; 2DNFs, those DNFs with at most
two literals per term. We omit the proof of the following proposition, since it is essentially
the same as that of Proposition 2 (simply follow the execution of the algorithm).
Proposition 4 Let D be a class of DNFs that is stable under removal of occurrences of
literals and for which the TAUTOLOGY problem is polynomial. If  is restricted to belong
to D,  is a clause and A is a subset of V ar(), then searching for a best explanation for
 = (, , A) can be done in polynomial time.
Thus we can establish that abduction is tractable if (among others)  is in Horn-renamable
DNF (including the Horn and reverse Horn cases) or in 2DNF, and  is a clause.
Finally, let us point out that with a very similar proof we can obtain polynomiality
for some problems obtained by strengthening the restriction of Proposition 4 over , but
weakening that over .
Proposition 5 If  is represented by a Horn (resp. reverse Horn) DNF of k terms and
 by a positive (resp. negative) CNF of k 0 clauses, and A is a subset of V ar(), then
searching for a best explanation for  = (, , A) can be done in time O((k + |A|)kk 0 n).
The same holds if  is represented by a positive (resp. negative) DNF of k terms and  by
a Horn (resp. reverse Horn) CNF of k 0 clauses.
Once again note that variables, literals and terms are all special cases of (reverse) Horn
CNFs, and that variables, positive (resp. negative) clauses and positive (resp. negative)
terms are all special cases of positive (resp. negative) CNFs.

7. Discussion and Perspectives
The general algorithm presented in this note allows us to derive new polynomial restrictions
of abduction problems; even if this is not discussed here, for lack of space, it also allows to
unify some previously known such restrictions (such as  in 2CNF and  in 2DNF, or 
in monotone CNF and  given as a clause). The following list summarizes the main new
polynomial restrictions:
  given as an affine formula and  as a disjunction of linear equations (Proposition 2)
  in Horn-renamable DNF and  given as a clause (Proposition 4)
  in 2DNF and  given as a clause (Proposition 4)
  in Horn (reverse Horn) DNF and  in positive (negative) CNF (Proposition 5)
  in negative (positive) DNF and  in reverse Horn (Horn) CNF (Proposition 5).
Moreover, even if there is no guarantee for efficiency in the general case the presentation of
our algorithm does not depend on the syntactic form of  or , and it uses only standard
operations on Boolean functions (projection, conjunction, negation).
7

fiBruno Zanuttini

Another interesting feature of this algorithm is that before minimization it computes
the explanations intentionnally. Consequently, all the full explanations can be enumerated
with roughly the same delay as the models of the formula representing them (0 ). However,
of course, there is no guarantee that two of them would not be minimized into the same
best explanation, which prevents from concluding that our algorithm can enumerate all the
best explanations; trying to extend it into this direction would be an interesting problem.
For more details about enumeration we refer the reader to Eiter and Makinos work (Eiter
& Makino, 2002).
As identified by Selman and Levesque (1990), central to the task is the notion of projection over a set of variables, and our algorithm isolates this subtask. However, our notion
of projection only concerns variables, and not literals, which prevents from imposing a sign
to the literals the hypotheses are formed upon, contrariwise to more general formalizations
proposed for abduction, as Marquis (Marquis, 2000). Even if we think this is not a prohibiting restriction, it would be interesting to try to fix that weakness of our algorithm
while preserving its polynomial classes.
Another problem of interest is the behaviour of our algorithm when  and  are not
only propositional formulas, but more generally multivalued theories, in which the domain
of variables is not restricted to be {0, 1}: e.g., signed formulas (Beckert et al., 1999). This
framework is used, for instance, for configuration problems by Amilhastre et al. (2002). It
is easily seen that our algorithm is still correct in this framework; however, there is still left
to study in which cases its running time is polynomial.
Finally, problems of great interest are those of deciding the relevance or the necessity of
an abducible (Eiter & Gottlob, 1995). An abducible x is said to be relevant to an abduction
problem  if there is at least one best explanation for  containing x or x, and necessary
to  if all the best explanations for  contain x or x. It is easily seen that x is necessary
for  = (, , A) if and only if 0 = (, , A\{x}) has no explanation, hence showing that
polynomial restrictions for the search for explanations are polynomial as well for deciding
the necessity of an hypothesis as soon as they are stable under the substitution of A\{x}
for A, which is the case for all restrictions considered in this note. Contrastingly, we do
not know of any such relation for relevance, and the study of this problem would also be of
great interest.

Acknowledgments
The author wishes to thank the anonymous referees of this version and those of a previous
one (Proc. JNPC02, in French), as well as Jean-Jacques Hebrard, for very valuable and
constructive comments.

References
Amilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency restoration and explanations
in dynamic CSPs  application to configuration. Artificial Intelligence, 135 (12),
199234.
8

fiLogic-Based Abduction

Beckert, B., Hahnle, R., & Manya, F. (1999). Transformations between signed and classical clause logic. In Proc. 29th International Symposium on Multiple-Valued Logics
(ISMVL99), pp. 248255. IEEE Computer Society Press.
Bylander, T., Allemang, D., Tanner, M., & Josephson, J. (1989). Some results concerning
the computational complexity of abduction. In Proc. 1st International Conference on
Principles of Knowledge Representation and Reasoning (KR89), pp. 4454. Morgan
Kaufmann.
Coste-Marquis, S., & Marquis, P. (1998). Characterizing consistency-based diagnoses.
In Proc. 5th International Symposium on Artificial Intelligence and Mathematics
(AIMATH98).
Curtis, C. (1984). Linear algebra. An introductory approach. Springer Verlag.
del Val, A. (2000). The complexity of restricted consequence finding and abduction. In
Proc. 17th National Conference on Artificial Intelligence (AAAI00), pp. 337342.
AAAI Press/MIT Press.
Eiter, T., & Gottlob, G. (1995). The complexity of logic-based abduction. Journal of the
ACM, 42 (1), 342.
Eiter, T., & Makino, K. (2002). On computing all abductive explanations. In Proc. 18th
National Conference on Artificial Intelligence (AAAI02), pp. 6267. AAAI Press.
Eshghi, K. (1993). A tractable class of abduction problems. In Proc. 13th International
Joint Conference on Artificial Intelligence (IJCAI93), pp. 38. Morgan Kaufmann.
Goebel, R. (1997). Abduction and its relation to constrained induction. In Proc. IJCAI97
workshop on abduction and induction in AI.
Hobbs, J., Stickel, M., Appelt, D., & Martin, P. (1993). Interpretation as abduction. Artificial Intelligence, 63, 69142.
Kavvadias, D., & Sideri, M. (1998). The inverse satisfiability problem. SIAM Journal on
Computing, 28 (1), 152163.
Khardon, R., & Roth, D. (1996). Reasoning with models. Artificial Intelligence, 87, 187213.
Lang, J., Liberatore, P., & Marquis, P. (2002). Conditional independence in propositional
logic. Artificial Intelligence, 141, 79121.
Marquis, P. (2000). Consequence finding algorithms. In Handbook of Defeasible Reasoning
and Uncertainty Management Systems (DRUMS), Vol. 5, pp. 41145. Kluwer Academic.
Reiter, R., & de Kleer, J. (1987). Foundations of assumption-based truth maintenance systems: preliminary report. In Proc. 6th National Conference on Artificial Intelligence
(AAAI87), pp. 183188. AAAI Press/MIT Press.
Schaefer, T. (1978). The complexity of satisfiability problems. In Proc. 10th Annual ACM
Symposium on Theory Of Computing (STOC78), pp. 216226. ACM Press.
Selman, B., & Levesque, H. (1990). Abductive and default reasoning: a computational core.
In Proc. 8th National Conference on Artificial Intelligence (AAAI90), pp. 343348.
AAAI Press.
9

fiBruno Zanuttini

Stumptner, M., & Wotawa, F. (2001). Diagnosing tree-structured systems. Artificial Intelligence, 127, 129.
Zanuttini, B. (2002). Approximating propositional knowledge with affine formulas. In
Proc. 15th European Conference on Artificial Intelligence (ECAI02), pp. 287291.
IOS Press.
Zanuttini, B. (2003). New polynomial classes for logic-based abduction. Tech. rep., Universite de Caen, France.

10

fiJournal of Artificial Intelligence Research 19 (2003) 569-629

Submitted 05/01; published 12/03

Accelerating Reinforcement Learning
through Implicit Imitation
Bob Price

price@cs.ubc.ca

Department of Computer Science
University of British Columbia
Vancouver, B.C., Canada V6T 1Z4

Craig Boutilier

cebly@cs.toronto.edu

Department of Computer Science
University of Toronto
Toronto, ON, Canada M5S 3H5

Abstract
Imitation can be viewed as a means of enhancing learning in multiagent environments.
It augments an agents ability to learn useful behaviors by making intelligent use of the
knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can
accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a
mentor, a reinforcement-learning agent can extract information about its own capabilities
in, and the relative value of, unvisited parts of the state space. We study two specific
instantiations of this model, one in which the learning agent and the mentor have identical
abilities, and one designed to deal with agents and mentors with different action sets. We
illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and
demonstrating improved performance and convergence through observation of single and
multiple mentors. Though we make some stringent assumptions regarding observability
and possible interactions, we briefly comment on extensions of the model that relax these
restricitions.

1. Introduction
The application of reinforcement learning to multiagent systems offers unique opportunities
and challenges. When agents are viewed as independently trying to achieve their own ends,
interesting issues in the interaction of agent policies (Littman, 1994) must be resolved (e.g.,
by appeal to equilibrium concepts). However, the fact that agents may share information
for mutual gain (Tan, 1993) or distribute their search for optimal policies and communicate reinforcement signals to one another (Mataric, 1998) offers intriguing possibilities for
accelerating reinforcement learning and enhancing agent performance.
Another way in which individual agent performance can be improved is by having a
novice agent learn reasonable behavior from an expert mentor. This type of learning can
be brought about through explicit teaching or demonstration (Atkeson & Schaal, 1997;
Lin, 1992; Whitehead, 1991a), by sharing of privileged information (Mataric, 1998), or
through an explicit cognitive representation of imitation (Bakker & Kuniyoshi, 1996). In
imitation, the agents own exploration is used to ground its observations of other agents
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPrice & Boutilier

behaviors in its own capabilities and resolve any ambiguities in observations arising from
partial observability and noise. A common thread in all of this work is the use of a mentor
to guide the exploration of the observer. Typically, guidance is achieved through some form
of explicit communication between mentor and observer. A less direct form of teaching
involves an observer extracting information from a mentor without the mentor making an
explicit attempt to demonstrate a specific behavior of interest (Mitchell, Mahadevan, &
Steinberg, 1985).
In this paper we develop an imitation model we call implicit imitation that allows an
agent to accelerate the reinforcement learning process through the observation of an expert
mentor (or mentors). The agent observes the state transitions induced by the mentors
actions and uses the information gleaned from these observations to update the estimated
value of its own states and actions. We will distinguish two settings in which implicit
imitation can occur: homogeneous settings, in which the learning agent and the mentor
have identical actions; and heterogeneous settings, where their capabilities may differ. In
the homogeneous setting, the learner can use the observed mentor transitions directly to
update its own estimated model of its actions, or to update its value function. In addition,
a mentor can provide hints to the observer about the parts of the state space on which it
may be worth focusing attention. The observers attention to an area might take the form of
additional exploration of the area or additional computation brought to bear on the agents
prior beliefs about the area. In the heterogeneous setting, similar benefits accrue, but with
the potential for an agent to be misled by a mentor that possesses abilities different from
its own. In this case, the learner needs some mechanism to detect such situations and to
make efforts to temper the influence of these observations.
We derive several new techniques to support implicit imitation that are largely independent of any specific reinforcement learning algorithm, though they are best suited for use
with model-based methods. These include model extraction, augmented backups, feasibility
testing, and k-step repair. We first describe implicit imitation in homogeneous domains,
then we describe the extension to heterogeneous settings. We illustrate its effectiveness
empirically by incorporating it into Moore and Atkesons (1993) prioritized sweeping algorithm.
The implicit imitation model has several advantages over more direct forms of imitation
and teaching. It does not require any agent to explicitly play the role of mentor or teacher.
Observers learn simply by watching the behavior of other agents; if an observed mentor
shares certain subtasks with the observer, the observed behavior can be incorporated (indirectly) by the observer to improve its estimate of its own value function. This is important
because there are many situations in which an observer can learn from a mentor that is
unwilling or unable to alter its behavior to accommodate the observer, or even communicate
information to it. For example, common communication protocols may be unavailable to
agents designed by different developers (e.g., Internet agents); agents may find themselves
in a competitive situation in which there is disincentive to share information or skills; or
there may simply be no incentive for one agent to provide information to another.1
Another key advantage of our approachwhich arises from formalizing imitation in the
reinforcement learning contextis the fact that the observer is not constrained to directly
1. For reasons of consistency, we will use the term mentor to describe any agent from which an observer
can learn, even if the mentor is an unwilling or unwitting participant.

570

fiImplicit Imitation

imitate (i.e., duplicate the actions of) the mentor. The learner can decide whether such
explicit imitation is worthwhile. Implicit imitation can thus be seen as blending the
advantages of explicit teaching or explicit knowledge transfer with those of independent
learning. In addition, because an agent learns by observation, it can exploit the existence
of multiple mentors, essentially distributing its search. Finally, we do not assume that
the observer knows the actual actions taken by the mentor, or that the mentor shares a
reward function (or goals) with the mentor. Again, this stands in sharp contrast with many
existing models of teaching, imitation, and behavior learning by observation. While we make
some strict assumptions in this paper with respect to observability, complete knowledge of
reward functions, and the existence of mappings between agent state spaces, the model can
be generalized in interesting ways. We will elaborate on some of these generalizations near
the end of the paper.
The remainder of the paper is structured as follows. We provide the necessary background on Markov decision processes and reinforcement learning for the development of our
implicit imitation model in Section 2. In Section 3, we describe a general formal framework
for the study of implicit imitation in reinforcement learning. Two specific instantiations
of this framework are then developed. In Section 4, a model for homogeneous agents is
developed. The model extraction technique is explained and the augmented Bellman backup
is proposed as a mechanism for incorporating observations into model-based reinforcement
learning algorithms. Model confidence testing is then introduced to ensure that misleading
information does not have undue influence on a learners exploration policy. The use of
mentor observations to to focus attention on interesting parts of the state space is also
introduced. Section 5 develops a model for heterogeneous agents. The model extends
the homogeneous model through feasibility testing, a device by which a learner can detect
whether the mentors abilities are similar to its own, and k-step repair, whereby a learner
can attempt to mimic the trajectory of a mentor that cannot be duplicated exactly. Both
of these techniques prove crucial in heterogeneous settings. The effectiveness of these models
is demonstrated on a number of carefully chosen navigation problems. Section 6 examines
conditions under which implicit imitation will and will not work well. Section 7 describes
several promising extensions to the model. Section 8 examines the implicit imitation model
in the context of related work and Section 9 considers future work before drawing some
general conclusions about implicit imitation and the field of computational imitation more
broadly.

2. Reinforcement Learning
Our aim is to provide a formal model of implicit imitation, whereby an agent can learn
how to act optimally by combining its own experience with its observations of the behavior
of an expert mentor. Before doing so, we describe in this section the standard model of
reinforcement learning used in artificial intelligence. Our model will build on this singleagent view of learning how to act. We begin by reviewing Markov decision processes, which
provide a model for sequential decision making under uncertainty, and then move on to
describe reinforcement learning, with an emphasis on model-based methods.
571

fiPrice & Boutilier

2.1 Markov Decision Processes
Markov decision processes (MDPs) have proven very useful in modeling stochastic sequential decision problems, and have been widely used in decision-theoretic planning to model
domains in which an agents actions have uncertain effects, an agents knowledge of the environment is uncertain, and the agent can have multiple, possibly conflicting objectives. In
this section, we describe the basic MDP model and consider one classical solution procedure.
We do not consider action costs in our formulation of MDPs, though these pose no special
complications. Finally, we make the assumption of full observability. Partially observable
MDPs (POMDPs) (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Smallwood &
Sondik, 1973) are much more computationally demanding than fully observable MDPs. Our
imitation model will be based on a fully observable model, though some of the generalizations of our model mentioned in the concluding section build on POMDPs. We refer the
reader to Bertsekas (1987); Boutilier, Dean and Hanks (1999); and Puterman (1994) for
further material on MDPs.
An MDP can be viewed as a stochastic automaton in which actions induce transitions
between states, and rewards are obtained depending on the states visited by an agent.
Formally, an MDP can be defined as a tuple hS, A, T, Ri, where S is a finite set of states or
possible worlds, A is a finite set of actions, T is a state transition function, and R is a reward
function. The agent can control the state of the system to some extent by performing actions
a  A that cause state transitions, movement from the current state to some new state.
Actions are stochastic in that the actual transition caused cannot generally be predicted
with certainty. The transition function T : S  A  (S) describes the effects of each
action at each state. T (si , a) is a probability distribution over S; specifically, T (si , a)(sj )
is the probability of ending up in state sj  S when action a is performed at state si .
We will denote this quantity by Pr(si , a, sj ). We require that 0  Pr(si , a, sj )  1 for all
P
si , sj , and that for all si , sj S Pr(si , a, sj ) = 1. The components S, A and T determine
the dynamics of the system being controlled. The assumption that the system is fully
observable means that the agent knows the true state at each time t (once that stage is
reached), and its decisions can be based solely on this knowledge. Thus, uncertainty lies
only in the prediction of an actions effects, not in determining its actual effect after its
execution.
A (deterministic, stationary, Markovian) policy  : S  A describes a course of action
to be adopted by an agent controlling the system. An agent adopting such a policy performs
action (s) whenever it finds itself in state s. Policies of this form are Markovian since the
action choice at any state does not depend on the system history, and are stationary since
action choice does not depend on the stage of the decision problem. For the problems we
consider, optimal stationary Markovian policies always exist.
We assume a bounded, real-valued reward function R : S  <. R(s) is the instantaneous reward an agent receives for occupying state s. A number of optimality criteria
can be adopted to measure the value of a policy , all measuring in some way the reward
accumulated by an agent as it traverses the state space through the execution of . In this
work, we focus on discounted infinite-horizon problems: the current value of a reward received t stages in the future is discounted by some factor  t (0   < 1). This allows simpler
572

fiImplicit Imitation

computational methods to be used, as discounted total reward will be finite. Discounting
can be justified on other (e.g., economic) grounds in many situations as well.
The value function V : S  < reflects the value of a policy  at any state s; this is
simply the expected sum of discounted future rewards obtained by executing  beginning
at s. A policy   is optimal if, for all s  S and all policies , we have V (s)  V (s).
We are guaranteed that such optimal (stationary) policies exist in our setting (Puterman,
1994). The (optimal) value of a state V  (s) is its value V (s) under any optimal policy   .
By solving an MDP, we refer to the problem of constructing an optimal policy. Value
iteration (Bellman, 1957) is a simple iterative approximation algorithm for optimal policy
construction. Given some arbitrary estimate V 0 of the true value function V  , we iteratively
improve this estimate as follows:
V n (si ) = R(si ) + max{
aA

X

Pr(si , a, sj )V n1 (sj )}

(1)

sj S

The computation of V n (s) given V n1 is known as a Bellman backup. The sequence of value
functions V n produced by value iteration converges linearly to V  . Each iteration of value
iteration requires O(|S|2 |A|) computation time, and the number of iterations is polynomial
in |S|.
For some finite n, the actions a that maximize the right-hand side of Equation 1 form an
optimal policy, and V n approximates its value. Various termination criteria can be applied;
for example, one might terminate the algorithm when
kV i+1  V i k 

(1  )
2

(2)

(where kXk = max{|x| : x  X} denotes the supremum norm). This ensures the resulting
value function V i+1 is within 2 of the optimal function V  at any state, and that the induced
policy is -optimal (i.e., its value is within  of V  ) (Puterman, 1994).
A concept that will be useful later is that of a Q-function. Given an arbitrary value
function V , we define QVa (si ) as
QVa (si ) = R(si ) + 

X

Pr(si , a, sj )V (sj )

(3)

sj S

Intuitively, QVa (s) denotes the value of performing action a at state s and then acting in a
manner that has value V (Watkins & Dayan, 1992). In particular, we define Qa to be the
Q-function defined with respect to V  , and Qna to be the Q-function defined with respect
to V n1 . In this manner, we can rewrite Equation 1 as:
V n (s) = max{Qna (s)}
aA

(4)

We define an ergodic MDP as an MDP in which every state is reachable from any other
state in a finite number of steps with non-zero probability.
573

fiPrice & Boutilier

2.2 Model-based Reinforcement Learning
One difficulty with the use of MDPs is that the construction of an optimal policy requires
that the agent know the exact transition probabilities Pr and reward model R. In the specification of a decision problem, these requirements, especially the detailed specification of the
domains dynamics, can impose an undue burden on the agents designer. Reinforcement
learning can be viewed as solving an MDP in which the full details of the model, in particular Pr and R, are not known to the agent. Instead, the agent learns how to act optimally
through experience with its environment. We provide a brief overview of reinforcement
learning in this section (with an emphasis on model-based approaches). For further details,
please refer to the texts of Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996),
and the survey of Kaelbling, Littman and Moore (1996).
In the general model, we assume that an agent is controlling an MDP hS, A, T, Ri and
initially knows its state and action spaces, S and A, but not the transition model T or
reward function R. The agent acts in its environment, and at each stage of the process
makes a transition hs, a, r, ti; that is, it takes action a at state s, receives reward r and
moves to state t. Based on repeated experiences of this type it can determine an optimal
policy in one of two ways: (a) in model-based reinforcement learning, these experiences can
be used to learn the true nature of T and R, and the MDP can be solved using standard
methods (e.g., value iteration); or (b) in model-free reinforcement learning, these experiences
can be used to directly update an estimate of the optimal value function or Q-function.
Probably the simplest model-based reinforcement learning scheme is the certainty equivalence approach. Intuitively, a learning agent is assumed to have some current estimated
c
transition model Tb of its environment consisting of estimated probabilities Pr(s,
a, t) and
b
an estimated rewards model R(s). With each experience hs, a, r, ti the agent updates its esc to obtain an policy 
b that would be optimal
timated models, solves the estimated MDP M
if its estimated models were correct, and acts according to that policy.
To make the certainty equivalence approach precise, a specific form of estimated model
and update procedure must be adopted. A common approach is to used the empirical distribution of observed state transitions and rewards as the estimated model. For instance,
if action a has been attempted C(s, a) times at state s, and on C(s, a, t) of those occasions
c
state t has been reached, then the estimate Pr(s,
a, t) = C(s, a, t)/C(s, a). If C(s, a) = 0,
some prior estimate is used (e.g., one might assume all state transitions are equiprobable). A
Bayesian approach (Dearden, Friedman, & Andre, 1999) uses an explicit prior distribution
over the parameters of the transition distribution Pr(s, a, ), and then updates these with
each experienced transition. For instance, we might assume a Dirichlet (Generalized Beta)
distribution (DeGroot, 1975) with parameters n(s, a, t) associated with each possible successor state t. The Dirichlet parameters are equal to the experience-based counts C(s, a, t)
plus a prior count P (s, a, t) representing the agents prior beliefs about the distribution
(i.e., n(s, a, t) = C(s, a, t) + P (s, a, t)). The expected transition probability Pr(s, a, t) is then
P
c can be solved
n(s, a, t)/ t0 n(s, a, t0 ). Assuming parameter independence, the MDP M
using these expected values. Furthermore, the model can be updated with ease, simply
increasing n(s, a, t) by one with each observation hs, a, r, ti. This model has the advantage
over a counter-based approach of allowing a flexible prior model and generally does not
574

fiImplicit Imitation

assign probability zero to unobserved transitions. We will adopt this Bayesian perspective
in our imitation model.
One difficulty with the certainty equivalence approach is the computational burden of rec with each update of the models Tb and R
b (i.e., with each experience). One
solving an MDP M
could circumvent this to some extent by batching experiences and updating (and re-solving)
the model only periodically. Alternatively, one could use computational effort judiciously to
apply Bellman backups only at those states whose values (or Q-values) are likely to change
the most given a change in the model. Moore and Atkesons (1993) prioritized sweeping
c
algorithm does just this. When Tb is updated by changing Pr(s,
a, t), a Bellman backup is
b
b a). Suppose the
applied at s to update its estimated value V , as well as the Q-value Q(s,
b
b
magnitude of the change in V (s) is given by V (s). For any predecessor w, the Q-values
b
c
Q(w,
a0 )hence values Vb (w)can change if Pr(w,
a0 , s) > 0. The magnitude of the change
c
is bounded by Pr(w,
a0 , s)Vb (s). All such predecessors w of s are placed in a priority
0
c
queue with Pr(w, a , s)Vb (s) serving as the priority. A fixed number of Bellman backups
are applied to states in the order in which they appear in the queue. With each backup,
any change in value can cause new predecessors to be inserted into the queue. In this way,
computational effort is focused on those states where a Bellman backup has the greatest
impact due to the model change. Furthermore, the backups are applied only to a subset
of states, and are generally only applied a fixed number of times. By way of contrast, in
the certainty equivalence approach, backups are applied until convergence. Thus prioritized
sweeping can be viewed as a specific form of asynchronous value iteration, and has appealing
computational properties (Moore & Atkeson, 1993).
Under certainty equivalence, the agent acts as if the current approximation of the model
is correct, even though the model is likely to be inaccurate early in the learning process. If
the optimal policy for this inaccurate model prevents the agent from exploring the transitions which form part of the optimal policy for the true model, then the agent will fail to
find the optimal policy. For this reason, explicit exploration policies are invariably used to
ensure that each action is tried at each state sufficiently often. By acting randomly (assuming an ergodic MDP), an agent is assured of sampling each action at each state infinitely
often in the limit. Unfortunately, the actions of such an agent will fail to exploit (in fact,
will be completely uninfluenced by) its knowledge of the optimal policy. This explorationexploitation tradeoff refers to the tension between trying new actions in order to find out
more about the environment and executing actions believed to be optimal on the basis of
the current estimated model.
The most common method for exploration is the greedy method in which the agent
chooses a random action a fraction  of the time, where 0 <  < 1. Typically,  is decayed
over time to increase the agents exploitation of its knowledge. In the Boltzmann approach,
each action is selected with a probability proportional to its value:
Prs (a) = P

eQ(s,a)/
e
a0 A

Q(s,a0 )/

(5)

The proportionality can be adjusted nonlinearly with the temperature parameter  . As
  0 the probability of selecting the action with the highest value tends to 1. Typically,
 is started high so that actions are randomly explored during the early stages of learning.
As the agent gains knowledge about the effects of its actions and the value of these effects,
575

fiPrice & Boutilier

the parameter  is decayed so that the agent spends more time exploiting actions known to
be valuable and less time randomly exploring actions.
More sophisticated methods attempt to use information about model confidence and
value magnitudes to plan a utility-maximizing exploration plan. An early approximation
of this scheme can be found in the interval estimation method (Kaelbling, 1993). Bayesian
methods have also been used to calculate the expected value of information to be gained
from exploration (Meuleau & Bourgine, 1999; Dearden et al., 1999).
We concentrate in this paper on model-based approaches to reinforcement learning.
However, we should point out that model-free methodsthose in which an estimate of the
optimal value function or Q-function is learned directly, without recourse to a domain
modelhave attracted much attention. For example, TD-methods (Sutton, 1988) and
Q-learning (Watkins & Dayan, 1992) have both proven to be among the more popular
methods for reinforcement learning. Our methods can be modified to deal with model-free
approaches, as we discuss in the concluding section. We also focus on so-called tablebased (or explicit) representations of models and value functions. When state and action
spaces are large, table-based approaches become unwieldy, and the associated algorithms
are generally intractable. In these situations, approximators are often used to estimate the
values of states. We will discuss ways in which our techniques can be extended to allow for
function approximation in the concluding section.

3. A Formal Framework for Implicit Imitation
To model the influence that a mentor agent can have on the decision process or the learning
behavior of an observer, we must extend the single-agent decision model of MDPs to account
for the actions and objectives of multiple agents. In this section, we introduce a formal
framework for studying implicit imitation. We begin by introducing a general model for
stochastic games (Shapley, 1953; Myerson, 1991), and then impose various assumptions
and restrictions on this general model that allow us to focus on the key aspects of implicit
imitation. We note that the framework proposed here is useful for the study of other forms
of knowledge transfer in multiagent systems, and we briefly point out various extensions of
the framework that would permit implicit imitation, and other forms of knowledge transfer,
in more general settings.
3.1 Non-Interacting Stochastic Games
Stochastic games can be viewed as a multiagent extension of Markov decision processes.
Though Shapleys (1953) original formulation of stochastic games involved a zero-sum (fully
competitive) assumption, various generalizations of the model have been proposed allowing
for arbitrary relationships between agents utility functions (Myerson, 1991).2 Formally,
an n-agent stochastic game hS, {Ai : i  n}, T, {Ri : i  n}i comprises a set of n agents
(1  i  n), a set of states S, a set of actions Ai for each agent i, a state transition function
T , and a reward function Ri for each agent i. Unlike an MDP, individual agent actions do
not determine state transitions; rather it is the joint action taken by the collection of agents
that determines how the system evolves at any point in time. Let A = A1      An be
2. For example, see the fully cooperative multiagent MDP model proposed by Boutilier (1999).

576

fiImplicit Imitation

the set of joint actions; then T : S  A  (S), with T (si , a)(sj ) = Pr(si , a, sj ) denoting
the probability of ending up in state sj  S when joint action a is performed at state si .
For convenience, we introduce the notation Ai to denote the set of joint actions A1 
    Ai1  Ai+1      An involving all agents except i. We use ai  ai to denote the
(full) joint action obtained by conjoining ai  Ai with ai  Ai .
Because the interests of the individual agents may be at odds, strategic reasoning and
notions of equilibrium are generally involved in the solution of stochastic games. Because
our aim is to study how a reinforcement agent might learn by observing the behavior of an
expert mentor, we wish to restrict the model in such a way that strategic interactions need
not be considered: we want to focus on settings in which the actions of the observer and
the mentor do not interact. Furthermore, we want to assume that the reward functions of
the agents do not conflict in a way that requires strategic reasoning.
We define noninteracting stochastic games by appealing to the notion of an agent projection function which is used to extract an agents local state from the underlying game. In
these games, an agents local state determines all aspects of the global state that are relevant
to its decision making process, while the projection function determines which global states
are identical from an agents local perspective. Formally, for each agent i, we assume a local
state space Si , and a projection function Li : S  Si . For any s, t  S, we write s i t
iff Li (s) = Li (t). This equivalence relation partitions S into a set of equivalence classes
such that the elements within a specific class (i.e., L1
i (s) for some s  Si ) need not be
distinguished by agent i for the purposes of individual decision making. We say a stochastic
game is noninteracting if there exists a local state space Si and projection function Li for
each agent i such that:
1. If s i t, then ai  Ai , ai  Ai , wi  Si we have
X

{Pr(s, ai  ai , w) : w  L1
i (wi )} =

X

{Pr(t, ai  ai , w) : w  L1
i (wi )}

2. Ri (s) = Ri (t) if s i t
Intuitively, condition 1 above imposes two distinct requirements on the game from the
perspective of agent i. First, if we ignore the existence of other agents, it provides a notion
of state space abstraction suitable for agent i. Specifically, Li clusters together states
s  S only if each state in an equivalence class has identical dynamics with respect to
the abstraction induced by Li . This type of abstraction is a form of bisimulation of the
type studied in automaton minimization (Hartmanis & Stearns, 1966; Lee & Yannakakis,
1992) and automatic abstraction methods developed for MDPs (Dearden & Boutilier, 1997;
Dean & Givan, 1997). It is not hard to showignoring the presence of other agentsthat
the underlying system is Markovian with respect to the abstraction (or equivalently, w.r.t.
Si ) if condition 1 is met. The quantification over all ai imposes a strong noninteraction
requirement, namely, that the dynamics of the game from the perspective of agent i is
independent of the strategies of the other agents. Condition 2 simply requires that all
states within a given equivalence class for agent i have the same reward for agent i. This
means that no states within a class need to be distinguishedeach local state can be viewed
as atomic.
577

fiPrice & Boutilier

A noninteracting game induces an MDP Mi for each agent i where Mi = hSi , Ai , Pri , Ri i
where Pri is given by condition (1) above. Specifically, for each si , ti  Si :
Pri (si , ai , ti ) =

P

{Pr(s, ai .ai , t) : t  L1
i (ti )}

where s is any state in L1
i (si ) and ai is any element of Ai . Let i : Sa  Ai be an
optimal policy for Mi . We can extend this to a strategy iG : S  Ai for the underlying
stochastic game by simply applying i (si ) to every state s  S such that Li (s) = si . The
following proposition shows that the term noninteracting indeed provides an appropriate
description of such a game.
Proposition 1 Let G be a noninteracting stochastic game, Mi the induced MDP for agent
i, and i some optimal policy for Mi . The strategy iG extending i to G is dominant for
agent i.
Thus each agent can solve the noninteracting game by abstracting away irrelevant aspects of the state space, ignoring other agent actions, and solving its personal MDP
Mi .
Given an arbitrary stochastic game, it can generally be quite difficult to discover whether
it is noninteracting, requiring the construction of appropriate projection functions. In what
follows, we will simply assume that the underlying multiagent system is a noninteracting
game. Rather than specifying the game and projection functions, we will specify the individual MDPs Mi themselves. The noninteracting game induced by the set of individual
MDPs is simply the cross product of the individual MDPs. Such a view is often quite
natural. Consider the example of three robots moving in some two-dimensional office domain. If we are able to neglect the possibility of interactionfor example, if the robots can
occupy the same 2-D position (at a suitable level of granularity) and do not require the
same resources to achieve their tasksthen we might specify an individual MDP for each
robot. The local state might be determined by the robots x, y-position, orientation, and
the status of its own tasks. The global state space would be the cross product S1  S2  S3
of the local spaces. The individual components of any joint action would affect only the
local state, and each agent would care (through its reward function Ri ) only about its local
state.
We note that the projection function Li should not be viewed as equivalent to an observation function. We do not assume that agent i can only distinguish elements of Si in
fact, observations of other agents states will be crucial for imitation. Rather the existence
of Li simply means that, from the point of view of decision making with a known model,
the agent need not worry about distinctions other than those made by Li . Assuming no
computational limitations, an agent i need only solve Mi , but may use observations of other
agents in order to improve its knowledge about Mi s dynamics.3
3.2 Implicit Imitation
Despite the very independent nature of the agent subprocesses in a noninteracting multiagent system, there are circumstances in which the behavior of one agent may be relevant to
3. We elaborate on the condition of computational limitations below.

578

fiImplicit Imitation

another. To keep the discussion simple, we assume the existence of an expert mentor agent
m, which is implementing some stationary (and presumably optimal) policy m over its
local MDP Mm = hSm , Am , Prm , Rm i. We also assume a second agent o, the observer, with
local MDP Mo = hSo , Ao , Pro , Ro i. While nothing about the mentors behavior is relevant
to the observer if it knows its own MDP (and can solve it without computational difficulty),
the situation can be quite different if o is a reinforcement learner without complete knowledge of the model Mo . It may well be that the observed behavior of the mentor provides
valuable information to the observer in its quest to learn how to act optimally within Mo .
To take an extreme case, if mentors MDP is identical to the observers, and the mentor
is an expert (in the sense of acting optimally), then the behavior of the mentor indicates
exactly what the observer should do. Even if the mentor is not acting optimally, or if the
mentor and observer have different reward functions, mentor state transitions observed by
the learner can provide valuable information about the dynamics of the domain.
Thus we see that when one agent is learning how to act, the behavior of another can
potentially be relevant to the learner, even if the underlying multiagent system is noninteracting. Similar remarks, of course, apply to the case where the observer knows the MDP
Mo , but computational restrictions make solving this difficultobserved mentor transitions
might provide valuable information about where to focus computational effort.4 The main
motivation underlying our model of implicit imitation is that the behavior of an expert
mentor can provide hints as to appropriate courses of action for a reinforcement learning
agent.
Intuitively, implicit imitation is a mechanism by which a learning agent attempts to
incorporate the observed experience of an expert mentor agent into its learning process.
Like more classical forms of learning by imitation, the learner considers the effects of the
mentors action (or action sequence) in its own context. Unlike direct imitation, however,
we do not assume that the learner must physically attempt to duplicate the mentors
behavior, nor do we assume that the mentors behavior is necessarily appropriate for the
observer. Instead, the influence of the mentor is on the agents transition model and its
estimate of value of various states and actions. We elaborate on these points below.
In what follows, we assume a mentor m and associated MDP Mm , and a learner or
observer o and associated MDP Mo , as described above. These MDPs are fully observable.
We focus on the reinforcement learning problem faced by agent o. The extension to multiple
mentors is straightforward and will be discussed below, but for clarity we assume only one
mentor in our description of the abstract framework. It is clear that certain conditions must
be met for the observer to extract useful information from the mentor. We list a number
of assumptions that we make at different points in the development of our model.
Observability: We must assume that the learner can observe certain aspects of the mentors behavior. In this work, we assume that state of the mentors MDP is fully
observable to the learner. Equivalently, we interpret this as full observability of the
underlying noninteracting game, together with knowledge of the mentors projection
4. For instance, algorithms like asynchronous dynamic programming and prioritized sweeping can benefit
from such guidance. Indeed, the distinction between reinforcement learning and solving MDPs is viewed
by some as rather blurry (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). Our focus is on the
case of an unknown model (i.e., the classical reinforcement learning problem) as opposed to one where
computational issues are key.

579

fiPrice & Boutilier

function Lm . A more general partially observable model would require the specification of an observation or signal set Z and an observation function O : So Sm  (Z),
where O(so , sm )(z) denotes the probability with which the observer obtains signal z
when the local states of the observer and mentor are so and sm , respectively. We do
not pursue such a model here. It is important to note that we do not assume that the
observer has access to the action taken by m at any point in time. Since actions are
stochastic, the state (even if fully observable) that results from the mentor invoking a
specific control signal is generally insufficient to determine that signal. Thus it seems
much more reasonable to assume that states (and transitions) are observable than the
actions that gave rise to them.
Analogy: If the observer and the mentor are acting in different local state spaces, it is clear
that observations made of the mentors state transitions can offer no useful information
to the observer unless there is some relationship between the two state spaces. There
are several ways in which this relationship can be specified. Dautenhahn and Nehaniv
(1998) use a homomorphism to define the relationship between mentor and observer
for a specific family of trajectories (see Section 8 for further discussion).
A slightly different notion might involve the use of some analogical mapping h : Sm 
So such that an observed state transition s  t provides some information to the
observer about the dynamics or value of state h(s)  So . In certain circumstances, we
might require the mapping h to be homomorphic with respect to Pr(, a, ) (for some,
or all, a), and perhaps even with respect to R. We discuss these issues in further detail
below. In order to simplify our model and avoid undue attention to the (admittedly
important) topic of constructing suitable analogical mappings, we will simply assume
that the mentor and the observer have identical state spaces; that is, Sm and So are
in some sense isomorphic. The precise sense in which the spaces are isomorphicor
in some cases, presumed to be isomorphic until proven otherwiseis elaborated below
when we discuss the relationship between agent abilities. Thus from this point we
simply refer to the state S without distinguishing the mentors local space Sm from
the observers So .
Abilities: Even with a mapping between states, observations of a mentors state transitions
only tell the observer something about the mentors abilities, not its own. We must
assume that the observer can in some way duplicate the actions taken by the mentor
to induce analogous transitions in its own local state space. In other words, there must
be some presumption that the mentor and the observer have similar abilities. It is
in this sense that the analogical mapping between state spaces can be taken to be
a homomorphism. Specifically, we might assume that the mentor and the observer
have the same actions available to them (i.e., Am = Ao = A) and that h : Sm  So
is homomorphic with respect to Pr(, a, ) for all a  A. This requirement can be
weakened substantially, without diminishing its utility, by requiring only that the
observer be able to implement the actions actually taken by the mentor at a given
state s. Finally, we might have an observer that assumes that it can duplicate the
actions taken by the mentor until it finds evidence to the contrary. In this case,
there is a presumed homomorphism between the state spaces. In what follows, we
will distinguish between implicit imitation in homogeneous action settingsdomains
580

fiImplicit Imitation

in which the analogical mapping is indeed homomorphicand heterogeneous action
settingswhere the mapping may not be a homomorphism.
There are more general ways of defining similarity of ability, for example, by assuming
that the observer may be able to move through state space in a similar fashion to the
mentor without following the same trajectories (Nehaniv & Dautenhahn, 1998). For
instance, the mentor may have a way of moving directly between key locations in state
space, while the observer may be able to move between analogous locations in a less
direct fashion. In such a case, the analogy between states may not be determined by
single actions, but rather by sequences of actions or local policies. We will suggest
ways for dealing with restricted forms of analogy of this type in Section 5.
Objectives: Even when the observer and mentor have similar or identical abilities, the
value to the observer of the information gleaned from the mentor may depend on
the actual policy being implemented by the mentor. We might suppose that the
more closely related a mentors policy is to the optimal policy of the observer, the
more useful the information will be. Thus, to some extent, we expect that the more
closely aligned the objectives of the mentor and the observer are, the more valuable
the guidance provided by the mentor. Unlike in existing teaching models, we do
not suppose that the mentor is making any explicit efforts to instruct the observer.
And because their objectives may not be identical, we do not force the observer to
(attempt to) explicitly imitate the behavior of the mentor. In general, we will make
no explicit assumptions about the relationship between the objectives of the mentor
and the observer. However, we will see that, to some extent, the closer they are,
the more utility can be derived from implicit imitation.
Finally, we remark on an important assumption we make throughout the remainder
of this paper: the observer knows its reward function Ro ; that is, for each state s, the
observer can evaluate Ro (s) without having visited state s. This view is consistent
with view of reinforcement learning as automatic programming. A user may easily
specify a reward function (e.g., in the form of a set of predicates that can be evaluated
at any state) prior to learning. It may be more difficult to specify a domain model
or optimal policy. In such a setting, the only unknown component of the MDP Mo is
the transition function Pro . We believe this approach to reinforcement learning is, in
fact, more common in practice than the approach in which the reward function must
be sampled.
To reiterate, our aim is to describe a mechanism by which the observer can accelerate
its learning; but we emphasize our position that implicit imitationin contrast to explicit
imitationis not merely replicating the behaviors (or state trajectories) observed in another
agent, nor even attempting to reach similar states. We believe the agent must learn
about its own capabilities and adapt the information contained in observed behavior to
these. Agents must also explore the appropriate application (if any) of observed behaviors,
integrating these with their own, as appropriate, to achieve their own ends. We therefore
see imitation as an interactive process in which the behavior of one agent is used to guide
the learning of another.
581

fiPrice & Boutilier

Given this setting, we can list possible ways in which an observer and a mentor can (and
cannot) interact, contrasting along the way our perspective and assumptions with those of
existing models in the literature.5 First, the observer could attempt to directly infer a
policy from its observations of mentor state-action pairs. This model has a conceptual
simplicity and intuitive appeal, and forms the basis of the behavioral cloning paradigm
(Sammut, Hurst, Kedzier, & Michie, 1992; Urbancic & Bratko, 1994). However, it assumes
that the observer and mentor share the same reward function and action capabilities. It
also assumes that complete and unambiguous trajectories (including action choices) can be
observed. A related approach attempts to deduce constraints on the value function from
the inferred action preferences of the mentor agent (Utgoff & Clouse, 1991; Suc & Bratko,
1997). Again, however, this approach assumes congruity of objectives. Our model is also
distinct from models of explicit teaching (Lin, 1992; Whitehead, 1991b): we do not assume
that the mentor has any incentive to move through its environment in a way that explicitly
guides the learner to explore its own environment and action space more effectively.
Instead of trying to directly learn a policy, an observer could attempt to use observed
state transitions of other agents to improve its own environment model Pro (s, a, t). With
a more accurate model and its own reward function, the observer could calculate more
accurate values for states. The state values could then be used to guide the agent towards
distant rewards and reduce the need for random exploration. This insight forms the core
of our implicit imitation model. This approach has not been developed in the literature,
and is appropriate under the conditions listed above, specifically, under conditions where
the mentors actions are unobservable, and the mentor and observer have different reward
functions or objectives. Thus, this approach is applicable under more general conditions
than many existing models of imitation learning and teaching.
In addition to model information, mentors may also communicate information about the
relevance or irrelevance of regions of the state space for certain classes of reward functions.
An observer can use the set of states visited by the mentor as heuristic guidance about
where to perform backup computations in the state space.
In the next two sections, we develop specific algorithms from our insights about how
agents can use observations of others to both improve their own models and assess the
relevance of regions within their state spaces. We first focus on the homogeneous action
case, then extend the model to deal with heterogeneous actions.

4. Implicit Imitation in Homogeneous Settings
We begin by describing implicit imitation in homogeneous action settingsthe extension
to heterogeneous settings will build on the insights developed in this section. We develop
a technique called implicit imitation through which observations of a mentor can be used
to accelerate reinforcement learning. First, we define the homogeneous setting. Then we
develop the implicit imitation algorithm. Finally, we demonstrate how implicit imitation
works on a number of simple problems designed to illustrate the role of the various mechanisms we describe.
5. We will describe other models in more detail in Section 8.

582

fiImplicit Imitation

4.1 Homogeneous Actions
The homogeneous action setting is defined as follows. We assume a single mentor m and
observer o, with individual MDPs Mm = hS, Am , Prm , Rm i and Mo = hS, Ao , Pro , Ro i,
respectively. Note that the agents share the same state space (more precisely, we assume
a trivial isomorphic mapping that allows us to identify their local states). We also assume
that the mentor is executing some stationary policy m . We will often treat this policy as
deterministic, but most of our remarks apply to stochastic policies as well. Let the support
set Supp(m , s) for m at state s be the set of actions a  Am accorded nonzero probability
by m at state s. We assume that the observer has the same abilities as the mentor in
the following sense: s, t  S, am  Supp(m , s), there exists an action ao  Ao such that
Pro (s, ao , t) = Prm (s, am , t). In other words, the observer is able to duplicate (in a the sense
of inducing the same distribution over successor states) the actual behavior of the mentor;
or equivalently, the agents local state spaces are isomorphic with respect to the actions
actually taken by the mentor at the subset of states where those actions might be taken.
This is much weaker than requiring a full homomorphism from Sm to So . Of course, the
existence of a full homomorphism is sufficient from our perspective; but our results do not
require this.
4.2 The Implicit Imitation Algorithm
The implicit imitation algorithm can be understood in terms of its component processes.
First, we extract action models from a mentor. Then we integrate this information into
the observers own value estimates by augmenting the usual Bellman backup with mentor
action models. A confidence testing procedure ensures that we only use this augmented
model when the observers model of the mentor is more reliable than the observers model
of its own behavior. We also extract occupancy information from the observations of mentor
trajectories in order to focus the observers computational effort (to some extent) in specific
parts of the state space. Finally, we augment our action selection process to choose actions
that will explore high-value regions revealed by the mentor. The remainder of this section
expands upon each of these processes and how they fit together.
4.2.1 Model Extraction
The information available to the observer in its quest to learn how to act optimally can be
divided into two categories. First, with each action it takes, it receives an experience tuple
hs, a, r, ti; in fact, we will often ignore the sampled reward r, since we assume the reward
function R is known in advance. As in standard model-based learning, each such experience
can be used to update its own transition model Pro (s, a, ).
Second, with each mentor transition, the observer obtains an experience tuple hs, ti.
Note again that the observer does not have direct access to the action taken by the mentor,
only the induced state transition. Assume the mentor is implementing a deterministic,
stationary policy m , with m (s) denoting the mentors choice of action at state s. This
policy induces a Markov chain Prm (, ) over S, with Prm (s, t) = Pr(s, m (s), t) denoting
583

fiPrice & Boutilier

the probability of a transition from s to t.6 Since the learner observes the mentors state
c m of this chain: Pr
c m (s, t) is simply estimated by
transitions, it can construct an estimate Pr
the relative observed frequency of mentor transitions s  t (w.r.t. all transitions taken from
s). If the observer has some prior over the possible mentor transitions, standard Bayesian
update techniques can be used instead. We use the term model extraction for this process
of estimating the mentors Markov chain.
4.2.2 Augmented Bellman Backups
c m of the mentors Markov chain. By
Suppose the observer has constructed an estimate Pr
the homogeneity assumption, the action m (s) can be replicated exactly by the observer at
state s. Thus, the policy m can, in principle, be duplicated by the observer (were it able
to identify the actual actions used). As such, we can define the value of the mentors policy
from the observers perspective:

Vm (s) = Ro (s) + 

X

Prm (s, t)Vm (t)

(6)

tS

Notice that Equation 6 uses the mentors dynamics but the observers reward function.
Letting V denote the optimal (observers) value function, clearly V (s)  Vm (s), so Vm
provides a lower bound on the observers value function.
More importantly, the terms making up Vm (s) can be integrated directly into the Bellman equation for the observers MDP, forming the augmented Bellman equation:
(

(

V (s) = Ro (s) +  max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

X

)

Prm (s, t)V (t)

(7)

tS

This is the usual Bellman equation with an extra term added, namely, the second
P
summation, tS Prm (s, t)V (t) denoting the expected value of duplicating the mentors
action am . Since this (unknown) action is identical to one of the observers actions, the
term is redundant and the augmented value equation is valid. Of course, the observer using
the augmented backup operation must rely on estimates of these quantities. If the observer
exploration policy ensures that each state is visited infinitely often, the estimates of the Pro
terms will converge to their true values. If the mentors policy is ergodic over state space S,
then Prm will also converge to its true value. If the mentors policy is restricted to a subset
of states S 0  S (those forming the basis of its Markov chain), then the estimates of Prm
for the subset will converge correctly with respect to S 0 if the chain is ergodic. The states
in S  S 0 will remain unvisited and the estimates will remain uninformed by data. Since
the mentors policy is not under the control of the observer, there is no way for the observer
to influence the distribution of samples attained for Prm . An observer must therefore be
able to reason about the accuracy of the estimated model Prm for any s and restrict the
application of the augmented equation to those states where Prm is known with sufficient
accuracy.
6. This is somewhat imprecise, since the initial distribution of the Markov chain is unknown. For our
purposes, it is only the dynamics that are relevant to the observer, so only the transition probabilities
are used.

584

fiImplicit Imitation

While Prm cannot be used indiscriminately, we argue that it can be highly informative
early in the learning process. Assuming that the mentor is pursuing an optimal policy (or at
least is behaving in some way so that it tends to visit certain states more frequently), there
will be many states for which the observer has much more accurate estimates of Prm (s, t)
than it does for Pro (s, a, t) for any specific a. Since the observer is learning, it must explore
both its state spacecausing less frequent visits to sand its action spacethus spreading
its experience at s over all actions a. This generally ensures that the sample size upon which
Prm is based is greater than that for Pro for any action that forms part of the mentors
policy. Apart from being more accurate, the use of Prm (s, t) can often give more informed
value estimates at state s, since prior action models are often flat or uniform, and only
become distinguishable at a given state when the observer has sufficient experience at state
s.
We note that the reasoning above holds even if the mentor is implementing a (stationary) stochastic policy (since the expected value of stochastic policy for a fully-observable
MDP cannot be greater than that of an optimal deterministic policy). While the direction offered by a mentor implementing a deterministic policy tends to be more focused,
empirically we have found that mentors offer broader guidance in moderately stochastic
environments or when they implement stochastic policies, since they tend to visit more of
the state space. We note that the extension to multiple mentors is straightforwardeach
mentor model can be incorporated into the augmented Bellman equation without difficulty.
4.2.3 Model Confidence
When the mentors Markov chain is not ergodic, or if the mixing rate7 is sufficiently low, the
mentor may visit a certain state s relatively infrequently. The estimated mentor transition
model corresponding to a state that is rarely (or never) visited by the mentor may provide a
very misleading estimatebased on the small sample or the prior for the mentors chainof
the value of the mentors (unknown) action at s; and since the mentors policy is not under
the control of the observer, this misleading value may persist for an extended period. Since
the augmented Bellman equation does not consider relative reliability of the mentor and
observer models, the value of such a state s may be overestimated;8 that is, the observer can
be tricked into overvaluing the mentors (unknown) action, and consequently overestimating
the value of state s.
To overcome this, we incorporate an estimate of model confidence into our augmented
backups. For both the mentors Markov chain and the observers action transitions, we
assume a Dirichlet prior over the parameters of each of these multinomial distributions
(DeGroot, 1975). These reflect the observers initial uncertainty about the possible transition probabilities. From sample counts of mentor and observer transitions, we update
these distributions. With this information, we could attempt to perform optimal Bayesian
estimation of the value function; but when the sample counts are small (and normal approximations are not appropriate), there is no simple, closed form expression for the resultant
distributions over values. We could attempt to employ sampling methods, but in the in7. The mixing rate refers to how quickly a Markov chain approaches its stationary distribution.
8. Note that underestimates based on such considerations are not problematic, since the augmented Bellman
equation then reduces to the usual Bellman equation.

585

fiPrice & Boutilier

V
M

VO-

VO VM

Figure 1: Lower bounds on action values incorporate uncertainty penalty
terest of simplicity we have employed an approximate method for combining information
sources inspired by Kaelblings (1993) interval estimation method.
Let V denote the current estimated augmented value function, and Pro and Prm denote
2 denote the variance
the estimated observer and mentor transition models. We let o2 and m
in these model parameters.
An augmented Bellman backup with respect to V using confidence testing proceeds
as follows. We first compute the observers optimal action ao based on the estimated
augmented values for each of the observers actions. Let Q(ao , s) = Vo (s) denote its value.
For the best action, we use the model uncertainty encoded by the Dirichlet distribution to
construct a lower bound Vo (s) on the value of the state to the observer using the model
(at state s) derived from its own behavior (i.e., ignoring its observations of the mentor).
We employ transition counts no (s, a, t) and nm (s, t) to denote the number of times the
observer has made the transition from state s to state t when the action a was performed,
and the number of times the mentor was observed making the transition from state s to
t, respectively. From these counts, we estimate the uncertainty in the model using the
P
variance of a Dirichlet distribution. Let  = no (s, a, t) and  = t0 St no (s, a, t0 ). Then
the model variance is:
2
model
(s, a, t) =

( +

)2

+
+ ( +  + 1)

(8)

The variance in the Q-value of an action due to the uncertainty in the local model
can be found by simple application of the rule for combining linear combinations of variances, V ar(cX + dY ) = c2 V ar(X) + d2 V ar(Y ) to the expression for the Bellman backup,
P
V ar(R(s) +  t P r(t|s, a)V (t). The result is:
 2 (s, a) =  2

X
t

2
model
(s, a, t)v(t)2

(9)

Using Chebychevs inequality,9 we can obtain a confidence level even though the Dirichlet
distributions for small sample counts are highly non-normal. The lower bound is then
Vo (s) = Vo (s)co (s, ao ) for some suitable constant c. One may interpret this as penalizing
9. Chebychevs inequality states that 1  k12 of the probability mass for an arbitrary distribution will be
within k standard deviations of the mean.

586

fiImplicit Imitation

2
2
FUNCTION augmentedBackup( V ,Pro ,omodel
,Prm ,mmodel
,s,c)

a = arg maxaAo

P
tS

Pr(s, a, t)V (t)

P

Vo (s) = Ro (s) +  PtS Pro (s, a , t)V (t)
Vm (s) = Ro (s) +  tS Prm (s, t)V (t)

P

2
2

o2 (s, a ) = 
(t)2
P tS2 omodel (s, a , t)V
2
2
2
m (s) = 

(s, t)V (t)
tS mmodel

Vo (s) = Vo (s)  c  o (s, a )

Vm
(s) = Vm (s)  c  m (s)
IF Vo (s) > Vm (s) THEN
V (s) = Vo (s)
ELSE
V (s) = Vm (s)
END

Table 1: Implicit Backup
the value of a state by subtracting its uncertainty from it (see Figure 1).10 The value
Vm (s) of the mentors action m (s) is estimated similarly and an analogous lower bound
Vm (s) on it is also constructed. If Vo (s) > Vm (s), then we say that Vo (s) supersedes
Vm (s) and we write Vo (s)  Vm (s). When Vo (s)  Vm (s) then either the mentor-inspired
model has, in fact, a lower expected value (within a specified degree of confidence) and
uses a nonoptimal action (from the observers perspective), or the mentor-inspired model
has lower confidence. In either case, we reject the information provided by the mentor and
use a standard Bellman backup using the action model derived solely from the observers
experience (thus suppressing the augmented backup)the backed up value is Vo (s) in this
case.
An algorithm for computing an augmented backup using this confidence test is shown
in Table 1. The algorithm parameters include the current estimate of the augmented value
2
function V , the current estimated model Pro and its associated local variance omodel
,
2
and the model of the mentors Markov chain Prm and its associated variance mmodel . It
calculates lower bounds and returns the mean value, Vo or Vm , with the greatest lower
bound. The parameter c determines the width of the confidence interval used in the mentor
rejection test.
4.2.4 Focusing
The augmented Bellman backups improves the accuracy of the observers model. A second
way in which an observer can exploit its observations of the mentor is to focus attention on
the states visited by the mentor. In a model-based approach, the specific focusing mecha10. Ideally, we would like to take not only the uncertainty of the model at the current state into account,
but also the uncertainty of future states as well (Meuleau & Bourgine, 1999).

587

fiPrice & Boutilier

nism we adopt is to require the observer to perform a (possibly augmented) Bellman backup
at state s whenever the mentor makes a transition from s. This has three effects. First, if
the mentor tends to visit interesting regions of space (e.g., if it shares a certain reward structure with the observer), then the significant values backed up from mentor-visited states
will bias the observers exploration towards these regions. Second, computational effort will
c m (s, t) changes,
be concentrated toward parts of state space where the estimated model Pr
and hence where the estimated value of one of the observers actions may change. Third,
computation is focused where the model is likely to be more accurate (as discussed above).
4.2.5 Action Selection
The integration of exploration techniques in the action selection policy is important for any
reinforcement learning algorithm to guarantee convergence. In implicit imitation, it plays a
second, crucial role in helping the agent exploit the information extracted from the mentor.
Our improved convergence results rely on the greedy quality of the exploration strategy to
bias an observer towards the higher-valued trajectories revealed by the mentor.
For expediency, we have adopted the -greedy action selection method, using an exploration rate  that decays over time. We could easily have employed other semi-greedy
methods such as Boltzmann exploration. In the presence of a mentor, greedy action selection becomes more complex. The observer examines its own actions at state s in the usual
way and obtains a best action ao which has a corresponding value Vo (s). A value is also
calculated for the mentors action Vm (s). If Vo (s)  Vm (s), then the observers own action
model is used and the greedy action is defined exactly as if the mentor were not present.
If, however, Vm (s)  Vo (s) then we would like to define the greedy action to be the action
dictated by the mentors policy at state s. Unfortunately, the observer does not know which
action this is, so we define the greedy action to be the observers action closest to the
mentors action according to the observers current model estimates at s. More precisely,
the action most similar to the mentors at state s, denoted m (s), is that whose outcome
distribution has minimum Kullback-Leibler divergence from the mentors action outcome
distribution:
(

m (s) = argmina 

X

)

Pro (s, a, t) log Prm (s, t)

(10)

t

The observers own experience-based action models will be poor early in training, so there
is a chance that the closest action computation will select the wrong action. We rely on the
exploration policy to ensure that each of the observers actions is sampled appropriately in
the long run.11
In our present work we have assumed that the state space is large and that the agent
will therefore not be able to completely update the Q-function over the whole space. (The
intractability of updating the entire state space is one of the motivations for using imitation
techniques). In the absence of information about the states true values, we would like to
bias the value of the states along the mentors trajectories so that they look worthwhile to
explore. We do this by assuming bounds on the reward function and setting the initial Qvalues over the entire space below this bound. In our simple examples, rewards are strictly
11. If the mentor is executing a stochastic policy, the test based on KL-divergence can mislead the learner.

588

fiImplicit Imitation

positive so we set the bounds to zero. If mentor trajectories intersect any states valued by
the observing agent, backups will cause the states on these trajectories to have a higher
value than the surrounding states. This causes the greedy step in the exploration method
to prefer actions that lead to mentor-visited states over actions for which the agent has no
information.
4.2.6 Model Extraction in Specific Reinforcement Learning Algorithms
Model extraction, augmented backups, the focusing mechanism, and our extended notion of
the greedy action selection, can be integrated into model-based reinforcement learning algorithms with relative ease. Generically, our implicit imitation algorithm requires that: (a)
c m (s, t) of the Markov chain induced by the mentors
the observer maintain an estimate Pr
policythis estimate is updated with every observed transition; and (b) that all backups
performed to estimate its value function use the augmented backup (Equation 7) with confic o (s, a, t)
dence testing. Of course, these backups are implemented using estimated models Pr
c
and Prm (s, t). In addition, the focusing mechanism requires that an augmented backup be
performed at any state visited by the mentor.
We demonstrate the generality of these mechanisms by combining them with the wellknown and efficient prioritized sweeping algorithm (Moore & Atkeson, 1993). As outlined
c
in Section 2.2, prioritized sweeping works by maintaining an estimated transition model Pr
b Whenever an experience tuple hs, a, r, ti is sampled, the estimated
and reward model R.
model at state s can change; a Bellman backup is performed at s to incorporate the revised
model and some (usually fixed) number of additional backups are performed at selected
states. States are selected using a priority that estimates the potential change in their values
based on the changes precipitated by earlier backups. Effectively, computational resources
(backups) are focused on those states that can most benefit from those backups.
Incorporating our ideas into prioritized sweeping simply requires the following changes:
c o (s, a, t) is up With each transition hs, a, ti the observer takes, the estimated model Pr
dated and an augmented backup is performed at state s. Augmented backups are then
performed at a fixed number of states using the usual priority queue implementation.
c m (s, t) is updated
 With each observed mentor transition hs, ti, the estimated model Pr
and an augmented backup is performed at s. Augmented backups are then performed
at a fixed number of states using the usual priority queue implementation.

Keeping samples of mentor behavior implements model extraction. Augmented backups
integrate this information into the observers value function, and performing augmented
backups at observed transitions (in addition to experienced transitions) incorporates our
focusing mechanism. The observer is not forced to follow or otherwise mimic the actions
of the mentor directly. But it does back up value information along the mentors trajectory
as if it had. Ultimately, the observer must move to those states to discover which actions
are to be used; in the meantime, important value information is being propagated that can
guide its exploration.
Implicit imitation does not alter the long run theoretical convergence properties of the
underlying reinforcement learning algorithm. The implicit imitation framework is orthogonal to -greedy exploration, as it alters only the definition of the greedy action, not when
589

fiPrice & Boutilier

the greedy action is taken. Given a theoretically appropriate decay factor, the -greedy
strategy will thus ensure that the distributions for the action models at each state are
sampled infinitely often in the limit and converge to their true values. Since the extracted
model from the mentor corresponds to one of the observers own actions, its effect on the
value function calculations is no different than the effect of the observers own sampled
action models. The confidence mechanism ensures that the model with more samples will
eventually come to dominate if it is, in fact, better. We can therefore be sure that the convergence properties of reinforcement learning with implicit imitation are identical to that
of the underlying reinforcement learning algorithm.
The benefit of implicit imitation lies in the way in which the models extracted from the
mentor allow the observer to calculate a lower bound on the value function and use this
lower bound to choose its greedy actions to move the agent towards higher-valued regions
of state space. The result is quicker convergence to optimal policies and better short-term
practical performance with respect to accumulated discounted reward while learning.
4.2.7 Extensions
The implicit imitation model can easily be extended to extract model information from
multiple mentors, mixing and matching pieces extracted from each mentor to achieve good
results. It does this by searching, at each state, the set of mentors it knows about to find
the mentor with the highest value estimate. The value estimate of the best mentor is then
compared using the confidence test described above with the observers own value estimate.
The formal expression of the algorithm is given by the multi-augmented Bellman equation:
(

(

V (s) = Ro (s) +  max max
aAo

max

mM

X

X

)

Pro (s, a, t)V (t)

,

tS

)

Prm (s, t)V (t)

(11)

tS

where M is the set of candidate mentors. Ideally, confidence estimates should be taken
into account when comparing mentor estimates with each other, as we may get a mentor
with a high mean value estimate but large variance. If the observer has any experience
with the state at all, this mentor will likely be rejected as having poorer quality information
than the observer already has from its own experience. The observer might have been
better off picking a mentor with a lower mean but more confident estimate that would
have succeeded in the test against the observers own model. In the interests of simplicity,
however, we investigate multiple mentor combination without confidence testing.
Up to now, we have assumed no action costs (i.e., the agents rewards depend only on
the state and not on the action selected in the state); however, we can use more general
reward functions (e.g., where reward has the form R(s, a)). The difficulty lies in backing
up action costs when the mentors chosen action is unknown. In Section 4.2.5 we defined
the closest action function . The  function can be used to choose the appropriate reward.
The augmented Bellman equation with generalized rewards takes the following form:
(

(

V (s) = max max Ro (s, a) + 
aAo

X
tS

590

)

Pro (s, a, t)V (t)

,

fiImplicit Imitation

Ro (s, (s)) + 

X

)

Prm (s, t)V (t)

tS

We note that Bayesian methods could be used could be used to estimate action costs
in the mentors chain as well. In any case, the generalized reward augmented equation can
readily be amended to use confidence estimates in a similar fashion to the transition model.
4.3 Empirical Demonstrations
The following empirical tests incorporate model extraction and our focusing mechanism into
prioritized sweeping. The results illustrate the types of problems and scenarios in which
implicit imitation can provide advantages to a reinforcement learning agent. In each of the
experiments, an expert mentor is introduced into the experiment to serve as a model for the
observer. In each case, the mentor is following an -greedy policy with a very small  (on
the order of 0.01). This tends to cause the mentors trajectories to lie within a cluster
surrounding optimal trajectories (and reflect good if not optimal policies). Even with a
small amount of exploration and some environment stochasticity, mentors generally do not
cover the entire state space, so confidence testing is important.
In all of these experiments, prioritized sweeping is used with a fixed number of backups per observed or experienced sample.12 -greedy exploration is used with decaying .
Observer agents are given uniform Dirichlet priors and Q-values are initialized to zero. Observer agents are compared to control agents that do not benefit from a mentors experience,
but are otherwise identical (implementing prioritized sweeping with similar parameters and
exploration policies). The tests are all performed on stochastic grid world domains, since
these make it clear to what extent the observers and mentors optimal policies overlap (or
fail to). In Figure 2, a simple 10  10 example shows a start and end state on a grid.
A typical optimal mentor trajectory is illustrated by the solid line between the start and
end states. The dotted line shows that a typical mentor-influenced trajectory will be quite
similar to the observed mentor trajectory. We assume eight-connectivity between cells so
that any state in the grid has nine neighbors including itself, but agents have only four
possible actions. In most experiments, the four actions move the agent in the compass
directions (North, South, East and West), although the agent will not initially know which
action does which. We focus primarily on whether imitation improves performance during
learning, since the learner will converge to an optimal policy whether it uses imitation or
not.
4.3.1 Experiment 1: The Imitation Effect
In our first experiment we compare the performance of an observer using model extraction
and an expert mentor with the performance of a control agent using independent reinforcement learning. Given the uniform nature of this grid world and the lack of intermediate
rewards, confidence testing is not required. Both agents attempt to learn a policy that
maximizes discounted return in a 10  10 grid world. They start in the upper-left corner
and seek a goal with value 1.0 in the lower-right corner. Upon reaching the goal, the agents
12. Generally, the number of backups was set to be roughly equal to the length of the optimal noise-free
path.

591

fiPrice & Boutilier

S

X

Figure 2: A simple grid world with start state S and goal state X

50
Obs
FA Series
Ctrl

Average Reward per 1000 Steps

40

30

20

10

Delta

0

10

0

500

1000

1500

2000

2500

3000

3500

4000

4500

Simulation Steps

Figure 3: Basic observer and control agent comparisons

are restarted in the upper-left corner. Generally the mentor will follow a similar if not identical trajectory each run, as the mentors were trained using a greedy strategy that leaves
one path slightly more highly valued than the rest. Action dynamics are noisy, with the
intended direction being realized 90% of the time, and one of the other directions taken
otherwise (uniformly). The discount factor is 0.9. In Figure 3, we plot the cumulative
number of goals obtained over the previous 1000 time steps for the observer Obs and
control Ctrl agents (results are averaged over ten runs). The observer is able to quickly
incorporate a policy learned from the mentor into its value estimates. This results in a
steeper learning curve. In contrast, the control agent slowly explores the space to build a
model first. The Delta curve shows the difference in performance between the agents.
Both agents converge to the same optimal value function.
592

fiImplicit Imitation

30

25

Average Reward per 1000 Steps

20

Basic
Scale

15

10
Stoch
5

0

5

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 4: Delta curves showing the influence of domain size and noise

4.3.2 Experiment 2: Scaling and Noise
The next experiment illustrates the sensitivity of imitation to the size of the state space and
action noise level. Again, the observer uses model-extraction but not confidence testing.
In Figure 4, we plot the Delta curves (i.e., difference in performance between observer and
control agents) for the Basic scenario just described, the Scale scenario in which the
state space size is increased 69 percent (to a 13  13 grid), and the Stoch scenario in
which the noise level is increased to 40 percent (results are averaged over ten runs). The
total gain represented by the area under the curves for the observer and the non-imitating
prioritized sweeping agent increases with the state space size. This reflects Whiteheads
(1991a) observation that for grid worlds, exploration requirements can increase quickly
with state space size, but that the optimal path length increases only linearly. Here we see
that the guidance of the mentor can help more in larger state spaces.
Increasing the noise level reduces the observers ability to act upon the information
received from the mentor and therefore erodes its advantage over the control agent. We
note, however, that the benefit of imitation degrades gracefully with increased noise and is
present even at this relatively extreme noise level.
4.3.3 Experiment 3: Confidence Testing
Sometimes the observers prior beliefs about the transition probabilities of the mentor can
mislead the observer and cause it to generate inappropriate values. The confidence mechanism proposed in the previous section can prevent the observer from being fooled by
misleading priors over the mentors transition probabilities. To demonstrate the role of the
confidence mechanism in implicit imitation, we designed an experiment based on the scenario illustrated in Figure 5. Again, the agents task is to navigate from the top-left corner
to the bottom-right corner of a 10  10 grid in order to attain a reward of +1. We have cre593

fiPrice & Boutilier

+5

+5

+5

+5

Figure 5: An environment with misleading priors
ated a pathological scenario in which islands of high reward (+5) are enclosed by obstacles.
Since the observers priors reflect eight-connectivity and are uniform, the high-valued cells
in the middle of each island are believed to be reachable from the states diagonally adjacent
with some small prior probability. In reality, however, the agents action set precludes this
and the agent will therefore never be able to realize this value. The four islands in this
scenario thus create a fairly large region in the center of the space with a high estimated
value, which could potentially trap an observer if it persisted in its prior beliefs.
Notice that a standard reinforcement learner will quickly learn that none of its actions
take it to the rewarding islands; in contrast, an implicit imitator using augmented backups
could be fooled by its prior mentor model. If the mentor does not visit the states neighboring
the island, the observer will not have any evidence upon which to change its prior belief that
the mentor actions are equally likely to take one in any of the eight possible directions. The
imitator may falsely conclude on the basis of the mentor action model that an action does
exist which would allow it to access the islands of value. The observer therefore needs a
confidence mechanism to detect when the mentor model is less reliable than its own model.
To test the confidence mechanism, we have the mentor follows a path around the outside
of the obstacles so that its path cannot lead the observer out of the trap (i.e., it provides
no evidence to the observer that the diagonal moves into the islands are not feasible). The
combination of a high initial exploration rate and the ability of prioritized sweeping to
spread value across large distances then virtually guarantees that the observer will be led to
the trap. Given this scenario, we ran two observer agents and a control. The first observer
used a confidence interval with width given by 5, which, according to the Chebychev rule,
should cover approximately 96 percent of an arbitrary distribution. The second observer
was given a 0 interval, which effectively disables confidence testing. The observer with no
confidence testing consistently became stuck. Examination of the value function revealed
consistent peaks within the trap region, and inspection of the agent state trajectories showed
that it was stuck in the trap. The observer with confidence testing consistently escaped the
trap. Observation of its value function over time shows that the trap formed, but faded
away as the observer gained enough experience to with its own actions to allow it to ignore
594

fiImplicit Imitation

45
CR Series

40

Ctrl

35

Average Reward per 1000 Steps

Obs
30

25

20

15

10

5

0

5

Delta

0

2000

4000

6000

8000

10000

12000

Simulation Steps

Figure 6: Misleading priors may degrade performance
overcome erroneous priors over the mentor actions. In Figure 6, the performance of the
observer with confidence testing is shown with the performance of the control agent (results
are averaged over 10 runs). We see that the observers performance is only slightly degraded
from that of the unaugmented control agent even in this pathological case.
4.3.4 Experiment 4: Qualitative Difficulty
The next experiment demonstrates how the potential gains of imitation can increase with
the (qualitative) difficulty of the problem. The observer employs both model extraction and
confidence testing, though confidence testing will not play a significant role here.13 In the
maze scenario, we introduce obstacles in order to increase the difficulty of the learning
problem. The maze is set on a 25  25 grid (Figure 7) with 286 obstacles complicating the
agents journey from the top-left to the bottom-right corner. The optimal solution takes
the form of a snaking 133-step path, with distracting paths (up to length 22) branching
off from the solution path necessitating frequent backtracking. The discount factor is 0.98.
With 10 percent noise, the optimal goal-attainment rate is about six goals per 1000 steps.
From the graph in Figure 8 (with results averaged over ten runs), we see that the control
agent takes on the order of 200,000 steps to build a decent value function that reliably leads
to the goal. At this point, it is only achieving four goals per 1000 steps on average, as its
exploration rate is still reasonably high (unfortunately, decreasing exploration more quickly
leads to slower value function formation). The imitation agent is able to take advantage of
the mentors expertise to build a reliable value function in about 20,000 steps. Since the
control agent has been unable to reach the goal at all in the first 20,000 steps, the Delta
between the control and the imitator is simply equal to the imitators performance. The
13. The mentor does not provide evidence about some path choices in this problem, but there are no
intermediate rewards which would cause the observer to make use of the misleading mentor priors at
these states.

595

fiPrice & Boutilier

Figure 7: A complex maze
7

CMB Series
6

Average Reward per 1000 Steps

5

4

3

Obs

2

Delta

1

0

0

Ctrl

0.5

1

1.5

2

2.5
5

x 10

Simulation Steps

Figure 8: Imitation in a complex space
imitator can quickly achieve the optimal goal attainment rate of six goals per 1000 steps,
as its exploration rate decays much more quickly.
4.3.5 Experiment 5: Improving Suboptimal Policies by Imitation
The augmented backup rule does not require that the reward structure of the mentor and
observer be identical. There are many useful scenarios where rewards are dissimilar but
the value functions and policies induced share some structure. In this experiment, we
demonstrate one interesting scenario in which it is relatively easy to find a suboptimal
solution, but difficult to find the optimal solution. Once the observer finds this suboptimal
path, however, it is able to exploit its observations of the mentor to see that there is a
596

fiImplicit Imitation

1

4

*
*
*
*
*
*
*
*
*

*
*
*
*
*
*
*
*
*

2

3
5

Figure 9: A maze with a perilous shortcut

shortcut that significantly shortens the path to the goal. The structure of the scenario
is shown in Figure 9. The suboptimal solution lies on the path from location 1 around
the scenic route to location 2 and on to the goal at location 3. The mentor takes the
vertical path from location 4 to location 5 through the shortcut.14 To discourage the
use of the shortcut by novice agents, it is lined with cells (marked *) from which the
agent immediately jumps back to the start state. It is therefore difficult for a novice agent
executing random exploratory moves to make it all the way to the end of the shortcut
and obtain the value which would reinforce its future use. Both the observer and control
therefore generally find the scenic route first.
In Figure 10, the performance (measured using goals reached over the previous 1000
steps) of the control and observer are compared (averaged over ten runs), indicating the
value of these observations. We see that the observer and control agent both find the longer
scenic route, though the control agent takes longer to find it. The observer goes on to find
the shortcut and increases its return to almost double the goal rate. This experiment shows
that mentors can improve observer policies even when the observers goals are not on the
mentors path.
4.3.6 Experiment 6: Multiple Mentors
The final experiment illustrates how model extraction can be readily extended so that the
observer can extract models from multiple mentors and exploit the most valuable parts of
each. Again, the observer employs model extraction and confidence testing. In Figure 11,
the learner must move from start location 1 to goal location 4. Two expert agents with
different start and goal states serve as potential mentors. One mentor repeatedly moves from
location 3 to location 5 along the dotted line, while a second mentor departs from location
2 and ends at location 4 along the dashed line. In this experiment, the observer must
14. A mentor proceeding from 5 to 4 would not provide guidance without prior knowledge that actions are
reversible.

597

fiPrice & Boutilier

35

CSB Series
30

Average Reward per 1000 Steps

25

Obs

20
Delta
15

10
Ctrl
5

0

0

0.5

1

1.5

2

2.5

3
4

x 10

Simulation Steps

Figure 10: Transfer with non-identical rewards

1

3

2

5

4

Figure 11: Multiple mentors scenario

combine the information from the examples provided by the two mentors with independent
exploration of its own in order to solve the problem.
In Figure 12, we see that the observer successfully pulls together these information
sources in order to learn much more quickly than the control agent (results are averaged
over 10 runs). We see that the use of a value-based technique allows the observer to choose
which mentors influence to use on a state-by-state basis in order to get the best solution
to the problem.
598

fiImplicit Imitation

60

Obs
CMM Series

Average Reward per 1000 Steps

50

40

Ctrl
30

20
Delta

10

0

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 12: Learning from multiple mentors

5. Implicit Imitation in Heterogeneous Settings
When the homogeneity assumption is violated, the implicit imitation framework described
above can cause the learners convergence rate to slow dramatically and, in some cases,
cause the learner to become stuck in a small neighborhood of state space. In particular,
if the learner is unable to make the same state transition (or a transition with the same
probability) as the mentor at a given state, it may drastically overestimate the value of
that state. The inflated value estimate causes the learner to return repeatedly to this state
even though its exploration will never produce a feasible action that attains the inflated
estimated value. There is no mechanism for removing the influence of the mentors Markov
chain on value estimatesthe observer can be extremely (and correctly) confident in its
observations about the mentors model. The problem lies in the fact that the augmented
Bellman backup is justified by the assumption that the observer can duplicate every mentor
action. That is, at each state s, there is some a  A such that Pro (s, a, t) = Prm (s, t) for
all t. When an equivalent action a does not exist, there is no guarantee that the value
calculated using the mentor action model can, in fact, be achieved.
5.1 Feasibility Testing
In such heterogeneous settings, we can prevent lock-up and poor convergence through the
use of an explicit action feasibility test: before an augmented backup is performed at s, the
observer tests whether the mentors action am differs from each of its actions at s, given
its current estimated models. If so, the augmented backup is suppressed and a standard
Bellman backup is used to update the value function. 15 By default, mentor actions are
15. The decision is binary; but we could envision a smoother decision criterion that measures the extent to
which the mentors action can be duplicated.

599

fiPrice & Boutilier

assumed to be feasible for the observer; however, once the observer is reasonably confident
that am is infeasible at state s, augmented backups are suppressed at s.
Recall that uncertainty about the agents true transition probabilities are captured by a
Dirichlet distribution derived from sampled transitions. Comparing am with ao is effected by
a difference of means test with respect to the corresponding Dirichlets. This is complicated
by the fact that Dirichlets are highly non-normal for small parameter values and transition
distributions are multinomial. We deal with the non-normality by requiring a minimum
number of samples and using robust Chebychev bounds on the pooled variance of the
distributions to be compared. Conceptually, we will evaluate Equation 12:
r

| Pro (s, ao , t)  Prm (s, t)|
2
2
no (s,ao ,t)omodel
(s,ao ,t)+nm (s,t)mmodel
(s,t)
no (s,ao ,t)+nm (s,t)

> Z/2

(12)

Here Z/2 is the critical value of the test. The parameter  is the significance of the test,
or the probability that we will falsely reject two actions as being different when they are
actually the same. Given our highly non-normal distributions early in the training process,
the appropriate Z value for a given  can be computed from Chebychevs bound by solving
2 = 1  Z12 for Z/2 .
When we have too few samples to do an accurate test, we persist with augmented
backups (embodying our default assumption of homogeneity). If the value estimate is
inflated by these backups, the agent will be biased to obtain additional samples, which will
then allow the agent to perform the required feasibility test. Our assumption is therefore
self-correcting. We deal with the multivariate complications by performing the Bonferroni
test (Seber, 1984), which has been shown to give good results in practice (Mi & Sampson,
1993), is efficient to compute, and is known to be robust to dependence between variables. A
Bonferroni hypothesis test is obtained by conjoining several single variable tests. Suppose
the actions ao and am result in r possible successor states, s1 ,    , sr (i.e., r transition
probabilities to compare). For each si , the hypothesis Ei denotes that ao and am have the
same transition probability to successor state si ; that is Pr(s, am , si ) = Pr(s, ao , si ). We let
Ei denote the complementary hypothesis (i.e., that the transition probabilities differ). The
Bonferroni inequality states:
"

Pr

r
\

#

Ei  1 

i=1

r
X



Pr Ei



i=1

T

Thus we can test the joint hypothesis ri=1 Ei the two action models are the sameby
testing each of the r complementary hypotheses Ei at confidence level /r. If we reject
any of the hypotheses we reject the notion that the two actions are equal with confidence
at least . The mentor action am is deemed infeasible if for every observer action ao , the
multivariate Bonferroni test just described rejects the hypothesis that the action is the same
as the mentors.
Pseudo-code for the Bonferroni component of the feasibility test appears in Table 2. It
assumes a sufficient number of samples. For efficiency reasons, we cache the results of the
feasibility testing. When the duplication of the mentors action at state s is first determined
to be infeasible, we set a flag for state s to this effect.
600

fiImplicit Imitation

FUNCTION feasible(m,s) : Boolean
FOR each a in Ao do
allSuccessorProbsSimilar = true
FOR each t in successors(s) do
 = |P roq
(s, a, t)  P rm (s, t)|
no (s,a,t) 2

(s,a,t)+nm (s,t) 2

omodel
z =  /
no (s,a,t)+nm (s,t)
IF z > z/2r
allSuccessorProbsSimilar = false
END FOR
IF allSuccessorProbsSimilar
return true
END FOR
RETURN false

mmodel

(s,t)

Table 2: Action Feasibility Testing
5.2 k-step Similarity and Repair
Action feasibility testing essentially makes a strict decision as to whether the agent can
duplicate the mentors action at a specific state: once it is decided that the mentors action is
infeasible, augmented backups are suppressed and all potential guidance offered is eliminated
at that state. Unfortunately, the strictness of the test results in a somewhat impoverished
notion of similarity between mentor and observer. This, in turn, unnecessarily limits the
transfer between mentor and observer. We propose a mechanism whereby the mentors
influence may persist even if the specific action it chooses is not feasible for the mentor; we
instead rely on the possibility that the observer may approximately duplicate the mentors
trajectory instead of exactly duplicating it.
Suppose an observer has previously constructed an estimated value function using augmented backups. Using the mentor action model (i.e., the mentors chain Prm (s, t)), a high
value has been calculated for state s. Subsequently, suppose the mentors action at state s
is judged to be infeasible. This is illustrated in Figure 13, where the estimated value at state
s is originally due to the mentors action m (s), which for the sake of illustration moves
with high probability to state t, which itself can lead to some highly-rewarding region of
state space. After some number of experiences at state s, however, the learner concludes
that the action m (s)and the associated high probability transition to tis not feasible.
At this point, one of two things must occur: either (a) the value calculated for state
s and its predecessors will collapse and all exploration towards highly-valued regions
beyond state s ceases; or (b) the estimated value drops slightly but exploration continues
towards the highly-valued regions. The latter case may arise as follows. If the observer
has previously explored in the vicinity of state s, the observers own action model may be
sufficiently developed that they still connect the higher value-regions beyond state s to state
s through Bellman backups. For example, if the learner has sufficient experience to have
learned that the highly-valued region can be reached through the alternative trajectory
s  u  v  w, the newly discovered infeasibility of the mentors transition s  t will not have
a deleterious effect on the value estimate at s. If s is highly-valued, it is likely that states
close to the mentors trajectory will be explored to some degree. In this case, state s will
601

fiPrice & Boutilier

Infeasible Transition

s

High-value
State

t

w
"Bridge"

u

v

Figure 13: An alternative path can bridge value backups around infeasible paths
not be as highly-valued as it was when using the mentors action model, but it will still be
valued highly enough that it will likely to guide further exploration toward the area. We call
this alternative (in this case s  u  v  w) to the mentors action a bridge, because it allows
value from higher value regions to flow over an infeasible mentor transition. Because
the bridge was formed without the intention of the agent, we call this process spontaneous
bridging.
Where a spontaneous bridge does not exist, the observers own action models are generally undeveloped (e.g., they are close to their uniform prior distributions). Typically, these
undeveloped models assign a small probability to every possible outcome and therefore diffuse value from higher valued regions and lead to a very poor value estimate for state s.
The result is often a dramatic drop in the value of state s and all of its predecessors; and
exploration towards the highly-valued region through the neighborhood of state s ceases.
In our example, this could occur if the observers transition models at state s assign low
probability (e.g., close to prior probability) of moving to state u due to lack of experience
(or similarly if the surrounding states, such as u or v, have been insufficiently explored).
The spontaneous bridging effect motivates a broader notion of similarity. When the
observer can find a short sequence of actions that bridges an infeasible action on the
mentors trajectory, the mentors example can still provide extremely useful guidance. For
the moment, we assume a short path is any path of length no greater than some given
integer k. We say an observer is k-step similar to a mentor at state s if the observer can
duplicate in k or fewer steps the mentors nominal transition at state s with sufficiently
high probability.
Given this notion of similarity, an observer can now test whether a spontaneous bridge
exists and determine whether the observer is in danger of value function collapse and the
concomitant loss of guidance if it decides to suppress an augmented backup at state s. To do
this, the observer initiates a reachability analysis starting from state s using its own action
model Pro (s, a, t) to determine if there is a sequence of actions with leads with sufficiently
high probability from state s to some state t on the mentors trajectory downstream of
the infeasible action.16 If a k-step bridge already exists, augmented backups can be safely
suppressed at state s. For efficiency, we maintain a flag at each state to mark it as bridged.
Once a state is known to be bridged, the k-step reachability analysis need not be repeated.
If a spontaneous bridge cannot be found, it might still be possible to intentionally set out
to build one. To build a bridge, the observer must explore from state s up to k-steps away,
hoping to make contact with the mentors trajectory downstream of the infeasible mentor
16. In a more general state space where ergodicity is lacking, the agent must consider predecessors of state
s up to k steps before s to guarantee that all k-step paths are checked.

602

fiImplicit Imitation

action. We implement a single search attempt as a k2 -step random walk, which will result
in a trajectory on average k steps away from s as long ergodicity and local connectivity
assumptions are satisfied. In order for the search to occur, we must motivate the observer to
return to the state s and engage in repeated exploration. We could provide motivation to the
observer by asking the observer to assume that the infeasible action will be repairable. The
observer will therefore continue the augmented backups which support high-value estimates
at the state s and the observer will repeatedly engage in exploration from this point. The
danger, of course, is that there may not in fact be a bridge, in which case the observer will
repeat this search for a bridge indefinitely. We therefore need a mechanism to terminate
the repair process when a k-step repair is infeasible. We could attempt to explicitly keep
track of all of the possible paths open to the observer and all of the paths explicitly tried by
the observer and determine the repair possibilities had been exhausted. Instead, we elect
to follow a probabilistic search that eliminates the need for bookkeeping: if a bridge cannot
be constructed within n attempts of k-step random walk, the repairability assumption is
judged falsified, the augmented backup at state s is suppressed and the observers bias to
explore the vicinity of state s is eliminated. If no bridge is found for state s, a flag is used
to mark the state as irreparable.
This approach is, of course, a very nave heuristic strategy; but it illustrates the basic
import of bridging. More systematic strategies could be used, involving explicit planning
to find a bridge using, say, local search (Alissandrakis, Nehaniv, & Dautenhahn, 2000).
Another aspect of this problem that we do not address is the persistence of search for
bridges. In a specific domain, after some number of unsuccessful attempts to find bridges,
a learner may conclude that it is unable to reconstruct a mentors behavior, in which case
the search for bridges may be abandoned. This involves simple, higher-level inference, and
some notion of (or prior beliefs about) similarity of capabilities. These notions could also
be used to automatically determine parameter settings (discussed below).
The parameters k and n must be tuned empirically, but can be estimated given knowledge of the connectivity of the domain and prior beliefs about how similar (in terms of
length of average repair) the trajectories of the mentor and observer will be. For instance,
n > 8k  4 seems suitable in an 8-connected grid world with low noise, based on the number
of trajectories required to cover the perimeter states of a k-step rectangle around a state.
We note that very large values of n can reduce performance below that of non-imitating
agents as it results in temporary lock up.
Feasibility and k-step repair are easily integrated into the homogeneous implicit imitation framework. Essentially, we simply elaborate the conditions under which the augmented
backup will be employed. Of course, some additional representation will be introduced to
keep track of whether a state is feasible, bridged, or repairable, and how many repair attempts have been made. The action selection mechanism will also be overridden by the
bridge-building algorithm when required in order to search for a bridge. Bridge building
always terminates after n attempts, however, so it cannot affect long run convergence. All
other aspects of the algorithm, however, such as the exploration policy, are unchanged.
The complete elaborated decision procedure used to determine when augmented backups
will be employed at state s with respect to mentor m appears in Table 3. It uses some
internal state to make its decisions. As in the original model, we first check to see if the
observers experience-based calculation for the value of the state supersedes the mentor603

fiPrice & Boutilier

FUNCTION use augmented?(s,m) : Boolean
IF Vo (s)  Vm (s) THEN RETURN false
ELSE IF f easible(s, m) THEN RETURN true
ELSE IF bridged(s, m) THEN RETURN false
ELSE IF reachable(s, m) THEN
bridged(s,m) := true
RETURN false
ELSE IF not repairable(s, m) THEN return false
ELSE % we are searching
IF 0 < search steps(s, m) < k THEN % search in progress
return true
IF search steps(s, m) > k THEN % search failed
IF attempts(s) > n THEN
repairable(s) = false
RETURN false
ELSE
reset search(s,m)
attempts(s) := attempts(s) + 1
RETURN true
attempts(s) :=1 % initiate first attempt of a search
initiate-search(s)
RETURN true

Table 3: Elaborated augmented backup test

based calculation; if so, then the observer uses its own experience-based calculation. If
the mentors action is feasible, then we accept the value calculated using the observationbased value function. If the action is infeasible we check to see if the state is bridged. The
first time the test is requested, a reachability analysis is performed, but the results will be
drawn from a cache for subsequent requests. If the state has been bridged, we suppress
augmented backups, confident that this will not cause value function collapse. If the state
is not bridged, we ask if it is repairable. For the first n requests, the agent will attempt a
k-step repair. If the repair succeeds, the state is marked as bridged. If we cannot repair
the infeasible transition, we mark it not-repairable and suppress augmented backups.
We may wish to employ implicit imitation with feasibility testing in a multiple-mentor
scenario. The key change from implicit imitation without feasibility testing is that the
observer will only imitate feasible actions. When the observer searches through the set of
mentors for the one with the action that results in the highest value estimate, the observer
must consider only those mentors whose actions are still considered feasible (or assumed to
be repairable).
5.3 Empirical Demonstrations
In this section, we empirically demonstrate the utility of feasibility testing and k-step repair
and show how the techniques can be used to surmount both differences in actions between
agents and small local differences in state-space topology. The problems here have been
604

fiImplicit Imitation

chosen specifically to demonstrate the necessity and utility of both feasibility testing and
k-step repair.
5.3.1 Experiment 1: Necessity of Feasibility Testing
Our first experiment shows the importance of feasibility testing in implicit imitation when
agents have heterogeneous actions. In this scenario, all agents must navigate across an
obstacle-free, 10  10 grid world from the upper-left corner to a goal location in the lowerright. The agent is then reset to the upper-left corner. The first agent is a mentor with
the NEWS action set (North, South, East, and West movement actions). The mentor
is given an optimal stationary policy for this problem. We study the performance of three
learners, each with the Skew action set (N, S, NE, SW) and unable to duplicate the mentor
exactly (e.g., duplicating a mentors E-move requires the learner to move NE followed by
S, or move SE then N). Due to the nature of the grid world, the control and imitation
agents will actually have to execute more actions to get to the goal than the mentor and
the optimal goal rate for both the control and imitator are therefore lower than that of the
mentor. The first learner employs implicit imitation with feasibility testing, the second uses
imitation without feasibility testing, and the third control agent uses no imitation (i.e., is a
standard reinforcement learning agent). All agents experience limited stochasticity in the
form of a 5% chance that their action will be randomly perturbed. As in the last section,
the agents use model-based reinforcement learning with prioritized sweeping. We set k = 3
and n = 20.
The effectiveness of feasibility testing in implicit imitation can be seen in Figure 14.
The horizontal axis represents time in simulation steps and the vertical axis represents the
average number of goals achieved per 1000 time steps (averaged over 10 runs). We see that
the imitation agent with feasibility testing converges much more quickly to the optimal
goal-attainment rate than the other agents. The agent without feasibility testing achieves
sporadic success early on, but frequently locks up due to repeated attempts to duplicate
infeasible mentor actions. The agent still manages to reach the goal from time to time, as
the stochastic actions do not permit the agent to become permanently stuck in this obstaclefree scenario. The control agent without any form of imitation demonstrates a significant
delay in convergence relative to the imitation agents due to the lack of any form of guidance,
but easily surpasses the agent without feasibility testing in the long run. The more gradual
slope of the control agent is due to the higher variance in the control agents discovery time
for the optimal path, but both the feasibility-testing imitator and the control agent converge
to optimal solutions. As shown by the comparison of the two imitation agents, feasibility
testing is necessary to adapt implicit imitation to contexts involving heterogeneous actions.
5.3.2 Experiment 2: Changes to State Space
We developed feasibility testing and bridging primarily to deal with the problem of adapting
to agents with heterogeneous actions. The same techniques, however, can be applied to
agents with differences in their state-space connectivity (ultimately, these are equivalent
notions). To test this, we constructed a domain where all agents have the same NEWS
action set, but we alter the environment of the learners by introducing obstacles that arent
present for the mentor. In Figure 15, the learners find that the mentors path is obstructed
605

fiPrice & Boutilier

40

Feas

35
FS Series

Average Reward per 1000 Steps

30

25

Ctrl

20

15
NoFeas
10

5

0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 14: Utility of feasibility testing
S

X

Figure 15: Obstacle map and mentor path
by obstacles. Movement toward an obstacle causes a learner to remain in its current state.
In this sense, its action has a different effect than the mentors.
In Figure 16, we see that the results are qualitatively similar to the previous experiment.
In contrast to the previous experiment, both imitator and control use the NEWS action set
and therefore have a shortest path with the same length as that of the mentor. Consequently,
the optimal goal rate of the imitators and control is higher than in the previous experiment.
The observer without feasibility testing has difficulty with the maze, as the value function
augmented by mentor observations consistently leads the observer to states whose path to
the goal is directly blocked. The agent with feasibility testing quickly discovers that the
mentors influence is inappropriate at such states. We conclude that local differences in
state are well handled by feasibility testing.
Next, we demonstrate how feasibility testing can completely generalize the mentors
trajectory. Here, the mentor follows a path which is completely infeasible for the imitating
agent. We fix the mentors path for all runs and give the imitating agent the maze shown
606

fiImplicit Imitation

50
Feas
FO Series

45

Average Reward per 1000 Steps

40

35

Ctrl

30

25

20

15

10

5

0

NoFeas

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 16: Interpolating around obstacles

S

Observer
Mentor
X

Figure 17: Parallel generalization
in Figure 17 in which all but two of the states the mentor visits are blocked by an obstacle.
The imitating agent is able to use the mentors trajectory for guidance and builds its own
parallel trajectory which is completely disjoint from the mentors.
The results in Figure 18 show that gain of the imitator with feasibility testing over the
control agent diminishes, but still exists marginally when the imitator is forced to generalize
a completely infeasible mentor trajectory. The agent without feasibility testing does very
poorly, even when compared to the control agent. This is because it gets stuck around the
doorway. The high value gradient backed up along the mentors path becomes accessible to
the agents at the doorway. The imitation agent with feasibility will conclude that it cannot
proceed south from the doorway (into the wall) and it will then try a different strategy.
The imitator without feasibility testing never explores far enough away from the doorway
to setup an independent value gradient that will guide it to the goal. With a slower decay
schedule for exploration, the imitator without feasibility testing would find the goal, but this
607

fiPrice & Boutilier

60

Feas
50

FP Series

Average Reward per 1000 Steps

Ctrl

40

30

20

10

NoFeas
0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 18: Parallel generalization results

would still reduce its performance below that of the imitator with feasibility testing. The
imitator with feasibility testing makes use of its prior beliefs that it can follow the mentor
to backup value perpendicular to the mentors path. A value gradient will therefore form
parallel to the infeasible mentor path and the imitator can follow along side the infeasible
path towards the doorway where it makes the necessary feasibility test and then proceeds
to the goal.
As explained earlier, in simple problems there is a good chance that the informal effects
of prior value leakage and stochastic exploration may form bridges before feasibility testing
cuts off the value propagation that guides exploration. In more difficult problems where the
agent spends a lot more time exploring, it will accumulate sufficient samples to conclude
that the mentors actions are infeasible long before the agent has constructed its own bridge.
The imitators performance would then drop down to that of an unaugmented reinforcement
learner.
To demonstrate bridging, we devised a domain in which agents must navigate from the
upper-left corner to the bottom-right corner, across a river which is three steps wide and
exacts a penalty of 0.2 per step (see Figure 19). The goal state is worth 1.0. In the figure,
the path of the mentor is shown starting from the top corner, proceeding along the edge of
the river and then crossing the river to the goal. The mentor employs the NEWS action
set. The observer uses the Skew action set (N, NE, S, SW) and attempts to reproduce
the mentor trajectory. It will fail to reproduce the critical transition at the border of the
river (because the East action is infeasible for a Skew agent). The mentor action can
no longer be used to backup value from the rewarding state and there will be no alternative
paths because the river blocks greedy exploration in this region. Without bridging or an
optimistic and lengthly exploration phase, observer agents quickly discover the negative
states of the river and curtail exploration in this direction before actually making it across.
608

fiImplicit Imitation

Figure 19: River scenario
If we examine the value function estimate (after 1000 steps) of an imitator with feasibility
testing but no repair capabilities, we see that, due to suppression by feasibility testing, the
darkly shaded high-value states in Figure 19 (backed up from the goal) terminate abruptly
at an infeasible transition without making it across the river. In fact, they are dominated by
the lighter grey circles showing negative values. In this experiment, we show that bridging
can prolong the exploration phase in just the right way. We employ the k-step repair
procedure with k = 3.
Examining the graph in Figure 20, we see that both imitation agents experience an early
negative dip as they are guided deep into the river by the mentors influence. The agent
without repair eventually decides the mentors action is infeasible, and thereafter avoids
the river (and the possibility of finding the goal). The imitator with repair also discovers
the mentors action to be infeasible, but does not immediately dispense with the mentors
guidance. It keeps exploring in the area of the mentors trajectory using a random walk,
all the while accumulating a negative reward until it suddenly finds a bridge and rapidly
converges on the optimal solution.17 The control agent discovers the goal only once in the
ten runs.

6. Applicability
The simple experiments presented above demonstrate the major qualitative issues confronting an implicit imitation agent and how the specific mechanisms of implicit imitation
address these issues. In this section, we examine how the assumptions and the mechanisms we presented in the previous sections determine the types of problems suitable for
implicit imitation. We then present several dimensions that prove useful for predicting the
performance of implicit imitation in these types of problems.
17. While repair steps take place in an area of negative reward in this scenario, this need not be the case.
Repair doesnt imply short-term negative return.

609

fiPrice & Boutilier

15

Average Reward per 1000 Steps

10

FB Series

5
Ctrl
Ctrl
0
NoRepair

Repair
5

NoRepair
10

15

20

Repair

0

1000

2000

3000
Simulation Steps

4000

5000

6000

Figure 20: Utility of bridging
We have already identified a number of assumptions under which implicit imitation is
applicablesome assumptions under which other models of imitation or teaching cannot be
applied, and some assumptions that restrict the applicability of our model. These include:
lack of explicit communication between mentors and observer; independent objectives for
mentors and observer; full observability of mentors by observer; unobservability of mentors
actions; and (bounded) heterogeneity. Assumptions such as full observability are necessary
for our modelas formulatedto work (though we discuss extension to the partially observable case in Section 7). Assumptions of lack of communication and unobservable actions
extend the applicability of implicit imitation beyond other models in the literature; if these
conditions do not hold, a simpler form of explicit communication may be preferable. Finally,
the assumptions of bounded heterogeneity and independent objectives also ensure implicit
imitation can be applied widely. However, the degree to which rewards are the same and
actions are homogeneous can have an impact on the utility (i.e., the acceleration of learning offered by) implicit imitation. We turn our attention to predicting the performance of
implicit imitation as a function of certain domain characteristics.
6.1 Predicting Performance
In this section we examine two questions: first, given that implicit imitation is applicable,
when can implicit imitation bias an agent to a suboptimal solution; and second, how will
the performance of implicit imitation vary with structural characteristics of the domains
one might want to apply it to? We show how analysis of the internal structure of state space
can be used to motivate a metric that (roughly) predicts implicit imitation performance.
We conclude with an analysis of how the problem space can be understood in terms of
distinct regions playing different roles within an imitation context.
610

fiImplicit Imitation

In the implicit imitation model, we use observations of other agents to improve the observers knowledge about its environment and then rely on a sensible exploration policy to
exploit this additional knowledge. A clear understanding of how knowledge of the environment affects exploration is therefore central to understanding how implicit imitation will
perform in a domain.
Within the implicit imitation framework, agents know their reward functions, so knowledge of the environment consists solely of knowledge about the agents action models. In
general, these models can take any form. For simplicity, we have restricted ourselves to
models that can be decomposed into local models for each possible combination of a system
state and agent action.
The local models for state-action pairs allow the prediction of a j-step successor state
distribution given any initial state and sequence of actions or local policy. The quality of
the j-step state predictions will be a function of every action model encountered between
the initial state and the states at time j  1. Unfortunately, the quality of the j-step
estimate can be drastically altered by the quality of even a single intermediate state-action
model. This suggests that connected regions of state space, the states of which all have
fairly accurate models, will allow reasonably accurate future state predictions.
Since the estimated value of a state s is based on both the immediate reward and the
reward expected to be received in subsequent states, the quality of this value estimate
will also depend on the quality of the action models in those states connected to s. Now,
since greedy exploration methods bias their exploration according to the estimated value
of actions, the exploratory choices of an agent at state s will also be dependent on the
connectivity of reliable action models at those states reachable from s. Our analysis of
implicit imitation performance with respect to domain characteristics is therefore organized
around the idea of state space connectivity and the regions such connectivity defines.
6.1.1 The Imitation Regions Framework
Since connected regions play an important role in implicit imitation, we introduce a classification of different regions within the state space shown graphically in Figure 21. In what
follows, we describe of how these regions affect imitation performance in our model.
We first observe that many tasks can be carried out by an agent in a small subset of
states within the state space defined for the problem. More precisely, in many MDPs, the
optimal policy will ensure that an agent remains in a small subspace of state space. This
leads us to the definition of our first regional distinction: relevant vs. irrelevant regions. The
relevant region is the set of states with non-zero probability of occupancy under the optimal
policy.18 An -relevant region is a natural generalization in which the optimal policy keeps
the system within the region a fraction 1   of the time.
Within the relevant region, we distinguish three additional subregions. The explored
region contains those states where the observer has formulated reliable action models on the
basis of its own experience. The augmented region contains those states where the observer
lacks reliable action models but has improved value estimates due to mentor observations.
18. One often assumes that the system starts in one of a small set of states. If the Markov chain induced by
the optimal policy then is not ergodic, then the irrelevant region will be nonempty. Otherwise it will be
empty.

611

fiPrice & Boutilier

Observer
Explored
Region
Blind
Region

Mentor
Augmented
Region

Irrelevant
Region

Reward
Figure 21: Classification of regions of state space
Note that both the explored and augmented regions are created as the result of observations
made by the learner (of either its own transitions or those of a mentor). These regions will
therefore have significant connected components; that is, contiguous regions of state space
where reliable action or mentor models are available. Finally, the blind region designates
those states where the observer has neither (significant) personal experience nor the benefit
of mentor observations. Any information about states within the blind region will come
(largely) from the agents prior beliefs.19
We can now ask how these regions interact with an imitation agent. First we consider the
impact of relevance. Implicit imitation makes the assumption that more accurate dynamics
models allow an observer to make better decisions which will, in turn, result in higher returns
sooner in the learning process. However, not all model information is equally helpful: the
imitator needs only enough information about the irrelevant region to be able to avoid it.
Since action choices are influenced by the relative value of actions, the irrelevant region will
be avoided when it looks worse than the relevant region. Given diffuse priors on action
models, none of the actions open to an agent will initially appear particularly attractive.
However, a mentor that provides observations within the relevant region can quickly make
the relevant region look much more promising as a method of achieving higher returns and
therefore constrain exploration significantly. Therefore, considering problems just from the
point of view of relevance, a problem with a small relevant region relative to the entire space
combined with a mentor that operates within the relevant region will result in maximum
advantage for an imitation agent over a non-imitating agent.
In the explored region, the observer has sufficiently accurate models to compute a good
policy with respect to rewards within the explored region. Additional observations on
19. Our partitioning of states into explored, blind and augmented regions bears some resemblance to Kearns
and Singhs (1998) partitioning of state space into known and unknown regions. Unlike Kearns and Singh,
however, we use the partitions only for analysis. The implicit imitation algorithm does not explicitly
maintain these partitions or use them in any way to compute its policy.

612

fiImplicit Imitation

the states within the explored region provided by the mentor can still improve performance
somewhat if significant evidence is required to accurately discriminate between the expected
value of two actions. Hence, mentor observations in the explored region can help, but will
not result in dramatic speedups in convergence.
Now, we consider the augmented region in which the observers Q-values have been
augmented with observations of a mentor. In experiments in previous sections, we have
seen that an observer entering an augmented region can experience significant speedups in
convergence due to the information inherent in the augmented value function about the
location of rewards in the region. Characteristics of the augmented zone, however, can
affect the degree to which augmentation improves convergence speed.
Since the observer receives observations of only the mentors state, and not its actions,
the observer has improved value estimates for states in the augmented region, but no policy.
The observer must therefore infer which actions should be taken to duplicate the mentors
behavior. Where the observer has prior beliefs about the effects of its actions, it may be able
to perform immediate inference about the mentors actual choice of action (perhaps using
KL-divergence or maximum likelihood). Where the observers prior model is uninformative,
the observer will have to explore the local action space. In exploring a local action space,
however, the agent must take an action and this action will have an effect. Since there is no
guarantee that the agent took the action that duplicates the mentors action, it may end up
somewhere different than the mentor. If the action causes the observer to fall outside of the
augmented region, the observer will lose the guidance that the augmented value function
provides and fall back to the performance level of a non-imitating agent.
An important consideration, then, is the probability that the observer will remain in
augmented regions and continue to receive guidance. One quality of the augmented region
that affects the observers probability of staying within its boundaries is its relative coverage
of the state space. The policy of the mentor may be sparse or complete. In a relatively
deterministic domain with defined begin and end states, a sparse policy covering few states
may be adequate. In a highly stochastic domain with many start and end states, an agent
may need a complete policy (i.e., covering every state). Implicit imitation will provide
more guidance to the agent in domains that are more stochastic and require more complete
policies, since the policy will cover a larger part of the state space.
As important as the completeness of a policy is in predicting its guidance, we must
also take into account the probability of transitions into and out of the augmented region.
Where the actions in a domain are largely invertible (directly, or effectively so), the agent
has a chance of re-entering the augmented region. Where ergodicity is lacking, however,
the agent may have to wait until the process undergoes some form of reset before it has
the opportunity to gather additional evidence regarding the identity of the mentors actions
in the augmented region. The reset places the agent back into the explored region, from
which it can make its way to the frontier where it last explored. The lack of ergodicity
would reduce the agents ability to make progress towards high-value regions before resets,
but the agent is still guided on each attempt by the augmented region. Effectively, the
agent will concentrate its exploration on the boundary between the explored region and the
mentor augmented region.
The utility of mentor observations will depend on the probability of the augmented
and explored regions overlapping in the course of the agents exploration. In the explored
613

fiPrice & Boutilier

regions, accurate action models allow the agent to move as quickly as possible to high
value regions. In augmented regions, augmented Q-values inform agents about which states
lead to highly-valued outcomes. When an augmented region abuts an explored region, the
improved value estimates from the augmented region are rapidly communicated across the
explored region by accurate action models. The observer can use the resultant improved
value estimates in the explored region, together with the accurate action models in the
explored region, to rapidly move towards the most promising states on the frontier of the
explored region. From these states, the observer can explore outward and thereby eventually
expand the explored region to encompass the augmented region.
In the case where the explored region and augmented region do not overlap, we have a
blind region. Since the observer has no information beyond its priors for the blind region,
the observer is reduced to random exploration. In a non-imitation context, any states that
are not explored are blind. However, in an imitation context, the blind area is reduced in
effective size by the augmented area. Hence, implicit imitation effectively shrinks the size
of the search space of the problem even when there is no overlap between explored and
augmented spaces.
The most challenging case for implicit imitation transfer occurs when the region augmented by mentor observations fails to connect to both the observer explored region and
the regions with significant reward values. In this case, the augmented region will initially
provide no guidance. Once the observer has independently located rewarding states, the
augmented regions can be used to highlight shortcuts. These shortcuts represent improvements on the agents policy. In domains where a feasible solution is easy to find, but
optimal solutions are difficult, implicit imitation can be used to convert a feasible solution
to an increasingly optimal solution.
6.1.2 Cross regional textures
We have seen how distinctive regions can be used to provide a certain level of insight into how
imitation will perform in various domains. We can also analyze imitation performance in
terms of properties that cut across the state space. In our analysis of how model information
impacts imitation performance, we saw that regions connected by accurate action models
allowed an observer to use mentor observations to learn about the most promising direction
for exploration. We see, then, that any set of mentor observations will be more useful
if it is concentrated on a connected region and less useful if dispersed about the state
space in unconnected components. We are fortunate in completely observable environments
that observations of mentors tend to capture continuous trajectories, thereby providing
continuous regions of augmented states. In partially observable environments, occlusion
and noise could lessen the value of mentor observations in the absence of a model to predict
the mentors state.
The effects of heterogeneity, whether due to differences in action capabilities in the
mentor and observer or due to differences in the environment of the two agents, can also
be understood in terms of the connectivity of action models. Value can propagate along
chains of action models until we hit a state in which the mentor and observer have different
action capabilities. At this state, it may not be possible to achieve the mentors value
and therefore, value propagation is blocked. Again, the sequential decision making aspect
614

fiImplicit Imitation

of reinforcement learning leads to the conclusion that many scattered differences between
mentor and observer will create discontinuity throughout the problem space, whereas a
contiguous region of differences between mentor and observer will cause discontinuity in
a region, but leave other large regions fully connected. Hence, the distribution pattern of
differences between mentor and observer capabilities is as important as the prevalence of
difference. We will explore this pattern in the next section.
6.2 The Fracture Metric
We now try to characterize connectivity in the form of a metric. Since differences in reward structure, environment dynamics and action models that affect connectivity all would
manifest themselves as differences in policies between mentor and observer, we designed a
metric based on differences in the agents optimal policies. We call this metric fracture.
Essentially, it computes the average minimum distance from a state in which a mentor and
observer disagree on a policy to a state in which mentor and observer agree on the policy. This measure roughly captures the difficulty the observer faces in profitably exploiting
mentor observations to reduce its exploration demands.
More formally, let m be the mentors optimal policy and o be the observers. Let S
be the state space and Sm 6=o be the set of disputed states where the mentor and observer
have different optimal actions. A set of neighboring disputed states constitutes a disputed
region. The set S  Sm 6=o will be called the undisputed states. Let M be a distance
metric on the space S. This metric corresponds to the number of transitions along the
minimal length path between states (i.e., the shortest path using nonzero probability
observer transitions).20 In a standard grid world, it will correspond to the Manhattan
distance. We define the fracture (S) of state space S to be the average minimal distance
between a disputed state and the closest undisputed state:
(S) =

1

X

|Sm 6=o | sS

m 6=o

min

tSSm 6=o

M (s, t).

(13)

Other things being equal, a lower fracture value will tend to increase the propagation
of value information across the state space, potentially resulting in less exploration being
required. To test our metric, we applied it to a number of scenarios with varying fracture
coefficients. It is difficult to construct scenarios which vary in their fracture coefficient yet
have the same expected value. The scenarios in Figure 22 have been constructed so that the
length of all possible paths from the start state s to the goal state x are the same in each
scenario. In each scenario, however, there is an upper path and a lower path. The mentor
is trained in a scenario that penalizes the lower path and so the mentor learns to take the
upper path. The imitator is trained in a scenario in which the upper path is penalized
and should therefore take the lower path. We equalized the difficulty of these problems
as follows: using a generic -greedy learning agent with a fixed exploration schedule (i.e.,
a fixed initial rate and decay) in one scenario, we tuned the magnitude of penalties and
their exact placement along loops in their other scenarios so that a learner using the same
exploration policy would converge to the optimal policy in roughly the same number of
steps in each.
20. The expected distance would give a more accurate estimate of fracture, but is more difficult to calculate.

615

fiPrice & Boutilier

X

X

S

S

(a)  = 0.5

X

S

(b)  = 1.7

S

(c)  = 3.5

X

(d)  = 6.0

Figure 22: Fracture metric scenarios


0.5
1.7
3.5
6.0

5  102
60%

1  102
70%

Observer Initial Exploration Rate I
5  103 1  103 5  104 1  104
90%
0%
80%
90%
90 %
30%
100 %
30 %
70 %

5  105

1  105

100 %

100 %

Figure 23: Percentage of runs (of ten) converging to optimal policy given fracture  and
initial exploration rate I

In Figure 22(a), the mentor takes the top of each loop and in an optimal run, the imitator
would take the bottom of each loop. Since the loops are short and the length of the common
path is long, the average fracture is low. When we compare this to Figure 22(d), we see
that the loops are very longthe majority of states in the scenario are on loops. Each of
these states on a loop has a distance to the nearest state where the observer and mentor
policies agree, namely, a state not on the loop. This scenario therefore has a high average
fracture coefficient.
Since the loops in the various scenarios differ in length, penalties inserted in the loops
vary with respect to their distance from the goal state and therefore affect the total discounted expected reward in different ways. The penalties may also cause the agent to
become stuck in a local minimum in order to avoid the penalties if the exploration rate is
too low. In this set of experiments, we therefore compare observer agents on the basis of
how likely they are to converge to the optimal solution given the mentor example.
Figure 23 presents the percentage of runs (out of ten) in which the imitator converged
to the optimal solution (i.e., taking only the lower loops) as a function of exploration rate
and scenario fracture.21 We can see a distinct diagonal trend in the table illustrating that
increasing fracture requires the imitator to increase its levels of exploration in order to find
21. For reasons of computational expediency, only the entries near the diagonal have been computed. Sampling of other entries confirms the trend.

616

fiImplicit Imitation

the optimal policy. This suggests that fracture reflects a feature of RL domains that is may
be important in predicting the efficacy of implicit imitation.
6.3 Suboptimality and Bias
Implicit imitation is fundamentally about biasing the exploration of the observer. As such,
it is worthwhile to ask when this has a positive effect on observer performance. The short
answer is that a mentor following an optimal policy for an observer will cause an observer to
explore in the neighborhood of the optimal policy and this will generally bias the observer
towards finding the optimal policy.
A more detailed answer requires looking explicitly at exploration in reinforcement learning. In theory, an -greedy exploration policy with a suitable rate of decay will cause
implicit imitators to eventually converge to the same optimal solution as their unassisted
counterparts. However, in practice, the exploration rate is typically decayed more quickly
in order to improve early exploitation of mentor input. Given practical, but theoretically
unsound exploration rates, an observer may settle for a mentor strategy that is feasible,
but non-optimal. We can easily imagine examples: consider a situation in which an agent
is observing a mentor following some policy. Early in the learning process, the value of the
policy followed by the mentor may look better than the estimated value of the alternative
policies available to the observer. It could be the case that the mentors policy actually
is the optimal policy. On the other hand, it may be the case that one of the alternative
policies, with which the observer has neither personal experience, nor observations from a
mentor, is actually superior. Given the lack of information, an aggressive exploitation policy might lead the observer to falsely conclude that the mentors policy is optimal. While
implicit imitation can bias the agent to a suboptimal policy, we have no reason to expect
that an agent learning in a domain sufficiently challenging to warrant the use of imitation
would have discovered a better alternative. We emphasize that even if the mentors policy
is suboptimal, it still provides a feasible solution which will be preferable to no solution for
many practical problems.
In this regard, we see that the classic exploration/exploitation tradeoff has an additional
interpretation in the implicit imitation setting. A component of the exploration rate will
correspond to the observers belief about the sufficiency of the mentors policy. In this
paradigm, then, it seems somewhat misleading to think in terms of a decision about whether
to follow a specific mentor or not. It is more a question of how much exploration to
perform in addition to that required to reconstruct the mentors policy.
6.4 Specific Applications
We see applications for implicit imitation in a variety of contexts. The emerging electronic
commerce and information infrastructure is driving the development of vast networks of
multi-agent systems. In networks used for competitive purposes such as trade, implicit
imitation can be used by an RL agent to learn about buying strategies or information
filtering policies of other agents in order to improve its own behavior.
In control, implicit imitation could be used to transfer knowledge from an existing
learned controller which has already adapted to its clients to a new learning controller with
a completely different architecture. Many modern products such as elevator controllers
617

fiPrice & Boutilier

(Crites & Barto, 1998), cell traffic routers (Singh & Bertsekas, 1997) and automotive fuel
injection systems use adaptive controllers to optimize the performance of a system for
specific user profiles. When upgrading the technology of the underlying system, it is quite
possible that sensors, actuators and the internal representation of the new system will be
incompatible with the old system. Implicit imitation provides a method of transferring
valuable user information between systems without any explicit communication.
A traditional application for imitation-like technologies lies in the area of bootstrapping
intelligent artifacts using traces of human behavior. Research within the behavioral cloning
paradigm has investigated transfer in applications such as piloting aircraft (Sammut et al.,
1992) and controlling loading cranes (Suc & Bratko, 1997). Other researchers have investigated the use of imitation to simplify the programming of robots (Kuniyoshi, Inaba, &
Inoue, 1994). The ability of imitation to transfer complex, nonlinear and dynamic behaviors
from existing human agents makes it particularly attractive for control problems.

7. Extensions
The model of implicit imitation presented above makes certain restrictive assumptions regarding the structure of the decision problem being solved (e.g., full observability, knowledge
of reward function, discrete state and action space). While these simplifying assumptions
aided the detailed development of the model, we believe the basic intuitions and much of
the technical development can be extended to richer problem classes. We suggest several
possible extensions in this section, each of which provides a very interesting avenue for
future research.
7.1 Unknown Reward Functions
Our current paradigm assumes that the observer knows its own reward function. This
assumption is consistent with the view of RL as a form of automatic programming. We
can, however, relax this constraint assuming some ability to generalize observed rewards.
Suppose that the expected reward can be expressed in terms of a probability distribution
over features of the observers state, Pr(r|f (so )). In model-based RL, this distribution
can be learned by the agent through its own experience. If the same features can be
applied to the mentors state sm , then the observer can use what it has learned about the
reward distribution to estimate expected reward for mentor states as well. This extends
the paradigm to domains in which rewards are unknown, but preserves the ability of the
observer to evaluate mentor experiences on its own terms.
Imitation techniques designed around the assumption that the observer and the mentor
share identical rewards, such as Utgoffs (1991), would of course work in the absence of a
reward function. The notion of inverse reinforcement learning (Ng & Russell, 2000) could be
adapted to this case as well. A challenge for future research would be to explore a synthesis
between implicit imitation and reward inversion approaches to handle an observers prior
beliefs about some intermediate level of correlation between the reward function of observer
and mentor.
618

fiImplicit Imitation

7.2 Interaction of agents
While we cast the general imitation model in the framework of stochastic games, the restriction of the model presented thus far to noninteracting games essentially means that the
standard issues associated with multiagent interaction do not arise. There are, of course,
many tasks that require interactions between agents; in such cases, implicit imitation offers
the potential to accelerate learning. A general solution requires the integration of imitation
into more general models for multiagent RL based on stochastic or Markov games (Littman,
1994; Hu & Wellman, 1998; Bowling & Veloso, 2001). This would no doubt be a rather
challenging, yet rewarding endeavor.
To take a simple example, in simple coordination problems (e.g., two mobile agents
trying to avoid each other while carrying out related tasks) we might imagine an imitator
learning from a mentor by reversing the roles of their roles when considering how the
observed state transition is influenced by their joint action. In this and more general
settings, learning typically requires great care, since agents learning in a nonstationary
environment may not converge (say, to equilibrium). Again, imitation techniques offer
certain advantages: for instance, mentor expertise can suggest means of coordinating with
other agents (e.g., by providing a focal point for equilibrium selection, or by making clear
a specific convention such as always passing to the right to avoid collision).
Other challenges and opportunities present themselves when imitation is used in multiagent settings. For example, in competitive or educational domains, agents not only have to
choose actions that maximize information from exploration and returns from exploitation;
they must also reason about how their actions communicate information to other agents.
In a competitive setting, one agent may wish to disguise its intentions, while in the context
of teaching, a mentor may wish to choose actions whose purpose is abundantly clear. These
considerations must become part of any action selection process.
7.3 Partially Observable Domains
The extension of this model to partially observable domains is critical, since it is unrealistic
in many settings to suppose that a learner can constantly monitor the activities of a mentor.
The central idea of implicit imitation is to extract model information from observations of
the mentor, rather than duplicating mentor behavior. This means that the mentors internal
belief state and policy are not (directly) relevant to the learner. We take a somewhat
behaviorist stance and concern ourselves only with what the mentors observed behaviors
tell us about the possibilities inherent in the environment. The observer does have to keep a
belief state about the mentors current state, but this can be done using the same estimated
world model the observer uses to update its own belief state.
Preliminary investigation of such a model suggests that dealing with partial observability
is viable. We have derived update rules for augmented partially observable updates. These
updates are based on a Bayesian formulation of implicit imitation which is, in turn, based
on Bayesian RL (Dearden et al., 1999). In fully observable contexts, we have seen that more
effective exploration using mentor observations is possible in fully observable domains when
this Bayesian model of imitation is used (Price & Boutilier, 2003). The extension of this
model to cases where the mentors state is partially observable is reasonably straightforward.
We anticipate that updates performed using a belief state about the mentors state and
619

fiPrice & Boutilier

action will help to alleviate fracture that could be caused by incomplete observation of
behavior.
More interesting is dealing with an additional factor in the usual exploration-exploitation
tradeoff: determining whether it is worthwhile to take actions that render the mentor more
visible (e.g., ensuring the mentor remains in view so that this source of information remains
available while learning).
7.4 Continuous and Model-Free Learning
In many realistic domains, continuous attributes and large state and action spaces prohibit
the use of explicit table-based representations. Reinforcement learning in these domains
is typically modified to make use of function approximators to estimate the Q-function
at points where no direct evidence has been received. Two important approaches are
parameter-based models (e.g., neural networks) (Bertsekas & Tsitsiklis, 1996) and the
memory-based approaches (Atkeson, Moore, & Schaal, 1997). In both these approaches,
model-free learning is generally employed. That is, the agent keeps a value function but uses
the environment as an implicit model to perform backups using the sampling distribution
provided by environment observations.
One straightforward approach to casting implicit imitation in a continuous setting would
employ a model-free learning paradigm (Watkins & Dayan, 1992). First, recall the augmented Bellman backup function used in implicit imitation:
(

(

V (s) = Ro (s) +  max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

)

X

Prm (s, t)V (t)

(14)

tS

When we examine the augmented backup equation, we see that it can be converted to a
model-free form in much the same way as the ordinary Bellman backup. We use a standard
Q-function with observer actions, but we will add one additional action which corresponds
to the action am taken by the mentor.22 Now imagine that the observer was in state so ,
took action ao and ended up in state s0o . At the same time, the mentor made the transition
from state sm to s0m . We can then write:


Q(so , ao ) = (1  )Q(so , ao ) + (Ro (so , ao ) +  max

max

a0 Ao





	
Q(s0o , a0 ) ,

Q(sm , am ) = (1  )Q(sm , am ) + (Ro (sm , am ) +  max max
0

a Ao





Q(s0o , am )

	
Q(s0m , a0 ) ,

(15)


Q(s0m , am )

As discussed earlier, the relative quality of mentor and observer estimates of the Qfunction at specific states may vary. Again, in order to avoid having inaccurate prior beliefs
about the mentors action models bias exploration, we need to employ a confidence measure
to decide when to apply these augmented equations. We feel the most natural setting for
these kind of tests is in the memory-based approaches to function approximation. Memorybased approaches, such as locally-weighted regression (Atkeson et al., 1997), not only provide estimates for functions at points previously unvisited, they also maintain the evidence
22. This doesnt imply the observer knows which of its actions corresponds to am .

620

fiImplicit Imitation

set used to generate these estimates. We note that the implicit bias of memory-based approaches assumes smoothness between points unless additional data proves otherwise. On
the basis of this bias, we propose to compare the average squared distance of the query from
the exemplars used in the estimate of the mentors Q-value to the average squared distance
from the query to the exemplars used in the observer-based estimate to heuristically decide
which agent has the more reliable Q-value.
The approach suggested here does not benefit from prioritized sweeping. Prioritizedsweeping, has however, been adapted to continuous settings (Forbes & Andre, 2000). We
feel a reasonably efficient technique could be made to work.

8. Related Work
Research into imitation spans a broad range of dimensions, from ethological studies, to
abstract algebraic formulations, to industrial control algorithms. As these fields have crossfertilized and informed each other, we have come to stronger conceptual definitions and
a better understanding of the limits and capabilities of imitation. Many computational
models have been proposed to exploit specialized niches in a variety of control paradigms,
and imitation techniques have been applied to a variety of real-world control problems.
The conceptual foundations of imitation have been clarified by work on natural imitation. From work on apes (Russon & Galdikas, 1993), octopi (Fiorito & Scotto, 1992), and
other animals, we know that socially facilitated learning is widespread throughout the animal kingdom. A number of researchers have pointed out, however, that social facilitation
can take many forms (Conte, 2000; Noble & Todd, 1999). For instance, a mentors attention
to an object can draw an observers attention to it and thereby lead the observer to manipulate the object independently of the model provided by the mentor. True imitation
is therefore typically defined in a more restrictive fashion. Visalberghi and Fragazy (1990)
cite Mitchells definition:
1. something C (the copy of the behavior) is produced by an organism
2. where C is similar to something else M (the Model behavior)
3. observation of M is necessary for the production of C (above baseline levels of C
occurring spontaneously)
4. C is designed to be similar to M
5. the behavior C must be a novel behavior not already organized in that precise way in
the organisms repertoire.
This definition perhaps presupposes a cognitive stance towards imitation in which an
agent explicitly reasons about the behaviors of other agents and how these behaviors relate
to its own action capabilities and goals.
Imitation can be further analyzed in terms of the type of correspondence demonstrated
by the mentors behavior and the observers acquired behavior (Nehaniv & Dautenhahn,
1998; Byrne & Russon, 1998). Correspondence types are distinguished by level. At the
action level, there is a correspondence between actions. At the program level, the actions
621

fiPrice & Boutilier

may be completely different but correspondence may be found between subgoals. At the
effect level, the agent plans a set of actions that achieve the same effect as the demonstrated
behavior but there is no direct correspondence between subcomponents of the observers
actions and the mentors actions. The term abstract imitation has been proposed in the
case where agents imitate behaviors which come from imitating the mental state of other
agents (Demiris & Hayes, 1997).
The study of specific computational models of imitation has yielded insights into the
nature of the observer-mentor relationship and how it affects the acquisition of behaviors by
observers. For instance, in the related field of behavioral cloning, it has been observed that
mentors that implement conservative policies generally yield more reliable clones (Urbancic
& Bratko, 1994). Highly-trained mentors following an optimal policy with small coverage of
the state space yield less reliable clones than those that make more mistakes (Sammut et al.,
1992). For partially observable problems, learning from perfect oracles can be disastrous, as
they may choose policies based on perceptions not available to the observer. The observer is
therefore incorrectly biased away from less risky policies that do not require the additional
perceptual capabilities (Scheffer, Greiner, & Darken, 1997). Finally, it has been observed
that successful clones would often outperform the original mentor due to the cleanup effect
(Sammut et al., 1992).
One of the original goals of behavioral cloning (Michie, 1993) was to extract knowledge
from humans to speed up the design of controllers. For the extracted knowledge to be
useful, it has been argued that rule-based systems offer the best chance of intelligibility
(van Lent & Laird, 1999). It has become clear, however, that symbolic representations are
not a complete answer. Representational capacity is also an issue. Humans often organize
control tasks by time, which is typically lacking in state and perception-based approaches
to control. Humans also naturally break tasks down into independent components and
subgoals (Urbancic & Bratko, 1994). Studies have also demonstrated that humans will
give verbal descriptions of their control policies which do not match their actual actions
(Urbancic & Bratko, 1994). The potential for saving time in acquisition has been borne out
by one study which explicitly compared the time to extract rules with the time required to
program a controller (van Lent & Laird, 1999).
In addition to what has traditionally been considered imitation, an agent may also face
the problem of learning to imitate or finding a correspondence between the actions and
states of the observer and mentor (Nehaniv & Dautenhahn, 1998). A fully credible approach
to learning by observation in the absence of communication protocols will have to deal with
this issue.
The theoretical developments in imitation research have been accompanied by a number
of practical implementations. These implementations take advantage of properties of different control paradigms to demonstrate various aspects of imitation. Early behavioral cloning
research took advantage of supervised learning techniques such as decision trees (Sammut
et al., 1992). The decision tree was used to learn how a human operator mapped perceptions to actions. Perceptions were encoded as discrete values. A time delay was inserted in
order to synchronize perceptions with the actions they trigger. Learning apprentice systems
(Mitchell et al., 1985) also attempted to extract useful knowledge by watching users, but the
goal of apprentices is not to independently solve problems. Learning apprentices are closely
related to programming by demonstration systems (Lieberman, 1993). Later efforts used
622

fiImplicit Imitation

more sophisticated techniques to extract actions from visual perceptions and abstract these
actions for future use (Kuniyoshi et al., 1994). Work on associative and recurrent learning models has allowed work in the area to be extended to learning of temporal sequences
(Billard & Hayes, 1999). Associative learning has been used together with innate following
behaviors to acquire navigation expertise from other agents (Billard & Hayes, 1997).
A related but slightly different form of imitation has been studied in the multi-agent
reinforcement learning community. An early precursor to imitation can be found in work
on sharing of perceptions between agents (Tan, 1993). Closer to imitation is the idea of
replaying the perceptions and actions of one agent for a second agent (Lin, 1991; Whitehead,
1991a). Here, the transfer is from one agent to another, in contrast to behavioral clonings
transfer from human to agent. The representation is also different. Reinforcement learning
provides agents with the ability to reason about the effects of current actions on expected
future utility so agents can integrate their own knowledge with knowledge extracted from
other agents by comparing the relative utility of the actions suggested by each knowledge
source. The seeding approaches are closely related. Trajectories recorded from human
subjects are used to initialize a planner which subsequently optimizes the plan in order
to account for differences between the human effector and the robotic effector (Atkeson &
Schaal, 1997). This technique has been extended to handle the notion of subgoals within
a task (Atkeson & Schaal, 1997). Subgoals are also addressed by others (Suc & Bratko,
1997). Our own work is based on the idea of an agent extracting a model from a mentor
and using this model information to place bounds on the value of actions using its own
reward function. Agents can therefore learn from mentors with reward functions different
than their own.
Another approach in this family is based on the assumption that the mentor is rational
(i.e., follows an optimal policy), has the same reward function as the observer and chooses
from the same set of actions. Given these assumptions, we can conclude that the action
chosen by a mentor in a particular state must have higher value to the mentor than the
alternatives open to the mentor (Utgoff & Clouse, 1991) and therefore higher value to the
observer than any alternative. The system of Utgoff and Clouse therefore iteratively adjusts
the values of the actions until this constraint is satisfied in its model. A related approach
uses the methodology of linear-quadratic control ( Suc & Bratko, 1997). First a model of
the system is constructed. Then the inverse control problem is solved to find a cost matrix
that would result in the observed controller behavior given an environment model. Recent
work on inverse reinforcement learning takes a related approach to reconstructing reward
functions from observed behavior (Ng & Russell, 2000). It is similar to the inversion of the
quadratic control approach, but is formulated for discrete domains.
Several researchers have picked up on the idea of common representations for perceptual functions and action planning. One approach to using the same representation for
perception and control is based on the PID controller model. The PID controller represents
the behavior. Its output is compared with observed behaviors in order to select the action
which is closest to the observed behavior (Demiris & Hayes, 1999). Explicit motor action
schema have also been investigated in the dual role of perceptual and motor representations
(Mataric, Williamson, Demiris, & Mohan, 1998).
Imitation techniques have been applied in a diverse collection of applications. Classical control applications include control systems for robot arms (Kuniyoshi et al., 1994;
623

fiPrice & Boutilier

Friedrich, Munch, Dillmann, Bocionek, & Sassin, 1996), aeration plants (Scheffer et al.,
1997), and container loading cranes (Suc & Bratko, 1997; Urbancic & Bratko, 1994). Imitation learning has also been applied to acceleration of generic reinforcement learning (Lin,
1991; Whitehead, 1991a). Less traditional applications include transfer of musical style
(Canamero, Arcos, & de Mantaras, 1999) and the support of a social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999). Imitation has also
been investigated as a route to language acquisition and transmission (Billard et al., 1999;
Oliphant, 1999).

9. Concluding Remarks
We have described a formal and principled approach to imitation called implicit imitation.
For stochastic problems in which explicit forms of communication are not possible, the
underlying model-based framework combined with model extraction provides an alternative
to other imitation and learning-by-observation systems. Our new approach makes use of a
model to compute the actions an imitator should take without requiring that the observer
duplicate the mentors actions exactly. We have shown implicit imitation to offer significant
transfer capability on several test problems, where it proves to be robust in the face of
noise, capable of integrating subskills from multiple mentors, and able to provide benefits
that increase with the difficulty of the problem.
We have seen that feasibility testing extends implicit imitation in a principled manner
to deal with the situations where the homogeneous action assumption is invalid. Adding
bridging capabilities preserves and extends the mentors guidance in the presence of infeasible actions, whether due to differences in action capabilities or local differences in state
spaces. Our approach also relates to the idea of following in the sense that the imitator
uses local search in its model to repair discontinuities in its augmented value function before acting in the world. In the process of applying imitation to various domains, we have
learned more about its properties. In particular we have developed the fracture metric to
characterize the effectiveness of a mentor for a given observer in a specific domain. We have
also made considerable progress in extending imitation to new problem classes. The model
we have developed is rather flexible and can be extended in several ways: for example, a
Bayesian approach to imitation building on this work shows great potential (2003); and we
have initial formulations of promising approaches to extending implicit imitation to multiagent problems, partially observable domains and domains in which the reward function is
not specified a priori.
A number of challenges remain in the field of imitation. Bakker and Kuniyoshi (1996)
describe a number of these. Among the more intriguing problems unique to imitation are:
the evaluation of the expected payoff for observing a mentor; inferring useful state and
reward mappings between the domains of mentors and those of observers; and repairing
or locally searching in order to fit observed behaviors to an observers own capabilities
and goals. We have also raised the possibility of agents attempting to reason about the
information revealed by their actions in addition to whatever concrete value the actions
have for the agent.
Model-based reinforcement has been applied to numerous problems. Since implicit imitation can be added to model-based reinforcement learning with relatively little effort, we
624

fiImplicit Imitation

expect that it can be applied to many of the same problems. Its basis in the simple but
elegant theory of Markov decision processes makes it easy to describe and analyze. Though
we have focused on some simple examples designed to illustrate the different mechanisms
required for implicit imitation, we expect that variations on our approach will provide interesting directions for future research.

Acknowledgments
Thanks to the anonymous referees for their suggestions and comments on earlier versions of
this work and Michael Littman for editorial suggestions. Price was supported by NCE IRISIII Project BAC. Boutilier was supported by NSERC Research Grant OGP0121843, and
the NCE IRIS-III Project BAC. Some parts of this paper were presented in Implicit Imitation in Reinforcement Learning, Proceedings of the Sixteenth International Conference
on Machine Learning (ICML-99), Bled, Slovenia, pp.325334 (1999) and Imitation and
Reinforcement Learning in Agents with Heterogeneous Actions, Proceedings Fourteenth
Biennial Conference of the Canadian Society for Computational Studies of Intelligence (AI
2001), Ottawa, pp.111120 (2001).

References
Alissandrakis, A., Nehaniv, C. L., & Dautenhahn, K. (2000). Learning how to do things with
imitation. In Bauer, M., & Rich, C. (Eds.), AAAI Fall Symposium on Learning How to Do
Things, pp. 16 Cape Cod, MA.
Atkeson, C. G., & Schaal, S. (1997). Robot learning from demonstration. In Proceedings of the
Fourteenth International Conference on Machine Learning, pp. 1220 Nashville, TN.
Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning for control. Artificial
Intelligence Review, 11 (1-5), 75113.
Bakker, P., & Kuniyoshi, Y. (1996). Robot see, robot do: An overview of robot imitation. In AISB96
Workshop on Learning in Robots and Animals, pp. 311 Brighton,UK.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall,
Englewood Cliffs.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont, MA.
Billard, A., & Hayes, G. (1997). Learning to communicate through imitation in autonomous robots.
In Proceedings of The Seventh International Conference on Artificial Neural Networks, pp.
76368 Lausanne, Switzerland.
Billard, A., & Hayes, G. (1999). Drama, a connectionist architecturefor control and learning in
autonomous robots. Adaptive Behavior Journal, 7, 3564.
Billard, A., Hayes, G., & Dautenhahn, K. (1999). Imitation skills as a means to enhance learning of a
synthetic proto-language in an autonomous robot. In Proceedings of the AISB99 Symposium
on Imitation in Animals and Artifacts, pp. 8895 Edinburgh.
Boutilier, C. (1999). Sequential optimality and coordination in multiagent systems. In Proceedings of
the Sixteenth International Joint Conference on Artificial Intelligence, pp. 478485 Stockholm.
625

fiPrice & Boutilier

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions
and computational leverage. Journal of Artificial Intelligence Research, 11, 194.
Bowling, M., & Veloso, M. (2001). Rational and convergent learning in stochastic games. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, pp. 10211026
Seattle.
Breazeal, C. (1999). Imitation as social exchange between humans and robot. In Proceedings of the
AISB99 Symposium on Imitation in Animals and Artifacts, pp. 96104 Edinburgh.
Byrne, R. W., & Russon, A. E. (1998). Learning by imitation: a hierarchical approach. Behavioral
and Brain Sciences, 21, 667721.
Canamero, D., Arcos, J. L., & de Mantaras, R. L. (1999). Imitating human performances to automatically generate expressive jazz ballads. In Proceedings of the AISB99 Symposium on
Imitation in Animals and Artifacts, pp. 11520 Edinburgh.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Conference on Artificial
Intelligence, pp. 10231028 Seattle.
Conte, R. (2000). Intelligent social learning. In Proceedings of the AISB00 Symposium on Starting
from Society: the Applications of Social Analogies to Computational Systems Birmingham.
Crites, R., & Barto, A. G. (1998). Elevator group control using multiple reinforcement learning
agents. Machine-Learning, 33 (23), 23562.
Dean, T., & Givan, R. (1997). Model minimization in Markov decision processes. In Proceedings of
the Fourteenth National Conference on Artificial Intelligence, pp. 106111 Providence.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic planning.
Artificial Intelligence, 89, 219283.
Dearden, R., Friedman, N., & Andre, D. (1999). Model-based bayesian exploration. In Proceedings
of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pp. 150159 Stockholm.
DeGroot, M. H. (1975). Probability and statistics. Addison-Wesley, Reading, MA.
Demiris, J., & Hayes, G. (1997). Do robots ape?. In Proceedings of the AAAI Fall Symposium on
Socially Intelligent Agents, pp. 2831 Cambridge, MA.
Demiris, J., & Hayes, G. (1999). Active and passive routes to imitation. In Proceedings of the
AISB99 Symposium on Imitation in Animals and Artifacts, pp. 8187 Edinburgh.
Fiorito, G., & Scotto, P. (1992). Observational learning in octopus vulgaris. Science, 256, 54547.
Forbes, J., & Andre, D. (2000). Practical reinforcement learning in continuous domains. Tech. rep.
UCB/CSD-00-1109, Computer Science Division, University of California, Berkeley.
Friedrich, H., Munch, S., Dillmann, R., Bocionek, S., & Sassin, M. (1996). Robot programming by
demonstration (RPD): Support the induction by human interaction. Machine Learning, 23,
163189.
Hartmanis, J., & Stearns, R. E. (1966). Algebraic Structure Theory of Sequential Machines. PrenticeHall, Englewood Cliffs.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework and an
algorithm. In Proceedings of the Fifthteenth International Conference on Machine Learning,
pp. 242250 Madison, WI.
Kaelbling, L. P. (1993). Learning in Embedded Systems. MIT Press, Cambridge,MA.
626

fiImplicit Imitation

Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journal
of Artificial Intelligence Research, 4, 237285.
Kearns, M., & Singh, S. (1998). Near-optimal reinforcement learning in polynomial time. In Proceedings of the Fifthteenth International Conference on Machine Learning, pp. 260268 Madison,
WI.
Kuniyoshi, Y., Inaba, M., & Inoue, H. (1994). Learning by watching: Extracting reusable task
knowledge from visual observation of human performance. IEEE Transactions on Robotics
and Automation, 10 (6), 799822.
Lee, D., & Yannakakis, M. (1992). Online miminization of transition systems. In Proceedings of the
24th Annual ACM Symposium on the Theory of Computing (STOC-92), pp. 264274 Victoria,
BC.
Lieberman, H. (1993). Mondrian: A teachable graphical editor. In Cypher, A. (Ed.), Watch What
I Do: Programming by Demonstration, pp. 340358. MIT Press, Cambridge, MA.
Lin, L.-J. (1991). Self-improvement based on reinforcement learning, planning and teaching. Machine
Learning: Proceedings of the Eighth International Workshop (ML91), 8, 32327.
Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and
teaching. Machine Learning, 8, 293321.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the Eleventh International Conference on Machine Learning, pp. 157163 New
Brunswick, NJ.
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed Markov decision
processes. Annals of Operations Research, 28, 4766.
Mataric, M. J. (1998). Using communication to reduce locality in distributed multi-agent learning.
Journal Experimental and Theoretical Artificial Intelligence, 10 (3), 357369.
Mataric, M. J., Williamson, M., Demiris, J., & Mohan, A. (1998). Behaviour-based primitives for
articulated control. In R. Pfiefer, B. Blumberg, J.-A. M. . S. W. W. (Ed.), Fifth International
conference on simulation of adaptive behavior SAB98, pp. 165170 Zurich. MIT Press.
Meuleau, N., & Bourgine, P. (1999). Exploration of multi-state environments: Local mesures and
back-propagation of uncertainty. Machine Learning, 32 (2), 117154.
Mi, J., & Sampson, A. R. (1993). A comparison of the Bonferroni and Scheffe bounds. Journal of
Statistical Planning and Inference, 36, 101105.
Michie, D. (1993). Knowledge, learning and machine intelligence. In Sterling, L. (Ed.), Intelligent
Systems. Plenum Press, New York.
Mitchell, T. M., Mahadevan, S., & Steinberg, L. (1985). LEAP: A learning apprentice for VLSI
design. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,
pp. 573580 Los Altos, California. Morgan Kaufmann Publishers, Inc.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less
data and less real time. Machine Learning, 13 (1), 10330.
Myerson, R. B. (1991). Game Theory: Analysis of Conflict. Harvard University Press, Cambridge.
Nehaniv, C., & Dautenhahn, K. (1998). Mapping between dissimilar bodies: Affordances and the
algebraic foundations of imitation. In Proceedings of the Seventh European Workshop on
Learning Robots, pp. 6472 Edinburgh.
627

fiPrice & Boutilier

Ng, A. Y., & Russell, S. (2000). Algorithms for inverse reinforcement learning. In Proceedings of the
Seventeenth International Conference on Machine Learning, pp. 663670 Stanford.
Noble, J., & Todd, P. M. (1999). Is it really imitation? a review of simple mechanisms in social
information gathering. In Proceedings of the AISB99 Symposium on Imitation in Animals
and Artifacts, pp. 6573 Edinburgh.
Oliphant, M. (1999). Cultural transmission of communications systems: Comparing observational
and reinforcement learning models. In Proceedings of the AISB99 Symposium on Imitation
in Animals and Artifacts, pp. 4754 Edinburgh.
Price, B., & Boutilier, C. (2003). A Bayesian approach to imitation in reinforcement learning. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence Acapulco.
to appear.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley and Sons, Inc., New York.
Russon, A., & Galdikas, B. (1993). Imitation in free-ranging rehabilitant orangutans (pongopygmaeus). Journal of Comparative Psychology, 107 (2), 147161.
Sammut, C., Hurst, S., Kedzier, D., & Michie, D. (1992). Learning to fly. In Proceedings of the
Ninth International Conference on Machine Learning, pp. 385393 Aberdeen, UK.
Scassellati, B. (1999). Knowing what to imitate and knowing when you succeed. In Proceedings of
the AISB99 Symposium on Imitation in Animals and Artifacts, pp. 105113 Edinburgh.
Scheffer, T., Greiner, R., & Darken, C. (1997). Why experimentation can be better than perfect
guidance. In Proceedings of the Fourteenth International Conference on Machine Learning,
pp. 331339 Nashville.
Seber, G. A. F. (1984). Multivariate Observations. Wiley, New York.
Shapley, L. S. (1953). Stochastic games. Proceedings of the National Academy of Sciences, 39,
327332.
Singh, S. P., & Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation in
cellular telephone systems. In Advances in Neural information processing systems, pp. 974
980 Cambridge, MA. MIT Press.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable Markov
processes over a finite horizon. Operations Research, 21, 10711088.
Suc, D., & Bratko, I. (1997). Skill reconstruction as induction of LQ controllers with subgoals.
In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pp.
914919 Nagoya.
Sutton, R. S. (1988). Learning to predict by the method of temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML93, pp. 33037.
Urbancic, T., & Bratko, I. (1994). Reconstruction human skill with machine learning. In Eleventh
European Conference on Artificial Intelligence, pp. 498502 Amsterdam.
628

fiImplicit Imitation

Utgoff, P. E., & Clouse, J. A. (1991). Two kinds of training information for evaluation function
learning. In Proceedings of the Ninth National Conference on Artificial Intelligence, pp. 596
600 Anaheim, CA.
van Lent, M., & Laird, J. (1999). Learning hierarchical performance knowledge by observation.
In Proceedings of the Sixteenth International Conference on Machine Learning, pp. 229238
Bled, Slovenia.
Visalberghi, E., & Fragazy, D. (1990). Do monkeys ape?. In Parker, S., & Gibson, K. (Eds.),
Language and Intelligence in Monkeys and Apes, pp. 247273. Cambridge University Press,
Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Whitehead, S. D. (1991a). Complexity analysis of cooperative mechanisms in reinforcement learning. In Proceedings of the Ninth National Conference on Artificial Intelligence, pp. 607613
Anaheim.
Whitehead, S. D. (1991b). Complexity and cooperation in q-learning. In Machine Learning. Proceedings of the Eighth International Workshop (ML91), pp. 363367.

629

fi
	fffi	ffffff
!"#	%$'&(*),+--.0/1+2.3+045

6789:;<&-0=-+0>
?8ff#$;@&-0=-.

ACBEDGFIH"JKGLNMPORQTSGFISGJUVJVHWJVXZY

[]\_^`\badcfehgiaWk:
j lRmonqp%r
s1tWuWv@wyx'z{|q}
~'i,]00_01h
*
_W
I00

*0_d~W0b
I
_]0]
0
,0_0
 iy00!0E  00]W0]
0
fifi!qqfifiqfi
"`\b:Pe

n p Z\ba l
q
~W0bI_
'0


qq}s11|qv@wy{wyW|"1}}{t"

~W0_R

y00yiy00h 
fifi!fiqVfififififififififififi

fi1
IVfi0'1V1I1'ifiG011V]1Tb11oV0o1107]
		
fffi
10E0d1
1Vyy 1fiV*1	 fi7oV01fi1@7	
01
 WV0	
 7V V
  7d01fi7
 fi fi
  

 V1 7 ooVfi01o E fi 7fi 
fi1 E0 "
 !$#%%V	
 7
 
 o&
 7
 1'
 )(* E71  7 0+ 7
 07
 7@	
 0V7
 fi V 1
%1	 +0, 0
 0
 
 -'h	
 +070V:	
 fifi
 /.  1 710R710
 "
 !V
 0'1
 11
 071
 
2
 3fi1Vfi7 fiq
 E'
 4
 
1@
 fiV,
 1
 0R1
 7V,
 V 0 	
 * /
 fi5
 
6
 7
 1'
 V,
 

 	
 fi 71 17
 V1,
 7VW0fi0
 *1&
   fi+11 07
 
  
 8
 9
 !0+ 1
 /: 
 
 1fi 0 
7V
 h
 7,
 01
 7V7
 fi 0 0Z	
 99
 !;.d  01fi 

 '	
 0
 01
 0+ V
 10 1
 
1
 17 0'
 <
 =7 V0
   
 
  
  
 7  +0 Vfi 0fi>
 #ff
 !"?@(A.d 	+0 
V1, 7
 V 0o	 18
 9
 ! V
 01Z1
 7V,
 V 1V6
 <
 =7<V
 0
 @V
 1Z
1
 0
 fi 01,
 70
 d7 1h71
 ]V
 h7]'
 0,
 7fi '1'7 
  d 
 ff
 !B?
7Vfi 0T1,
 0
 C	+01"
 +0fiD
 V14
 /: 
 EV1,
 7
 V0
  fi,
 701VV7
 
 01C
 771
1
 1,
 81I+ 
 07
 h
 9
 ! 0
 ff
 !B?-

FDG HI fiJLK9MNOJ I
PQ/RSRATVUWYX%Z[X%\VW4Z]^_W4R/^W4U_W4Z`]aU'b/W4^'Q/ZCcfiW4^_]	XVd Z]e[X%fgTVQ/]hXjiTSkffXVd ZlQ/Ud ZS\mXR/^_TVfCX%f*d2n2d ]eoi/d U]a^d f/Q/p
]	d TVZCq*r&]@U_TSkW'RgTSd Z`]	sU_b/W;^_WcfiWd tVW4U&U_TSkW;Z/W4uvd Z*wxTV^k5X%]	d TVZmX%fgTVQ/]9]ab/WhiVTSkffXVd ZCq-yBTuzU_b/TVQCn2i{U_b/W
Q/RBi/X%]aW6b/W4^ji/d U_]a^d f/QS]	d TVZ|d Z}]ab/W~n2d \Vb]TSwC]abCd Ud ZCwTV^kffX%]	d TVZC*S//>d Ufe[wX%^]ab/WlkTVU_]
cfiTSkffkTVZ>kW4]abSTgijd Zmc4X%UW']abSWhd ZCwTV^kffX%]	d TVZ>cfiTSkW4U{d Z]ab/WhwxTV^kTSw1X%ZW4tVW4Z]	qyBTu9W4tVW4^s)]ab/W4^_W6X%^_W
ZQCkW4^_TVQ/U&u@Wn2n pZ/T	uZlW41XVkRCn W4U0Ub/T	u8d ZS\6]abCX%]0ZCXVd tVWjcfiTVZCi/d ]	d TVZCd Z/\lc4X%Zn WXVi6]aT5R/^_TVfCn WkUqLW
\Sd tVWQ/U_]&]u9T5TSwS]abSWkvb/W4^_Wq
 eAff b/W~~/;77),
6affTVU_]aWn2n W4^s"VVVVEtVTVUYP/XatX%Z]	sBVVVVahPQ/R/RgTVU_W8]abCX%]
  n/  p \[W

eVTVQC ^_W TVZX\SXVkW U_b/TuX%ZCi5\Sd tVW4ZX5cOb/TSd2cfiWhTSwg]ab/^_W4WiT`TV^UqCW4bCd ZCi5TVZ/Wjd U6XYc4X%^/fgW4bCd ZCiff]ab/W
TV]ab/W4^_UX%^_W\VTSX%]aUqTVQ5RCd2cOiVT`TV^;Vq&CWwTV^_W0TVRgW4ZCd ZS\5iT`TV^'VsS5TVZ`]ey;XVn2n2s]ab/WbSTVU_]0ub/T{Z/TuU
ubCX%]jd UfgW4bCd ZCijWXVcObziTTV^-TVRgW4Z/UjiT`TV^VsDu0bCd2cbobCX%UYX6\VTSX%]	qmy"W5]ab/W4ZX%U_`U8eVTVQzd2wCeVTVQoU_]	d2n2n
uX%Z`]{]aT]	X%VWYubCX%]	 U{fgW4bCd Z*iliVT`TV^5VsDTV^{]aT]	X%VW5ub*X%]	 U{fgW4bCd ZCimiTTV^5d Z/U_]aWXVi/qPbSTVQCn2ijeVTVQ
U_u8d ]	cObCYr;UU_QCkffd Z/\h]abCX%]	sEd ZCd ]	d2XVn2n es]ab/Wjc4X%^uX%U;WQ*XVn2n emn2d VWn e]aTfffgWfgW4bCd ZCi WXVcOb~TSwS]ab/W5iTTV^_Us
ZCXVd tVWhcfiTVZCi/d ]	d TVZCd Z/\ U_Q/\V\VW4U]aU9]abCX%]	s\Sd tVW4Z]abCX%]'d ]d U"Z/TV]9fgW4bCd ZCiffiTTV^'Vs7d ]d U9WQ*XVn2n e~n2d VWn eff]aT8fgW
fgW4bCd ZCiiTTV^8ffX%ZCiiVT`TV^8Vq  b`QSUs`]ab/W4^_WYd UZST6^_WX%U_TVZ]aTffUu8d ]	cbCq;yBTu9W4tVW4^sEX%Z/TV]ab/W4^ X%^_\VQCkW4Z]
U_Q/\V\VW4U]aUBeVTVQ6U_b/TVQCn2i;U_u d ]	cbC@d2wSX@\VTSX%];d UfgW4bCd Z*i8iTTV^0ubCd2cOb6bCX%R/RgW4Z/ULu8d ]abhR/^_TVfCX%f*d2n2d ]eVVVas
  +--.b##7
;	I	;@I	9R	9	9b8
ff#$#fiffff0$V#"#;V

fi " |qxq}t~"}s1|

U_u8d ]	cObCd Z/\jb/Wn R/U*d2wXjc4X%^6d U'fgW4bCd ZCiiVT`TV^6Yu0bCd2cblbCX%R/RgW4Z/U;u8d ]abRS^_TVfCX%fCd2n2d ]eoVVVas7U_u8d ]	cb*d Z/\
bQ/^_]aUq*bCd2cb[X%^_\VQCkW4Z]8d U^d \Vb`]	
 e	@ bSW',	7%,/	%E7)+"a@X%^py;d2n2n Wn/XVn 1sVVVVS{X%^iZ/W4^sVVVV/5TVU_]aWn2n W4^s
 n/  p \[W
VVVVa{{wA]abS^_W4W8RS^d U_TVZ/W4^_Uh`s*sEX%ZCils/]u9T>X%^_W8]aTYfgW8W4`WcfiQ/]aWiSsSfSQ/]8iTW4U;ZSTV];Z/T	uubCd2cObCq
 bQ/UsC ]abCd Z/U0]abCX%]']ab/WRS^_TVfCX%fCd2n2d ]e]abCX%]0-u8d2n2n/fgWW4WcfiQ/]aWimd UhVVjwxTV^'[V`44Vq8y"WUXaeU
]aT]ab/WaXVd2n W4^s %PSd ZCcfiWWd ]ab/W4^TV^jd UcfiW4^_]	XVd ZCn e\VTSd Z/\>]aTfgWjW4`WcfiQ/]aWiSsBeVTVQu d2n2n*\Sd tVWokWZ/T
d ZCwTV^kffX%]	d TVZoX%fgTVQ/]k0ejT	uZ[cbCX%ZCcfiW4Uhd2w/eVTVQ~\Sd tVWjkW]ab/WZ*XVkWTSw7TVZ/W5kffX%ZCsWd ]ab/W4^h9TV^8sub/T
d U@\VTSd Z/\8]aT8fgWW4`WcfiQS]aWi/q2>CQ/]"]abSW4ZCsZSTYkffX%]a]aW4^-ubCX%]B]abSW0aXVd2n W4^@UXaeUsZ*XVd tVWhcfiTVZCiSd ]	d TVZCd Z/\n WXViU
]aTfffgWn2d W4tVW]abCX%]b*d U8cbCX%Z*cfiW{TSw7W4`WcfiQ/]	d TVZu9W4Z] iVT	uZ}w^_TSkVV{]aTVVVq
 bSW4^_W>X%^_WZ`Q*kW4^_TVQ/UhTV]ab/W4^ffu@Wn2n pZ/T	uZW4gXVkR*n W4Uffub/W4^_WZCXVd tVW}cfiTVZ*i/d ]	d TVZCd Z/\\Sd tVW4UYubCX%]
U_W4WkUj]aT}fgW[X%Zd ZCX%R/RS^_TVR/^d2X%]aW[X%Z/U_u@W4^sd ZCc4n QCi/d ZS\>]ab/W+fi	[A,+a{X%^iZ/W4^sVVVV
tVTVU6P/XatX%Z`]	s*VVVVsLVV%S;X%Z*i6]ab/W,/fi,7),
a^_W4Q/ZCiSsDVVVVLPbCXVwW4^sDVVVVLy;XVn RgW4^_Z[
 Q/]a]	n WsEVVVVaq &
b`eiTW4U{ZCXVd tVWlcfiTVZCi/d ]	d TVZCd Z/\~\Sd tVW5]ab/Wffu0^_TVZ/\}X%Z/U_u@W4^Yd Z>UQCcb>W41XVkRCn W4Ur'UYX%^_\VQSWif`e
y'XVn RAW4^Z|X%ZCi  Q/]a]	n WmaVVVV8X%ZCi}PbCXVwW4^jaVVVVas*]ab/Wff^WXVn1R/^_TVf*n Wkd U]abCX%]8u9WlX%^_WffZ/TV]cfiTVZCi/d p
]	d TVZCd Z/\md Z~]ab/W8^d \Vb]0U_RCXVcfiWq94w7u9W8u9TV^_>d Z[X5n2X%^_\VW4^~U_TVR/bCd U]	d2c4X%]aWi/8U_RCXVcfiWsu0b/W4^_W{u@W]	X%VW ]ab/W
R/^_TV]aT1cfiTSnQ/U_Wife~ffTVZ]elad ZmDgXVkR*n W Vq2V@X%ZCi]ab/W;XVd2n W4^ad Z11XVkRCn W8Vq2V-d Z]aTjXVc4cfiTVQ/Z]	sgcfiTVZ/p
i/d ]	d TVZCd ZS\~iT`W4UiVWn2d tVW4^0]ab/W^d \Vb]X%Z/U_u@W4^q&BTVQ/\VbCn ejU_RgWX%gd Z/\Ss]abSWU_TVR/bCd U_]	d2c4X%]aWi6U_RCXVcfiW5cfiTVZ/Ud U]aU
TSwXVn2n]abSW{RgTVU_Ud fCn W{UWQ/W4Z*cfiW4U;TSw7W4tVW4Z]aU']ab*X%] cfiTVQCn2iffbCX%R/RgW4ZoawTV^;W41XVkRCn Ws/ubCX%]85TVZ`]eu@TVQCn2i
UX_e|d ZoWXVcObc4d ^cfiQCkU]	X%ZCcfiWs*TV^8u0bCX%]8]abSW5aXVd2n W4^ u@TVQCn2i~UXaezd ZoWXVcObc4d ^cfiQCkU_]	X%Z*cfiWasLu8d ]ab]ab/Wd ^
R/^_TVf*X%fCd2n2d ]eq + y"T	u@W4tVW4^s)u@TV^_1d Z/\d Zj]ab/W'U_TVR/bCd U]	d2c4X%]aWi U_RCXVcfiW'bCX%U-R/^_TVfCn WkU@]aTTSq&TV^&TVZ/W']abCd Z/\Ss
d ]d UhZ/TV]XVn uX_e`Uc4n WX%^6u0bCX%]8]abSWj^Wn W4t)X%Z]hR/^_TVfCX%fCd2n2d ]	d W4U~d Z[]ab/WU_TVRSbCd U_]	d2c4X%]aWilU_RCXVcfiW>X%^_WqVTV^
W41XVkRCn WsubCX%];d U9]ab/WR/^_TVf*X%fCd2n2d ]e6]abCX%]9]ab/WaXVd2n W4^9UXaeU&d2wg&X%ZCiff0X%^_W]aTfgWW4WcfiQ/]aWi/5fiZCiW4Wi/s
d Z5UTSkW c4X%U_W4UsAd ]d U"Z/TV]9W4tVW4Zmc4n WX%^-ub*X%]B]ab/W0Wn WkW4Z`]aU-TSw]ab/W8n2X%^\VW4^@URCXVcfiW X%^_Wq&5TV^_W4TtVW4^s`W4tVW4Z
ub/W4Z]abSWffWn WkW4Z]aUX%ZCij]abSWff^_Wn W4tX%Z`]8R/^_TVfCX%fCd2n2d ]	d W4UX%^_WffZ/T	u0ZCsg]ab/W6Ud 4WYTSwD]abSWffU_TVR/bCd U]	d2c4X%]aWi
U_RCXVcfiWjkffXaefgWcfiTSkWYX%Zod UU_Q/Ws1X%U0]abSWYwxTSn2n Tu8d Z/\ffW41XVkRCn WU_b/T	u0Uq
 e
C PQ/RSRATVUW;]abCX%]Xu9TV^n2iiVW4Ucfi^d fgW4UubCd2cObTSwCVV{RgW4TVRCn WbCXatVWjX6cfiW4^_]	XVd Zi/d UWX%U_Wq
 n/  p \[W
ru9TV^n2ioc4X%ZofAWlcbCX%^XVcfi]aW4^d 4Wilf`eX6]aQSRCn WffTSw-VV%UX%ZCi>%UsEu0b/W4^_Wff]ab/W5O]abzcfiTSkRATVZSW4Z`]Yd U
d d ZCi/d t1d2iQCXVnAbCX%U]ab/Wi/d U_WX%UWq  b/W4^_W~X%^_W~ &-- RATVUUd fCn W6u@TV^n2iUqQ/^]ab/W4^{U_Q/RSRATVUW ]ab*X%]{]ab/W
%X%\VW4Z]	d ZvQSW4U_]	d TVZvd UX>cfiTSkRSQ/]aW4^YUe`U_]aWk5qfiZCd ]	d2XVn2n es-]abSWX%\VW4Z]YbCX%UYZ/Td Z*wxTV^k5X%]	d TVZCsX%ZCi
cfiTVZ/Ud2iVW4^_U;XVn2nS &-- u9TV^n2iVU"W	QCXVn2n en2d VWn eq  b/WX%\VW4Z]9]abSW4Zff^_WcfiWd tVW4U{d ZCwTV^kffX%]	d TVZ5]ab*X%];d U'X%U_UQCkWi
]aTfgWh]a^_Q/W~X%fgTVQ/]ubCd2cOb>u9TV^n2id U{]ab/W~XVcfi]aQCXVngu@TV^n2i/q  bCd U5d Z*wxTV^k5X%]	d TVZ|cfiTSkW4UYd Z>]abSW~wxTV^kTSw
U_]	X%]aWkW4Z]aU{n2d VW%d ZCiSd tgd2iQ*XVn@d U9Ud2chTV^'d Z*i/d tgd2iVQCXVn6d U"b/WXVn ]abegTV^6%X%]n WX%U_]'RAW4TVR*n WbCXatVW;]ab/W
i/d U_WX%UWVq"XVcObU_QCcbjU_]	X%]aWkW4Z]c4X%ZfgWhd2iW4Z]	d /Wi8u8d ]abX;U_W4]9TSwRATVUUd fCn W'u9TV^n2iVUq-TV^&W4gXVkRCn Ws
]ab/W U_]	X%]aWkW4Z]l%X%]ffn WX%U_]ffRgW4TVRCn W bCX_tVW ]abSWi/d UWX%U_Wc4X%ZlfgWd2iW4Z]	d /WiYu8d ]ab]abSW8U_W4]TSwg]aQSRCn W4U
u8d ]abX%]5n WX%U_]5%UqYTV^Ud2kRCn2d2c4d ]es"X%U_U_Q*kWh]abCX%]]ab/WX%\VW4Z`]jd U\Sd tVW4Zd ZCwTV^kffX%]	d TVZUX_egd ZS\]ab/W
XVcfi]aQCXVnEu9TV^n2i}d Ud Z}U_W4]Vs-wxTV^8t)X%^d TVQ/UU_W4]aUqmPQSR/RgTVU_W~X%]U_TSkW5RgTSd Z`]8]ab/WX%\VW4Z]8b*X%U8fgW4W4Z
%D`,0-@,	ff
fi
fixO	E7			ff
fi
fix7fifffi	fix!#"%$&('*),+	-V.x0/%21ff
/	, 3 4A6 57x! 8 9ff:-; x=
 <>?,".
 @
6fi AVC
 BED44F#"$G&('*),+H8 	:-ff fi /fi ?ff	I	6 ?-a2
 fi V8 fi 	x
J /8 K1	L/MO, -8 NNOP?@%fi ,fi Qfi Vfi  /fi x0R :@ H ff R 6 %ff 	:- C fi V,	 
S%4 T1A% x,	 / DC<2? /U ?fi AVO	 W<( 	  Xfi ;?fifi ARU	IVY D
 :Z9
6 [#
 1fi fi ,.
 S,\
 T1	 ]x	 ^ ff	 x,
C:-X 1,	 x;; @/
 0
 x	 ffx 9/%R
 D+6 _D+8 CR
 fi 

`7a7a

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

]aTSn2ih]abCX%]-]ab/WffXVcfi]aQCXVnu9TV^n2id Ud Z} & ^i^i^i	4Fj7q  b/W4ZCsgXVw]aW4^iTSd Z/\~cfiTVZCiSd ]	d TVZCd Z/\Ss]ab/WffX%\VW4Z]bCX%UX
Q/ZCd2wTV^kR/^TVfCX%fCd2n2d ]eTVZo &Rk i^i^i k jSq
CQ/]bST	uiVT`W4U]abSWYX%\VW4Z`];VW4W4R~]a^XVcOTSw7]ab/W{u@TV^n2iU8d ]8cfiTVZSUd2iW4^_URgTVU_Ud fCn WY]8cfiW4^]	XVd ZCn eu8d2n2n
Z/TV]W4RCn2d2c4d ]	n en2d U_]{]ab/Wk51]ab/W4^_W~X%^W6Ud2kRCn e]aT`T}kffX%Zeq-Z/WhRgTVU_Ud f*d2n2d ]ed U{]abCX%]5d ]VW4W4R/U8]a^XVc
TSw7ubCX%]8d ]0bCX%UfgW4W4Z~]aTSn2i/]ab/W{RgTVU_Ud fCn Wu9TV^n2iVU8X%^_W{]ab/W4Z]ab/W{TVZ/W4UhcfiTVZ/Ud U_]aW4Z]0u d ]abubCX%]8d ]0bCX%U
fgW4W4Zm]aTSn2i/qYCQ/]]abCd U5n WXViVU]aT]u@TTVf`t1d TVQ/U{R/^TVfCn WkU8cb/WcOgd Z/\wxTV^5cfiTVZ/Ud U_]aW4ZCcfielu8d ]abmubCX%]ffd ]
bCX%U&fgW4W4Z]aTSn2ikffX_eYfgWbCX%^iSsAX%Z*id2wCd ]&bCX%U&fgW4W4Z]aTSn2m
i lm]abCd ZS\VUwxTV^8n2X%^\V.
W l"s^_WkWk0fAW4^d Z/\h]ab/Wk
XVn2nBkffX_efgWd ZCwWX%Ud fCn Wq8fiZUd ]aQCX%]	d TVZ/U{u0b/W4^_Wh]ab/W4U_W6]u9TRS^_TVfCn WkU5X%^d UWs"X%ZX%\VW4Z]YkffXaemZ/TV]fgW
X%fCn W{]aTlcfiTVZCi/d ]	d TVZX%RSR/^_TVR/^d2X%]aWn eq
11XVkRCn W~Vq2 RS^_T	t1d2iW4U{U_TSkW~kTV]	d t)X%]	d TVZzwxTV^u@TV^_1d Z/\}d Z>]ab/WhUk5XVn2n W4^s"kTV^W Z*XVd tVWffU_RCXVcfiWq1p
XVkRCn W4U6Vq2jX%ZCi~Vq2U_b/Tu]abCX%];]abCd U6d U0Z/TV]hXVn uX_e`U5X%R/RS^_TVR/^d2X%]aWq  bQ/UsEX%ZTVftgd TVQSUh	Q/W4U_]	d TVZd U
ub/W4Zd ]5d U5X%R/RS^_TVR/^d2X%]aWq{]]aQ/^_ZSU'TVQS]']ab*X%]']ab*d U5Q/W4U]	d TVZ|d UbCd \VbCn e^_Wn W4t)X%Z]5d Z]ab/WhU_]	X%]	d U_]	d2c4XVn
X%^_WX%U@TSwD	+o n	+8fia,,YjX%ZCq
i ph,/q&^d \Sd ZCXVn2n e6U_]aQCiSd Wiu8d ]abCd ZY]ab/W4U_W8cfiTVZ]aW4`]aU
aQSfCd ZCs*VVVVK
 rXau8d2is
 rd2cVW4es@VVVVasBd ]uX%U5n2X%]aW4^5wxTVQ/Z*iff]abCX%]ffd ]ffXVn U_TRCn2X_e`U5XwQ/ZCi/XVkW4Z]	XVn
^_TSn W{d Z6]ab/W-U_]	X%]	d U_]	d2c4XVnVu9TV^_8TVZl) n	 n	)/
'u t{n Wd ZfCX%QCkffsSVVVVaq&EQ*d2n2i/d Z/\TVZ6R/^_W4t1d TVQ/UX%R/p
R/^_TSXVcOb/W4UsAyBWd ]aaX%Z>X%ZCi5BQ/fCd ZaVVVVCR/^W4U_W4Z`]aWijX&ZSWcfiW4U_UX%^_e~X%ZCi{U_]
Q vc4d W4Z]'cfiTVZ*i/d ]	d TVZmwTV^@ub/W4Z
cfiTVZCi/d ]	d TVZ*d Z/\Yd Z6]ab/WjZCXVd tVW;U_RCXVcfiW{d U'X%R/R/^_TVR/^d2X%]aW;
q w"T	uXVi/X_e`U9]abCd UBU_TVpc4XVn2n Wi}
 x0y{z*	,,	//
V
 y"/|
 p0}cfiTVZ*i/d ]	d TVZd U6X%ZmW4U_]	X%f*n2d U_b/WiY]aT`TSnLd ZmU_Q/^tgd tXVnCX%Z*XVn e`Ud Uq awTV^T	tVW4^_t1d W4uUs1U_W4Wad2n2n2s
tX%Z~iW4V
^ ~EXVX%ZCsBTVfCd Z/Us)VVVVE
 w;d Wn U_W4Z*s/VVVVaq2W&W41XVkffd Z/W&]abCd U0cfi^d ]aW4^d TVZ~d Z TVQS^T	uZCs^X%]ab/W4^
i/d AW4^_W4Z];cfiTVZ`]aW4]	sAX%ZCiU_b/Tu]abCX%];d ]'X%R/RCn2d W4U"^X%]ab/W4^B^X%^_Wn eq&PVRAWc4d *c4XVn2n esVu@WU_b/Tu]abCX%]B]ab/W4^W8X%^_W
^_WXVn2d U_]	d2c9U_W4]a]	d ZS\VU&ub/W4^_W;]ab/W;UXVkRCn W0URCXVcfiWhd U-U_]a^QCcfi]aQ/^_WiYd ZjUQCcbXuXae5]abCX%]d ]{d Ud2kRgTVU_Ud fCn W;]aT
UX%]	d Uw
e Lr60sCX%ZCihu@WR/^_Ttgd2iVWjX5cfi^d ]aW4^d TVZ]aTjb/Wn R[iW4]aW4^kffd Z/W8ub/W4]ab/W4^0TV^;Z/TV];]abCd U d U']ab/Wc4X%U_Wq
W~XVn U_T\Sd tVW~X6A,)ficObCX%^XVcfi]aW4^d X%]	d TVZTSwD]abS
W rh cfiTVZ*i/d ]	d TVZCsgfe~\Sd t1d Z/\}X ^X%ZCiVTSkffd 4Wi
XVn \VTV^d ]abCk]abCX%]{\VW4Z/W4^X%]aW4UjXVn2nBX%ZCijTVZ*n eiSd U_]a^d f/Q/]	d TVZSU5wxTV^{u0bCd2c
b Lr6bSTSn2iUsg]ab/W4^W4f`elU_TSn t1d Z/\
X%ZTVRgW4ZR/^TVfCn WkvRgTVU_Wi fed2n2n)W4]8XVn2qDaVVVVaq
W ]ab/W4Z~Ub/T	u]abCX%]0]ab/W8Ud ]aQCX%]	d TVZd U0u@TV^_U_Wd2wS]abSWjd ZCwTV^kffX%]	d TVZiTW4U0Z/TV]8cfiTSkWd Z]ab/WYwTV^k
TSwDX%ZjW4tVW4Z`]	q0TV^-]abCX%]c4X%UWsUW4tVW4^XVn`\VW4Z/W4^XVn2d X%]	d TVZ/UTSw1cfiTVZ*i/d ]	d TVZCd Z/\hbCX_tVW;fgW4W4ZjR/^_TVRgTVU_Wi/;
q 7W4^_p
bCX%R/U0]ab/W{fgW4U_]&Z/T	uZ[X%^_
W  fi	YS//u 4W4A^_W4esEVVVV'aXVn U_T5Z/TuZX%=
U `L 
  %)
`
X%ZCim~/o ph4
 pyB	+ n	W
 9/fi7
 z
 yVU}``S~u t&QCn2n fCXVc1s*VVVV2
 LUd U_2
 X% ^s*VVVVPb/TV^WY
4TVbSZ/U_TVZCs-VVVVhaXVn U_Tm`ZST	uZX%U	fi,/47a
q 4W4A^_W4e|cfiTVZCi/d ]	d TVZ*d Z/\[d UX5\VW4Z/W4^XVn2d X%]	d TVZTSw
TV^i/d Z*X%^_emcfiTVZ*i/d ]	d TVZCd Z/\SDl0}Q/RBi/X%]	d Z/\d U8X\VW4Z/W4^XVn2d X%]	d TVZmTSR
w 4W4A^_W4emcfiTVZ*i/d ]	d TVZCd Z/\Sq
WYU_b/T	u]abCX%q
] W47^W4ecfiTVZ*i/d ]	d TVZCd Z/\SsEub/W4Z|X%RSRCn2d2c4X%fCn Ws@c4X%ZofgW6Q/U_]	d /WiQSZCiW4^5X%ZX%RSR/^_TVp
R/^d2X%]aW\VW4ZSW4^XVn2d X%]	d TVZzTSwB]ab/
W rhcfiTVZ*i/d ]	d TVZCqr6n ]ab/TVQ/\Vbd ]6bCX%U5fAW4W4ZX%^\VQ/Wi/sQSUd Z/\kTVU_]	n e
X%1d TSkffX%]	d2c5cObCX%^XVcfi]aW4^d X%]	d TVZ/Us*]abCX%]Yl0QSRiSX%]	d Z/\>aX%ZCib/W4ZCcfiW~XVn U
T 4W4A^_W4ecfiTVZCi/d ]	d TVZCd Z/\S8d Us
ub/W4ZX%RSRCn2d2c4X%fCn Ws1]abSW~/
^_WX%U_TVZ*X%fCn W6uX_em]aTQ/RBi/X%]aWhR/^_TVfCX%fCd2n2d ]e[UW4Ws1Wq \Sq2s@u LUd U_2
 X% ^s"VVVV
Pb/TV^W
 TVb/Z/U_TVZ*sVVVVaas9d ]5d U{u@Wn2nAZ/T	uZ]ab*X%]]ab/W4^_W~X%^_W6Ud ]aQ*X%]	d TVZ/U{ub/W4^W~X%R/RCn e1d Z/\>;
n WXViUh]aTmRCX%^XViTgd2c4XVn2sBbCd \VbCn e|cfiTVQSZ`]aW4^d Z]aQCd ]	d tVW^_W4UQCn ]aUayBQ/Z]aW4^sVVVVPWd2iVW4ZCwxWn2iSs@VVVV9tX%Z
^XVX%UU_W4ZCsDVVVVaq
W ).
 -	8 	|
 ph8R/^_TVfCn WkvtX%Z^XVX%U_U_W4Z*s/VVVVa4QCie5d Un TVU_]d ZX
 eS LTVZ/Ud2iW4^C]abS
  n/  p \[W

^_W4\Sd TVZ8]abCX%]9d U@iSd tgd2iWid Z`]aT]u9T;bCXVn tVW4Us)-n Q/W;X%ZCiWi-]aW4^_^d ]aTV^_esWXVcb TSwub*d2cbjd U@wxQ/^]ab/W4^9i/d t1d2iWi
d Z]aToyBWXViSQCX%^]aW4^_q
U LTSkRCX%Z`eX%^_WXX%ZCi>PWcfiTVZC
i LTSkRCX%Ze[X%^WXVq6rR/^d TV^d2sH4QCie[cfiTVZ/Ud2iW4^UYd ]
W	QCXVn2n en2d VWn eff]abCX%]9U_b/Whd Ud ZmX%Z`e6TSw]ab/W4UWhwTVQ/^'	QCXVi^X%Z`]aUq-PbSW8cfiTVZ`]	XVcfi]aU&bSW4^9TuZ5b/WXViSQCX%^]aW4^_U
fe8^XViSd TSsSX%ZCi6d UB]aTSn2im-c4X%Z* ]BfgW&U_Q/^_W-ub/W4^W&eVTVQX%^_Wq*4wVeVTVQlX%^Wd ZWi]aW4^^d ]aTV^_esV]abSWTgiSiU0X%^_W
V2&]ab*X%]"eVTVQX%^_W d Zm0
y LTSkRCX%ZeX%^_WXq2q2q2jr&]@]abCd U"RgTSd Z]B]ab/W;^XVi/d T{\Sd tVW4U-TVQ/]	q&l0~Q/RBi/X%]	d Z/\

`7a7

fi " |qxq}t~"}s1|

TVZ]abCd Ud Z*wxTV^k5X%]	d TVZon WXViU0]aT~X6iSd U_]a^d f/Q/]	d TVZu0b/W4^_W]ab/WRgTVU_]aW4^d TV^&RS^_TVfCX%fCd2n2d ]ejTSwSfgWd Z/\d Z}-n Q/W
]aW4^_^d ]aTV^ed U0\V^_WX%]aW4^;]ab*X%ZoVVVqBfiZCiW4WiSsAd2wLQ
y zbCXVihUXVd2iow7eVTVQ[X%^_W5d ZBWih]aW4^_^d ]aTV^_es/]ab/WT1i/iU
X%^_
W "6]ab*X%]8eVTVQX%^_Wld ZQ
y  cfiTSkR*X%Z`eX%^_WXlqq4q4Vs*]ab/W4ZzwxTV^XVn2;
n   Vs-XVc4cfiTV^i/d Z/\l]aT;
Q/RBi/X%]	d Z/\Ss]abSWRATVU]aW4^d TV^0R/^TVfCX%fCd2n2d ]ejTSw7fgWd Z/\~d Zo-n Q/W{]aW4^_^d ]aTV^_e>d U8XVn uX_e`U'\V^_WX%]aW4^']abCX%ZoVVVq
-^T	tVWX%ZCizy'XVn RAW4^Z aVVVV RS^_T	t1d2iWXzUTVR/bCd U_]	d2c4X%]aWi[URCXVcfiWlub/W4^_W[cfiTVZCi/d ]	d TVZCd ZS\\Sd tVW4U
ubCX%]d UX%^_\VQCX%fCn e>]abSWmkTV^Wmd Z]aQCd ]	d tVW}X%Z/U_u@W4^d Z]abSWQCieCW4Z/XVk5d ZRS^_TVfCn WkffsCZCXVkWn e}]abCX%]
d2wQ
y U_W4ZCiUXkW4U_UX%\VWTSwL]ab/WwxTV^k %d2wBeVTVQX%^Wmd ZWi]aW4^_^d ]aTV^_esL]ab/W4Z]ab/WjTgi/iVUX%^_e
W -
]abCX%]@eVTVQmX%^Whd Zm0
y cfiTSkRCX%ZeX%^_WXV0]ab/W4e
Z 4QCie1 U"RgTVU_]aW4^d TV^9R/^TVfCX%fCd2n2d ]e5TSwfAWd ZS\Yd ZYWXVcObTSw)]ab/W
]u@TQCXViV^X%Z`]aUd Zl-n Q/W^WkffXVd Z/U'X%]V%Sq&PWd2iW4Z*wxWn2iYaVVVVasU_]a^_W4Z/\V]ab/W4Z*d Z/\{^_W4U_Q*n ]aU"TSw1^d WiSkffX%Z
X%ZCiYPbCd2kTVZe~aVVVVasU_b/Tu9Wi]abCX%]@]ab/W4^_W d U/{U_TVR/b*d U_]	d2c4X%]aWi8U_RCXVcfiW6d Z5ubCd2cObmcfiTVZ*i/d ]	d TVZCd Z/\ u8d2n2n
\Sd tVW~]abSWUXVkW}X%Z/U_u@W4^X%Ul0d Z]abCd Uc4X%UWqaPW4W}XVn U_T|u rX_u8d2i/sVVVVaswxTV^5Ud2kffd2n2X%^ff^_W4UQCn ]aU
XVn TVZ/\l]ab/W4U_Wn2d ZSW4Uq2WYU]a^_W4Z/\V]ab/W4Zo]ab/W4U_W5^W4U_QCn ]aUfemU_b/Tu8d Z/\~]ab*X%]	sEW4tVW4Zd ZXc4n2X%U_U TSw&kQCcOb
Ud2kR*n W4^Ud ]aQCX%]	d TVZ/U6ub/W4^W
W 4W4A^_W4e>cfiTVZCi/d ]	d TVZCd Z/\lc4X%Z/Z/TV]fgW5X%R/RCn2d WiSas)Q/Ud Z/\~l0d Z]ab/WZCXVd tVW
U_RCXVcfiW cfiTV^_^_W4U_RgTVZCiU]aTjcfiTVZCi/d ]	d TVZCd Z/\jd Zff]ab/WU_TVRSbCd U_]	d2c4X%]aWiURCXVcfiW8d ZffW4U_UW4Z`]	d2XVn2n effTVZ*n e8]a^d t1d2XVnSc4X%UW4Uq
 S7  	/~}/ n	`,7 
 b/W4UW5^_W4U_Q*n ]aU ]	X%VW4Z]aTV\VW4]abSW4^6U_b/T	u]abCX%]~	/	fi
mO`|

j>o nY47fi	
 0+o 	+j~	 n	5

 ph+//,	%aq  bCX%]6d U]ab/WkffXVd Z
kW4U_UX%\VW{TSw7]abCd URCX%RgW4^q
Wff^WkffX%^_l]abCX%]	sBXVn ]ab/TVQ/\Vbo]ab/W4^_W~X%^_W~cfiW4^]	XVd Z}Ud2kffd2n2X%^d ]	d W4Us1TVQ/^^W4U_QCn ]aUYX%^_W~	QCd ]aW~i/d AW4^_W4Z]
d ZU_R*d ^d ]{wx^_TSk ]ab/Wu9Wn2n pZ/TuZ~^_W4UQCn ]aU&TSwrd2XVcfiTVZCd U6X%ZCiDX%fgWn2n1aVVVVaq  b/W4emcfiTVZ/Ud2iW4^_Wi6ub/W4Z
XRgTVU_]aW4^d TV^YR/^_TVfCX%f*d2n2d ]ecfiTVQCn2ifgWt1d W4u9Wi|X%U5]ab/W~^_W4U_QCn ]ffTSwcfiTVZCiSd ]	d TVZCd Z/\|XRS^d TV^ffR/^_TVf*X%fCd2n2d ]e
TVZU_TSkWn2X%^_\VW4^U_RCXVcfiWq CeuX_e|TSw cfiTVZ]a^X%U_]	s'u@WbCXatVW|XlS`Win2X%^\VW4^U_RCXVcfiWd Zk5d ZCiz]ab/W
U_TVR/b*d U_]	d2c4X%]aWiffURCXVcfiWVasCX%Z*iX%^_WYd Z]aW4^_W4U_]aWid Z~ub/W4Z}cfiTVZ*i/d ]	d TVZCd Z/\md Z]ab/W{ZCXVd tVW8U_RCXVcfiWYX%Z*ih]ab/W
U_TVR/b*d U_]	d2c4X%]aWi6U_R*XVcfiWYX%\V^_W4Wq
fi]5d UYXVn U_Tu@TV^_]ab>U]a^_W4U_Ud Z/\]abCX%]]ab/W~i/d U_]	d ZCcfi]	d TVZ}fAW4]u9W4W4Zo]ab/W6ZCXVd tVWX%Z*ij]ab/W6U_TVR/bCd U]	d2c4X%]aWi
U_RCXVcfiWd U"W4Z`]	d ^_Wn e Q/Z/^Wn2X%]aWi]aT]ab/W&R/bCd2n TVUTVR/bCd2c4XVnt1d W4u]abCX%]TVZ/W&bCX%UTSwR/^_TVf*X%fCd2n2d ]eX%ZCi'b/TuTVZ/W
U_b/TVQ*n2iiTYR/^_TVfCX%f*d2n2d U_]	d2chd ZCwxW4^W4ZCcfiWqTV^W41XVkRCn Ws7]ab/W R/^_TVf*X%fCd2n2d ]	d W4Uffd Zl]abSW5TVZ`]ey'XVn2n7R/Q/4n W
c4X%ZjfAW0tgd W4u@WiYX%U9]ab/W0R*X%^_]	d2c4d RCX%Z]	 U@UQ/fDWcfi]	d tVW0R/^_TVf*X%fCd2n2d ]	d W4UX%fgTVQ/]9]ab/W8n T1c4X%]	d TVZTSw]ab/W c4X%^X%ZCi
X%fgTVQ/]-ubCX%]{5TVZ`]eYu8d2n2nUXae5QSZCiW4^-ubCX%]{c4d ^cfiQCkU_]	X%ZCcfiW4U1XVn ]aW4^_Z*X%]	d tVWn es/]ab/W4emc4X%ZfgW;t1d W4u@WiX%U
%w^_WQSW4Z`]	d U_]	5R/^_TVf*X%fCd2n2d ]	d W4Us&d ZCwW4^_^_Wiowx^_TSkuX%]	cObCd Z/\l]ab/WffTVZ]e|y'XVn2nEU_b/T	uTVZ]aWn W4tgd Ud TVZwTV^
kffX%ZeYu@W4W4U X%ZCih]abSW4ZU_W4]a]	d Z/\ff]abSWR/^_TVfCX%fCd2n2d ]	d W4U;WQ*XVn)]aTffTVf/U_W4^tVWiwx^WQ/W4Z*c4d W4Uq  b/WRS^_TVfCn Wk
u@WYXVi/i^_W4UU&Tgc4cfiQ/^UfgTV]abowx^TSkXffw^_WQSW4Z`]	d U_]X%Z*iwx^TSkXU_Q/fEWcfi]	d tVW{U]	X%ZCcfiWq
 bSW ^W4U_]TSw1]abCd URCX%RgW4^ffd UTV^_\SX%ZCd 4Wi}X%UYwxTSn2n TuUq{ZPWcfi]	d TVZu@W~wTV^kffXVn2d 4W6]ab/WhZ/TV]	d TVZ>TSw
ZCXVd tVWX%Z*iU_TVR/bCd U]	d2c4X%]aWiU_RCXVcfiW4UqhZPVWcfi]	d TVZVs1u@WcfiTVZ/Ud2iW4^{]ab/W~c4X%U_WYub/W4^_Wh]ab/Wld ZCwxTV^kffX%]	d TVZ
cfiTSkW4Ud Zj]ab/W6wxTV^kTSwDX%ZW4tVW4Z]	qLWffiW4Ucfi^d fgW0]abSm
W Lr6cfiTVZCi/d ]	d TVZ>X%ZCiU_b/Tuz]abCX%]d ]{d U@tgd TSn2X%]aWi
d ZX \VW4Z/W4^XVn1U_W4]a]	d Z/\TSwEubCd2cOb]ab/W~ffTVZ]ey'XVn2nBX%ZCij]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W~X%^_W6U_RgWc4d2XVnBc4X%UW4Uq
fiZPVWcfi]	d TVZju@W6\Sd tVW6U_W4tVW4^XVncObCX%^XVcfi]aW4^d X%]	d TVZSU8TS!
w rh;qSW6UQ/R/RCn e}cfiTVZCiSd ]	d TVZ/UQ/ZCiW4^'ubCd2cOb
d ]ffd U\VQCX%^X%Z]aW4Wi]aTb/TSn2imX%ZCiY\VQCX%^X%Z]aW4WijZ/TV]]aTjbSTSn2i/sLX%ZCiffu@Wh\Sd tVW~X8^X%ZCiTSkffd 4WimXVn \VTV^d ]ab*k
]abCX%]8\VW4Z/W4^X%]aW4UXVn2n9X%Z*iTVZCn ei/d U]a^d f/Q/]	d TVZ/UjwxTV^ ubCd2cO
b Lr6b/TSn2iUqYfiZ|PWcfi]	d TVZ6u@WlcfiTVZ/Ud2iW4^
]ab/Wc4X%U_WubSW4^_W{]ab/Wd ZCwxTV^kffX%]	d TVZ[d U;Z/TV] d Z]ab/WjwTV^k TSw*X%ZlW4tVW4Z]	q@W8/^U_]8cfiTVZ/Ud2iVW4^0Ud ]aQCX%]	d TVZSU
ub/W4^
W 4W4A^_W4e>cfiTVZCi/d ]	d TVZCd ZS\mc4X%ZfAWjX%R/RCn2d Wi/q9WUb/T	u]abCX%
] W47^W4ecfiTVZ*i/d ]	d TVZCd Z/\md Z~]ab/W8ZCXVd tVW
U_RCXVcfiW5\Sd tVW4U]ab/W~X%R/RS^_TVR/^d2X%]aW~X%Z/Uu9W4^Yd Xh\VW4Z/W4^XVn2d 4W
i Lr6$cfiTVZCiSd ]	d TVZ}b/TSn2iUqW5]ab/W4Z>U_b/Tu
]abCX%]	s`]eRCd2c4XVn2n esDX%RSRCn egd ZS\l0d Z]ab/W'ZCXVd tVWU_RCXVcfiW6iTW4U@ZSTV]9\Sd tVW]ab/W6X%R/R/^TVR/^d2X%]aW6X%Z/U_u@W4^q*W
cfiTVZCc4n QCiVW{u8d ]abU_TSkWYi/d UcfiQ/UUd TVZTSw7]ab/W5d2kRCn2d2c4X%]	d TVZlTSw7]ab/W4U_W{^_W4UQCn ]aU8d ZoPWcfi]	d TVZ[Vq

`7a7

fib
9G

c edbgfb]fbz}zhcyz1

s1t yz|qu

C KG  FJ K"NNM/K  ?q

gN

-Q/^'wxTV^k5XVnSkT1iWn/d UX&U_RgWc4d2XVnSc4X%UW0TSw`]ab/W k0QCn ]	d pX%\VW4Z]-U_e`U]aWkU'w^XVkW4u@TV^_ay;XVn RgW4^_Zm/X%\Sd Z*s
VVVVasubCd2cObmd U@W4U_U_W4Z]	d2XVn2n eff]ab/W'UXVkWhX%U9]ab*X%]9Q/UWifeV^d Wi/kffX%Z>X%ZCiYy;XVn RgW4^_ZmaVVVVE]aTkT1iWn
fgWn2d Ww^_W4t1d Ud TVZCqW5X%U_U_QCkW']abCX%]&]ab/W4^W6d U&UTSkWW4`]aW4^ZCXVn`u9TV^n2id Z}X;U_W4
] vsgX%ZCiX%Z}X%\VW4Z]&ub/T
kffX%VW4UhTVf/U_W4^t)X%]	d TVZ/U TV^h\VW4]aUd ZCwxTV^kffX%]	d TVZzX%fgTVQ/]8]abCX%]u@TV^n2i/qWmc4X%ZiW4Ucfi^d fgWY]ab/W5Ud ]aQ*X%]	d TVZ
fe[XhR*XVd ^j  ^ asEub/W4^_
W  
 
d U8]ab/W~XVcfi]aQCXVnDu9TV^n2iSs"X%ZC
i &d U]ab/WX%\VW4Z]	 U+"sEubCd2cOb
W4U_U_W4Z]	d2XVn2n ecb*X%^XVcfi]aW4^d 4W4Ujb/W4^~d ZCwTV^kffX%]	d TVZC
q 
d UffubCX%]6u@W}c4XVn2n Wi]abSWZCXVd tVWU_RCXVcfiW}d Z]ab/W
d Z]a^_TgiVQCcfi]	d TVZCqTV^{]abSWffR/Q/^_RgTVU_W4UTSwE]abCd U{RCX%RgW4^sAu@WX%U_UQCkW6]abCX%W
] CbCX%U{]abSW~wxTV^
k  & ^ i^i^i^ jgas
ub/W4^#
W %-d UL]ab/W9TVfSU_W4^_tX%]	d TVZ6]abCX%]L]ab/WX%\VW4Z]0kffX%VW4U0X%]*]	d2kW-s  V^ i^i^i	6 l"q  b*d UC^_W4RS^_W4U_W4Z]	X%]	d TVZ
d2kRCn2d2c4d ]	n eX%U_U_Q*kW4U]abCX%]]ab/WX%\VW4Z]^_WkWk0fAW4^UW4tVW4^_e`]ab*d Z/\U_b/W bCX%UTVf/U_W4^tVWiUd ZCcfiW6b/W4^5n T1c4XVn
U_]	X%]aWlW4ZCcfiT1iW4UlXVn2n"]ab/WR/^W4tgd TVQ/UYTVf/U_W4^_tX%]	d TVZ/Uaq  b`Q/UsBu@Wd \VZ/TV^WkWkTV^ed U_U_Q/W4UYb/W4^_Wq|W
XVn U_T|d \VZSTV^_W}cfiTSkR/Q/]	X%]	d TVZ*XVn0d U_U_QSW4UsQ/U_]6U_TX%U5]aT>fgWX%f*n W]aT|wxT1cfiQ/U6TVZu0b/W4ZcfiTVZCi/d ]	d TVZCd ZS\|d U
X%R/R/^TVR/^d2X%]aWq
rvRCXVd ^8 h
  & ^ i^i^i^ %jga0d U8c4XVn2n WiX^_Q/ZCqCr^_Q/Z}kffX_ejfgWt1d W4u9WiX%U8XffcfiTSkR*n W4]aWjiW4Ucfi^d R/]	d TVZ
TSwgub*X%];bCX%RSRAW4ZSU0T	tVW4^]	d2kWd ZmTVZ/W RgTVU_Ud fCn W8W4`WcfiQS]	d TVZTSw1]ab/W U_eU_]aWkffq TV^Ud2kRCn2d2c4d ]esd Zm]abCd U
RCX%RgW4^su@WX%U_UQCkW@]abCX%]C]abSW9U_]	X%]aW-TSw]abSW9u@TV^n2ihiTW4U*ZSTV]-cObCX%Z/\VW-T	tVW4^]	d2kWq  b/WffU_TVR/bCd U_]	d2c4X%]aWi
U_RCXVcfiW5d U0]ab/WU_W4]0TSw*XVn2nRATVUUd fCn W^_Q/ZSUq
fiZ]ab/W55TVZ`]e}y;XVn2n)RSQ/4n Ws]ab/WZCXVd tVW{URCXVcfiW{bCX%U&]ab/^W4Wu9TV^n2iVUs^_W4R/^_W4UW4Z`]	d Z/\6]abSW]ab/^_W4WRgTVU_p
Ud fCn Wn T1c4X%]	d TVZ/U{TSwD]ab/W~c4X%^q  bSW8U_TVRSbCd U_]	d2c4X%]aWiU_R*XVcfiW~iW4Ucfi^d fgW4UubCX%]ff5TVZ`]eu@TVQCn2ijbCX_tVW6UXVd2i
d Z>XVn2ngc4d ^cfiQCkU_]	X%Z*cfiW4Uad2q Wq2s15TVZ]e1 U;7fi0X%U&u@Wn2ngX%U-ub/W4^_W;]ab/Whc4X%^d Uq  b/W0]abS^_W4W4pR/^d U_TVZSW4^_U
R/Q/4n W>d U6]a^_WX%]aWid ZiW4]	XVd2n&d ZDgXVkR*n WVq2jfgWn Tu8ql|b*d2n Wd Z]abSW4U_W>c4X%U_W4U6]ab/WU_TVR/bCd U]	d2c4X%]aWi
U_RCXVcfiW[d U5U_]	d2n2n^_Wn2X%]	d tVWn eUd2kRCn WsB]ab*d Uld U5ZST|n TVZ/\VW4^j]ab/Woc4X%U_W[wxTV^Y]ab/
W 4Q*ieCW4Z/aXVkffd ZRSQ/4n Wq
r6n ]ab/TVQ/\Vb]ab/WhZCXVd tVWhU_RCXVcfiW6b*X%U'TVZ*n ewTVQ/^Wn WkW4Z`]aUsBcfiTVZ/U_]a^_Q*cfi]	d Z/\]ab/WhU_TVR/bCd U_]	d2c4X%]aWiU_RCXVcfiWd Z/p
tVTSn tVW4UffcfiTVZ/Ud2iW4^d Z/\lXVn2n/]ab/W]abCd ZS\VU0]abCX%]h0
y cfiTVQCn2i6b*XatVW UXVd2i/su0bCd2cb[d UhwX%^ w^_TSk c4n WX%^sCX%ZCiff]ab/W
cfiTVZCi/d ]	d TVZSUQ/ZCiVW4^ubCd2cb0
y  UXaeUjX%ZemRCX%^_]	d2cfiQCn2X%^{]ab*d Z/\Sq~-^T	tVWlX%Z*i>y;XVn RgW4^_ZaVVVV8i/d UcfiQ/U_U
]ab/WYiSd vcfiQCn ]	d W4U8d ZocfiTVZSU_]a^_QCcfi]	d Z/\5U_QCcbXU_TVR/bCd U]	d2c4X%]aWiffU_R*XVcfiWq
fiZ\VW4Z/W4^XVn2s"Z/TV]6TVZCn ezd Ud ]6Z/TV]c4n WX%^5ub*X%] ]abSWU_TVR/bCd U_]	d2c4X%]aWiU_RCXVcfiW}d Us"f/Q/] ]ab/WZ/W4WiwTV^
X~U_TVR/bCd U_]	d2c4X%]aWiURCXVcfiWX%ZCi>]abSW[wTV^k d ]mk0Q/U_]j]	X%VWkffXae|fgWcfiTSkW[c4n WX%^TVZ*n eXVwx]aW4^]abSW[wXVcfi]	q
TV^W41XVkRCn Wsd Zm]ab/
W 4QCiVe}EW4ZSXVkffd ZmR/^TVfCn WkffsSfAWwTV^_WcfiTVZ`]	XVcfi]	d ZS\b/WXVi/	QCX%^_]aW4^_U
s QCieu@TVQCn2i
XVn2kTVU_]~cfiW4^]	XVd ZCn e}Z/TV]hbCX_tVWbCXViX5UTVR/bCd U_]	d2c4X%]aWimURCXVcfiW>d Zk5d ZCioW4tVW4ZX%U_U_QCkffd ZS\U_bSWYuX%U~X%Z
W4RAW4^]d ZjR/^_TVfCX%f*d2n2d ]e1as1X%ZCiYcfiTVQ*n2iZSTV]@b*XatVW`ZST	uZj]ab/WffwxTV^kd ]-u@TVQCn2i8bCXatVW]aTh]	X%VWQ/Z`]	d2nAXVw]aW4^
b/WX%^d ZS\6b/WXVi/	QCX%^_]aW4^ U;^_W4U_RgTVZ/U_Wq
fiZzX%Ze|c4X%U_Ws0d2w*]ab/WmX%\VW4Z]6bCX%UXffRS^d TV^8RS^_TVfCX%fCd2n2d ]e>TVZ[]ab/WYUW4
]  TSwLRgTVU_Ud f*n W5^_Q/ZSUjd Z[]ab/W
U_TVR/b*d U_]	d2c4X%]aWi[U_RCXVcfiWs XVw]aW4^b/WX%^d Z/\[TV^TVf/UW4^_tgd ZS
\  & ^ i^i^i^ %_asUb/Wc4X%Z$cfiTVZCi/d ]	d TVZCs;]aT[\VW4]oX
RgTVU_]aW4^d TV^'TV
Z mqTV^kffXVn2n esA]ab/WjX%\VW4Z]ffd UhcfiTVZ*i/d ]	d TVZCd Z/\b/W4^;R/^d TV^'TVZ]ab/W U_W4]'TSwA^_Q/Z/U;ub/W4^_Wb/W4^
n T1c4XVnU_]	X%]aWYX%]0]	d2kq
W d 
U  & ^ i^i^i^ _aq
n WX%^n e8]ab/W{X%\VW4Z]	 UBR/^_TVf*X%fCd2n2d ]
e 1^LTV
Z  d ZCiVQCcfiW4UX@R/^_TVfCX%fCd2n2d ]q
e D,^ TV
Z  fejkffX%^_\Sd Z*XVn2d X%p
]	d TVZCqEWX%^W'd Z]aW4^_W4U_]aWihd ZubSW4]ab/W4^E]ab/WX%\VW4Z`]&c4X%ZcfiTSkR/Q/]aW9b/W4^ERgTVU_]aW4^d TV^CTV
Z  XVw]aW4^*TVf/U_W4^_t1d Z/\
 & ^ i^i^i^ %4&d ZoX^_Wn2X%]	d tVWn eUd2kRCn WuXaesu d ]ab/TVQ/]bCX_tgd ZS\6]aT5u@TV^_md Z~]ab/W{U_TVR/b*d U_]	d2c4X%]aWi6U_R*XVcfiWq

 TVZ/Ud2iW4^&]ab/W{]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W5d ZokTV^_WYiW4]	XVd2n2q;yBW4^_W]abSW{ZCXVd tVW{U_R*XVcfiWYd U
 p \[We/ L
   `6464Vs7ub/W4^_Wd U]ab/Wu@TV^n2iYub/W4^_Wd U'ZSTV];W4WcfiQ/]aWi/q0WX%^W8TVZCn ed Z`]aW4^_W4U]aWi
d Z^_Q/Z/UYTSw{n W4Z/\V]ab Vs-U_Tl  Vq  b/W~UW4]
 TSw-TVfSU_W4^_tX%]	d TVZ/U>ubCX%]X%\VW4Z`]>c4X%ZzfgW]aTSn2i/d U
V  6% V4  6  VVq&y"W4^_W%  64 V{cfiTV^_^_W4URATVZ*iUB]aT{]ab/W;TVf/U_W4^_tX%]	d TVZ5]ab*X%]"Wd ]ab/W4^0TV^*u8d2n2n
/EfgW&W4WcfiQ/]aWijad2q Wq2s]ab/W0XVd2n W4^9UXae1d Z/\%*u d2n2nfgW&W4`WcfiQ/]aWiSVaUd2kffd2n2X%^n esA  6  {cfiTV^_^W4U_RgTVZCiU

 n/

`7a_

fi " |qxq}t~"}s1|

]aTff]ab/WXVd2n W4^;UXae1d Z/\[%@u8d2n2nfAWW4WcfiQ/]aWi/Vq  b/WU_TVR/b*d U_]	d2c4X%]aWiffURCXVcfiWYcfiTVZ/Ud U]aUTSw7]ab/WYwTVQ/^&^_Q/Z/U
``6aa4)64aa4`64aa4`64aui

w"TV]aW']ab*X%]9]ab/W4^Whd U-Z/T8^_Q/Z5u d ]abjTVfSU_W4^_tX%]	d TVZ464asUd ZCcfiW;]abSW0aXVd2n W4^&u8d2n2nZSTV]"]aWn2ng]abCX%]@b/W
u8d2n2n)fgWW4WcfiQ/]aWi/q
r6c4cfiTV^i/d Z/\h]aTh]ab/WU_]aTV^_es]abSW;R/^d TV^.D^, d Zj]abSW'Z*XVd tVW'URCXVcfiWbCX%UZ1^8h  VVhwTV^[vq
Z D^TVZ]ab/W{^Q/Z/U{d U0Z/TV]cfiTSkRCn W4]aWn eU_RgWc4d /WihfeY]abSW{U_]aTV^_eqLfiZRCX%^_]	d2cfiQ*n2X%^s
 b/W5wQCn2n1i/d U]a^d f/Q/]	d TV
u@WX%^W9Z/TV]L]aTSn2i;]ab/W-R/^_TVf*X%fCd2n2d ]e{u d ]ab u0bCd2cbh]ab/W-aXVd2n W4^*u d2n2n	UXaej9X%ZCih"d2w/&u8d2n2nZ/TV]CfgW@W4`WcfiQS]aWi/q
W^W4]aQ/^_Z]aTff]abCd U0RATSd Z]8d ZoDgXVkRCn W5Vq2Vq
 J
9G K e

I K"NNJ I

rvRCX%^_]	d2cfiQCn2X%^n eUd2kRCn W{UW4]a]	d Z/\md U;ub/W4^_W]ab/WYX%\VW4Z];TVf/UW4^_tVW4U0TV^6n WX%^_Z/U;]abCX%]0]ab/WW4`]aW4^_Z*XVnu@TV^n2i
d Ud Z[U_TSkWU_W4]
 vqVTV^6Ud2kRCn2d2c4d ]esu@WX%UU_QCkWj]ab/^_TVQ/\VbSTVQ/]8]ab*d U6RCX%RgW4^8]abCX%]h]ab/WX%\VW4Z]
kffX%VW4U{TVZ*n e~TVZ/WhTVf/U_W4^t)X%]	d TVZCs"X%ZCimkffX%VW4UYd ]ffX%]]ab/Wh/^_U_]'U_]aW4RTSwD]ab/Wh^_Q/ZCq  bQ/Us7]ab/WhU_W4=
] vTSw
RgTVU_Ud fCn W TVf/U_W4^t)X%]	d TVZ/U5cfiTVZSUd U_]aUTSw1Z/TVZSWkR/]e~U_Q/f/U_W4]aUTS!
w vq  bQ/Us*X%Z`e^_Q/
Z lc4X%ZmfgW8u0^d ]a]aW4Z
X%
U     
 
 a*ub/W4^_0
W d U-]ab/WhXVcfi]aQ*XVnu@TV^n2iX%ZCij d U{XZ/TVZSWkR/]e5U_Q/f/U_W4]9TSC
w vq-yBTu9W4tVW4^F
s 
iTW4U&Z/TV]&Z/WcfiW4U_UX%^d2n emcfiTVZSUd U_]&TSwg]ab/WZ/TVZ/WkR/]eYU_Q/f/U_W4]aU&TS
w vq-PTSkWU_Q/fSU_W4]aUkffXaejZ/W4tVW4^fgW
TVf/U_W4^tVWi/q-TV^9W41XVkRCn WsAd ZDgXVkR*n W8Vq2Vs7hd U9Z/W4tVW4^"]aTSn2i]abCX%]Bb/W0u8d2n2nfgW&W4`WcfiQS]aWi/sUTY 46 
d UZ/TV]&TVfSU_W4^_tVWi/q*WYX%UU_QCkW]abCX%]]abSW6X%\VW4Z]	 U'TVf/U_W4^_tX%]	d TVZ/U8X%^_W5XVc4cfiQS^X%]aWsDd Z]abCX%]d2wS]ab/WffX%\VW4Z]
TVf/U_W4^tVW4U $d ZX^_Q/
Z /s]ab/W4Z]abSWYXVcfi]aQCXVnu@TV^n2id 
Z d U d Zoq  bCX%]d Us)u@WYX%U_U_Q*kW]abCX%]8XVn2n/^_Q/Z/U
X%^_WYTSwE]abSWwxTV^
k     
 
 a0u0b/W4^_
W  q5Zz11XVkRCn WVq2Vs-XVc4cfiQ/^XVcfied U W4ZCwTV^cfiWifem]ab/W
^_W	QCd ^_WkW4Z]]abCX%]0^_Q/Z/U-bCX_tVW]ab/W5wTV^k   
    6 `
 aaq
 bSW8TVf/UW4^_t)X%]	d TVZTV^ffd ZCwTV^kffX%]	d TVZTVfS]	XVd Z/WimiT`W4UZ/TV]bCX_tVWh]aTjfgW W4gXVcfi]	n elTSwg]ab/WwTV^k ]ab/W
XVcfi]aQCXVn/u9TV^n2id U d Zo8VqBfi]U_
Q vcfiW4U]abCX%]hd ]8d U0W	QCd tXVn W4Z`];]aT5U_QCcObXU]	X%]aWkW4Z`]	q  b*d U8d U0]ab/Wjc4X%U_W
d ZfgTV]abm]ab/WffTVZ]ey'XVn2nAR/Q/4n WX%ZCiY]ab/W6]ab/^_W4W4pRS^d U_TVZ/W4^_UR/QS4n Wq5TV^{W41XVkRCn Ws"d Zl]abSW ]abS^_W4W4p
R/^d UTVZ/W4^_U R/Q/4n Ws*fgWd Z/\~]aTSn2i]abCX%]u8d2n2nEfgW5W4WcfiQ/]aWid U W4U_U_W4Z]	d2XVn2n eoW	QCd tXVn W4Z`]h]aTlTVf/U_W4^_t1d Z/\
   6   5Wd ]ab/W4^ 8TV^8@u8d2n2nZSTV]fgWW4`WcfiQ/]aWiSaq
fiZz]abCd UU_W4]a]	d Z/\Ss0u9Wc4X%Z$X%U_|ub/W4]ab/W4^sXVw]aW4^TVf/U_W4^_t1d Z/\s]ab/WX%\VW4Z`]}c4X%Z$cfiTSkRSQ/]aWmb/W4^
RgTVU_]aW4^d TV^TV
Z  f`ecfiTVZCi/d ]	d TVZCd Z/\TVZq TVQ/\Vb*n eU_RgWX%gd ZS\Ss/]abCd UffXVkTVQSZ`]aU]aTX%U_gd ZS\ju0b/W4]ab/W4^
TVf/U_W4^tgd Z/\ffd U]ab/W&UXVkWX%U;iSd UcfiT	tVW4^d ZS\{]abCX%]0d UB]a^Q/Wq  bCd U0kffXae Z/TV]fgW-]ab/W{c4X%U_W8d Z6\VW4Z/W4^XVn 
TVf/U_W4^tgd Z/\~TV^fgWd Z/\]aTSn2i kffX_ec4X%^_^_ekTV^_Wd Z*wxTV^k5X%]	d TVZ}]abCX%ZoQ/U]{]ab/WwXVcfi]]abCX%]j d U8]a^_Q/Wq
TV^W41XVkRCn WsLd2wBwTV^;U_TSkW8^_WX%UTVZ[hZ/TuU;]ab*X%];]ab/W aXVd2n W4^'u@TVQCn2i5Z/W4tVW4^UXaed2wAb/WcfiTVQCn2i5b/Wn R
d ]U_T]ab*X%]	s&d ZoRCX%^]	d2cfiQCn2X%^s&d2w&ffX%ZCiu8d2n2nEfgW5W4WcfiQ/]aWi/s*]ab/W4Zb/WYu8d2n2n9iVW4/ZCd ]aWn e>UX_e|asC]ab/W4Z
b/WX%^d ZS\m8ad2q Wq2sgTVf/U_W4^_t1d Z/\   6 4V9]aWn2n U5k0QCcbkTV^W8]abCX%Zm]ab/WwXVcfi]']ab*X%]']abSW8]a^_QSW8u@TV^n2imd U
TVZ/WTSg
w   TV0
^ %q]&UX_e`U;]abCX%]&]ab/W{]a^Q/Wu9TV^n2i~kQ/U]fgQ
W awTV^ d2wS]ab/W]a^_Q/Wu@TV^n2ihu@W4^_Z
W   s)]ab/W
aXVd2n W4^0u@TVQCn2ihbCX_tVWUXVd2iaq
fiZ8]abSW9^_Wk5XVd ZCiW4^*TSw]abCd ULRCX%RgW4^Eu@WX%U_UQCkW"]ab*X%[
]  d UL/Z*d ]aWq9TV^LW4tVW4^_e8UcfiW4ZCX%^d T'u@WcfiTVZ/Ud2iW4^
u@W5iW4/Z/W5X'U_W4]&TSwSRgTVU_Ud fCn WTVfSU_W4^_tX%]	d TVZ/
U s1cfiTVZSUd U_]	d Z/\ffTSwSZ/TVZ/WkR/]ejU_Q/f/UW4]aU&TS
w vq&TV^;\Sd tVW4Z
 X%ZC
i 8s]ab/W{U_W4]&TSw7^_Q/ZS[
U  d U0]abSW4ZoiW4/Z/Wi ]aT5fAW]abSWU_W4]

 

V aF06

>8i

{d tVW4ZTVQ/^X%U_U_QCkR/]	d TVZ/U]abCX%]&]abSW{U_]	X%]aWYiTW4UZ/TV]cObCX%Z/\VW{TtVW4^0]	d2kWjX%ZCih]abCX%]&]ab/W5X%\VW4Z] 5
k X%VW4U
TVZCn e|TVZ/WTVf/U_W4^_tX%]	d TVZCs;]ab/WmU_W4] TSw&^_Q/ZSUc4X%ZfgWmt1d W4u9WiX%UX~UQ/f/U_W4]TSw
8q 
 bCd2n W

`7a7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

Q/U_]]	X%1d Z/\ ]aTmfgWmX5U_Q/f/UW4]TSw
u@TVQCn2iUn2d \Vb]	n e}Ud2kR*n2d2wxe>]abSWYR/^W4U_W4Z`]	X%]	d TVZbSW4^_Ws&d Z
\VW4Z/W4^XVn2s1u9W~cfiW4^_]	XVd Z*n euX%Z`]]aT}XVn2n Tu$U_W	Q/W4ZCcfiW4UTSwETVf/U_W4^t)X%]	d TVZ/Uq5uLTVZ/Ud2iW4^swTV^W4gXVkR*n WsBX%Z
l1piTTV^0tVW4^_Ud TVZTSwA]ab/WjffTVZ]eoy'XVn2nSR/^_TVfCn Wkffsub/W4^_WffTVZ]eTVRgW4Z/U XU_W	Q/W4ZCcfiW8TSwiT`TV^Uq2  bCd U
w^XVkW4u@TV^_YW4]aW4ZCiU;ZCX%]aQ/^XVn2n ej]aT5]ab*X%]U_W4]a]	d Z/\Sq
b/W4Z/W4tVW4^@u9W0U_RgWX% TSwgX8i/d U_]a^d f/QS]	d TV
Z 1^9TVW
Z lsu9W d2kRCn2d2c4d ]	n eX%U_UQCkW&]abCX%]9]ab/WR/^_TVf*X%fCd2n2d ]e
TSw1X%Ze6U_W4]9TVZYubCd2cObYu@W cfiTVZCiSd ]	d TVZd U@U_]a^d2cfi]	n e5\V^_WX%]aW4^-]abCX%ZlV[
q ~/W4H
]   X%ZCZ
i WfAW;]u@T8^X%ZCiTSk
tX%^d2X%fCn W4U6TV
Z ls*ub/W4^
W W d Uh]ab/WXVcfi]aQCXVnCu9TV^n2i[X%ZC
i   d U ]abSWYTVf/UW4^_tVWiW4tVW4Z`]	q  bQ/Us-wTV^
     
 aaC
s W /   X%ZCW
i      8qh{d tVW4ZXYiSd U_]a^d f/Q/]	d TV
Z 1^TVZm^_Q/ZS
U ls7u@WiVW4Z/TV]aW
f
e D,^  ]ab/W}kffX%^_\Sd ZCXVn0i/d U_]a^d f/QS]	d TVZTS!
w W>s;X%ZCilf
e D^  ]abSW}kffX%^_\Sd ZCXVniSd U_]a^d f/Q/]	d TVZTSK
w   q
TV^'W4gXVkRCn WsCwTV
^ C4{
 vC
s 18^ }u 8;d U;U_bSTV^_]8wTVm
^ D^	 W 
  0X%ZC
i 1,^ >u fi;d U;Ub/TV^_]8wTV^
1^	   
   >8aq
~/W4.
] D^-fAW6X;R/^d TV^-TV
Z  X%ZCin W4Z
] 1,^   D^u    8EfgW']ab/WRgTVU_]aW4^d TV^{XVwx]aW4^&TVfSU_W4^_t1d Z/\q
 b/W5k5XVd ZoQSW4U_]	d TVZ~u@WYX%U_d Z]abCd U;RCX%RgW4^d UQ/ZCiW4^&u0bCX%]cfiTVZCi/d ]	d TVZ/U;u9WbCXatVW

1^  u 


 D^,>u 

aV

wTV^0XVn2n?vq  bCX%]d Usu@W&uX%Z`]]aTZ/T	uQSZCiW4^*ubCX%]cfiTVZCi/d ]	d TVZ/U"]ab/W-RATVU]aW4^d TV^ d Z*iQCcfiWi;f`e
1^ gc4X%ZfffgWcfiTSkR/Q/]aWi6w^_TSk]ab/W&R/^d TV^TVZ fecfiTVZCiSd ]	d TVZCd Z/\TVZ6]ab/W&TVf/UW4^_t)X%]	d TVZ*q@aDgXVkRCn WVq2
fgWn T	u\Sd tVW4UhXffcfiTVZ*cfi^_W4]aWjc4X%U_Wq2WU]a^_W4U_U0]ab*X%]1^8X%Z*i1^,X%^_Wji/d U_]a^d f/QS]	d TVZ/UTVZms)ubCd2n WD^,
X%ZCZ
i D^  X%^_W;iSd U_]a^d f/Q/]	d TVZSUDTV
Z  TVf/]	XVd Z/Wif`e6kffX%^_\Sd ZCXVn2d X%]	d TVZw^_TS
k D^9X%ZCZ
i D^ s	^_W4U_RgWcfi]	d tVWn e1aq
w"TV]aW]abCX%]8aV0d U0W	QCd t)XVn W4Z]	n eU_]	X%]aWiX%U

D^	W    

8

 1^W  W

>wTV^8XVn2n]>i

aV

aVW	QCd tXVn W4Z`]	n esLaVad Uhc4XVn2n Wi6]ab/WmrhcfiTVZCi/d ]	d TVZCVqB]8c4X%Z~fAW5cObCX%^XVcfi]aW4^d 4WimX%U8wTSn2n T	u0U . 
 e& z8@D	@oK||gG}Lo>7|+1^[
 :
	+/fffi6,)on
/



\b"a\

/h*

zO%}9q1^	   # ;	1^W       D^	W  W 8!	ff
 >m
zu}* 'ff	n /\W  9/_`	S	/*G'ffn		S\    	n		#W>mff&X >
Oz u}D
 ^   W     1^   W 8R	6?|)~V1^W 
 !
h
 
zO%}D ^   W  h
   1 ^   W    #	 m!h
 6    ,)	
 ^W  h
D
 !
 /qD ^	W    2
 
TV^'cfiTSkRCn W4]aW4Z/W4U_UaX%ZCi'fgWc4X%Q/U_Wd ]0d UQ/UWwxQCn)wTV^TVQ/^0n2X%]aW4^  b/W4TV^_Wk Vq2VasVu@W&R/^_Ttgd2iVWX@R/^_TTSw
TSw  b/W4TV^_Wk Vq2ffd Z]ab/WYX%RSRAW4Z*i/d gq
 bSW8/^U_] cfiTVZCiSd ]	d TVZd Z  b/W4TV^_WkVq2d U'Q/U_]haVaq  b/W ]abCd ^ilX%ZCimwxTVQS^_]ab[cfiTVZ*i/d ]	d TVZ/UQSU_]	d2wxe
]ab/WZ*XVkW%cfiTSX%^_U_W4Z*d Z/\X%]"^X%ZCiTSkffVqCfiZ`]aQ*d ]	d tVWn es/^_U_]U_TSkWu9TV^n2.
i  
  d U"^_WXVn2d 4Wi/sgX%ZCi]ab/W4Z

%CGff!D
fi1	Y4VfiYR!fififi?-%7RxXfi;4gfiFXxfiX9AAK	Ix[	VfiE/%fix%	18D`
_[  ,	ff x ,?<h7 X5XA _ O 	<(  %ff@A _[ M
 ?O 2K^x fi _[ x  ff9!4V fi2! Xx e<hA ^5XA
_[ E9o-7/	 Q fi U8 4-_ ff  I	x ff J 8NN P?9:;	fix  0	 <h  %ff@AV 	x x %'D _[ 	 fi44- 
 , 6: F ffHff 4D 8DM1T 8 fi:% x J VI  ?A X5 _ P? Y/ fi 2%@ x K	V fi ?D 8m
: E  fiV ,	ff ff@
 , 6: UI E fiK  J 8NN 8P J VI  F  ,	@ _ 
P 	R	V fi ff 2D fiX	ox 6#x 1 6 x %fiff,
`7a

fi " |qxq}t~"}s1|

U_TSkW%cfiTSX%^_U_W4Z*d Z/\kWcb*X%ZCd UkffliWc4d2iW4U6ub*d2cb[W4tVW4Z`]  U_Q*cb[]abCX%] vd U6^W4tVWXVn Wi
]aT]ab/WX%\VW4Z`]	q  b/WW4tVW4Z]5 d Uffc4XVn2n WiX%cfiTSX%^U_W4ZCd Z/\S8TSwUhq  b/W ]abCd ^ilX%ZCiwTVQ/^_]abcfiTVZCi/d ]	d TVZSU
W4AWcfi]	d tVWn eYUX_eff]abCX%]@]ab/W'R/^_TVfCX%fCd2n2d ]eff]abCX%;
] $d UcfiTSX%^U_W4Z/Wi8]aTvd U-]ab/W;UXVkW6wxTV^XVn2n| >8q  bCd U
kWX%Z/U;]abCX%]0]abSW%cfiTSX%^_U_W4Z*d Z/\lkWcObCX%ZCd Uk55d U'U_QCcb]abCX%]0]abSW{R/^_TVfCX%f*d2n2d ]eTSwATVf/U_W4^tgd Z/\ld U'Z/TV]
X%AWcfi]aWifffeY]abSW{U_RgWc4d Cc@tXVn Q/W{TS
w v>]abCX%]uX%U0^_WXVn2d 4WiSq
fiZ|]abSW^_Wk5XVd ZCiW4^jTSw-]abCd URCX%RgW4^s@ub/W4Zu@WUX_e 1^UX%]	d U/W4
U Lrh;VsBu@WkWX%Zz]abCX%
] 1^
UX%]	d U_SW4U5cfiTVZCi/d ]	d TVZ|aXV@TSw  b/W4TV^_Wk Vq2TV^s1WQCd tXVn W4Z]	n es"X%Ze~TSw1]abSW6TV]ab/W4^]ab/^_W4WcfiTVZCi/d ]	d TVZSU
	5
 8q  bQ/Us 1^ UX%]	d U_/W4
U Lr60~kWX%Z/U8]ab*X%]cfiTVZCiSd ]	d TVZCd Z/\d Z]ab/WYZCXVd tVWYURCXVcfi
W 
cfiTSd ZCc4d2iW4Uu8d ]abvcfiTVZCi/d ]	d TVZCd ZS\zd Z|]ab/W~U_TVRSbCd U_]	d2c4X%]aWioU_RCXVcfi
W  u8d ]abR/^_TVf*X%fCd2n2d ]eVq  bS
W Lr6
cfiTVZCi/d ]	d TVZYW4`R*n2XVd Z/U"ubecfiTVZCi/d ]	d TVZCd ZS\Yd Z5]ab/W0ZCXVd tVW0U_R*XVcfiW8d U"Z/TV]'X%R/R/^_TVR/^d2X%]aW8d Z5]ab/W ffTVZ]e~y'XVn2n
R/Q/4n WTV^6]abSW]ab/^_W4W4pR/^d UTVZ/W4^_U6R/Q/4n Wq}WocfiTVZ/Ud2iW4^6]ab/W]ab/^_W4W4pR/^d U_TVZ/W4^_U6R/QS4n W}d ZiW4]	XVd2n2{X
Ud2kffd2n2X%^hX%ZCXVn eUd U X%R/R*n2d W4U]aT5TVZ`]e}y;XVn2n2q
 eE fiZ}]ab/W5]ab/^_W4W4pR/^d UTVZ/W4^_URSQ/4n WsDu0bCX%]Yd U` U8R/^d TV^Yi/d U]a^d f/Q/]	d TVZ1^TVZlzfiZ
 n/  p \
11XVkRCn W6Vq2;u9WffX%U_UQCkWi]abCX%]9]abSW6k5X%^_\Sd ZCXVnAi/d U_]a^d f/QS]	d TVZeD^  TVZ d U@QSZCd2wxTV^kffqDr'RCX%^_]w^_TSk
]abCd UR
s D^6d UQ/ZSU_RgWc4d /Wi/
q wBTu U_Q/R/RgTVU_W{]ab*X%]66TVf/U_W4^_tVW4U5   6   Y	]abSW aXVd2n W4^UX_e`U5Vaqw'XVd tVW
cfiTVZCi/d ]	d TVZ*d Z/\>u@TVQCn2in WXVi~]aT|XViTVRS]6]ab/W}i/d U_]a^d fSQ/]	d TV
Z 1,^ }u fi   6   Vaq  bCd Ui/d U_]a^d f/Q/]	d TVZ
UX%]	d U_SW4
U 1,^ }   fi   6   V  VVVqPTVR/bCd U_]	d2c4X%]aWi|cfiTVZCi/d ]	d TVZCd ZS\n WXViUl~]aT|XViTVR/]6]abSW}i/d U_p
]a^d f/QS]	d TVZ 1,^   1^	u       6   VaqCelR*X%^_]5ai/@TSw  b/W4TV^_Wk Vq2VsDZCXVd tVWmcfiTVZCi/d ]	d TVZCd ZS\od U
X%R/R/^TVR/^d2X%]aW>ad2q Wq20
s D^   1,^ }u fi   6   VaTVZ*n ed2w]abSWjaXVd2n W4^d U6W	QCXVn2n en2d VWn e[]aTUX_ejd Z
fgTV]ab>u@TV^n2iZ
U   X%ZC
i   qP/d Z*cfiW6]ab/WffaXVd2n W4^jk0Q/U_]UX_e]abCX%]Y;u8d2n2ngfgW6W4WcfiQ/]aWi}d Z>u@TV^n2
i   s9d ]
wTSn2n T	uU{]ab*X%
] D^ W   `6 
    4  Vq  b`Q/Us*cfiTVZ*i/d ]	d TVZCd Z/\}d UffX%R/R/^TVR/^d2X%]aWhTVZCn ed2w
]ab/WjXVd2n W4^ U RS^_TV]aTgcfiTSn-d U U_QCcObo]abCX%]hb/WmiW4/ZCd ]aWn e}UXaeUffd 
Z   s&d2q Wq2sLW4tVW4Zd2w*fgTV]abzYX%ZCioYX%^_W
W4WcfiQ/]aWi/q&CQ/];d2w`]abCd Ud U"]ab/W8c4X%UWsubSW4Zff]ab/WaXVd2n W4^"UX_e`UsScfiTVZCiSd ]	d TVZCd Z/q
\ D,^ TVZl   6 4{d US
X%R/R/^TVR/^d2X%]aWsVUd ZCcfiW0]ab/W4Zm'`ZST	uU9]abCX%]"b/W0u8d2n2nVfgWW4WcfiQ/]aWi/q  bSWu9TV^n2i5c4X%Z/Z/TV]9fgV
W   s7wxTV^9]ab/W4Z
]ab/W'XVd2n W4^-u@TVQCn2i8bCXatVWUXVd2ijq  b/W4^_WwTV^_Ws1/W
 ph"
 	+	
  B7fi,EasAcfiTVZCi/d ]	d TVZ*d Z/\
d Z]ab/WZCXVd tVWU_R*XVcfiWYc4X%Z/Z/TV]8cfiTSd Z*c4d2iWu8d ]ab[cfiTVZCi/d ]	d TVZCd ZS\d Z]ab/W{UTVR/bCd U_]	d2c4X%]aWi5U_RCXVcfiWYwTV^0fgTV]abTSw
bCd U^W4U_RgTVZ/U_W4Uq
b WwTSn2n T	u8d Z/\W41XVkRCn W6Ub/T	uU]abCX%]5d Zm\VW4Z/W4^XVn2sBd ZmU_W4]a]	d Z/\VU{TSwD]ab/Wh]eRAWX%^d Ud Z/\>d Zl]abSW5TVZ]e
 /
y'XVn2n7X%ZCi]ab/W;]ab/^_W4W4pR/^d UTVZ/W4^_U"RSQ/4n Ws`]ab/WLr6zcfiTVZCi/d ]	d TVZc4X%ZTVZCn e6fgWUX%]	d U_/Wijd ZjtVW4^_e6U_RgWc4d2XVn
c4X%U_W4U
 e
g PQ/RSRATVUW&]abCX%]#  V & 4 + Vs7X%Z*ifgTV]abl & X%ZCiY + X%^_W;TVf/U_W4^_tVWiu8d ]abjRgTVUd ]	d tVW
 n/  p \
R/^_TVf*X%fCd2n2d ]eq  bCd UYd U{]ab/W~c4X%UWwxTV^fATV]abffTVZ]ey'XVn2n"X%ZCij]ab/W6]abS^_W4W4pR/^d U_TVZSW4^_U{R/Q/4n Wq2  b/W4Z
]ab/
W LrhcfiTVZCi/d ]	d TVZ   b/W4TV^Wk Vq2Vac4ac4X%Z/Z/TV]Yb/TSn2iwTV^5fgTV]abv & X%ZCi| + QSZCn W4U_UD^	W

 &k  + hd U6Wd ]ab/W4^YTV^~VqTV^5U_Q/R/RgTVU_WY]abCX%]1^    &  Vs1^    +  Vs;X%ZCi
  D^	 W  &Kk  + 

 Vq8vd ]ab/TVQS]Yn TVU_U{TSwE\VW4Z/W4^XVn2d ]esE]abSW4^_W~d U{U_TSkW
W  &  &  + X%ZCi
 + [ &!k  + U_QCcbl]abCX%=
] 1^	 W   & 
  jX%ZC
i 1^ W   + [
  Vq P/d ZCcfiWTVfSU_W4^_tX%]	d TVZ/U5X%^_W
XVc4cfiQ/^X%]aWsAu@WkQ/U];bCX_tV
W D^ W   &     &   Vq42
w Lr6|b/TSn2iU6wxTV^ff & s7]ab/W4Zlu9Wk0Q/U_]
bCX_tV
W 1^	     & W   +   VqCQ/] ]ab/W4
Z 1^     + W   +   VqCQ/]hUd ZCcfiW
1^	     + [
  VsLd ]6wTSn2n T	u0U]abCX%];]abSW4^_Wd UU_TSk
W  .  + U_QCcObm]abCX%m
] D^ W   . [
  X%ZCi
1^	     + W   . 2 Vq  bCd U cfiTVZ`]a^XViSd2cfi]aU;]ab/W
W rhcfiTVZCi/d ]	d TVZCq
PTYub/W4ZiVT`W4
U Lr6bSTSn2i/  b/WR/^W4tgd TVQ/U'W41XVkRCn WW4bCd fCd ]aWimXYcfiTSkfCd Z*X%]	d TVZTS2
w X%Z*
i 
wTV^
ubCd2cObLr6c4X%ZTVZCn ejfAW'UX%]	d U_SWid Z%iW4\VW4Z/W4^X%]aW5c4X%U_W4UqLfiZ]ab/WZ/W4]&U_Wcfi]	d TVZCs)u@WU_bCXVn2nU_]aQCiVe
]abCd U8	Q/W4U_]	d TVZ[wxTV^ X%^_fCd ]a^X%^_ecfiTSk0fCd ZCX%]	d TVZ/U'TS
w X%ZC
i vq

`7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

 
9G 1	"NN I  e

fiZo]abCd U U_Wcfi]	d TVZCsu9WjR/^_Ttgd2iWYU_TSkWcbCX%^XVcfi]aW4^d X%]	d TVZ/UffTSwLu0b/W4Z}]ab/WeLr6cfiTVZCi/d ]	d TVZ[b/TSn2iUs-wTV^
/ZCd ]a
W X%ZCivq@&Q/^B^_W4U_Q*n ]aU"W4]aW4ZCi{WX%^n2d W4^9^_W4UQCn ]aU"TSw1{d2n2n2stX%ZliW4^#~EXVX%ZCsAX%ZCiffBTVfCd Z/U'aVVVVaq
WS^_U_]W4bCd fCd ]8X{Ud2kRCn WUd ]aQCX%]	d TVZd Z~ub*d2c
b rhd U0\VQCX%^X%Z`]aW4Wi5]aT5b/TSn2iSsEX%ZCihu@WUb/T	u]abCX%]
]abCd Ud UB]ab/WTVZCn ehUd ]aQ*X%]	d TVZmd Zffu0bCd2cbd ]d UB\VQCX%^X%Z]aW4Wi]aTb/TSn2i/qEW0]abSW4Z5U_b/Tu]abCX%]	sAwxTV^'X%^_fCd ]a^X%^_e
 X%ZC
i vs7u9Wc4X%ZcfiTVZ/U_]a^_Q*cfi]6X%p VtXVn Q/Wi>kffX%]a^d owx^TSk$u0bCd2cbX8U_]a^_TVZ/\Z/WcfiW4UUX%^_eocfiTVZCiSd ]	d TVZ
wTV
^ Lr6v]aT>b/TSn2ic4X%ZfAWTVf/]	XVd ZSWi/q]6]aQS^_Z/U6TVQ/]6]abCX%]	s'd ZU_TSkW}c4X%U_W4U5TSw'd Z`]aW4^W4U_]	#
s Lrh d U
^_TVQ/\Vb*n eU_RgWX%1d Z/\S@\VQCX%^X%Z]aW4WiS]aTb/TSn2iW41cfiW4R/]Yd Z%iW4\VW4Z/W4^X%]aW6Ud ]aQCX%]	d TVZSUqLd ZCXVn2n es1u@W
d Z]a^_TgiVQCcfiW5X'Z/W4uR/^_T1cfiWiQ/^XVn26cbCX%^XVcfi]aW4^d X%]	d TVZlTS
w Lr60u@WR/^_Ttgd2iW5X6kWcbCX%ZCd UkvU_QCcOb]abCX%]
X i/d U_]a^d f/QS]	d TV
Z 1^c4X%ZjfAW;]ab/TVQ/\Vb]9TSw1X%UX%^d Ud ZS\w^_TSk]ab/W kWcObCX%ZCd Ukd2w1X%ZCi{TVZCn ed2U
w D^"UX%]	d U_SW4U
Lr60q

:e

\ l!#" g

kn a nql ^`\b\ r

^`



 p%r

W /^_U_]8cfiTVZSUd2iW4^']ab/W8TVZ*n eUd ]aQCX%]	d TVZmub/W4^_WqLrhvd U'\VQCX%^X%Z]aW4WiY]aTYb/TSn2iSEd2w7]ab/W U_W4]aUhd ZX%^_W
RCXVd ^_u d U_W5i/d U_TSd Z`]	q

:e/ * x0yS
E,,	)/QD^ff
/{6;`,5(/-,_'6

[<a"ZW " ^ "  l

  //+5 

bCX%]bCX%R/RgW4Z/U6d2w1]abSW6U_W4]aU5d 
Z X%^_WhZ/TV]RCXVd ^u8d U_Wi/d U_TSd Z`]	r'^_Wh]ab/W4^_WhU_]	d2n2nc4X%UW4UYacfiTSkf*d p
ZCX%]	d TVZ/U;TSw8sUvs1X%ZCii/d U_]a^d f/Q/]	d TVZ/U&TVZmLub/W4Zrhb/TSn2iU  b/W4^_WffX%^_Wsf/Q/]&]ab/W4eX%^_WY	QCd ]aW
U_RgWc4d2XVn2q

:e

\ l!#$

n&%



 p%r

WZ/T	uzR/^W4U_W4Z`]X n WkffkffX0]abCX%]-R/^_T	t1d2iW4UX;Z/W4uvcObCX%^XVcfi]aW4^d X%]	d TVZTSwCLr6d ZY]aW4^kU-TSwDXUd2kRCn W
VV%pkffX%]a^d 1q  b/Wn WkffkffXXVn2n TuUQ/U*]aT6iVW4]aW4^kffd Z/WwxTV^kffX%ZeYcfiTSk0fCd ZCX%]	d TVZ/U"TSw]X%ZCisu0b/W4]ab/W4^
Xffi/d U]a^d f/Q/]	d TVZTV
Z  W41d U_]aU0]abCX%]0UX%]	d U_/W4m
U LrhX%ZCih\Sd tVW4UhcfiW4^_]	XVd Zu@TV^n2iU0RATVUd ]	d tVW{R/^_TVfCX%f*d2n2d ]eq
Ld ~X&UW4!
] TSw^Q/Z/Usu0b/TVU_Wu@TV^n2iUX%^_W8d Z5UTSkW0/Z*d ]aWU_W4Q
]  X%ZCi{ub/TVU_W0TVf/U_W4^_tX%]	d TVZ/UcfiTSkW
w^_TSkU_TSkWSZCd ]aW-U_W40
]   V & ^ i^i^i4 j VqCWUX_eh]abCX%(
] '  d U0X%W
Z fi|
 p^Wn2X%]	d tVW0]aW
T  X%ZCi
 *fisEX%ZCi 5
 |
 W 9
d2)w 'b*X%U0]ab/WYwTV^k  &Rk i^i^i k _jSs/ub/W4^_WWXVcO
b &*-d U;Wd ]abSW4^ +
 **TV^ (
''q
  ,#
q ~/W4(
] .  /
 ' & ^ i^i^i	 '10fgW{]abSW{U_W4]0TSg
w YpX%]aTSkU;^Wn2X%]	d tVW ]a
T  X%ZC
i 8qBWc4X%Z~]abCd Z/
TS2
w . X%U5XR*X%^_]	d ]	d TVZTSw1]ab/W u@TV^n2iU5XVc4cfiTV^i/d ZS\j]aTu0bCX%]hc4X%ZfgW TVf/U_W4^_tVWiSq  u9Tu@TV^n2i.
U  & X%ZCi
 + X%^_Wd Zl]abSW8UXVkW UW43
] '1*@4
 . d2w1]abSW4^_WX%^W ZSTYTVf/UW4^_t)X%]	d TVZSU]abCX%]ffi/d U_]	d Z/\VQCd Ubl]abSWkffA]abCX%]ffd Us
]ab/W4^_Wd U'Z/TYTVfSU_W4^_tX%]	d TVZ$
 UQCcbl]abCX%#
]  & X%ZCW
i  + [
 
q r"W4SZ/W]ab/6
W 5lkffX%]a^d 8
 7
u8d ]abW4Z]a^d W4:
U 9*(X%U wxTSn2n TuU

9*( 




2d w)'*R
TV]ab/W4^_u d U_Wq

aV

W6c4XVn2n;7~]ab/WxQyB	%,/ph,awTV^#vX%ZCivaq;w"TV]aW']abCX%]9WXVcObY^T	u-d Z<7}cfiTV^_^W4U_RgTVZCiU
]aTXQ/Z*d2Q/W X%]aTSkd 
Z .u@Whc4XVn2n`]abCd U@]ab/WhX%]aTSk ,%`SS8]aT8^T	u,q  bCd UkffX%]a^d ~aXVcfi]aQCXVn2n es
d ]aU0]a^X%Z/U_RgTVU_W*uX%U0/^_U]d Z]a^_T1iQCcfiWifSQ/]wxTV^ Xffi/d AW4^_W4Z`]0R/Q/^_RgTVU_WCfem{d2n2n/W4]8XVn2qDaVVVVaq

`7&=

fi " |qxq}t~"}s1|

 n/

W4]aQS^_ZCd Z/\6]aTDgXVkRCn WYVq2Vs)]ab/WqLrh;XVcfi]aW4^d d Z/\mkffX%]a^d >d U\Sd tVW4Z~fe

e/

 p \














ub/W4^W ]abSW~cfiTSn QCkZ/UYcfiTV^^_W4U_RgTVZCiY]aTo & X%ZCi} + X%ZCij]ab/W6^_TuUYcfiTV^_^W4U_RgTVZCiY]aT]ab/W6]ab/^_W4W~X%]aTSkU
 &   + 4 & k  + X%ZCi +   & qTV^ffW41XVkRCn WsB]ab/W>wXVcfi]5]abCX%]hW4Z`]a^_e>9 .0& TSw"]abCd U~kffX%]a^d zd U
d ZCi/d2c4X%]aW4U;]abCX%]8 & c4X%Z/Z/TV]fgWTVf/U_W4^tVWid2w7]ab/W5XVcfi]aQCXVnu@TV^n2im d U8d Zo +?  & q
fiZ]ab/WYwTSn2n T	u8d ZS\ln WkffkffXVs?AC@ B iW4ZSTV]aW4U0]ab/W{]a^X%Z/U_RgTVU_WTSwS]ab/WY^_Tu8tVWcfi]aTV^4A @ sDX%Z*i ff
 @ iW4Z/TV]aW4U;]ab/W
^_TutVWcfi]aTV^hcfiTVZSUd U_]	d Z/\YTSwCXVn2nD%Uq
     	"G ,)/'|n	|	,	n	/0
 E 	g
of
 n eE
xQyB	,/ph%o;. 
 /

Dh\

/j%
Q*/j
F7,8

zO%} E 	;D^qffSj,,	)}|n	[

/~
17  ,ffp ,Y	/	j	+	/fi|p
7&,%`SS}}|pG'1^W H''  JILK-/ln	,	
A @   A & ^i^i^i	 A jS	,	/ A   1^   FW 	h WD^W zF#0S
K 5x0yV	N7  2A @ B   @ B 
A   ,ff_	9  2i^i^iul;#9D^ff,M-

zu} E 	7  ,6p ,ff//ffGff4	,	@Ghfffi{667HS
2O QP RTS 6ff9G
,,	 )/{|n	#
,	`%@,%`SSYU7 WV ffo

O QP RTS  / XgXg>Y''K Z'$,%`Shjfiz[7  i
@ )	17  A_@ B   @ B 9	]?	{XVn2n`XE aO QP R&S "
9ff	fij7	n	,	\A^
@ ] ~
ff	Y>%o1^l|n	Z e1^,  E
X  zffophS-GD^m 
XE})z^}1^,MK xQy/zu}D^	   FW F	  A 6S
q1^	



>

 ! 

w"TV]aW]abCX%]0fCd UBW4UU_W4Z`]	d2XVn2n eXcfiTVZtVW4^_U_WTSwAaXVaq*rZCX%]aQ/^XVn	Q/W4U_]	d TVZ6]aTYX%Ud UubSW4]ab/W4^0fCAu@TVQCn2i
U_]	d2n2nb/TSn2i d2wu@W9^_W4R*n2XVcfiWi~%wxTV^XVn2nbXg !O Q
 
 P R&S ]abSW4^_W9W41d U_]aUD ^CUX%]	d Uwxe1d Z/\L rhlu d ]abD ^8  XE>
fe%wTV^5XVn2ni/d U_]a^d f/QS]	d TVZ/UfiXg
 T	tVW4^
 ]ab/W4^_WhW4gd U]aU1 ^UX%]	d Uwe1d Z/\L r6u8d ]ab1 ^,  Xg&
 q2  b/W

X%Z/U_u@W4^8d U;Z/TSU_W4WYDgXVkR*n W{Sq2VfCaad2d2aq
~/WkffkffXYSq2jUXaeU6]abCX%]Xmi/d U_]a^d fSQ/]	d TVZ1^h]abCX%]6UX%]	d U_SW4ULrh X%Z*iX%]6]abSWjUXVkW]	d2kWbCX%U
1^	   c
 ''
 YwxTV
^ 5 iSd 7W4^_W4Z]6X%]aTSkfi
U ' c4X%ZlW41d U_]6d2wX%ZCi6TVZ*n eod2wX5cfiW4^_]	XVd ZmU_W4]'TSw_5 n2d Z/WX%^
W	QCX%]	d TVZ/U'd W
Z lQ/Z/Z/T	uZSU*b*X%U'X@U_TSn Q/]	d TVZ*q*Z~kffX%ZehUd ]aQCX%]	d TVZ/U9TSw7d Z]aW4^_W4U_]	d
s 5 ] l}Z/TV]aW0]abCX%]e5
j
kffX_ejfgW5X%Uhn2X%^_\VWYX%U   Va
q w"TV]U_QS^_R/^d Ud ZS\Sn e5]ab/W4ZCsEd ZU_QCcObUd ]aQCX%]	d TVZ/U;]ab/W4^_W{TSw]aW4Zc4X%ZfgW5S
i/d U_]a^d f/Q/]	d TV
Z D^]abCX%]0UX%]	d U_/W4
U rh;sDX%U0u@W{U_b/Tud Z]abSW{Z/W4]U_Q/f/UWcfi]	d TVZCq0-Z~]ab/W{TV]ab/W4^;bCX%ZCi/s
@ iTW4U9bCX_tVW X0U_TSn Q/]	d TVZd g
d2w]ab/W;U_W4]"TSwW	QCX%]	d TVZ/f
U 7  AC@ B  8
Z A @ s]ab/W4Zj]ab/W;U_W4]"TSw1XVn2nU_TSn Q/]	d TVZ/U{wTV^kU
j X%ZCi~]ab/W5RgTVUd ]	d tVWjTV^_]abCX%Z]
]ab/Wmd Z`]aW4^_UWcfi]	d TVZTSwX%ZzX v5Z/W5U_QSf/U_RCXVcfiWad2q WqX6beRAW4^RCn2X%Z/W&TSw
j
h V i q  b/W4U_W"UTSn Q/]	d TVZ/U&X%^_W"Q/U_]D]abSW'cfiTVZCi/d ]	d TVZ*XVnR/^TVfCX%fCd2n2d ]	d W4[
U 1^	    
 ;W}F
 	CwxTV^-XVn2n
i/d U_]a^d f/Q/]	d TVZ/U&wxTV^*ubCd2cO
b rhb/TSn2iU*]abCX%]*bCXatVW@U_Q/RSRATV^]9cfiTV^_^_W4U_RgTVZCiSd Z/\]a3
T 7  q  b/W4UWcfiTVZCiSd ]	d TVZCXVn
R/^_TVf*X%fCd2n2d ]	d W4U5kffXae]ab/W4ZmfgW8W4]aW4ZCiWiY]aT>Xi/d U_]a^d f/QS]	d TVZlTtVW4.
^ feU_W4]a]	d Z/
\ 1^   XE wTV^ffX%Z
X%^_fCd ]a^X%^_eli/d U]a^d f/Q/]	d TV^
Z XET	tVW4^;]ab/Wu9TV^n2iVUd Z}X%]aTSkU8cfiTV^_^_W4URATVZ*i/d Z/\h]a[
T 7  1XVn2U
n 1^cfiTVZ/U_]a^QCcfi]aWi
d Z]abCd U0uX_eUX%]	d Uwe
e rh;q

`77`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

PQ*kffkffX%^d d Z/\Ss/u9W8bCXatVW ]ab/W8^_WkffX%^_X%fCn WjwXVcfi]0]ab*X%]hwTV^ X%Ze\Sd tVW4ZmU_W4]0TSwX%]aTSkU. ]ab/W4^_WX%^_W
TVZCn e8]u@TRATVUUd fCd2n2d ]	d W4UEWd ]ab/W4^/6i/d U_]a^d f/QS]	d TVZ W41d U_]aUBu0bCd2cb6bCX%UD^	W <''!wTV^0XVn2nj'.
X%ZCiUX%]	d U_/W4
U Lr60sDTV0
^ 	j'iSd U_]a^d f/Q/]	d TVZS
U XE TtVW4^ u@TV^n2iUcfiTV^_^_W4URATVZ*i/d Z/\]aTX%]aTSkUd Zg''s
]ab/W4^_W W41d U_]aUffXYi/d U]a^d f/Q/]	d TVZlUX%]	d Uwegd ZS
\ Lr6|u8d ]abkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/]	d TVZTtVW4^u9TV^n2iU;W	QCXVnS]aT
XE>q

:e
k

\ l!#" g

kn a nql ^`\b\ rml

 ^`f p%r

WjZ/T	uR/^_W4U_W4Z]jXff]ab/W4TV^_Wk ]abCX%]\Sd tVW4Uh]u9TmW4RCn2d2c4d ]X%Z*iWX%U_ep]aTVpcb/WcOoUQ]vc4d W4Z]jcfiTVZCi/d ]	d TVZSU
Q/ZCiVW4^ubCd2cO
b Lr6c4X%ZSZ/TV]0b/TSn2i5Q/ZCn W4UU]ab/WRS^_TVfCX%fCd2n2d ]	d W4U'TSwAU_TSkWX%]aTSkUhX%Z*i/%TV^0TVf/UW4^_t)X%]	d TVZSU
X%^_WVq  b/W]abSW4TV^_Wk d U'R/^_TtVWifffeU_b/Tu8d Z/\5]ab*X%];]ab/WcfiTVZCi/d ]	d TVZlTS2
w ~Wkffk5XSq2VaXVc4X%Z/Z/TV];b/TSn2i
Q/ZCiVW4^&]ab/WU_]	X%]aWilcfiTVZCi/d ]	d TVZ/Uq
W9f/^d W n/e;^_Wc4XVn2n4U_TSkWU_]	X%ZCi/X%^iiW4/ZCd ]	d TVZ/U@wx^_TSkn2d ZSWX%^@XVn \VW4f/^XVqEr[U_W4]DTSw	tVWcfi]aTV^_:
U o @ & ^ i^i^ij
 o@ 0
d Ulc4XVn2n Wi+/,%
a	//d2w9]ab/W4^_W~W4gd U_]mcfiT`^W vlc4d W4Z`]aq
U p & ^ i^i^i pT0 Z/TV]mXVn2nL4W4^_TSU_QCcb]abCX%]
0 p&*t@ * @ 7]ab/WtVWcfi]aTV^_U5X%^Wv uS	+a`//{d2wA]abSW4^_W8W41d U_]6cfiT`^W vlc4d W4Z`]aU
U p & ^ i^i^i	 p&0Z/TV]
*sr & o 
0 p&*w@ * '
0
XVn2n4W4^_TS7U_QCcOb8]abCX%]
^ y @ d U&c4XVn2n Wi6X%Zv u/'|
 pm/
*sr & o  @ X%ZCi *
r & p&*  VqCrtVWcfi]aTVx
0 p&*t@ * @ X%ZCi 0 p&* Vq
TS(
w o @ & ^ i^i^iz
 o @ 0 d2w/]ab/W4^_WW41d U_]{cfiT`^W vc4d W4Z]a6
U p & ^ i^i^i	 p&0zU_QCcOb]abCX%]
*
r & o  y
*sr & 



  696 h,)/
 n		|
 	,	 n	/Z
\b"a\ 
:e*{
 E 	U
/+	e7h~xQyB	,/=ph%H#/q



V & ^i^i^i	4Fj7/%
Z

zO%}c|S	`~'	fi7, l4	?}Gj6^7vSln	,	^y @   y & ^i^i^i y j 
&vu/6p S>G6ff{G6} )j y  ] ff}VV^i^i^i6l"
/ y  ~ 	 j|h
p 8jffVV^i^i^i 6"l * 		fiff/%o)1^l 
,MK-Yx0v
y ,H1 ^	    ~K
 /q1 ^W ^''K
 V	 ,	>
 |
p '
,%`SSYa}
zu}9Y
 j7 ,84	 ,	1} GY
 jfihGfi
 7; 
/,,+_`	S	/0	 )&/0vu/	+
a`/	S1
 	ff	fi0B/%o )1 ^' 1MK&	'xQ
y )E1 ^W
 
''!
 	 {	
 fi|
p ',fi	O`//jjfiz[}
Oz u}@
 	n }	} S/G#
l 
/,+~/_`	/	/;663
 7v/l~%	 )qXE

,1XE>
 Y''H
 Q '%`//a}9 	fi5{)/	 )
,,	 )mXg  )	j eD
 ^%o ) 	 /xQy S
 ^W ''  XE>
D
 Y'' 8|
p ',,fi	O`//fia}B 	1 ^	  
8  X  a,
fi]8d U'u9Wn2n/`Z/TuZ~]ab*X%] d ZoX%ZN5 q
l kffX%]a^d gsCX%] kTVU_]0>
l ^_TuU c4X%ZfAWjn2d Z/WX%^n e}d ZCiW4RgW4ZCiVW4Z`]	q

fiZkffX%Zec4X%UW4U"TSwgd Z`]aW4^W4U_]'ac4wqSDgXVkR*n WSq2-fgWn T	u8as]ab/WZQCk0fgW4^TSwAX%]aTSkU5 d Un2X%^_\VW4^"]abCX%ZY]ab/W
ZQCkfgW4^-TSwSTVf/U_W4^_tX%]	d TVZ/V
U l"sU_T6]ab*X%]&]ab/W4^_W5k0Q/U_]-W4gd U]&U_Q/f/U_W4]aQ
U }TSw/^_T	u0UTS
w 7l]ab*X%]X%^_W5n2d Z/WX%^n e
iW4RgW4ZCiW4Z]	q  bQ/UsR*X%^_] fCBTSw  b/W4TV^_Wk$Sq jR/Q/]aU0ZSTVZ`]a^d t1d2XVn*cfiTVZ/U_]a^XVd Z]aUTVZ]ab/Wi/d U_]a^d fSQ/]	d TVZ/U
]abCX%]UX%]	d Uwx
e Lr60q
 bSW8^_W	QCd ^_WkW4Z`]ffd ZmRCX%^_]6aXVkffXaeU_W4WkU_TSkW4ub*X%]'TVfSUcfiQ/^_W f/Q/]hd ]6c4X%ZfgW8WX%Ud2n e[cOb/WcVWi
X%ZCiX%R/R*n2d Wi>d Z|X ZQCkfgW4^TSwEUd ]aQCX%]	d TVZ/Us"X%UYd2n2n Q/U]a^X%]aWi}d Z|DgXVkRCn W6Sq2X%ZCijSq2hfgWn Tu8
q LX%^_]
ac4UXaeU0]abCX%]8d ZkffX%Z`ejTV]ab/W4^hc4X%U_W4U;TSw*d Z]aW4^_W4U_]0u0b/W4^_WZ/Wd ]ab/W4^0RCX%^_]8aXVLZSTV^8fCX%R/R*n2d W4Us)W4tVW4Zd2w
X6i/d U_]a^d f/Q/]	d TVZTV
Z W41d U_]aUUX%]	d Uwe1d Z/
\ Lr60s`]ab/WR/^_TVfCX%f*d2n2d ]	d W4UTSwEkffX%1d Z/\6]ab/WTVf/UW4^_t)X%]	d TVZSU8X%^_W
cfiTSkRCn W4]aWn eiVW4]aW4^kffd Z/Wifel]ab/W6R/^_TVfCX%f*d2n2d ]emTSwCt)X%^d TVQSU{W4tVW4Z`]aUd Z>]ab/Wffu@TV^n2iT1c4cfiQ/^_^d Z/\SsgubCd2cOb
U_W4WkU;^X%]ab/W4^QSZ/^_WX%U_TVZCX%f*n Wq

`7

fi " |qxq}t~"}s1|

W Lrh;XVcfi]aW4^d d Z/\k5X%]a^d YTSw*11XVkRCn WSq2Vq0wBTV]	d2cfiW{]abSW4^_WW4gd U]aU8X%Z
 n/  p \eE LTVZ/Ud2iVW4^@]abSW
@ X%ZCi b*X%U0Z/T6Z/W4\SX%]	d tVWcfiTSkRgTVZ/W4Z]aU
X vffZ/W5cfiTSk0fCd ZCX%]	d TVZlTSw7]ab/W/^_U]&]u@T5^_TuU]abCX%]8d U0Z/TV] ff
 
 V






V










i

@ X%ZCiYbCX%UZ/T
P d2kffd2n2X%^n esA]ab/W4^_W W4gd U_]aU5X%ZXv5Z/WcfiTSk0fCd ZCX%]	d TVZ}TSwg]ab/Wn2X%U_]]u@T^_T	uU]ab*X%]6d UZ/TV] j
/
Z/W4\SX%]	d tVW8cfiTSkRATVZSW4Z`]aUqCfi]wxTSn2n TuU'w^_TSk  b/W4TV^_WkSq SaXVD]abCX%]L]ab/W4^_Wd UZ/Tffi/d U]a^d f/Q/]	d TVZhUX%]	d Uwe1d Z/\
Lr6]abCX%]\Sd tVW4U{fgTV]abTSwD]ab/W TVf/U_W4^_tX%]	d TVZ/U.    & X%ZCi    + RgTVUd ]	d tVW6R/^TVfCX%fCd2n2d ]e[X%ZCi
Wd ]ab/W4^aXV&\Sd tVW4UhfgTV]a
b W  &1  + X%ZC
i W  &;k  + RgTVUd ]	d tVWYR/^_TVfCX%fCd2n2d ]eTV^jfC&\Sd tVW4U
fgTV]a
b   v +   & X%ZC
i   v & k  + RgTVUd ]	d tVWR/^_TVf*X%fCd2n2d ]eqm4wLfgTV]ab[TVf/U_W4^_tX%]	d TVZ/U6bCX_tVW
RgTVUd ]	d tVW R/^_TVfCX%fCd2n2d ]es/]abSW4
Z Lr6c4X%Zb/TSn2i5TVZCn e}d2w7]ab/W8R/^_TVfCX%fCd2n2d ]eTSw &Rk  + d U'Wd ]ab/W4^6{TV^hVq
a11XVkRCn WjVq2ffXVn ^_WXVieU_b/TuU]abCd U;Q/Ud Z/\~X6kTV^_WYi/d ^_Wcfi] X%^_\VQCkW4Z]	q2
 bSW6Z/W4`]W41XVkRCn W~wQ/^_]abSW4^5d2n2n Q/U_]a^X%]aW4U8]abCX%]5d Z>\VW4Z/W4^XVn2s9d ]5c4X%Z}fgWhtVW4^_ei/d vcfiQCn ]]aTUX%]	d Uwxe
Lr60q
]   V & 4 + 4 . VsX%Z*iXVn2nE]abS^_W4WTVf/U_W4^_tX%]	d TVZ/Uc4X%ZfgWkffXViW
 n/  p \e& PQ/R/RgTVU_WY]abCX%
u8d ]abRgTVUd ]	d tVWhR/^_TVf*X%fCd2n2d ]eq]]aQ/^Z/U;TVQS]']ab*X%]5d Zl]ab*d UUd ]aQCX%]	d TVZ>]ab/WLr6 cfiTVZCi/d ]	d TVZc4X%Zb/TSn2i/s
f/Q/] TVZCn e|d2w;aXVm
 1^ W v &k  +Zk  .   lad2q Wq2s'XVn2nETSw; & s + s0X%ZCi[ . kQ/U]8b/TSn2iSasfC
1^	 W >aa &gk  +    . T haa +Rk  .    & & haa &gk  .    + a  ad2q Wq2s`W41XVcfi]	n e5]u@T TSw1 & s
 + s)X%ZCih . k0Q/U_]*b/TSn2i/asac4R
 1^ W >a &/ a + ; . a 0a + a & 0 . a ;a . a + 0 & aa  
ad2q Wq2s1W41XVcfi]	n elTVZ/WhTSw9 & s + sATV^ff . kQSU_]b/TSn2i/asATV^5ai/9TVZ/WhTSw"a &e a +  . a
 la +#k  . as
a +  a & 6 . a&
 6a & k  . ETV^a .  a & 6 + ab
 6a & k  + DbCX%U9RS^_TVfCX%fCd2n2d ]e8Wd ]abSW4^@W41XVcfi]	n e
TVZ/W{TSw* & s1 + sTV^  . bSTSn2iUs)TV^]abSW{^_WkffXVd ZCd ZS\6]u@T5fgTV]abb/TSn2i/aq
Wm/^_U]lcbSWc]ab*X%
] Lr6 c4X%Zzb/TSn2i|d Z XVn2n9]ab/W4U_Wc4X%U_W4Uq]YU_b/TVQCn2i}fgWc4n WX%^]abCX%e
] Lr6
c4X%Zb/TSn2i[d Zc4X%U_W}aXVaq5TV^_W4TtVW4^sB]abSW4^_WmX%^WjZSTcfiTVZSU_]a^XVd Z]aUffTV
Z 1^	    +
 *CW   hwTV^
 > &Rk  +Vk  . W4gcfiW4RS]	s/feY]abS
W Lr6cfiTVZCiSd ]	d TVZCsEwTV^0WXVcOb/Wih,s]ab/W{R/^_TVf*X%fCd2n2d ]e}k0Q/U_]fgW
]ab/W{UXVkW5wxTV^ XVn2?
n   &Rk  +Vk  . s1X%ZCih]abSW{]ab/^_W4WR/^TVfCX%fCd2n2d ]	d W4U k0Q/U_]0U_QCk]aTVaq
TV^ c4X%U_WYfCas1n W4Q
] '1*DfgW]ab/WffX%]aTSk u0b/W4^_WW4gXVcfi]	n e]u9T6TSw* & sg + s1X%Z*ij . b/TSn2i/s1X%ZCi
 *"iTW4U
Z/TV]b/TSn2iSs"wTV^{  V4V4VqPQ/R/RgTVU_Wh]abCX%W
] D^   g
 ' & 4' + 4' .   V
q wBTV]aW5]abCX%]	sDUd ZCcfiW~XVn2n
]ab/^_W4W5TVf/U_W4^_tX%]	d TVZ/Ujc4X%Z}fAW~k5XViWffu8d ]ab>RgTVUd ]	d tVW5R/^_TVfCX%fCd2n2d ]es9X%]jn WX%U_]]u9TTS1
w ' & 
s ' + s"X%ZC<
i ' .
k0Q/U_]{bCX_tVW5RgTVUd ]	d tVWYR/^_TVfCX%fCd2n2d ]eqy"W4ZCcfiWYu9Wmc4X%Zi/d U_]	d ZS\VQCd U_b>fgW4]u@W4W4Z]u@T~U_Q/fBc4X%U_W4Uffad2&TVZCn e
]u@T5TSwS]ab/Wk bCXatVW{RgTVUd ]	d tVW8R/^_TVfCX%f*d2n2d ]esDX%ZCi~ad2d2XVn2n]ab/^W4WbCXatVW8RATVUd ]	d tVW{R/^_TVfCX%f*d2n2d ]eq
TV^8U_Q/fBc4X%U_W~ad2asEU_Q/R/RgTVU_Whu8d ]abSTVQ/]5n TVU_UTSwC\VW4Z/W4^XVn2d ]e]abCX%]{TVZ*n 
e ' & X%ZC<
i ' + bCXatVWYRgTVUd ]	d tVW
R/^_TVf*X%fCd2n2d ]eq  b/W4Z>d ]d2kffkWi/d2X%]aWn e>wTSn2n T	uU8w^_TSk]ab/=
W Lr6cfiTVZCiSd ]	d TVZ]abCX%]&]ab/W4^_Wffk0Q/U_]-fgWU_TSkW
u8d ]ab 
  ~U_QCcOb]abCX%e
] 1^	     . W
 ' & c' + U_QCcOb
    CswTV^mXVn2Kn  k
]abCX%e
] 1^	 W
h




V

q

b
/
Q

U
s
#
1


^






W







w
V
T
m
^
V
X
n2K
n  k
 ' + U_QCcOb



 &

 
]abCX%
] 1^ W   
  Vs0X%ZC
i D^	     + W       wTV^XVn2
n  
 ' & U_QCcOb[]abCX%]
1^	 W   K Vq
PQSfc4X%UWad2d2Bd U'kTV^W8d Z`]aW4^W4U_]	d Z/\Sq  b/W&^T	uU9TSw]abS
W Lrh;XVcfi]aW4^d d Z/\kffX%]a^d x
 7}cfiTV^_^W4U_RgTVZCi/d Z/\
]aq
T ' & F
s ' + sLX%ZC4
i ' . X%^_WaYVasajVasX%ZCimajVasA^W4U_RgWcfi]	d tVWn eW
q w"T	u ~Wk5kffX{Sq2VaXV&]aWn2n U
Q/U]abCX%]6d2K
w D^;UX%]	d U/W4W
U Lr60sS]ab/W4Zmu9Wk0Q/U_]'bCXatV[
W 7A_@ B   @ B wxTV^U_TSk
W A @   A &  A +  A . "u8d ]ab
 *CW}+
 *fiaq  b/W4U_W]ab/^_W4W5n2d Z/WX%^;WQCX%]	d TVZSU0bCX_tVWU_TSn Q/]	d TVZ
A *  D^    +

A
&

 A+  A. 
`77a





i

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

P/d ZCcfiW]abCd UU_TSn Q/]	d TVZd UQ/Z*d2Q/WsEd ]8wxTSn2n TuU'f`ee~Wkffk5XSq2VfC]ab*X%]6BiSd U_]a^d f/Q/]	d TVZSU]abCX%]UX%]	d Uwxe
Lr6 kQSU_]8b*XatVW>cfiTVZCi/d ]	d TVZCXVn*RS^_TVfCX%fCd2n2d ]	d W4
U 1^    +
 *CW v+
 *fi  VVVs;X%ZCi~]abCX%]h]ab/Wd ^
kffX%^_\Sd Z*XVnCi/d U]a^d f/Q/]	d TVZ/U'TV
Z 
c4X%ZmfgWjX%^_fCd ]a^X%^_eq  bCd U wQCn2n eocObCX%^XVcfi]aW4^d 4W4U{]abSW8U_W4]'TSwBi/d U]a^d f/Q/p
]	d TVZ/
U 1^wTV^8u0bCd2c
b Lr6b/TSn2iUd Z[]abCd Uc4X%U_W
q wBTV]aW]abCX%]wTV^   V4V4VsUd ZCcfiWYu@Wc4X%Zu^d ]aW
 *fia2
 1^ W o+
 *fiu@W{bCXatV
W 1^    +
 *  1^ }
 * A *`VV{U_Tff]abCX%]	sCd Z
A *  1^	    +
cfiTVZ]a^X%U_]]aT]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/]	d TVZlT	tVW4
^ vsS]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/]	d TVZlT	tVW4
^  c4X%Z/ZSTV]'fgW
cbSTVU_W4ZX%^_f*d ]a^X%^d2n eq

fiZ~c4X%U_W8ac4as7d ]U_b/TVQCn2i6XVn U_TfgW{c4n WX%^"]ab*X%]Lrh|c4X%Zffb/TSn2i/q@5TV^_W4T	tVW4^sE1^ -  +*CW  h
d U-Wd ]ab/W4^{;TV^VsgiW4RgW4ZCiSd Z/\{TVZub/W4]ab/W4^ +*q&Ld ZCXVn2n es1wTV^c4X%U_Wffai/as`U_QSR/RgTVU_W0]abCX%].1^W
 & a +Qk  . a  VW
q rhzbSTSn2iU5d }]ab/W4^W W41d U_]aU>U_QCcOb]abCX%]=1^    + W     
X%ZC
i 1^	 W   .    h    wxTV^ffXVn2
n [ + k  . U_QCcOb]abCX%
] 1^	    h[
 vVq8a{w
cfiTVQ/^_UWC
s 1^	     & W  h  ffwTV^8XVn24
n  > & U_QCcb~]abCX%
] 1^	 W   K
 Vq2

w"T	uu@W&U_b/Tu]abCX%
] Lr6c4X%Z/Z/TV]bSTSn2i6d Z~X%Z`e TV]ab/W4^0c4X%U_Wq&Ld ^_U_]UQ/R/RgTVU_W@]abCX%]0{1^W
 ff& k  +_k  . eVq  bQ/Us	]ab/W4^_WkQ/U]EfgW;X%]&n WX%U_]LTVZ/W"TV]ab/W4^&X%]aTSkZ'lU_QCcOb8]abCX%][1^	W q''2Vq
 b/W"^_T	uzcfiTV^_^_W4U_RgTVZCi/d ZS\&]aT0]ab/W;X%]aTSkv &^k  +k  . d U@a;0Vaq@PQ/R/RgTVU_WRd UC]ab/W"^_TucfiTV^_^W4U_RgTVZCi/d Z/\
]aT>]ab/W~TV]ab/W4^X%]aTS
k ';qP/d Z*cfi8
W 7 d UX>%pokffX%]a^d 1sB]abSWtVWcfi]aTV^ma}V  m\Sd tVW4Uld UlX%ZXvffZ/W
cfiTSk0fCd ZCX%]	d TVZTSwa>V5X%ZC
i m]abCX%]d UffZ/TVZS4W4^_TX%ZCimbCX%U5Z/TVZ/Z/W4\SX%]	d tVW[cfiTSkRgTVZ/W4Z`]aUq]6Z/Tu
wTSn2n T	uU;f`e  b/W4TV^WkvSq ff]abCX%
] rhc4X%Z/Z/TV]b/TSn2i~d Z]abCd U c4X%UWq
P d2k5d2n2X%^~X%^_\VQCkW4Z]aU6\Sd tVW}XcfiTVZ`]a^XVi/d2cfi]	d TVZd ZXVn2n*]ab/WTV]ab/W4^~c4X%U_W4U"u@W>n WXatVWoiVW4]	XVd2n Uff]aT]ab/W
/
^_WXViW4^q

:e
c

" 

k ` "  l

4

 
!#"  

\ba

%

^` %" l)

 _e{&"


\_^

"

\_

f" fi

W^` " l)

fiZTVZ/WTSw7]ab/Wd ^8kffXVd Z~]ab/W4TV^_WkUsE{d2n2n2s)tX%ZiW4^~CXVX%ZCsDX%ZCiBTVfCd Z/UaVVVVs*PWcfi]	d TVZVU_b/T	u]abCX%]
]ab/
W Lr6 X%U_U_QCkR/]	d TVZd U6Q/Z`]aW4U]	X%fCn W}wx^TSk TVfSU_W4^_tX%]	d TVZ/U6TS2
w   XVn TVZ/Ws;d Z]abSWjUW4Z/U_Wj]abCX%]6]ab/W
X%U_U_Q*kR/]	d TVZ 1^UX%]	d U_/W4W
U rh;Yd2kRgTVU_W4U'Z/Tj^_W4U_]a^d2cfi]	d TVZSU5X%]hXVn2n7TVZm]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/]	d TVZ
1^  TVZ   qffTV^WR/^_Wc4d U_Wn es@]abSW4eoU_bST	u]abCX%]lwxTV^YW4tVW4^_e[/ZCd ]aWU_W4
] 
TSw"u9TV^n2iUs"W4tVW4^_e[U_W4]
vTSwETVf/U_W4^_tX%]	d TVZ/Us"X%ZCiYW4tVW4^_e[iSd U_]a^d f/Q/]	d TV
Z X  TV
Z sg]abSW4^_Wd U5Xi/d U_]a^d f/QS]	d TV
Z 1^  TVe
Z U_QCcOb
]abCX%
] 1^   ]ab/WmkffX%^_\Sd ZCXVnDTS
w 1^  TV
Z {d U WQ*XVng]a
T XE X%ZC
i 1^  UX%]	d U/W4q
U Lr60q  b/W~X%Q/]abSTV^_U
U_QCk5kffX%^d 4W]abCd UhX%U Lr6d UW4tVW4^e`]abCd ZS\SVq
 W|k0Q/U_]fgWc4X%^_WwQCnd Zd Z]aW4^_R/^_W4]	d Z/\]ab*d U^_W4U_QCn ]	q  b/W4TV^Wk Sq U_b/TuU]abCX%]	shwxTV^kffX%Z`e

cfiTSk0fCd ZCX%]	d TVZ/UTSw2X%ZCivsLrhvc4X%Zlb/TSn2i/+lwxTV^hi/d U_]a^d f/QS]	d TVZ/UmD^0u8d ]ab1^	W c''  
wTV^8U_TSkWlX%]aTSk\
U ''q>Z]ab/WYR/^_W4t1d TVQ/UU_Wcfi]	d TVZSUs*u9Wc4XVn2n WiU_QCcObzi/d U]a^d f/Q/]	d TVZ/U>%iW4\VW4Z/W4^X%]aWVq2
fiZ>TVQ/^{t1d W4u8s1]ab*d UUX_e`U8]abCX%]Yd Z>U_TSkWc4X%U_W4U;
s Lr6W4AWcfi]	d tVWn ec4X%Z/Z/TV]b/TSn2i/q  TU_W4Wffu0b`esD/^_U_]
U_Q/RSRATVUWYu@W>X%^_W\Sd tVW4ZXjU_W4
] 
TSwBu9TV^n2iU~X%ZCi[XjU_W4
] TSwTVfSU_W4^_tX%]	d TVZ/U
q w"T	u u9W}kffX_ewW4Wn
cfiTVZ/CiVW4Z`]Y57,%-]ab*X%]'UTSkW~ - 
 X%ZCiYU_TSk
W  - 
 
c4X%Z/Z/TV]T1c4cfiQ/^5d ZR/^XVcfi]	d2cfiWq8fiZm]abCd U
c4X%U_Wsu9W X%^_W&u8d2n2n2d Z/\{]aTcfiTVZ/Ud2iW4^"TVZCn ei/d U_]a^d f/QS]	d TVZ/
U 1^BTV
Z m ]abCX%]"bCXatV
W D^     -   Vs
1^	 W   -   Vq aTV^W4gXVkR*n WZ
s 
kffXaefAWXR/^_T1iQCcfi]5U_RCXVcfi
W 
   X%ZCi|d ]
d U `Z/TuZ]abCX%]8UTSkWmcfiTSk0fCd ZCX%]	d TV
Z l
 X%Z*
i Yd 
Z 5c4X%ZZSW4tVW4^ T1c4cfiQ/^ ]aTV\VW4]ab/W4^B]ab/W4Z
1^	      6 4a  Vq2
 r9W4/Z/m
W   ]aTfgW]abSW0U_Q/fSU_W4]BTSY
w vcfiTVZ/Ud U_]	d Z/\8TSw1XVn2nS]abCX%]"u@W c4X%ZSZ/TV]
X-R/^d TV^d^_QCn W0TVQ/]	Ud2kffd2n2X%^n eE
s Z-d U9]ab/WU_QSf/U_W4]BTSY
w  cfiTVZ/Ud U_]	d ZS\TSw1XVn2
n ]abCX%]9u9W8c4X%ZSZ/TV]'X-RS^d TV^d
^_QCn W9TVQ/]	q@Ee  b/W4TV^_WkSq Ssd ]-d U*U_]	d2n2n	RATVUUd fCn WB]abCX%[
]   X%ZC
i   X%^_W@U_QCcOb]abCX%]	sW4tVW4Zd2wu@W"^_W4U]a^d2cfi]
]aT^_Q/ZSU'u0b/W4^_WhTVZCn elTVf/U_W4^_tX%]	d TVZ/UYd 
Z 6X%^_W~k5XViW!
s Lr6 c4X%Z>TVZCn eb/TSn2id2;
w 1^	 W 
 ''  
wTV^@UTSkWhX%]aTSkUZSTVZ/WkR/]effU_Q/f/UW4]aU
 '  q  bCd UkWX%Z/U@]abCX%Z
] Lr6k5XaelwTV^cfiW'Q/U@]aTX%UUd \VZ

`77

fi " |qxq}t~"}s1|

R/^_TVf*X%fCd2n2d ]em0]aT6UTSkWW4tVW4Z`]aU]ab*X%]	s17%,su9W4^W6cfiTVZSUd2iW4^_Wi RgTVU_Ud f*n Wq@DgXVkRCn W4UVq26X%ZCi8Sq2
d2n2n Q/U_]a^X%]aW]abCd URSb/W4Z/TSkW4Z/TVZ*q*WYkffXaeU_Q*kffkffX%^d 4W]abCd UhX%UU_TSkW4]	d2kW4UmLr6d U;Z/TV]abCd Z/\SVq
{d tVW4Z]ab/W4^_WwTV^_W]abCX%
] Lr6d2kRgTVU_W4U&UQCcbU]a^_TVZ/\cfiTVZCi/d ]	d TVZ/Us)]ab/W^_WXViW4^k5Xaeju9TVZCiVW4^&ub`e
]ab/W4^_Wd UffU_T|k0QCcbU_]aQCieTSw9]ab/
W Lr6 cfiTVZCiSd ]	d TVZd Z]ab/W~U_]	X%]	d U_]	d2cfiUn2d ]aW4^X%]aQ/^_Wq  b/W^_WX%U_TVZvd U
]abCX%]6U_TSkWTSwB]abSWU_RgWc4d2XVn*Ud ]aQCX%]	d TVZ/Umd ZubCd2c
b Lrh b/TSn2iU6TSw]aW4ZX%^d U_Wd Zkffd U_Ud Z/\iSX%]	X>X%ZCi
U_Q/^tgd tXVnX%ZCXVn eUd U"R/^TVfCn WkUq@yBW4^W8d U'X%ZffW41XVkRCn W-PQSR/RgTVU_W9]ab*X%]B]ab/WU_W4]"TSwTVf/UW4^_t)X%]	d TVZSU'c4X%Z5fgW
 *fisubSW4^_W;WXVcOb *"d UX0RCX%^_]	d ]	d TVZTSCw  ]abCX%]d UsAX0U_W4]@TSw)RCXVd ^_u d U_WhiSd U_TSd Z]
u^d ]a]aW4Z>X%Z
U    *
r
&

U_Q/fSU_W4]aU;TSK
w  ub/TVU_W8Q/ZCd TVZd m
U vaq8Q/^_]ab/W4^'U_Q/RSRATVUW]abCX%]'TVf/U_W4^_tX%]	d TVZ/UffX%^_W \VW4Z/W4^X%]aWiYfe]ab/W
wTSn2n T	u8d Z/\mR/^TgcfiW4U_UsEu0bCd2cb[u@Wmc4XVn2
n {:f)z*q}PVTSkWYfgW4]u@W4W4ZX%Z*
i d Ucb/TVU_W4ZXVc4cfiTV^iSd Z/\
]aTU_TSkWX%^_fCd ]a^X%^eoi/d U_]a^d fSQ/]	d TVc
Z X - d ZCiW4RgW4ZCiVW4Z`]	n e
s 
 
d U6cOb/TVU_W4ZXVc4cfiTV^i/d ZS\j]aq
T XE>q  b/W
X%\VW4Z]]ab/W4ZTVf/U_W4^_tVW4U0]ab/WQ/ZCd2	Q/Wh  * U_Q*cb]abCX%[
]  >q*fiZ`]aQ*d ]	d tVWn es)]ab/WRCX%^_]	d ]	d TVZ/U  * kffXae
^_W4R/^W4U_W4Z`]-]ab/W'TVf/U_W4^t)X%]	d TVZ/U&]ab*X%]c4X%ZfgW6kffXViW'u8d ]ab>X;RCX%^_]	d2cfiQCn2X%^&U_W4ZSU_TV^q  bQ/UC
s X - iW4]aW4^k5d Z/W4U
]ab/WR/^_TVf*X%fCd2n2d ]eo]abCX%]X5R*X%^_]	d2cfiQCn2X%^6U_W4Z/UTV^d U~cOb/TVU_W4ZC
 X  iW4]aW4^kffd Z/W4Uh]ab/WR/^TVfCX%fCd2n2d ]eo]abCX%]~X
RCX%^_]	d2cfiQ*n2X%^u@TV^n2i}d UYcOb/TVU_W4ZCq  b/W6UW4Z/U_TV^YX%ZCi]ab/Wffu@TV^n2i]aTV\VW4]ab/W4^iW4]aW4^k5d Z/Wff]ab/WffTVfSU_W4^_tX%]	d TVZ
]abCX%]6d UffkffXViWq0]6d UWX%U_e~]aTjU_W4W ]abCX%]]abCd UffkWcbCX%Z*d Uk d ZCiQCcfiW4U6XYiSd U_]a^d f/Q/]	d TVZlTV
Z  wxTV^'ubCd2cOb
Lr6b/TSn2iVUq
b    &   + s  &  
 vVs`X%ZCi  +  V h
 $
 v&cfiTV^_^_W4U_RgTVZCiVU1]aT X
 bSWU_RgWc4d2XVnc4X%U_WBu8d ]a
Ud2kR*n Whkffd UUd Z/\i/X%]	X;R/^_TVfCn Wka11XVkRCn WSq2;fgWn T	u8aq*fiZ]aQCd ]	d tVWn es)Wd ]ab/W4^cfiTSkR*n W4]aW5d ZCwxTV^kffX%]	d TVZ
d Uff\Sd tVW4ZCs"TV^ff]ab/W4^W}d UffZ/Ti/X%]	X>X%]~XVn2n2qfiZ]abCd UcfiTVZ]aW4`]	#
s Lr6d UffTSw]aW4Zc4XVn2n Wiz
 xQyV#p ,S
{fi/|
 pqfiZkTV^_W^_WXVn2d U_]	d2c5rh R/^_TVfCn WkUs*u9WkffXaeoTVf/U_W4^_tVW>XYtVWcfi]aTV^ffu8d ]ab[U_TSkWjTSw;d ]aU
cfiTSkRgTVZ/W4Z]aU5kffd U_Ud ZS\SqfiZlUQCcbc4X%U_W4U]abS
W Lrh cfiTVZ*i/d ]	d TVZ>U_TSkW4]	d2kW4U{U_]	d2n2nAb/TSn2iUqZmR/^XVcfi]	d2c4XVn
kffd U_Ud Z/\i/X%]	XLR/^TVfCn WkUs]ab/WB\VTSXVnd UETSwx]aW4Z8]aT8d ZCwW4^D]ab/W0i/d U_]a^d f/Q/]	d TV
Z D^1TVZ^_Q/ZSE
U  w^_TSk[U_QCc4cfiW4UUd tVW
TVf/U_W4^t)X%]	d TVZ/U6TS!
w W&q  bCX%]d UsLTVZ/WjTVf/U_W4^tVW4U~X5UXVkRCn W> )&/ 4 ),+0/ ^ i^i^i4 ) j / sLub/W4^_W ) * /  q
 e`RCd2c4XVn2n esV]ab/W ) * / X%^_W{X%U_U_Q*kWi]aT'fgWX%Zd2q2d2q2i/q@ad ZCiW4RgW4ZCiVW4Z`]	n eYd2iVW4Z`]	d2c4XVn2n ei/d U_]a^d f/QS]aWi//UXVkRCn W
TSw-TVQ/]	cfiTSkW4UTS
w W&q  b/WcfiTV^_^_W4U_RgTVZCiSd Z/\ u@TV^n2iU
  & 6  + ^ i^i^iTVQ/]	cfiTSkW4UTS[
w   X%^_WS
TVf/U_W4^tVWi/[
q r9W4RAW4Z*i/d Z/\hTVZ]ab/WUd ]aQCX%]	d TVZCU
s D^{kffXaejfgWhcfiTSkR*n W4]aWn ejQSZ/`ZST	uZjTV^d UX%U_U_QCkWi]aT6fgW
XkWk0fgW4^TSwCU_TSkW5RCX%^XVkW4]a^d2cYwXVk5d2n elTSw-i/d U]a^d f/Q/]	d TVZ/Uq64wD]ab/W5Z`QCk0fgW4^{TSwCTVf/U_W4^_tX%]	d TVZ/
U ld U
n2X%^_\VWs]ab/W4Zc4n WX%^n e8]ab/W9UXVkRCn W' )&/ 4 ),+0/ ^ i^i^i4 ) j / c4X%ZhfAW"Q/U_Wi0]aT;TVf/]	XVd ZX"^_WX%U_TVZCX%f*n W9W4U_]	d2k5X%]aW
TS2
w 1^  sS]ab/WkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/]	d TVZ~TV
Z   q{CQ/]0TVZSWd U6d Z`]aW4^W4U_]aWimd Z]ab/WwxQ*n2nEi/d U_]a^d f/Q/]	d TV
Z D^q
 bCX%] i/d U_]a^d f/QS]	d TVZQ/U_QCXVn2n ec4X%Z/Z/TV]0fAW5d Z*wxW4^_^Wihu8d ]ab/TVQ/]k5X%gd Z/\lXVi/i/d ]	d TVZCXVnCX%U_U_QCkRS]	d TVZ/UsU_QCcOb
X%U0]abSW
W LrhX%U_U_QCkR/]	d TVZCq
s rX%ZCd Wn Us6 BTVfCd Z/Us VVVVa~PVQ/R/RgTVU_Wl]abCX%]oX
 n/  p \es@ aXVi/X%R/]aWiwx^_TSk aPScbCX%^wU_]aWd ZCm
kWi/d2c4XVn"U_]aQCiVezd UcfiTVZCiQCcfi]aWi>]aTo]aW4U]5]ab/WW47Wcfi]jTSwXZ/W4u i^_Q/\Sq  b/Wi^_Q/\|d UmXVi/kffd ZCd U]aW4^_Wi
]aToXh\V^TVQ/RTSwCRCX%]	d W4Z`]aU8TVZ|Xhu@W4W4gn emfCX%Ud UqCWwxTV^_W5]ab/WffW4RAW4^d2kW4Z`]5d UU_]	X%^_]aWi}X%Z*iXVwx]aW4^d ]Yd U
/ZCd Ub/Wi/sU_TSkW cbCX%^XVcfi]aW4^d U_]	d2cUXaes]ab/WfCn TT1i'R/^_W4U_U_QS^_WATSw`]ab/W&RCX%]	d W4Z]aUd UkWX%U_Q/^_WiSq  bSWi/X%]	X
X%^_W]abQ/Ui/d 7W4^W4ZCcfiW4U'd Zfff*n T`T1iR/^_W4UU_Q/^_WwTV^'d ZCiSd tgd2iQ*XVnRCX%]	d W4Z`]aU@fgWwxTV^WX%ZCi5XVw]aW4^"]ab/W0]a^_WX%]	kW4Z]	q
fiZR/^XVcfi]	d2c4XVngU_]aQ*i/d W4UTSw1]abCd Ugd ZCiSsSTSw]aW4Z>U_W4tVW4^XVngTSw1]ab/W6R*X%]	d W4Z`]aUYiV^_TVRTVQ/]TSwD]abSW6W4`RgW4^d2kW4Z]	q
TV^;U_QCcb~RCX%]	d W4Z`]aU']ab/W4^_Wjd U]ab/W4ZZ/Tli/X%]	XVqLWjkTgiWn]ab*d U X%U wTSn2n T	u0U
  d U]ab/WU_W4]TSwSRATVUUd fCn W
tXVn Q/W4ULTSw]ab/WcObCX%^XVcfi]aW4^d U]	d2c*u9WX%^_Wd Z`]aW4^_W4U]aWi d ZWq \Sq2sfCn TT1iR/^W4U_U_Q/^_W'i/d 7W4^W4ZCcfiWa
q    &   +
u8d ]ab  &  
 vVs@X%ZCi  +  V h
  
 vX%UjX%fgT	tVWqVTV^>%cfiTSkR*n2d W4^_UlRCX%]	d W4Z`]aUh]abCX%]i/d2i
Z/TV]5i^TVRTVQ/]	as1u@WffTVf/U_W4^tV
W      VsEub/W4^
W  d U{]ab/W6tXVn Q/WffTSwE]ab/W~cObCX%^XVcfi]aW4^d U_]	d2cu@W6uX%Z]
]aTokWX%UQ/^_WqTV^i^_TVRgTVQ/]aUs1u9WffTVfSU_W4^_tV=
W W  
]abCX%]jd UsDu9W5TVf/U_W4^_tVWffZSTV]abCd Z/\}X%]YXVn2n2aq5W
]abQ/U{bCX_tVWs@wTV^W41XVkRCn Ws@XhU_W	Q/W4ZCcfiW5TSwETVfSU_W4^_tX%]	d TVZ/Uj &    & V4 +    + V4 .  l4 2 
  2 V4   l^ i^i^i	4 j   j7Vq4wC]abCd U UXVkRCn Wmd Un2X%^\VWYW4Z/TVQS\VbCsDu@Wc4X%ZoQ/UWld ]]aTlTVf/]	XVd ZX
^_WX%U_TVZ*X%fCn WW4U_]	d2kffX%]aW{TSw/]ab/WR/^_TVf*X%fCd2n2d ]eY]abCX%]X'RCX%]	d W4Z]i^_TVR/U-TVQ/]]ab/W^X%]	d T5TSwTVQ/]	cfiTSkW4U0u8d ]ab

`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

+*   ]aT]abSW6]aTV]	XVn1ZQCkfgW4^TSwCTVQ/]	cfiTSkW4UaqhWlc4X%Z|XVn UT\VW4]jX ^_WX%UTVZCX%fCn WffW4U_]	d2k5X%]aW5TSwC]ab/W
i/d U_]a^d f/Q/]	d TVZTSw?W wTV^&]ab/WffcfiTSkRCn e1d Z/\6RCX%]	d W4Z]aUq  TV\VW4]ab/W4^0]ab/W4U_W]u@T~i/d U_]a^d f/Q/]	d TVZ/UiW4]aW4^kffd Z/W
]ab/WYiSd U_]a^d f/Q/]	d TVZTSwg  q

WX%^_Wjd Z]aW4^_W4U_]aWimd Zl]ab/WW4AWcfi]'TSwA]ab/Wji^Q/\~d Z]abSW\VW4Z/W4^XVnSRgTVR/QCn2X%]	d TVZC
q "ZCwTV^_]aQ/ZCX%]aWn es*d ]
k X_efgW"]ab/Wc4X%U_W-]abCX%]L]ab/W@W47Wcfi]TVZi^TVRATVQS]aU@d Ui/d AW4^_W4Z`]w^_TSk]ab/W9W4AWcfi]TVZcfiTSkRCn2d W4^_Uq@aPScbCX%^wp
ff
U_]aWd ZCs4rX%ZCd Wn UsX%Z*iTVf*d Z/U@aVVVVLi/d UcfiQSU_U@X%ZXVcfi]aQCXVnkWi/d2c4XVn4U]aQCieffd Z8u0bCd2cb8R/b`eUd2c4d2X%Z/U*QCi\VWi
]ab/W6W4AWcfi]{TVZ|i^TVRATVQS]aU']aTfgWhtVW4^_ei/d AW4^_W4Z`]5w^_TSk]ab/W6W4AWcfi]{TSw@cfiTSkRCn2d W4^_Uq2  b/W4Zmu9W~c4X%ZSZ/TV]
d ZCwW4^]ab/Wmi/d U_]a^d fSQ/]	d TVZ}TV
Z 
wx^_TSk]ab/W5TVfSU_W4^_tX%]	d TVZ/U & 4 + ^ i^i^i@XVn TVZ/WYu d ]ab/TVQ/]YkffX%1d Z/\[XVi/i/d p
]	d TVZCXVn7X%U_U_QCkRS]	d TVZ/U0X%fgTVQ/]Bb/Tu]ab/W8i/d U]a^d f/Q/]	d TVZ~wTV^'i^TVRATVQS]aU0d UB^_Wn2X%]aWi]aT{]ab/Wi/d U_]a^d f/Q/]	d TVZwTV^
cfiTSkRCn2d W4^U
q SW4^bCX%R/UB]ab/W0Ud2kRCn W4U_]UQCcbX%UU_QCkR/]	d TVZ5]abCX%]BTVZ/Wc4X%ZmkffX%VW8d UB]abCX%]"]ab/W8i/d U_]a^d f/Q/]	d TVZ
TSY
w W wTV^6iV^_TVRgTVQ/]aUff5d ZwXVcfi]]ab/W UXVkWX%U]ab/WiSd U_]a^d f/Q/]	d TVZmTSY
w W wTV^ffcfiTSkRCn2d W4^_U]ab/Wi/X%]	X
X%^_Wz%k5d U_Ud Z/\|X%]5^X%ZCiTSkffVq{w;cfiTVQS^_U_Ws"]abCd UlX%U_UQCkR/]	d TVZd U5Q/U]6]ab/
W Lr6 X%U_UQCkR/]	d TVZCqEe
s rhb/TSn2iU8d wxTV^hXVn24
n  
 
 b/W4TV^WkVq2VaXVa

D^W     v  D^	W  W 
 v  D^	W  

a

ubCd2cOb>kWX%Z/U&Q/U_]@]abCX%]&]ab/Wffi/d U_]a^d fSQ/]	d TVZTSwF d U{d ZCiVW4RAW4Z*iW4Z`]@TSwu0b/W4]ab/W4^{X;RCX%]	d W4Z`]iV^_TVR/U-TVQ/]
    vETV^-Z/TV]	q  bQ/Us1 Lr6c4X%ZfgWhX%UU_QCkWi/s]ab/W4Zu@W6c4X%Z>d ZCwW4^-]ab/Wffi/d U_]a^d fSQ/]	d TVZYTVZ
ubCd2cObod U0u0bCX%]&u9WjX%^_W{^_WXVn2n e>d Z]aW4^_W4U_]aWi~d Z*aq
lX%Z`effRS^_TVfCn WkUd Zmk5d U_Ud Z/\i/X%]	X8X%Z*i{U_QS^_tgd tXVnSX%Z*XVn e`Ud U{X%^_W;TSw)]ab/W;gd ZCijd2n2n Q/U_]a^X%]aWijX%fgTtVW  b/W
X%ZCXVn eUd U@u9TVQCn2ifgW\V^_WX%]	n eYUd2kRCn2d /WiYd2U
w Lr6}bSTSn2iUsVfSQ/]Bub/W4]abSW4^"TV^@Z/TV]9]ab*d U'd U@U_Tjd U@Z/TV]c4n WX%^q
fi]jd U ]ab/W4^_WwTV^_W5TSw*TVftgd TVQSUd Z]aW4^_W4U_] ]aTd ZtVW4U_]	d \SX%]aWub/W4]abSW4^s9w^_TSkTVf/U_W4^_t1d Z/\~]abSW[%cfiTSX%^_U_W4Z/WiS
i/X%]	X6 )&/ 4 )+0/ ^ i^i^i	4 ) j / XVn TVZ/Ws1d ]kffX_emXVn ^_WXViVe5fgWRATVUUd fCn W']aT6]aW4U_]&]ab/WffX%U_U_QCkR/]	d TVZ]abCX%Z
] Lr6
b/TSn2iUq;TV^'W41XVkRCn WsTVZSWYkffd \Vb] d2kffX%\Sd Z/W8]abCX%]0]abSW4^_WYX%^_Wi/d U_]a^d f/QS]	d TVZ/UTV
Z   wxTV^;ubCd2c
b Lr6
Ud2kR*n ec4X%Z/Z/TV]&b/TSn2iSq*w)]abSW'WkRCd ^d2c4XVn1i/d U_]a^d f/Q/]	d TVZTSw]ab/Wff
 *1u@W4^_W%c4n TVU_Wffad Z]ab/WffX%R/R/^TVR/^d2X%]aW
U_W4Z/UW@]aT}Xi/d U_]a^d f/QS]	d TVZ]abCX%]^_QCn W4UTVQ/W
] rh;sS]abSW6U_]	X%]	d U_]	d2c4d2X%Zkffd \Vb`]5d Z*wxW4^]abCX%q
] 1^ffiTW4U{Z/TV]
UX%]	d Uw
e rh;;
q "ZCwTV^_]aQ/ZCX%]aWn esgd2C
w  d U-/ZCd ]aWs`]ab/W4Zj]ab/W^_W4U_Q*n ]9TSwDd2n2n2st)X%Z>iW4.
^ ~EXVX%ZCsgX%ZCiBTVfCd Z/U
aVVVVsPWcfi]	d TVZV"^_WwxW4^^_Wi6]aTX%];]abSWfgW4\Sd Z/ZCd Z/\YTSwA]abCd U'U_Wcfi]	d TVZmU_b/T	u0U;]abCX%]'u@Wjc4X%ZmZ/W4tVW4^'^_QCn W
TVQ/
] rhd Z]abCd UuXaeq

W6X%^_W6d Z`]aW4^_W4U]aWijd Zj]ab/WhQ/W4U]	d TVZjTSwu0b/W4]ab/W4^Qrhzc4X%Zb/TSn2i5d ZXZ/TVZCiW4\VW4Z/W4^X%]aW0UW4Z/U_Ws
\Sd tVW4Z X%Z*ivqffV^_TSk]abCd URgTSd Z]'TSwDtgd W4u sA]ab/WhUn TV\SX%ZvU_TSkW4]	d2kW4UqLr6 d UZ/TV]abCd ZS\Sjk5X%VW4U
U_W4Z/UWqfiZcfiTVZ]a^X%U_]	s{d2n2nW4]mXVn2q;aVVVV8u@W4^_Wd Z]aW4^_W4U_]aWid Z]ab/W[QSW4U_]	d TVZ|u0b/W4]ab/W4^Lr6 c4X%Z
fgWY]aW4U]aWiwx^TSkTVf/U_W4^_tX%]	d TVZ/U6TS2
w   XVn TVZ/Wq^_TSk]ab*X%]6RATSd Z]hTSwt1d W4u8s]abSWjUn TV\SX%Z Lr6d U
W4tVW4^_e]abCd Z/\Sffk5X%VW4URgW4^wxWcfi]-U_W4ZSU_WqLfiZwXVcfi]	sD{d2n2n2st)X%Z}iW4
^ ~CXVX%ZCs1X%ZCiBTVfCd Z/U&u@W4^_WffQCd ]aW5X_uX%^Ws
X%ZCiW4RCn2d2c4d ]	n e6U_]	X%]aWiSs]abCX%Q
] Lrh|d2kRgTVU_W4UBtVW4^_ehU_]a^TVZ/\5X%U_U_QCkR/]	d TVZ/UBTVZY]ab/Wi/d U_]a^d fSQ/]	d TV
Z 1^qEfiZ
X n2X%]aW4^&RCX%RgW4^sSd ]@uX%U-W4tVW4Z}d2kR*n2d2c4d ]	n e5U_]	X%]aWi8]abCX%]d ZU_TSkW6c4X%U_W4
U Lr6wTV^cfiW4Z
U 1^	 W q
 ''  
wTV^U_TSkW{X%]aTSk
U '|aTVfCd Z/Us/TV]aZCd ]a4esSPScbCX%^wU_]aWd ZCsVVVVsAPWcfi]	d TVZ~Vq2Vaq&-QS^&cfiTVZ]a^d f/QS]	d TVZd U
]aT6R/^_Ttgd2iVW;]ab/WR/^Wc4d U_W5cfiTVZCi/d ]	d TVZ/Uu ~/WkffkffX;Sq26X%ZCi  b/W4TV^WkSq S*Q/ZCiW4^@ubCd2cOb]abCd U-bCX%R/RgW4Z/Uq
BTVfCd Z/Us;T1iZCd ]a4es;X%ZCiP/cbCX%^wxU_]aWd ZaVVVV5XVn U_T|d Z]a^_T1iQCcfiWiX-X_eVW4Ud2X%ZvkW4]ab/Tgian2X%]aW4^
W4]aW4ZCiWife[PScbCX%^wU_]aWd Z>W4]jXVn2qBaVVVVa]ab*X%]YXVn2n T	uU8TVZ/Wff]aTU_RgWc4d2we[X6R/^d TV^5i/d U_]a^d f/Q/]	d TVZ>TtVW4^
XhRCX%^XVkW4]aW4q
^ oubCd2cObd ZCiSd2c4X%]aW4Us9d ZXhR/^_Wc4d U_W5U_W4Z/U_WsDb/T	u kQCcO
b D^YiW4t1d2X%]aW4Ujw^_TSk Lr60qVTV^
W41XVkRCn W.
s   }cfiTV^_^_W4URATVZ*iUff]aT}]abSW~U_W4]YTSwi/d U_]a^d f/QS]	d TVZ/
U D^5UX%]	d Uwe1d Z/
\ Lr60q  b/W~RS^_Wc4d U_W
cfiTVZ/Z/Wcfi]	d TVZlfgW4]u@W4W4Z~]abCd U0u9TV^_X%ZCihTVQS^_UZ/W4WiU wxQ/^]ab/W4^d Z`tVW4U]	d \SX%]	d TVZCq

`7_

fi " |qxq}t~"}s1|

:e $ T\  nql"   Wagi\ l \ba
fiZ  b/W4TV^_Wk Vq2X%ZCi
~ Wk5kffX8Sq2

n ^ " l)

c

 
" `^0a "s:k ^ "  l a

n ^ " 

v%2"%lg!

u9W~iW4Ucfi^d fgW
i rh d ZX%ZXVn \VW4f/^XVd2c'uX_es"X%U5XcfiTSn2n Wcfi]	d TVZoTSw
R/^_TVf*X%fCd2n2d ]	d W4U&UX%]	d Uwegd ZS\~cfiW4^_]	XVd ZW	QCXVn2d ]	d W4UqLfiU&]ab/W4^_WffXhkTV^_WR/^_T1cfiWiQ/^XVn20uXaejTSw^W4R/^_W4U_W4Z]	d Z/\
Lr60~ZRCX%^_]	d2cfiQCn2X%^shiTW4U]ab/W4^W>W4gd U]oXmUd Z/\Sn WkWcbCX%Z*d Uk ]abCX%]~\Sd tVW4U^d UW]a
T Lr6 U_QCcOb
]abCX%]o7 n	%c4X%U_WmTSZ
w Lr6 c4X%ZzfAWtgd W4u@WiX%U>XU_RgWc4d2XVn;c4X%U_WTSw&]abCd U>kWcbCX%Z*d Ukff WbCX_tVW
XVn ^_WXVieW4ZCcfiTVQ/Z]aW4^_WimX8RATVUUd fCn Wjc4X%Z*i/d2i/X%]aWwxTV^U_Q*cbXYkWcbCX%ZCd Ukff-]ab/
W {:f)zR/^_T1cfiWiQ/^_Wq
fiZjPWcfi]	d TVZ Sq 0u@W'iW4Ucfi^d fgWi-]abCd U@kWcbCX%ZCd UkX%ZCid ZCi/d2c4X%]aWi0]abCX%]@d ]C\VW4Z/W4^X%]aW4U*TVZCn e5i/d U_]a^d fSQ/]	d TVZ/U
]abCX%]*UX%]	d UwW
e Lr60d
q "ZCwTV^_]aQ/ZCX%]aWn esX%ULu@W"Z/TuoUb/T	u8s]ab/W4^W9W41d U_;
] rh[i/d U_]a^d f/Q/]	d TVZ/U*]abCX%]@c4X%ZSZ/TV]
fgW5d Z`]aW4^R/^_W4]aWiX%U;fAWd ZS\6\VW4Z/W4^X%]aWi6fN
e {:f*q
-QS^EW41XVkRCn W'd UCfCX%U_Wi0TVZjX%Z W4gXVkRCn W"\Sd tVW4Zhfe6d2n2n2st)X%ZiW4;
^ ~CXVX%ZCs)X%ZCiBTVfCd Z/U@aVVVVasVub/T
u@W4^_WXVcfi]aQCXVn2n e]abSW8/^U_];]aT>cfiTVZ/Ud2iW4^ub/W4]ab/W4^]ab/W4^_W W41d U_fi
]  ZCX%]aQ/^XVn2LkWcObCX%ZCd UkU]ab*X%];\VW4Z/W4^X%]aW
XVn2nLX%ZCiYTVZCn eiSd U_]a^d f/Q/]	d TVZSU'UX%]	d Uwxe1d Z/
\ Lr60q  b/W4eUb/T	u ]abCX%]ffd ZmU_W4tVW4^XVnARS^_TVfCn WkU'TSw1U_QS^_tgd tXVn
X%ZCXVn eUd UsLTVfSU_W4^_tX%]	d TVZ/UX%^_W\VW4Z/W4^X%]aWiXVc4cfiTV^i/d ZS\l]aTlubCX%] ]ab/W4ezc4XVn2n@Xfi/|
 ph,
 phS/
	,,	//Y	7 ph4q  b/W4e>XVn U_T5Ub/T	u]abCX%]]ab/Wd ^;^X%ZCiTSkffd 4WiffUcbSWkW{\VW4Z/W4^X%]aW4UTVZCn e>i/d U]a^d f/Q/p
]	d TVZ/U']abCX%]'UX%]	d Uw
e Lrh;qfiZwXVcfi]	s/]abSW^X%ZCiVTSkffd 4WikTVZ/TV]aTVZ/WjcfiTSX%^U_W4ZCd Z/\jUcbSWkW8]aQS^_Z/UTVQ/];]aT
fgWffX'U_RgWc4d2XVn1c4X%U_W{TS
w {:fzLs1XVn ]ab/TVQ/\Vbu@W5iT6Z/TV]-R/^_TtVW{]abCd U&b/W4^Wq{d2n2n2s)tX%Z}iW4
^ ~EXVX%Z*sDX%ZCi
TVf*d Z/U"U_bST	ufehW4gXVkRCn W0]abCX%]9]ab/W^X%Z*iTSkffd 4Wi5cfiTSX%^U_W4ZCd Z/\8Ucb/WkW4UiTZ/TV]BU_]
Q vlcfiW]aT\VW4Z/W4^X%]aW
XVn2U
n Lr6i/d U]a^d f/Q/]	d TVZ/Uq@DU_U_W4Z]	d2XVn2n e]ab/WUXVkWW41XVkRCn WU_bST	uU&]abCX%{
] {:fzoiVT`W4U&Z/TV]&Wd ]abSW4^q
 n/  p \eE LTVZ/Ud2iW4^-U_QSfc4X%UWhad2d2*TSwC11XVkRCn WSq26X%\SXVd ZCqV~/W4] & 4 + sg . X%ZCi!' & sC' + X%ZCi
' . fgW6X%U8d Z]abCX%]&W41XVkRCn Ws1X%ZCiX%UU_QCkWffwTV^&Ud2kRCn2d2c4d ]ej]ab*X%].  ' & U' + !' . q  b/WW4gXVkRCn W
U_b/Tu9Wij]ab*X%]]ab/W4^_WhW4gd U]aU5i/d U_]a^d f/QS]	d TVZ/=
U 1^UX%]	d Uwe1d Z/
\ Lrh$d Zl]ab*d U5c4X%U_W6u8d ]ab1^Y'1*V$wTV^
l VV4V4VVshXVn2n"bCXat1d Z/\cfiTVZCi/d ]	d TVZCXVn9R/^TVfCX%fCd2n2d ]	d W4
U 1^	    +
 *UW
    VV[wxTV^XVn2n
| * q
q n WX%^n es" & 4 + X%ZCi . c4X%Z/Z/TV]fgWh\V^_TVQ/RgWiY]aTV\VW4]ab/W4^]aT}wTV^k X8U_W4]TSwERCX%^_]	d ]	d TVZ/U{TSw
vq&PTSs)W4tVW4Z]ab/TVQ/\V
b Lr6bSTSn2iU8wTV
^ 1^	C
s Q6f)zc4X%ZSZ/TV]fgWQ/U_Wih]aT5Ud2kQ*n2X%]aq
W 1^	q

bCd2n W{d2n2n2s0t)X%Z iW4
^ ~EXVX%Z*sX%ZCiTVfCd ZSUaVVVVshPWcfi]	d TVZ$VX%U_|ub/W4]ab/W4^j]ab/W4^_WmW41d U_]aU}X
\VW4Z/W4^XVn9kWcb*X%ZCd UkwTV^\VW4Z/W4^X%]	d Z/\oXVn2n"X%Z*iTVZCn eLrhi/d U_]a^d fSQ/]	d TVZ/UsD]ab/W4e[iVTZ/TV]YkffX%VW5]abCd U
	Q/W4U_]	d TVZ}kffX%]ab/Wk5X%]	d2c4XVn2n ejRS^_Wc4d U_Wq*r'U&Z/TV]aWi8f`em{d2n2n2st)X%Z>iW4Z
^ ~CXVX%ZCsgX%ZCiTVfCd Z/Us]abSW'RS^_TVfCn Wk
b/W4^_Wjd U]abCX%]0u d ]ab/TVQ/]X%ZecfiTVZSU_]a^XVd Z]0]aT5u0bCX%]cfiTVZ/U_]	d ]aQ/]aW4U6XtXVn2d2i~kWcbCX%ZCd Ukffs)]ab/W4^_Wjd U c4n WX%^n e
X-]a^d t1d2XVnVUTSn Q/]	d TVZ5]aT8]ab/WR/^_TVf*n Wkff9d tVW4ZmX{iSd U_]a^d f/Q/]	d TV
Z 1^BUX%]	d Uwxe1d Z/q
\ rh;su9W;Ud2kRCn ei^X_uX
u@TV^n2
i $XVc4cfiTV^i/d Z/\h]a
T 18^ sgX%ZCi]ab/W4Zi^Xau |U_QCcOb]abCX%H
]  >vXVc4cfiTV^i/d ZS\ ]aTh]ab/W6i/d U_]a^d f/Q/]	d TVZ
1^	    
 W   aq  bCd Ud U0TVftgd TVQSUn emcOb/WX%]	d Z/\md ZU_TSkWU_W4ZSU_WqLfiZ`]aQCd ]	d tVWn esCd ]U_W4WkU0]abCX%]
X  ^_WX%U_TVZCX%fCn W&kWcbCX%ZCd Uk Ub/TVQCn2i~Z/TV]hfAWmXVn2n T	u@Wim]aTcb/TTVU_W> XVc4cfiTV^i/d ZS\l]aTXmi/d U_]a^d f/Q/]	d TVZ
N
iW4RgW4ZCi/d ZS\{TV
Z  q*fi]iTW4U-Z/TV]@bCXatVW]abCX%]-1d ZCi{TSwEcfiTVZ`]a^_TSn`TtVW4^&]ab/W;TVfSU_W4^_tX%]	d TVZ/U&]abCX%]X%^_WffkffXViWq
PTmub*X%]cfiTVQ/Z]aU~X%U~X^_WX%U_TVZCX%f*n W>kWcb*X%ZCd Ukff$Z]aQCd ]	d tVWn es"]ab/W>kWcb*X%ZCd UkU_b/TVQ*n2i~fgW
X%fCn W{]aTmcfiTVZ`]a^TSnTVZCn eubCX%]c4X%ZfgW5cfiTVZ`]a^_TSn2n Wild ZX%Z~W4`RgW4^d2kW4Z]	XVnUW4]aQ/RCqLWc4X%Z~]abCd Z/jTSw7]ab/W
kWcObCX%ZCd UkX%U X%Z%X%\VW4Z`]	]abCX%]&Q/U_W4UX'UW4]&TSw/U_W4ZSU_TV^_U&]aTffTVf/]	XVd Zd ZCwxTV^kffX%]	d TVZoX%fgTVQ/]-]ab/Wu@TV^n2i/q
^ XE>q|bCd2n W]abSWmX%\VW4Z]~kffXaezcfiW4^_]	XVd ZCn ecb/TTVU_WubCd2cOb
 b/WX%\VW4Z]~iT`W4UhZ/TV]hbCX_tVW>cfiTVZ`]a^_TSn*TtVW4a
U_W4Z/UTV^']aTQ/UWs*d ]ffd UZ/TV]'^_WX%U_TVZCX%f*n W ]aT>X%U_UQCkW ]abCX%]U_b/Wc4X%ZcfiTVZ`]a^_TSn7]ab/Wd ^TVQS]aR/Q/]6TV^'W41XVcfi]	n e
ubCX%]]ab/W4e[c4X%ZU_W4Z/U_Waq{Z*iW4Wi/sA\Sd tVW4Z|X8u@TV^n2q
i  sA]ab/W6TVfSU_W4^_tX%]	d TVZ^_W4]aQ/^_Z/WiYfe~]ab/WhU_W4ZSU_TV^5d U
wQCn2n eliW4]aW4^kffd Z/Wi/q  bCd Ud U&W41XVcfi]	n ej]ab/WU_W4]aQ/R}d2kRCn WkW4Z`]aWid Z]ab/6
W {:f)zUcOb/WkW5i/d UcfiQSU_U_Wi
d ZPWcfi]	d TVZSq Ss*ubCd2cObu@Wj]ab/W4^_WwTV^_WY^W4\SX%^i[X%UXln W4\Sd ]	d2kffX%]aWokWcObCX%ZCd Uk5qWjZ/T	ud Z]a^_T1iQCcfiW
XhR/^TgcfiWiQS^_!
W {:f  s1ub*d2cb}W4]aW4ZCi
U {:f*s9X%Z*i]aQ/^_Z/UTVQ/]{]aT~\VW4ZSW4^X%]aWmXVn2n"X%ZCiTVZCn e
i/d U_]a^d f/Q/]	d TVZ/U{UX%]	d Uwxe1d Z/
\ Lrh;!
q 4QSU_]5n2d V\
W {:f)z*e
s {:f  X%U_U_QCkW4U]abCX%]{]abSW4^_Wd UjXcfiTSn p
n Wcfi]	d TVZTSwAU_W4Z/UTV^_Us*X%ZCi~d ]6cfiTVZ/U_QCn ]aU6X\Sd tVW4ZlUW4Z/U_TV^'u8d ]ab[XYcfiW4^_]	XVd ZmR/^_WiW4]aW4^kffd Z/Wi5R/^_TVfCX%fCd2n2d ]eq

`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

y"T	u@W4tVW4^sAQ/ZCn2d VU
W {:f*2
s Q6f)z  kffXaed \VZ/TV^_WX{U_W4ZSU_TV^;^WXVi/d Z/\Sq  b/WjiWc4d Ud TVZlu0b/W4]ab/W4^
TV^ZSTV]YX6U_W4ZSU_TV^^_WXViSd Z/\od Ujd \VZSTV^_Wi}d UXVn2n T	u@Wi]aT[iW4RgW4ZCiTVZCn emTVZo]ab/WffU_W4Z/UTV^]abCX%]8bCX%UfgW4W4Z
cbSTVU_W4Zfej]ab/WjX%\VW4Z]	sEX%ZCiffTVZ~]ab/WTVf/U_W4^_tX%]	d TVZ\VW4ZSW4^X%]aWifffej]abCX%]0U_W4ZSU_TV^qfi]8d U;Z/TV]8XVn2n T	u@WiY]aT
iW4RgW4ZCi8TVZ~]abSW5XVcfi]aQCXVnu@TV^n2i/sUd Z*cfiW]ab/WYX%\VW4Z] kffX_eYZ/TV]0`Z/TuubCX%]&]abSWYXVcfi]aQCXVn)u@TV^n2id Uq0-ZCcfiW
X%\SXVd ZCs7]ab/W8R/^_T1cfiWiQ/^_WYd U~^_WX%U_TVZ*X%fCn Wd Z]abCX%]']ab/WX%\VW4Z`]ffd U'TVZCn e}XVn2n T	u@Wij]aTmcfiTVZ]a^_TSn7ubCX%]hc4X%Z
fgW5cfiTVZ`]a^TSn2n Wid ZoX%Z~W4`RgW4^d2kW4Z]	XVn)U_W4]aQSRCq
W Z/T	uvR/^W4U_W4Z`Q
] {:f  s]ab/W4ZX%^_\VQ/W ]abCX%]hd ]hd U~^_WX%U_TVZCX%f*n Wjd Zl]ab/W8U_W4Z/U_WX%fATtVWsX%ZCi
]abCX%]8d ]0\VW4Z/W4^X%]aW4U6XVn2nDX%ZCihTVZCn e
e Lr6i/d U]a^d f/Q/]	d TVZ/Uq
[<a21\ rk a\

{:f)z 

VqZD^_W4RCX%^X%]	d TVZ*
Ld >X%ZoX%^_f*d ]a^X%^_e>i/d U_]a^d fSQ/]	d TVZ^Xg




Ld >XU_W4](O

TVZvq

TSw/R*X%^_]	d ]	d TVZ/U0TSwRvs1X%ZCihSlX%ZX%^fCd ]a^X%^_ei/d U_]a^d fSQ/]	d TVZ^X[TVZ[O5q

 b/T`TVUW6Z`QCk0fgW4^_Ufiff h V4V{X%ZCi<w_  h V4;wxTV^WXVcOb}RCXVd ^ja"  @U_QCcb}]abCX%]  
 
O X%ZCi   UX%]	d Uwe1d Z/\>]ab/W[wxTSn2n Tu8d Z/\cfiTVZSU_]a^XVd Z]	swxTV^YWXVcb s U_QCcb]abCX%]
X  hKV
  
X;
   Y _  i
S
) P  / F  P   `
Vq{&W4Z/W4^X%]	d TVZC
Vq2b/T`TVUW.v
Vq2b/T`TVUW
Vq2m

 O
 x

XVc4cfiTV^iSd Z/\ff]aTaXE>q

XVc4cfiTV^i/d Z/\5]aTaXq[~/W4]8fgW]ab/WQ/ZCd2	Q/WU_W4]8d Z

d ]ab~RS^_TVfCX%fCd2n2d ]e
U]aW4RVq2Vq

  _ 

s)^W4]aQ/^_Z} 480X%ZCi6bCXVn ]	qL



U_QCcOb]abCX%]Vv>q

d ]ab~R/^_TVf*X%fCd2n2d ]eq

_ 

s\VTff]aT

fi]hd UWX%U_e~]aTjUW4W8]abCX%6
] {:f)zd U']abSW8U_RgWc4d2XVnLc4X%U_W TSw){:f)zFub/W4^_W! _   YwTV^ffXVn2n
a"  aq8rhn2n Tu8d Z/\^w_ \Sd tVW4U8Q/UffXn2d ]a]	n WkTV^_W3nSW4gd fCd2n2d ]eq  TQ/ZCiVW4^_U_]	X%ZCiY]ab/Wh^_TSn W6TSwE]ab/W
cfiTVZ/U_]a^XVd Z`]YSas1Z/TV]aW6]abCX%]w_ d U{]abSW RS^_TVfCX%fCd2n2d ]e]abCX%]]abSW~XVn \VTV^d ]abCkiTW4UZ/TV]]aW4^k5d ZCX%]aW~X%]
U_]aW4RoVq2Vs\Sd tVW4Z]abCX%]$X%Z*i  X%^_W5cOb/TVU_W4ZoX%]&U]aW4RoVq2VqLfi]wxTSn2n TuU0]abCX%]&]abSWR/^_TVfCX%fCd2n2d ]e4l]abCX%]
XRCXVd ^   4;d UZ/TV]TVQS]aR/Q/]X%]U_]aW4RVq25wTV^U_TSkWYd U

  

 
  `
) P / F P 

X  Y _  i

b Q/UsAS*UXaeU&]abCX%]-]ab/WR/^_TVf*X%fCd2n2d ]e4]abCX%]{X0R*XVd ^&ub/TVU_W'/^_U_]cfiTSkRgTVZ/W4Z]d U[d U&Z/TV]9TVQS]aR/Q/]
 
X%]U_]aW4R[Vq2ffd U]ab/WUXVkWYwTV^8XVn2n4 vq
{:f)z  c4X%Z\VW4Z/W4^X%]aW>]ab/
W Lrh i/d U_]a^d fSQ/]	d TVZ$d Z11XVkRCn W}Sq2Vs'ubCd2cbcfiTVQCn2i[Z/TV]fgW
\VW4Z/W4^X%]aWif`^
e {:fzLq  TU_W4W6]abCd UsAQSUd Z/\]ab/W6UXVkWhZ/TV]	X%]	d TVZzX%U5d Z]ab/W6W41XVkRCn WsBcfiTVZ/Ud2iW4^
]ab/WmU_W4]TSw&R*X%^_]	d ]	d TVZ/x
U O    &   +   . u8d ]ab  *  V *  ' * Vq ~W4q
] X    &   X    +  
X;-  .   VVV`
s  / )  VsCX%ZCN
i   )  Vq&fi] d UWX%U_e~]aTYtVW4^d2we]abCX%]6wxTV^6XVn2g
n $
 vsu@W8bCX_tVW

]abCX%]
 P   F  P   ` X    Y  _   VVVsU_Tl]abCX%]h]ab/WcfiTVZ/U_]a^XVd Z]~Shd U UX%]	d U_/Wi/q5TV^_W4T	tVW4^s
i/d ^_Wcfi]c4XVn2cfiQCn2X%]	d TVZU_bST	uUj]abCX%]	swTV^mX%^_f*d ]a^X%^_e XE>s@]ab/WiSd U_]a^d f/Q/]	d TV{
Z 1^  TVZ^_QSZ/U5\VW4Z/W4^X%]aWi
fa
e Q6f)zF-u8d ]ab~]abCd Uhcb/TSd2cfiW TSw7RCX%^XVkW4]aW4^U d U'R/^_Wc4d U_Wn e]ab/W8Q/ZCd2	Q/W5i/d U_]a^d fSQ/]	d TVZUX%]	d Uwe1d Z/\
Lr6d Z]ab*d U8c4X%U_Wq

`7

fi " |qxq}t~"}s1|

PT~ub`ed 
U {:fz`'^_WX%U_TVZCX%f*n W$1tVW4Zo]ab/TVQ/\Vbou9Wffb*XatVWjZ/TV]{\Sd tVW4ZX~wTV^kffXVnBiW4SZCd ]	d TVZ
TSw'^WX%U_TVZCX%fCn WaXVn ]abSTVQ/\Vb|d ]5c4X%Z>fgWiTVZ/Wd Z]ab/W6^_Q/ZSUhw^XVkW4u@TV^__W4U_U_W4Z]	d2XVn2n esDWXVcOb>U_]aW4R>TSw
]ab/WoXVn \VTV^d ]abCk c4X%ZviW4RgW4ZCilTVZCn eTVZd ZCwTV^kffX%]	d TVZvXatXVd2n2X%fCn W~]aTo]ab/WW4`RgW4^d2kW4Z`]aW4^s9ubSW4^_W]ab/W
%d ZCwTV^kffX%]	d TVZCd UhW4ZCcfiT1iWi[d Z[]ab/WTVf/U_W4^_tX%]	d TVZ/U~kffXViVWYfe>]ab/WW4RAW4^d2kW4Z`]aW4^~d Z[]ab/WcfiTVQ/^_UWjTSw
^_Q/ZSZCd Z/\h]ab/W5XVn \VTV^d ]abCk5asCd ]8d U0Z/TV]bCX%^i ]aTYU_W4W{]abCX%{
] {:fz  UX%]	d U_/W4U;TVQ/^8d Z]aQCd ]	d tVWjiW4Ud2iW4^_p
X%]	XVq  b/WjVW4e}RgTSd Z]d Uh]abCX%]XVn2nC]ab/Wj^_Wn W4tX%Z`]6U_]aW4RSUd Z[]ab/WXVn \VTV^d ]abCk c4X%ZfgWmc4X%^_^d WilTVQ/] f`e
X%ZmW4`RgW4^d2kW4Z`]aW4^q  bSWRCX%^XVkW4]aW4^_fi
U 5X%ZC4
i  _  wxTV^
 OX%ZCi
 N
 X%^_WcbSTVU_W4ZmfAWwTV^_W ]ab/W
XVn \VTV^d ]abCk fAW4\Sd ZSUg]abCd U5c4X%ZcfiW4^_]	XVd Z*n efgWiTVZ/WhfeX%ZW4RAW4^d2kW4Z`]aW4^q5P/d2kffd2n2X%^n esBd ]ffd UU_]a^XVd \Vb]ap
wTV^_uX%^i5]aTcbSWcl]abCX%]]ab/W8W	QCX%]	d TVZS9b/TSn2iUffwTV^;WXVcO
b 
 vq;r'U6wTV^']ab/WXVn \VTV^d ]abCk d ]aU_Wn2ws
]ab/WYW4`RgW4^d2kW4Z]aW4^ bCX%UZ/T[cfiTVZ`]a^TSnDTtVW4^6]ab/WlcOb/TSd2cfiWYTS
w hL]abCd Ud UjcOb/TVU_W4Z[f`emZCX%]aQS^_WlXVc4cfiTV^iSd Z/\
d ]aUliSd U_]a^d f/Q/]	d TVZ*(
s X  qy"T	u@W4tVW4^s-]ab/WW4RgW4^d2kW4Z]aW4^lc4X%ZRgW4^wTV^kU_]aW4RSUVq2}X%ZCiVq2Vs@]ab*X%]d U
cbST`TVUd ZS\  !
 OXVc4cfiTV^iSd Z/\6]aTh]ab/WR/^_TVfCX%fCd2n2d ]emiSd U_]a^d f/Q/]	d TVq
Z X;&sAX%Z*i^W4Wcfi]	d Z/\6]abSW'TVfSU_W4^_tX%]	d TVZ
u8d ]abR/^_TVf*X%fCd2n2d ]4
e  _  s`Ud ZCcfiW]ab/WW4RAW4^d2kW4Z`]aW4^-Z/T	u0U&fATV]abj]ab/WU_W4Z/UTV^cOb/TVU_W4Z}ad2q Wq2s  -X%ZCi
]ab/W{TVfSU_W4^_tX%]	d TVZaaq
] {:fFiTW4UW4gXVcfi]	n eu0bCX%]&u9WuX%Z]	q
 bSW5wxTSn2n Tu8d Z/\5]abSW4TV^_Wk U_bST	uU]ab*X%f
\b"a\ 
 eE @ 	n 	Y,	g G8%)/|n	'jR G8%
{/jY,	R G8|	n	S,
:
1^@%	)Y. C,MK-xQy ;//
 ;'@ 	/;G'"`fi|ph		%
z ,a
 GM(
} {:fz`0)	8ff	&7 /mE1^	a5	W>/  h4  / 
V'7fi| | +j`{:fz`8fi	),S 48,



9G6 zEJ

I KMqWNOJ I ~JF I 

fiZ]abSWR/^_W4t1d TVQ/UhU_Wcfi]	d TVZCs"u9W>X%U_U_Q*kWil]abCX%]6]ab/W>d ZCwTV^kffX%]	d TVZ^_WcfiWd tVWi>uX%U6TSwB]ab/W}wTV^k ]ab/W
XVcfi]aQCXVnLu@TV^n2id U~d ZVq}ZCwTV^kffX%]	d TVZkQ/U] fgW>d Z]abCd U~wTV^k]aT|X%RSRCn ecfiTVZCiSd ]	d TVZCd Z/\SqzCQ/]~d Z
\VW4Z/W4^XVn2s*d ZCwTV^kffX%]	d TVZiTW4U;Z/TV]hXVn uX_e`UffcfiTSkWd Z~UQCcblZCd2cfiWR*XVcX%\VW4Uq&fiZ~]abCd U'U_Wcfi]	d TVZmu@WU_]aQCiVe
kTV^_W\VW4Z/W4^XVn)]e`RgW4UTSw7TVf/U_W4^t)X%]	d TVZ/UsDn WXVi/d Z/\ff]aT5\VW4Z/W4^XVn2d X%]	d TVZ/UTSwLcfiTVZCi/d ]	d TVZCd ZS\Sq

:e

\&Za\

%-

 lr)" ^ "  )
l "l)

7W4^_bCX%R/UE]ab/WUd2kRCn W4U]DRgTVU_Ud fCn W\VW4Z/W4^XVn2d X%]	d TVZd UE]aT8X%U_U_QCkW]abCX%]D]ab/W4^_W0d U-X*RCX%^_]	d ]	d TVZV & ^i^i^i	4 j 
TSw
X%ZCi]ab/WX%\VW4Z]TVfSU_W4^_tVW4Uq &  & ^i^i^i^gjAFj7s1ub/W4^W &
 ^^  ^^Xgj  Vq  bCd Ujd U]aT~fgW
d Z]aW4^_R/^_W4]aWiX%UX%Z[TVf/U_W4^_tX%]	d TVZ]abCX%]n WXViU6]ab/W>X%\VW4Z]6]aTmfAWn2d W4tVW> u8d ]ab[R/^_TVfCX%fCd2n2d ]eE	s&wTV^
  V^ i^i^i	6 l"qLr6c4cfiTV^i/d ZS\ff]a
T 4W4A^_W4ecfiTVZCi/d ]	d TVZCd ZS\Ss\Sd tVW4ZX5i/d U_]a^d f/QS]	d TV^
Z XgTV
Z vs
XE>u6

  & Xg>ufi &   ^^  Ej;XEufiFjAui
4W4A^_W4ecfiTVZ*i/d ]	d TVZCd Z/\d U>iW4/Z/Wi>TVZCn ed2w.C*q od2kR*n2d W4Uj]ab*X%]4XE>a+* V d2w.C*  X%ZCi
XE>a+*fi  Vs]abSW4Z;*Xg}ufi**d U]	X%VW4Z5]aTfAW{Vq
 n WX%^n e8TV^i/d ZCX%^ejcfiTVZCi/d ]	d TVZ*d Z/\5d U]ab/W-U_RgWc4d2XVn
c4X%U_W8TSwW47^W4e}cfiTVZCi/d ]	d TVZCd Z/\Yub/W4^_WWC
 *  YwxTV^;U_TSkW{@U_TSsDX%U6d U0U]	X%ZCi/X%^i/su9WYiVWn2d fAW4^X%]aWn eQ/U_W
]ab/W{UXVkWZ/TV]	X%]	d TVZwxTV^QSRiSX%]	d Z/\hQ/Ud Z/\ W47^_W4e>cfiTVZCiSd ]	d TVZCd Z/\X%ZCi6TV^i/d ZCX%^_emcfiTVZCi/d ]	d TVZCd ZS\Sq
WffZST	u uX%Z`]]aTiW4]aW4^kffd Z/W6u0b/W4ZQ/RBi/X%]	d Z/\d Z>]ab/W6ZCXVd tVW5U_RCXVcfiW6Q/Ud ZS\e W47^W4e[cfiTVZCiSd ]	d TVZ/p
&

 & ^i^i^i	^EjSFj7

d Z/\[d UX%R/RS^_TVR/^d2X%]aWq  bQ/UsEu@WlX%U_U_Q*kW5]abCX%]8]ab/WlX%\VW4Z]	 U6TVf/U_W4^t)X%]	d TVZ/U Z/TubCX_tVWY]abSWlwTV^kTSw
 &  & ^ i^i^i	^  j  j wxTV^&UTSkWRCX%^_]	d ]	d TVZoV & ^i^i^i4 j ;TSwFvq&urd 7W4^W4Z`]TVfSU_W4^_tX%]	d TVZ/U8kffX_esDd Z\VW4Z/p
W4^XVn2s-QSU_Wi/d AW4^_W4Z]YR*X%^_]	d ]	d TVZ/Uq2 4Q/U_]mX%Uu9Wi/d2iwTV^j]ab/W[c4X%UW]abCX%]jTVf/U_W4^t)X%]	d TVZ/U>X%^_WlW4tVW4Z]aU
aPWcfi]	d TVZVsS/^_U_];RCX%^X%\V^X%RSbCas/u@W TVZCcfiWX%\SXVd ZX%U_UQCkW]ab*X%];]ab/WX%\VW4Z]	 UTVf/U_W4^_tX%]	d TVZ/UffX%^_WXVc4cfiQ/p
^X%]aWqjbCX%]jiVT`W4U ]abCX%]jkWX%Zzd Zo]ab/W5R/^W4U_W4Z`]jcfiTVZ`]aW4]	WYUd2kRCn em^_WQ*d ^_W5]abCX%]	s-cfiTVZCiSd ]	d TVZCXVn

`7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

TVZkffX%1d Z/\]ab/WhTVf/U_W4^_tX%]	d TVZCsg]abSW RS^_TVfCX%fCd2n2d ]e~TSw"*^_WXVn2n ed UWC*0wTV^'
  V^ i^i^i	6 l"su@W{bCXatVW

D^W>*C   



V^i^i^i6l"q  bCX%]ffd UswTV^

 & ^i^i^i	^EjSFj7  C*Gi
aV
 bCd Uc4n WX%^n ej\VW4Z/W4^XVn2d 4W4U]abSW;^_W	QCd ^_WkW4Z`]@TSwDXVc4cfiQ/^XVcfieY\Sd tVW4Z}d Z]ab/Whc4X%UW']ab*X%]@]abSW;TVf/UW4^_t)X%]	d TVZSU
X%^_W{W4tVW4Z]aUq
w"TV]&U_Q/^_RS^d Ud Z/\Sn es`]ab/W4^_W5d UX'\VW4ZSW4^XVn2d X%]	d TVZTSwS]ab/W
W Lr6cfiTVZCiSd ]	d TVZ]abCX%]d U&Z/W4WiWi ]aT5\VQCX%^_p
X%Z]aW4W]abCX%
] W47^W4emcfiTVZCiSd ]	d TVZCd Z/\lc4X%Z~fAW5X%RSRCn2d Wih]aTff]ab/WZCXVd tVW{U_RCXVcfiWq
&

 o A|,|	+W1^heL`%V & ^i^i^i4Fj7YGZ*/{A|,|	+	
\b"a\ 
:e L
 & ^ i^i^i^gj)ffR &  ^^  gj   E ) 8 |	,	n	 &  & ^i^i^i^gj7FjE#Lp 
@VV^i^i^i 6"
l W* lK+Sfffi6,	)n	+	S
Oz %}9=D
 ^	   8V &
 e1 ^W     8  D ^8
 6 &  & ^i^i^i^gj7Fj7R 
 >*fi
zu}D ^    W     D ^	   W +*fi	 8?
 [+*)D ^	W 
 !
h
 
LX%^_]fffC"TSw  b/W4TV^WkVq2d U6X%ZCXVn TV\VTVQ/U{]aTjRCX%^_]6ac49TSw  b/W4TV^WkVq2Vq  bSW4^_WX%^WXZ`Q*kfgW4^


TSwEcfiTVZ*i/d ]	d TVZ/U-WQ*d t)XVn W4Z]&]aTfCC]abCX%]-u9WffcfiTVQCn2i8bCX_tVWU_]	X%]aWi/sUd2kffd2n2X%^d ZU_R*d ^d ]9]aTh]ab/W6cfiTVZCi/d ]	d TVZSU
d Z  b/W4TV^_Wk Vq2V
q w"TV]aW6]abCX%]]ab/W4U_WX%^_WhW4tVW4ZkTV^_W U_]a^d Z/\VW4Z]ffcfiTVZCi/d ]	d TVZ/U]ab*X%ZX%^W ^WQCd ^WiwTV^
TV^i/d Z*X%^_emcfiTVZ*i/d ]	d TVZCd Z/\ff]aT5fAW5X%RSR/^_TVR/^d2X%]aWq
DgXVkR*n W4UhVq2jX%ZCi6Sq2YXVn ^_WXVieUQ/\V\VW4U_]0]ab*X%]0]ab/W4^_WX%^_WZSTV]0]aTTmkffX%ZeZ/TVZ]a^d t1d2XVnUcfiW4ZCX%^d TVU
ub/W4^W6X%RSRCn egd ZS
\ 4W4A^_W4emcfiTVZ*i/d ]	d TVZCd Z/\ff]aT6]abSW{ZCXVd tVWU_RCXVcfiWjd U8X%R/R/^_TVRS^d2X%]aWq&yBTu9W4tVW4^s/Q/U_]{X%U8wTV^
]ab/WhTV^d \Sd ZCXV2
n rh cfiTVZ*i/d ]	d TVZCsA]ab/W4^_WiTW41d U_]U_RgWc4d2XVn7Ud ]aQCX%]	d TVZ/Ujd Zlu0bCd2cb\VW4Z/W4^XVn2d 4W
i Lr6 d U
X6^WXVn2d U_]	d2c5X%U_UQCkR/]	d TVZCqlTV^ TV^i/d ZCX%^
e Lr60s1u9WmkW4Z`]	d TVZSWi]ab/\
W {:fzkWcObCX%ZCd Uk aPWcfip
]	d TVZmSq2Vaq6TVW
^ 4W4A^_W4eocfiTVZ*i/d ]	d TVZCd Z/\SsXUd2kffd2n2X%^ffkWcObCX%ZCd Uk kffX_e~fgWjX^_WXVn2d U_]	d2c6kT1iWn*d ZmU_TSkW
Ud ]aQCX%]	d TVZSUub/W4^W~XVn2ngTVf/U_W4^_tX%]	d TVZ/U8^_WwxW4^]aT]ab/WffUXVkWffRCX%^_]	d ]	d TVZzV & ^ i^i^i4
 j7hTS;
w vq W5Z/Tu
iW4Ucfi^d fAWX"UcfiW4ZCX%^d TffwTV^*UQCcbX9Ud ]aQCX%]	d TVZCq-PQSR/RgTVU_0
W cfiTVZ/Ud U]aUTS]
w ="TVf/U_W4^_tX%]	d TVZ/(
U  & ^ i^i^i	 2
u8d ]a
b `*  C* &  & ^ i^i^i	^ C*(jSF
 jU_QCcOb>]abCX%]5XVn2!
n C*(V
q w"T	u8sD/
 laX%^_f*d ]a^X%^_e1{cfiTVZCi/d ]	d TVZCXVn"i/d U_p
]a^d f/QS]	d TVZ/W
U 18^ 	sg  V^ i^i^i6 l"sgTV
Z vq{fiZ]aQCd ]	d tVWn eH
s D,^ ffd 
U D,^ >u fiF
 aq
q TVZ/Ud2iW4^]ab/WwTSn2n T	u8d ZS\
kWcObCX%ZCd Ukff6/^_U_]X%Z[TVf/U_W4^_tX%]	d TVZ `*{d Ucb/TVUW4ZaXVc4cfiTV^i/d Z/\]aTmU_TSkWi/d U_]a^d fSQ/]	d TVJ
Z X  TV
Z a
]ab/W4ZX{UW4]   d UffcbSTVU_W4Zu d ]abR/^TVfCX%fCd2n2d ]
e  *( ad2q Wq2sLXVc4cfiTV^i/d Z/\]aTj]ab/Wji/d U]a^d f/Q/]	d TVZd ZCiQCcfiWifff`e
`*fia)/ZCXVn2n esDXu9TV^n2
i v>
 d U8cOb/TVU_W4Z[XVc4cfiTV^i/d Z/\ff]a
T 18^ q
4w*]ab/WjTVf/U_W4^_tX%]	d TVJ
Z `*X%ZCiu@TV^n2
i  X%^_Wj\VW4Z/W4^X%]aWi]abCd UhuX_esL]ab/W4Z]ab/W5\VW4ZSW4^XVn2d 4W
i Lr6
cfiTVZCi/d ]	d TVZb/TSn2iUs]ab*X%]8d UsDcfiTVZCiSd ]	d TVZCd Z/\d Z]abSWU_TVR/bCd U_]	d2c4X%]aWi5U_RCXVcfiWjcfiTSd ZCc4d2iW4U0u d ]a
b 4W4A^_W4ecfiTVZ/p
i/d ]	d TVZCd ZS\S

:e *S	{`%lV & ^i^i^i	4 j YGZ /j5BGj|	,	n	/
 8|n7S67n	,%	)qX   \X  Y`*fi #	6/'|VV^i^i^i^&

7,;5,,	)1^h ,ffeX   1^  z7X  'phfi	/D6#D^h0} S
1^5 K-	{6		S	fi
,} x0y/z8*ffpogzu}8} & ^i^i^i	4jY

[<a"ZW " ^ "  l



D^_TVRgTVUd ]	d TVZvVq2[iWkTVZ/U_]a^X%]aW4Uj]abCX%]	s&W4tVW4Z]ab/TVQ/\Vb]ab/WX%ZCXVn TV\VQ/WmTSw@]ab/WLrh cfiTVZCiSd ]	d TVZ
W4R/^_W4U_U_Wijd Z  b/W4TV^_WkVq2hd U9b*X%^i{]aThUX%]	d Uwxed Zj\VW4Z/W4^XVn2s1X%]n WX%U_]{d2w)]ab/W'U_W4]V & ^i^i^i4 j  d U-]ab/W
UXVkW6wTV^XVn2n`TVf/UW4^_t)X%]	d TVZSUs]ab/W4ZwxTV^-W4tVW4^_eYU_QCcbU_W4]-TSw)TVfSU_W4^_tX%]	d TVZ/U-]ab/W4^_WW4gd U],|ph'R/^d TV^_UZ1^
TV
Z  wTV^ubCd2cOb>]ab/
W rhBpX%ZCXVn TV\VQ/W}UX%]	d U/Wi}wTV^jXVn2nDTVf/U_W4^_tX%]	d TVZ/Uq5r;U8u9WYU_b/Tu Z/W4]	s9wTV^
l0QSRiSX%]	d Z/\Ss]abCd U8d U;Z/T~n TVZ/\VW4^0]abSWYc4X%U_Wq
`7&=

fi " |qxq}t~"}s1|

:e $Z"%l"k*

\ p%n ^

"

\

 l ^`a"

%

 r:n ^ %" l)

bCX%]@X%fgTVQ/]9c4X%UW4UEub/W4^WB]ab/W;cfiTVZ/U]a^XVd Z`]aU&X%^_W9Z/TV]9d Z]abSWBU_RgWc4d2XVnwTV^k[ubSW4^_W04W4A^_W4e1 U@cfiTVZCiSd ]	d TVZ/p
d Z/\c4X%ZfgWhX%R/R*n2d Wi/
 7W4^_bCX%RSU@]abSW6kTVU_]cfiTSk5kTVZ}X%R/R/^_TSXVcOb>d Zj]ab*d Uc4X%U_Wffd U@]aThQ/UWhl09qAd tVW4Z
XcfiTVZSU_]a^XVd Z]ubSW4^_W8XcfiTVZSU_]a^XVd Z]d U@Ud2kRCn eXU_W4]"TSwR/^TVfCX%fCd2n2d ]eli/d U_]a^d f/QS]	d TVZ/U d Z]aQCd ]	d tVWn es`]ab/W
i/d U_]a^d f/Q/]	d TVZ/U UX%]	d Uwegd ZS\]ab/WcfiTVZ/U_]a^XVd Z`]	hX%ZCiXffRS^d TV^iSd U_]a^d f/Q/]	d TV>
Z XE TV
Z vsC]ab/Wld2iWXmd U ]aT
RCd2cOgsgXVkTVZ/\XVn2nAi/d U_]a^d fSQ/]	d TVZ/U@UX%]	d Uwegd Z/\h]ab/W6cfiTVZ/U_]a^XVd Z`]	s]ab/W'TVZ/W']abCX%]{d UY%c4n TVU_W4U_]	']aT ]ab/WR/^d TV^
i/d U_]a^d f/Q/]	d TVZCsAubSW4^_Wh]ab/W%c4n TVU_W4Z/W4U_UhTS
w X   ]a^
T XE d UYkWX%U_Q/^WijQ/Ud Z/\^_Wn2X%]	d tVWffW4Z]a^_TVReq  b/W
fi	+o nh	/fi7=
 ,	,	x
 X   /3
 XE u t-QCn2n fCXVcOl ~Wd f*n W4^sgVVVVC
 LT	tVW4^  bSTSkffX%UsAVVVVd U
iW4/ZSWiX%U
X     
X    hn TV\
i
Xg}  
FX
  bSW0n TV\SX%^d ]abCkb/W4^_W0d UD]	X%VW4Z8]aT]ab/WBfCX%UW0Vd2wTX      L]ab/W4ZUX   hn TV\gYX   ha/Xgo a
d U']	X%VW4Z]aTYfgWYVq  bCd U d U^_WX%UTVZCX%fCn W Ud ZCcfiWjn2d2k  -  n TV\g*V  Yd2wQvVq2  b/W^_Wn2X%]	d tVW6W4Z/p
]a^_TVRezd U5/ZCd ]aWR/^_Ttgd2iVWil]abCX%]aX   d Um|	,+
oS/))5u8d ]ab^W4U_RgWcfi]6]aTX  s0d Z]abCX%]~d2w
XE>    Vs)]ab/W48
Z X       VsDwTV^8XVn2n4 vq&-]ab/W4^_u d U_Ws1d ]8d U iVW4/Z/Wi ]aTfffgW5d Z//Z*d ]aWq
 bSWhcfiTVZ/U]a^XVd Z`]aU&u@WffcfiTVZ/Ud2iW4^-b/W4^WhX%^_W6XVn2ngc4n TVU_WiX%ZCicfiTVZtVW45U_W4]aU&TSw)RS^_TVfCX%fCd2n2d ]elkWX%U_QS^_W4Uq
fiZo]abCd Uc4X%U_Ws&d ]d U `ZST	uZ[]abCX%] ]ab/W4^_Wd UXffQ/Z*d2Q/Wli/d U_]a^d f/QS]	d TVZ]ab*X%]8UX%]	d U/W4Uh]ab/WmcfiTVZSU_]a^XVd Z]aU
X%ZCizkffd ZCd2k5d 4W4Uj]abSWl^Wn2X%]	d tVWW4Z`]a^_TVReq {d tVW4Z$XZ/TVZ/WkR/]ecfiTVZSU_]a^XVd Z^
]  X%Z*iX~R/^_TVf*X%fCd2n2d ]e
i/d U_]a^d f/Q/]	d TVa
Z XETV
Z vs/n W4
] XE>u 8BiW4Z/TV]aW]abSW8i/d U_]a^d fSQ/]	d TVZ6]abCX%];kffd Z*d2kffd 4W4U"^_Wn2X%]	d tVW;W4Z`]a^TVR`e
u8d ]ab^W4U_RgWcfi]&]aN
T X  q
4w*]ab/WcfiTVZ/U_]a^XVd Z]aUhbCXatVW]ab/WwxTV^k ]aTmubCd2cO
b W47^_W4e1 UQCn Wd UX%RSRCn2d2c4X%fCn WsL]abCX%]d Us-d2wL]ab/W4e
bCX_tVW~]ab/W[wxTV^k /
 X  
 X   a+
 *  ;*fifi  V^ i^i^i	6 l"}wTV^5UTSkW~RCX%^]	d ]	d TVZvV & ^ i^i^i4F
 jAVs9]ab/W4Z
d ]d UYu@Wn2n9`Z/TuZ]abCX%]Y]abSWi/d U_]a^d f/Q/]	d TVZ]abCX%]mk5d ZCd2kffd 4W4UW4Z`]a^TVR`e^Wn2X%]	d tVWl]aTX~R/^d TV<
^ Xg d U
XE>u 6 &  & ^ i^i^i	^ EjS
 jS0UW4Ws/Wq \Sq2s*u rd2XVcfiTVZCd U5
 DX%fgWn2n2sCVVVVaaq  bQ/UsDl0oQ/RBi/X%]	d Z/\6\VW4ZSW4^_p
XVn2d 4W4
U 4W4A^_W4emcfiTVZ*i/d ]	d TVZCd Z/\maX%ZCihb/W4ZCcfiW5XVn UT5U_]	X%ZCiSX%^icfiTVZCi/d ]	d TVZCd ZS\Saq
 T~U_]aQ*ie[l0Q/RBi/X%]	d Z/\}d Z}TVQ/^jw^XVkW4u@TV^_1s1u@WlX%U_U_Q*kW6]abCX%]]abSWffTVf/U_W4^_tX%]	d TVZ/UX%^_WffZ/Tu
X%^_fCd ]a^X%^_elc4n TVU_WicfiTVZtVW4>cfiTVZ/U_]a^XVd Z]aUTVZ]ab/WR/^TVfCX%fCd2n2d ]ekWX%U_QS^_Wq*r;\SXVd Z*su9WffX%U_U_Q*kW]abCX%]&]ab/W
TVf/U_W4^t)X%]	d TVZ/UX%^_WXVc4cfiQS^X%]aWmd Zo]abCX%]	s@cfiTVZCi/d ]	d TVZCXVnDTVZkffX%1d Z/\~]abSWffTVf/U_W4^_tX%]	d TVZCsC]ab/WcfiTVZSU_]a^XVd Z]aU
b/TSn2i/qTV^ZST	u8sEu9WlwxT1cfiQ/U{TVZo]ab/WffUd2kR*n W4U_]{RgTVU_Ud fCn W~c4X%UW5]abCX%]c4X%Z/Z/TV]{fgW6bCX%ZCiSn Wif`
e W47^_W4e
Q/RBi/X%]	d Z/\SqZ]abCd U5c4X%UWsBcfiTVZ/U_]a^XVd Z]aUYTVf/UW4^_t)X%]	d TVZSU-U_]	d2n2nAbCX_tVW6]ab/WwxTV^k  &  & ^ i^i^i^ gj7F
 j7sgf/Q/]
Z/Tu]ab/W+
 *fi U6iT5Z/TV]'bCX_tVW8]aTwxTV^k X{RCX%^_]	d ]	d TVZ]ab/W4eokffX_eT	tVW4^n2X%RX%Z*i/%TV^;ZSTV]hcfiTtVW4W
^ ;X%ZCi
]ab/
W  * iTZ/TV]bCX_tVW6]aTU_QCk]aT>Vq5PQ*cbX%ZTVf/U_W4^t)X%]	d TVZd U5XVc4cfiQ/^X%]aWd2w"d ]UX%]	d U_SW4UYaVasgQ/U_]ffX%U
fgWwxTV^Wq
W~c4X%ZmZ/Tu X%U~]ab/W UXVkWQSW4U_]	d TVZ/U]abCX%]u@WX%U_VWijfgWwTV^_WX%fATVQS];TV^iSd ZCX%^_e}cfiTVZCi/d ]	d TVZ*d Z/\
X%ZC
i W47^W4emcfiTVZCiSd ]	d TVZCd Z/\d Z]abSW{ZCXVd tVW{U_R*XVcfiWq
Vq-fiU&]ab/W4^_WffX%Z}XVn ]aW4^_Z*X%]	d tVWYcbCX%^XVcfi]aW4^d X%]	d TVZTSw/]ab/W5cfiTVZCi/d ]	d TVZSU&Q/ZCiW4^@ubCd2cOb};mQ/RBi/X%]	d Z/\
cfiTSd Z*c4d2iW4U{u8d ]abcfiTVZCi/d ]	d TVZ*d Z/\}d Zm]ab/WhU_TVR/bCd U_]	d2c4X%]aWiU_RCXVcfiW  bCX%]ffd UsX%^_Wh]ab/W4^_WX%ZCXVn TV\VQSW4U
TSw  bSW4TV^_WkVq2ffX%ZCi  b/W4TV^_WkVq2YwxTV^ ;Q/RBi/X%]	d Z/\S
Vq-r'^_W ]ab/W4^_WcfiTSkfCd Z*X%]	d TVZ/UTSw!X%ZCi
wxTV^'ubCd2cOb[d ]6d U'Z/TV];W4tVW4ZmRgTVU_Ud f*n W]abCX%]6;c4X%Z
cfiTSd Z*c4d2iWu8d ]abcfiTVZCi/d ]	d TVZCd Z/\ld Z]ab/W{U_TVRSbCd U_]	d2c4X%]aWi6URCXVcfiW


d ]ab~^_W4\SX%^i6]aTQSW4U_]	d TVZVsDd ] d UWX%U_e]aTffR/^_Ttgd2iVWYXffcfiTVQ/Z]aW4^_W41XVkRCn WUb/T	u8d ZS\6]abCX%]]ab/W4^_Wjd UZ/T
TVftgd TVQ/U&X%ZCXVn TV\VQ/W9]aT  bSW4TV^_WkVq2'wxTV^-;"q  b/W4^_W;d U-X0cfiTVZ/U_]a^XVd Z`]>U_Q*cb]ab*X%]D]ab/W'cfiTVZ*i/d ]	d TVZ8TSw

`77`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

RCX%^_]aXVDTSw  b/W4TV^_Wk$Vq20b/TSn2iUwxTV^l0Q/RBi/X%]	d Z/\{u0b/W4^_WX%U"R*X%^_]'fC9iTW4U9Z/TV]9b/TSn2i/q-W'TSkffd ]
]ab/W8iVW4]	XVd2n U"b/W4^_Wq2h{wAcfiTVQ/^_UWsSd ];d U9RATVUUd fCn W&]abCX%]B]abSW4^_W8X%^_WUTSkWQ*d ]aW8i/d AW4^_W4Z`]'cfiTVZCi/d ]	d TVZ/U9]abCX%]
cb*X%^XVcfi]aW4^d 4Wub/W4Zl0|Q/RBi/X%]	d Z/\[cfiTSd ZCc4d2iW4U6u8d ]abcfiTVZCi/d ]	d TVZCd ZS\[d Z[]ab/WYU_TVR/bCd U_]	d2c4X%]aWilURCXVcfiWq
y"T	u@W4tVW4^s/W4tVW4Z[d2w7]ab/W4eW4gd U_]	sU_QCcb[cfiTVZCi/d ]	d TVZ/UhkffXaefgWQ/ZCd Z]aW4^_W4U_]	d ZS\d Z]abCX%];]ab/W4emk5XaebCX%^iSn e
W4tVW4^X%R/RCn eqfiZCiW4Wi/sX%UXRCX%^_]	d2XVn;X%Z/U_u@W4^Y]aTzQSW4U_]	d TVZvVs@u@WlZ/T	u d Z`]a^_T1iQCcfiW[XtVW4^_eUd2kRCn W
U_W4]a]	d Z/\d Z~u0bCd2cb[;}QSRiSX%]	d Z/\6Z/WcfiW4U_UX%^d2n eon WXViU']aTlX^W4U_QCn ] iSd 7W4^_W4Z] w^_TSk cfiTVZCi/d ]	d TVZCd Z/\md Z
]ab/W{UTVR/bCd U_]	d2c4X%]aWiffU_RCXVcfiWq

~/W4]' & X%ZCi5 + fAW0]u@T8U_QSf/U_W4]aUBTSwU U_Q*cb5]ab*X%]Q &   &   + sg +   +   & sg .   & k  + s
%X ZCi 2    a & & + *X%^_W'XVn2n	Z/TVZ/WkR/]eqLTVZ/Ud2iW4^@X'cfiTVZ/U_]a^XVd Z]*TSw]ab/WwTV^km   &  & ^ +  + s
ub/W4^W. & ^ + X%^W&fATV]ab~d ZlaV4Vaq*W8d Z`tVW4U]	d \SX%]aW;ub*X%]bCX%R/RgW4Z/U&d2wu@W&Q/U_W{l0QSRiSX%]	d Z/\TVZ[q
P/d ZCcfiWff & X%ZCi + TtVW4^n2X%RX%Z*ijiVT ZSTV]cfiT	tVW4^]ab/WURCXVcfiWs1d Z\VW4Z/W4^XVnU4W4A^_W4emcfiTVZCi/d ]	d TVZCd Z/\~c4X%ZSZ/TV]
fgWX%RSRCn2d Wij]aTQ/RBi/X%]aWhTVZq  b/W4^_WX%^_WhU_TSkW6Ud ]aQ*X%]	d TVZ/U{ub/W4^Ws*iVW4U_RCd ]aW6]ab/WhT	tVW4^n2X%RCsKW47^_W4e
cfiTVZCi/d ]	d TVZ*d Z/\jc4X%ZYW4U_U_W4Z]	d2XVn2n e5fgWX%R/RCn2d Wi/qCW'UXaeh]abCX%]9TVf/U_W4^_tX%]	d TVZ<   &  & ^ +  + d U`L
+o d sXVwx]aW4^5l0Q/RBi/X%]	d ZS\5TVZTVZ/W TSw1]ab/WcfiTVZ/U]a^XVd Z`]aW
U  &  & TV=
^  +  + sS]abSW TV]abSW4^5cfiTVZ/U_]a^XVd Z]
b/TSn2iU6X%U'u9Wn2n2q  bCX%]hd U
s d 
U 4W4A^_W4epn2d VWu d ]ab^_W4URAWcfi]']a<
T X  ;d2wAWd ]ab/W4fi
^ X  a + 6 &  &    +
TV\
^ Xg}a & 6 +  +    & qoPVQ/R/RgTVU_Wff]abCX%\
] XE>a + 6 &  &    + ]abSW4Zd ]d UhWX%U_e}]aTmU_bST	u]abCX%]
XE>u 6 &  &   XE>u 6 &  & ^  +  + aq
fiZ`]aQ*d ]	d tVWn es d2w&]ab/W%c4n TVU_W4U_]	i/d U]a^d f/Q/]	d TVZX   ]aTJXE ]abCX%]UX%]	d U_/W4U^X   a &    & XVn U_T
UX%]	d U_SW4
U X   a +    + s]abSW4ZxX   d U*]abSWc4n TVU_W4U]0i/d U_]a^d f/QS]	d TVZ8]aTfiXE$]ab*X%]CUX%]	d U/W4U]ab/WcfiTVZ/U_]a^XVd Z]
   &  & ^  +  + q.w"TV]aW8]ab*X%]hl0oQ/RBi/X%]	d Z/\5TVZ* d U;W	QCd tXVn W4Z`]']aTW47^_W4e}cfiTVZ*i/d ]	d TVZCd Z/\jTVZ
*4a  Cau   aq  bQ/Us/d2w_d UQ4W4A^_W4epn2d VWs]abSW4Z5Q/RBi/X%]	d Z/\u8d ]ab4d UBW	QCd tXVn W4Z`]9]aTqW47^_W4e
Q/RBi/X%]	d Z/\Sq



 on 	z Gm
 %S/,	  / &  + Gm|	n	S,ff
 :e
' @
;* &  & ^C* +  + R   V4 
ZD^e,o%	) ,)	* 1^    & ,
1^	    + H
9/1^,}   D^W  hHv0	?$ E 	HD^  1^u  
*
 * ,"
 /~
KD^  * ff=phSLGD^ * 96f & Q + {/fi	fi
	ffB
6//-|n 1^   1^8u`*,\	Z,  V4
\b"a\

`* 

TV^ /Wi} & X%ZCi} + sDu@Wlc4X%Zd2iW4Z]	d2wxeX%ZTVf/U_W4^_tX%]	d TVZ &  & ^ +  + u8d ]ab}]ab/WYRCXVd ^ju & ^ + {
aV4V + qBZ*iW4^ffTVQ/^mcfiTVZCi/d ]	d TVZ/UYTVZ & X%ZCi + s9]ab/WU_W4]ffTSwXVn2nW47^_W4epn2d VWlTVf/U_W4^_tX%]	d TVZ/Uld UX
U_Q/fSU_W4]TSw9u~/W4fgW4U_\VQ/WkWX%U_QS^_W6TSwD]ab*d U{U_W4]	q  bQ/Usg]ab/W6UW4]TSwDTVf/UW4^_t)X%]	d TVZSUjwTV^ubCd2cb;
cfiTVZCi/d ]	d TVZ*d Z/\YcfiTV^_^_W4URATVZ*iU]aTYcfiTVZCi/d ]	d TVZ*d Z/\Yd Z6]ab/WUTVR/bCd U_]	d2c4X%]aWi{U_RCXVcfiW8d U;Xu~/W4fgW4U_\VQ/WBkWX%UQ/^_W
&UW4]'d Z5]abSW0U_RCXVcfiW;TSw`RATVUUd fCn WTVf/UW4^_t)X%]	d TVZSU[
q wBTV]aW'b/Tu9W4tVW4^s`]abCX%]9]abCd U"UW4]'iW4RgW4ZCiU"TVZ5]ab/W;R/^d TV^
X  T	tVW4
^ vq
r^W4U_QCn ]jUd2kffd2n2X%^]aT  b/W4TV^_Wk Vq2uX%UR/^T	tVWiof`ePWd2iW4ZCwWn2i|aVVVVaX%Z*icfiTVZSUd2iW4^X%fCn e
\VW4Z/W4^XVn2d 4WilferXau8d2i[aVVVVaaqPWd2iVW4ZCwxWn2iU_b/TuU8]ab*X%]	s*Q/ZCiW4^8tVW4^_e>u@WX%|cfiTVZCi/d ]	d TVZ/Us&;
Q/RBi/X%]	d Z/\c4X%Z/Z/TV]{cfiTSd ZCc4d2iVWu8d ]abU_TVR/bCd U]	d2c4X%]aWicfiTVZCi/d ]	d TVZCd ZS\~d2w]ab/WTVf/U_W4^t)X%]	d TVZ/U&b*XatVW]ab/WffwTV^k
]ab/W|cfiTVZ*i/d ]	d TVZCXVn-R/^TVfCX%fCd2n2d ]ezTSw  \Sd tVW4s
Z  d
U *aX%U[d U]ab/Wc4X%U_W|d Z]ab/
W 4QCiVeCW4Z/aXVkffd Z
R/^_TVf*n Wkffaq  b/W4TV^_Wk Vq2U_bST	uU6]abCX%]6]ab*d Ud Ud2kRgTVU_Ud f*n WjW4tVW4ZwxTV^6TVf/UW4^_t)X%]	d TVZSUffTSwB]abSW}kQCcOb
Ud2kR*n W4^}wxTV^
k  &  & ^  +  + s&QSZCn W4U_Uu@Wc4X%Z^_WiQCcfiWl]ab/WR/^_TVf*n Wk ]a
T W47^W4ecfiTVZCi/d ]	d TVZ*d Z/\ad Z
ubCd2cOboc4X%U_W  b/W4TV^_WkVq25X%R/RCn2d W4Uaq

`7

fi " |qxq}t~"}s1|

	8n
,`
W4tVW4Z]
W4tVW4Z]
RS^_TVfCX%fCd2n2d ]e
tVWcfi]aTV^
RS^_TVfCX%fCd2n2d ]e
tVWcfi]aTV^

CG{|	,	n	/[
RCXVd ^_u8d UW5i/d U_TSd Z`]
%X ^_fCd ]a^X%^e
U_W4]
TSw
W4tVW4Z`]aU
R/^_TVfCX%f*d2n2d ]	d W4U
TSw
RCX%^_]	d ]	d TVZ
R/^_TVfCX%f*d2n2d ]	d W4UTSw]u@T
T	tVW4^n2X%RSRCd Z/\ffU_W4]aU

p;7+	 47+,
	+h`ff%)

ZCXVd tVW
cfiTVZCi/d ]	d TVZ*d Z/\
ZCXVd tVW
cfiTVZCi/d ]	d TVZ*d Z/\
4W4A^_W4e
cfiTVZCi/d ]	d TVZ*d Z/\
l0

	 m/	 ,
7,S//
XVn uX_e`U6u 1^_TVRgTVUd ]	d TVZSq2V
d rhb/TSn2iU
  b/W4TV^_Wk Vq2V
d  \VW4Z/W4^XVn2d X%]	d TVZ TSwLrh
b/TSn2iU8  bSW4TV^_Wk Vq2V
d2w/fgTV]abTVf/U_W4^t)X%]	d TVZ/m
U 4W4A^_W4ep
n2d VWj  b/W4TV^WkVq2V

Ld \VQ/^_WYVLTVZCi/d ]	d TVZ/U;Q/ZCiW4^0ubCd2cbQ/RBi/X%]	d Z/\~d Z]ab/W{ZCXVd tVW8U_RCXVcfiWjcfiTSd Z*c4d2iW4U;u d ]abcfiTVZCi/d ]	d TVZ*d Z/\
d Z]ab/W{U_TVRSbCd U_]	d2c4X%]aWi6URCXVcfiWq

9G6

NMfiNJ I

W&bCX_tVW&U_]aQCi/d Wi']ab/W{c4d ^cfiQ*kU_]	X%ZCcfiW4UQ/Z*iW4^LubCd2cbhTV^iSd ZCX%^_ejcfiTVZ*i/d ]	d TVZCd Z/\Ss]W47^W4ejcfiTVZCi/d ]	d TVZ*d Z/\Ss
X%ZCiY;~QSRiSX%]	d Z/\Yd ZlX0ZCXVd tVW;U_RCXVcfiW6c4X%ZYfgWQ/U_]	d /WiSsVub/W4^WQSU_]	d /Wi/8wxTV^@Q/U'kWX%Z/U6%X%\V^_W4W4U
u8d ]abvcfiTVZCi/d ]	d TVZCd Z/\d Z]ab/WUTVR/bCd U_]	d2c4X%]aWi}U_RCXVcfiWVq  b/W}kffXVd ZvkW4U_UX%\VWTSw9]abCd UffRCX%RgW4^d UY]abCX%]	s
W41cfiW4R/] wTV^ 	QCd ]aWURAWc4d2XVnCc4X%U_W4Us/]ab/W]ab/^W4WYkW4]ab/T1iU c4X%ZSZ/TV]0fgWQ/U_]	d /Wi/q'Ld \VQ/^_WU_QCkffk5X%^d 4W4U
]ab/WYk5XVd Zod Z/Ud \Vb]aU0TSwS]abCd URCX%RgW4^8d ZkTV^_W5iW4]	XVd2n2q
r'Uu@WYkW4Z]	d TVZ/Wid Z]ab/W5d Z]a^_T1iQCcfi]	d TVZCs]ab/W5d2iWXTSw*cfiTSkRCX%^d ZS\~X%ZQ/RBi/X%]aW^_QCn Wffd ZoX~Z*XVd tVW
U_RCXVcfiW~u8d ]abvcfiTVZCi/d ]	d TVZCd Z/\d Z XU_TVR/b*d U_]	d2c4X%]aWioU_RCXVcfiWd UYZ/TV]YZ/W4u88d ]mX%R/RgWX%^_Umd Z]ab/
W Lr6
n2d ]aW4^X%]aQ/^WX%ZCij]ab/W~l0n2d ]aW4^X%]aQ/^_WaX%U8u9Wn2nX%UYd ZR*X%RAW4^UU_QCcbX%Ujay;XVn RgW4^_Z|  Q/]a]	n Ws9VVVV
X%ZCi u rXau8d2ih
 r'd2cOVW4esSVVVVaaqLfiZjXViSi/d ]	d TVZh]aT;f/^d Z/\Sd Z/\0]abSW4U_W9]u9T'U_]a^X%Z*iUCTSw^W4U_WX%^cbh]aTV\VW4]ab/W4^s
TVQ/^@T	uZ>cfiTVZ]a^d f/Q/]	d TVZSUX%^W;]ab/W6wTSn2n T	u8d Z/\S0aXVEu@W'U_b/T	uz]abCX%]@]ab/
W Lr6wx^XVkW4u9TV^_lc4X%ZjfgW;Q/U_Wi
X%UXff\VW4Z/W4^XVnD]aTTSnD]aTc4n2X%^d2we|kffX%ZeTSw*]ab/Wju9Wn2n pZ/T	u0ZR*X%^XViT	W4U TSwcfiTVZCi/d ]	d TVZCXVnER/^_TVf*X%fCd2n2d ]e1
fC*u@W\Sd tVWjX'\VW4Z/W4^XVnDcbCX%^XVcfi]aW4^d X%]	d TVZmTS
w Lr6d Z]aW4^kUTSw*X'fCd ZCX%^_ept)XVn QSWikffX%]a^d 1s)U_b/Tu8d Z/\
]abCX%]d ZkffX%Z`e[^_WXVn2d U]	d2c6UcfiW4ZCX%^d TVUs9]ab/
W Lr6 cfiTVZCiSd ]	d TVZ //b/TSn2i  b/W4TV^_WkSq Saac4u@W
iW4/ZSWX}kWcbCX%Z*d U
k {:f)z  ]abCX%]ff\VW4ZSW4^X%]aW4UlXVn2nX%Z*imTVZCn ei/d U_]a^d f/QS]	d TVZ/U6UX%]	d Uwegd ZS\ Lr6
  b/W4TV^WkSq2Va1ai/Du9W;U_b/Tu|]abCX%]9]ab/
W rhzcfiTVZCi/d ]	d TVZbCX%UX&ZCX%]aQS^XVnVW4]aW4Z/Ud TVZ]aTc4X%U_W4U9u0b/W4^_W
4W4A^_W4evcfiTVZCi/d ]	d TVZCd Z/\c4X%ZfgWX%R/RCn2d Wi  bSW4TV^_Wk Vq2Va5X%ZCizW u@WU_b/Tu]ab*X%]Z/
T LrhBpn2d VW
cfiTVZCi/d ]	d TVZc4X%Zb/TSn2id Z\VW4Z/W4^XVnDwxTV^ c4X%U_W4Uub/W4^W'TVZ*n eml0|aX%ZCihZ/TV
] W47^_W4e1*Q/RBi/X%]	d Z/\c4X%ZfgW
X%R/RCn2d Wi~  b/W4TV^_Wk Vq2Vaq
-QS^9^_W4UQCn ]aU9UQ/\V\VW4U_]-]abCX%]@u9TV^_1d Z/\d ZY]abSW;ZCXVd tVWU_R*XVcfiWhd U-^X%]ab/W4^@R/^_TVf*n WkffX%]	d2c4q-Zj]ab/W'TV]ab/W4^
bCX%ZCiSsX%Uu@W-TVf/U_W4^_tVWi6d Z6]abSWd Z]a^_T1iQCcfi]	d TVZCsu@TV^_1d Z/\ffd Z6]ab/W@U_TVR/bCd U_]	d2c4X%]aWiU_RCXVcfiWW4tVW4Z~X%U_U_Q*kffd Z/\
d ]8c4X%ZfAWffcfiTVZ/U]a^_QCcfi]aWi/d U;R/^_TVfCn Wk5X%]	d2c-]aT`TSq0PTffubCX%]X%^W{]ab/W5XVn ]aW4^_ZCX%]	d tVW4U
TV^~TVZ/W]abCd Z/\Ss d ]}d Uu9TV^]abTVf/U_W4^_t1d Z/\[]abCX%]}l0vQ/RBi/X%]	d ZS\d UZSTV]}XVn uX_e`UUTfCXVi/q fiZ
kffX%ZeU_QCc4cfiW4U_UwxQCn/R/^XVcfi]	d2c4XVn*X%R/RCn2d2c4X%]	d TVZ/UsS]abSW%cfiTVZ/U_]a^XVd Z`]	8TVZlubCd2cbl]aT5QSRiSX%]aWYd U;TSwA]abSWjwTV^k
j
W l"s1ub/W4^_
W *'d U]ab/W6]ab>TVQ/]	cfiTSkWYTSw9Xh^X%ZCiTSktX%^d2X%fCn =
W  TVZ
j & *
r & * m wxTV^{UTSkWn2X%^_\V=
vq  b*X%]hd Us7u@W8TVfSU_W4^_tVWX%ZlWkRCd ^d2c4XVnLXatVW4^X%\VW6TSw1TVQ/]	cfiTSkW4UTSYw [qfiZmU_QCcbXjc4X%U_WsA]ab/W;
i/d U_]a^d f/Q/]	d TVZd Ul%c4n TVU_Wad Z]ab/WX%R/R/^_TVR/^d2X%]aW~i/d U_]	X%ZCcfiW~kWX%UQ/^_W9]aT]ab/W~i/d U_]a^d f/Q/]	d TVZmu9W~X%^^d tVW

`77a

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

X%]'f`eUTVR/bCd U_]	d2c4X%]aWicfiTVZCiSd ]	d TVZCd Z/\Sq  bCX%]6d UsCd2wK1^   D^8u[  asD^,  D^	u   
j
j iVW4Z/TV]aW4U{]ab/Wml1pwTSn2i5RS^_TgiVQCcfi];TSw9X R/^_TVfCX%f*d2n2d ]eiSd U_]a^d f/Q/]	d TVZ8sA]ab/W4Z
i 
j & *
r & * > aasX%ZC<
j8 u 1,^   j tX%ZXVkRAW4ZSb/TVQ/]ff LTtVW4^s-VVVV
wTV^U_]
Q vlc4d W4Z`]	n en2X%^_\V
W l"s1u@WffbCXatVWj]abCX%]Yu D,^  
-
^ QS Z`uXVn2i/s1VVVVEPV`e^kUsgVVVV
 !vffZSgsgVVVVaq  b`QSUsgd ZU_QCcbc4X%U_W4U l0aXVn2kTVU_]	0cfiTSd ZCc4d2iVW4U
u8d ]ab>UTVR/bCd U_]	d2c4X%]aWicfiTVZCi/d ]	d TVZCd ZS\oXVwx]aW4^XVn2n2q~aPVW4Wu rXau8d2i/s@VVVV wxTV^Xi/d UcfiQ/UUd TVZ>TSwEb/T	u]abCd U
^_W4U_Q*n ]c4X%Z~fgW^_WcfiTVZCc4d2n Wiffu8d ]ab]ab/W{^W4U_QCn ]aUTSw*PWcfi]	d TVZVq2
CQ/]Lub/W4Zh]abCd UURAWc4d2XVnUd ]aQ*X%]	d TVZiTW4U*ZSTV]&X%RSRCn esd ]0d Uu9TV^]abX%Ugd Z/\u0b/W4]ab/W4^L]ab/W4^_W-W41d U_]aU0X%Z
%X R/R/^TSXVcbowTV^Q/RBi/X%]	d Z/\d Z]ab/WZCXVd tVWU_RCXVcfiW{]ab*X%]c4X%Z~fgWWX%Ud2n e>X%R/RCn2d Wid ZR/^XVcfi]	d2c4XVnUd ]aQCX%]	d TVZSUs
eVW4];n WXViU"]aTq	_sSd ZffU_TSkWwTV^kffXVn2n e R/^_Tt)X%fCn W&UW4Z/U_WsQ/RBi/X%]aWihi/d U]a^d f/Q/]	d TVZ/U]ab*X%Z6]ab/WkW4]ab/T1iU
u@WBbCXatVW'cfiTVZ/Ud2iW4^_WiS rtVW4^ehd Z]aW4^_W4U_]	d Z/\hc4X%ZCi/d2i/X%]aWs	TSw]aW4Zjd ZCwTV^kffXVn2n effX%R/RCn2d Wi&f`e'b`Q*kffX%Z5X%\VW4Z`]aUs
d U{]aTUd2kRCn e	/5]ab/WX_t)XVd2n2X%fCn WYW4`]a^Xd ZCwTV^kffX%]	d TVZCq8fi]]aQ/^_Z/UTVQ/]]abCX%]]abSW4^_W~X%^_W6Ud ]aQCX%]	d TVZSU
ub/W4^W{]abCd U0QSRiSX%]aW{^_QCn W{fgW4bCX_tVW4U;fgW4]a]aW4^sCd Z[XR/^_Wc4d U_W8U_W4Z/U_Ws/]abCX%Z]abSW{]ab/^_W4WkW4]ab/T1iU0u@WbCX_tVW
cfiTVZ/Ud2iVW4^_Wi/q  b*d U&u8d2n2n)fgWW4`R*n TV^_Wid ZowQ/]aQ/^_Wu@TV^_1q
r Z/TV]ab/W4^Yd UU_Q/W6]abCX%]Z/W4WiVUYwxQS^_]ab/W4^W4RCn TV^X%]	d TVZd U]ab/W6U_Q/f/]	n W~iSd U_]	d ZCcfi]	d TVZ>fgW4]u@W4W4Z u@WX%1
'
X%ZCiU_]a^_TVZ/\S0Lrh;subCd2cObuX%UCfS^_TVQ/\Vb]D]aT0TVQ/^-X%]a]aW4Z`]	d TVZhfeffX%ZCw^_WiX%W4\VW4^q  b/W"]aW4^kffd Z/TSn TV\Ve
d U~iQ/Wj]a
T XVkW4U~TVfCd Z/UsLub/TlTVZCn e}tVW4^e}^_WcfiW4Z]	n ei/d UcfiT	tVW4^Wim]ab/W>i/d U_]	d ZCcfi]	d TVZ*qfi] ]aQS^_Z/UhTVQ/]
]abCX%]]ab/W6ZSTV]	d TVZ>TSH
w rhu9W6Q/UWd Z}]abCd URCX%RgW4^5	u9WX%1
 rh;d UUn2d \Vb`]	n ei/d AW4^_W4Z]5wx^TSk]ab/W
U_]a^_TVZS\S/tVW4^_Ud TVZlTS2
w rh|Q/U_Wi5fe}{d2n2n7W4]hXVn2q*aVVVVaq  b/W4et1d W4u]ab/
W Lr6vX%U_U_QCkRS]	d TVZ[	QCd ]aW
n2d ]aW4^XVn2n2n eX%UX%Z|X%U_U_QCkR/]	d TVZ|X%fgTVQ/]5X}%cfiTSX%^_U_W4Z*d Z/\~R/^TgcfiW4U_UVqhrhiVQ/U]aWij]aTTVQ/^{Z/TV]	X%]	d TVZ*sD]ab/W4e
u^d ]aWja{d2n2nW4]8XVn2q2sCVVVVsRCX%\VWjVVVah%Ld ^_U_]	n e]abSW^X%ZCiTSkvtX%^d2X%fCn 
W W TSw*d Z`]aW4^_W4U] d U^_WXVn2d 4WiS
U_WcfiTVZCiSn es1X5/,a7)+5 	fi	SLR/^TgcfiW4U_UQSU_QCXVn2n elX%UU_Tgc4d2X%]aWi6u8d ]ab}wxWX%]aQS^_W4U&TSwCkWX%U_Q/^_WkW4Z]
TV^TVf/U_W4^_tX%]	d TVZCXVn9^_W4U_]a^d2cfi]	d TVZ/Us&^X%]ab/W4^]abCX%Zz]ab/WUc4d W4Z`]	d CcYR/bSW4Z/TSkW4Z/TVZQ/ZCiW4^YU_]aQCied ]aUWn2waas
\Sd tVW4Z~]abSWt)XVn Q/.
W ]	X%VW4Z~f
e W}s^_W4RCn2XVcfiW4U0]ab*d U&t)XVn Q/Wf`emXU_W4]UQCcb]abCX%
] >8q2  bQ/Usgd Z
]ab/Wd ^Dt1d W4u8s	]ab/W;i/d U]a^d f/Q/]	d TVZ{TV
Z d U9cfiTVZSU_]a^_QCcfi]aWi8wx^_TSkXi/d U_]a^d fSQ/]	d TVZTVq
Z  X%ZCiXacfiTVZ*cfiW4R/]aQCXVn2n e
Q/Z/^Wn2X%]aWi/DUW4]@TSwEcfiTVZCi/d ]	d TVZCXVngi/d U_]a^d f/Q/]	d TVZ/.
U 1^    W  hasTVZ/WhwTV^-WXVc
b  e
 vq  bCd U
d2kRCn2d W4UY]abCX%
] D^	    W   ffd UmXu@Wn2n piW4/Z/WimZQCk0fgW4^6W4tVW4Zd2#
w 1^	 W  h  Vq
{d2n2n2sDt)X%Z|iVW4
^ ~EXVX%ZCs"X%ZCi>TVf*d Z/U5aVVVV]abSW4ZL K-S6]abS
W Lr6 cfiTVZCi/d ]	d TVZzX%Um%wxTV^jXVn2nB 
 s
1^	    
 W
Z  Vq  bCd Umd UjQ/U_]YRCX%^_]mai/{TSw  b/W4TV^_Wk Vq2Vs
  d UcfiTVZ/U_]	X%Z]d 
ubCd2cObZ/T	uvbCX%U]aTbSTSn2i5W4tVW4Zd2!
w 1^    h  Vq  b`QSUs7]ab/WhU_W4]TSw"i/d U_]a^d f/Q/]	d TVZ/UUX%]	d Uwe1d Z/\
U_]a^_TVZS
\ Lr6 d U6XUQ/f/U_W4];TSwg]ab/W U_W4];UX%]	d Uwxe1d Z/\u@WX%
 Lr60qCTVfCd Z/UffX%ZC
i X%W4\VW4^Ub/T	u]abCX%]]ab/W
d ZCc4n Q/Ud TVZc4X%ZfgWU_]a^d2cfi]lX%ZCim]ab*X%]ff]abCd Ulc4X%Z|bCXatVWU_Q/f/U]	X%Z`]	d2XVncfiTVZSU_WQSW4ZCcfiW4Uq  b/W4^_WwTV^_Ws"TVQ/^
cb*X%^XVcfi]aW4^d X%]	d TVZ/UjTS0
w rhd ZPVWcfi]	d TVZX%R/RCn e>TVZ*n eo]aTu@WX%
 rh;s@X%Z*iwxQ/^]ab/W4^h^_W4U_WX%^cObd U
Z/W4WiWi6]aTffU_W4W{]ab/WW4`]aW4Z]]aT5u0bCd2cb]abSW4emXVn U_TmX%R/RCn ej]aTffU_]a^_TVZS
\ Lrh;q
-QS^Yi/d UcfiQ/UUd TVZ}b/W4^WffbCX%UwxT1cfiQ/U_Wi>cfiTSkRCn W4]aWn e>TVZ]ab/WffR/^_TVf*X%fCd2n2d U_]	d2c5c4X%UWqyBTu9W4tVW4^s*]abSW4U_W
	Q/W4U_]	d TVZ/UhXVn U_TlkffX%VW8U_W4Z/U_WjwxTV^;TV]ab/W4^0^_W4RS^_W4U_W4Z]	X%]	d TVZ/U;TSw7Q/Z*cfiW4^_]	XVd Z`]eq9Z]aW4^_W4U_]	d Z/\Sn es*^d WiSkffX%Z
X%ZCily;XVn RgW4^_ZaVVVV@U_bST	uv]abCX%]']abCX%]r{ffpU]e1n WfffgWn2d Wwg^_W4t1d Ud TVZrhn2cOb/TVQ/^_C
^ TV Z*s*Q
 X%^iW4ZCwTV^_Us*
lX%gd Z/UTVZCsLVVVVc4X%ZlfgW8^_W4R/^_W4U_W4Z]aWimd Zm]aW4^kUTSwBcfiTVZCi/d ]	d TVZ*d Z/\YQSUd Z/\XY	QCXVn2d ]	X%]	d tVW6^_W4R/^W4U_W4Z/p
]	X%]	d TVZTSwQ/ZCcfiW4^]	XVd Z`]elc4XVn2n WiX'7
,o +
 p ,)fi4]aTiT]ab*d Us`]ab/W0RCn2X%QSUd fCd2n2d ]emkWX%U_Q/^W k0Q/U_]
UX%]	d Uwe[]ab/WoX%ZCXVn TV\VQSW~TSw  b/W4TV^_Wk Vq2VaXVas-U_T>]abCX%]5TVf/U_W4^_tX%]	d TVZ/Uc4X%^_^_eZ/T|kTV^_Wd ZCwxTV^kffX%]	d TVZ
]abCX%Z[]ab/WwXVcfi]h]abCX%] ]ab/W4ezX%^_WY]a^Q/W
q wB
T Lr6pn2d VW>cfiTVZCi/d ]	d TVZd Uh\Sd tVW4Z]aTm\VQCX%^X%Z]aW4W]abCX%]h]abCd U
cfiTVZCi/d ]	d TVZbSTSn2iU5wxTV^RCn2X%QSUd fCd2n2d ]ekWX%U_QS^_W4U]ab/TVQ/\VbCqfi]u9TVQ*n2i5fgWd Z]aW4^_W4U_]	d Z/\]aTZ/T	u d2wD]ab/W4^_W
X%^_W5X%ZCXVn TV\VQSW4U0]a
T Lr6wTV^&TV]ab/W4^^_W4R/^W4U_W4Z`]	X%]	d TVZSUTSw/QSZCcfiW4^_]	XVd Z]es)U_Q*cb}X%U;` 	+
 p ,)fi	
u r9Q/fgTSd U8
 D^XViWsDVVVVBTVm
^ 
 !	)/	/ aPVbCXVwxW4^s1VVVVaq

`77

fi " |qxq}t~"}s1|

d

I J;\/K

 

I fi

r R/^Wn2d2kffd ZCX%^_etVW4^_Ud TVZlTSwA]abCd U;RCX%RgW4^ X%R/RgWX%^_Uhd Z 1/,/
 x0, K-	
 a/	
	/ff 9
 ,/G.9/~*8		fi	S4s7VVVVqVXVkW4U;TVfCd ZSU;X%ZCi6lX%ZCwx^Wi=X%W4\VW4^cfiTSkffkW4Z]aWi
TVZ}X%ZWX%^n2d W4^ i^XVw]@TSwS]abCd UR*X%RAW4^qEWu9TVQCn2i tVW4^emk0QCcb}n2d VW]aT6]abCX%Z/j]ab/Wk wxTV^&]abSWd ^8d Z/Ud \Vb]ap
wQCn`^_WkffX%^_Us`ubCd2cOb}n Wi ]aT6U_W4tVW4^XVn)Ud \VZ*d Cc4X%Z`]d2kR/^_T	tVWkW4Z`]aUhd Zj]abSWRCX%RgW4^qCWYXVn U_T6]abCX%Z/Y]ab/W
^_WwW4^_W4W4UhTSw]ab/<
W "r;{UQ/fCkffd U_Ud TVZzX%ZCi~]ab/
W \xQuyUQ/fCkffd U_Ud TVZzwTV^h]ab/Wd ^hRAW4^cfiW4R/]	d tVW>cfiTSkffkW4Z]aUq
W wBW4]abSW4^n2X%ZCiU{-^_\SX%Z*d X%]	d TVZ
 b/W;/^_U_]X%Q/]ab/TV^-uX%U@UQ/R/RgTV^_]aWifeX0]a^XatVWn\V^X%Z`]{XauX%^iWi8feff]ab/
wTV^P/c4d W4Z]	d CcW4UWX%^cbu w"{aq  b/WYU_WcfiTVZCi[X%Q/]ab/TV^ uX%UhU_QSR/RgTV^_]aWi}d ZRCX%^_]8f`e w'P/[QSZCiW4^
\V^X%Z]aUB_4PpVVVV%S{X%ZCm
i   LpVVVVVVVsfej
 w;mQSZCiW4^*\V^X%Z`]aQ
U w;VVV%VpV%p%pV%SF
s w;VVV%VpV%p
%pVVVVs1X%ZC=
i w'VVV%VpV%p%p%SVsSf`eh]ab/
W r"
T rffQCn ]	d2iSd Uc4d RCn2d ZCX%^_[
e "ZCd tVW4^_Ud ]e~W4UWX%^cbjZ*d ]	d2X%]	d tVW
aN
 ;BBR/^_TV\V^XVk XVi/kffd ZCd U_]aW4^Wi6fej]abSWYZ
 w'Q/Z*iW4^&\V^X%Z]am
U w'VVV%VpV%pVVV~X%ZC
i w;VVV%VpV%p
%pVVVVsLX%ZCihfe>Xff-Q/\V\VW4Z/bSWd2k Wn2n TuU_bCd R*sCXffVQCn f/^d \Vb]8Wn2n TuU_bCd RCs*X%ZCiX{\V^X%Z]8wx^TSk w"{q
P/X%f/f*X%]	d2c4XVn*U_Q/R/RgTV^_]Yw^_TSk LjX%ZCi~]ab/W>y"W4f/^_W4
u BZ*d tVW4^_Ud ]eTS
w W4^_Q/UXVn Wk d U~XVn U_Tm\V^X%]aWwQCn2n e
XVcZ/Tu8n Wi\VWi/q

eK[


I K"NW


G J*J+

fiZ]abCd UU_Wcfi]	d TVZCsDu9W6R/^T	t1d2iWh]ab/W6R/^_TTSwxUTSw@XVn2nA]ab/W6^_W4U_Q*n ]aU5d Z]ab/W6RCX%RgW4^q5VTV^YcfiTVZ`tVW4Z*d W4ZCcfiWs1u@W
^_W4U_]	X%]aW8]ab/W^_W4U_Q*n ]aUb/W4^_Wq

L87|+1^ m /55**	
/ fi88	)n	+	/
zO%}9q1^	   # ;
 	1 ^W  
      D^	W  W 8!	ff
 >m
zu}*'ff	n /\W  
 9/_`	S	/*G'
 ff	n 	S\    		n 	#W
 >mff&X >
zOu}D ^   W     1 ^   W 8R	 6?
 |)~V1 ^W 
 !
h
 
zO%}D ^   W  h
   1 ^   W    #	 m!h
 6    ,)	
 ^W  h
D
 !
 /qD ^	W    2
 


\b"a\



e7

  PQ/RSRATVUW6aXVBb/TSn2iUq"WuX%Z]0]aT5Ub/T	u]abCX%]VW   X%ZCi   X%^Wjd ZCiW4RgW4ZCiVW4Z`]	s
w TV^XVn2n\v>q&Ld Wv>q*4wCD^	W    ]ab/W4Z]ab/W'W4tVW4Z]aU8X%^_W']a^d tgd2XVn2n emd ZCiW4RgW4ZCiW4Z]	q-PT
U_Q/RSRATVUW']ab*X%]1^	   2Vq0n WX%^n e
[<aR

1^	W    


k W>8  D^	W    



Ud ZCcfiWTVf/U_W4^_t1d Z/\~d2kRCn2d W4U]abCX%]0]ab/W{]a^_Q/Wu@TV^n2id Uhd Z8'aqDCeYRCX%^]8aXVas

1^	   W 



 1^	    

 bQ/Us

}ui

 ^W      k W >  D^W  W>8a
1
U_b/Tu8d Z/\6]abCX%]W   d U d ZCiW4RgW4ZCiVW4Z`]-TSw   s\Sd tVW4ZW8q
`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

w"W4`]'U_Q/RSRATVUW]abCX%] fCb/TSn2iUsCX%ZCi$d U'U_QCcOb~]abCX%]m1^	W   ;Vq^_TSk$RCX%^_]hfC
d ]'d Ud2kffkWiSd2X%]aW0]abCX%]#1^   W   k W>8  1^   W >aq&5TV^_W4T	tVW4^s
Ud ZCcfiW$o8sCc4n WX%^n e1^	   W   k W   D^   W   aq.LX%^_]6ac4
Z/TuwxTSn2n TuUq
n WX%^n eaiSBwxTSn2n TuU'd2k5kWi/d2X%]aWn e~w^_TSk ac4aq  bQ/Us/d ]"^_WkffXVd Z/UB]aTU_b/T	u]abCX%];aXV"wTSn2n T	u0U'w^_TSk
ai/aqCWiT']abCd UfeU_b/Tu8d Z/\']abCX%]0ai/Ld2kRCn2d W4U0ac4X%Z*i0]ab*X%]&ac4d2kRCn2d W4U0aXVaq-PTU_Q/R/RgTVU_W9]abCX%]0ai/
b/TSn2iUq;PQSR/RgTVU_W]abCX%
] D^	    
 W     wTV^ XVn2?
n  oUQCcb]ab*X%
] 1^ W   K
 Vq
^_TSk ]ab/W5iW4/Z*d ]	d TVZTSwLcfiTVZCi/d ]	d TVZCXVn/R/^_TVfCX%fCd2n2d ]e
D^    
 W>8
F   b) rb /_-  1 ^    k W   a21^W >8

 a21^W
F   b) rb /_-  1 ^   W   ?1^W  h

H


D

^


W


h

a

2

1


^


W


>

8




F  b) rb /_





>



 bQ/Us1ac4wTSn2n T	u0U w^_TSkai/aq
Ld ZCXVn2n esg]aTU_W4W ]abCX%]ffaXVwTSn2n T	uU5w^_TSk ac4asAU_Q/R/RgTVU_W8]abCX%]6ac4@b/TSn2iUq0wU d UU_QCcObm]abCX%]
1^	 W  h  VsA]ab/W4ZaXVd Uffd2kffkWiSd2X%]aWsgU_TU_Q/R/RgTVU_W8]abCX%]=1^W   0 Vq  Sb W4ZCs/QSUd Z/\
ac4X%ZCi6]ab/WYwXVcfi]]abCX%V
] W  
   >8s)u@W{bCXatVW]abCX%]








1^ 
1^ 
1^W
1^ 
1^ 
1^W
1^ 

 W  
 
 W   ?D^W  ha21^   
   >?D^   ha21^W  
  k W >?D^W  ha21^W >?D^  
 ?D ^W  ha21^W >?D^   8
  a2D ^W}
W >8a

8

X%U iVW4Ud ^_Wi/q

*lxQy$/+K	h*,,	)/1^q  5/l/+ 
S{G'`,ff( /-4 	6
[<aR   Ld ^_U_]{U_QSR/RgTVU_W6]abCX%]{]ab/W5U_W4]aUjd Z X%^_W5RCXVd ^_u8d U_Wli/d U_TSd Z`]	q  b/W4Z|wTV^{WXVcbR/^_TVf*X%fCd2n2d ]e
i/d U_]a^d f/Q/]	d TVZ1^TVZel
 sAWXVcOb|
 sX%ZCiYWXVcOb>u9TV^n2iq
 |vU_QCcObl]ab*X%]D ^    V
 Vsd ]
k0Q/U_]&fgW]ab/WYc4X%U_W]abCX%]1
 ^	   W     Vq  bQ/Us`RCX%^_]8aiS*TSw  b/W4TV^_WkVq2ffX%RSRCn2d W4Uq
TV^0]ab/W5cfiTVZ`tVW4^U_WsU_QSR/RgTVU_W;]ab*X%]&]ab/WU_W4]aU8d Z
 X%^_WZ/TV]-RCXVd ^_u8d UW5i/d U_TSd Z`]	q  b/W4Z]ab/W4^_WW41d U_]
U_W4]aUh"4  
 U_QCcOb~]abCX%]0fATV]ab    X%ZCi k   X%^_W{Z/TVZSWkR/]eq0~ W4]V - > k   qV
 n WX%^n e
]ab/W4^_W&W41d U_]aU;Xi/d U]a^d f/Q/]	d TVZD
 ^TVZ U_QCcObff]abCX%]1 ^	   2
 Vs1 ^     2
 Vs]1 ^W 
 - W    V^1 ^    - W    Q
 VqCQ/]]abSW4Z 1 ^    -   z#
 Vq
[<a"ZW " ^ "  l

 bQ/U

:eS

1^	   

-

W 

0 

1^	   

-

 

>a

X%ZCih]abSWWLrhcfiTVZCi/d ]	d TVZ[RCX%^_]8aXVLTSw  bSW4TV^_WkVq2V0d U0tgd TSn2X%]aWi/q
  jj	;GY,)/ |n	6|	n	S
of
 n :e
L E 
~xQyB	%,/ph%H# /

Dh\

`7_

S%+@/m
7,

fi " |qxq}t~"}s1|

zO%} E 	;D^qffSj,,	)}|n	[

/~
17  ,ffp ,Y	/	j	+	/fi|p
7&,%`SS}}|pG'1^W H''  JILK-/ln	,	
A @   A & ^i^i^i	 A jS	,	/ A   1^   FW 	h WD^W zF#0S
K Yx0y	a7  _A @ B   @ B 
A   ,ff_	9  V^i^i^i	6l;.9D^ff,Mzu} E 	7  ,6p ,ff//ffGff4	,	@Ghfffi{66H7 S
2O QP R S 6ff9G
,,	)/{|n	#
,	`%@,%`SSYU7  V ffo

O QP R S  / XgXg>Y''K Z'$,%`Shjfiz[7  i
@ )	17  A_@ B   @ B 9	]?	{XVn2n`XE aO QP R&S "
9ff	fij7	n	,	\A^
@ ] ~
ff	Y>%o1^l|n	Z e1^,  E
X  zffophS-GD^m 
XE})z^}1^,MK xQy/zu}D^	   FW F	  A 6S
q1^	



>

 ! 

 TV^RCX%^]haXVasgUQ/R/RgTVU_W]ab*X%]D^ffd U6Xi/d U]a^d f/Q/]	d TVZmTVZ]abCX%]UX%]	d U_SW4UWLrh;q~/W4]=5fgW
]ab/W'Z`Q*kfgW4^@TSw)^_T	u0Ud Zq7  sgX%ZCijn W4].C*  1^W q'*fiasgwxTV^-  V^i^i^i	^s`ub/W4^W:'1*Bd U-]ab/W6X%]aTSk
cfiTV^_^_W4URATVZ*i/d Z/\h]aTff]ab/W{]ab~^_TuTSF
w 7  q wBTV]aW8]abCX%
] C*5wxTV^  V^ i^i^i	^ 
q n WX%^n es
[<aR





   t

1^   F ;W q
 '*  
 i

aV

fi]&WX%Ud2n e}wxTSn2n TuU w^_TSkv]abSWLr6cfiTVZCi/d ]	d TVZ]abCX%]

D^   F W q
 '1*fi  1^   F
 HW

>F	

wTV^8XVn2n'1*F	sUTaVd U0W	QCd tXVn W4Z`]0]aT




   

1^    ;W}F	   i

aV

a Vd2kRCn2d W4U]ab*X%]
   Y A   jwTV^  V^i^i^i	^q=~/W4]q9@ *fAW ]ab/Wh^_Tu d Z7  cfiTV^_^W4U_RgTVZCi/d Z/\
]aTq'*fiqhP/d ZCcfiW9@ *bCX%UffXjjX%U5d ]aU	]abcfiTSkRgTVZ/W4Z]6d2w'1*H 6X%ZCimXY8TV]ab/W4^_u d U_WsLd ]6wTSn2n T	u0U]abCX%]
9@ *gA @ B  ffX%ZCi bSW4ZCcfiU
W 7  2A @ B   @ B q
TV^hRCX%^]YfCas-n W4
] fgWff]ab/W5ZQCk0fgW4^TSw*^_TuUjd >
Z 7  s@n W4c
] 9 @ & ^ i^i^i	j
 98@ jfgWff]ab/WY^_T	u0U8TSw7  s-X%ZCi
n W4N
] ' & ^ i^i^i ';fAW~]ab/WocfiTV^_^_W4URATVZ*i/d Z/\X%]aTSkUqLd  Xg  OQP REs;X%ZCi>U_W4
] C*  Xg}Y '*ffwTV^
  V^ i^i^i	^ V
q ~/W4
] 1^fgW]ab/W{QSZCd2QSW6iSd U_]a^d f/Q/]	d TVZ~TV
Z  UQCcb]ab*X%]

D^Wq'1*

D^W
 q''

D^   FW <
 '1* 

C*4wTV^0 

V^ i^i^i	^ /
ffd2w)'|x.  /' & ^i^i^i	';V
w '*L>s
A  d2)

TV]ab/W4^_u d U_Wq

w"TV]aW]abCX%]1^md Uld ZCiW4Wi|XR/^TVfCX%fCd2n2d ]eiSd U_]a^d f/Q/]	d TVZ|TVZ ms9Ud ZCcfiW   1^W 
 '' 
@
1^	  q' * !
 6wxTV^;  V^i^i^	i ^ sEX%ZCi/sUd ZCcfiWu9WYX%^WYX%U_U_QCk5d Z/\6]abCX%]7  2A_@ B   B s
j
 ^	W      q' *   9 @ * _ A @ B  V
1
r
&

`77

aV

Vs

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

 q fi]{^_Wk5XVd Z/U{]aTU_bST	u ]abCX%]q1 ^{UX%]	d U/W4UqLr6$X%Z*ij]abCX%]
 V^i^i^i	^qn WX%^n eD^8  Xg>
A  D^   HW oFaq0{d tVW4ZlhoVV^i^i^i	6l"Vs/U_Q/R/RgTVU_W]abCX%]]abSW4^_W{W41d U_] X%]aTSkU3'1*' * S
cfiTV^_^_W4URATVZ*i/d Z/\h]aTff^_TuUa9@ *9X%Z*i9 @ *S TSwF7  U_QCcOb]abCX%]'1*' *S >F q  b/W4Z
 ^	   \ W <'1*  1 ^   F_ W q' * S   A 7i
D
fi]"Z/TuwTSn2n T	uU@f`e  b/W4TV^_Wk$Vq2Vac4E]abCX%]#1
 ^"UX%]	d U_/W4U@]ab/WL r6cfiTVZCi/d ]	d TVZwxTV^ & ^i^i^i 47j q-5TV^_W4p
T	tVW4^s  b/W4TV^WkVq2Vai/asEd ] kQ/U]&fAW]abSW5c4X%U_W]abCX%]1
 ^	   ;W
 }F	   A 	 q
wTV^{

 bSWffR/^_TTSwCTSw  b/W4TV^_Wk Sq fSQCd2n2iU{TVZ~Wkffk5X6Sq2lX%ZCi]ab/WmwxTSn2n Tu8d Z/\lR/^_TVRgTVUd ]	d TVZCsEubCd2cOb
U_b/TuU-]abCX%]-]ab/WffcfiTVZCi/d ]	d TVZTSw)R*X%^_]{fCDTSw  b/W4TV^_WkSq d UXVcfi]aQCXVn2n ejU_]a^_TVZ/\VW4^&]ab*X%Zj]abSWhcfiTVZCiSd ]	d TVZ
TSw7RCX%^_]aXVaqB]d U]abSW4^_WwxTV^W{Z/TV]U_QS^_R/^d Ud ZS\ ]ab*X%]8d ]n WXViU0]aTlXU_]a^_TVZ/\VW4^hcfiTVZCc4n Q/Ud TVZ*q
 ,5,_
 (
 }G~fiY67$ff+/%+la`/	SZ	)
eC 9	fi~7
 'vu/
l_`	/	/'	=	ffg>p 3' %`//mmfiv^} /mA  
/
VV^i^i^i6l"9 6' F~M@j7,n	Ny @ {>u/5,|pm	/o6ffY8
},)	 y  ] 
 h>VV^ i^i^i6 l"/ y  ~ 

[<a"ZW " ^ "  g
l 

[<aR   PQ/R/RgTVU_Wh]abCX%]{]abSW4^_WffW41d U_]aUjXhUQ/f/U_W4]}TSwE^_T	u0UTSw17]abCX%]Yd Ujn2d Z/WX%^n eiW4RgW4ZCiW4Z]f/Q/]
Z/TV]ffXvffZ/Wn eiW4RgW4ZCiW4Z]	q d ]ab/TVQ/]5n TVU_UTSw1\VW4Z/W4^XVn2d ]es9n W4]8o @ & ^i^i^izo @ 5fgW8]ab/Wh^_TuUYd Z}q  b/W4^_W
*
r p *  X%Z*i *
r p * @ * VqW6/^U_];Ub/T	u ]abCX%]ffd ZwXVcfi]
W41d U_
] p & ^ i^i^i p\ffU_QCcbm]abCX%
]  
o 

&
&
W4tVW4^_e^T	G
u o@ dc
Z }d UhX%ZX vffZSWjcfiTSk0fCd ZCX%]	d TVZTSwg]ab/W8TV]ab/W4^^_T	u0UqLd U_TSkW8YVV^ i^i^i^ V
q ~/W4]
*
r * ffX%ZCi
  Y p\  *
r & p&*   *
r4 p&*@X%ZCin W4]  *  p&*@wTV^#
   q  b/W4Z

& 







p&*to @ * 
p&*to @    o @ ffi
 w* o @ * 
*


r


*
r
&
&
&


TV^  V^i^i^i^sLn W4]  *    *fi/S
 q  b/W4Z
*
r &  *  jX%ZCi *
r &  * o @ *  o @ 	qwBTu d2we'1*K F ~
wTV^BU_TSkW  V^i^i^i	^ffX%Z*iU_TSkWj  V^i^i^i	6l"s]ab/W4Zo @ * bCX%U;X{{X%U'd ]aU"b]ablcfiTSkRgTVZ/W4Z`]	qCr6n U_TSso @ *
d U8X%Z}XvffZSW5cfiTSkfCd Z*X%]	d TVZ~TSwS]ab/W^_T	u0UTSw|
} u8d ]ab~Z/ThZ/W4\SX%]	d tVWjcfiTSkRgTVZ/W4Z]aUs)UTo @ *@d U]ab/WffiW4Ud ^_Wi
*
r

tVWcfi]aTV^q



  {YG {%)/0
 n		'|
 	 n	S
\b"a\ 
e9L
 E 	
/+	e7h~xQyB	,/=ph%H#/q



V & ^i^i^i	4FjA5/5,+0

zO%}c|S	`~'	fi7, l4	?}Gj6^7vSln	,	^y @   y & ^i^i^i y jA
&vu/6p S>G6ff{G6} )j y  ] ff}VV^i^i^i6l"
/ y  ~ 	 j|h
p 8jffVV^i^i^i 6"l * 		fiff/%o)1^l 
,MK-Yx0v
y ,H1 ^	W    ~K
 /q1 ^  ^''K
 V	 ,	>
 |
p '
,%`SSYa}
zu}9Y
 j7 ,84	 ,	1} GY
 jfihGfi
 7; 
/,,+_`	S	/0	 )&/0vu/	+
a`/	S1
 	ff	fi0B/%o )1 ^' 1MK&	'xQ
y )E1 ^  
''!
 	 {	
 fi|
p ',fi	O`//jjfiz[}
Oz u}@
 	n }	} S/G#
l 
/,+~/_`	/	/;663
 7v/l~%	 )qXE

,1X  Y''H
 Q '%`//a}9 	fi5{)/	 )
,,	 )mX   )	j eD
 ^%o ) 	 /xQy S
 ^W ''  XE>
D
 Y'' 8|
p ',,fi	O`//fia}B 	1 ^	  
8  X  a,
`7

fi " |qxq}t~"}s1|

VTV^RCX%^_]aXVasU_Q/RSRATVUW>]abCX%]} cfiTVZ/Ud U]aUlTSw^o @ & ^i^i^i	jo @ s5cfiTV^^_W4U_RgTVZCi/d Z/\|]aTvX%]aTSkU

' & ^ i^i^i'HqCelX%UU_QCkR/]	d TVZ*s]abSW4^_WW4gd U]cfiT`W^vc4d W4Z]aU6p & ^i^i^i	p_U_Q*cb]abCX%] *sr & p&*  Vs1X%ZCiX
*sr p&*t@ B* U_QCcbm]abCX%]W4tVW4^_ecfiTSkRgTVZ/W4Z];TSw3@ d UZ/TVZ/Z/W4\SX%]	d tVWqYPQ/R/RgTVU_WsSfeuX_eTSw
tVWcfi]aTV^y @ 
o
y
&
cfiTVZ]a^XVi/d2cfi]	d TVZCs]abCX%].1^@UX%]	d U_SW4U.rhzX%ZCi]abCX%].;*  1^Wq'1* !8wTV^9-VV^i^i^i^Vq0Ee
[<aR

~Wk5kffX'Sq2VaXVas/u@W{bCX_tVW

y @ 2 A @ 


*
r
&

pT*wo @ *





2A @ 
&p *fio @ *YA @  
&p * 
*
r &
s* r &

V

aV

ub/W4^WA @ d UffiW4/Z/WilX%Uffd Z~WkffkffXSq2VqhTV^  V^i^i^i6l"sCd2wK1^W    [ {]abSW4ZD^W 
F k W 	  1^   F. X%Z*i1^W 	. VsEU_T A = Vq~CeX%U_U_QCkR/]	d TVZCs
XVn2n]abSWcfiTSkRgTVZ/W4Z]aUYTSw[y @ X%ZCiHA @ X%^_WZ/TVZ/Z/W4\SX%]	d tVWq  b/W4^_WwTV^_Wsd2w9]ab/W4^_W~W41d U_]aUjb UQCcb|]abCX%]
1^	    F ~2  X%ZCi y ~0Vs]abSW4Zmy @ bA @ Vq  bCd UcfiTVZ`]a^XViSd2cfi]aUaVas1X%ZCi{R*X%^_]aXV@d U-R/^_TtVWi/q
TV^RCX%^_]6fCasAU_Q/R/RgTVU_W8]abCX%]]ab/W4^_WhW41d U_]aU5X8U_Q/f/U_W4fi
] }TSw1^_TuUTS
w 7]abCX%]ffd U5n2d Z/WX%^n eiW4RgW4Z/p
iW4Z];fSQ/];ZSTV]hX vffZSWn eoiW4RgW4ZCiW4Z]	q8PVQ/R/RgTVU_WsfeuXaeTSw"cfiTVZ]a^XVi/d2cfi]	d TVZCsg]ab*X%m
] D^'UX%]	d U_/W4=
U Lr6
X%ZCi~]abCX%
] 1^	 W 
 ';
  mwTV^XVn2n-X%]aTSk\
U 'cfiTV^^_W4U_RgTVZCi/d Z/\]aTXff^_Tud g
Z }
q "d2cO|X%ZX%]aTSk
'ffcfiTV^_^_W4URATVZ*i/d Z/\~]aTlU_QCcObzX5^_Tu8q}Ce D^_TVRgTVUd ]	d TVZor6q2mX%ZCi  b/W4TV^_Wk Sq SaXVasu@WYb*XatVWj]abCX%]
1^	    F ~  ffwTV^8XVn2n)  U_QCcOb]abCX%
] '  >
 ~qCQ/]]abSW4
Z 1^ W q
 '    VsDX%ZCihu@W{bCX_tVW
X%^_^d tVWiX%]8X6cfiTVZ]a^XVi/d2cfi]	d TVZCq
TV^RCX%^_]>ac4as'U_Q/RSRATVUW]abCX%
] } cfiTVZSUd U_]aUTSw0]ab/W^_TuU o @ & ^ i^i^i	j
 o@ j 
q ~W4
] 7  fgW]ab/e
W ll
U_Q/f*kffX%]a^d lTS
w 7cfiTVZ/Ud U]	d Z/\~TSwC]ab/Wff^T	uU8TS1
w }qlP/d ZCcfiW5]ab/W4U_W5^T	uUX%^_Wn2d Z/WX%^n ed Z*iW4RgW4ZCiW4Z]	s"X
U_]	X%ZCiSX%^iY^_W4U_QCn ]TSw9n2d Z/WX%^YXVn \VW4f/^X UXaeU{]abCX%
] 7  d UYd Z`tVW4^]	d fCn W
q ~/W4W
] 1^fgWXi/d U_]a^d f/QS]	d TVZTVe
Z 
@
@
&
UX%]	d Uwe1d Z/
\ Lrh;qgE
e ~WkffkffX;Sq2VaXVa
s 7  A @   B q  bQ/U1
s A @  Y 7    q&TV^&  V^ i^i^i	6 llu9Wffk0Q/U_]
bCX_tVW A  - 	2
 D^ W |
 	asgubSW4^_W    D^    
 aq{d tVW4
Z D,^ >Y ';wTV^{WXVcObX%]aTS
k ''s
u@WYc4X%Zc4n WX%^n eU_TSn tVWwxTV^0]ab/W   Uq
\b"a\ 
 eL @ 	n 6	 G%)/0|n	;ffF G{,+'/ff5	6|	n	S,
:
1^o%	) 6MK&	xQy   	fi}S>G`fi|ph	,m
{:f)z  )]	 h? S m!D^aW}  h4  /  V8
7fi| 	 
`{:fF8fi	)%/ 48,
]abCX%]}d2w1
 ^}d U}XlR/^_TVf*X%fCd2n2d ]eTVZ U_QCcOb]abCX%]	shwxTV^UTSkWU_W4]a]	d Z/\TSw
[<aR   Ld ^_U_]u@WU_b/Tu
]ab/WRCX%^XVkW4]aW4^_U6TSw1{:f)z  sVD
 ^	a[
 W
 /    4  /   Vhd U6]ab/WjR/^_TVf*X%fCd2n2d ]eo]abCX%]
{:f)zFB^_W4]aQ/^_Z/U{h
 4as)]ab/W4ZD ^@UX%]	d U_/W4UL r60qgCe  bSW4TV^_WkVq2VsDd ]-U_Q]
v cfiW4U-]aT6U_b/Tuz]abCX%]	s
wTV^WXVcbffU_W4]0
 X%ZCi'u@TV^n2iU2 & 6 + >U_QCcb6]ab*X%]1 ^    & 2
 X%ZCim1 ^    + 2
Vsu9W'bCXatVW1
 ^	   W   &   1 ^   W   + aq@PVTU_Q/RSRATVUW]abCX%]K & 6 + }s
 ^	W   &  Vs{X%ZCiD ^	W   +  Vq/~ W4]  
1
     ` X&  aa   _  aq
fiZ`]aQCd ]	d tVWn es.  d Uff]ab/W~R/^_TVfCX%fCd2n2d ]e[]abCX%]ff]ab/WoXVn \VTV^d ]abCk]aW4^kffd ZCX%]aW4Uld2k5kWi/d2X%]aWn eX%]ffU_]aW4RvVq2
u8d ]abm 49cfiTVZCi/d ]	d TVZCXVnTVZ5U_TSkWVv
 }fgWd Z/\5cOb/TVU_W4ZlX%]9U_]aW4RlVq2Vq["w TV]	d2cfiW6wxTV^wxQ/]aQS^_W-^_WwxW4^W4ZCcfiW
]abCX%]	sDwTV^8XVn2n] s





 )
  _

   
X  
) P  /    P F _

aa

  _ 





 

aVV

ub/W4^Wd U&iW4/Z/Wi-fe5SaqCr;U*W4RCn2XVd Z/Wi d Z]ab/WkffXVd Z8]aW4]	swTV^EfgTV]ab  V4Vsjd UE]abSW"R/^_TVf*X%fCd2n2d ]e
]abCX%]8]ab/WXVn \VTV^d ]abCk iTW4UZSTV]{]aW4^kffd ZCX%]aWmX%]U_]aW4RVq26\Sd tVW4Z]abCX%](*d UjcbSTVU_W4Zzd ZoU_]aW4RVq2Vqjfi]

`_Y

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

WX%Ud2n e}wTSn2n T	uU;]abCX%]]ab/WR/^_TVfCX%fCd2n2d ]eY]ab*X%]8(*4d UTVQS]aR/Q/]X%]0U]aW4RoVq2ffd U

X    * u  



+
 * 
     ^^   X  u Va  ui
bQ/UsUD^	W  (* k    8  Xg}(*fiu  Va  
 aq?"Ud Z/\~aVVas/u9WbCXatVW8]abCX%]
1^	W  (
 *  
1^W  (* k      EX >( *  
   EX >(*ui
     _
     _


a

Ld ZCXVn2n esSu9W8bCXatVW8]abCX%]D^   W  (*fi    Va  asEwTV^0  V4Vq  bQ/UsCD^UX%]	d U_SW4U
]ab/WrhcfiTVZCi/d ]	d TVZCq
TV^]abSWcfiTVZtVW4^_U_WsgUQ/R/RgTVU_W{]abCX%]=D^;UX%]	d U/W4U]ab/WLrh cfiTVZ*i/d ]	d TVZCqm~W4]  V & ^i^i^i	4 j Vq
Wcb/TTVU_W5]abSWffRCX%^XVkW4]aW4^UwTVfi
^ {:f)z  X%UwTSn2n T	uUqPVW4]UXg}   D^W  hX%ZCion W4]
 *aq; d ]ab/TVQ/]ffn TVU_UTSwg\VW4Z/W4^XVn2d ]esgu@WX%U_U_QCkW8]abCX%]  *; jTV]abSW4^_u8d U_Ws7]	X%VW 
 *  1^    
]aTlcfiTVZSUd U_]0TSwA]abSTVU_WU_W4]aU']abCX%]hX%^_WTVf/UW4^_tVWi6u8d ]abRgTVUd ]	d tVW8RS^_TVfCX%fCd2n2d ]esCX%ZCiiTff]ab/W8R/^_TTSw/QSUd Z/\
  aq
 *fiVq~PW4U
] X;  *fi  D^    +
 *fi  *X%ZCq
i  ; )  Vq
TV^   V^ i^i^i	6 l"s@n W4]  *  V
 * +
  
b Q/Us*]ab/WjU_W4] (*8d UXVn uXaeU6^_W4Wcfi]aWi/sLQ/Z*n W4U_U (*  F	q2PSd ZCcfiWD^W vF ] 1^  
j
  
  feX%U_U_Q*kR/]	d TVZCsBd ]5k0Q/U_]fgWh]ab/W~c4X%U_Wff]ab*X%]  kffd Z r & 1^    QVqqw"T	u$U_W4]
w )    2
 D^ W }+
 *fiaq
W/^U_]"U_bST	uz]abCX%]	su8d ]abj]ab/W4U_WRCX%^XVkW4]aW4^&U_W4]a]	d Z/\VUsu@Whc4X%Z}cOb/TTVU_Wfi&U_QCcOb]abCX%]{cfiTVZ/U_]a^XVd Z]

S&d U-UX%]	d U_/WiS[
q ~W4{
]  
b v
  U_QCcObj]ab*X%f
] XE> h!
 
 P   )  P   F X;  Y t_ "q&TV^WXVcO
Vs)u@W{bCX_tVW









X  Yw_ 
j*
r  P   F    P)    `P     X; *fiY _ )

 
 &

*  F  X    * Y   
*  ) ; X    * Y L ) i

 b/W0n2X%U]DWQ*XVn2d ]e5wTSn2n T	uUCfAWc4X%QSU_W   V *   * Vq  b`QSUswTV^9XL/Wi-,s
  F  P   ) X    * Y  _ 
d UWd ]ab/W4^:X  *fiY /L  d2w >+fi* sTV^6X;&
   *Y ; ) d2wg *q]8wTSn2n T	u0U0]abCX%]


















/ *a 21^W +*a 
*  e /L  *EV 
 *  )  D^	  
*aa
21^W +*a
 *
 e /L D^   
j*
r *  )1/^   *     * F  D^	  +*U W* >



*




&


h Ud ZCcfiWW1^UX%]	d U_/W4Urhe

1


^



+

C
*


W


h








*
F







,
i


 bQ/Us    S d2wXg} aXEo   Vs@U_T>]abSW4U_W~RCX%^XVkW4]aW4^YUW4]a]	d Z/\VUmX%^WX%R/R/^TVR/^d2X%]aWowTV^
{:f)zF]	X%1d Z/\N  wxTV^8X%ZeUQCcb]ab*X%]Xg} !Vaq;ffTV^W4T	tVW4^s    q

WZ/Tu U_b/T	u ]abCX%]	s0u8d ]abz]ab/W4U_WmRCX%^XVkW4]aW4^U_W4]a]	d Z/\VU
s 1^	W
  k    jd U]ab/W
R/^_TVf*X%fCd2n2d ]ez]abCX%]<Q6f)zFYb*XVn ]aUu8d ]abh4asffwTV^oXVn2n[ 
%X ZCi q n WX%^n e d2w
1^	 W  h  Vs]abCd Uhd U0]a^_Q/WsUd ZCcfiW{]ab/W4
Z 1^	 W   k      VsEX%ZCi6]ab/W{R/^_TVf*X%fCd2n2d ]e
]abCX%N
] {:f)zFhb*XVn ]aUu8d ]abzTVQ/]aR/Q/]m  4d U>X%]>kTVU_<
] XE>    D^W
    VqPT
U_Q/RSRATVUW0]abCX%
] D^	 W   H
 Vq  b/W4Z>d ]-U_]
Q vlcfiW4U@]aT6Ub/T	uz]abCX%
] D^	    +*UW   -d U&]ab/W

`_=

fi " |qxq}t~"}s1|

R/^_TVf*X%fCd2n2d ]e ]abCX%]0 4*ad UTVQ/]aR/Q/]	sV\Sd tVW4Z5]abCX%]2vd U;cb/TVUW4ZX%]]ab/W&/^_U]*U]aW4RCq-EQS]]ab/W{X%^_\VQCkW4Z]

TSw7]ab/W/^_U]&bCXVn2w/TSw7]ab/WRS^_T`TSwSU_b/TuU]abCX%]0]abCd UR/^_TVf*X%fCd2n2d ]e>d UQ/U_]x qCQ/]






X%U iVW4Ud ^_Wi/q

&  
b
 


 ) 
T_Y)

1^  

&




     

)



/)&

//

b   


h Ud ZCcfiWU 
/



b

 

 +*fia21^W >+*fi
h Ud ZCcfiWD^UX%]	d U_/W4ULrhe
1^   +*CW  h



 {
 ;7fi|	
1^8ZeC'`%V & ^i^i^i4FjAh6QD/'A|,|	+	
\b"a\ 
:eS *
 & ^ i^i^i^gj)ffR &  ^^  gj   E ) 8 |	,	n	 &  & ^i^i^i^gj7FjE#Lp 
@VV^i^i^i 6"
l W* lK+Sfffi6,	)n	+	S
Oz %}9=D
 ^	   8V &
 e1 ^W     8  D ^8
 6 &  & ^i^i^i^gj7Fj7R 
 >*fi
zu}D ^    W     D ^	   W +*fi	 8?
 [+*)D ^	W 
 !
h
 


  bSW~R/^_TTSwd UYUd2kffd2n2X%^md ZU_RCd ^d ]ff]aTo]ab*X%]YTSw  bSW4TV^_Wk Vq2VqPQ/R/RgTVU_W]abCX%]maXV{bSTSn2iUs
 >+*fis1X%ZCiD^W  h!Vq  bSW4Z
D^W  
     
D


^


W


 ?D^   8
 a21^W   

    

j
j
D

^




6



^

^
i
^
i
	
i
^



?

1


^


W


 a21^    

 8
& &
 a2D^8 
 C4* 1^,>fi+fi* ?1^   
C

*
4
1

	
^




8
a

2

1

8
^


a



*





[<aR

P/d2kffd2n2X%^n es

D^	    W >+*fi
 D^	W >*Y   ?D^	 
CS  1 ^    6 &  & ^i^i^i	^ j

   fi+*?D^ 
CS  C *\D ^8

 a+*fi
 C*_1 ^   8a21 ^8}
 ^W  
 bQ/UsCD^W        D
 K
 Vq

 a2D^	W >*
 
 j ?1^W  a2D^ 
 }+*fi
 8a21 ^W





* 0wTV^

XVn2n$o

*



*

U_QCcOb~]abCX%]m1^

 

TV^]ab/WcfiTVZtVW4^_U_Ws-U_QSR/RgTVU_W]abCX%]fCbSTSn2iUmX%Z*i D
 ^	    Vq${d tVW4Z  *fisd2w
1^	W     Vs1]ab/W4ZaXV-]a^d t1d2XVn2n emb/TSn2iUsAU_TUQ/R/RgTVU_W ]abCX%]W1^/W  h0VqPQ/R/RgTVU_W
]abCX%] *qn WX%^n e D^6 &  & ^i^i^i^gjAFj7  ;*_1^8fi+*aqwBTu8s*Q/Ud Z/\[fCas*u@WYbCX_tVW
]abCX%]









D^	W   
D^	W    
D^	    W
D^	W +*C 
C*\D ^8
 fi+*

 
 

 ?D^   ha21^W  
>+*fi?1
 ^W   a2D^   8
8

?


1
^W   a2D^W }+*fi


h Q/Ud ZS\~aVLq

`_u`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

 bQ/Us1aXVLbSTSn2iUq

 6|	 ,		n /
:eS */,0'`,V & ^i^i^i 47j 6G0 /66EG#
  / & ^i^i^i2` *   * &  & ^i^i^i^ *j  j ,)	&C *( S87n	%Y%	)
X   aX  Y`*fi0.	6SVV^i^i^i	^;	fi7, %o1^q ,
Xg  1^, zff6Xg {p /*6mD^5}Y/D^5,MK-85		S	fi
,xQy
/z `%z}66*ffpo%};	{ & ^i^i^i	4jE
TSw7u@TV^n2iUsEXU_W4]  / & ^i^i^i 2`
 TSwSTVfSU_W4^_tX%]	d TVZ/U;u d ]abi/d U_]a^d f/Q/]	d TVZ
[<aR   {d tVW4Z[XU_W4]
X  UX%]	d Uwxe1d Z/\X  Y`* mwxTV^65 VV^i^i^i ^ Vs0X%ZCi[X%^_f*d ]a^X%^_e|i/d U]a^d f/Q/]	d TVZ/UD ^8 TVZF	 s 
V^i^i^	i 6"
l sAu@W6W4RCn2d2c4d ]	n ecfiTVZ/U_]a^_QCcfi]5X8R/^d TV^W1 ^TVZe
 ]abCX%]UX%]	d U_/W4UL r6zU_Q*cb]abCX%]fiXE  1 ^,&
 s
ub/W4^WW1
 ^  d U0]abSW5kffX%^_\Sd ZCXVn)TSwRD ^TVZ$
 X%Z*i1 ^,  1 ^8
 ufi  aq
{d tVW4Zv
 }F	 s1iVW4/Z/W
 ^	a5
D
 
 	  /   `*fi6W>
 /   h
 V  X  Y`*u; *(7D ^,	 h
 ui
ay"T	uv]ab/W{R/^TVfCX%fCd2n2d ]e}d U'U_RCn2d ];Q/RTtVW4^6XVn2n/]ab/W^Q/Z/UV6
 U_Q*cb~]ab*X%]  /   `*-X%ZCiW}
    
d U{d ^_^_Wn W4tX%Z`]	q2{]-^_Wk5XVd Z/U-]aTcOb/Wcj]abCX%]Z1
 ^{d U{Xhi/d U_]a^d fSQ/]	d TVZTVZq X%ZCi8]abCX%]{d ]@UX%]	d U_/W4U{XVn2n]ab/W
[<a"ZW " ^ "  l

^_W	QCd ^_WkW4Z]aUqLfi]d U0WX%Uej]aTlcb/WcO]abCX%]



j
1^	   `* 
X  Y `*fiuC*(7D^,	
r & F  

fi ]8wxTSn2n TuU0]ab*X%]
*
r & 1^   `*fi
]ab/WYk5X%^_\Sd ZCXVn)TSw1^TVZqL4w]v>







 X  Y `*ui

 sU_bST	u8d Z/\6]ab*X%]1^8d U XR/^_TVfCX%f*d2n2d ]e>kWX%U_Q/^WYX%ZCixX
V
s]ab/W4Z



dU



1^,>fiF  D^,> a2D^aF
 bfiW)  /  _  )  /




	ff

S    	
 _fi")  / 
 _)  /  bfi)
 CS 	
 
)
 S    _ ) // 	ff

 ^   ui
1

  b  ) _S /
 / 
  
_"
fi ) / 

 d ZCXVn2n esgZ/TV]aWh]abCX%]	swTV^VV^i^i^i6l"VswTV^ffXVn2nEF'U_QCcObm]abCX%]=1^W
L
]abCX%]
D^    `*CW  h
 fi")  /    b
_
   ) /


 b fiW) /    b   )  /
 	
 fi")  /  
_

 b  fi )  /   
 	
 fi")  /    b
_
 )   /


 b fiW) /    b ) 1  t /
 t
	
 r 
 b ) b fi ) 1    t  / /
 D^    `*CW> 	

 VsAu9WhbCX_tVW
  V

U_Tff]abSW\VW4Z/W4^XVn2d 4WiLrhcfiTVZCi/d ]	d TVZb/TSn2iU8wTV^8V & ^i^i^i4Fj7Vq

 TR/^_TtVW  b/W4TV^_Wk Vq2VsBu@W/^_U_]hZ/W4WiU_TSkWfCXVcO`\V^_TVQSZCimTVZkffd ZCd2k0QCk^_Wn2X%]	d tVWW4Z`]a^TVR`e
i/d U_]a^d f/Q/]	d TVZ/UqLd U_TSkW{U_R*XVcfiW X%ZCi~n W4]8 & ^i^i^i4Fj5fgW{U_Q/fSU_W4]aUTSwq0~W4] fAW]ab/WU_W4]0TSw

`_Y

fi " |qxq}t~"}s1|

u & ^i^i^i	^EjS-wxTV^-ub*d2cb]ab/W4^_W'W41d U_]aUU_TSkWffi/d U]a^d f/Q/]	d TVZ<Xgu d ]ab<XE>a*fi  ;*9wxTV^-
%X ZCicXE> = mwTV^~XVn2n {vqw"T	u n W4][XE fgWmXmi/d U_]a^d f/Q/]	d TVZu8d ]abJXE> =
@    ^i^i^i	  jA j s1n W4]
 
 vq&{d tVW4ZXtVWcfi]aTV^ 
&



V^ i^i^i	6 l
mwTV^~XVn2n


)  /"!#!#!
) /
X    h 
%$ t $ XE> a
  
t 

ub/W4^W   d Uh]ab/Wd ZCi/d2c4X%]aTV^~wQ/ZCcfi]	d TVZCs&d2q Wq   h  ld2wR v X%ZCiYTV]ab/W4^_u8d UWs&X%ZCi  


)7   
  
 ) /&"!#!#! %$   $ ) / gX } jd UlXZSTV^kffXVn2d X%]	d TVZ wXVcfi]aTV^q~W4];*  X   a*fiYwxTV^Y 
V^i^i^i	6l"q0EeuLUd U2X% ^sDVVVVs  b/W4TV^_WkU Vq2ffX%ZCiVq2VasCd ]8wxTSn2n TuU0]ab*X%]

Xg}u6
&

 & ^i^i^i^Ej7jS

X  
 '



j

aVV

@    ^i^i^i	  j 0
 TV^_W4T	tVW4^s@wTV^WXVcb}tVWcfi]aTV^u & ^i^i^i^ j 0(ffsg]abSW4^_W~d U5X tVWcfi]aTV^ 
5
 
U_QCcOb
&
]abCX%]aVV*b/TSn2iUqaTV^X%Z>d ZCwTV^kffXVn7X%ZCi8WX%U_eiW4^d tX%]	d TVZTSwDaVVasU_W4W6uLT	tVW4^  bSS
T kffX%UsAVVVVs
LbCX%RS]aW4^8Vaq2
ofnq e1E 	?   &  & ^i^i^i^EjAFj6|phYu & ^i^i^i^gj7;) E 	  & ^i^i^i  j7m
n	,	)	Zz|%}Y
. & ^i^i^i^EjU.9  *  V	{,|ph0@>VV^i^i^i6l"B
XE>
 a+*C6 &  & ^i^i^i	^C*  & +*  & ^;* R& +* R& ^i^i^i^EjAFjS  ;*fii

Dh\



 vd ]ab/TVQS]9n TVU_UETSw\VW4Z/W4^XVn2d ]esX%U_U_Q*kW]abCX%]
d ]8wTSn2n T	uUhwx^TSkaVV]abCX%]

[<aR

X   6 +
U_Tff]ab*X%]

&



Vq  X%gd Z/\

*  X    a+*fiDwTV^D 

V^i^i^i	6l"s

j  j     +*   * "!#!#! % $   $ X   ha
 


 & ^i^i^i	^EjSjS  Xg>u6 +  + ^i^i^i^ j Fj7ui
j jS   * X%ZCi<XE>a+*U6 &  & ^i^i^iXEj7jS  ;*;wxTV^  V^i^i^i	6l"s7u@W
V^ i^i^i	6 l"q  bQ/Us+Xg>u6 +  + ^i^i^i^gjAFj7  Xg}u6 &  & ^i^i^i	^Ej7jS

XE>u6

P/d ZCcfi\
W XE>a+
 *U6 +  + ^i^i^i	^
bCX_tVW8]abCX%]m;*   * wxTV^' 
X%ZCi/s1d Z~RCX%^_]	d2cfiQCn2X%^s

 + ^i^i^i^



&

 &  Xg}a & 6
&

 & ^i^i^i^Ej7jS

 XE>a & 6
+

 + ^i^i^i^gjAFj7ui

@on	oK G~,)/j/o>#  / &  + }G~|	,	n	/8
`*  ;* & 
 + R    V4 
ZD^e,o%	) ,)	1^    & ,
1^	    + H
9/1
 ^,}
    D^W*   hHv0	?$ E 	HD^ *  1^u  
*
`*fi,"/~
KD^   ff=h
p SLGD^ 96f & Q + {/fi	fi
	ffB
*
6//-|n 1
 ^   1 ^8
 u `*,\	 Z,  V4
~ W4] &   &  + ^ +   +  & ^ .   &;k  + s"X%Z*i 2    a &  + aqlP/d ZCcfiW
[<aR   
 & ^ + ^ . ^ 2 X%^_W~XVn2nX%U_UQCkWiY]aTfgW8Z/TVZSWkR/]es1u9W6bCX_tVW,  aV4V + sAub/W4^W- d U5iVW4/Z/WimX%U
X%fgT	tVWs]abCX%]'d Us.$d U"]ab/W0U_W4]'u & ^ + gU_QCcOb5]abCX%]"]ab/W4^_WW41d U_]aUX{i/d U_]a^d fSQ/]	d TVZNX  u8d ]ab4X  a &  
 & Xg>
 a +    + s2Xg>
 h
 K
 5wxTV^8XVn2n?v
 v
 q*4wD ^ *  D ^,>
 u `*wxTV^0  V4Vs)]abSW4Z
pE1 ^,}
 u  &   a  pAu1 ^8}
 u  +   1 ^,}
 
aVV


\b"a\



:e
*
& ^C* +

`_ua

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

ub/W4^W3p  D^	    & aqLWR/^_TtVW]ab/W]ab/W4TV^_Wkvfe5Ub/T	u8d ZS\ ]ab*X%]aVV0c4X%Z/Z/TV]&b/TSn2id2wWd ]abSW4^
 & TV
^  + d U9Z/TV]0W47^_W4epn2d VWqP/d Z*cfiWu9W;bCXatVWhX%U_U_QCkWi]abCX%]'uC* & ^;* + >aV4V +   wTV^"  V4Vs
+ wxTV^@
u@Whc4X%ZX%R/RCn e~aVVC]aTx`*BwxTV^@  V4Vq  bQ/UsV]abSW4^_W X%^_W'tVWcfi]aTV^_U  * &   * + B
 V40U_QCcOb
]abCX%]	sDwTV^8XVn2]
n 
 vs
1,^ >  `*fi     
   
   *   D,^ >  u i
aVV

*

 *

 *  &   
 D^,>u & as\D^,>u +  `*   *  &   * D^8u + as\D^,>u .  `*fi 

u 2   *    *  & 1
 ^  u 2 aq"n Q/\V\Sd ZS\]abC d UYd Z]aToaVVasEu9W6TVf/]	XVd Zo]ab/W

aVVEd2kRCn2d W4UH1^u &  `* 
 *  &   
   * D^  u  . asK1^ 

wTSn2n T	u8d Z/\mwxTVQS^&WQCX%]	d TVZSU

D^8u
+





D^8u
.





D^  u 

p   /
 
 1 ^8u &  
 &
p   
 * 1^8u + 

 &
p   
/
 +* 
 1^,>u
 &

2





p

D^8u





&





&

1^  u 

2





 p7   +* 1 ^,>u & 
+
a  p7  +*/* 1
 ^,>u + 
 +
+* 
 +*/* D
 ^,>u
.   a  p7  
+

a  7
p  1^  u 2 ui

 +
a

.


a%S

P/d ZCcfiW&u@W&bCX_tVW8X%U_U_QCkWi]abCX%]1^ !wTV^;XVn2nM evs/d ];k0Q/U_]LfgW&]ab/Wc4X%U_W]ab*X%]1^  u * 2Vs
wTV^ff  V^ i^i^i	fiSq  b`Q/U
s 1^u T*fiffwXVcfi]aTV^_U5TVQ/]6TSw"]ab/W]abWQ*X%]	d TVZX%fgT	tVWqEe]ab/W}cObCX%Z/\VW~TSw

tX%^d2X%fCn W4U   pA  & s1    a  p7a  + 2
s  *( 
  hX%ZCi UTSkW^_W4u^d ]	d ZS\Ssu9WU_W4W]ab*X%]a%S

d UW	QCd tXVn W4Z`];]aT

  Y +0&
 
  &+  a   Y ++
 
aVV
 Y &&   &+   &&  &+   a   aY +0&   ++   +0&  ++ ui
4ws wxTV^~U_TSkW,s;fgTV]abHW* & X%ZCi>v* + X%^_WZSTVZ/4W4^_TSs;]ab/W4Z]ab/W]abS^_W4WWQ*X%]	d TVZ/UTSwhaVVhbCX_tVW>Z/T
U_TSn Q/]	d TVZSUhwTV^  [aV4Vaq9QCd tXVn W4Z]	n esd2wwxTV^'U_TSkW8,s/fgTV]ab  * & X%ZCi  * + X%^_W8Z/TVZ/4W4^_TSsS]ab/W4Zl]ab/W
wTVQ/^0W	QCX%]	d TVZ/U'TSwa%SBb*XatVWZST5U_TSn Q/]	d TVZSU wTV^:
p oaV4Vaq{PTld ];TVZCn e^_WkffXVd Z/U']aT5Ub/T	u]abCX%] wTV^
U_TSkW ,sSfgTV]ab  * & X%ZCi  * + X%^_W Z/TVZ/4W4^_TSq  TjU_W4W]ab*d Us7Z/TV]aW8]ab*X%]0feoX%U_U_QCkR/]	d TVZ[wTV^'U_TSkW%s)`*
d UZSTV]q4W4A^_W4epn2d VWqCQ/]]ab/W4Zzd ]jwTSn2n T	uUw^_TSk ~/WkffkffXhr6q2X%fgTtVW5]abCX%]8fATV]ab  * & X%ZCi  * + X%^_W




 

Z/TVZ/4W4^TSq  bQ/Us]ab/W]ab/W4TV^Wk

&&



a

d UR/^T	tVWi/q

zfiq| I ?q
r6n2cb/TVQ/^^CTV ZCsF0qD"q2sC{X% ^iW4ZCwTV^_UsFAq2s*$lX%1d Z/U_TVZCsr'qCaVVVVaq{-Z]ab/WYn TV\Sd2c0TSwA]ab/W4TV^_e}cb*X%Z/\VW
R*X%^_]	d2XVn-kW4W4]wxQ/Z*cfi]	d TVZ/UwxTV^cfiTVZ]a^XVcfi]	d TVZX%ZCi~^_W4t1d Ud TVZCq`
 %/-G[
 |/Mpm + E 	4s10s
VVSVVVq

-X%^_py'd2n2n Wn2sq2s*$/XVn 1sL0qCaVVVVaq8PTSkW]aWX%U_W4^_U6cfiTVZCcfiW4^_Z*d Z/\mcfiTVZ*i/d ]	d TVZCXVnSR/^_TVfCX%fCd2n2d ]	d W4Uq*	
S)
s sEVV SVVVq

LTtVW4^s

 q7lq2sA

 bSTSkffX%UsEqVr6qAaVVVVaq9
ffp /'GZaph*%q1

`_u

d2n W4esUwBW4uzTV^_1q

fi " |qxq}t~"}s1|

LUd U2X% ^s-4q;aVVVVaq324piSd tVW4^_\VW4ZCcfiW\VW4TSkW4]a^_e|TSw-R/^_TVf*X%fCd2n2d ]eiSd U_]a^d f/Q/]	d TVZSUmX%ZCikffd ZCd2k5d X%]	d TVZ
RS^_TVfCn WkUq*
 
x //+G:
 9 | +s54DaVasC%SSVVVq
LUd U2X% ^s14qaVVVVaq6b`en WX%U_]{UQCX%^W4UYX%ZCi>kffX%1d2k0QCkW4Z]a^_TVR`e1r;ZX%1d TSkffX%]	d2cjX%R/R/^_TSXVcOb>]aT
d Z*wxW4^_W4Z*cfiWYwxTV^ n2d Z/WX%^ d ZtVW4^_U_W{RS^_TVfCn WkUq0
x //+G:
 |/,as|E Sas*VVVSVVVVq
rXau8d2iSsrhq?g qaVVVVaqSr|ZSTV]aW&TVZ~kffX%gd2k0QCkW4Z]a^_TVReX%ZCi6-X_eVW4Ud2X%ZlcfiTVZCiSd ]	d TVZCd Z/\Sq2q+"Z/R/QSfCn2d U_b/Wi
k5X%Z`Q/Ucfi^d R/]	q

rXau8d2iSsgrhqRgq2ssrd2cVW4es;qLqaVVVVaq~Cd VWn2d b/T`T1iX%ZCim-XaeVW4Ud2X%Zzd ZCwW4^_W4ZCcfiWwx^TSkU_Wn Wcfi]	d tVWn e
^W4RATV^]aWii/X%]	XVq0
 ),SEG6
 xVh
p 	%,4|/;
x ,s[ 6EaVVVasL%SSVVVq
rd2XVcfiTVZCd Us[g q2s" D X%fgWn2n2s-P/q!E~ q"aVVVVaq}PTSkWmXVn ]aW4^_ZCX%]	d tVW4Uh]aT@X_eVW4U Uh^_QCn WqYfiZ-^_TSwkffX%ZCs@-q2s
 -u@W4ZCs8{qa"iUq2as3"fiff
 |/,S1 /on 	%[G|
 *
  ,/|a 8n /ff
 *8	 	fi	S}
9+"
 ,/|h
p sR/R*qAVSVVq
r9Q/fgTSd Us
r q2s1 ^XViWsy'qaVVVVaq r'Z$d Z`]a^_T1iQCcfi]	d TVZz]aTRgTVU_Ud f*d2n2d U_]	d2cmX%ZCizwxQ/44evn TV\Sd2cfiUq fiZ
PVbCXVwxW4^s/{q2s7
 WX%^n2sg qa"iVUq2asgBy ,/	 g S	%By ,//4sR/RCq)%SSVVVqAffTV^\SX%Z
tX%QCwk5X%Z/ZCsgP/X%Zo^X%ZCc4d UcfiTSq
^_W4QSZCi/sU
 q19qDaVVVVaqQ1 Q/4n W{TV^0RCX%^XViTgq0xVh
p 	%4|/	)sR|E SasCVVVSq
^d WiSkffX%ZCs_{
t q2sPVbCd2kTVZesr6qaVVVVaqE X_e`Z/W4UkffX%1d2kQCkW4Z`]a^TVR`eR/^W4Ucfi^d R/]	d TVZX%ZCi-R/^_TVf*X%fCd2n2d ]e
]abSW4TV^_eqQ
 ),SDG:
 |S,,)"	_sF sDVVSVVVq
^d WiSkffX%ZCs;
w q2sC$y'XVn RAW4^ZCsF` q6qEaVVVVaq5T1iWn2d Z/\jfAWn2d Wwd Z[ieZCXVkffd2c&Ue`U_]aWkUqF9 X%^_]04DwTVQ/Z/p
iSX%]	d TVZ/Uq0
x % K-	Ca /+		S4sFd DaVas*VVSVVVq
^d WiSkffX%ZCs4;
w q2sy;XVn RgW4^_ZCs4` q46q`aVVVVaq15TgiVWn2d Z/\;fgWn2d Wwd ZieZCXVkffd2cDU_e`U]aWkUq9 X%^_]*_^_W4t1d Ud TVZ
X%Z*ihQ/RBi/X%]aWq
 ),/DGmxQou2"y 	,fi	s2+ 0sCVVSVVVq
{X%^iVZ/W4^s1lqDaVVVVaq|/,Sx|/		/ K-xVh
p ,- |ff
 6h
 ~7h
p "),
/!Ion 	%
,/aq;P/d2kTVZ} P/cOb`QSU_]aW4^sC"
w W4uTV^_1q
{X%^iVZ/W4^s1lqDaVVVVaq#0
x |0@ +7 yV^_W4WkffX%ZL TSq
{d2n2n2s6;q.'
r q2s;tX%ZiW4^C~ XVX%ZCshq2s6 BTVfCd Z/Usm qaVVVVaq L TSX%^_U_W4ZCd ZS\X%]^X%ZCiTSk5 bCX%^XVcfip
]aW4^d UX%]	d TVZ/Us&cfiTVZ/Wcfi]aQ/^_W4UX%ZCi}cfiTVQSZ`]aW4^_pW41XVkRCn W4UqjZJ"fi7VL
 ,,|/+*8 /
-,asRSRCq1VVSV%Sq
-^_TtVWs7rhq
 q2sEy;XVn RgW4^_Z*sF qhqCaVVVVaq1 ^TVfCX%fCd2n2d ]eQSRiSX%]aW;cfiTVZCiSd ]	d TVZCd Z/\5tUqCcfi^_TVU_U_pW4Z]a^_TVReq
fiZm"fi7*
 ,,	/z*8	 	fi	/,sg S	%/m
x , K-Ha /	+	/,z	UxQ gG} s
RSRCq1VVSV%Sq

-^QS Z`uXVn2i/sUgq1aVVVVaq;PV]a^_TVZ/\hW4Z`]a^TVR`e>cfiTVZCcfiW4Z]a^X%]	d TVZCs/\SXVkW{]ab/W4TV^_eX%ZCiXVn \VTV^d ]abCkffd2c-^X%Z*iTSkp
ZSW4U_Uq7q
Z "fi7 S%/W
 x0//)**8 /8o*|
 p;7)/ E ,//
 *,s`R/RCq
VV SVVVq
y'XVn RAW4^ZCs]qVhq2s7/X%\Sd ZCsA0qSaVVVVaqBffT1iWn2n2d Z/\8`ZST	u8n Wi\VWhX%ZCi5XVcfi]	d TVZd ZliSd U_]a^d f/Q/]aWiU_e`U]aWkUq
I%o )>*| p;7)S48
s 4ESas*VV SVVVq

`_u

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

y'XVn RAW4^ZCs`q/6q2sL  Q/]a]	n WslqC;qCaVVVVaqWt-Z/T	u n Wi\VWsAR/^_TVfCX%f*d2n2d ]esLX%ZCimXVitVW4^_UX%^d W4Uq=`)%/
6 h
 x8*s:9;0DSasCVVSVVVq
y"Wd ]aX%ZCs[rq2s-Q/fCd Z*sHrq@aVVVVaql\VZ/TV^X%fCd2n2d ]e|X%Z*iocfiTSX%^_UWiSX%]	XVqx/S
YGx|S,as0s
V%V/
 SVVVVq
y"Q/Z`]aW4^s[r'q@aVVVVaqX%Q/UXVn2d ]eX%Z*ik5X%gd2k0QCk W4Z`]a^_TVRe}QSRiSX%]	d Z/\Sqa/,/S[`%/-G
x"7fi| o ph
 yB,//4<
s 4EaVasEVV VSVVq

4W4A^_W4esL0qq*aVVVVaq=1^TVfCX%fCn WZ/Tu8n Wi\VWq'fiZ~CX%)X%]aTVUsA4q*a"i/q2as!a/,/S0*
X)4p
\9
,7mG\
 |/		S7* \"fi| 
ffp Ga /)		n  E 	4s*RSRCqVVSVVVqV"w TV^_]ab/p
y"TSn2n2X%ZCi/srhkU]aW4^i/XVkffq

t{n Wd ZfCX%QCkffs;rq9aVVVVaq|/)8nonKxS
,8x|/ 	 E ,//S7qmP]	X%]	d U_]	d2cfiU~d Z}]abSWly"WXVn ]ab
PSc4d W4ZCcfiW4UqEPVR/^d Z/\VW4^_p/=W4^n2X%\SsCB
w W4uTV^_1q
t-QCn2n f*XVc1sDP/q1aVVVVaq.a 8 8h
p  * ,%YSx|/	aqvd2n W4eq
t-QCn2n f*XVc1s"P/q2sB ~ Wd fCn W4^s90qAr6qBaVVVVaql-Zd ZCwTV^kffX%]	d TVZX%Z*ijU_Q]vc4d W4ZCcfieqx//+6Gj	
ph|/,as>6+6VsEVSVVq
5TVU_]aWn2n W4^s&Lq"aVVVVaq*
  	 [*+	/S\"fi|	 +7h
p ff"fi|, |	 
m<|S
)/_qjrhi/iSd U_TVZ/p
W4Un W4esCBWXVi/d Z/\SsDlX%U_Uq

w'd Wn U_W4ZCsP/q@aVVVVaqv*,,	//l.yB/|p /8|/p )+"x0
,|phaqDbCqhrqE]abSW4Ud Us
r9W4RCX%^]	kW4Z`];TSw  b/W4TV^_W4]	d2c4XVnEPV]	X%]	d U_]	d2cfiUs)BZCd tVW4^Ud ]eTSwRLTVRgW4Z/bCX%\VW4ZCq
TVf*d Z/Us2
 q2s"TV]aZCd ]a4es1r6q2sB P/cObCX%^wU_]aWd ZCs2
r qaVVVVaqPW4Z/Ud ]	d t1d ]eX%ZCXVn eUd UYwTV^U_Wn Wcfi]	d TVZofCd2X%U
X%Z*i Q/ZCkWX%U_QS^_WiYcfiTVZ*wxTVQ/Z*i/d Z/\d Z}kffd U_Ud Z/\i/X%]	XhX%ZCic4X%Q/UXVn1d ZCwxW4^W4ZCcfiWffkTgiVWn UqDfiZ}y'XVn2n TV^s
lq1"q2s1 CW4^_^esUr'q1a9iUq2as+|/,,E	+1Affph+	q*m"]nfi]ph	S9S
*+/,;S%+as/5r?=TSn QCkWYVVVs/R/RCq1SVVqEPVR/^d Z/\VW4^_p/=W4^n2X%\Sq

QSfCd ZCsYrqDaVVVVaqBfiZCwW4^_W4ZCcfiW5X%ZCi~kffd U_Ud Z/\~iSX%]	XVqQ-p ,	Vs8@+4sEVVSVVVq
P/cObCX%^wU_]aWd ZCs4rq`{q2s?rX%ZCd Wn Usq2sTVfCd ZSUs\`q`lqaVVVVaqZCcfiTV^RATV^X%]	d Z/\0R/^d TV^CfgWn2d WwU-X%fgTVQ/]CU_W4p
n Wcfi]	d TVZfCd2X%Ud Z`]aT ]ab/W8X%Z*XVn e`Ud U@TSw`^X%ZCiTSk5d 4Wi]a^d2XVn U@u8d ]ablk5d U_Ud Z/\{TVQS]	cfiTSkW4UH
q -|
 ph	%o q
 TX%R/RgWX%^q
PWd2iW4Z*wxWn2i/s  q1aVVVVaq;DZ`]a^_TVRe>X%ZCihQ/Z*cfiW4^_]	XVd Z`]eq9+,7G:|S		/,4s++4sSVVSVVq
PbCXVwW4^s/qSaVVVVaqHxffp ,R*%6G#Hn		/,4qR1^d ZCcfiW4]aTVZN"ZCd tVW4^_Ud ]e1^_W4UUsD^d ZCcfiW4p
]aTVZ*C
s w;hq `q
PbCXVwW4^s{q)aVVVVaqLTVZCi/d ]	d TVZCXVnRS^_TVfCX%fCd2n2d ]eqa/	%//|/]yB7n*s;+4EaVas7VVSVVVq
Pb/TV^Ws0`q"q2s; 4TVbSZ/U_TVZCs'0qL q0aVVVVaqr'1d TSkffX%]	d2ciVW4^d t)X%]	d TVZTSw9]ab/WR/^d ZCc4d RCn WTSwkffX%1d p
k0QCk$W4Z`]a^TVR`eoX%Z*i6]ab/W R/^d ZCc4d R*n W{TSwBkffd ZCd2k5d2kQCk cfi^_TVU_U_pW4Z]a^_TVRequHK S//8
a  ph *,,Rs 7*A6+@EaVas*VSVVq
Pe`^kUs-q;aVVVVaqX%1d2k0QCk
|SS	,48
s @14sDV S%Sq

W4Z`]a^_TVRed ZCwW4^_W4ZCcfiW[X%U>XU_RgWc4d2XVn'c4X%U_WTSwcfiTVZCi/d ]	d TVZCXVn2d X%]	d TVZCq

`_7

fi " |qxq}t~"}s1|

KvffZ/1sqSaVVVVaq  b/WcfiTVZ/U]a^XVd Z`]@^_QCn W0TSw]abSW8kffX%1d2kQCkW4Z`]a^TVR`e6R/^d ZCc4d RCn Wqe|/)	;%
/x"
 +AGh~		%4"	_s>6g,s/SSVVq
tX%ZXVkRgW4Z/b/TVQ/]	sq2sLLTtVW4^s  qLaVVVVaqjX%1d2k0QCkW4Z]a^_TVR`e[X%ZCicfiTVZCi/d ]	d TVZ*XVnAR/^_TVfCX%fCd2n2d ]eq
uHKSSS
 a8 	p *%s7*6gSas/SVVSVVq
tX%ZV^XVX%U_U_W4ZCsg-qEq7aVVVVaqErR/^TVfCn WkwTV^-^_Wn2X%]	d tVW5d ZCwTV^kffX%]	d TVZ>kffd ZCd2k5d 4W4^_UqH-%`)%/
	{3
 "
7j6 :|/		S48
s 4+6sDVV SVVVq
tVTVU PSXatX%Z`]	sCqDaVV%Saq.x',h~%
)qPV]	q1X%^_]	d ZSU lX%U_U lX%^_VW4]LX%RgW4^_fCXVcOgq
tVTVU6P/XatX%Z`]	s*lqDaX_eoVVsEVVVVaq&r;U_>lX%^d2n eZCq9fi5	/4q  b/W4^_Wu9W4^_WXVn U_TlwTSn2n T	u0Q/R
X%^]	d2c4n W4Ud 
Z "fio~/6TVZ r9Wc4q;VsVVVVslX%^cb VVsVVVVsZ4Q*n eVVsVVVVs8X%ZCi
cfi]aTVfAW4^hVVsDVVVVq
tVTVUP/X_t)X%Z]	sq&aPVW4R/]	q&Vs;VVVVaqr'U_X%^d2n e`ZCqJ9m~/4s'VVqzTSn2n T	u0pQ/RX%^_]	d2c4n W4U
X%RSRAWX%^Wid c
Z "ff~/&TV
Z r9Wc4qEVsDVVVYRCq1VV0X%Z*iW4fCqDVVsEVVV5RCqDVVaq

`_u

fiJournal of Artificial Intelligence Research 19 (2003) 25-71

Submitted 10/02; published 08/03

Answer Set Planning Under Action Costs
Thomas Eiter
Wolfgang Faber

EITER @ KR . TUWIEN . AC . AT
FABER @ KR . TUWIEN . AC . AT

Institut fur Informationssysteme, TU Wien
Favoritenstr. 9-11, A-1040 Wien, Austria

Nicola Leone

LEONE @ UNICAL . IT

Department of Mathematics, University of Calabria
I-87030 Rende (CS), Italy

Gerald Pfeifer
Axel Polleres

PFEIFER @ DBAI . TUWIEN . AC . AT
POLLERES @ KR . TUWIEN . AC . AT

Institut fur Informationssysteme, TU Wien
Favoritenstr. 9-11, A-1040 Wien, Austria

Abstract
Recently, planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems. In this paper, we present the language K c , which
extends the declarative planning language K by action costs. K c provides the notion of admissible and optimal plans, which are plans whose overall action costs are within a given limit resp.
minimum over all plans (i.e., cheapest plans). As we demonstrate, this novel language allows for
expressing some nontrivial planning tasks in a declarative way. Furthermore, it can be utilized for
representing planning problems under other optimality criteria, such as computing shortest plans
(with the least number of steps), and refinement combinations of cheapest and fastest plans. We
study complexity aspects of the language K c and provide a transformation to logic programs, such
that planning problems are solved via answer set programming. Furthermore, we report experimental results on selected problems. Our experience is encouraging that answer set planning may
be a valuable approach to expressive planning systems in which intricate planning problems can be
naturally specified and solved.

1. Introduction
Recently, several declarative planning languages and formalisms have been introduced, which allow
for an intuitive encoding of complex planning problems involving ramifications, incomplete information, non-deterministic action effects, or parallel actions (see e.g., Giunchiglia & Lifschitz, 1998;
Lifschitz, 1999b; Lifschitz & Turner, 1999; McCain & Turner, 1998; Giunchiglia, 2000; Cimatti &
Roveri, 2000; Eiter et al., 2000b, 2003b).
While these systems are designed to generate any plans that accomplish the planning goals, in
practice one is often interested in particular plans that are optimal with respect to some objective
function by which the quality (or the cost) of a plan is measured. A common and simple objective
function is the length of the plan, i.e., the number of time steps to achieve the goal. Many systems
are tailored to compute shortest plans. For example, CMBP (Cimatti & Roveri, 2000) and GPT
(Bonet & Geffner, 2000) compute shortest plans in which each step consists of a single action,
while the Graphplan algorithm (Blum & Furst, 1997) and descendants (Smith & Weld, 1998; Weld,
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Anderson, & Smith, 1998) compute shortest plans where in each step actions might be executed in
parallel.
However, there are other, equally important objective functions to consider. In particular, if
executing actions causes some cost, we may desire a plan which minimizes the overall cost of the
actions.
In answer set planning (Subrahmanian & Zaniolo, 1995; Dimopoulos, Nebel, & Koehler, 1997;
Niemela, 1998; Lifschitz, 1999b), a recent declarative approach to planning where plans are encoded by the answer sets of a logic program, the issue of optimal plans under an objective value
function has not been addressed in detail so far (see Section 8 for more details). In this paper, we
address this issue and present an extension of the planning language K (Eiter et al., 2000b, 2003b),
where the user may associate costs with actions, which are then taken into account in the planning
process. The main contributions of our work are as follows.
 We define syntax and semantics of the planning language Kc , which modularly extends the
language K: Costs are associated to an action by extending the action declarations with an
optional cost construct which describes the cost of executing the respective action.
The action costs can be static or dynamic, as they may depend on the current stage of the plan
when an action is considered for execution. Dynamic action costs are important and have
natural applications, as we show on a simple variant of the well-known Traveling Salesperson
Problem, which is cumbersome to model and solve in other, similar languages.
 We analyze the computational complexity of planning in the language K c , and provide completeness results for major planning tasks in the propositional setting, which locate them in
suitable slots of the Polynomial Hierarchy and in classes derived from it. These results provide insight into the intrinsic computational difficulties of the respective planning problems,
and give a handle for efficient transformations from optimal planning to knowledge representation formalisms, in particular to logic programs.
 We show, in awareness of the results of the complexity analysis, how planning with action
costs can be implemented by a transformation to answer set programming, as done in a system prototype that we have developed. The prototype, ready for experiments, is available at
http://www.dlvsystem.com/K/.
 Finally, we present some applications which show that our extended language is capable
of easily modeling optimal planning under various criteria: computing (1) cheapest plans
(which minimize overall action costs); (2) shortest plans (with the least number of steps);
and, refinement combinations of these, viz. (3) shortest plans among the cheapest, and (4)
cheapest plans among the shortest. Notice that, to our knowledge, task (3) has not been
addressed in other works so far.
The extension of K by action costs provides a flexible and expressive tool for representing
various problems. Moreover, since Ks semantics builds on states of knowledge rather than on
states of the world, we can deal with both incomplete knowledge and plan quality, which is, to the
best of our knowledge, completely novel.
Our experience is encouraging that answer set planning, based on powerful logic programming
engines, allows for the development of declarative planning systems in which intricate planning
26

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

tasks can be specified and solved. This work complements and extends the preliminary results
presented in our previous work (Eiter et al., 2002a).
The remainder of this paper is organized as follows. In the next section, we briefly review
the language K by informally presenting its main constituents and features on a simple planning
example. After that, we define in Section 3 the extension of K by action costs, and consider some
first examples for the usage of Kc . Section 4 is devoted to the analysis of complexity issues. In
Section 5, we consider applications of K c . We show that various types of particular optimization
problems can be expressed in K c , and also consider some practical examples. In Section 6, we
present a transformation of K c into answer set programming, and in Section 7, we report about a
prototype implementation and experiments. After a discussion of related work in Section 8, we
conclude the paper with an outlook on ongoing and future work.

2. Short Review of Language K
In this section, we give a brief informal overview of the language K, and refer to (Eiter et al., 2003b)
and to the Appendix for formal details. We assume that the reader is familiar with the basic ideas
of planning and action languages, in particular with the notions of actions, fluents, goals and plans.
For illustration, we shall use the following planning problem as a running example.
Problem 1 [Bridge Crossing Problem] Four persons want to cross a river at night over a plank
bridge, which can only hold up to two persons at a time. They have a lamp, which must be used
when crossing. As it is pitch-dark and some planks are missing, someone must bring the lamp back
to the others; no tricks (like throwing the lamp or halfway crosses, etc.) are allowed.
Fluents and states. A state in K is characterized by the truth values of fluents, describing relevant
properties of the domain of discourse. A fluent may be true, false, or unknown in a state  that is,
states in K are states of knowledge, as opposed to states of the world where each fluent is either true
or false (which can be easily enforced in K, if desired). Formally, a state is any consistent set s of
(possibly negated) legal fluent instances.
An action is applicable only if some precondition (a list of literals over some fluents) holds in
the current state. Its execution may cause a modification of truth values of some fluents.
Background knowledge. Static knowledge which is invariant over time in a K planning domain
is specified in a normal (disjunction-free) Datalog program  that has a single answer set and can
be viewed as a set of facts. For our example, the background knowledge specifies the four persons:
person(joe). person(jack). person(william). person(averell).

Type declarations. Each fluent or action must have a declaration where the ranges of its arguments are specified. For instance,
crossTogether(X, Y) requires person(X), person(Y), X < Y. 1

specifies the arguments of the action crossTogether, where two persons cross the bridge together,
while
across(X) requires person(X).
1. < here is used instead of inequality to avoid symmetric rules.

27

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

specifies a fluent describing that a specific person is on the other side of the river. Here the literals
after requires must be classical literals of the static background knowledge (like person(X) and
person(Y)), or literals of built-in predicates (such as X < Y). Our implementation of K, the DLV K system (Eiter, Faber, Leone, Pfeifer, & Polleres, 2003a), currently supports the built-in predicates
A < B, A <= B, A != B with the obvious meaning of less-than, less-or-equal and inequality for
strings and numbers, the arithmetic built-ins A = B + C and A = B  C which stand for integer
addition and multiplication, and the predicate #int(X) which enumerates all integers (up to a
user-defined limit).
Causation rules. Causation rules (rules for brevity) are syntactically similar to rules of the
action language C (Giunchiglia & Lifschitz, 1998; Lifschitz, 1999a; Lifschitz & Turner, 1999) and
are of the basic form:
caused f if B after A.

where A is a conjunction of fluent and action literals, possibly including default negation, B is a
conjunction of fluent literals, again possibly including default negation, and f is a fluent literal.
Informally, such a rule reads: if B is known to be true in the current state and A is known to be true
in the previous state, then f is known to be true in the current state as well. Both the if-part and
the after-part are allowed to be empty (which means that they are true). A causation rule is called
dynamic, if its after-part is not empty, and is called static otherwise.
Causation rules are used to express effects of actions or ramifications. For example,
caused across(X) after cross(X), -across(X).
caused -across(X) after cross(X), across(X).

describe the effects of a single person crossing the bridge in either direction.
Initial state constraints. Static rules can apply to all states or only to the initial states (which
may not be unique). This is expressed by the keywords always : and initially : preceding
sequences of rules where the latter describes initial state constraints that must be satisfied only in
the initial state. For example,
initially : caused -across(X).

enforces the fluent across to be false in the initial state for any X satisfying the declaration of the
fluent across, i.e., for all persons. The rule is irrelevant for all subsequent states.
Executability of actions.

This is expressed in K explicitly. For instance,

executable crossTogether(X, Y) if hasLamp(X).
executable crossTogether(X, Y) if hasLamp(Y).

declares that two persons can jointly cross the bridge if one of them has a lamp. The same action
may have multiple executability statements. A statement
executable cross(X).

with empty body says that cross is always executable, provided that the type restrictions on X are
respected. Dually,
nonexecutable a if B.

prohibits the execution of action a if condition B is satisfied. For example,
nonexecutable crossTogether(X, Y) if differentSides(X, Y).

28

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

says that persons X and Y can not cross the bridge together if they are on different sides of the bridge.
In case of conflicts, nonexecutable A overrides executable A.
Default and strong negation. K supports strong negation (, also written as -). Note, however, that for a fluent f, in a state neither f nor -f needs to hold. In this case the knowledge about
f is incomplete. In addition, weak negation (not), interpreted like default negation in answer set
semantics (Gelfond & Lifschitz, 1991), is permitted in rule bodies. This allows for natural modeling of inertia and default properties, as well as dealing with incomplete knowledge in general. For
example,
caused hasLamp(joe) if not hasLamp(jack), not hasLamp(william), not hasLamp(averell).

expresses the conclusion that by default, joe has the lamp, whenever it is not evident that any of the
other persons has it.
Macros.

K provides a number of macros as syntactic sugar. For example,

inertial across(X).

informally states that across(X) holds in the current state, if across(X) held at the previous state,
unless -across(X) is explicitly known to hold. This macro expands to the rule
caused across(X) if not -across(X) after across(X).

Moreover, we can totalize the knowledge of a fluent by declaring total f. which is a shortcut for
caused f if not -f.

caused -f if not f.

The intuitive meaning of these rules is that unless a truth value for f can be derived, the cases where
f resp. -f is true will both be considered.
Planning domains and problems. In K, a planning domain PD = h, hD, Rii has a background
knowledge , action and fluent declarations D, and rules and executability conditions R; a planning
problem P = hPD, qi has a planning domain PD and a query
q = g1 , . . . , gm , not gm+1 , . . . , not gn ? (l)
where g1 , . . . , gn are ground fluents and l  0 is the plan length. For instance, the goal query
across(joe), across(jack), across(william), across(averell)? (5)

asks for plans which bring all four persons across in 5 steps.
Plans are defined using a transition-based semantics, where the execution of a set of actions
transforms a current state into a new state. An (optimistic) plan for P is a sequence P = hA 1 , . . . , Al i
of sets of action instances A1 , A2 , . . . , Al in a trajectory T = hhs0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,
hsl1 , Al , sl ii from a legal initial state s0 to state sl in which all literals of the goal are true. That
is, starting in s0 , the legal transition t1 = hs0 , A1 , s1 i, modeling the execution of the actions in A 1
(which must be executable), transforms s 0 into the state s1 . This is then followed by legal transitions
ti = hsi1 , Ai , si i, for i = 2, 3, . . . , l (cf. Appendix for details). A plan is sequential, if |A i |  1
for all i = 1, . . . , l, i.e., each step consists of at most one action; such plans can be enforced by
including the keyword noConcurrency.
Besides optimistic plans, in K we also support stronger secure (or conformant) plans. A secure
plan must be guaranteed to work out under all circumstances (Eiter et al., 2000b), regardless of
incomplete information about the initial state and possible nondeterminism in the action effects.
29

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

For better readability, in the following we will not always describe K planning problems P
strictly in terms of sets of declarations, rules and executability conditions, but optionally use the
more compact representation of K programs of the following general form:
fluents :
actions :
initially :
always :
goal :

FD
AD
IR
CR
q

where the (optional) sections fluents through always consist of lists of fluent declarations F D ,
action declarations AD , initial state constraints IR and executability conditions and causation rules
CR , respectively. Together with the background knowledge  and the goal query q, they specify
a K planning problem P = hh, hD, Rii, qi, where D is given by F D plus AD and R by IR plus
CR . 2
2.1 Solving the Bridge Crossing Problem
Using the above constructs, a K encoding of the Bridge Crossing Problem, assuming that joe
initially carries the lamp, is shown in Figure 1. There are simple five-step plans (l = 5), in which
joe always carries the lamp and brings all others across. One of them is:
P = h {crossTogether(joe, jack)}, {cross(joe)}, {crossTogether(joe, william)},
{cross(joe)}, {crossTogether(joe, averell)} i

3. Actions with Costs
Using the language K and the system prototype, DLV K , we can already express and solve some
involved planning tasks, cf. (Eiter et al., 2003b). However, K and DLV K alone offer no means
for finding optimal plans under an objective cost function. In general, different criteria of plan
optimality can be relevant, such as optimality wrt. action costs as shown in the next example, which
is a slight elaboration of the Bridge Crossing Problem, and a well-known brain teasing riddle:
Problem 2 [Quick Bridge Crossing Problem] The persons in the bridge crossing scenario need
different times to cross the bridge, namely 1, 2, 5, and 10 minutes, respectively. Walking in two
implies moving at the slower rate of both. Is it possible that all four persons get across within 17
minutes?
On first thought this is infeasible, since the seemingly optimal plan where joe, who is the fastest,
keeps the lamp and leads all the others across takes 19 minutes altogether. Surprisingly, as we will
see, the optimal solution indeed only takes 17 minutes.
In order to allow for an elegant and convenient encoding of such optimization problems, we
extend K to the language K c in which one can assign costs to actions.
3.1 Syntax of Kc
Let  act ,  f l , and  var denote (finite) sets of action names, fluent names and variable symbols.
Furthermore, let Lact , Lf l , and Ltyp denote the sets of action, fluent, and type literals, respectively,
2. This is also the format of the input files of our system prototype, which will be presented in Section 7.

30

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

actions :

cross(X) requires person(X).
crossTogether(X, Y) requires person(X), person(Y), X < Y.
takeLamp(X) requires person(X).

fluents :

across(X) requires person(X).
differentSides(X, Y) requires person(X), person(Y).
hasLamp(X) requires person(X).

initially : -across(X). hasLamp(joe).
always :

executable crossTogether(X, Y) if hasLamp(X).
executable crossTogether(X, Y) if hasLamp(Y).
nonexecutable crossTogether(X, Y) if differentSides(X, Y).
executable cross(X) if hasLamp(X).
executable takeLamp(X).
nonexecutable takeLamp(X) if hasLamp(Y), differentSides(X, Y).
caused
caused
caused
caused

across(X) after crossTogether(X, Y),
across(Y) after crossTogether(X, Y),
-across(X) after crossTogether(X, Y),
-across(Y) after crossTogether(X, Y),

-across(X).
-across(Y).
across(X).
across(Y).

caused across(X) after cross(X), -across(X).
caused -across(X) after cross(X), across(X).
caused hasLamp(X) after takeLamp(X).
caused -hasLamp(X) after takeLamp(Y), X != Y, hasLamp(X).
caused differentSides(X, Y) if across(X), -across(Y).
caused differentSides(X, Y) if -across(X), across(Y).
inertial across(X).
inertial -across(X).
inertial hasLamp(X).
noConcurrency.
goal :

across(joe), across(jack), across(william), across(averell)? (l)

Figure 1: K encoding of the Bridge Crossing Problem
formed from the action names, fluent names, and predicates in the background knowledge (including
built-in predicates), respectively, using terms from a nonempty (finite) set of constants con .
Kc extends action declarations as in K with costs as follows.
Definition 3.1 An action declaration d in K c is of the form:
p(X1 , . . . , Xn ) requires t1 , . . . , tm costs C where c1 , . . . , ck .

(1)

where (1) p   act has arity n  0, (2) X1 , . . . , Xn   var , (3) t1 , . . . , tm , c1 , . . . , ck are from
Ltyp such that every Xi occurs in t1 , . . . , tm , (4) C is either an integer constant, a variable from the
set of all variables occurring in t1 , . . . , tm , c1 , . . . , ck (denoted by  var (d)), or the distinguished
variable time, (5)  var (d)   var  {time}, and (6) time does not occur in t 1 , . . . tm .
31

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

If m = 0, the keyword requires is omitted; if k = 0, the keyword where is omitted and
costs C is optional. Here, (1) and (2) state that parameters to an action must be variables, and
not fixed values. Informally, (3) means that all parameters of an action must be typed in the
requires part. Condition (4) asserts that the cost is locally defined or given by the stage of the
plan, which is referenced through the global variable time. Conditions (5) and (6) ensure that all
variables are known and that type information of action parameters is static, i.e., does not depend
on time.
Planning domains and planning problems in K c are defined as in K.
For example, in the elaborated Bridge Crossing Problem, the declaration of cross(X) can be
extended as follows: suppose a predicate walk(Person, Minutes) in the background knowledge
indicates that Person takes Minutes to cross. Then, we may simply declare
cross(X) requires person(X) costs WX where walk(X, WX).

3.2 Semantics of Kc
Semantically, Kc extends K by the cost values of actions at points in time. In any plan P =
hA1 , . . . , Al i, at step 1  i  l, the actions in Ai are executed to reach time point i.
A ground action p(x1 , . . . , xn ) is a legal action instance of an action declaration d wrt. a K c
planning domain PD = h, hD, Rii, if there exists some ground substitution  for  var (d) 
{time} such that Xi  = xi , for 1  i  n and {t1 , . . . , tm }  M , where M is the unique answer
set of the background knowledge . Any such  is called a witness substitution for p(x 1 , . . . , xn ).
Informally, an action instance is legal, if it satisfies the respective typing requirements. Action costs
are now formalized as follows.
Definition 3.2 Let a = p(x1 , . . . , xn ) be a legal action instance of a declaration d of the form (1)
and let  be a witness substitution for a. Then

if the costs part of d is empty;
 0,
cost (p(x1 , . . . , xn )) =
val(C), if {c1 , . . . , ck }  M ;

undefined otherwise.

where M is the unique answer set of  and val :  con  IN is defined as the integer value for
integer constants and 0 for all non-integer constants.
By reference to the variable time, it is possible to define time-dependent action costs; we shall consider an example in Section 5.2. Using cost  , we now introduce well-defined legal action instances
and define action cost values as follows.
Definition 3.3 A legal action instance a = p(x 1 , . . . , xn ) is well-defined iff it holds that (i) for any
time point i  1, there is some witness substitution  for a such that time = i and cost  (a) is an
integer, and (ii) cost (a) = cost0 (a) holds for any two witness substitutions ,  0 which coincide
on time and have defined costs. For any well-defined a, its unique cost at time point i  1 is given
by costi (a) = cost (a) where  is as in (i).
In this definition, condition (i) ensures that some cost value exists, which must be an integer,
and condition (ii) ensures that this value is unique, i.e., any two different witness substitutions  and
 0 for a evaluate the cost part to the same integer cost value.
32

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

An action declaration d is well-defined, if all its legal instances are well-defined. This will be
fulfilled if, in database terms, the variables X1 , . . . , Xn together with time in (1) functionally determine the value of C. In our framework, the semantics of a K c planning domain PD = h, hD, Rii
is only well-defined for well-defined action declarations in PD. In the rest of this paper, we assume
well-definedness of Kc unless stated otherwise.
Using costi , we now define costs of plans.
Definition 3.4 Let P = hPD, Q ? (l)i be a planning problem. Then, for any plan P = hA 1 , . . . , Al i
for P, its cost is defined as
costP (P ) =

Pl

j=1

P


cost
(a)
.
j
aAj

A plan P is optimal for P, if costP (P )  costP (P 0 ) for each plan P 0 for P, i.e., P has least cost
among all plans for P. The cost of a planning problem P, denoted cost P , is given by costP =
costP (P  ), where P  is an optimal plan for P.
In particular, costP (P ) = 0 if P = hi, i.e., the plan is void. Note that cost P is only defined if a
plan for P exists.3
Usually one only can estimate some upper bound of the plan length, but does not know the exact
length of an optimal plan. Although we have only defined optimality for a fixed plan length l, we
will see in Section 5.1 that by appropriate encodings this can be extended to optimality for plans
with length at most l.
Besides optimal plans, also plans with bounded costs are of interest, which motivates the following definition.
Definition 3.5 A plan P for a planning problem P is admissible wrt. cost c, if cost P (P )  c.
Admissible plans impose a weaker condition on the plan quality than optimal plans. They are
particularly relevant if optimal costs are not a crucial issue, as long as the cost stays within a given
limit, and if optimal plans are difficult to compute. We might face questions like Can I make it
to the airport within one hour?, Do I have enough change to buy a coffee? etc. which amount
to admissible planning problems. As we shall see, computing admissible plans is complexity-wise
easier than computing optimal plans.
3.3 An Optimal Solution for the Quick Bridge Crossing Problem
To model the Quick Bridge Crossing Problem in K c , we first extend the background knowledge as
follows, where the predicate walk describes the time a person needs to cross and max determines
which of two persons is slower:
walk(joe, 1). walk(jack, 2). walk(william, 5). walk(averell, 10).
max(A, B, A) :- walk( , A), walk( , B), A >= B.
max(A, B, B) :- walk( , A), walk( , B), B > A.

Next, we modify the declarations for cross and crossTogether from Figure 1 by adding costs:
3. In the following, subscripts will be dropped when clear from the context.

33

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

cross(X) requires person(X) costs WX where walk(X, WX).
crossTogether(X, Y) requires person(X), person(Y), X < Y
costs Wmax where walk(X, WX), walk(Y, WY), max(WX, WY, Wmax).

The declaration of takeLamp remains unchanged, as the time to hand over the lamp is negligible.
Using this modified planning domain, the 5-step plan reported in Section 2.1 has cost 19. Actually, it is optimal for plan length l = 5. However, when we relinquish the first intuition that the
fastest person, joe, always has the lamp and consider the problem under varying plan length, then
we can find the following 7-step plan:
P = h {crossTogether(joe, jack)}, {cross(joe)}, {takeLamp(william)},
{crossTogether(william, averell)}, {takeLamp(jack)}, {cross(jack)},
{crossTogether(joe, jack)} i

Here, costP (P ) = 17, and thus P is admissible with respect to cost 17. This means that the Quick
Bridge Crossing Problem has a positive answer. In fact, P has least cost over all plans of length
l = 7, and is thus an optimal 7-step plan. Moreover, P has also least cost over all plans that emerge
if we consider all plan lengths. Thus, P is an optimal solution for the Quick Bridge Crossing
Problem under arbitrary plan length.
3.4 Bridge Crossing under Incomplete Knowledge
The language K is well-suited to model problems which involve uncertainty such as incomplete
initial states or non-deterministic action effects at a qualitative level. The enriched language K c
gracefully extends to secure (conformant) plans as well, which must reach the goal under all circumstances (Eiter et al., 2000b, 2003b). More precisely, an optimistic plan hA 1 , . . . , An i is secure,
if it is applicable under any evolution of the system: starting from any legal initial state s 0 , the first
action set A1 (for plan length l  1) can always be executed (i.e., some legal transition hs 0 , A1 , s1 i
exists), and for every such possible state s 1 , the next action set A2 can be executed etc., and after
having performed all actions, the goal is always accomplished (cf. Appendix for a formal definition).
While secure plans inherit costs from optimistic plans, there are different possibilities to define
optimality of secure plans. We may consider a secure plan as optimal, if it has least cost either
 among all optimistic plans, or
 among all secure plans only.
In the first alternative, there might be planning problems which have secure plans, but no optimal
secure plans. For this reason, the second alternative appears to be more appropriate.
Definition 3.6 A secure plan P is optimal for a planning problem P, if it has least cost among all
secure plans for P, i.e., costP (P )  costP (P 0 ) for each secure plan P 0 for P. The secure cost of
P, denoted costsec (P), is costsec (P) = costP (P  ), where P  is any optimal secure plan for P.
The notion of admissible secure plans is defined analogously.
For example, assume that it is known that at least one person in the bridge scenario has a lamp,
but that neither the exact number of lamps nor the allocation of lamps to persons is known. If the
four desperate persons now ask for a plan which brings them safely across the bridge, we need a
(fast) secure plan that works under all possible initial situations. In K c , this can be modeled by
replacing the initially-part with the following declarations:
34

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

initially : total hasLamp(X).
caused false if -hasLamp(joe), -hasLamp(jack),
-hasLamp(william), -hasLamp(averell).

The first statement says that each person either has a lamp or not, and the second that at least
one of them must have a lamp. For a detailed discussion on the use of the total statement for
modeling incomplete knowledge and non-determinism we refer to (Eiter et al., 2003b).
As we can easily see, an optimal secure solution will take at least 17 minutes, since the original
case (where only joe has a lamp) is one of the possible initial situations, for which the cost of
an optimistic plan which is optimal over all plan lengths was 17. However, a secure plan which
is optimal over all plan lengths requires at least 8 steps now (but no higher cost): Different from
optimistic plans, we need one extra step at the beginning which makes sure that one of those who
walk first (above, joe and jack) has the lamp, which is effected by the proper takeLamp action.
An example of such a plan is the following which has cost 17:
P = h {takeLamp(joe)}, {crossTogether(joe, jack)}, {cross(joe)},
{takeLamp(william)}, {crossTogether(william, averell)}, {takeLamp(jack)},
{cross(jack)}, {crossTogether(joe, jack)} i

We can easily check that P works for every possible initial situation. Thus, it is an optimal (secure)
plan for plan length 8, and moreover also for arbitrary plan length.

4. Computational Complexity
In this section, we will address the computational complexity of K c , complementing similar results
for the language K (Eiter et al., 2003b).
4.1 Complexity Classes
We assume that the reader is familiar with the basic notions of complexity theory, such as P, NP,
problem reductions and completeness; see e.g. (Papadimitriou, 1994) and references therein. We
recall that the Polynomial Hierarchy (PH) contains the classes  P0 = P0 = P0 = P and Pi+1 =
P
P
NPi , Pi+1 = co-Pi+1 , Pi+1 = Pi , for i  0. In particular, P1 = NP and P2 = PNP .
Note that these classes contain decision problems (i.e., problems where the answer is yes or
no). While checking well-definedness and deciding plan existence are such problems, computing
a plan is a search problem, where for each problem instance I a (possibly empty) finite set S(I) of
solutions exists. To solve such a problem, a (possibly nondeterministic) algorithm must compute the
alternative solutions from this set in its computation branches, if S(I) is not empty. More precisely,
search problems are solved by transducers, i.e., Turing machines equipped with an output tape. If
the machine halts in an accepting state, then the contents of the output tape is the result of the
computation. Observe that a nondeterministic machine computes a (partial) multi-valued function.
As an analog to NP, the class NPMV contains those search problems where S(I) can be computed by a nondeterministic Turing machine in polynomial time; for a precise definition, see (SelP
man, 1994). In analogy to Pi+1 , by Pi+1 MV = NPMVi , i  0, we denote the generalization of
NPMV where the machine has access to a  Pi oracle.
Analogs to the classes P and Pi+1 , i  0, are given by the classes FP and F Pi+1 , i  0,
which contain the partial single-valued functions (that is, |S(I)|  1 for each problem instance
35

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

I) computable in polynomial time using no resp. a  Pi oracle. We say, abusing terminology, that
a search problem A is in FP (resp. FPi+1 ), if there is a partial (single-valued) function f  FP
(resp. f  FPi+1 ) such that f (I)  S(I) and f (I) is undefined iff S(I) = . For example,
computing a satisfying assignment for a propositional CNF (FSAT) and computing an optimal tour
in the Traveling Salesperson Problem (TSP) are in F P2 under this view, cf. (Papadimitriou, 1994).
A partial function f is polynomial-time reducible to another partial function g, if there are
polynomial-time computable functions h 1 and h2 such that f (I) = h2 (I, g(h1 (I))) for all I and
g(h1 (I)) is defined whenever f (I) is defined. Hardness and completeness are defined as usual.
4.2 Problem Setting
We will focus on the following questions:
Checking Well-Definedness: Decide whether a given action description is well-defined wrt. a
given planning domain PD, resp. whether a given planning domain PD is well-defined.
Admissible Planning: Decide whether for planning problem P an admissible (optimistic/secure)
plan exists wrt. a given cost value c, and find such a plan.
Optimal Planning: Find an optimal (optimistic/secure) plan for a given planning problem.
Notice that (Eiter et al., 2003b) focused on deciding the existence of optimistic/secure plans,
rather than on actually finding plans, and presented a detailed study of the complexity of this task
under various restrictions for ground (propositional) planning problems. In this paper, we confine
the discussion to the case of planning problems P = hPD, Q ? (l)i which look for polynomial length
plans, i.e., problems where the plan length l is bounded by some polynomial in the size of the input.
We shall consider here mainly ground (propositional) planning, and assume that the planning
domains are well-typed and that the unique model of the background knowledge can be computed
in polynomial time. In the general case, by well-known complexity results on logic programming,
cf. (Dantsin, Eiter, Gottlob, & Voronkov, 2001), already evaluating the background knowledge is
EXPTIME-hard, and the problems are thus provably intractable. We recall the following results,
which appear in (or directly follow from) previous work (Eiter et al., 2003b).
Proposition 4.1 Deciding, given a propositional planning problem P and a sequence P = hA 1 , . . . ,
Al i of action sets, (i) whether a given sequence T = ht 1 , . . . , tl i is a legal trajectory witnessing that
P is an optimistic plan for P is feasible in polynomial time, and (ii) whether P is a secure plan for
P is P2 -complete.
4.3 Results
We start by considering checking well-definedness. For this problem, it is interesting to investigate
the non-ground case, assuming that the background knowledge is already evaluated. This way we
can assess the intrinsic difficulty of this task obtaining the following result.
Theorem 4.2 (Complexity of checking well-definedness) Given a K c planning domain PD =
h, hD, Rii and the unique model M of , checking (i) well-definedness of a given action declaration d of form (1) wrt. PD and (ii) well-definedness of PD are both  P2 -complete.
36

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Proof. Membership: As for (i), d is violated if it has a nonempty costs part and a legal action
instance a = p(x1 , . . . , xn ) such that either (1) there exist witness substitutions  and  0 for a such
that time = time 0 , cost (a) = val(C) and cost0 (a) = val(C 0 ), and val(C) 6= val(C 0 ),
or (2) there is no witness substitution  for a such that cost  (a) = val(C) is an integer. Such
an a can be guessed and checked, via a witness substitution, in polynomial time, and along with
a also  and  0 as in (1); note that, by definition, all variables must be substituted by constants
from the background knowledge (including numbers), and so must be values for time if it occurs in
c1 , . . . , ck . Given a, we can decide (2) with the help of an NP oracle. In summary, disproving welldefinedness of d is nondeterministically possible in polynomial time with an NP oracle. Hence,
checking well-definedness of d is in co-P2 = P2 . The membership part of (ii) follows from (i),
since well-definedness of PD reduces to well-definedness of all action declarations in it, and P2 is
closed under conjunctions.
Hardness: We show hardness for (i) by a reduction from deciding whether a quantified Boolean
formula (QBF)
Q = XY.c1      ck
where each ci = Li,1      Li,`i , i = 1, . . . , k, is a disjunction of literals L i,j on the atoms
X = x1 , . . . , xn and Y = xn+1 . . . , xm , is true. Without loss of generality, we may assume that
each ci contains three (not necessarily distinct) literals, which are either all positive or all negative.
We construct a planning domain PD and d as follows. The background knowledge, , is
bool(0). bool(1).
pos(1, 0, 0). pos(0, 1, 0). pos(0, 0, 1). pos(1, 1, 0). pos(1, 0, 1). pos(0, 1, 1). pos(1, 1, 1).
neg(0, 0, 0). neg(1, 0, 0). neg(0, 1, 0). neg(0, 0, 1). neg(1, 1, 0). neg(1, 0, 1). neg(0, 1, 1).

Here, bool declares the truth values 0 and 1. The facts pos(X 1 , X2 , X3 ) and neg(X1 , X2 , X3 ) state
those truth assignments to X1 , X2 , and X3 such that the positive clause X1  X2  X3 resp. the
negative clause X1  X2  X3 is satisfied.
The rest of the planning domain PD consists of the single action declaration d of form
p(V1 , ..., Vn ) requires bool(V1), ..., bool(Vn) costs 0 where c1 , ..., ck .

where
ci

=



pos(Vi,1 , Vi,2 , Vi,3 ), if ci = xi,1  xi,2  xi,3 ,
neg(Vi,1 , Vi,2 , Vi,3 ), if ci = xi,1  xi,2  xi,3 ,

i = 1, . . . , k.

For example, the clause c = x1  x3  x6 is mapped to c = pos(V1 , V3 , V6 ). It is easy to see that
each legal action instance a = p(b1 , . . . , bn ) of d corresponds 1-1 to the truth assignment  a of X
given by a (xi ) = bi , for i = 1, . . . , n. Furthermore, a has a cost value defined (which is 0) iff the
formula Y (c1 a      ck a ) is true. Thus, d is well-defined wrt. PD iff Q is true. Since PD and
2
d are efficiently constructible, this proves P2 -hardness.
Observe that in the ground case, checking well-definedness is much easier. Since no substitutions need to be guessed, the test in the proof of Theorem 4.2 is polynomial. Thus, by our assumption
on the efficient evaluation of the background program, we obtain:
Corollary 4.3 In the ground (propositional) case, checking well-definedness of an action description d wrt. a Kc planning domain PD = h, hD, Rii, resp. of PD as a whole, is possible in
polynomial time.
37

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

We remark that checking well-definedness can be expressed as a planning task in K, and also
by a logic program; we refer to (Eiter, Faber, Leone, Pfeifer, & Polleres, 2002b) for details.
We now turn to computing admissible plans.
Theorem 4.4 (Complexity of admissible planning) For polynomial plan lengths, deciding whether
a given (well-defined) propositional planning problem hPD, qi has (i) some optimistic admissible
plan wrt. to a given integer b is NP-complete, and finding such a plan is complete for NPMV, (ii)
deciding whether hPD, qi has some secure admissible plan wrt. to a given integer b is  P3 -complete,
and computing such a plan is P3 MV-complete. Hardness holds in both cases for fixed plan length.
As for the proof we refer to the Appendix. We finally address the complexity of computing
optimal plans.
Theorem 4.5 (Complexity of optimal planning) For polynomial plan lengths, (i) computing an
optimal optimistic plan for hPD, Q ? (l)i in K c is FP2 -complete, and (ii) computing an optimal
secure plan for hPD, Q ? (l)i in K c is FP4 -complete. Hardness holds in both cases even if the plan
length l is fixed.
The proof again can be found in the in the Appendix.
We remark that in the case of unbounded plan length, the complexity of computing plans increases and requires (at least) exponential time in general, since plans might have exponential length
in the size of the planning problem. Thus, in practical terms, constructing such plans is infeasible,
since they occupy exponential space. Furthermore, as follows from previous results (Eiter et al.,
2003b), deciding the existence of an admissible optimistic resp. secure plan for a planning problem wrt. a given cost is PSPACE-complete resp. NEXPTIME-complete. We leave a more detailed
analysis of complexity aspects of K c for further work.

5. Applications
5.1 Cost Efficient versus Time Efficient Plans
In this section, we show how the language K c can be used to minimize plan length in combination
with minimizing the costs of a plan. This is especially interesting in problem settings where parallel
actions are allowed (cf. (Kautz & Walser, 1999; Lee & Lifschitz, 2001)).
For such domains with parallel actions, Kautz and Walser propose various criteria to be optimized, for instance the number of actions needed, or the number of necessary time steps when
parallel actions are allowed, as well as combinations of these two criteria (1999). By exploiting
action costs and proper modeling, we can solve optimization problems of this sort. For example,
we can single out plans with a minimal number of actions simply by assigning cost 1 to all possible
actions.
We consider the following optimization problems:
() Find a plan with minimal cost (cheapest plan) for a given number of steps.
() Find a plan with minimal time steps (shortest plan).
() Find a shortest among the cheapest plans.
38

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

() Find a cheapest among the shortest plans.
Problem () is what we have already defined as optimal plans so far. We will now show how to
express () in terms of optimal cost plans as well, and how to extend this elaboration with respect
to the combinations () and ().
5.1.1 C HEAPEST P LANS

WITH

G IVEN P LAN L ENGTH ()

As a guiding example, we refer to Blocks World with parallel moves allowed, where apart from
finding shortest plans also minimizing the total number of moves is an issue. A Kc encoding for this
domain, where plans are serializable, is shown in Figure 2. Serializability here means that parallel
actions are non-interfering and can be executed sequentially in any order, i.e. the parallel plan can
be arbitrarily unfolded to a sequential plan.
fluents :

on(B, L) requires block(B), location(L).
blocked(B) requires block(B).
moved(B) requires block(B).

actions :

move(B, L) requires block(B), location(L) costs 1.

always :

executable move(B, L) if B != L.
nonexecutable move(B, L) if blocked(B).
nonexecutable move(B, L) if blocked(L).
nonexecutable move(B, L) if move(B1, L), B < B1, block(L).
nonexecutable move(B, L) if move(B, L1), L < L1.
nonexecutable move(B, B1) if move(B1, L).
caused
caused
caused
caused

on(B, L) after move(B, L).
blocked(B) if on(B1, B).
moved(B) after move(B, L).
on(B, L) if not moved(B) after on(B, L).

Figure 2: Kc encoding for the Blocks World domain
The planning problem emerging from the initial state and the goal state depicted in Figure 3 can
be modeled using the background knowledge  bw :
block(1). block(2). block(3). block(4). block(5). block(6).
location(table).
location(B) :- block(B).

and extending the program in Figure 2 as follows:
initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).
goal :

on(1, 3), on(3, table), on(2, 4), on(4, table), on(6, 5), on(5, table) ?(l)

1
2

3
4

1
3

5
6

2
4

6
5

Figure 3: A simple Blocks World instance
39

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Each move is penalized with cost 1, which results in a minimization of the total number of moves.
Let Pl denote the planning problem for plan length l.
For l = 2, we have an optimal plan which involves six moves, i.e. cost P2 = 6:
P2 = h {move(1, table), move(3, table), move(5, table)}, {move(1, 3), move(2, 4), move(6, 5)} i

By unfolding the steps, this plan gives rise to similar plans of length l = 3, . . . , 6 that have cost 6.
For l = 3, we can find among others the following optimal plan, which has cost 5:
P3 = h {move(3, table)}, {move(1, 3), move(5, table)}, {move(2, 4), move(6, 5)} i

This plan can not be further parallelized to having only two steps. For any plan length l > 3, we
will obtain optimal plans similar to P 3 , extended by void steps. Thus a plan which is cheapest over
all plan lengths has cost 5 and needs three steps. Note that shortest parallel plans (of length 2) are
more expensive, as explained above.
5.1.2 S HORTEST P LANS ()
Intuitively, it should be possible to include the minimization of time steps in the cost function. We
describe a preprocessing method which, given a K planning domain PD, a list Q of ground literals,
and an upper bound i  0 for the plan length, generates a planning problem P  (PD, Q, i) such that
the optimal plans for P correspond to shortest plans which reach Q in PD in at most i steps, i.e.,
to plans for hPD, Q ? (l)i such that l  i is minimal. We assume that no action costs are specified
in the original planning domain PD, and minimizing time steps is our only target.
First we rewrite the planning domain PD to PD  as follows: We introduce a new distinct fluent
gr and a new distinct action finish, defined as follows:
fluents :
actions :

gr.
finish costs time.

Intuitively, the action finish represents a final action, which we use to finish the plan. The later
this action occurs, the more expensive the plan as we assign time as cost. The fluent gr (goal
reached) shall be true and remain true as soon as the goal has been reached, and it is triggered by
the finish action.
This can be modeled in K c by adding the following statements to the always section of the
program:
executable finish if Q, not gr.
caused gr after finish.
caused gr after gr.

Furthermore, we want finish to occur exclusively and we want to block the occurrence of any
other action once the goal has been reached. Therefore, for every action A in PD, we add
nonexecutable A if finish.

and add not gr to the if-part of each executability condition for A. Finally, to avoid any inconsistencies from static or dynamic effects as soon as the goal has been reached, we add not gr to the
if part of any causation rule of the PD except nonexecutable rules which remain unchanged. 4
We define now P (PD, Q, i) = hPD , gr ?(i + 1)i. We take i + 1 as the plan length since we
need one additional step to execute the finish action.
4. There is no need to rewrite nonexecutable rules because the respective actions are already switched off by
rewriting the executability conditions.

40

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

By construction, it is easy to see that any optimal plan P = hA 1 , . . . , Aj , Aj+1 , . . . , Ai+1 i for
the planning problem P must have Aj+1 = {finish} and Aj+2 = . . . = Ai+1 =  for some
j  {0, . . . , i}. We thus have the following desired property.
Proposition 5.1 The optimal plans for P  are in 1-1 correspondence to the shortest plans reaching Q in PD. More precisely, P = hA1 , . . . , Aj+1 , , . . . , i is an optimal optimistic plan for
P (PD, Q, i) and Aj+1 = {finish} if and only if P 0 = hA1 , . . . , Aj i is an optimistic plan for
hPD, Q ? (j)i where j  {0, . . . , i}, and hPD, Q ? (j 0 )i has no optimistic plan for each j 0 < j.
In our Blocks World example, using this method we get all 2-step plans, if we choose i  2.
To compute shortest plans over all plan lengths, we can set the upper bound i large enough such
that plans of length l  i are guaranteed to exist. A trivial such bound is the total number of legal
states which is in general exponential in the number of fluents.
However, many typical applications have an inherent, much smaller bound on the plan length.
For instance, in a Blocks World with n blocks, any goal configuration can be reached within at most
2n  sinit  sgoal steps, where sinit and sgoal are the numbers of stacks in the initial and the goal
state, respectively.5 Therefore, 6 is an upper bound for the plan length of our simple instance.
We remark that this approach for minimizing plan length is only efficient if an upper bound
close to the optimum is known. Searching for a minimum length plan by iteratively increasing the
plan length may be much more efficient if no such bound is known, since a weak upper bound can
lead to an explosion of the search space (cf. the benchmarks in Section 7.2).
5.1.3 S HORTEST

AMONG THE

C HEAPEST P LANS ()

In the previous subsection, we have shown how to calculate shortest plans for K programs without
action costs. Combining arbitrary K c programs and the rewriting method described there is easy.
If we want to find a shortest among the cheapest plans, we can use the same rewriting, with just a
little change. All we have to do is setting the costs of all actions except finish at least as high as
the highest possible cost of the finish action. This is is obviously the plan length i + 1. So, we
simply modify all action declarations
A requires B costs C where D.

in P by multiplying the costs with factor i + 1:
A requires B costs C1 where C1 = (i + 1)  C, D.

This lets all other action costs take priority over the cost of finish and we can compute plans
satisfying criterion (). Let P denote the resultant planning problem. Then we have:
Proposition 5.2 The optimal plans for P  are in 1-1 correspondence to the shortest among the
cheapest plans reaching Q in PD within i steps. More precisely, P = hA 1 , . . . , Aj+1 , , . . . , i
is an optimal optimistic plan for P (PD, Q, i) and Aj+1 = {finish} if and only if (i) P 0 =
hA1 , . . . , Aj i is a plan for Pj = hPD, Q ? (j)i, where j  {0, . . . , i}, and (ii) if P 00 = hA1 , . . . , Aj 0 i
is any plan for Pj 0 = hPD, Q ? (j 0 )i where j 0  i, then either costPj 0 (P 00 ) > costPj (P 0 ) or
costPj 0 (P 00 ) = costPj (P 0 ) and j 0  j.
Figure 4 shows P for our Blocks World instance where i = 6. One optimal plan for P  is
5. One can solve any Blocks World problem sequentially by first unstacking all blocks which are not on the table
(n  sinit steps) and then building up the goal configuration (n  sgoal steps).

41

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

fluents :

on(B, L) requires block(B), location(L).
blocked(B) requires block(B).
moved(B) requires block(B).
gr.

actions :

move(B, L) requires block(B), location(L) costs C where C = 7  1.
finish costs time.

always :

executable move(B, L) if B != L, not gr.
nonexecutable move(B, L) if blocked(B).
nonexecutable move(B, L) if blocked(L).
nonexecutable move(B, L) if move(B1, L), B < B1, block(L).
nonexecutable move(B, L) if move(B, L1), L < L1.
nonexecutable move(B, B1) if move(B1, L).
caused
caused
caused
caused

on(B, L) if not gr after move(B, L).
blocked(B) if on(B1, B), not gr.
moved(B) if not gr after move(B, L).
on(B, L) if not moved(B), not gr after on(B, L).

executable finish if on(1, 3), on(3, table), on(2, 4), on(4, table),
on(6, 5), on(5, table), not gr.
caused gr after finish.
caused gr after gr.
nonexecutable move(B, L) if finish.
initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).
goal :

gr? (7)

Figure 4: Computing the shortest plan for a Blocks World instance with a minimum number of
actions

P = h {move(3, table)}, {move(1, 3), move(5, table)},
{move(2, 4), move(6, 5)}, {finish}, , ,  i,

which has costP (P ) = 39. We can now compute the optimal cost wrt. optimization () by subtracting the cost of finish and dividing by i + 1: (39  4)  (i + 1) = 35  7 = 5. Thus, we
need a minimum of 5 moves to reach the goal. The minimal number of steps is obviously all steps,
except the final finish action, i.e. 3. Thus, we need at least 3 steps for a plan with five moves.
5.1.4 C HEAPEST

AMONG THE

S HORTEST P LANS ()

Again, we can use the rewriting for optimization (). The cost functions have to be adapted similarly
as in the previous subsection, such that now the cost of the action finish takes priority over all other
actions costs. To this end, it is sufficient to set the cost of finish high enough, which is achieved
by multiplying it with a factor F higher than the sum of all action costs of all legal action instances
at all steps j = 1, . . . , i + 1. Let P denote the resulting planning problem. We have:
Proposition 5.3 The optimal plans for P  are in 1-1 correspondence to the cheapest among the
shortest plans reaching Q in PD within i steps. More precisely, P = hA 1 , . . . , Aj+1 , , . . . , i
42

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

is an optimal optimistic plan for P (PD, Q, i) and Aj+1 = {finish} if and only if (i) P 0 =
hA1 , . . . , Aj i is a plan for Pj = hPD, Q ? (j)i, where j  {0, . . . , i}, and (ii) if P 00 = hA1 , . . . , Aj 0 i
is any plan for Pj 0 = hPD, Q ? (j 0 )i where j 0  i, then either j 0 > j, or j 0 = j and costPj 0 (P 00 ) 
costPj (P 0 ).
In our example, there are 36 possible moves. Thus, we could take F = 36  (i + 1) and
would set the costs of finish to time  36  (i + 1). However, we only need to take into account
those actions which can actually occur simultaneously. In our example, at most six blocks can
be moved in parallel. Therefore, it is sufficient to set F = 6  (i + 1) and assign finish cost
time  F = time  42. Accordingly, the action declarations are modified as follows:
actions :

move(B, L) requires block(B), location(L) costs 1.
finish costs C where C = time  42.

An optimal plan for the modified planning problem P is:
P = h {move(1, table), move(3, table), move(5, table)},
{move(1, 3), move(2, 4), move(6, 5)}, {finish}, , , , i

We have costP (P ) = 132. Here, we can compute the optimal cost wrt. optimization () by simply
subtracting the cost of finish, i.e. 132  3  42 = 6, since finish occurs at time point 3.
Consequently, we need a minimum of 6 moves for a shortest plan, which has length 3  1 = 2.
And indeed, we have seen that (and how) the optimization problems () through () can be
represented in Kc . We remark that the transformations P  , P , and P all work under the restrictions
to secure and/or sequential plans as well.
5.2 Traveling Salesperson
As another illustrating example for optimal cost planning, we will now introduce some elaboration
of the Traveling Salesperson Problem.
Traveling Salesperson Problem (TSP). We start with the classical Traveling Salesperson Problem (TSP), where we have a given set of cities and connections (e.g., roads, airways) of certain costs.
We want to know a most economical round trip which visits all cities exactly once and returns to
the starting point (if such a tour exists). Figure 5 shows an instance representing the capitals of all
Austrian provinces. The dashed line is a flight connection, while all other connections are roads;
each connection is marked with the costs in traveling hours.
brg ... Bregenz
eis ... Eisenstadt
gra ... Graz
ibk ... Innsbruck
kla ... Klagenfurt
lin ... Linz
sbg ... Salzburg
stp ... St. Plten
vie ... Vienna

lin
sbg

2

2

ibk

2

5

vie
1

2
3
2

2

2
kla

Figure 5: TSP in Austria

eis

2

3

43

2
stp 1

1

1
brg

1

gra

1

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

We represent this in Kc as follows. The background knowledge  T SP defines two predicates
city(C) and conn(F, T, C) representing the cities and their connections with associated costs. Connections can be traveled in both ways:
conn(brg, ibk, 2). conn(ibk, sbg, 2). conn(ibk, vie, 5). conn(ibk, kla, 3).
conn(sbg, kla, 2). conn(sbg, gra, 2). conn(sbg, lin, 1). conn(sbg, vie, 3).
conn(kla, gra, 2). conn(lin, stp, 1). conn(lin, vie, 2). conn(lin, gra, 2).
conn(gra, vie, 2). conn(gra, eis, 1). conn(stp, vie, 1). conn(eis, vie, 1).
conn(stp, eis, 2). conn(vie, brg, 1).
conn(B, A, C) :- conn(A, B, C).
city(T) :- conn(T, , ).

A possible encoding of TSP starting in Vienna (vie) is the K c program in Figure 6. It includes two
actions for traveling from one city to another and for directly returning to the starting point at the
end of the round trip as soon as all cities have been visited.
actions :

travel(X, Y) requires conn(X, Y, C) costs C.
return from(X) requires conn(X, vie, C) costs C.

fluents :

unvisited. end.
in(C) requires city(C).
visited(C) requires city(C).

always :

executable travel(X, Y) if in(X).
nonexecutable travel(X, Y) if visited(Y).
executable return from(X) if in(X).
nonexecutable return from(X) if unvisited.
caused unvisited if city(C), not visited(C).
caused end after return from(X).
caused in(Y) after travel(X, Y).
caused visited(C) if in(C).
inertial visited(C).

noConcurrency.
initially : in(vie).
goal :
end? (9)

Figure 6: Traveling Salesperson
The problem has ten optimal 9-step solutions with cost 15. We show only the first five here, as the
others are symmetrical:
P1 = h {travel(vie, stp)},
{travel(lin, sbg)},
{return from(brg)}
P2 = h {travel(vie, eis)},
{travel(sbg, gra)},
{return from(brg)}
P3 = h {travel(vie, eis)},
{travel(gra, kla)},
{return from(brg)}
P4 = h {travel(vie, lin)},
{travel(gra, kla)},

{travel(stp, eis)},
{travel(sbg, kla)},
i
{travel(eis, stp)},
{travel(gra, kla)},
i
{travel(eis, stp)},
{travel(kla, sbg)},
i
{travel(lin, stp)},
{travel(kla, sbg)},

{travel(eis, gra)}, {travel(gra, lin)},
{travel(kla, ibk)}, {travel(ibk, brg)},
{travel(stp, lin)}, {travel(lin, sbg)},
{travel(kla, ibk)}, {travel(ibk, brg)},
{travel(stp, lin)}, {travel(lin, gra)},
{travel(sbg, ibk)}, {travel(ibk, brg)},
{travel(stp, eis)}, {travel(eis, gra)},
{travel(sbg, ibk)}, {travel(ibk, brg)},

44

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

{return from(brg)}
P5 = h {travel(vie, gra)},
{travel(lin, sbg)},
{return from(brg)}

i
{travel(gra, eis)}, {travel(eis, stp)}, {travel(stp, lin)},
{travel(sbg, kla)}, {travel(kla, ibk)}, {travel(ibk, brg)},
i

TSP with variable costs. Let us now consider an elaboration of TSP, where we assume that
the costs of traveling different connections may change during the trip. Note that three of the
five solutions in our example above include traveling from St.Polten to Eisenstadt or vice versa on
the second day. Let us now assume that the salesperson, who starts on Monday, has to face some
exceptions which might increase the cost of the trip. For instance, (i) heavy traffic jams are expected
on Tuesdays on the route from St.Polten to Eisenstadt or (ii) the salesperson shall not use the flight
connection between Vienna and Bregenz on Mondays as only expensive business class tickets are
available on this connection in the beginning of the week. So we have to deal with different costs
for the respective connections depending on the particular day.
To this end, we first add to the background knowledge T SP a new predicate cost(A, B, W, C)
representing the cost C of traveling connection A to B on weekday W which can take exceptional
costs into account:
cost(A, B, W, C) :- conn(A, B, C), #int(W), 0 < W, W <= 7, not ecost(A, B, W).
ecost(A, B, W) :- conn(A, B, C), cost(A, B, W, C1), C != C1.

The original costs in the predicate conn(A, B, C) now represent defaults, which can be overridden
by explicitly adding different costs. For instance, to represent the exceptions (i) and (ii), we add:
cost(stp, eis, 2, 10). cost(vie, brg, 1, 10).

setting the exceptional costs for these two critical connections to 10. Weekdays are coded by integers
from 1 (Monday) to 7 (Sunday). We represent a mapping from time steps to the weekdays by the
following rules which we also add to  T SP :
weekday(1, 1).
weekday(D, W) :- D = D1 + 1, W = W1 + 1, weekday(D1, W1), W1 < 7.
weekday(D, 1) :- D = D1 + 1, weekday(D1, 7).

Note that although the modified background knowledge T SP is not stratified (since cost is defined
by cyclic negation), it has a total well-founded model, and thus a unique answer set.
Finally, we change the costs of traveling and returning in the K c program from Figure 6:
actions :

travel(X, Y) requires conn(X, Y, C1) costs C
where weekday(time, W), cost(X, Y, W, C).
return from(X) requires conn(X, vie, C1) costs C
where weekday(time, W), cost(X, vie, W, C).

Since now the costs for P1 (which includes traveling from St.Polten to Eisenstadt) on the second
day have increased due to exception (i), only four of the plans from above remain optimal. Note
that unlike the default costs, exceptional costs do not apply bidirectionally, so the exception does
not affect P2 and P3 . Furthermore, due to exception (ii) the symmetrical round trips starting with
the flight trips to Bregenz are no longer optimal.
The presented encoding proves to be very flexible, as it allows for adding arbitrary exceptions
for any connection on any weekday by simply adding the respective facts; moreover, even more
involved scenarios, where exceptions are defined by rules, can be modeled.
45

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

5.3 A Small Example for Planning under Resource Restrictions
Although planning with resources is not the main target of our approach, the following encoding
shows that action costs can also be used in order to model optimization of resource consumption in
some cases. An important resource in real world planning is money. For instance, let us consider a
problem about buying and selling (Lee & Lifschitz, 2001):
I have $6 in my pocket. A newspaper costs $1 and a magazine costs $3. Do I have
enough money to buy one newspaper and two magazines?
In Kc , this can be encoded in a very compact way by the following background facts:
item(newspaper, 1). item(magazine, 2).

combined with the following short K c program:
actions :

buy(Item, Number) requires item(Item, Price), #int(Number)
costs C where C = Number  Price.

fluents :

have(Item, Number) requires item(Item, Price), #int(Number).

always :

executable buy(Item, Number).
nonexecutable buy(Item, N1) if buy(Item, N2), N1 < N2.
caused have(Item, Number) after buy(Item, Number).

goal :

have(newspaper, 1), have(magazines, 2) ? (1)

The action buy is always executable, but one must not buy two different amounts of a certain
item at once. Obviously, no admissible plan wrt. cost 6 exists, as the optimal plan for this problem,
h{buy(newspaper, 1), buy(magazine, 2)} i has cost P = 7. Therefore, the answer to the problem
is no.
Our approach considers only positive action costs and does not directly allow modeling full
consumer/producer/provider relations on resources in general, in favor of a clear non-ambiguous
definition of optimality. For instance, by allowing negative costs one could always add a producer
action to make an existing plan cheaper, whereas in our approach costs are guaranteed to increase
monotonically, allowing for a clear definition of plan costs and optimality.
On the other hand, we can encode various kinds of resource restrictions by using fluents to represent these resources. We can then model production/consumption as action effects on these fluents
and add restrictions as constraints. This allows us to model even complex resource or scheduling
problems; optimization, however, remains restricted to action costs.

6. Transformation to Logic Programming
In this section, we describe how planning under action costs can be implemented by means of a
transformation to answer set programming. It extends our previous transformation (Eiter et al.,
2003a), which maps ordinary K planning problems to disjunctive logic programs under the answer
set semantics (Gelfond & Lifschitz, 1991), and takes advantage of weak constraints, cf. (Buccafurri,
Leone, & Rullo, 1997, 2000), as implemented in the DLV system (Faber & Pfeifer, 1996; Eiter,
Faber, Leone, & Pfeifer, 2000a). In addition, we show how this translation can be adapted to the
language of Smodels (Simons, Niemela, & Soininen, 2002).
6.1 Disjunctive Logic Programs with Weak Constraints
First, we give a brief review of disjunctive logic programs with weak constraints.
46

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Syntax

A disjunctive rule (for short, rule) R is a construct
a1 v    v an :- b1 ,    , bk , not bk+1 ,    , not bm .

(2)

where all ai and bj are classical literals over a function-free first-order alphabet, and n  0, m 
k  0. The part left (resp. right) of :- is the head (resp. body) of R, where :- is omitted if
m = 0. We let H(R) = {a1 , . . ., an } be the set of head literals and B(R) = B + (R)  B  (R)
the set of body literals, where B + (R) = {b1 ,. . . , bk } and B  (R) = {bk+1 , . . . , bm }. A (strong)
constraint is a rule with empty head (n = 0).
A weak constraint is a construct
: b1 ,    , bk , not bk+1 ,    , not bm . [w :]

(3)

where w is an integer constant or a variable occurring in b 1 , . . . , bk and all bi are classical literals.6
B(R) is defined as for (2).
A disjunctive logic program (DLPw ) (simply, program) is a finite set of rules, constraints and
weak constraints; here, superscript w indicates the potential presence of weak constraints.
Semantics The answer sets of a program  without weak constraints are defined as usual (Gelfond & Lifschitz, 1991; Lifschitz, 1996). There is one difference, though: We do not consider
inconsistent answer sets. The answer sets of a program  with weak constraints are defined by
selection from the answer sets S of the weak-constraint free part  0 of  as optimal answer sets.
A weak constraint c of form (3) is violated, if it has an instance for which its conjunction is
satisfied with respect to the candidate answer set S, i.e., there exists a substitution mapping  from
the variables in c to the Herbrand base of  such that {b 1 ,    , bk }  S and {bk+1 ,    , bm }
M = ; we then call w the violation value of c wrt. . 7 The violation cost of c wrt. S, denoted
costc (S), is the sum of all violation values over all violating substitutions for c wrt. S; the cost of
S, denoted cost (S), is then
X
cost (S) =
costc (S),
c  weak constraints of 

i.e., the sum of violation costs of weak constraints in  wrt. S. An answer set M of  is now
selected (called an optimal answer set), if cost  (M ) is minimal over all answer sets of .
From (Buccafurri et al., 2000) we know that given a head-cycle-free disjunctive program, deciding whether a query q is true in some optimal answer set is  P2 -complete. The respective class for
computing such an answer set is FP2 -complete. Together with the results from Section 4 this indicates that translations of optimal planning problems to head-cycle-free disjunctive logic programs
with weak constraints or the language of Smodels are feasible in polynomial time.
6.2 Translating Kc to DLPw
We extend our original transformation lp(P), which naturally maps a K planning problem P into a
weak-constraint free program (Eiter et al., 2003a), to a new translation lp w (P), such that the optimal
answer sets of lpw (P) correspond to the optimal cost plans for the K c planning problem P.
6. The colon in [w :] stems from the DLV language, which allows to specify a priority layer after the colon. We do not
need priority layers in our translation, but stick to the DLV syntax.
7. A weak constraint c is only admissible, if all possible violation values in all candidate answer sets S are integers.
Thus, if w is a variable, then  must guarantee that w can only be bound to an integer.

47

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Basically, in lp(P) fluent and action literals are extended by an additional time parameter, and
executability conditions as well as causations rules are modularly translated (rule by rule) into corresponding program rules and constraints; disjunction is used for guessing the actions which should
be executed in the plan at each point in time.
6.2.1 R EVIEW

OF THE

T RANSLATION lp(P)

The basic steps of the translation from K programs to logic programs are as follows (cf. (Eiter et al.,
2003a) for details):
Step 0 (Macro Expansion):

First, replace all macros in the K program by their definitions.

Step 1 (Background Knowledge): The background knowledge  of P is already given as a logic
program and is included in lp(P), without further modification.
Step 2 (Auxiliary Predicates):

To represent steps, we add the following facts to lp(P)

time(0)., . . . , time(l). next(0, 1)., . . . , next(l  1, l).

where l is the plan length of the query q = G?(l) in P at hand.
Step 3 (Causation Rules): Causation rules are mapped to rules in lp(P) by adding type information and extending fluents and actions with a time stamp using time and next. For example,
caused across(X) after cross(X), -across(X).

leads to rule across(X, T1) :- cross(X, T0), -across(X, T0), person(X), next(T0, T1 ).
in lp(P) where T1 , T0 are new variables. Here, type information person(X) for across(X), and
-across(X), taken from the type declaration, is added, which helps to avoid unsafe logic programming rules.
Step 4 (Executability Conditions): Similarly, each executability condition is translated to a disjunctive rule guessing whether an action occurs at a certain time step. In our running example,
executable cross(X) if hasLamp(X).

becomes cross(X, T0)  -cross(X, T0) :- hasLamp(X, T0), person(X), next(T0, T1 ).
which encodes a guess whether at time point T 0 action cross(X) should happen; again, type information person(X) is added as well as next(T 0 , T1 ) to ensure that T0 is not the last time point.
Step 5 (Initial State Constraints): Initial state constraints are transformed like static causation
rules in Step 3, but using the constant 0 instead of the variable T 1 and thus need no auxiliary predicate for the time stamp. For instance,
initially : caused -across(X).

becomes, by again adding the type information -across(X, 0) :- person(X).
Step 6 (Goal Query):
goal :

Finally, the query q:

g1 (t1 ), . . . , gm (tm ), not gm+1 (tm+1 ), . . . , not gn (tn ) ? (l).

is translated as follows, where goal reached is a new 0-ary predicate symbol:
goal reached :- g1 (t1 , l), . . . , gm (tm , l), not gm+1 (tm+1 , l), . . . , not gn (tn , l).
:- not goal reached.
48

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

6.2.2 E XTENDING

THE

T RANSLATION

TO

ACTION C OSTS

The extended translation lpw (P) for a Kc problem P first includes all rules of lp(Pnc ), where Pnc
results from P by stripping off all cost parts. Furthermore, the following step is added:
Step 7 (Action Costs):

For any action declaration d of form (1) with a nonempty costs-part, add:

(i) A new rule rd of the form

costp (X1 , . . . , Xn , T, C) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,
c1 , . . . , ck , U = T + 1.

(4)

where costp is a new symbol, T and U are new variables and  = {time  U}. As an optimization,
U = T + 1 is only present if U occurs elsewhere in r d .
: costp (X1 , . . . , Xn , T, C). [C :]

(ii) A weak constraint wcd of the form

(5)

For example, the cross action from the Quick Bridge Crossing Problem is translated to
costcross(X, T, WX):- cross(X, T), person(X), walk(X, WX).
: costcross(X, T, WX). [WX :]

As we showed in previous work (Eiter et al., 2003a), the answer sets of lp(P) correspond to
trajectories of optimistic plans for P. The following theorem states a similar correspondence result
for lpw (P) and optimal plans for P. We define, for any consistent set of ground literals S, the sets
ASj = {a(t) | a(t, j  1)  S, a   act } and sSj = {f (t) | f (t, j)  S, f (t)  Lf l }, for all j  0.
Theorem 6.1 (Answer Set Correspondence) Let P = hPD, qi be a (well-defined) K c planning
problem, and let lpw (P) be the above program. Then,
(i) for each optimistic plan P = hA1 , . . . , Al i of P and supporting trajectory T = hhs 0 , A1 , s1 i,
hs1 , A2 , s2 i, . . . , hsl1 , Al , sl ii of P , there exists some answer set S of lp w (P) such that
Aj = ASj for all j = 1, . . . , l, sj = sSj , for all j = 0, . . . , l and costP (P ) = costlpw (P) (S);
(ii) for each answer set S of lpw (P), the sequence P = hA1 , . . . , Al i is a solution of P, i.e., an
optimistic plan, witnessed by the trajectory T = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . , hsl1 , Al , sl ii
with costP (P ) = costlpw (P) (S), where Aj = ASj and sk = sSk for all j = 1, . . . , l and
k = 0, . . . , l.
The proof is based on the resp. correspondence result for K (Eiter et al., 2003a). For the details,
we refer to the Appendix.
From this result and the definitions of optimal cost plans and optimal answer sets, we conclude
the following result:
Corollary 6.2 (Optimal answer set correspondence) For any well-defined K c planning problem
P = hPD, Q ? (l)i, the trajectories T = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii of optimal plans P for P
correspond to the optimal answer sets S of lp w (P), such that Aj = ASj for all j = 1, . . . , l and
sj = sSj , for all j = 0, . . . , l.
Proof. For each a  Aj , the weak constraint (5) causes a violation value of cost j (a). Furthermore, these are the
P onlyPcost violations. Thus, a candidate answer set S is optimal if and only if
costlpw (P) (S) = lj=1 aAj costj (a) = costP (P ) is minimal, i.e., S corresponds to an optimal
plan.
2
A similar correspondence result also holds for admissible plans:
49

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Corollary 6.3 (Answer set correspondence for admissible plans) For any well-defined K c planning problem P = hPD, Q ? (l)i, the trajectories T = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii of admissible plans P for P wrt. cost c correspond to the answer sets S of lp w (P) having costlpw (P) (S)  c,
such that Aj = ASj for all j = 1, . . . , l and sj = sSj , for all j = 0, . . . , l.
As for secure planning, we have introduced a technique to check security of an optimistic plan
for certain planning problem instances by means of a logic program (Eiter et al., 2003a). This
method carries over to planning with action costs in a straightforward way, and optimal resp. admissible secure plans can be similarly computed by answer set programming.
6.3 Alternative Translation for Smodels
Apart from the presented translation using weak constraints, one could also choose an alternative
approach for the translation to answer set programming. Smodels (Simons et al., 2002) supports
another extension to pure answer set programming allowing to minimize over sets of predicates.
This approach could be used in an alternative formulation of Step 7:
Step 7a:

For action declarations with nonempty costs-parts, we add a new rule of form
cost(p, X1 , . . . , Xn , 0, . . . , 0, T, C) :- t1 , . . . , tm , c1 , . . . , ck , U = T + 1.

(6)

similar to Step 7 above, with two differences: (1) action name p is now a parameter, and (2) we add
l  n parameters with constant 0 between X n and T where l is the maximum arity of all actions in
PD. This is necessary in order to get unique arity l + 2 for predicate cost. Furthermore, we add
occurs(p, X1 , . . . , Xn , 0, . . . , 0, T) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,.

(7)

This second rule adds the same 0 parameters as for to achieve unique arity l + 1 of the new
predicate occurs. Using Smodels syntax, we can now compute optimal plans by adding
minimize[occurs(A, X1, ..., Xl , T) : cost(A, X1, ..., Xl , T, C) = C].

Note that Smodels does not support disjunction in rule heads, so we also need to modify Step 4,
expressing the action guess via unstratified negation or Smodels choice rules.

7. Implementation
We have implemented an experimental prototype system, DLV K , for solving K planning problems (Eiter et al., 2003a). An improved version of this prototype it is now capable of optimal
and admissible planning with respect to the extended syntax of K c , available for experiments at
http://www.dlvsystem.com/K/ .
DLVK has been realized as a frontend to the DLV system (Faber & Pfeifer, 1996; Eiter et al.,
2000a). First, the planning problem at hand is transformed as described in the previous section.
Then, the DLV kernel is invoked to produce answer sets. For optimistic planning the (optimal, if
action costs are defined) answer sets are then simply translated back into suitable output for the user
and printed.
In case the user specified that secure/conformant planning should be performed, our system has
to check security of the plans computed. In normal (non-optimal) planning, this is simply done by
checking each answer set returned right before transforming it back to user output. In the case of
50

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

optimal secure planning, on the other hand, the candidate answer set generation of the DLV kernel
has to be intercepted: The kernel proceeds computing candidate answer sets, returning an answer
set with minimal violation cost value, by running through all candidates. Here, in order to generate
optimal secure plans, the planning frontend interrupts computation, allowing only answer sets which
represent secure plans to be considered as candidates.
Checking plan security is done by rewriting the translated program wrt. the candidate answer
set/plan in order to verify whether the plan is secure. The rewritten check program is tested by a
separate invocation of the DLV kernel. As for further details on the system architecture we refer to
(Eiter et al., 2003a)
7.1 Usage
Suppose the background knowledge and the program depicted in Figure 1 with the cost extensions
from Section 3.3 are stored in files crossing.bk and crossing.plan; then, by invoking the
program with the command line
dlv  FPcrossing.plancrossing.bk  planlength = 7

we compute all optimal plans solving this problem in seven steps. In the output we find, after a
supporting trajectory, the following optimal plan:
PLAN : crossTogether(joe, jack) : 2; cross(joe) : 1; takeLamp(william);
crossTogether(william, averell) : 10; takeLamp(jack);
cross(jack) : 2; crossTogether(joe, jack) : 2 COST : 17

For each action, its cost is shown after a colon, if it is non-zero. The switch -planlength=i can
be used to set the plan length; it overrides any plan length given in the query-part of the planing
problem. Using -planlength=5, we get plans with cost 19, as there are no cheaper plans of that
length.
The user is then asked whether to perform the optional security check and whether to look for
further (optimal) plans, respectively. The switch -FPsec can be used instead of -FP to obtain
secure plans only.
The command line option -costbound=N effects the computation of all admissible plans
with respect to cost N . For example, the resource problem described in Section 5.3 can be solved
by the following call to our prototype:
dlv  FPbuying.bkbuying.plan  N = 10  planlength = 1  costbound = 6

Correctly, no admissible plan is found. When calling the system again without cost bound, the
prototype calculates the following optimal cost plan:
PLAN : buy(newspaper, 1) : 1, buy(magazine, 2) : 6

COST : 7

The current prototype supports simple bounded integer arithmetics. The option -N=10 used
above sets an upper bound of N = 10 for the integers which may be used in a program; the builtin predicate #int is true for all integers 0 . . . N . Setting N high enough, taking into account
the outcome of built-in arithmetic predicates A = B + C and A = B  C, is important to get
correct results. Further details on the prototype are given on the DLV K web site at http://www.
dlvsystem.com/K/.
51

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

7.2 Experiments
Performance and experimental results for DLV K (without action costs and optimal planning) were
reported in previous work (Eiter et al., 2003a). In this section, we present some encouraging experimental results for planning with action costs, in particular for parallel Blocks World and TSP. All
experiments were performed on a Pentium III 733MHz machine with 256MB of main memory running SuSE Linux 7.2. We set a time limit of 4000 seconds for each tested instance where exceeding
this limit is indicated by - in the result tables.
Where possible, we also report results for CCALC and CMBP, two other logic-based planning
systems whose input languages (C+ resp. AR) have capabilities similar to K resp. K c .
CCALC. The Causal Calculator (CCALC) is a model checker for the languages of causal theories
(McCain & Turner, 1997). It translates programs in the action language C+ into the language of
causal theories which are in turn transformed into SAT problems; these are then solved using a SAT
solver (McCain & Turner, 1998). The current version of CCALC uses mChaff (Moskewicz et al.,
2001) as its default SAT solver. Minimal length plans are generated iteratively increasing the plan
length up to an upper bound. CCALC is written in Prolog. For our tests, we used version 2.04b of
CCALC which we obtained from <URL:http://www.cs.utexas.edu/users/tag/cc/
> and a trial version of SICStus Prolog 3.9.1. We used encodings taken from (Lee & Lifschitz,
2001) for parallel Blocks World adapted for CCALC 2.0. These encodings are included in the
current download version of the system. For sequential Blocks World we adapted the encodings
by adding the C+ command noConcurrency. which resembles the respective K command. All
results for CCALC include 2.30sec startup time.
CMBP. The Conformant Model Based Planner (CMBP) (Cimatti & Roveri, 2000) is based on the
model checking paradigm and exploits symbolic Boolean function representation techniques such
as Binary Decision Diagrams (Bryant, 1986). CMBP allows for computing sequential minimal
length plans, where the user has to declare an upper bound for the plan length. Its input language
is an extension of AR (Giunchiglia, Kartha, & Lifschitz, 1997). Unlike K or action languages
such as C+ (Lee & Lifschitz, 2001), this language only supports propositional actions. CMBP is
tailored for conformant planning. The results reported complement a previous comparison which
also shows the encoding for sequential Blocks World in CMBP (Eiter et al., 2003a). For our tests,
we used CMBP 1.0, available at <URL:http://sra.itc.it/people/roveri/cmbp/>.
7.2.1 B LOCKS W ORLD
Tables 14 show the results for our different Blocks World encodings in Section 5.1 on several
configurations: P0 denotes our simple instance from Figure 3, while P1P5 are instances used in
previous work (Eiter et al., 2003a; Erdem, 1999).
Table 1 shows the results for finding a shortest sequential plan. The second and third column
show the number of blocks and the length of a shortest plan (i.e., the least number of moves) solving
the respective blocks world instance. The execution time for solving the problem using the shortestplan encoding P in Section 5.1 is shown in column five, using the upper bound shown in the fourth
column on the plan length. Column six shows the execution time for finding the shortest plan in
an incremental plan length search starting from 0, similar to the method used for CCALC. The
remaining two columns show the results for CCALC and CMBP.
52

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Problem
P0
P1
P2
P3
P4
P5

#blocks
6
4
5
8
11
11

min. #moves (=#steps)
5
4
6
8
9
11

upper bound #steps
6
4
7
10
16
16

DLVK

DLVK
inc

0.48s
0.05s
0.24s
25.32s
-

0.29s
0.08s
0.27s
2.33s
8.28s
12.63s

CCALC
4.65s
3.02s
4.02s
10.07s
27.19s
32.27s

CMBP
21.45s
0.13s
8.44s
-

Table 1: Sequential Blocks World - shortest plans

Problem
P0
P0
P1
P2
P3
P4
P5

#blocks
6
6
4
5
8
11
11

#steps(fixed)
2
3
3
5
4
5
7

min. #moves
6
5
4
6
9
13
15

DLVK
0.05s
0.09s
0.04s
0.10s
0.21s
0.81s
327s

Table 2: Parallel Blocks World - cheapest plans: Minimal number of moves at fixed plan length ()

Table 2 shows the execution times for parallel blocks world with fixed plan length where the
number of moves is minimized, i.e. problem () in Section 5.1. We used the encoding in Figure 2,
which generates parallel serializable plans. As CCALC and CMBP do not allow for optimizing
other criteria than plan length, we only have results for DLV K here.
Next, Table 3 shows some results for finding a shortest parallel plan, i.e. problem () in Section 5.1. First, the minimal possible number of steps is given. We processed each instance (i) using
the encoding P from Section 5.1, (ii) without costs by iteratively increasing the plan length and
(iii) using CCALC, by iteratively increasing the plan length until a plan is found. For every result,
the number of moves of the first plan computed is reported separately. As CMBP only supports
sequential planning, it is not included in this comparison.
Finally, Table 4 shows the results for the combined optimizations () and () for parallel Blocks
World as outlined in Section 5.1. The second column again contains the upper bound for the plan

upper bound
P0
P1
P2
P3
P4
P5

6
4
7
10
16
16

min. #steps
2
3
5
4
5
7

DLVK
#moves
6
5
9
-

time
0.52s
0.07s
0.39s
-

DLVK
inc
#moves
6
5
9
12
18
26

time
0.09s
0.08s
0.21s
0.43s
1.54s
3.45s

Table 3: Parallel Blocks World - shortest plan ()

53

CCALC
#moves
time
6
4.05s
4
2.95s
6
3.70s
9
7.69s
13
20.45s
15
23.22s

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

()
P0
P1
P2
P3
P4
P5

upper bound
6
4
7
10
16
16

steps/moves
3/5
3/4
5/6
5/8
9/9
11/11

()

DLVK

DLVK
inc

38.5s
0.07s
2.08s
-

0.18s
0.11s
0.21s
1.57s
-

CCALC
5.89s
3.47s
5.65s
15.73s
73.64s
167s

steps/moves
2/6
3/4
5/6
4/9
5/13
7/15

DLVK

DLVK
inc

0.26s
0.08s
0.78s
177s
-

0.09s
0.08s
0.28s
0.45s
1.86s
323s

Table 4: Parallel Blocks World - (),()

length of the respective instance. The following three columns present the results on finding a
shortest among the cheapest plans, i.e. problem () in Section 5.1:
DLVK refers to the results for our combined minimal encoding P  and as described in Section 5.1;
DLVK
inc refers to the results for incrementally searching for the shortest among the cheapest plans:

This is done by means of the -costbound=i command line option taking the minimal
sequential costs (i.e., the shortest sequential plan length as computed in Table 1) as an upper
cost limit. As our encodings compute serializable plans, the minimal sequential length can be
used as cost limit in this special case.
CCALC A similar technique can be used with CCALC when encoding bound costs through additive fluents (Lee & Lifschitz, 2001).
Note that the incremental strategy (used by DLV K
inc and CCALC) takes advantage of our specific formulation of the parallel Blocks World problem: In general, when allowing parallel actions
which are not necessarily serializable and have arbitrary costs, the optimal parallel cost might differ
from the optimal sequential solution. In particular, plans which are longer than the cheapest sequential plans (which, in this example, coincide with the shortest sequential plans) may need to be
considered. This makes incremental search for a solution of problem () infeasible in general.
The last test is finding a cheapest among the shortest plans, that is, problem () in Section 5.1.
Again we have tested the integrated encoding with an upper bound (P  ) resp. incrementally finding
the shortest plan. Unlike for problem (), we cannot derive a fixed cost limit from the sequential
solution here; we really need to optimize costs, which makes an encoding in CCALC infeasible.
Blocks World  Results The Blocks World experiments show that DLV K can solve various optimization tasks in a more effective and flexible way than the systems compared. On the other hand,
as already stated above, for the minimal plan length encodings in Section 5.1, we can only solve
the problems where a tight upper bound for the plan length is known. Iteratively increasing the plan
length is more effective, especially if the upper bound is much higher than the actual optimal solution. This becomes drastically apparent when execution times seem to explode from one instance
to the next, in a highly non-linear manner as in Table 1 where a solution for P3 can be found in
reasonable time whereas P4 and P5 could not be solved within the time limit of 4000 seconds. This
observation is also confirmed in the other tables (instance P5 in Table 2, etc.) and is partly explained
by the behavior of the underlying DLV system, which is not geared towards plan search, and as a
general purpose problem solver uses heuristics which might not work out well in some cases. In
particular, during the answer set generation process in DLV, no distinction is made between actions
54

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

and fluents, which might be useful for planning tasks to control the generation of answer sets resp.
plans; this may be part of further investigations.
Interestingly, CCALC finds better quality parallel solutions for problem () (cf. Table 3), i.e.
solutions with fewer moves, although it is significantly slower than our system on these instances.
For the incremental encoding of problem (), CCALC seems even more effective than our system.
However, CCALC offers no means of optimization; it allows for admissible but not for optimal
planning. This makes our approach more flexible and general. As stated above, we could fortunately
exploit the fixed cost bound in this particular example for CCALC, which is not possible in general
instances of problem ().
Problem () is also intuitively harder than simply finding a shortest plan or a cheapest among
all shortest plans in general: While these problems can always be solved incrementally, for () we
must consider all plans of all lengths. A longer plan may be cheaper, so we cannot freeze the plan
length once a (shortest) plan has been incrementally found.
7.2.2 TSP
Some experimental results on TSP with variable costs are reported in Tables 5 and 6. Unlike for
blocks world, no comparable systems were available; none of the systems from above supports cost
optimal planning as needed for solving this problem. Here, the plan length is always given by the
number of cities.
Table 5 shows the results for our TSP instance on the Austrian province capitals as in Figure 5
(nine cities, 18 connections), with and without the exceptional costs as in Section 5.2 (with and without subscript exc in the table). Further instances reported in this table with different cost exceptions
(we, lwe, rnd) are described below.
Results for some bigger TSP instances, given by the capitals of the 15 members of the European
Union (EU) with varying connection graphs and exceptional costs are shown in Table 6. We have
used the flight distances (km) between the cities as connection costs. Instances TSP EU 1TSPEU 6
have been generated by randomly choosing a given number of connections from all possible connections between the 15 cities. Note that TSP EU 1 has no solution; the time reported here is until
DLVK terminated, and for all other instances until the first optimal plan was found.
We have also tested some instances of more practical relevance than simply randomly choosing
connections: TSPEU 7 is an instance where we have taken the flight connections of three carriers
(namely, Star Alliance, Alitalia, and Luxair), and in TSP EU 8 we have included only direct connections of at most 1500km. Such a capital hopping is of interest for a small airplane with limited
range, for instance.
For each instance in Tables 56 we have measured the execution time:
 without exceptional costs,
 with 50% costs for all connections on Saturdays and Sundays (weekends, we)
 with 50% costs for all connections on Fridays, Saturdays and Sundays (long weekends, lwe),
 for some random cost exceptions (rnd): We have added a number of randomly generated exceptions with costs between 0 and 10 for TSP Austria and between 0 and 3000 for the instances
EU1 to EU8.
55

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Instance
TSPAustria
TSPAustria,exc
TSPAustria,we
TSPAustria,lwe
TSPAustria,rnd
TSPAustria,rnd
TSPAustria,rnd
TSPAustria,rnd

#cost exceptions
0
2
36
54
10
50
100
200

cost/time
15/0.31s
15/0.32s
12/0.34s
11/0.35s
14/0.30s
15/0.31s
23/0.35s
36/0.37s

Table 5: TSP  Results for TSPAustria with varying exceptions

Instance
TSPEU 1
TSPEU 1,we
TSPEU 1,lwe
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 2
TSPEU 2,we
TSPEU 2,lwe
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 3
TSPEU 3,we
TSPEU 3,lwe
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 4
TSPEU 4,we
TSPEU 4,lwe
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 5
TSPEU 5,we
TSPEU 5,lwe
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd

#conn.
30
30
30
30
30
30
30
30
30
30
30
30
30
30
35
35
35
35
35
35
35
35
35
35
35
35
35
35
40
40
40
40
40
40
40
40

#except.
0
60
90
100
200
300
400
0
60
90
100
200
300
400
0
70
105
100
200
300
400
0
70
105
100
200
300
400
0
80
120
100
200
300
400
500

cost/time
-/9.11s
-/11.93s
-/13.82s
-/11.52s
-/12.79s
-/14.64s
-/16.26s
16213/13.27s
13195/16.41s
11738/18.53s
15190/15.54s
13433/16.31s
13829/18.34s
13895/20.59s
18576/24.11s
15689/28.02s
14589/30.39s
19410/26.75s
22055/29.64s
18354/31.54s
17285/32.66s
16533/36.63s
12747/41.72s
11812/43.12s
15553/39.17s
13216/41.19s
16413/43.51s
13782/45.69s
15716/91.83s
12875/97.73s
12009/100.14s
13146/85.69s
12162/83.44s
12074/76.81s
12226/82.97s
13212/82.53s

Instance
TSPEU 6
TSPEU 6,we
TSPEU 6,lwe
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 7
TSPEU 7,we
TSPEU 7,lwe
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 8
TSPEU 8,we
TSPEU 8,lwe
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd

#conn.
40
40
40
40
40
40
40
40
55
55
55
55
55
55
55
55
55
55
64
64
64
64
64
64
64
64
64
64
64

#except.
0
80
120
100
200
300
400
500
0
110
165
100
200
300
400
500
600
700
0
128
192
100
200
300
400
500
600
700
800

cost/time
17483/142.7s
14336/150.3s
13244/154.7s
15630/142.5s
14258/137.2s
11754/120.5s
11695/111.4s
12976/120.8s
15022/102.6s
12917/112.2s
11498/116.2s
13990/104.2s
12461/100.8s
13838/106.9s
12251/96.58s
16103/109.2s
14890/110.3s
17070/110.7s
10858/3872s
9035/3685s
8340/3324s
10283/2603s
9926/1372s
10028/1621s
8133/597.7s
8770/573.3s
8220/360.7s
6787/324.6s
11597/509.5s

Table 6: TSP  Various instances for the capitals of the 15 EU members
56

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

TSP  Results Instance TSPEU 8 shows the limits of our system: the given data allows for many
possible tours, so finding an optimal one gets very tricky. On the other hand, a realistic instance like
TSPEU 7 with real airline connections is solved rather quickly, which is not very surprising: Most
airlines have a central airport (for instance Vienna for Austrian Airlines) and few direct connections
between the destinations served. This allows for much fewer candidate answer sets, when (as in
reality) the number of airlines we consider is limited. E.g., TSP EU 7 has no solution at all if only
two out of Star Alliance, Alitalia, and Luxair are allowed. Of course, we cannot compete with
dedicated TSP solvers/algorithms, which are able to solve much bigger TSP instances and have not
been considered here. However, to our knowledge, none of these solvers can deal with features such
as incomplete knowledge, defaults, time dependent exceptional costs, etc. directly. Our results even
show that execution times are stable yet in case of many exceptions. In contrast, instance TSP EU 8
shows that exceptions can also cause a significant speedup. This is due to the heuristics used by the
underlying DLV system, which can single out better solutions faster if costs are not spread evenly
like in TSPEU 8 without exceptional costs.
Note that, we have also experimented with the alternative Smodels translation sketched in Section 6.3. We refrain from detailed discussion here, since the (i) translation is optimized for DLV and
Smodels performance was worse (around factor 10 for the tested TSP instances) than DLV and (ii)
there is no integrated planning frontend available for Smodels providing a high-level planning language. Nevertheless, we have shown that our approach can, with minor modifications, be adopted
in a planning system based on Smodels.

8. Related Work
In the last years, it has been widely recognized that plan length alone is only one criterion to be
optimized in planning. Several attempts have been made to extend planners to also consider action
costs.
The PYRRHUS system (Williams & Hanks, 1994) is an extension of UCPOP planning which
allows for optimal planning with resources and durations. Domain-dependent knowledge can be
added to direct the heuristic search. A utility model has to be defined for a planning problem
which can be used to express an optimization function. This system supports a language extension
of ADL (Pednault, 1989), which is a predecessor of PDDL (Ghallab et al., 1998). The algorithm is
a synthesis of branch-and-bound optimization with a least-commitment, plan-space planner.
Other approaches based on heuristic search include the use of an A* strategy together with
action costs in the heuristics (Ephrati, Pollack, & Mihlstein, 1996) and work by Refanidis and
Vlahavas who use multi-criteria heuristics to obtain near-optimal plans, considering multiple criteria
apart from plan length alone (Refanidis & Vlahavas, 2001). However, the described heuristics is not
fully admissible, and only guarantees optimal plans under certain restrictions (Haslum & Geffner,
2000). In fact, most heuristic state-space planners are not able to guarantee optimality.
A powerful approach has been suggested by Nareyek, who describes planning with resources
as a structural constraint satisfaction problem (SCSP), and then solves that problem by local search
combined with global control. However, this work promotes the inclusion of domain-dependent
knowledge; the general problem has an unlimited search space, and no declarative high-level language is provided (Nareyek, 2001).
Among other related approaches, Kautz and Walser generalize the Planning as Satisfiability
approach to use integer optimization techniques for encoding optimal planning under resource pro57

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

duction/consumption (Kautz & Walser, 1999). First, they recall that integer logic programming
generalizes SAT, as a SAT formula can be translated to a system of inequalities. Second, they extend effects and preconditions of actions similar to a STRIPS extension proposed by Koehler for
modeling resource consumption/production (Koehler, 1998). Kautz and Walser allow for arbitrary
optimization functions but they use a non-declarative, low-level representation based on the algebraic modeling language AMPL (Fourer, Gay, & Kernighan, 1993). They mention that Koehlers
STRIPS-like formalization can be mapped to their approach. However, they can not express nondeterminism or incomplete knowledge. There is an implementation of this approach called ILPPLAN, which uses the AMPL package (http://www.ampl.com/). Unfortunately, AMPL is
not freely available, so we could not compare the system with our approach experimentally.
Lee and Lifschitz describe the extension C+ of the action language C which allows for an intuitive encoding of resources and costs by means of so called additive fluents (Lee & Lifschitz,
2001). This way admissible planning can be realized, but optimization has not been considered in
that framework so far. An implementation of a planner based on this language is CCALC (McCain,
1999) which has already been described in the previous section. Another implementation of a planning system based on the action language C is Cplan (Giunchiglia, 2000; Ferraris & Giunchiglia,
2000). The Cplan system mainly focuses on conformant planning and does not support the advanced
features of C+. Furthermore, the code is no longer maintained.
Son and Pontelli propose to translate action language B to prioritized default theory and answer
set programming. They allow to express preferences between actions and rules at the object level
in an interpreter but not as a part of the input language (Son & Pontelli, 2002). However, these
preferences are orthogonal to our approach as they model qualitative preferences as opposed to our
overall value function of plans/trajectories.

9. Conclusion and Outlook
This work continues a research stream which pursues the usage of answer set programming for
building planning systems which offer declarative planning languages based on action languages,
where planning tasks are specified at a high level of abstraction (Lifschitz, 1999a, 1999b). For
representation of practical planning problems, such languages must have high expressiveness and
provide convenient constructs and language elements.
Towards this goal, we have presented the planning language K c , which extends the declarative
planning language K (Eiter et al., 2000b, 2003a) by action costs which are taken into account for
generating optimal plans, i.e., plans that have least total execution cost, and for admissible plans
wrt. a given cost bound, i.e., plans whose total execution cost stays within a given limit. As a basis
for implementation issues, we have investigated the computational complexity of the major planning tasks in this language, where we have derived complexity results sharply characterizing their
computational cost. Furthermore, we have presented a transformation of optimal and admissible
planning problems in K c to logic programming under the optimal answer set semantics (Buccafurri
et al., 1997, 2000), and we have described the DLV K prototype implemented on top of the KR tool
DLV, which computes this semantics.
As we have shown, Kc allows for the representation of intricate planning problems. In particular,
we have demonstrated this for a variant of the Traveling Salesperson Problem (TSP), which could
be conveniently specified in Kc . A strength of Kc is that, via the underlying language K, states of
knowledge, i.e., incomplete states, can be suitably respected in secure plans, i.e., conformant plans
58

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

which work under all circumstances, including nondeterministic action effects. K c is a flexible
language which, by exploiting time-dependent action costs, allows for the representation of planning
under various optimality criteria such as cheapest plans, shortest plans, and combinations thereof.
Our experiments have shown that various instances of the problems we considered, including
realistic instances of the TSP variant, could be decently solved. On the other hand, the current
version of DLVK does not scale to large problem instances in general, and, unsurprisingly, can not
compete with high-end planning tools or specialized algorithms for a particular problem such as
TSP. We do not see this as a shortcoming, though, since our main goal at this point was to demonstrate the usefulness of the expressive capabilities of our formalism to easily represent non-trivial
planning and optimization tasks, which are especially involved from the viewpoint of knowledge
representation. In this way, non-trivial instances of such problems of medium size (which one may
often encounter) can be solved with little effort.
Several issues remain for further work. As for the implementation, performance improvements
may be gained via improvements of the underlying DLV engine, which are subject of current work.
Furthermore, alternative, more efficient transformations of Kc to logic programming might be researched, e.g. ones that involve preprocessing of the planning problem performing means-end analysis to simplify the logic program constructed.
Another issue is further language extensions. For example, a crucial difference between our
approach and resource-based approaches is that the former hinges on action costs, while the latter
build on fluent values, which is a somewhat different view of the quality of a plan. A possible way to
encompass this in our language is to allow that dynamic fluent values contribute to action costs; this
needs to be carefully elaborated, though: While for deterministic planning under complete knowledge this extension is straightforward, in non-deterministic domains with incomplete knowledge it
would possibly result in ambiguities. Different trajectories of the same plan possibly yield different
costs when fluent values contribute to action costs. In favor of an intuitive definition of plan costs
and optimality we refrained from this extension at the current state.
A further possible extension are negative action costs, which are useful for modeling producer/consumer relations among actions and resources. Allowing for different priorities among
actions, i.e., different cost levels, would increase the flexibility and allow for optimizing different
criteria at once. Finally, the duration of actions is an important issue. In the current language, the
effects of actions are assumed to materialize in the next state. While by coding techniques, we
may express delayed effects over several states in time and/or interleaving actions, constructs in the
language would be desirable. Investigating these issues is part of our ongoing and future work.

Acknowledgments
We are are grateful to Joohyung Lee for his help on using CCALC and to Paul Walser for his useful
informations on ILPPLAN. Furthermore, we thank Michael Gelfond for interesting discussions and
suggestions, and the anonymous reviewers for their detailed and helpful comments.
This work was supported by FWF (Austrian Science Funds) under the projects P14781 and
Z29-N04 and the European Commission under project FET-2001-37004 WASP and IST-200133570 INFOMIX.
A preliminary, shorter version of this paper was presented at the 8th European Conference on
Logics in Artificial Intelligence (JELIA02), Cosenza, Italy, September 2002.
59

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Appendix A. The Language K
This appendix contains, in shortened form, the definition of the language K and a translation of K
to answer set programs; see (Eiter et al., 2003b, 2003a) for more details and examples.
A.1 Basic Syntax
We assume  act ,  f l , and  typ disjoint sets of action, fluent and type names, respectively, i.e.,
predicate symbols of arity  0, and disjoint sets  con and  var of constant and variable symbols.
Here,  f l ,  act describe dynamic knowledge and  typ describes static background knowledge. An
action (resp. fluent, type) atom is of form p(t 1 , . . . , tn ), where p   act (resp.  f l ,  typ ) has arity n
and t1 , . . . , tn   con   var . An action (resp. fluent, type) literal l is an action (resp. fluent, type)
atom a or its negation a, where  (alternatively, ) is the true negation symbol. We define
.l = a if l = a and .l = a if l = a, where a is an atom. A set L of literals is consistent, if
L  .L = . Furthermore, L+ (resp. L ) is the set of positive (resp. negative) literals in L. The
set of all action (resp. fluent, type) literals is denoted as L act (resp. Lf l , Ltyp ). Furthermore, Lf l,typ
+
= Lf l  Ltyp , Ldyn = Lf l  L+
act , and L = Lf l,typ  Lact .
All actions and fluents must be declared using statements as follows.
Definition A.1 (action, fluent declaration) An action (resp. fluent) declaration, is of the form:
p(X1 , . . . , Xn ) requires t1 , . . . , tm

(8)

+
var where n  0 is the arity of p, t , . . . , t 
where p  L+
1
m
act (resp. p  Lf l ), X1 , . . . , Xn  
Ltyp , m  0, and every Xi occurs in t1 , . . . , tm .

If m = 0, the keyword requires may be omitted. Causation rules specify dependencies of
fluents on other fluents and actions.
Definition A.2 (causation rule) A causation rule (rule, for short) is an expression of the form
caused f if b1 , . . . , bk , not bk+1 , . . . , not bl after a1 , . . . , am , not am+1 , . . . , not an

(9)

where f  Lf l {false}, b1 , . . . , bl  Lf l,typ , a1 , . . . , an L, l  k  0, and n  m  0.
Rules where n = 0 are static rules, all others dynamic rules. When l = 0 (resp. n = 0), if (resp.
after) is omitted; if both l = n = 0, caused is optional.
We access parts of a causation rule r by h(r) = {f }, post + (r) = {b1 , . . . , bk }, post (r) =
{bk+1 , . . . , bl }, pre+ (r) = {a1 , . . . , am }, pre (r) = {am+1 , . . . , an }, and lit(r) = {f, b1 , . . . , bl ,
a1 , . . . , an }. Intuitively, pre(r) = pre + (r)  pre (r) (resp. post(r) = post+ (r)  post (r))
accesses the state before (resp. after) some action(s) happen.
Special static rules may be specified for the initial states.
Definition A.3 (initial state constraint) An initial state constraint is a static rule of the form (9)
preceded by initially.
The language K allows conditional execution of actions, where several alternative executability
conditions may be specified.
60

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Definition A.4 (executability condition) An executability condition e is an expression of the form
executable a if b1 , . . . , bk , not bk+1 , . . . , not bl

(10)

where a  L+
act and b1 , . . . , bl  L, and l  k  0.
If l = 0 (i.e., executability is unconditional), if is skipped. The parts of e are accessed by h(e) =
{a}, pre+ (e) = {b1 , . . . , bk }, pre (e) = {bk+1 , . . . , bl }, and lit(e) = {a, b1 , . . . , bl }. Intuitively,
pre(e) = pre+ (e)  pre (e) refers to the state at which some actions suitability is evaluated. The
state after action execution is not involved; for convenience, we define post+ (e) = post (e) = .
All causal rules and executability conditions must satisfy the following condition, which is
similar to safety in logic programs: Each variable in a default-negated type literal must also occur in
some literal which is not a default-negated type literal. No safety is requested for variables appearing
in other literals. The reason is that variables appearing in fluent and action literals are implicitly safe
by the respective type declarations.
Notation. For any causal rule, initial state constraint, and executability condition r and   {post, pre, b},
we define (r) = + (r)    (r), where bs (r) = posts (r)  pres (r).
A.1.1 P LANNING D OMAINS

AND

P LANNING P ROBLEMS

Definition A.5 (action description, planning domain) An action description hD, Ri consists of a
finite set D of action and fluent declarations and a finite set R of safe causation rules, safe initial
state constraints, and safe executability conditions which do not contain positive cyclic dependencies among actions. A K planning domain is a pair PD = h, ADi, where  is a disjunction-free
normal Datalog program (the background knowledge) which is safe and has a total well-founded
model (cf. (van Gelder, Ross, & Schlipf, 1991)) 8 and AD is an action description. We call PD
positive, if no default negation occurs in AD.
Definition A.6 (planning problem) A planning problem P = hPD, qi is a pair of a planning domain PD and a query q, i.e.,
g1 , . . . , gm , not gm+1 , . . . , not gn ? (i)

(11)

where g1 , . . . , gn  Lf l are variable-free, n  m  0, and i  0 denotes the plan length.
A.2 Semantics
We start with the preliminary definition of the typed instantiation of a planning domain. This is
similar to the grounding of a logic program, with the difference being that only correctly typed
fluent and action literals are generated.
Let PD = h, hD, Rii be a planning domain, and let M be the (unique) answer set of  (Gelfond & Lifschitz, 1991). Then, (p(X 1 , . . . , Xn )) is a legal action (resp. fluent) instance of an action (resp. fluent) declaration d  D of the form (8), if  is a substitution defined over X1 , . . . , Xn
such that {(t1 ), . . . , (tm )}  M . By LPD we denote the set of all legal action and fluent instances. The instantiation of a planning domain respecting type information is as follows.
8. A total well-founded model, if existing, corresponds to the unique answer set of a datalog program. Allowing for
multiple answer sets of  would eventually lead to ambiguities in our language.

61

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Definition A.7 (typed instantiation) For any planning domain PD = h, hD, Rii, its typed instantiation is given by PD = h, hD, Rii, where  is the grounding of  (over  con ) and
R = {(r) | r  R,   r }, where r is the set of all substitutions  of the variables in r using
 con , such that lit((r))  Ldyn  LPD  (.LPD  L
f l ).
In other words, in PD we replace  and R by their ground versions, but keep of the latter only
rules where the atoms of all fluent and action literals agree with their declarations. We say that a
PD = h, hD, Rii is ground, if  and R are ground, and moreover that it is well-typed, if PD and
PD coincide.
A.2.1 S TATES

AND

T RANSITIONS

Definition A.8 (state, state transition) A state w.r.t a planning domain PD is any consistent set
s  Lf l  (lit(PD)  lit(PD) ) of legal fluent instances and their negations. A state transition
is any tuple t = hs, A, s0 i where s, s0 are states and A  Lact  lit(PD) is a set of legal action
instances in PD.
Observe that a state does not necessarily contain either f or f for each legal instance f of a
fluent, and may even be empty (s = ). State transitions are not constrained; this will be done in the
definition of legal state transitions below. We proceed in analogy to the definition of answer sets
(Gelfond & Lifschitz, 1991), considering first positive (i.e., involving a positive planning domain)
and then general planning problems.
In what follows, we assume that PD = h, hD, Rii is a well-typed ground planning domain
and that M is the unique answer set of . For any other PD, the respective concepts are defined
through its typed grounding PD.
Definition A.9 (legal initial state) A state s 0 is a legal initial state for a positive PD, if s 0 is the
least set (w.r.t. ) such that post(c)  s 0  M implies h(c)  s0 , for all initial state constraints
and static rules c  R.
For a positive PD and a state s, a set A  L +
act is called executable action set w.r.t. s, if for each
a  A there exists an executability condition e  R such that h(e) = {a}, pre + (e)Lf l,typ  sM ,
+

pre+ (e)L+
act  A, and pre (e)(Lact sM ) = . Note that this definition allows for modeling
dependent actions, i.e. actions which depend on the execution of other actions.
Definition A.10 (legal state transition) Given a positive PD, a state transition t = hs, A, s 0 i is
called legal, if A is an executable action set w.r.t. s and s 0 is the minimal consistent set that satisfies
all causation rules w.r.t. s  A  M . That is, for every causation rule r  R, if (i) post(r)  s 0  M ,
(ii) pre(r)  Lf l,typ  s  M , and (iii) pre(r)  Lact  A all hold, then h(r) 6= {false} and
h(r)  s0 .
This is now extended to general a well-typed ground PD containing default negation using a
Gelfond-Lifschitz type reduction to a positive planning domain (Gelfond & Lifschitz, 1991).
Definition A.11 (reduction) Let PD be a ground and well-typed planning domain, and let t =
hs, A, s0 i be a state transition. Then, the reduction PD t = h, hD, Rt ii of PD by t is the planning
domain where Rt is obtained from R by deleting
62

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

1. each r  R, where either post (r)(s0  M ) 6=  or pre (r)(sAM ) 6= , and
2. all default literals not L (L  L) from the remaining r  R.
Note that PD t is positive and ground. We extend further definitions as follows.
Definition A.12 (legal initial state, executable action set, legal state transition) For any planning
domain PD, a state s0 is a legal initial state, if s0 is a legal initial state for PD h,,s0 i ; a set A is an
executable action set w.r.t. a state s, if A is executable w.r.t. s in PD hs,A,i ; and, a state transition
t = hs, A, s0 i is legal, if it is legal in PD t .
A.2.2 P LANS
Definition A.13 (trajectory) A sequence of state transitions T = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . .,
hsn1 , An , sn ii, n  0, is a trajectory for PD, if s0 is a legal initial state of PD and all hs i1 , Ai , si i,
1  i  n, are legal state transitions of PD.
If n = 0, then T = hi is empty and has s0 associated explicitly.
Definition A.14 (optimistic plan) A sequence of action sets hA 1 , . . . , Ai i, i  0, is an optimistic
plan for a planning problem P = hPD, qi, if a trajectory T = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,
hsi1 , Ai , si ii exists in PD which accomplishes the goal, i.e., {g 1 , . . . gm }  si and {gm+1 , . . . , gn }
si = .
Optimistic plans amount to plans, valid plans etc as defined in the literature. The term
optimistic should stress the credulous view in this definition, with respect to incomplete fluent
information and nondeterministic action effects. In such cases, the execution of an optimistic plan
P might fail to reach the goal. We thus resort to secure plans.
Definition A.15 (secure plans (alias conformant plans)) An optimistic plan hA 1 , . . . , An i is a secure plan, if for every legal initial state s 0 and trajectory T = hhs0 , A1 , s1 i, . . . , hsj1 , Aj , sj ii such
that 0  j  n, it holds that (i) if j = n then T accomplishes the goal, and (ii) if j < n, then A j+1
is executable in sj w.r.t. PD, i.e., some legal transition hs j , Aj+1 , sj+1 i exists.
Note that plans admit in general the concurrent execution of actions. We call a plan hA 1 , . . . , An i
sequential (or non-concurrent), if |A j |  1, for all 1  j  n.
A.3 Macros
K includes several macros as shorthands for frequently used concepts. Let a  L +
act denote an
action atom, f  Lf l a fluent literal, B a (possibly empty) sequence b 1 , . . . , bk , not bk+1 , . . . ,
not bl where each bi  Lf l,typ , i = 1, . . . , l, and A a (possibly empty) sequence a 1 , . . . , am ,
not am+1 , . . . , not an where each aj  L, j = 1, . . . , n.
Inertia To allow for an easy representation of fluent inertia, K provides
inertial f if B after A.

Defaults



caused f if not .f, B after f, A.

A default value of a fluent can be expressed by the shortcut
default f.



caused f if not .f.

It is in effect unless some other causation rule provides evidence to the opposite value.
63

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Totality

For reasoning under incomplete, but total knowledge K provides (f positive):
total f if B after A.



caused f if not f, B after A.
caused f if not f, B after A.

This is is for instance useful to model non-deterministic action effects. For a discussion of the
full impact of this statement in modeling planning under incomplete knowledge and non-determinism,
we refer to our previous paper on the language K (Eiter et al., 2003b).
State Integrity

For integrity constraints that refer to the preceding state, K provides
forbidden B after A.

Non-executability



caused false if B after A.

For specifying that some action is not executable, K provides

nonexecutable a if B.



caused false after a, B.

By this definition, nonexecutable overrides executable in case of conflicts.
Sequential Plans To exclude simultaneous execution of actions, K provides
noConcurrency.



caused false after a1 , a2 .

where a1 and a2 range over all possible actions such that a 1 , a2  LPD  Lact and a1 6= a2 .
In all macros, if B (resp. after A) can be omitted, if B (resp. A) is empty.

Appendix B. Proofs
Proof of Theorem 4.4: Membership (i): The problems are in NP resp. NPMV, since if l is polynomial in the size of P, any optimistic plan P = hA 1 , . . . , Al i for P with a supporting trajectory
T = ht1 , . . . , ti i for P can be guessed and, by Proposition 4.1, verified in polynomial time. Furthermore, costP (P )  b can be efficiently checked, since costP (P ) is easily computed (all costs
are constants).
Hardness (i): K is a fragment of K c , and each K planning problem can be viewed as the problem
of deciding the existence of resp. finding an admissible plan wrt. cost 0. As was previously shown
(Eiter et al., 2003b), deciding existence of an optimistic plan for a given K planning problem is
NP-hard for fixed plan length l; hence, it is also NP-hard for Kc .
We show that finding an optimistic plan is hard for NPMV by a reduction from the well-known
SAT problem, cf. (Papadimitriou, 1994), whose instances are CNFs  = c 1   ck of clauses ci =
Li,1      Li,mi , where each Li,j is a classical literal over propositional atoms X = {x 1 , . . . , xn }.
Consider the following planning domain PD  for :
fluents :
actions :

x1 . . . . xn . state0. state1.
c1 costs 1. . . . ck costs 1.
ax1 . . . . axn .
initially : total x1 . . . . total xn .
caused state0.
always :
caused state1 after state0.
executable c1 after .L1,1 , . . . , .L1,m1 .
forbidden after .L1,1 , . . . , .L1,m1 , not c1 .

executable ck after .Lk,1 , . . . , .Lk,mk .
forbidden after .Lk,1 , . . . , .Lk,mk , not ck .
executable ax1 after x1 . forbidden after x1 , not ax1 .

executable axn after xn . forbidden after xn , not axn .

64

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

The fluents xi and state0 and the total statements in the initially-section encode the candidate truth assignments. The subsequent statements force c j to be executed iff the corresponding
clause is violated by the truth assignment encoded in the initial state. The final pairs of executable
and forbidden statements force actions ax i to be executed iff the corresponding fluents x i hold.
This is because it is necessary to directly extract the computed truth assignments from the plan,
since we are dealing with a function class. The fluent state1 identifies the state at time 1.
Consider now the planning problem P  = hPD , state1?(1)i. Clearly, each optimistic plan
P for P corresponds to a truth assignment  P of X and vice versa, and costP (P ) is the number
of clauses violated by P . Thus, the admissible optimistic plans for P  wrt. cost 0 correspond 1-1
to the satisfying assignments of . Clearly, constructing P  from  is efficiently possible, as is
constructing a satisfying truth assignment  from a corresponding plan P (because of the actions
axi ). This concludes the hardness proof.
Membership (ii): Since the security of each optimistic plan admissible wrt. cost k can be checked,
by Proposition 4.1, with a call to a  P2 -oracle, membership in P3 resp. in P3 MV follows by
analogous considerations as in (i) (where no oracle was needed).
Hardness (ii): For the decision variant,  P3 -hardness is again immediately inherited from the  P3 completeness of deciding the existence of a secure plan of a problem in the language K, with
hardness even for fixed plan length (Eiter et al., 2003b). For the plan computation variant, we give
a reduction from the following P3 MV-complete problem: An instance I is an open QBF
Q[Z] = XY [X, Y, Z]
where X = x1 , . . . , xl , Y = y1 , . . . , ym , and Z = z1 , . . . , zn , respectively, and [X, Y, Z] is
(w.l.o.g.) a 3CNF formula over X, Y , and Z. The solutions S(I) are all truth assignments over Z
for which Q[Z] is satisfied.
Suppose that [X, Y, Z] = c1 . . . ck where ci = ci,1 ci,2 ci,3 . Now consider the following
planning domain PDQ[Z] for Q[Z], which is a variant of the planning domain given in the proof of
Theorem 5.5 in (Eiter et al., 2003b):
fluents :
x1 . . . . xl . y1 . . . . ym . z1 . . . . zn . state0. state1.
actions :
az1 costs 0. . . . azn costs 0.
initially : total x1 . . . . total xl .
caused state0.
always :
caused state1 after state0.
executable az1 . executable az2 . . . . executable azn .
caused x1 after x1 . caused  x1 after  x1 .

caused xl after xl . caused  xl after  xl .
total y1 after state0. . . . total ym after state0.
caused z1 after az1 . caused  z1 after not az1 .

caused zn after azn . caused  zn after not azn .
forbidden .C1,1 , .C1,2 , .C1,3 after state0.

forbidden .Ck,1 , .Ck,2 , .Ck,3 after state0.
|X|

There are 2|X| many legal initial states s1 , . . . , s2 for PDQ[Z] , which correspond 1-1 to the
possible truth assignments to X and all these initial states contain state0. Starting from any initial
state si , executing a set of actions represents a truth assignment to the variables in Z. Since all
65

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

actions are always executable, there are 2 |Z| executable action sets A1 , . . . , A2|Z| , which represent
all truth assignments to Z.
|Y |
For each pair si and Aj there exist 2|Y | many successor state candidates si,1 , . . . , si,2 , which
contain fluents according to the truth assignment to X represented by s i , fluents according to the
truth assignment to Z represented by A j , and fluents according to a truth assignment to Y , and the
fluent state1. Of these candidate states, only those satisfying all clauses in [X, Y, Z] are legal,
by virtue of the forbidden statements.
It is not hard to see that an optimistic plan of form P = hA 1 i (where A1  {azi | zi  Z}) for
the goal state1 exists wrt. PDQ[Z] iff there is an assignment to all variables in X  Y  Z such
that the formula [X, Y, Z] is satisfied. Furthermore, P is secure iff A1 represents an assignment
to the variables in Z such that, regardless of which assignment to the variables in X is chosen
(corresponding to a legal initial state s i ), there is some assignment to the variables in Y such that all
clauses of [X, Y, Z] are satisfied (i.e., there is at least one state si,k reachable from si by executing
A1 ); any such si,k contains state1. In other words, P is secure iff [X, Y, Z] is true. Thus, the
admissible secure plans of PDQ[Z] wrt. cost 0, correspond 1-1 with the assignments to Z for which
Q[Z] is true.
Since PDQ[Z] is constructible from [X, Y, Z] in polynomial time, it follows that computing a
secure plan for P = hPDQ[Z] , qi, where q = state1 ? (1), is P3 MV-hard.
2
Proof of Theorem 4.5: Membership (i): Concerning membership, by performing a binary search
on the range [0, max] (where max is an upper bound on the plan costs for a plan of polynomial
length l given by l times the sum of all action costs) we can find out the least integer v such that
any optimistic plan P for P which is admissible wrt. cost v exists (if any optimistic plan exists);
clearly, we have costP (P ) = v and costP = v, and thus any such plan P is optimal. Since max
is single exponential in the representation size of P, the binary search, and thus computing cost P ,
is, by Theorem 4.4, feasible in polynomial time with an NP oracle. Subsequently, we can construct
an optimistic plan P such that costP (P ) = costP by extending a partial plan Pi = hA1 , . . . , Ai i,
i = 0, . . . , l  1 step by step as follows. Let A = {a 1 , . . . , am } be the set of all legal action
instances. We initialize Bi+1 := A and ask the oracle whether Pi can be completed to an optimistic
plan P = hA1 , . . . , Al i admissible wrt. costP such that Ai+1  (Bi+1 \ {a1 }). If the answer is
yes, then we update Bi+1 := Bi+1 \ {a1 }, else we leave Bi+1 unchanged. We then repeat this test
for aj , j = 2, 3, . . . , m; the resulting Bi+1 is an action set such that Pi+1 = hA1 , . . . , Ai , Ai+1 i
where Ai+1 = Bi+1 can be completed to an optimistic plan admissible wrt. cost P . Thus, Ai+1 is
polynomial-time constructible with an NP oracle.
In summary, we can construct an optimal optimistic plan in polynomial time with an NP oracle.
Thus, the problem is in FP2 .
Hardness (i): We show hardness for plan length l = 1 by a reduction from problem MAX WEIGHT
SAT (Papadimitriou, 1994), where an instance is a SAT instance  = c 1      ck as in the proof
of Theorem 4.4.(i), plus positive integer weightsP
w i , where i = 1, . . . , k. Then, S(I) contains those
truth assignments  of X for which wsat () = i : ci =true wi is maximal.
To that end, we take the planning domain PD  as in the proof of Theorem 4.4 and modify the
cost of ci to wi , for i = 1, . . . , k, thus constructing a new planning domain PD I . Consider now the
planning problem PI = hPDI , state1?(1)i. Since the actions cj are the only actions with nonzero
cost, any plan (corresponding toP
a truth assignment ) will be
P associated with the sum of weights
of violated clauses, wvio () = ( ki=1 wi )  wsat (). Since ki=1 wi is constant for I, minimizing
66

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

wvio () is equivalent to maximizing wsat (). Hence, there is a one-to-one correspondence between
optimal optimistic plans of PI (for which wvio () is minimal) and maximal truth assignments for I.
Furthermore, computing PI from I and extracting a MAX-WEIGHT SAT solution from an optimal
plan P is efficiently possible. This proves FP2 -hardness.
Membership (ii): The proof is similar to the membership proof of (i), but uses an oracle which asks
for completion of a partial secure plan P i = hA1 , . . . , Ai i to a secure plan P = hA1 , . . . , Al i such
that Ai+1  (Bi+1 \ {aj }) and P is admissible wrt. costP , rather than of a partial optimistic plan.
This oracle is, as easily seen, in P3 . Thus, computing an optimal secure plan is in F P4 .
Hardness (ii): We show hardness by a reduction from the following problem, which is F P4 complete (cf. (Krentel, 1992)): Given an open QBF Q[Z] = XY [X, Y, Z] like in the proof
of Theorem 4.4.(ii), compute the lexicographically first truth assignment of Z for which Q[Z] is
satisfied.
This can be accomplished by changing the cost of each action az i in PDQ[Z] from 0 to 2ni ,
i = 1, . . . , n. Let PD 0 [Q[Z]] be the resulting planning domain. Since the cost of az i (i.e., assigning
zi the value true) is greater than the sum of the costs of all az j for i + 1  j  n, an optimal
secure plan for the planning problem hPD 0 [Q[Z]], state1 ? (1)i amounts to the lexicographically
first truth assignment for Z such that Q[Z] is satisfied. Thus, FP4 -hardness of the problem follows.
2
Proof of Theorem 6.1: We prove the result by applying the well-known Splitting Set Theorem for
logic programs (Lifschitz & Turner, 1994). This theorem applies to logic programs  that can be
split into two parts such that one of them, the bottom part, does not refer to predicates defined in
the top part at all. The answer sets of the bottom part can then be extended to the answer sets
of the whole program by looking at the remaining (top) rules. Informally, a splitting set of  is
a set U of ground literals defining the bottom part bU () of a program. Each answer set Sb of
bU () can then be used to reduce the remaining rules  \ b U () to a program eU ( \ bU (), Sb )
involving only classical literals which do not occur in b U (), by evaluating the literals from b U ()
wrt. Sb . For each answer set Se of eU ( \ bU (), Sb ), the set S = Sb  Se then is an answer set of
the original program.
Disregarding weak constraints, we can split the program lp w (P) into a bottom part consisting
of lp(Pnc ), where Pnc is P with the cost information stripped off, and a top part containing the
remaining rules; we then derive the correspondence between optimistic plans for P and answer sets
of lpw (P) from a similar correspondence result for lp(P nc ) (Eiter et al., 2003a).
In detail, Theorem 3.1 in (Eiter et al., 2003a) states for any K-planning problem P a correspondence between the answer sets S of lp(P) and supporting trajectories T of optimistic plans
P = hA1 , . . . , Al i as in items (i) and (ii), with costs discarded. Thus, any answer set S 0 of lp(Pnc )
corresponds to some trajectory T 0 of an optimistic plan P 0 for Pnc and vice versa.
In what follows, when talking about lp(P nc ) and lpw (P), we mean the respective grounded
logic programs. lpw (P) augments lp(Pnc ) by rules (4) and weak constraints (5). Let now U =
lit(lp(Pnc )) be the set of all literals occurring in lp(P nc ). Clearly, U splits lpw (P) as defined in
(Lifschitz & Turner, 1994), where we disregard weak constraints in lp w (P), since the rules of form
(4) introduce only new head literals. Consequently, we get b U (lpw (P)) = lp(Pnc ). Then, for any
answer set S 0 of lp(Pnc ), each rule in eU (lpw (P) \ bU (lpw (P)), S 0 ) is of the form
costa (x1 , . . . , xn , t, c) :- Body.
67

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

From the fact that all these rules are positive, we can conclude that with respect to the split by U ,
any answer set S 0 of lp(Pnc ) induces a unique answer set S  S 0 of lpw (P). Therefore, modulo
costs, a correspondence between supporting trajectories T and candidate answer sets S as claimed
follows directly from Theorem 3.1 in (Eiter et al., 2003a).
It remains to prove that costP (P ) = costlpw (P) (S) holds for all candidate answer sets S corresponding to an optimistic plan P = hA 1 , . . . , Al i for P. By the correspondence shown above,
any action p(x1 , . . . , xn )  Aj corresponds to exactly one atom p(x 1 , . . . , xn , j  1)  ASj ,
j  {1, . . . , l}. Therefore, if p(x1 , . . . , xn ) is declared with a non-empty cost part, by (4) and
well-definedness, modulo x1 , . . . , xn , there is exactly one fact costp (x1 , . . . , xn , j  1, c) in the
model of eU (lpw (P) \ bU (lpw (P)), S).
Furthermore, by definition of (4), we have that c = costj (p(x1 , . . . , xn )), i.e., the cost of action
instance p(x1 , . . . , xn ) at time j. Consequently,
Pl the
P violation value of the weak constraint wc of
w
form (5) for p in lp (P) is costwc (S) =
j=1
p(x1 ,...,xn )Aj costj (p(x1 , . . . , xn )). Since all
violation values stem from weak constraints (5), in total we have cost lpw (P) (S) = costP (P ). This
proves the result.
2

References
Blum, A. L., & Furst, M. L. (1997). Fast Planning Through Planning Graph Analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (2000). Planning with Incomplete Information as Heuristic Search in
Belief Space. In Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), Proceedings of the
Fifth International Conference on Artificial Intelligence Planning and Scheduling (AIPS00),
pp. 5261, Breckenridge, Colorado, USA.
Bryant, R. E. (1986). Graph-based algorithms for boolean function manipulation. IEEE Transactions on Computers, C-35(8), 677691.
Buccafurri, F., Leone, N., & Rullo, P. (1997). Strong and Weak Constraints in Disjunctive Datalog.
In Dix, J., Furbach, U., & Nerode, A. (Eds.), Proceedings of the 4th International Conference
on Logic Programming and Non-Monotonic Reasoning (LPNMR97), No. 1265 in Lecture
Notes in AI (LNAI), pp. 217, Dagstuhl, Germany. Springer Verlag.
Buccafurri, F., Leone, N., & Rullo, P. (2000). Enhancing Disjunctive Datalog by Constraints. IEEE
Transactions on Knowledge and Data Engineering, 12(5), 845860.
Cimatti, A., & Roveri, M. (2000). Conformant Planning via Symbolic Model Checking. Journal of
Artificial Intelligence Research, 13, 305338.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity and Expressive Power of
Logic Programming. ACM Computing Surveys, 33(3), 374425.
Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding Planning Problems in Nonmonotonic
Logic Programs. In Proceedings of the European Conference on Planning 1997 (ECP-97),
pp. 169181. Springer Verlag.
Eiter, T., Faber, W., Leone, N., & Pfeifer, G. (2000a). Declarative Problem-Solving Using the
DLV System. In Minker, J. (Ed.), Logic-Based Artificial Intelligence, pp. 79103. Kluwer
Academic Publishers.
68

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2000b). Planning under Incomplete
Knowledge. In Lloyd, J., Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C.,
Pereira, L. M., Sagiv, Y., & Stuckey, P. J. (Eds.), Computational Logic - CL 2000, First International Conference, Proceedings, No. 1861 in Lecture Notes in AI (LNAI), pp. 807821,
London, UK. Springer Verlag.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002a). Answer Set Planning under
Action Costs. In Flesca, S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings of the
8th European Conference on Artificial Intelligence (JELIA), No. 2424 in Lecture Notes in
Computer Science, pp. 186197.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002b). Answer Set Planning under Action Costs. Tech. rep. INFSYS RR-1843-02-13, Institut fur Informationssysteme, Technische
Universitat Wien.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003a). A Logic Programming Approach
to Knowledge-State Planning, II: the DLV K System. Artificial Intelligence, 144(12), 157
211.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003b). A Logic Programming Approach
to Knowledge-State Planning: Semantics and Complexity. To appear in ACM Transactions
on Computational Logic.
Ephrati, E., Pollack, M. E., & Mihlstein, M. (1996). A Cost-directed Planner: Preliminary Report.
In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96),
pp. 1223  1228. AAAI Press.
Erdem, E. (1999). Applications of Logic Programming to Planning: Computational Experiments.
Unpublished draft. http://www.cs.utexas.edu/users/esra/papers.html.
Faber, W., & Pfeifer, G. (since 1996). DLV homepage.. http://www.dlvsystem.com/.
Ferraris, P., & Giunchiglia, E. (2000). Planning as Satisfiability in Nondeterministic Domains. In
Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI00), July
30  August 3, 2000, Austin, Texas USA, pp. 748753. AAAI Press / The MIT Press.
Fourer, R., Gay, D. M., & Kernighan, B. W. (1993). AMPL: A Modeling Language for Mathematical
Programming. Duxbury Press.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation in Logic Programs and Disjunctive
Databases. New Generation Computing, 9, 365385.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld,
D., & Wilkins, D. (1998).
PDDL  The Planning Domain Definition language. Tech. rep., Yale Center for Computational Vision and Control. Available at
http://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz.
Giunchiglia, E. (2000). Planning as Satisfiability with Expressive Action Languages: Concurrency,
Constraints and Nondeterminism. In Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), Proceedings of the Seventh International Conference on Principles of Knowledge Representation
and Reasoning (KR 2000), April 12-15, Breckenridge, Colorado, USA, pp. 657666. Morgan
Kaufmann.
69

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing Action: Indeterminacy and
Ramifications. Artificial Intelligence, 95, 409443.
Giunchiglia, E., & Lifschitz, V. (1998). An Action Language Based on Causal Explanation: Preliminary Report. In Proceedings of the Fifteenth National Conference on Artificial Intelligence
(AAAI 98), pp. 623630.
Haslum, P., & Geffner, H. (2000). Admissible Heuristics for Optimal Planning. In Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), Proceedings of the Fifth International Conference on
Artificial Intelligence Planning and Scheduling (AIPS00), pp. 140149, Breckenridge, Colorado, USA. AAAI Press.
Kautz, H., & Walser, J. P. (1999). State-space planning by integer optimization. In Proceedings of
the 16th National Conference on Artificial Intelligence (AAAI-99), pp. 526533.
Koehler, J. (1998). Planning Under Resource Constraints. In Proceedings of the 13th European
Conference on Artificial Intelligence (ECAI98), pp. 489493.
Krentel, M. (1992). Generalizations of Opt P to the Polynomial Hierarchy. Theoretical Computer
Science, 97(2), 183198.
Lee, J., & Lifschitz, V. (2001). Additive Fluents. In Provetti, A., & Cao, S. T. (Eds.), Proceedings
AAAI 2001 Spring Symposium on Answer Set Programming: Towards Efficient and Scalable
Knowledge Representation and Reasoning, pp. 116123, Stanford, CA. AAAI Press.
Lifschitz, V., & Turner, H. (1994). Splitting a Logic Program. In Van Hentenryck, P. (Ed.), Proceedings of the 11th International Conference on Logic Programming (ICLP94), pp. 2337,
Santa Margherita Ligure, Italy. MIT Press.
Lifschitz, V., & Turner, H. (1999). Representing Transition Systems by Logic Programs. In Gelfond,
M., Leone, N., & Pfeifer, G. (Eds.), Proceedings of the 5th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR99), No. 1730 in Lecture Notes in AI
(LNAI), pp. 92106, El Paso, Texas, USA. Springer Verlag.
Lifschitz, V. (1996). Foundations of Logic Programming. In Brewka, G. (Ed.), Principles of Knowledge Representation, pp. 69127. CSLI Publications, Stanford.
Lifschitz, V. (1999a). Action Languages, Answer Sets and Planning. In Apt, K., Marek, V. W.,
Truszczynski, M., & Warren, D. S. (Eds.), The Logic Programming Paradigm  A 25-Year
Perspective, pp. 357373. Springer Verlag.
Lifschitz, V. (1999b). Answer Set Planning. In Schreye, D. D. (Ed.), Proceedings of the 16th
International Conference on Logic Programming (ICLP99), pp. 2337, Las Cruces, New
Mexico, USA. The MIT Press.
McCain, N. (1999). The Causal Calculator Homepage.. http://www.cs.utexas.edu/
users/tag/cc/.
McCain, N., & Turner, H. (1997). Causal Theories of Actions and Change. In Proceedings of the
15th National Conference on Artificial Intelligence (AAAI-97), pp. 460465.
McCain, N., & Turner, H. (1998). Satisfiability Planning with Causal Theories. In Cohn, A. G.,
Schubert, L., & Shapiro, S. C. (Eds.), Proceedings Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR98), pp. 212223. Morgan Kaufmann
Publishers.
70

fiA NSWER S ET P LANNING U NDER ACTION C OSTS

Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an
Efficient SAT Solver. In Proceedings of the 38th Design Automation Conference, DAC 2001,
Las Vegas, NV, USA, June 18-22, 2001, pp. 530535. ACM.
Nareyek, A. (2001). Beyond the Plan-Length Criterion. In Local Search for Planning and Scheduling, ECAI 2000 Workshop, Vol. 2148 of Lecture Notes in Computer Science, pp. 5578.
Springer.
Niemela, I. (1998). Logic Programs with Stable Model Semantics as a Constraint Programming
Paradigm. In Niemela, I., & Schaub, T. (Eds.), Proceedings of the Workshop on Computational Aspects of Nonmonotonic Reasoning, pp. 7279, Trento, Italy.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Pednault, E. P. D. (1989). Exploring the Middle Ground between STRIPS and the Situation Calculus. In Proceedings of the 1st International Conference on Principles of Knowledge Representation and Reasoning (KR89), pp. 324332, Toronto, Canada. Morgan Kaufmann Publishers,
Inc.
Refanidis, I., & Vlahavas, I. (2001). A Framework for Multi-Criteria Plan Evaluation in Heuristic
State-Space Planning. In IJCAI-01 Workshop on Planning with Resources.
Selman, A. L. (1994). A Taxonomy of Complexity Classes of Functions. Journal of Computer and
System Sciences, 48(2), 357381.
Simons, P., Niemela, I., & Soininen, T. (2002). Extending and Implementing the Stable Model
Semantics. Artificial Intelligence, 138, 181234.
Smith, D. E., & Weld, D. S. (1998). Conformant Graphplan. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence, (AAAI98), pp. 889896. AAAI Press / The
MIT Press.
Son, T. C., & Pontelli, E. (2002). Reasoning About Actions in Prioritized Default Theory. In Flesca,
S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings of the 8th European Conference on
Artificial Intelligence (JELIA), No. 2424 in Lecture Notes in Computer Science, pp. 369381.
Subrahmanian, V., & Zaniolo, C. (1995). Relating Stable Models and AI Planning Domains. In
Sterling, L. (Ed.), Proceedings of the 12 th International Conference on Logic Programming,
pp. 233247, Tokyo, Japan. MIT Press.
van Gelder, A., Ross, K., & Schlipf, J. (1991). The Well-Founded Semantics for General Logic
Programs. Journal of the ACM, 38(3), 620650.
Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending Graphplan to Handle Uncertainty
& Sensing Actions. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, (AAAI98), pp. 897904. AAAI Press / The MIT Press.
Williams, M., & Hanks, S. (1994). Optimal Planning with a Goal-Directed Utility Model. In
Hammond, K. J. (Ed.), Proceedings of the Second International Conference on Artificial Intelligence Planning Systems (AIPS-94), pp. 176181. AAAI Press.

71

fiJournal of Artificial Intelligence Research 19 (2003) 155-203

Submitted 1/03; published 9/03

Representing and Aggregating Conflicting Beliefs
Pedrito Maynard-Zhang

maynarp@muohio.edu

Department of Computer Science and Systems Analysis
Miami University
Oxford, Ohio 45056, USA

Daniel Lehmann

lehmann@cs.huji.ac.il

School of Computer Science and Engineering
Hebrew University
Jerusalem 91904, Israel

Abstract
We consider the two-fold problem of representing collective beliefs and aggregating
these beliefs. We propose a novel representation for collective beliefs that uses modular,
transitive relations over possible worlds. They allow us to represent conflicting opinions
and they have a clear semantics, thus improving upon the quasi-transitive relations often
used in social choice. We then describe a way to construct the belief state of an agent
informed by a set of sources of varying degrees of reliability. This construction circumvents
Arrows Impossibility Theorem in a satisfactory manner by accounting for the explicitly
encoded conflicts. We give a simple set-theory-based operator for combining the information of multiple agents. We show that this operator satisfies the desirable invariants of
idempotence, commutativity, and associativity, and, thus, is well-behaved when iterated,
and we describe a computationally effective way of computing the resulting belief state.
Finally, we extend our framework to incorporate voting.

1. Introduction
We are interested in the multi-agent setting where agents are informed by sources of varying
levels of reliability, and where agents can iteratively combine their belief states. This setting
introduces three problems: (1) Finding an appropriate representation for collective beliefs;
(2) Constructing an agents belief state by aggregating the information from informant
sources, accounting for the relative reliability of these sources; and, (3) Combining the
information of multiple agents in a manner that is well-behaved under iteration.
In addressing the first problem, we take as a starting point total preorders over possible
worlds (i.e., interpretations of a specified language) used in the belief revision community to
represent individuals beliefs. The relations describe opinions on the relative likelihood of
worlds and can be viewed as encoding all of an agents conditional beliefs, i.e., not only what
he believes now, but what he would believe under all other conditions. This representation
is based on the semantical work (cf. Grove, 1988; Katsuno & Mendelzon, 1991) supporting
the Alchourron, Gardenfors, and Makinson proposal (Alchourron, Gardenfors, & Makinson,
1985; Gardenfors, 1988) (known as the AGM theory) for belief revision.
The social choice community has dealt extensively with the problem of representing
collective preferences (cf. Sen, 1986). However, the problem is formally equivalent to that
of representing collective beliefs, so the results are applicable. The classical approach has
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiMaynard-Zhang & Lehmann

been to use quasi-transitive relations  relations whose asymmetric restrictions are transitive
 over the set of objects. (Total preorders are a special subclass of these relations.) However,
these relations do not distinguish between group indifference and group conflict, and this
distinction can be crucial. Consider, for example, a situation in which all members of a
group are indifferent between movie a and movie b. If some passerby expresses a preference
for a, the group may very well choose to adopt this opinion for the group and borrow a.
However, if the group was already divided over the relative merits of a and b, we would be
wise to hesitate before choosing one over the other just because a new supporter of a appears
on the scene. We propose a representation in which the distinction is explicit. Specifically,
we propose modular, transitive relations and argue that they solve some of the unpleasant
semantical problems suffered by the earlier approach. (We define modularity precisely later,
but it can be viewed intuitively as a sufficient relaxation of the totality requirement on total
preorders to make the distinction between indifference and conflict possible.)
The second problem addresses how an agent should actually go about combining the
information received from a set of sources to create a belief state. Such a mechanism should
favor the opinions held by more reliable sources, yet allow less reliable sources to voice opinions when higher ranked sources have no opinion. True, under some circumstances it would
not be advisable for an opinion from a less reliable source to override the agnosticism of a
more reliable source, but often it is better to accept these opinions as default assumptions
until better information is available. We define a mechanism that does just this, relying on
our generalized representation to circumvent Arrows (1963) Impossibility Theorem when
there are sources of equal reliability.
To motivate the third problem, consider the following dynamic scenario: A robot controlling a ship in space receives from a number of communication centers on Earth information
about the status of its environment and tasks. Each center receives information from a
group of sources of varying credibility or accuracy (e.g., nearby satellites and experts) and
aggregates it. Timeliness of decision-making in space is often crucial, so we do not want
the robot to have to wait while each center sends its information to some central location
for it to be first combined before being forwarded to the robot. Instead, each center sends
its aggregated information directly to the robot. Not only does this scheme reduce dead
time, it also allows for anytime behavior on the robots part: the robot incorporates new
information as it arrives and makes the best decisions it can with whatever information it
has at any given point. This distributed approach is also more robust since the degradation
in performance is much more graceful should information from individual centers get lost
or delayed.
In such a scenario, the robot needs a mechanism for combining or fusing the belief states
of multiple agents potentially arriving at different times. Moreover, the belief state output
by the mechanism should be invariant with respect to the order of agent arrivals. We will
describe a simple set-theoretic mechanism that satisfies these requirements as well as a
computationally effective way of computing the resulting belief state.
The aggregation and fusion mechanisms described so far take into account quality of
support for opinions, but completely ignore quantity of support. However, the latter often provides sufficient information to resolve apparent conflicts. Take, for example, the
situation where all the sources for the robot above have equal credibility and all except a
small minority suggest the robot move the spaceship to avoid a potential collision with an
156

fiRepresenting and Aggregating Conflicting Beliefs

oncoming asteroid. In such a situation, we often prefer to resolve the conflict by siding with
the majority. To this end, we describe how to extend our framework to allow for voting,
introducing a novel modular closure operation in the process.
After some preliminary definitions, we address each of these topics in turn.

2. Preliminaries
We begin by defining various well-known properties of binary relations1 ; they will be useful
to us throughout the paper.
Definition 1 Suppose  is a relation over a finite set , i.e.,    .2 We will use
x  y to denote (x, y)  and x 6 y to denote (x, y) 6. The relation  is:
1. reflexive iff x  x for all x  . It is irreflexive iff x 6 x for all x  .
2. symmetric iff x  y  y  x for all x, y  . It is asymmetric iff x  y  y 6 x for
all x, y  . It is anti-symmetric iff x  y  y  x  x = y for all x, y  .
3. the asymmetric restriction of a relation 0 over  iff x  y  x 0 y  y 60 x for
all x, y  . It is the symmetric restriction of 0 iff x  y  x 0 y  y 0 x for all
x, y  .
4. total iff x  y  y  x for all x, y  .
5. modular iff x  y  x  z  z  y for all x, y, z  .
6. transitive iff x  y  y  z  x  z for all x, y, z  .
7. quasi-transitive iff its asymmetric restriction is transitive.
8. the transitive closure of a relation 0 over  iff, for some integer n,
x  y  w0 , . . . , wn  . x = w0 0    0 wn = y
for all x, y  . (We generally use + to denote the transitive closure of a relation
.)
9. acyclic iff w0 , . . . , wn  . w0 <    < wn implies wn 6< w0 for all integers n, where
< is the asymmetric restriction of .
10. a total preorder iff it is total and transitive. It is a total order iff it is also antisymmetric.
11. an equivalence relation iff it is reflexive, symmetric, and transitive.
12. fully connected iff x  y for all x, y  . It is fully disconnected iff x 6 y for all
x, y  .
Proposition 1
1. The transitive closure of a modular relation is modular.
1. We only use binary relations in this paper, so we will refer to them simply as relations.
2. For the readers convenience, we have included in Appendix B a key to most of the notational symbols
used throughout the paper.

157

fiMaynard-Zhang & Lehmann

2. Every transitive relation is quasi-transitive.
3. (Sen, 1986) Every quasi-transitive relation is acyclic.
Given a relation over a set of alternatives and a subset of these alternatives, we often
want to pick the subsets best elements with respect to the relation. We define this set
of best elements to be the subsets choice set:
Definition 2 If  is a relation over a finite set , < is its asymmetric restriction, and
X  , then the choice set of X with respect to  is
ch(X, ) = {x  X :6 x0  X. x0 < x}.
A choice function is one which assigns to every (non-empty) subset X a (non-empty) subset
of X:
Definition 3 A choice function over a finite set  is a function f : 2 \   2 \  such
that f (X)  X for every non-empty X  .
Now, every acyclic relation defines a choice function, one which assigns to each subset its
choice set:
Proposition 2 (Sen, 1986) Given a relation  over a finite set , the choice set operation
ch defines a choice function iff  is acyclic.3
If a relation is not acyclic, elements involved in a cycle are said to be in a conflict because
we cannot order them:
Definition 4 Given a relation < over a finite set , x and y are in a conflict wrt < iff there
exist w0 , . . . , wn , z0 , . . . , zm   such that x = w0 <    < wn = y = z0 <    < zm = x, where
x, y  .
Finally, the cardinality of a set  will be denoted kk.
Assume we are given some language L with a satisfaction relation |= for L. Let
W be a finite, non-empty set of possible worlds (interpretations) over L. For a world
w  W and a sentence p  L, w |= p iff p evaluates to true in w. Given a sentence p,
|p| = {w  W | w |= p}.

3. Representing Collective Beliefs
Our representation of collective beliefs generalizes the representation developed in the belief
revision community for the conditional beliefs of an individual, so we briefly review it. We
then consider implications from social choice for representing collective beliefs. Finally, we
describe our proposal and argue for its desirability.
3. Sens uses a slightly stronger definition of choice sets, but the theorem still holds in our more general
case.

158

fiRepresenting and Aggregating Conflicting Beliefs

3.1 Belief Revision Representation of Conditional Beliefs
Much of the belief revision field has built on the seminal work by Alchourron, Gardenfors,
and Makinson (Alchourron et al., 1985; Gardenfors, 1988) refered to as the AGM theory.
This work sought to formalize an Occams razor-like principle of minimal change: the set
of beliefs resulting from a revision should be one produced by modifying the original beliefs
minimally to accomodate the new information. To capture this principle precisely, they
proposed the famous AGM postulates which impose restrictions on belief change operators.
Subsequent model-theoretic work (Grove, 1988; Katsuno & Mendelzon, 1991; MaynardReid II & Shoham, 2001) showed that accepting these postulates amounts to assuming that
an individuals belief state is represented by a total preorder  over W; revision of the
individuals beliefs by a sentence p  L then consists of computing ch(|p|, ).
Kraus, Lehmann, and Magidor (1990) and Lehmann and Magidor (1992) developed a
similar central role for ordered structures in the semantics of nonmonotonic logics, and
Gardenfors and Makinson (1994) established the relation between the two topics. Semantically,  represents the weak relative likelihood of possible worlds: x  y means possible
world x is considered to be at least as likely as possible world y.4 If x  y and y  x, then
x and y are considered equally likely. We can also interpret  sententially using the famous
Ramsey Test (Ramsey, 1931): it encodes a set of conditional beliefs, i.e., not only what
is believed now (called the belief set), but all counterfactual beliefs as well (what would
be believed if other conditions were the case). According to this criteria, the conditional
belief if p then q holds if p and q are sentences in L and q is satisfied by all the worlds in
ch(|p|, ); we write Bel(p?q). If neither the belief p?q nor the belief p?q hold in the belief
state, it is said to be agnostic with respect to p?q, written Agn(p?q). The belief set induced
by the belief state consists of all those sentences q such that Bel(true?q) holds.
3.2 Social Choice Implications
Our first inclination, then, would be to use total preorders to represent collective beliefs
since they work so well for individuals beliefs. Unfortunately, such an approach is inherently
problematic as was discovered early on in the social choice community. That communitys
interest lies in representing collective preferences rather than collective beliefs; however, the
results are equally relevant since the classical representation of an individuals preferences is
also a total preorder. Instead of relative likelihood, relations represent relative preference;
instead of equal likelihood, indifference.
Arrows (1963) celebrated Impossibility Theorem showed that no aggregation operator
over total preorders exists satisfying the following small set of desirable properties:
Definition 5 Let f be an aggregation operator over the relations 1 , . . . , n of n individuals, respectively, over a finite set of alternatives , and let  = f (1 , . . . , n ).
 Restricted Range: The range of f is the set of total preorders over .
 Unrestricted Domain: The domain of f is the set of n-tuples of total preorders over
.
4. The direction of the relation symbol is unintuitive, but standard practice in the belief revision community.

159

fiMaynard-Zhang & Lehmann

 Pareto Principle: If x i y for all i, then x  y.5
 Independence of Irrelevant Alternatives (IIA): Suppose 0 = f (01 , . . . , 0n ). If, for
x, y  , x i y iff x 0i y for all i, then x  y iff x 0 y.
 Non-Dictatorship: There is no individual i such that, for every tuple in the domain
of f and every x, y  , x i y implies x  y.
Proposition 3 (Arrow, 1963) There is no aggregation operator that satisfies restricted
range, unrestricted domain, Pareto principle, independendence of irrelevant alternatives,
and nondictatorship.
This impossibility theorem led researchers to look for weakenings to Arrows framework that
would circumvent the result. One was to weaken the restricted range condition, requiring
that the result of an aggregation only satisfy totality and quasi-transitivity rather than the
full transitivity of a total preorder. This weakening was sufficient to guarantee the existence
of an aggregation function satisfying the other conditions, while still producing relations
that defined choice functions (Sen, 1986). However, this solution was not without its own
problems.
First, and perhaps most obviously, the domain and the range of the aggregation operator
are different, violating what is known in the belief revision literature as the principle of
categorical matching (cf. Gardenfors and Rotts 1995 survey). This problem is closely related
to the second which is that total, quasi-transitive relations have unsatisfactory semantics.
If  is total and quasi-transitive but not a total preorder, its indifference relation is not
transitive:
Proposition 4 Let  be a relation over a finite set  and let  be its symmetric restriction.
If  is total and quasi-transitive but not transitive, then  is not transitive.
There has been much discussion as to whether or not indifference should be transitive.
In many cases one feels indifference should be transitive; if Deb is indifferent between
plums and mangoes and also indifferent between mangoes and peaches, we would be greatly
surprised were she to profess a strong preference for plums over peaches.6 Thus, it seems
that total quasi-transitive relations that are not total preorders cannot be understood easily
as preference or indifference. Since the existence of a choice function is generally sufficient
for classical social choice problems, these issues were at least ignorable. However, in iterated
aggregation, the result of the aggregation must not only be usable for making decisions, but
must be interpretable as a new preference relation that may be involved in later aggregations
and, consequently, must maintain clean semantics.
Third, the totality assumption is excessively restrictive for representing aggregate preferences. In general, a binary relation  can express four possible relationships between a
pair of alternatives a and b: a  b and b 6 a, b  a and a 6 b, a  b and b  a, and a 6 b
and b 6 a. Totality reduces this set to the first three which, under the interpretation of
5. Technically, this is known as the weak Pareto principle. The strong Pareto principle states that x  y
if there exists i such that x i y and x i y for all i. Obviously, the strong version implies the weak
version, so Arrows theorem applies to it as well.
6. However, see Luces (1956) work on semiorders for some of the opposing arguments in the transitivity
of indifference debate.

160

fiRepresenting and Aggregating Conflicting Beliefs

relations as representing weak preference, correspond to the two strict orderings of a and b,
and indifference. However, consider the situation where a couple is trying to choose between
an Italian and an Indian restaurant, but one strictly prefers Italian food to Indian food,
whereas the second strictly prefers Indian to Italian. The couples opinions are in conflict,
a situation that does not fit into any of the three categories. Thus, the totality assumption
is essentially an assumption that conflicts do not exist. This, one may argue, is appropriate
if we want to represent preferences of one agent (but see Kahneman and Tverskys (1979)
persuasive arguments that individuals are often ambivalent). However, the assumption is
inappropriate if we want to represent aggregate preferences since individuals will almost
certainly have differences of opinion.
3.3 Generalized Belief States
Because belief aggregation is formally similar to preference aggregation, it is also susceptible to the problems faced by the social choice community. We take the view that much of
the difficulty encountered in previous attempts to define acceptable aggregation policies has
been the lack of explicit representations of conflicts among the individuals. We generalize
the total preorder representation so as to capture information about conflicts. This generalization opens the way for semantically clear aggregation policies, with the added benefit
of focusing attention on the culprit sets of worlds.
3.3.1 Modular, Transitive States
We take strict likelihood as primitive. Since strict likelihood is not necessarily total, it
is possible to represent agnosticism and conflicting opinions in the same structure. This
choice deviates from that of most authors, but is similar to that of Kreps (1990, p. 19) who is
interested in representing both indifference and incomparability. Unlike Kreps, rather than
use an asymmetric relation to represent strict likelihood (e.g., the asymmetric restriction of
a weak likelihood relation), we impose the less restrictive condition of modularity.
We formally define generalized belief states:
Definition 6 A generalized belief state  is a modular, transitive relation over W. The
set of possible generalized belief states over W is denoted B.
We interpret a  b to mean there is reason to consider a as strictly more likely than b.
We represent equal likelihood, which we also refer to as agnosticism, with the relationship
 defined such that x  y if and only if x 6 y and y 6 x. We define the conflict relation
corresponding to , denoted ./, so that x ./ y iff x  y and y  x. It describes situations
where there are reasons to consider either of a pair of worlds as strictly more likely than
the other. In fact, one can easily check that ./ precisely represents conflicts in a belief state
in the sense of Definition 4.
For convenience, we will refer to generalized belief states simply as belief states except
when to do so would cause confusion.
3.3.2 Discussion
Let us consider why our choice of representation is justified. First, we agree with the social
choice community that strict likelihood should be transitive.
161

fiMaynard-Zhang & Lehmann

As we discussed above, there is often no compelling reason why agnosticism/indifference
should not be transitive; we also adopt this view. However, transitivity of strict likelihood
by itself does not guarantee transitivity of agnosticism. A simple example is the following:
W = {a, b, c} and = {(a, c)}, so that = {(a, b), (b, c)}. However, if we buy that strict likelihood should be transitive, then agnosticism is transitive identically when strict likelihood
is also modular:
Proposition 5 Suppose a relation  is transitive and  is the corresponding agnosticism
relation. Then  is transitive iff  is modular.
In summary, transitivity and modularity are necessary if strict likelihood and agnosticism
are both required to be transitive.
We should point out that conflicts are also transitive in our framework. At first glance,
this may appear undesirable: it is entirely possible for a group to disagree on the relative
likelihood of worlds a and b, and b and c, yet agree that a is more likely than c. However, we
note that this transitivity follows from the cycle-based definition of conflicts (Definition 4),
not from our belief state representation. It highlights the fact that we are not only concerned
with conflicts that arise from simple disagreements over pairs of alternatives, but those that
can be inferred from a series of inconsistent opinions as well.
Now, to argue that modular, transitive relations are sufficient to capture relative likelihood, agnosticism, and conflicts among a group of information sources, we first point out
that adding irreflexivity would give us the class of relations that are asymmetric restrictions
of total preorders, i.e., conflict-free. Let T be the set of total preorders over W, T< , the set
of their asymmetric restrictions.
Proposition 6 T<  B and is the set of irreflexive relations in B.
Secondly, the following representation theorem shows that each belief state partitions
the possible worlds into sets of worlds either all equally likely or all potentially involved in
a conflict, and totally orders these sets; worlds in distinct sets have the same relation to
each other as do the sets.
Proposition 7  B iff there is a partition W = hW0 , . . . , Wn i of W such that:
1. For every x  Wi and y  Wj , i 6= j implies i < j iff x  y.
2. Every Wi is either fully connected (w  w0 for all w, w0  Wi ) or fully disconnected
(w 6 w0 for all w, w0  Wi ).
Figure 1 shows three examples of belief states: one which is a total preorder, one which
is the asymmetric restrictions of a total preorder, and one which is neither. (Each circle
represents all the worlds in W which satisfy the sentence inside. An arc between circles
indicates that w  w0 for every w in the head circle and w0 in the tail circle; no arc indicates
that w 6 w0 for each of these pairs. In particular, the set of worlds represented by a circle
is fully connected if there is an arc from the circle to itself, fully disconnected otherwise.)
Thus, generalized belief states are not a big change from the asymmetric restrictions
of total preorders. They merely generalize these by weakening the assumption that sets of
worlds not strictly ordered are equally likely, allowing for the possibility of conflicts. Now
we can distinguish between agnostic and conflicting conditional beliefs. A belief state  is
162

fiRepresenting and Aggregating Conflicting Beliefs

P

P

P

P

P

P

(a)

(b)

(c)

Figure 1: Three examples of generalized belief states: (a) a total preorder, (b) the asymmetric restriction of a total preorder, (c) neither.

agnostic about conditional belief p?q (i.e., Agn(p?q)) if the choice set of worlds satisfying
p contains both worlds which satisfy q and q and is fully disconnected. It is in conflict
about this belief, written Con(p?q), if the choice set is fully connected.
Finally, we compare the representational power of our definitions to those discussed in
the previous section. First, as a companion result to Proposition 6, it is obvious that B
subsumes the class of total preorders T and, in fact, T is the set of reflexive relations in B.
Proposition 8 T  B and is the set of reflexive relations in B.
Secondly, B neither subsumes nor is subsumed by the set of total, quasi-transitive relations,
and the intersection of the two classes is T . Let Q be the set of total, quasi-transitive
relations over W, and Q< , the set of their asymmetric restrictions.
Proposition 9
1. Q  B = T .
2. B 6 Q.
3. Q 6 B if W has at least three elements.
4. Q  B if W has one or two elements.
Because modular, transitive relations represent strict preferences, it is probably fairer to
compare them to the class of asymmetric restrictions of total, quasi-transitive relations.
Again, neither class subsumes the other, but this time the intersection is T< :
Proposition 10
1. Q<  B = T< .
2. B 6 Q< .
3. Q< 6 B if W has at least three elements.
4. Q<  B if W has one or two elements.
Note that generalized belief states as described are extremely rich and would require
optimization in practice to avoid high maintenance cost. Although this issue is somewhat
outside the scope of this paper, we do address (in the respective sections) ways to minimize
the further explosion of this complexity when the complications of fusion and voting are
introduced.
In the next section, we define a natural aggregation policy based on this new representation that admits clear semantics and obeys appropriately modified versions of Arrows
conditions.
163

fiMaynard-Zhang & Lehmann

4. Single-Agent Belief State Construction
Suppose an agent is informed by a set of sources, each with its individual belief state.
Suppose, further, that the agent has ranked the sources by level of credibility. We propose
an operator for constructing the agents belief state by aggregating the belief states of the
sources while accounting for the credibility ranking of the sources.
Example 1 We will use a running example from our space robot domain to help provide
intuition for our definitions. The robot sends to earth a stream of telemetry data gathered
by the spacecraft, as long as it receives positive feedback that the data is being received. At
some point it loses contact with the automatic feedback system, so it sends a request for
information to an agent on earth to find out if the failure was caused by a failure of the
feedback system or by an overload of the data retrieval system. In the former case, it would
continue to send data, in the latter, desist. As it so happens, there has been no overload,
but the computer running the feedback system has hung. The agent consults the following
three experts, aggregates their beliefs, and sends the results back to the robot:
1. sp , the computer programmer that developed the feedback program, believes nothing
could ever go wrong with her code, so there must have been an overload problem.
However, she admits that if her program had crashed, the problem could ripple through
to cause an overload.
2. sm , the manager for the telemetry division, unfortunately has out-dated information
that the feedback system is working. She was also told by the engineer who sold her
the system that overloading could never happen. She has no idea what would happen
if there was an overload or the feedback system crashed.
3. st , the technician working on the feedback system, knows that the feedback system
crashed, but doesnt know whether there was a data-overload. Not being familiar with
the retrieval system, she is also unable to speculate whether the data retrieval system
would have overloaded if the feedback system had not failed.
Let F and D be propositional variables representing that the feedback and data retrieval
systems, respectively, are okay. The belief states for the three sources are shown in Figure 2.
FD

FD

FD

F

D

F

FD

FD

F

sp

sm

st

Figure 2: The belief states of sp , sm , and st in Example 1.

164

fiRepresenting and Aggregating Conflicting Beliefs

4.1 Sources
Let us begin the formal development by defining sources and their belief states:
Definition 7 S is a finite set of sources. With each source s  S is associated a belief state
<s  B.
We denote the agnosticism and conflict relations of a source s by s and ./s , respectively. It
is possible to assume that the belief state of a source is conflict-free, i.e., acyclic. However,
this is not necessary if we allow sources to suffer from the human malady of being torn
between possibilities.
We assume that the agents credibility ranking over the sources is a total preorder built
on a totally ordered set of ranks (e.g., integers).
Definition 8 R is a totally ordered finite set of ranks.
Definition 9 rank : S  R assigns to each source a rank. Also, for S  S, ranks(S) denotes the set {r  R : s  S. rank(s) = r}.
Definition 10 The total preorder over S induced by the ordering over R will be denoted
w. That is, s w s0 iff rank(s)  rank(s0 ); we say s is as credible as s0 . The restriction of w
to S  S will be denoted wS .
We use A and  to denote the asymmetric and symmetric restrictions of w, respectively.7
The finiteness of S (R) ensures that a maximal source (rank) always exists, which is necessary for some of our results. Weaker assumptions are possible, but at the price of unnecessarily complicating the discussion. Also observe that R can be any arbitrary totally
ordered set. Thus, not only does it allow for numeric ranking systems (such as the integers),
but non-numeric systems as well (e.g., military ranks). Furthermore, this generality allows
our proposal to easily accommodate applications where new ranks need to be dynamically
added and it is inconvenient or impossible to change the rank labels of existing sources
(e.g., a large workers union where members are ranked by relative level and quality of
experience).
We are now ready to consider the source aggregation problem. In the following, assume
an agent is informed by a set of sources S  S. We look at two special casesaggregation
of equally ranked and strictly ranked sourcesbefore considering the general case.
4.2 Aggregating Equally Ranked Sources
Suppose all the sources have the same rank so that wS is fully connected. Intuitively, we
want to take all offered opinions seriously, so we take the union of the relations:
S
Definition 11 If S  S, then Un(S) is the relation sS <s .
By simply taking the union of the source belief states, we may lose transitivity. However,
we do not lose modularity:
Proposition 11 If S  S, then Un(S) is modular but not necessarily transitive.
7. Note that, unlike the relations representing belief states,  and w are read in the intuitive way, that is,
greater corresponds to better.

165

fiMaynard-Zhang & Lehmann

Thus, we know from Proposition 1 that we need only take the transitive closure of Un(S)
to get a belief state:
Definition 12 If S  S, then AGRUn(S) is the relation Un(S)+ .
Proposition 12 If S  S, then AGRUn(S)  B.
Intuitively, we are simply inferring opinions implied by the conflicts introduced by the
aggregation. We will show this formally when we consider the more general aggregation
operator below.
Not surprisingly, by taking all opinions of all sources seriously, we may generate many
conflicts, manifested as fully connected subsets of W.
Example 2 Suppose all three sources in the space robot scenario of Example 1 are considered equally credible, then the aggregate belief state will be the fully connected relation
indicating that there are conflicts over every belief.
4.3 Aggregating Strictly Ranked Sources
Next, consider the case where the sources are strictly ranked, i.e., wS is a total order. We
define a lexicographic operator such that lower ranked sources refine the belief states of
higher ranked sources. That is, in determining the ordering of a pair of worlds, the opinions
of higher ranked sources generally override those of lower ranked sources, and lower ranked
sources are consulted when higher ranked sources are agnostic:
Definition 13 If S  S, then AGRRf(S) is the relation
n

o
0
(x, y) : s  S. x <s y  s0  S. s0 AS s  x s y .
As with AGRUn(S), AGRRf(S) is not guaranteed to be transitive, but it is always modular:
Proposition 13 If S  S, then AGRRf(S) is modular but not necessarily transitive.
However, in the case that wS is a total order, the result of applying AGRRf is guaranteed
to be a belief state.
Proposition 14 If S  S and wS is a total order, then AGRRf(S)  B.
Example 3 Suppose, in the space robot scenario of Example 1, the technician is considered more credible than the manager who, in turn, is considered more credible than the
programmer. The aggregate belief state, shown in Figure 3, informs the robot (correctly)
that the feedback system has crashed, but that it shouldnt worry about an overload problem
and should keep sending data.
4.4 General Aggregation
In the general case, we may have several ranks represented and multiple sources of each rank.
It will be instructive to first consider the following seemingly natural strawman operator,
AGR : First combine equally ranked sources using AGRUn, then aggregate the strictly
ranked results using what is essentially AGRRf.
166

fiRepresenting and Aggregating Conflicting Beliefs

FD

FD

FD

FD

Figure 3: The belief state after aggregation in Example 3 when st A sm A sp .
Definition 14 Let S  S. For any r  R, let <r = AGRUn({s  S : rank(s) = r}) and
r , the corresponding agnosticism relation. AGR (S) is the relation



(x, y) : r  R. x <r y  r0  ranks(S). r0 > r  x r0 y .
AGR indeed defines a legitimate belief state:
Proposition 15 If S  S, then AGR (S)  B.
Unfortunately, a problem with this divide-and-conquer approach is it assumes the
result of aggregation is independent of potential interactions between the individual sources
of different ranks. Consequently, opinions that will eventually get overridden may still have
an indirect effect on the final aggregation result by introducing superfluous opinions during
the intermediate equal-rank aggregation step, as the following example shows:
Example 4 Let W = {a, b, c}. Suppose S  S such that S = {s0 , s1 , s2 } with belief states
<s0 = {(b, a), (b, c)} and <s1 =<s2 = {(a, b), (c, b)}, and where s2 A s1  s0 . Then AGR (S)
is {(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}. All sources are agnostic over a and c, yet
(a, c) and (c, a) are in the result because of the transitive closure in the lower rank involving
opinions ((b, c) and (b, a)) which actually get overridden in the final result.
Because of these undesired effects, we propose another aggregation operator which circumvents this problem by applying refinement (as defined in Definition 13) to the set of
source belief states before inferring new opinions via closure:
Definition 15 The rank-based aggregation of a set of sources S  S, denoted AGR(S), is
AGRRf(S)+ .
Encouragingly, AGR outputs a valid belief state:
Proposition 16 If S  S, then AGR(S)  B.
The output for our running space robot example is also reasonable:
Example 5 Suppose, in the space robot scenario of Example 1, the technician is still considered more credible than the manager and the programmer, but the latter two are considered
167

fiMaynard-Zhang & Lehmann

F

FD

FD

Figure 4: The belief state after aggregation in Example 5 when st A sm  sp .
equally credible. The aggregate belief state, shown in Figure 4, still gives the robot the correct information about the state of the system. The robot also learns for future reference
that there is some disagreement over whether or not there would have been a data overload
if the feedback system were working.
Furthermore, we observe that AGR, when applied to the set of sources in Example 4, does
indeed bypass the problem described above of extraneous opinion introduction:
Example 6 Assume W, S, and w are as in Example 4; AGR(S) = {(a, b), (c, b)} as desired. The concerned reader may note that s2 is a dictator in the sense that s2 s opinions
override all opposing opinions. However, this is reasonable in the context because all other
sources have strictly lower rank.
We observe that AGR behaves well in the special cases weve considered, reducing to
AGRUn when all sources have equal rank, and to AGRRf when the sources are totally
ranked:
Proposition 17 Suppose S  S.
1. If wS is fully connected, AGR(S) = AGRUn(S).
2. If wS is a total order, AGR(S) = AGRRf(S).
Another property of AGR is that its transitive closure part minimally extends the result of
AGRRf to make it complete (i.e., all conflicts represented explicitly) in the sense that new
opinions are only added between worlds already involved in a conflict:
Proposition 18 Suppose S  S,  = AGRRf(S), = AGR(S), and x 6 y for x, y  W.
If x  y, then x ./ y.
One small observation: AGR() =  is a property of our definition, reflecting the fact that
we should not generate opinions out of nothing.
4.5 Arrow, Revisited
Finally, a strong argument in favor of AGR is that it satisfies Arrows conditions. Technically, our setting is slightly different from that of Arrows, so we need to modify each
condition so that it is appropriate for our setting, yet retains the intended spirit of the
original condition. Let f be an operator which aggregates the belief states <s1 , . . . , <sn
over W of n sources s1 , . . . , sn  S  S, respectively, let  = f (<s1 , . . . , <sn ), and let wS
be a total preorder over S. We consider each condition separately.
168

fiRepresenting and Aggregating Conflicting Beliefs

Restricted range For our setting, the output of the aggregation function will be a modular, transitive belief state rather than a total preorder considered by Arrow.
Definition 16 (modified) Restricted Range: The range of f is B.
Unrestricted domain Similarly, the input to the aggregation function will be modular,
transitive belief states of sources rather than total preorders.
Definition 17 (modified) Unrestricted Domain: For each i, <si can be any member of B.
Pareto principle In Arrows setting, the relations represented non-strict relative likelihood (preference, actually) so that the asymmetric restrictions of the relations were used
to define the Pareto principle. However, in our setting, generalized belief states already
represent strict likelihood. Consequently, we use the actual input and output relations
of the aggregation function in place of their asymmetric restrictions to define the Pareto
principle. Obviously, because of AGRs ability to introduce conflicts, it will not satisfy the
original formal Pareto principle which would essentially require that if all sources have an
unconflicted belief of one world being strictly more likely than another, this must also be
true in the aggregate belief state. Neither condition is necessarily stronger than the other.
Definition 18 (modified) Pareto Principle: If x <si y for all i, then x  y.
Independence of irrelevant alternatives Conflicts are defined in terms of cycles, not
necessarily binary. By allowing the existence of conflicts, we effectively have made it possible
for outside worlds to affect the relation between a pair of worlds, viz., by involving them in
a cycle. As a result, we need to weaken IIA to say that the relation between worlds should
be independent of other worlds unless these other worlds put them in conflict. This makes
intuitive sense: if two worlds are put into conflict after aggregation due to a cycle involving
other worlds, we may need to access these other worlds to be able to detect the conflict.
Definition 19 (modified) Independence of Irrelevant Alternatives (IIA): Suppose
0
0
s01 , . . . , s0n  S such that si S s0i for all i, and 0 = f (<s1 , . . . , <sn ). Further suppose x <si y
0
iff x <si y for all i, x6 ./ y, and x6 ./0 y. Then x  y iff x 0 y.
Non-dictatorship As with the Pareto principle definition, we use the actual input and
output relations to define non-dictatorship since belief states represent strict likelihood.
From this perspective, our setting requires that informant sources of the highest rank be
dictators in the sense considered by Arrow. However, the setting originally considered
by Arrow was one where all individuals are ranked equally. Thus, we make this explicit in
our new definition of non-dictatorship by adding the pre-condition that all sources be of
equal rank. Now, AGR treats a set of equally ranked sources equally by taking all their
opinions seriously, at the price of introducing conflicts. So, intuitively, there are no dictators.
However, because Arrow did not account for conflicts in his formulation, all the sources will
be dictators by his definition. We need to modify the definition of non-dictatorship to
say that no source should always push opinions through without them ever being contested.
Definition 20 (modified) Non-Dictatorship: If si S sj for all i, j, then there is no i such
that, for every combination of source belief states and every x, y  W, x <si y and y 6<si x
implies x  y and y 6 x.
169

fiMaynard-Zhang & Lehmann

We now show that AGR indeed satisfies these conditions:
Proposition 19 Let S = {s1 , . . . , sn }  S and AGRf (<s1 , . . . , <sn ) = AGR(S). AGRf
satisfies (the modified versions of ) restricted range, unrestricted domain, Pareto principle,
IIA, and non-dictatorship.

5. Multi-Agent Fusion
So far, we have only considered the case where a single agent must construct or update
her belief state once informed by a set of sources. Multi-agent fusion is the process of
aggregating the belief states of a set of agents, each with its respective set of informant
sources. We proceed to formalize this setting.
5.1 Formalization
An agent A is informed by a set of sources S  S.8 Agent As induced belief state is the
belief state formed by aggregating the belief states of its informant sources, i.e., AGR(S).
We will use A and AS to denote special agents informed by  and S, respectively.
Assume the set of agents to fuse agree upon rank (and, consequently, w).9 We define
the fusion of this set to be an agent informed by the combination of informant sources:
Definition 21 Let A = {A1 , . . . , An } be a set of agents such that each agent
Sn Ai is informed
by Si  S. The fusion of A, written (A), is an agent informed by S = i=1 Si .
Not surprisingly given its set-theoretic definition, fusion is idempotent, commutative, and
associative. These properties guarantee the invariance required in multi-agent belief aggregation applications such as our space robot domain.
5.2 Computing Fusion Efficiently
In the multi-agent space robot scenario described in Section 1, we only have a direct need
for the belief states that result from fusion. We are only interested in the belief states of the
original sources in so far as we want the fused belief state to reflect its informant history.
An obvious question is whether it is possible to compute the belief state induced by the
agents fusion solely from their initial belief states, that is, without having to reference the
belief states of their informant sources. This is highly desirable because of the expense
of storingor, as in the case of our space robot example, transmittingall source belief
states; we would like to represent each agents knowledge as compactly as possible.
In fact, we can do this if all sources have equal rank. We simply take the transitive
closure of the union of the agents belief states:
Ai , agent A s induced belief state,
Proposition 20 Let A and S be as in Definition
i
+
S 21, 
A
i

is
As
induced
belief state.
and wS , fully connected. If A = (A), then
Ai A

8. Each source can be thought of as a primitive agent with fixed belief state.
9. We could easily extend the framework to allow for individual rankings, but we felt that the small gain
in generality would not justify the additional complexity and loss of perspicuity. Similarly, we could
consider each agent as having a credibility ordering only over its informant sources. However, it is
unclear how, for example, crediblity orderings over disjoint sets of sources should be combined into a
new credibility ordering since their union will not be total.

170

fiRepresenting and Aggregating Conflicting Beliefs

Unfortunately, the equal rank case is special. If we have sources of different ranks, we
generally cannot compute the induced belief state after fusion using only the agent belief
states before fusion, as the following simple example demonstrates:
Example 7 Let W = {a, b}. Suppose two agents A1 and A2 are informed by sources s1 with
belief state <s1 = {(a, b)} and s2 with belief state <s2 = {(b, a)}, respectively. A1 s belief state
is the same as s1 s and A2 s is the same as s2 s. If s1 A s2 , then the belief state induced by
(A1 , A2 ) is <s1 , whereas if s2 A s1 , then it is <s2 .
Thus, just knowing the belief states of the fused agents is not sufficient for computing the
induced belief state. We need to maintain more information about each agents informants.
The question is whether we can do better than storing all the original sources.
We might wonder whether it is possible to somehow compute a credibility rank for each
agent based on the credibility of her informant sources, then simply apply AGR to the
agents induced belief states. This works fine if, for every pair of agents, all the informants
of one are more credible than those of the other. However, this does not work in general if
each agent can have informants both more and less credible than those of another agent as
the following example demonstrates:
Example 8 Let W = {a, b, c}. Suppose agent A1 is informed by source s1 with belief state
<s1 = {(a, b), (b, c), (a, c)}, and suppose agent A2 is informed by sources s0 and s2 with belief
states <s0 = {(c, b), (b, a), (c, a)} and <s2 = {(b, a), (c, a)}, respectively. Further suppose that
s2 A s1 A s0 . Then A1 s induced belief state is <s1 and A2 s is <s0 . The belief state induced
by (A1 , A2 ) is {(b, c), (c, a), (b, a)}. On the otherhand, if we rank A1 over A2 and apply
AGR to their induced belief states, we get <s1 ; if we rank A2 over A1 , we get <s0 ; and,
if we rank them equally, we get the fully connected belief state. All of these are obviously
incorrect.
Hence, we need to store more information about the source of each opinion. However,
we can still do better than keeping the sources around if sources are totally preordered by
credibility. It is enough to store for each opinion of AGRRf(S) the rank of the highest
ranked source supporting it. We define pedigreed belief states which enrich belief states with
this additional information:
Definition 22 Let A be an agent informed by a set of sources S  S. As pedigreed belief
state is a pair (, l) where = AGRRf(S) and l : R such that l((x, y)) = max({rank(s) :
x <s y, s  S}). We use A
r to denote the restriction of As pedigreed belief state to r, that
A
is, r = {(x, y) : l((x, y)) = r}.
We verify that a pairs label is, in fact, the rank of the source used to determine the pairs
membership in AGRRf(S), not that of some higher ranked source:
Proposition 21 Let A be an agent informed by a set of sources S  S and with pedigreed
belief state (, l). Then A
r is the relation
n

o
0
(x, y) : s  S. x <s y  r = rank(s)  s0  S. s0 A s  x s y .
The belief state induced by a pedigreed belief state (, l) is, obviously, the transitive closure
of .
171

fiMaynard-Zhang & Lehmann

Now, given only the pedigreed belief states of a set of agents, we can compute the
new pedigreed belief state after fusion. We simply combine the labeled opinions using our
refinement techniques. We call this operation pedigreed fusion:
Definition 23 Let S and A be as in Definition 21, wS , a total preorder, and PA , the set
of pedigreed belief states of the agents in A. The pedigreed fusion of PA , written ped (PA ),
is (, l) where
1.  is the relation
n

o
Aj
0
0
i
(x, y) : Ai  A, r  R. x A
y

A

A,
r

R.
r
>
r

x

y
j
r
r0
over W, and
i
2. l : R such that l((x, y)) = max({r : x A
r y, Ai  A}).

Proposition 22 Let A, PA , S, and wS be as in Definition 23. Then ped (PA ) is the
pedigreed belief state of (A).
From the perspective of the induced belief states, we are essentially discarding unlabeled
opinions (i.e., those derived by the closure operation) before fusion. Intuitively, we are
learning new information so we may need to retract some of our inferred opinions. After
fusion, we re-apply closure to complete the new belief state. Interestingly, in the special
case where the sources are strictly-ranked, the closure is unnecessary:
Proposition 23 If A, PA , and S are as in Definition 23, wS is a total order, and
ped (PA ) = (, l), then + =.
Let us return once more to the space robot scenario considered in Example 1 to illustrate
pedigreed fusion.
Example 9 Suppose the arrogant programmer is not part of the telemetry team, but instead works for a company on the other side of the country. Then the robot has to request
information from two separate agents, one to query the manager and technician and one to
query the programmer. Assume that the agents and the robot all rank the sources the same,
assigning the technician rank 2 and the other two agents rank 1, which induces the same
credibility ordering used in Example 5. The agents pedigreed belief states and the result of
their fusion are shown in Figure 5.
The first agent does not provide any information about overloading and the second agent
provides incorrect information. However, we see that after fusing the two, the robot has a
belief state that is identical to what it computed in Example 5 when there was only one agent
informed by all three sources (weve only separated the top set of worlds so as to show the
labeling). Consequently, it now knows the correct state of the system. And, satisfyingly, the
final result does not depend on the order in which the robot receives the agents reports.
The savings obtained in required storage space by this scheme can be substantial. Suppose S is the set of an agents informant sources, n = kWk, and m = kSk. Explicitly storing
S (along with the rank of each source) requires O(n2 m) amount of space; this worst case
bound is reached when all the sources belief states are fully connected relations. On the
172

fiRepresenting and Aggregating Conflicting Beliefs

FD
1
FD

FD

1
FD
2

1
2

1

1
FD

1
FD

FD

1
2 2

1

2

FD

2

1

1

F

FD

FD

A1

A2

(A1 , A2)

Figure 5: The pedigreed belief states of agent A1 informed by sm and st and of agent A2
informed by sp , and the result of their fusion in Example 9.

other hand, storing a pedigreed belief state only requires O(n2 ) space.10 Moreover, not only
does the enriched representation allow us to conserve space, it also provides for potential
savings in the efficiency of computing fusion since, for each pair of worlds, we only need to
consider the opinions of the agents rather than those of all the sources in the combined set
of informants.
Incidentally, if we had used the strawman AGR as the basis for our general aggregation,
simply storing the rank of the maximum supporting sources would not give us sufficient
information to compute the induced belief state after fusion. To demonstrate this, we give
an example where two pairs of sources induce the same annotated agent belief states, yet
yield different belief states after fusion:
Example 10 Let W, S, and w be as in Example 4. Suppose agents A1 , A2 , A01 , and A02
are informed by sets of sources S1 , S2 , S10 , and S20 , respectively, where S1 = S2 = {s2 },
S10 = {s0 , s2 }, and S20 = {s1 , s2 }. AGR dictates that the pedigreed belief states of all four
agents equal <s2 with all opinions annotated with rank(s2 ). In spite of this indistinguishability, if A = ({A1 , A2 }) and A0 = ({A01 , A02 }), then As induced belief state equals <s2 ,
i.e., {(a, b), (c, b)}, whereas A0 s is {(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}.
Also notice that Maynard-Reid II and Shoham (2001) consider essentially the special
case of fusing two agents informed by strictly-ranked sources. They show the surprising
result that standard AGM belief revision can be modeled as the fusion of two agents, the
informant and the informee, where the informants sources are all strictly more credible than
the informees. Furthermore, they show that, because of its clean set-theoretic semantics,
fusion provides a very attractive, semantically well-behaved solution to the difficult problem
of iterated belief revision. Our general fusion definition satisfies all the examples of iterated
fusion they describe.
10. These bounds assume that the amount of space needed to store each rank is bounded by some small
constant.

173

fiMaynard-Zhang & Lehmann

6. Incorporating Voting
A potential drawback of the framework we have described is it does not account for
strength of support. For example, we cannot differentiate between the situation where
one thousand sources of the highest rank support a < b and only one source of that rank
supports b < a, and the situation where the one source supports a < b and the thousand
other sources support b < a. In both cases our framework yields a simple conflict between
a and b rather than acknowledging the overwhelming support one way or the other. This
additional information about strength of support is often sufficient to resolve what would
otherwise have appeared to be a conflict.
To address this problem, we generalize our framework to incorporate voting. We first
describe a family of aggregation operators based on voting of which AGR is a special case.
In the process, we introduce a novel modular closure operator. We discuss properties of
special members of this family including indiscriminate aggregation, simple majority, and
unanimity, as well as attractive properties of the family as a whole. We then describe an
extension of our setting to accommodate ranked individuals so that individuals of higher
rank are given precedence during aggregation. Finally, we consider fusion.
6.1 Voting Functions
We will use a pairwise voting strategy similar to the well-known Condorcets method. (For
more on the Condorcet method and the other methods and results from standard voting
theory we cite, see Blacks (1958) classical reference on voting theory). Condorcets method
considers each pair of worlds separately, ranking world x over world y in the aggregate if
and only if there are more votes for that ranking than there are for y over x. If one world
beats or ties all other worlds, it is known as the Condorcet winner. We deviate from this
method in that we use a fixed threshold proportion of support to decide on the acceptance
of an opinion in the aggregate rather than the size of its support relative to that of the
opposite opinion. Let countS (x, y) = k{s  S : x <s y}k for any S  S.
Definition 24 Let S  S. For p  [0, 1], the voting function for p, written vtp , maps S to
the relation
{(x, y) : countS (x, y) > 0 and countS (x, y)/kSk  p}.
This definition falls under the class of voting systems Black (1958) calls absolute majority
systems. It is motivated by the observation that relative support is many times less relevant
than strength of support. The support for the two possible rankings of two worlds may be
so low that neither can justifiably be considered part of the aggregate belief state. Similarly,
the support for both alternative rankings may be so high that it may be more reasonable to
introduce both and create a conflict rather than choose one with slightly higher support. Our
strategy will not be appropriate for all applications, of course, but there are many instances
when it is most appropriate. Also, our method satisfies a generalization of the Condorcet
criterion, a widely accepted criteria of good voting systems that requires the Condorcet
winner, if it exists, be the most likely world in the aggregate belief state. Our method never
produces a strict ranking of two worlds opposite to that of Condorcets method, although
there will be cases where Condorcets method ranks one world strictly more likely than
another and our method produces agnosticism or a conflict. As a result, the Condorcet
174

fiRepresenting and Aggregating Conflicting Beliefs

winner will always be among the most likely worlds. At any rate, our aggregation results
do not depend significantly on the choice of voting strategy; one could easily replace it with
a different strategy if desired.
That said, we make a few observations about our voting functions. First, the voting
function definition requires that we accept an opinion if at least p proportion of the sources
support it. However, we often want to specify that opinions only be accepted if strictly
more than some cutoff proportion of sources support it. For example, the best-known
voting function is the majority function where we only accept an opinion if it gets more
than 50% of the vote. We can easily specify majority vote with the function vt0.5+ (S)
where 0 <  < 0.5/kSk (e.g.,  = 0.25/kSk) so that tied opinions are rejected. In general,
to only accept opinions garnering more than p proportion of the vote (for 0  p < 1), it
suffices to use the function vtp+ (S) where
0<<

1  pkSk + bpkSkc
kSk

e.g.,  = (1  pkSk + bpkSkc)/(2kSk).11
Second, it is immediately obvious that the aggregate relation may contain conflicts if
p  0.5, even if the original source belief states are conflict-free. In fact, it is possible to
get conflicts in the aggregate of conflict-free belief states even for larger p, as the following
famous example demonstrates:
Example 11 Let W = {a, b, c} and S be such that 1/3 of the sources have belief state
{(a, b), (b, c), (a, c)}, 1/3 have {(b, c), (c, a), (b, a)}, and 1/3 have {(c, a), (a, b), (c, b)}. Then
vtp (S) = {(a, b), (b, c), (c, a)}, a cycle, for 1/3 < p  2/3. This is known as the Condorcet
paradox (cf. Brams and Fishburns (2002) voting survey).
Many solutions have been proposed for resolving such conflicts  using Borda counts or
instant runoff voting (aka single transferable vote) (cf. Center for Voting and Democracy,
2002) are two popular examples. As before, we do not attempt to resolve the conflicts but,
instead, make them explicit in a way that supports flexibility in the choice of resolution
methodology and allows for semantically well-behaved iteration of aggregation.
Third, the end-point members of the family of voting functions have special significance.
The voting function for 0 is equivalent to the union operator we saw earlier that takes all
opinions seriously, i.e., is indiscriminate:
Proposition 24 If S  S, then vt0 (S) = Un(S).
At the other extreme, we have the voting function for 1. In this case, it is equivalent to
taking the intersection of the sources belief states, i.e., only accepting unanimous opinions:
T
Proposition 25 If S  S, then vt1 (S) = sS <s .
In contrast to vt0 which generates many conflicts, vt1 generates a lot of agnosticism.
Fourth, voting functions are opinion-centered; that is, if the proportion of agnostic
sources is larger than p, the voting function for p will not necessarily reflect this agnosticism
as it would in the case of an opinion. If, for example, the belief states of three sources over
11. bxc denotes the floor of x, i.e., the largest integer less than or equal to x.

175

fiMaynard-Zhang & Lehmann

W = {a, b} are {(a, b)}, {(b, a)}, and {}, respectively, then the voting function for p = 1/3
will produce a conflict with respect to a and b, not agnosticism. However, this is not to say
that abstainers have no impact on the final result. The fact that abstainers are counted
among the total number of voters has the effect that agnosticism with respect to a pair of
worlds counts as a no vote for both possible opinions. This issue usually does not arise in
standard voting schemes because these usually assume that sources totally rank candidates.
However, the most important observation is that members of the family of voting operators do not produce belief states in general. As weve already shown, vt0 produces a
modular relation that is not necessarily transitive. At the other end of the spectrum, vt1
produces a transitive relation that is not necessarily modular:
Proposition 26 Suppose S  S. vt1 (S) is transitive but not necessarily modular.
For the other members of the family, the result may be neither modular nor transitive,
as the Condorcet paradox in Example 11 illustrates for 1/3 < p  2/3. In fact, we can
construct such a scenario for every 0 < p < 1:
Proposition 27 If kWk  3, then for every p  (0, 1), there exists S such that vtp (S) is
neither modular nor transitive.
Part of the problem is that voting may introduce conflicts which may imply other conflicts.
As before, we need to take the transitive closure to infer these implied conflicts. In the
Condorcet paradox example, this produces the fully connected belief state as we would
hope. Unfortunately, closing under transitivity does not necessarily restore modularity as
well, as the following example demonstrates:
Example 12 Let W = {a, b, c} and S be such that 1/3 of the sources have belief state
{(a, b), (b, c), (a, c)}, 1/3 have {(b, c), (c, a), (b, a)}, and 1/3 have {(b, a), (b, c)}. Then, for
p > 2/3, vtp (S) = vtp (S)+ = {(b, c)} which is not modular.
We solve this problem by defining a natural modular closure operation which converts
a transitive relation into a belief state. We will then define a modular-transitive closure
operation which will take the result of an arbitrary voting function and transform it into a
belief state using a transitive closure followed by a modular closure.
6.2 Modular-Transitive Closure
We start by defining a helper function which returns the level of a world in a relation, i.e.,
the length of the longest path (along strict edges) from a world to a member of the choice
set of W. For convenience, throughout this modular-transitive closure subsection we will
use  to denote an arbitrary relation,  and ./ to denote its asymmetric and symmetric
restrictions, respectively.
Definition 25 The level of x  W in a transitive relation  over W, written lev (x), is
(
0
if x  ch(W, )
lev (x) =
1 + max ({lev (y) : y  x}) otherwise.
yW

(Recall that ch is the choice set function defined in Definition 2.) The following simple
properties relating  and lev are immediate:
176

fiRepresenting and Aggregating Conflicting Beliefs

Proposition 28 Suppose  is a transitive relation over W and x, y  W.
1. If x  y, then lev (x) < lev (y).
2. If x ./ y, then lev (x) = lev (y).
3. If lev (x) < lev (y), then z. lev (z) = lev (x)  z  y.
4. If lev (x) = lev (y), then x  y iff y  x.
We now define the modular closure of a relation to be the relation that results from fully
connecting all equi-level alternatives unless they are fully disconnected:
Definition 26 The modular closure MC() of a transitive relation  over W is the relation
such that (x, y)  MC() iff
1. lev (x) < lev (y) or
2. lev (x) = lev (y) and x0 , y 0 . lev (x0 ) = lev (y 0 ) = lev (x)  x0 ./ y 0 .
Intuitively, as long as we have reason to doubt that some pair in a level are interchangeable,
we doubt all the pairs at that level, but only then. Note that the definition of MC is similar
to one of the equivalent constructions of the rational closure Lehmann and Magidor (1992)
describe.
We see that MC indeed makes any transitive relation modular while preserving transitivity:
Proposition 29 If  is a transitive relation over W, then MC()  B.
MC is an additive process that changes a relation minimally to achieve modularity while
preserving transitivity and the levels of the worlds:
Proposition 30 Suppose  is a transitive relation over W and  = MC().
1.  and  .
2. If  is modular, then  =.
3. lev (x) = lev (x) for all x  W.
4. If 0  B such that 0 and lev0 (x) = lev (x) for all x  W, then  0 .
We now define the modular, transitive closure of an arbitrary relation  as MC applied
to the transitive closure of , and show that the result is a belief state:
Definition 27 The modular, transitive closure MT() of a relation  over W is the relation MC (+ ).
Proposition 31 If  is a relation over W, then MT()  B.
MT is also a minimally additive operator:
Proposition 32 Suppose  is a relation over W and  = MT().
1.  .
2. If  is transitive, then  = MC().
3. If  is modular, then  =+ .
4. If  is modular and transitive, then  =.
5. If  has no conflicts, then neither does  .
177

fiMaynard-Zhang & Lehmann

6.3 The Aggregation Family
We are now fully equipped to solve the problem of incorporating voting into aggregation.
First, consider the special case where all sources have the same rank. Our aggregation
operators will construct an aggregate belief state by first applying voting, then closing
under MT:
Definition 28 If S  S and p  [0, 1], then AGREqp (S) = MT (vtp (S)).
Proposition 33 If S  S and p  [0, 1], then AGREqp (S)  B.
We can now easily generalize this definition to accommodate a ranking on the sources.
We accept an opinion if enough individuals at the highest rank with an opinion support it,
then close under MT:
Definition 29 If S  S and p  [0, 1], then AGRRf p (S) is the relation
n

o
0
(x, y) : s  S. x <s y  (x, y)  vtp ({s0  S : s0 S s})  s0  S. s0 AS s  x s y .
Definition 30 If S  S and p  [0, 1], then AGRp (S) = MT(AGRRf p (S)).
Proposition 34 If S  S and p  [0, 1], then AGRp (S)  B.
All the aggregation functions we have encountered so far are special cases of this general
family:
Proposition 35 Suppose S  S and p  [0, 1].
1. If wS is fully connected, then AGRp (S) = AGREqp (S).
2. If wS is a total order, then AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).
3. AGR0 (S) = AGR(S).
As an obvious consequence of the last property, AGR0 satisfies the modified Arrovian conditions.
Corollary 35.1 Let S = {s1 , . . . , sn }  S and AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRf
satisfies (the modified versions of ) restricted range, unrestricted domain, Pareto principle,
IIA, and non-dictatorship.
6.4 Fusion
Fusion is still defined as in Definition 21, i.e., the belief state created by fusion of a set of
agents is the aggregate belief state of the agents cumulative informant sets. However, we
now use AGRp rather than AGR to compute the aggregate belief state.
Once again, we want to compute fusion without storing all the belief states of all the
informant sources, if possible. Unfortunately, this is not possible in general for aggregation
functions based on voting. The reason is we need to keep track of the actual identity of
the sources supporting each opinion so as to avoid double-counting sources shared by
multiple agents.
178

fiRepresenting and Aggregating Conflicting Beliefs

However, we can often do better than the O(n2 m) space required to store the full sources,
where n = kWk and m is the number of informant sources. We only store those parts that
matter. Specifically, for a given source, we only store those opinions for which the source
is one of the highest ranked supporting an opinion over the corresponding worlds. We
can effectively accomplish this by extending the pedigreed belief state so that we label each
opinion not only with the rank of the highest ranking sources supporting an opinion over the
corresponding worlds, but also with the set of unique identifiers for the sources supporting
the particular opinion. We also maintain a table that stores, for each rank represented in
the set of informant sources, the set of identifiers for all sources at that rank. We call the
resulting representation a support pedigreed belief state.
Definition 31 Let A be an agent informed by a set of sources S  S. As support pedigreed
belief state is a triple (l, sup, rtab) where
 l : W  W  R  {} such that l(x, y) = max({rank(s) : x 6s y, s  S}  {}) where
 6 R and  < r for all r  R,
 sup : W  W  2S such that sup(x, y) = {s  S : rank(s) = l(x, y), x <s y}, and
 rtab : ranks(S)  2S such that rtab(r) = {s  S : rank(s) = r}.
Note that l is symmetric: l(x, y) = l(y, x). On the other hand, sup is not. Now, we can
easily compute an agents belief state from its support pedigreed belief state. To compute
the proportion of support for a particular opinion, we simply divide the size of the support
set for that opinion by the number of informant sources with the labeled rank.
Proposition 36 Let A be an agent informed by a set of sources S  S, with support pedigreed belief state (l, sup, rtab), and using aggregation function AGRp for p  [0, 1]. As belief
state is the relation
MT ({(x, y) : ksup(x, y)k > 0 and ksup(x, y)k/krtab(l(x, y))k  p})
Observe that, unlike with pedigreed belief states, support pedigreed belief states label all
possible opinions, not just those appearing in the agents induced belief state, i.e., whose
support falls below the threshhold. The reason is another agent may come along later with
enough new votes to cross the threshold, in which case the votes from the earlier sources
become relevant. Similarly, support pedigreed belief states maintain rank information even
for ranks not appearing as labels. No source of a particular rank may currently support
any opinion, but another agent may later bring sources of that rank supporting an opinion
hitherto unsupported by any source of equal or higher rank. The correct computation of
the proportion of support for this opinion must take into account the earlier sources.
Before we address fusion, let us consider the space required to store a support pedigreed
belief state (l, sup, rtab). l requires O(n2 ) space, rtab requires (m) space, and, if rmax
denotes the number of sources of a rank having the most sources with that rank, sup
requires O(n2 rmax ) space, for a total of O(n2 rmax + m) space.12 Thus, in a best-case
scenario where, for example, sources are strictly ranked, a support pedigreed belief state
only requires O(n2 + m) space since each opinion has at most one supporter. However, in
12. As before, we assume representing ranks requires constant space. We assume that we can represent each
source label with constant space as well.

179

fiMaynard-Zhang & Lehmann

the worst-case scenario where, for example, sources are all equally ranked so that rmax = m,
we will still need O(n2 m) space.
Now, computing the support pedigreed belief state resulting from fusion is straightforward. For each opinion, we set l to be the highest l value for that opinion among the agents
and set sup to be the union of sup sets for that opinion of all agents with that l value. And
for each rank represented in some agents rank table, we set rtab to be the union of the
rtab sets of all agents for whom it is defined.
Definition 32 Let S and A be as in Definition 21, wS , a total preorder, and PA , the set
of support pedigreed belief states of the agents in A. The support pedigreed fusion of PA ,
written sup (PA ), is (l, sup, rtab) where
1. l : R such that l((x, y)) = max({l0 (x, y) : (l0 , sup0 , rtab0 )  PA }),
2. sup : W  W  2S such that
[

sup(x, y) =

sup0 (x, y),

(l0 ,sup0 ,rtab0 )PA , l0 (x,y)=l(x,y)

and
3. rtab : ranks(S)  2S such that
[

rtab(r) =

rtab0 (r).

(l0 ,sup0 ,rtab0 )PA , rrange(rtab0 )

Proposition 37 Let A, PA , S, and wS be as in Definition 32. Then sup (PA ) is the
support pedigreed belief state of (A).
Thus, in addition to the potential savings in space gained by using support pedigreed
belief states, we also potentially save in the time needed to compute fusion since, for a given
opinion, we do not need to consider the opinions of lesser ranked sources.

7. Related Work
Much of the work in belief aggregation has been geared towards unbiased kinds of belief
pooling. Besides the work in social choice we described in Section 3.2, recent attempts from
the belief revision community (e.g. Borgida & Imielinski, 1984; Baral, Kraus, Minker, &
Subrahmanian, 1992; Liberatore & Schaerf, 1995; Makinson, 1997; Revesz, 1997; Konieczny
& Perez, 1998; Meyer, 2001; Benferhat, Dubois, Kaci, & Prade, 2002) have sought to modify
the AGM theory to capture fair revisions, that is, revisions where the revisee and revisers
beliefs are treated equally seriously. Like our proposal, Benferhat et al. and Meyer accommodate iterative merging. Benferhat et al.s proposal is also distinct in that they approach
the problem from a possibilistic logic point of view. Besides the restriction to equally-ranked
sources, these fairness-based proposals differ from ours in that they are generally syntactic
in nature in the sense that sentences are prioritized rather than possible worlds. Meyers
proposal is an exception; his belief states are epistemic states, structures in the style of
Spohns (1988) ordinal conditional functions (aka -rankings). In fact, Meyer, Ghose, and
180

fiRepresenting and Aggregating Conflicting Beliefs

Chopra (2001) have shown that a number of simple aggregation operators on epistemic
belief states also satisfy Arrows postulates when appropriately modified for this context
(unrestricted domain, restricted range, and IIA in particular need modification). Unfortunately, epistemic states are enriched total preorders and, thus, suffer from the problems we
described earlier, i.e., the inability to explicitly handle conflicts.
Cantwells (1998) work is also syntactic in nature, but does allow for sources of differing credibility. Cantwell addresses a complementary problem to our own: deciding what
information to reject given the subset of informing sources rejecting the information. He
assumes a generalization of our credibility ordering, a partial preorder over sets of sources.
He explores ways of inducing a partial preorder over sentences based on this ordering, then
uses this ordering to determine a subset (although not all) of the sentences to reject. Another difference from our work is that he only considers the non-counterfactual beliefs of
sources.
We are not, of course, the first to consider using the lexicographic ordering for aggregation purposes. Lexicographic operators have long been studied in the fields of management
and social science; Fishburn (1974) gives a good survey of much of that work. More recently, researchers in artificial intelligence have taken an interest in these operators; examples include Grosof (1991), Maynard-Reid II and Shoham (2001) and Andreka, Ryan, and
Schobbens (2002).
Grosof uses lexicographic aggregation of preorders as a means of tackling the problem of
default reasoning in the presence of conflicting defaults. Besides the more general preorders
being aggregated, another interesting difference from our work is that although Grosof does
not allow for sources of equal rank, he does allow for sources of incomparable rank, i.e., the
ranking on sources is a strict partial order. Thus, in the extreme case where the ordering
is completely disconnected, the operator reduces to our Un operator (and, thus, does not
necessarily preserve transitivity).
Andreka et al., on the other hand, frame their work in the context of preference aggregation. They go one step further than Grosof and allow input relations to be arbitrary.
They prove that the lexicographic operator is the only one that satisfies a variation on
Arrows properties  unanimity, IIA, preservation of transitivity, and a weaker version of
non-dictatorship. (We should point out that from the perspective of Arrows original framework, the relation with the highest priority is always a dictator.) They describe a collection
of other properties besides transitivity preserved by the operator. However, as in our work,
they do not preserve totality.
Our work derives much of its inspiration from Maynard-Reid II and Shohams work.
They restrict their attention to total preorders, but this does not create problems because
they assume sources to be totally ordered. They focus, instead, on the strong connection
between belief aggregation and iterated belief revision. They show that  can be used as
an iterated belief operator in an AGM-based setting, then compare its properties as such
against those of a representative sampling of well-known iterated belief operator proposals
 Boutiliers (1996) natural revision, Darwiche and Pearls (1997) operators, Spohns (1988)
ordinal conditional function revision, Lehmanns (1995) widening rank model revision, and
Williamss (1994) conditionalization and adjustment operators. They show that  is the
only operator among them that is semantically well-behaved: the results of all the other
operators depend on the order of iteration.
181

fiMaynard-Zhang & Lehmann

Finally, to our knowledge, none of these related approaches outside of social choice have
yet been extended to incorporate voting.

8. Conclusion
We have described a semantically clean representation  the class of modular, transitive
relations  for collective qualitative beliefs which allows us to represent conflicting opinions
without sacrificing the ability to make decisions. We have proposed an intuitive operator
which takes advantage of this representation so that an agent can combine the belief states of
a set of informant sources totally preordered by credibility. We showed that this operator
circumvents Arrows Impossibility result in a satisfactory manner. We also described a
mechanism for fusing the belief states of different agents that iterates well and extended
the framework to incorporate voting.
We have assumed that all agents share the credibility ranking on sources. In general,
these rankings can vary among agents, and even change over time. Furthermore, an agents
ranking function can depend on the context; different sources may have different areas of
expertise. Exploring the behavior of fusion in these more general settings is an obvious next
step.
Note that although we have described operators to incorporate voting, under no condition will any of these ever side with lower rank sources when they conflict with higher rank
sources, no matter how many of these disagreeing lower rank sources there are. An aggregation scheme that behaves differently would have to be built on fundamentally different
assumptions than our framework.
Another problem which deserves further study is developing a fuller understanding of
the properties of the Bel, Agn, and Con operators and how they interrelate.

Acknowledgments
We want to thank Yoav Shoham and the anonymous referees of this paper for their insightful
and comprehensive feedback. A preliminary version of this paper appears in the Proceedings
of the Seventh International Conference on Principles of Knowledge Representation and
Reasoning (KR2000) (the first authors last name was Maynard-Reid II at that time). This
work was partially supported by a National Physical Science Consortium Fellowship and
the Jean et Helene Alfassa Fund for Research in Artificial Intelligence.

Appendix A. Proofs
Proposition 1
1. The transitive closure of a modular relation is modular.
2. Every transitive relation is quasi-transitive.
3. (Sen, 1986) Every quasi-transitive relation is acyclic.
Proof:
1. Suppose a relation  over finite set  is modular, and + is the transitive closure of . Suppose x, y, z   and x + y. Then there exist w0 , . . . , wn such that
182

fiRepresenting and Aggregating Conflicting Beliefs

x = w0      wn = y. Since  is modular and w0  w1 , either w0  z or z  w1 .
In the former case, x = w0  z, so x + z. In the latter case, z  w1     wn = y, so
z + y.
2. Suppose  is a finite set, x, y, z  ,  is a transitive relation over , and < is its
asymmetric restriction. Suppose x < y and y < z. Then x  y, y 6 x, y  z, and
z 6 y. x  y and y  z imply x  z, and y  z and y 6 x imply z 6 x, both by
transitivity. So x < z.
2
Proposition 2 (Sen, 1986) Given a relation  over a finite set , the choice set operation
ch defines a choice function iff  is acyclic.
Proof:

See Sens (1986) proof. 2

Proposition 3 (Arrow, 1963) There is no aggregation operator that satisfies restricted
range, unrestricted domain, (weak) Pareto principle, independendence of irrelevant alternatives, and nondictatorship.
Proof:

See Arrows (1963) proof. 2

Proposition 4 Let  be a relation over a finite set  and let  be its symmetric restriction.
If  is total and quasi-transitive but not transitive, then  is not transitive.
Proof: Let  be a total, quasi-transitive, non-transitive relation. Suppose x  y and
y  z but x 6 z. By totality, z  x, so z  x. If x  y, then z  y by quasi-transitivity, a
contradiction. Thus, x  y. Similarly, if y  z, then y  x, a contradiction, so y  z. But
z  x, so x 6 z. Therefore,  is not transitive. 2
Proposition 5 Suppose a relation  is transitive and  is the corresponding agnosticism
relation. Then  is transitive iff  is modular.
Proof: Suppose  is transitive and suppose x  z, x, y, z  W. We prove by contradiction: Suppose x 6 y and y 6 z. By transitivity, z 6 y and y 6 x, so x  y and y  z. By
assumption, x  z, so x 6 z, a contradiction.
Suppose, instead,  is modular and suppose x  y and y  z, x, y, z  W. Then x 6 y,
y 6 x, y 6 z, and z 6 y. By modularity, x 6 z and z 6 x, so x  z. 2
Proposition 6 T<  B and is the set of irreflexive relations in B.
Proof: Let x, y, z  W. We first show that T<  B. Let  T< . Then there exists
 T such that  is the asymmetric restriction of . By definition,  is transitive, so
by Proposition 1, so is . Suppose x  y. Then x  y and y 6 x. Since  is total,
x  z or z  x. Suppose x  z. If y  z, then z 6 x (otherwise y  x by transitivity,
a contradiction), so x  z. And if, on the other hand, y 6 z, then z  y by totality, so
z  y. Suppose, instead, z  x. Then z  y by transitivity and y 6 z (otherwise y  x by
transitivity, a contradiction), so z  y. Thus, x  z or z  y, so  is modular.
Now we show that  B is in T< if and only if it is irreflexive. If  T< , it is asymmetric,
so it is irreflexive. Suppose, instead,  is irreflexive. We define a relationship , show that 
183

fiMaynard-Zhang & Lehmann

is its asymmetric restriction, and show that  is in T . Let  be defined as x  y iff y 6 x.
We first show that  is the asymmetric restriction of . Suppose 0 is the asymmetric
restriction of . If x 0 y, then x  y and y 6 x, so x  y. If, instead, x  y, then y 6 x.
By totality, x  y, so x 0 y. We next show that  T . If x 6 y then y  x. Otherwise,
x  y. But since  is irreflexive, y 6 x (otherwise x  x by transitivity), so x  y and 
is total. Next, suppose x  y and y  z. Then y 6 x and z 6 y. By modularity, z 6 x, so
x  z, and, thus  is transitive. 2
Proposition 7  B iff there is a partition W = hW0 , . . . , Wn i of W such that:
1. For every x  Wi and y  Wj , i 6= j implies i < j iff x  y.
2. Every Wi is either fully connected (w  w0 for all w, w0  Wi ) or fully disconnected
(w 6 w0 for all w, w0  Wi ).
Proof: We refer to the conditions in the proposition as conditions 1 and 2, respectively.
We prove each direction of the proposition separately.
(=) Suppose  B, that is,  is a modular and transitive relation over W. We use a
series of definitions and lemmas to show that a partition of W exists satisfying conditions 1
and 2. We first define an equivalence relation by which we will partition W. Two elements
will be equivalent if they look the same from the perspective of every element of W:
Definition 33 x  y iff for every z  W, x  z iff y  z and z  x iff z  y.
Lemma 7.1  is an equivalence relation over W.
Proof: Suppose x  W. For every z  W, x  z iff x  z and z  x iff z  x, so x  x.
Therefore,  is reflexive.
Suppose x, y  W and x  y. Then for every z  W, x  z iff y  z and z  x iff z  y.
But then for every z  W, y  z iff x  z and z  y iff z  x. Therefore, y  x, so  is
symmetric.
Suppose x, y, z  W, x  y, and y  z. Suppose further that w  W. By definition of
, x  w iff y  w and w  x iff w  y, and y  w iff z  w and w  y iff w  z. Therefore,
x  w iff z  w and w  x iff w  z. Since w is arbitrary, x  z, so  is transitive. 2
 partitions W into its equivalence classes. We use [w] to denote the equivalence class
containing w, that is, the set {w0  W : w  w0 }. Observe that two worlds in conflict
always appear in the same equivalence class:
Lemma 7.2 If x, y  W and x ./ y, then [x] = [y].
Proof: Suppose x, y  W and x ./ y. Since [x] is an equivalence class, it suffices to show
that y  [x], that is, x  y. Suppose z  W. By transitivity, if x  z, then y  z; if y  z,
then x  z; if z  x, then z  y; and, if z  y then z  x. Thus, x  z iff y  z and z  x
iff z  y, and since z is arbitrary, x  y. 2
We now define a total order over these equivalence classes:
Definition 34 For all x, y  W, [x]  [y] iff [x] = [y] or x  y.
184

fiRepresenting and Aggregating Conflicting Beliefs

Lemma 7.3  is well-defined, that is, if x  x0 and y  y 0 , then x  y iff x0  y 0 , for all
x, x0 , y, y 0  W.
Proof: Suppose x  x0 and y  y 0 , x, x0 , y, y 0  W. By the definition of , for every
z  W, x  z iff x0  z. In particular, x  y iff x0  y. Also by the definition of , for
every z 0  W, z 0  y iff z 0  y 0 . In particular, x0  y iff x0  y 0 . Therefore, x  y iff x0  y 0 .
2
Lemma 7.4  is a total order over the equivalence classes of W defined by .
Proof: Suppose x, y, z  W. We first show that  is total. By definition of , if x  y
or y  x, then [x]  [y] or [y]  [x], respectively. Suppose x 6 y and y 6 x, and suppose
z  W. By modularity of , x  z implies y  z, y  z implies x  z, z  x implies z  y,
and z  y implies z  x, so x  y. Therefore, [x] = [y], so [x]  [y] by the definition of .
Next, we show that  is anti-symmetric. Suppose [x]  [y] and [y]  [x]. Then [x] = [y]
or x  y and y  x. In the former case we are done, in the latter, the result follows from
Lemma 7.2.
Finally, we show that  is transitive. Suppose [x]  [y] and [y]  [z]. Obviously, if
[x] = [y] or [y] = [z], then [x]  [z]. Suppose not. Then x  y and y  z, so x  z by the
transitivity of . Therefore, [x]  [y] by the definition of . 2
We name the members of the partition W0 , . . . , Wn such that Wi  Wj iff i  j, where
n is an integer. Such a naming exists since every finite, totally ordered set is isomorphic to
some finite prefix of the integers.
We now check that this partition satisfies the two conditions. For the first condition,
suppose x  Wi , y  Wj , and i 6= j. We want to show that i < j iff x  y. Since i 6= j,
[x] 6= [y]. Suppose i < j. Then i  j, so [x]  [y]. Since [x] 6= [y], x  y by the definition of
. Now suppose, instead, that x  y. Then [x]  [y] by the definition of , so i  j. Since
[x] 6= [y], y 6 x by Lemma 7.2. Since [x] 6= [y] and y 6 x, [y] 6 [x] by the definition of ,
so j 6 i. Thus, i < j.
Finally, we show that each Wi is either fully connected or fully disconnected. Suppose
x, y, z  Wi so that x  y  z. It suffices to show that x  x iff y  z. By the definition of
, x  x iff y  x, and x  x iff x  z. Suppose x  x. Then, y  x and x  z, so y  z
by transitivity of . Suppose now, x 6 x. Then, y 6 x and x 6 z, so y 6 z by modularity
of .
(=) Suppose W = hW0 , . . . , Wn i is a partition of W and  is a relation over W satisfying the given conditions. We want to show that  is modular and transitive. We first
give the following lemma:
Lemma 7.5 Suppose W is a partition of W and  is a relation over W satisfying condition 1. If Wi , Wj  W, x  Wi , y  Wj , and x  y, then i  j.
Proof:

If i = j, were done. Suppose i 6= j. Then, since x  y, i < j by condition 1. 2

We now show  is modular. Suppose x  Wi , y  Wj , and x  y. Then i  j by
Lemma 7.5. Suppose z  Wk . Then i  k or k  j by the modularity of . Suppose i < k
or k < j. Then x  z or z  y by condition 1. Otherwise i = k = j, so x, y, z  Wi . Since
x  y, Wi is fully connected by condition 2, so x  z (and z  y).
185

fiMaynard-Zhang & Lehmann

Finally, we show that  is transitive. Suppose x  Wi , y  Wj , z  Wk , x  y, and
y  z. By Lemma 7.5, i  j and j  k, so i  k by the transitivity of . Suppose i < k.
Then x  z by condition 1. Otherwise i = k = j, so x, y, z  Wi . Since x  y, Wi is fully
connected by condition 2, so x  z. 2
(END OF PROPOSITION 7 PROOF)
Proposition 8 T  B and is the set of reflexive relations in B.
Proof: We first show that T  B. Let  T and x, y, z  W. By definition,  is transitive. Suppose x  y. Since  is total, x  z or z  x. If z  x, then z  y by transitivity,
so  is modular. On the other hand, the empty relation over W is modular and transitive,
but not total and, consequently, not in T .
Now we show that  B is in T if and only if it is reflexive. If  T , it is total, so it is
reflexive. If, instead,  is reflexive, then x  x so, by modularity, x  y or y  x. Thus, 
is total. And, since  B, it is transitive. 2
Proposition 9
1. Q  B = T .
2. B 6 Q.
3. Q 6 B if W has at least three elements.
4. Q  B if W has one or two elements.
Proof:
1. Suppose  Q  B. Then  is total and transitive and, hence, in T . Suppose  T .
By definition,  is total. Also by definition, it is transitive, so by Proposition 1, it is
quasi-transitive and, thus, in Q. By Proposition 8,  B and, so, in Q  B.
2. The empty relation is modular and transitive, but not total and, so, not in Q.
3. Suppose a and b are distinct elements of W. The relation W  W \ {(b, a)} is total,
and, since the asymmetric restriction is {(a, b)} which is transitive, it is also quasitransitive. However, if there are at least three elements in W, it is not transitive and,
so, not in B.
4. Suppose W has one element. Then B contains both possible relations over W, whereas
Q contains only the fully connected relation over W.
Suppose W has two elements a and b. Then B contains the empty relation, the fully
connected relation, and all the remaining eight relations which contain either (a, b) or
(b, a), but not both. Q, on the other hand, only contains the three reflexive relations
containing either (a, b) or (b, a).
2
Proposition 10
1. Q<  B = T< .
186

fiRepresenting and Aggregating Conflicting Beliefs

2. B 6 Q< .
3. Q< 6 B if W has at least three elements.
4. Q<  B if W has one or two elements.
Proof:
1. Suppose  Q<  B. Since  Q< , it is irreflexive, so since it is in B, it is in T< by
Proposition 6. Suppose, instead,  T< . By Proposition 6,  B. Let  T be a
relation such that  is its asymmetric restriction. (Obviously such a relation must
exist.) From Proposition 9,  Q, so  Q< . Thus,  Q<  B.
2. The fully connected relation over W is in B, but not asymmetric and, so, not in Q< .
3. Suppose a and b are distinct elements of W. If W has at least three elements, the
relation {(a, b)} is not modular and, thus, not in B, yet it is the asymmetric restriction
of the relation W  W \ {(b, a)} which is total and quasi-transitive (since {(a, b)} is
transitive).
4. Suppose W has one element. Then B contains both possible relations over W, whereas
Q< contains only the empty relation over W.
Suppose W has two elements a and b. Then B contains the empty relation, the fully
connected relation, and all eight of the remaining relations which contain either (a, b)
or (b, a), but not both. Q< , on the other hand, only contains the three irreflexive
relations.
2
Proposition 11 If S  S, then Un(S) is modular but not necessarily transitive.
Proof: Let = Un(S). Suppose x, y, z  W and x  y. Then there is some s  S such
that x <s y. By assumption, <s is modular, so x <s z or z <s y. By the definition of Un(S),
x  z or z  y, so  is modular.
Suppose a, b, c  W and S = {s1 , s2 } such that <s1 = {(a, b), (a, c)} and
s
< 2 = {(b, a), (c, a)}. Un(S) is not transitive. 2
Proposition 12 If S  S, then AGRUn(S)  B.
Proof: The transitive closure of any relation is transitive. Since Un(S) is modular, the
transitive closure of Un(S) is also modular by Proposition 1. 2
Proposition 13 If S  S, then AGRRf(S) is modular but not necessarily transitive.
Proof: We first prove modularity. Suppose x, y, z  W and (x, y)  AGRRf(S). Then
0
there exists s  S such that x <s y and for all s0 AS s  S, x s y. By modularity of <s ,
either x <s z or z <s y. Since S is finite, this implies that either there exists s0  S such
0
00
0
that x <s z and for all s00 AS s0  S, x s z, or there exists s0  S such that y <s z
00
and for all s00 AS s0  S, y s z. Thus, (x, z)  AGRRf(S) or (z, y)  AGRRf(S), so
AGRRf(S) is modular.
Suppose W = {x, y, z} and S = {s1 , s2 } such that s1 = {(x, y), (z, y)}, s2 = {(y, x), (y, z)},
and s1 S s2 . Then AGRRf(S) = {(x, y), (z, y), (y, x), (y, z)} which is not transitive. 2
187

fiMaynard-Zhang & Lehmann

Proposition 14 If S  S and wS is a total order, then AGRRf(S)  B.
Proof: Weve already proven in Proposition 13 that AGRRf(S) is modular. Let
= AGRRf(S) and suppose x, y, z  W. It remains to show that  is transitive. Suppose x  y and y  z. Then there exists s1  S such that x <s1 y and, for every s01  S,
0
0
s01 AS s implies x 6<s1 y and y 6<s1 x, and there exists s2  S such that y <s2 z and, for
0
0
every s02  S, s02 AS s implies y 6<s2 z and z 6<s2 y. Suppose s1 AS s2 (the case s2 AS s1 is
similar). Then y 6<s1 z and z 6<s1 y. By modularity, since x <s1 y and z 6<s1 y, x <s1 z. Let
0
0
0
s0  S and s0 AS s1 . Then x 6<s y and y 6<s x. And, since s1 AS s2 , s0 AS s2 , so y 6<s z
0
0
0
and z 6<s y. By modularity, x 6<s z and z 6<s x. Therefore, x  z. 2
Proposition 15 If S  S, then AGR (S)  B.
Proof: By Proposition 12, <r  B for every r  ranks(S). For convenience, we assume the
existence of a virtual source sr corresponding to each <r . Precisely, for each r  ranks(S),
assume there exists a source sr  S such that <sr =<r and rank(sr ) = r, and let S 0 be the
set of these sources. Then,



AGR (S) = (x, y) : r  R. x <r y  r0  ranks(S). r0 > r  x r0 y
n

o
0
=
(x, y) : s  S 0 . x <s y  s0  S 0 . s0 AS 0 s  x s y
= AGRRf(S 0 ).
Since there is one source in S 0 per rank r, and since > is a total order over R, wS 0 is a total
order. The result follows from Proposition 14. 2
Proposition 16 If S  S, then AGR(S)  B.
Proof: By Proposition 13, AGRRf(S) is modular. AGRRf(S)+ is obviously transitive,
and, by Proposition 1, it is modular as well. 2
Proposition 17 Suppose S  S.
1. If wS is fully connected, AGR(S) = AGRUn(S).
2. If wS is a total order, AGR(S) = AGRRf(S).
Proof:
1. Suppose wS is fully connected. Then the second half of the definition of AGRRf is
vacuously
that AGRRf(S) simplifies to {(x, y) : s  S. x <s y}. But this is
S true so
exactly sS <s , i.e., Un(S), so AGR(S) = AGRRf(S)+ = Un(S)+ = AGRUn(S).
2. Suppose wS is an total order. By Proposition 14, AGRRf(S) is transitive, so AGR(S) =
AGRRf(S)+ = AGRRf(S).
2
Proposition 18 Suppose S  S,  = AGRRf(S), = AGR(S), and x 6 y for x, y  W.
If x  y, then x ./ y.
188

fiRepresenting and Aggregating Conflicting Beliefs

Proof:

We first show the following lemma:

Lemma 18.1 Suppose S  S and  = AGRRf(S). For every integer n  2, if x, y  W,
x 6 y, there exist x0 , . . . , xn  W such that x = x0      xn = y, and n is the smallest
integer such that this is true, then xn      x0 .
Proof: Suppose x, y  W, x 6 y, and there exist x0 , . . . , xn  W such that x = x0 
    xn = y, and n is the smallest integer such that this is true. Consider any triple
xi1 , xi , xi+1 , where 1  i  n  1. First, xi1 6 xi+1 , otherwise there would be a chain
of shorter length than n between x and y. Now, since xi1  xi , there exists s1  S such
0
that xi1 <s1 xi and, for all s0 AS s1  S, xi1 s xi . Similarly, there exists s2  S such
0
that xi <s2 xi+1 and, for all s0 AS s2  S, xi s xi+1 . Thus, all sources with higher rank
than max(s1 , s2 ) are agnostic with respect to xi1 and xi+1 .
Suppose s1 AS s2 . Then xi s1 xi+1 so, by modularity, xi1 <s1 xi+1 . But then
xi1  xi+1 , a contradiction. Similarly, we derive a contradiction if s2 A s1 . Thus, s1 S s2 .
Now, since xi1 6 xi+1 and all sources with rank higher than s1 and s2 are agnostic
with respect to xi1 and xi+1 , xi1 6<s1 xi+1 . By modularity, xi+1 <s1 xi . Since s1 S s2 ,
and all the sources with higher rank than s2 are agnostic with respect to xi and xi+1 ,
xi+1  xi . Similarly, xi <s2 xi1 , so xi  xi1 . Since i was chosen arbitrarily between 1
and n  1, xn      x0 . And, in fact, all the opinions between these worlds originate
from sources of the same rank. 2
Now suppose x 6 y. If x  y, then there exist x0 , . . . , xn such that x = x0     
xn = y and n is the smallest positive integer such that this is true. Then, by Lemma 18.1,
y = xn      x0 = x, so y  x and x ./ y. 2
Proposition 19 Let S = {s1 , . . . , sn }  S and AGRf (<s1 , . . . , <sn ) = AGR(S). AGRf
satisfies (the modified versions of ) restricted range, unrestricted domain, Pareto principle,
IIA, and non-dictatorship.
Proof: Let = AGRf (<s1 , . . . , <sn ). Then = AGR(S).
Restricted range: AGRf satisfies restricted range by Proposition 16.
Unrestricted domain: AGRf satisfies unrestricted domain by Definition 7.
Pareto principle: Suppose x <si y for all si . In particular, x <s y where s is a maximal
rank source of S. Since s is maximal, it is vacuously true that for every s0 AS s  S, x 6<s y
0
and y 6<s x. Therefore, x  y, so AGRf satisfies the Pareto principle.
IIA: Let S 0 = {s01 , . . . , s0n }. First note that AGRRf satisfies IIA:
Lemma 19.1 Suppose S = {s1 , . . . , sn }  S, S 0 = {s01 , . . . , s0n }  S, si S s0i for all i,
0
 = AGRRf(S), and 0 = AGRRf(S 0 ). If, for x, y  W, x <si y iff x <si y for all i, then
x  y iff x 0 y.
0

Proof: Suppose si S s0i , and x <si y iff x <si y, for all i. Then x  y iff x 0 y since
Definition 13 only relies on the relative ranking of the sources and the relations between
x and y in their belief states to determine the relation between x and y in the aggregated
state. 2
189

fiMaynard-Zhang & Lehmann

Thus, IIA can only be disobeyed when the closure step of AGR introduces new opinions.
(Note that IIA is satisfied when there are no sources of equal rank since, by Proposition 17,
the closure step introduces no new opinions under these conditions.)
0
Now, suppose x, y  W, x <si y iff x <si y for all i, x6 ./ y, and x6 ./0 y. We show that
x  y implies x 0 y (the other direction is identical). Suppose x  y. Let  = AGRRf(S)
and 0 = AGRRf(S 0 ). Since x6 ./ y, x  y by Proposition 18. But then x 0 y by
Lemma 19.1, so x 0 y.
(END OF IIA SUB-PROOF)
Non-dictatorship: Suppose wS is fully connected and suppose x <si y and y 6<si x.
Let sj be such that y <sj x. Then x  y and y  x, so si is not a dictator. 2
(END OF PROPOSITION 19 PROOF)
Ai , agent A s induced belief state,
Proposition 20 Let A and S be as in Definition
i
S 21, 
+
A
i
and wS , fully connected. If A = (A), then

is
As
induced
belief state.
Ai A

Proof:

We will use the following lemma:

Lemma 20.1 If  is a set of relations over an arbitrary finite set , then



[

+



+  = 

[

+






where + is the transitive closure of .
S
S
+
+
+
0=
, and a, b  . Suppose a  b. Then
Proof: Let =

,




there exist 0 , . . . , n1   and w0 , . . . , wn   such that
+
a = w0 +
0    n1 wn = b

Thus, there exist x00 , . . . , x0m0 , . . ., x(n1)0 , . . . , x(n1)mn1 in  such that
a = w0 = x00 0    0 x0m0 = w1 =    = wn1 = x(n1)0 n1    n1 x(n1)mn1 = wn
and wn = b, so a 0 b.
Now suppose a 0 b. Then there exist 0 , . . . , n1   and w0 , . . . , wn   such that
a = w0 0    n1 wn = b
Obviously, this implies that
+
a = w0 +
0    n1 wn = b

which implies that
where  =

S


+
a = w0 +
     wn = b


 , so a  b. 2

190

fiRepresenting and Aggregating Conflicting Beliefs

Now, let  be the belief state induced by (A). Then = AGR(S). By Proposition 17,
= AGRUn(S), so

= Un(S)+ =

[

!+
<s


=

sS

+

[

s

Sn

i=1



<s  = 

[ [

+
<s 

Ai A sSi

Si

By the lemma,

= 


[



Ai A

[
sSi

+ +



<s   = 

[

+



AGRUn(Si ) = 

Ai A

[

+
Ai 

Ai A

2
Proposition 21 Let A be an agent informed by a set of sources S  S and with pedigreed
belief state (, l). Then A
r is the relation

o
n
0
(x, y) : s  S. x <s y  r = rank(s)  s0  S. s0 A s  x s y .
Proof: Suppose x A
r y. Then x  y and l((x, y)) = r. By Definitions 13 and 22, there
0
0
exists s  S such that x <s y and for every s0 AS s  S, x s y. In particular, if x <s y
for some s0  S, then s wS s0 , so rank(s)  rank(s0 ). Thus,
0

r = l((x, y)) = max({rank(s0 ) : x <s y, s0  S}) = rank(s).
Now suppose there exists s  S such that x <s y, r = rank(s), and, for every s0 AS s  S,
0
0
x s y. Then x  y. Moreover, since for every s0  S, x <s y implies s wS s0 which implies
rank(s)  rank(s0 ),
0

l((x, y)) = max({rank(s0 ) : x <s y, s0  S}) = rank(s) = r.
Therefore, x A
r y. 2
Proposition 22 Let A, PA , S, and wS be as in Definition 23. Then ped (PA ) is the
pedigreed belief state of (A).
Proof: Let ped (PA ) = (, l), 0 = AGRRf(S), and l0 :0  R such that l0 ((x, y)) =
max({rank(s) : x <s y, s  S}). It suffices to show that =0 and l = l0 .
Suppose x  y. We show that x 0 y, i.e., there exists s  S such that x <s y and, for
0
0
every s0 AS s  S, x 6<s y and y 6<s x, and that l0 ((x, y)) = l((x, y)). Since x  y, there
Aj
0
i
exists Ai and r such that x A
r y and, for every Aj  A and r > r  R, x 6r0 y and
A
s
i
y 6r0j x. Since x A
r y, there exists s  Si such that x < y, rank(s) = r, and, for evs
s
1
1
ery s1 AS s  Si , x 6< y and y 6< x. Si  S, so there exists s  S such that x <s y.
0
0
Now suppose s0 is a maximal rank source of S with x <s y or y <s x. Such an s0 exists since x <s y. Since wS is a total preorder, it suffices to show that s wS s0 . Suppose
0
0
s0  Sj . Since Sj  S, s0 is also a maximal rank source of Sj with x <s y or y <s x, so
191

fiMaynard-Zhang & Lehmann

A

A

j
j
Ai y, r = rank(s)  rank(s0 ), so s w s0 . Furx rank(s
S
0 ) y or y rank(s0 ) x. But since x r
thermore, l0 ((x, y)) = rank(s) = r = l((x, y)).
i
Now suppose x 0 y. We show that x  y, i.e., there exists Ai and r such that x A
r y
Aj
Aj
0
0
and, for every Aj  A and r > r  R, x 6r0 y and y 6r0 x, and that l((x, y)) = l ((x, y)).
0
Since x 0 y, there exists s  S such that x <s y and, for every s0 AS s  S, x 6<s y and
0
y 6<s x. Suppose s  Si . Since Si  S, it is also the case that for every s0 AS s  Si ,
0
0
Aj
Aj
0
i
x 6<s y and y 6<s x, so x A
rank(s) y. Now, let Aj and r be such that x r0 y or y r0 x. It
0

suffices to show that rank(s)  r0 . By Proposition 21, there exists s0  Sj such that x <s y
0
or y <s x and rank(s0 ) = r0 . But then s wS s0 , so rank(s)  rank(s0 ) = r0 . Furthermore,
l((x, y)) = rank(s) = l0 ((x, y)). 2
Proposition 23 If A, PA , and S are as in Definition 23, wS is a total order, and
ped (PA ) = (, l), then + =.
Proof: Since wS is a total order, AGR(S) = AGRRf(S) by Proposition 17. Thus, =
AGRRf(S) = AGR(S) = AGRRf(S)+ =+ . 2
Proposition 24 If S  S, then vt0 (S) = Un(S).
Proof: Suppose (x, y)  Un(S). Then S 6=  and x <s y for some s  S. Thus,
countS (x, y) > 0 and countS (x, y)/kSk > 0, so (x, y)  vt0 (S). Suppose, instead, (x, y) 6
Un(S). Then x 6<s y for all s  S, so countS (x, y) = 0, so (x, y) 6 vt0 (S). 2
T
Proposition 25 If S  S, then vt1 (S) = sS <s .
T
Proof: Suppose (x, y)  sS <s . Then S 6=  and x <s y for all s  S. Thus,
count
S (x, y) > 0 and countS (x, y)/kSk  1, so (x, y)  vt1 (S). Suppose, instead, (x, y) 6
T
s
s
sS < . Then there exists s  S such that x 6< y, so countS (x, y) < kSk. Thus,
countS (x, y)/kSk < 1, so (x, y) 6 vt1 (S). 2
Proposition 26 Suppose S  S. vt1 (S) is transitive but not necessarily modular.
Proof: Let W = {x, y, z} and S = {s1 , s2 } where <s1 = {(x, y), (y, z), (x, z)} and <s2 =
{(x, y), (z, y)}. Then vt1 (S) = {(x, y)} which is not modular. 2
Proposition 27 If kWk  3, then for every p  (0, 1), there exists S such that vtp (S) is
neither modular nor transitive.
Proof: Note that if kWk = 2, every relation over W is either transitive or modular
(but not necessarily both), and if kWk = 1 every relation over W is both modular and
transitive. Let W be a set of worlds such that kWk  3 and let x, y, and z denote three
distinct members of W. We will define S parameterized by p such that vtp (S) is the relation
{(x, y), (y, z)} which is neither transitive or modular.
Let S = {s1 , . . . , sn } satisfying the following conditions:
1. n = d1/p + 1e if p  1/3, d2/(1  p) + 1e otherwise.13
13. dxe denotes the ceiling of x, i.e., the smallest integer greater than or equal to x.

192

fiRepresenting and Aggregating Conflicting Beliefs

2. <s1 = {(w, y)|w  W, w 6= y}.
3. <s2 = {(y, w)|w  W, w 6= y}.
4. dpn1e of the remaining sources have belief state {(x, w)|w  W, w 6= x}{(w, z)|w 
W, w 6= z}.
5. The remaining sources have fully disconnected belief states.
It is clear that <si  B for all i. We make two observations: First, observe that pn > 1. If
p  1/3,
pn = pd1/p + 1e  1 + p > 1.
If p > 1/3,
pn = pd2/(1  p) + 1e  2p/(1  p) + p
which is a monotonically increasing function, so
pn > 2(1/3)/(1  1/3) + 1/3 = 4/3 > 1.
Second, note that the set described in the fifth condition is non-empty since the number
of sources described in conditions 2-4 is 2 + dpn  1e < 1 + pn which is less than n if
n > 1/(1  p). This is true if p  1/3 since for these values
n = d1/p + 1e > 1/p > 1/(1  p).
And it is also true if p > 1/3 since
n = d2/(1  p) + 1e > 2/(1  p) > 1/(1  p).
(x, y) appears only in <s1 and each of the belief states described in condition 3, so
countS (x, y) = 1 + dpn  1e  1 + pn  1 = pn
so (x, y)  vtp (S). Similarly, (y, z) appears only in <s2 and each of the belief states described
in condition 3, so
countS (y, z) = 1 + dpn  1e  1 + pn  1 = pn
so (y, z)  vtp (S). It remains to show that vtp (S) has no other members. We show
that the count of each pair is less than pn. countS (w, w) = 0 < pn for all w  W.
countS (z, w) = countS (w, x) = 0 for all w 6= y  W. For all w  W  {x, y}, (w, y)
only appears in <s1 , so countS (w, y) = 1 < pn from our first observation above. For all
w  W  {y, z}, (y, w) only appears in <s2 , so countS (y, w) = 1 < pn. Finally, for all
w  W  {x, y, z}, (x, z), (x, w), and (w, z) only appear in the belief states described in
condition 3, so
countS (x, z) = countS (x, w) = countS (w, z) = dpn  1e < pn.
2
Proposition 28 Suppose  is a transitive relation over W and x, y  W.
193

fiMaynard-Zhang & Lehmann

1. If x  y, then lev (x) < lev (y).
2. If x ./ y, then lev (x) = lev (y).
3. If lev (x) < lev (y), then z. lev (z) = lev (x)  z  y.
4. If lev (x) = lev (y), then x  y iff y  x.
Proof:
1. Suppose x  y. Then
lev (y) = 1 + max
0

y W



lev (y 0 ) : y 0  y

 1 + lev (x)
> lev (x).
2. Suppose x ./ y. If x  ch(W, ) then y  ch(W, ), so lev (x) = lev (y) = 0. Suppose x 6 ch(W, ). Then y 6 ch(W, ) and lev (x) = 1 + max
({lev (y 0 ) : y 0  x}).
0
y W

If y 0 is one such element, then y 0  y by transitivity, so


lev (y 00 ) : y 00  y  lev (x).
lev (y) = 1 + max
00
y W

By an identical argument, lev (x)  lev (y). Thus, lev (x) = lev (y).
3. Suppose lev (x) < lev (y). It is sufficient to prove the following: For every nonnegative integer l < lev (y), there exists z  y such that lev (z) = l. We prove by
induction on l. If l = lev (y)1, there must exist z  y and lev (z) = l by definition.
Assume there exists z  y for 0 < l < lev (y)  1 such that lev (z) = l. Since l > 0,
there exists z 0  z such that lev (z 0 ) = l  1. By transitivity, z 0  y.
4. Suppose lev (x) = lev (y). If x  y then x 6 y from the first part of this proposition,
otherwise lev (x) < lev (x), so y  x. Similarly, if y  x then y 6 x, so x  y.
2
Proposition 29 If  is a transitive relation over W, then MC()  B.
Proof: Let  = MC() and x, y, z  W. Suppose x  y. Then lev (x) < lev (y)
or lev (x) = lev (y) and x0 , y 0  W. (lev (x0 ) = lev (y 0 ) = lev (x)  x0 ./ y 0 ). If
lev (x) < lev (z) or lev (z) < lev (y), then x  z or z  y. Otherwise, lev (x) =
lev (y) = lev (z) and x0 , y 0  W. (lev (x0 ) = lev (y 0 ) = lev (x)  x0 ./ y 0 ), so x  z.
Thus,  is modular.
Now also suppose y  z. Then lev (y) < lev (z) or lev (y) = lev (z) and y 0 , z 0 
W. (lev (y 0 ) = lev (z 0 ) = lev (y)  y 0 ./ z 0 ). If lev (x) < lev (y) or lev (y) < lev (z),
then lev (x) < lev (z) by transitivity of <, so x  z. Otherwise, lev (x) = lev (y) =
lev (z) and x0 , y 0  W. (lev (x0 ) = lev (y 0 ) = lev (x)  x0 ./ y 0 ), so x  z. Thus,  is
transitive. 2
Proposition 30 Suppose  is a transitive relation over W and  = MC().
194

fiRepresenting and Aggregating Conflicting Beliefs

1.  and  .
2. If  is modular, then  =.
3. lev (x) = lev (x) for all x  W.
4. If 0  B such that 0 and lev0 (x) = lev (x) for all x  W, then  0 .
Proof:

Let x, y  W.

1. Suppose x  y. Then lev (x)  lev (y). If lev (x) < lev (y), x  y. Suppose
lev (x) = lev (y). Then x ./ y, so there exist x0 , y 0 such that lev (x0 ) = lev (y 0 ) =
lev (x) and x0 ./ y 0 , i.e., x0 = x and y 0 = y, so x  y.
Now suppose x  y. Then lev (x) < lev (y), so x  y and y 6 x, so x  y.
2. From the first part of this proposition,  , so it suffices to show  . Suppose
x  y.
Case 1: lev (x) < lev (y). Then, by Proposition 28, there exists z such that
lev (z) = lev (x) and z  y. By modularity, z  x or x  y. In the latter case,
were done. In the former case, z ./ x otherwise lev (z) 6= lev (x). Thus, x  y by
transitivity.
Case 2: lev (x) = lev (y). Then there exist x0 , y 0 such that lev (x0 ) = lev (y 0 ) =
lev (x) and x0 ./ y 0 . By modularity, x0  x or x  y 0 . In the former case x0 ./ x by
Proposition 28, in the latter x0 ./ x by Proposition 28 and transitivity. By modularity,
x0  y or y  x. Again applying Proposition 28 and transitivity, we have x ./ y, so
x  y.
3. We prove by induction on the level of x in .
Base case: lev (x) = 0. Then x  ch(W, ). Suppose y  x. Then lev (x) =
lev (y) and there exist x0 and y 0 such that lev (x0 ) = lev (y 0 ) = lev (x) = lev (y)
and x0 ./ y 0 , so x  y. Thus, x  ch(W,  ), so lev (x) = 0 = lev (x).
Inductive case: Assume that lev (x0 ) = lev (x0 ) for all x0 such that lev (x0 ) <
lev (x); we show that lev (x) = lev (x). Let z = arg max
({lev (y 0 ) : y 0  x});
0
y W

then lev (x) = 1 + lev (z). Also, let y = arg max
({lev (y 0 ) : y 0  x}); then
0
y W

lev (x) = 1 + lev (y). By the first part of this proposition,  , so y  x.
Thus, lev (z)  lev (y). Furthermore, lev (y) = lev (y) by the inductive hypothesis, so lev (z)  lev (y). Now lev (z) < lev (x) by Definition 26 (otherwise x  z, a contradiction). By the inductive hypothesis, lev (z) = lev (z), so
lev (z) < lev (x) = 1 + lev (y). Since levels are integral, lev (z)  lev (y), so
lev (z) = lev (y). Thus, lev (x) = 1 + lev (z) = 1 + lev (y) = lev (x).
4. Suppose 0  B, 0 , and lev0 (z) = lev (z) for all z  W. It is clear that for
any 00  B and x  W, x is in the partition corresponding to its level, i.e., Wlev00 (x) .
Since both  and 0 are both in B and both preserve the levels of all worlds in , the
must have identical partitions. Suppose x, y  W and Wi and Wj are the partitions
(for both  and 0 ) such that x  Wi and y  Wj .
195

fiMaynard-Zhang & Lehmann

Suppose x  y. If Wi 6= Wj then i < j by Proposition 7, so x 0 y, again by
Proposition 7. If, instead Wi = Wj , then lev (x) = lev (y). By Definition 26,
there exist x0 , y 0  W such that lev (x0 ) = lev (y 0 ) = lev (x) and x0 ./ y 0 . Thus,
x0 , y 0  Wi and, since 0 , x0 ./0 y 0 . By Proposition 7, Wi is either fully connected
or fully disconnected in 0 , so Wi must be fully connected in 0 . In particular, x 0 y.
2
Proposition 31 If  is a relation over W, then MT()  B.
Proof:

Since + is transitive, MT() = MC (+ )  B by Proposition 29. 2

Proposition 32 Suppose  is a relation over W and  = MT().
1.  .
2. If  is transitive, then  = MC().
3. If  is modular, then  =+ .
4. If  is modular and transitive, then  =.
5. If  has no conflicts, then neither does  .
Proof:

Let x, y  W.

1. + and, by the first property of Proposition 30, +  MC(+ ) = , so  .
2. Since  is transitive, + =. Thus,  = MC(+ ) = M C().
3. Since  is modular, + is modular by Proposition 1 so, by Proposition 30,
MC(+ ) =+ . Thus,  =+ .
4. Since  is transitive,  = MC() and since  is modular, MC() = by Proposition 30, so  =.
5. We first prove the following lemma:
Lemma 32.1 x and y are in conflict wrt + iff they are in conflict wrt .
Proof: The if direction is obvious since the transitive closure is a monotonically
additive operation. For the only if direction, suppose x and y are in conflict wrt
+ . Then there exist w0 , . . . , wn , z0 , . . . , zm  W such that
x = w0 +    + wn = y = z0 +    + zm = x.
But then, for each 0  i  n  1, there exist wi0 , . . . , wipi  W such that
wi = wi0      wipi = wi+1 .
Similarly, for each 0  j  m  1, there exist zj0 , . . . , zjqj  W such that
zj = zj0      zjqj = zj+1 .
Thus,
x = w0      wn = y = z0      zm = x,
so x and y are in conflict wrt . 2
196

fiRepresenting and Aggregating Conflicting Beliefs

Now, suppose x and y are in conflict wrt  . Then x ./ y since   B by Proposition 31. By Propositions 28 and 30,
lev+ (x) = lev (x) = lev (y) = lev+ (y)
So, since x  y, there exist x0 , y 0  W such that lev+ (x0 ) = lev+ (y 0 ) = lev+ (x)
and x0 ./+ y 0 by Definition 26. Thus, + has a conflict. By the lemma above,  must
also have a conflict.
2
Proposition 33 If S  S and p  [0, 1], then AGREqp (S)  B.
Proof:

Follows immediately from the definition of AGREqp and Proposition 31. 2

Proposition 34 If S  S and p  [0, 1], then AGRp (S)  B.
Proof:

Again, this follows immediately from Proposition 31. 2

Proposition 35 Suppose S  S and p  [0, 1].
1. If wS is fully connected, then AGRp (S) = AGREqp (S).
2. If wS is a total order, then AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).
3. AGR0 (S) = AGR(S).
Proof:

Assume x, y  W.

1. It suffices to show that AGRRf p (S) = vtp (S) when wS is fully connected. Suppose
(x, y)  AGRRf p (S). Then, by the definition of AGRRf p , there exists s  S such that
(x, y)  vtp ({s0  S : s0 S s}). Since wS is fully connected, {s0  S : s0 S s} = S,
so (x, y)  vtp (S).
Suppose, instead, (x, y)  vtp (S). By the definition of vtp , countS (x, y) > 0 so there
exists s  S such that x <s y. Pick one such s. Again, {s0  S : s0 S s} = S since
wS is fully connected, so (x, y)  vtp ({s0  S : s0 S s}). Finally, s0  S. s0 AS s 
0
x s y holds vacuously, so (x, y)  AGRRf p (S).
2. Suppose wS is a total order.
AGRRf(S) = AGR(S).

We have already shown in Proposition 17 that

Next we show that AGRRf p (S) = AGRRf(S). AGRRf p (S) is the set (x, y) such
that there exists s  S such that x <s y, (x, y)  vtp ({s0  S : s0 S s}), and,
0
for all s0 AS s  S, x s y. w is a total order, {s0  S : s0 S s} = {s}. Since
x <s y, so count{s} (x, y) = 1 > 0 and count{s} (x, y)/k{s}k = 1  p. Consequently,
(x, y)  vtp ({s0  S : s0 S s}), proving that this requirement is redundant when
wS is a total order. Thus, AGRRf p (S) is the set (x, y) such that x <s y and, for all
0
s0 AS s  S, x s y, i.e., AGRRf p (S) = AGRRf(S).
Finally, AGRp (S) = MT(AGRRf p (S)) = MT(AGRRf(S)). By Proposiion 14,
AGRRf(S) is modular and transitive, so AGRp (S) = AGRRf(S) by Proposition 32.
197

fiMaynard-Zhang & Lehmann

3. It suffices to show that AGRRf 0 (S) = AGRRf(S), since then
AGR0 (S) = MT(AGRRf 0 (S)) = MT(AGRRf(S)) = AGRRf(S)+
by Propositions 13 and 32, so AGR0 (S) = AGR(S).
0

Suppose (x, y)  AGRRf 0 (S). Then x <s y and, for all s0 AS s  S, x s y,
so (x, y)  AGRRf(S). Suppose, instead, (x, y)  AGRRf(S). Then x <s y and,
0
for all s0 AS s  S, x s y. Let S 0 = {s0  S : s0 S s}. Since x <s y and
s  S 0 , countS 0 (x, y) > 0 and countS 0 (x, y)/kS 0 k  0, so (x, y)  vt0 (S 0 ). Therefore,
(x, y)  AGRRf 0 (S).
2
Corollary 35.1 Let S = {s1 , . . . , sn }  S and AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRf
satisfies (the modified versions of ) restricted range, unrestricted domain, Pareto principle,
IIA, and non-dictatorship.
Proof:

Follows immediately from Propositions 35 and 19. 2

Proposition 36 Let A be an agent informed by a set of sources S  S, with support pedigreed belief state (l, sup, rtab), and using aggregation function AGRp for p  [0, 1]. As belief
state is the relation
MT ({(x, y) : ksup(x, y)k > 0 and ksup(x, y)k/krtab(l(x, y))k  p})
Proof:

Let
R = {(x, y) : ksup(x, y)k > 0 and ksup(x, y)k/krtab(l(x, y))k  p}

It suffices to show AGRRf p (S) = R. Suppose (x, y)  AGRRf p (S). Then there exists s  S
such that (a) x <s y, (b) (x, y)  vtp ({s0  S : s0 S s}), and (c) for all s0 AS s  S,
0
x s y. By the (a) and (c), rank(s) = max({rank(s) : x 6s y, s  S}  {}), so
l(x, y) = rank(s). Thus, {s0  S : s0 S s} = {s0  S : rank(s0 ) = l(x, y)} = rtab(l(x, y)),
so (x, y)  vtp (rtab(l(x, y))) by (b). By the definition of vtp , countrtab(l(x,y)) (x, y) > 0
and countrtab(l(x,y)) (x, y)/krtab(l(x, y))k  p. But sup(x, y) = {s0  S : rank(s0 ) =
0
0
l(x, y), x <s y} = {s0  rtab(l(x, y)) : x <s y}, so ksup(x, y)k = countrtab(l(x,y)) (x, y).
Thus, ksup(x, y)k > 0 and ksup(x, y)k/krtab(l(x, y))k  p, so (x, y)  R.
Now suppose (x, y)  R. Then (a) ksup(x, y)k > 0 and (b) ksup(x, y)k/krtab(l(x, y))k 
p. Suppose s  sup(x, y); by (a), at least one such s exists. By the definition of sup, x <s y,
satisfying the first condition of AGRRf p , and rank(s) = l(x, y). By the definition of l(x, y),
rank(s) = max({rank(s) : x 6s y, s  S}  {}), so for all s0  S such that rank(s0 ) >
0
rank(s) (i.e., s0 AS s), x s y. It only remains to show that (x, y)  vtp ({s0  S : s0 S s}).
Since rank(s) = l(x, y), {s0  S : s0 S s} = rtab(l(x, y)) as we showed above, so
vtp ({s0  S : s0 S s})
= vtp (rtab(l(x, y)))
= {(x0 , y 0 ) : countrtab(l(x,y)) (x0 , y 0 ) > 0, countrtab(l(x,y)) (x0 , y 0 )/krtab(l(x, y))k  p}.
As we showed above, ksup(x, y)k = countrtab(l(x,y)) (x, y). Making this substitution into (a)
and (b), we see that (x, y)  vtp ({s0  S : s0 S s}), so (x, y)  AGRRf p (S). 2
198

fiRepresenting and Aggregating Conflicting Beliefs

Proposition 37 Let A, PA , S, and wS be as in Definition 32. Then sup (PA ) is the
support pedigreed belief state of (A).
Proof:

Let sup (PA ) = (l, sup, rtab), l0 : W  W  R  {} such that
l0 ((x, y)) = max({l00 (x, y) : (l00 , sup00 , rtab00 )  PA }),

sup0 : W  W  2S such that
[

sup0 (x, y) =

sup00 (x, y),

(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)

and rtab0 : ranks(S)  R such that
rtab0 (r) =

[

rtab00 (r).

(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )

It suffices to show that l = l0 , sup = sup0 , and rtab = rtab0 .
Suppose x, y  W and agent Ai s support pedigreed belief state is (li , supi , rtabi ).
l(x, y) = max({rank(s) : x <s y, s  S}  {})


[
= max 
({rank(s) : x <s y, s  Si }  {})
Si informs Ai , Ai A


[
= max 
{max ({rank(s) : x <s y, s  Si }  {})}
Si informs Ai , Ai A
= max({l00 (x, y) : (l00 , sup00 , rtab00 )  PA })
= l0 (x, y).
Also,
sup(x, y) = {s  S : rank(s) = l(x, y), x <s y}
[
=
{s  Si : rank(s) = l(x, y), x <s y}
Si informs Ai , Ai A
[
=
{s  Si : rank(s) = l0 (x, y), x <s y}
Si informs Ai , Ai A
[
=
{s  Si : rank(s) = li (x, y), x <s y}
Si informs Ai , Ai A, li (x,y)=l0 (x,y)
[
=
sup00 (x, y)
(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)
0

= sup (x, y).

199

fiMaynard-Zhang & Lehmann

Finally,
rtab(r) = {s  S : rank(s) = r}
[
=
{s  Si : rank(s) = r}
Si informs Ai , Ai A
[
=
rtabi (r)
Si informs Ai , Ai A, rrange(rtabi )
[
=
rtab00 (r)
(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )
0

= rtab (x, y).
2

Appendix B. Notation key
: arbitrary finite set
a, b, c, . . .: specific elements of a set
x, y, z, . . .: arbitrary elements of a set
A, B, C, . . .: specific subsets of a set
X, Y, Z, . . .: arbitrary subsets of a set
: arbitrary set of relations
: arbitrary relation
+ : transitive closure of 
ch(X, ): choice set of X wrt 
kXk: cardinality of set X
W: finite set of possible worlds
w, W : element, subset of W, respectively
B: set of generalized belief states (modular, transitive relations)
: element of B, strict likelihood
: weak likelihood
: equal likelihood, agnosticism
./: conflict
Bel: belief of a conditional statement
Agn: agnosticism over a conditional statement
Con: conflict over a conditional statement
T : set of total preorders
T< : strict versions of total preorders
Q: set of total, quasi-transitive relations
Q< : strict versions of total, quasi-transitive relations
S: set of sources
s, S: element, subset of S, respectively
200

fiRepresenting and Aggregating Conflicting Beliefs

<s : belief state of source s
s : source agnosticism
./s : source conflict
R: set of ranks
r: element of R
rank(s): rank of source s
ranks(S): set of ranks of sources in S
w, wS : credibility ordering over S, S  S, respectively
Un: union of a set of belief states
AGRUn: aggregation via union
AGRRf: aggregation via refinement
AGR: general aggregation
A: set of agents
A: element of A
A : As induced belief state
(, l): pedigreed belief state
A
r : restriction of As pedigreed belief state to rank r
: fusion
ped : pedigreed fusion
vtp : voting function for p
lev: level of a world in a transitive relation
MC: modular closure
MT: modular, transitive closure
AGREqp : aggregation with voting without refinement
AGRRf p : aggregation with voting via refinement
AGRp : general aggregation with voting
(l, sup, rtab): support pedigreed belief state
sup : support pedigreed fusion

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. Journal of Symbolic Logic, 50, 510
530.
Andreka, H., Ryan, M., & Schobbens, P.-Y. (2002). Operators and laws for combining
preference relations. Journal of Logic and Computation, 12 (1), 1353.
Arrow, K. J. (1963). Social Choice and Individual Values (2nd edition). Wiley, New York.
Baral, C., Kraus, S., Minker, J., & Subrahmanian, V. S. (1992). Combining knowledge
bases consisting of first-order theories. Computational Intelligence, 8 (1), 4571.
Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2002). Possibilistic merging and distancebased fusion of propositional information. Annals of Mathematics and Artificial In201

fiMaynard-Zhang & Lehmann

telligence, 34 (13), 217252.
Black, D. (1958). The Theory of Committees and Elections. Cambridge University Press,
Cambridge.
Borgida, A., & Imielinski, T. (1984). Decision making in committees: A framework for
dealing with inconsistency and non-monotonicity. In Proceedings of the Workshop on
Nonmonotonic Reasoning, pp. 2132.
Boutilier, C. (1996). Iterated revision and minimal change of conditional beliefs. Journal
of Philosophical Logic, 25, 263305.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. In Arrow, K. J., Sen, A. K., &
Suzumura, K. (Eds.), Handbook of Social Choice and Welfare, Vol. 1 of Handbooks in
Economics, chap. 4, pp. 173236. Elsevier Science.
Cantwell, J. (1998). Resolving conflicting information. Journal of Logic, Language, and
Information, 7, 191220.
Center for Voting and Democracy
http://www.fairvote.org/irv/.

(2002).

Instant

runoff

voting.

Darwiche, A., & Pearl, J. (1997). On the logic of iterated belief revision. Artificial Intelligence, 89 (12), 129.
Fishburn, P. C. (1974). Lexicographic orders, utilities and decision rules: A survey. Management Science, 20 (11), 14421471.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
MIT Press.
Gardenfors, P., & Makinson, D. (1994). Nonmonotonic inference based on expectations.
Artificial Intelligence, 65 (1), 197245.
Gardenfors, P., & Rott, H. (1995). Belief revision. In Gabbay, D. M., Hogger, C. J., &
Robinson, J. A. (Eds.), Epistemic and Temporal Reasoning, Vol. 4 of Handbook of
Logic in Artificial Intelligence and Logic Programming, pp. 35132. Oxford University
Press, Oxford.
Grosof, B. (1991). Generalizing prioritization. In Proceedings of the Second International
Conference on Principles of Knowledge Representation and Reasoning (KR 91), pp.
289300.
Grove, A. (1988). Two modellings for theory change. Journal of Philosophical Logic, 17,
157170.
Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision under risk.
Econometrica, 47 (2), 263291.
Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision and minimal change. Artificial Intelligence, 52 (3), 263294.
Konieczny, S., & Perez, R. P. (1998). On the logic of merging. In Proceedings of the Sixth
International Conference on Principles of Knowledge Representation and Reasoning
(KR 98), pp. 488498.
202

fiRepresenting and Aggregating Conflicting Beliefs

Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models and cumulative logics. Artificial Intelligence, 44 (12), 167207. CoRR:
cs.AI/0202021.
Kreps, D. M. (1990). A Course in Microeconomic Theory. Princeton University Press.
Lehmann, D. (1995). Belief revision, revised. In Proceedings of the Fourteenth International
Joint Conference of Artificial Intelligence (IJCAI 95), pp. 15341540.
Lehmann, D., & Magidor, M. (1992). What does a conditional knowledge base entail?.
Artificial Intelligence, 55 (1), 160. CoRR: cs.AI/0202022.
Liberatore, P., & Schaerf, M. (1995). Arbitration: A commutative operator for belief revision. In Proceedings of the Second World Conference on the Fundamentals of Artificial
Intelligence (WOCFAI 95), pp. 217228.
Luce, D. R. (1956). Semiorders and a theory of utility discrimination. Econometrica, 24 (2),
178191.
Makinson, D. C. (1997). Screened revision. Theoria, 63 (12), 1423. Special issue on
non-prioritized belief revision.
Maynard-Reid II, P., & Shoham, Y. (2001). Belief fusion: Aggregating pedigreed belief
states. Journal of Logic, Language, and Information, 10 (2), 183209.
Meyer, T. (2001). On the semantics of combination operations. Journal of Applied NonClassical Logics, 11 (12), 5984.
Meyer, T., Ghose, A., & Chopra, S. (2001). Social choice, merging and elections. In Benferhat, S., & Besnard, P. (Eds.), Proceedings of the Sixth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU2001),
pp. 466477.
Ramsey, F. P. (1931). Foundations of Mathematics and Other Logical Essays. Routledge
and Kegan Paul, New York.
Revesz, P. Z. (1997). On the semantics of arbitration. International Journal of Algebra and
Computation, 7 (2), 133160.
Sen, A. (1986). Social choice theory. In Arrow, K. J., & Intriligator, M. D. (Eds.), Handbook of Mathematical Economics, Vol. III, chap. 22, pp. 10731181. Elsevier Science
Publishers.
Spohn, W. (1988). Ordinal conditional functions: A dynamic theory of epistemic states.
In Harper, W. L., & Skyrms, B. (Eds.), Causation in Decision, Belief Change, and
Statistics, II, pp. 105134. Kluwer Academic Publishers.
Williams, M.-A. (1994). Transmutations of knowledge systems. In Proceedings of the Fourth
International Conference on Principles of Knowledge Representation and Reasoning
(KR 94), pp. 619629.

203

fiJournal of Artificial Intelligence Research 19 (2003) 355-398

Submitted 10/02; published 10/03

An Architectural Approach to
Ensuring Consistency in Hierarchical Execution
Robert E. Wray

wrayre@acm.org

Soar Technology, Inc., 3600 Green Court, Suite 600
Ann Arbor, MI 48105 USA

John E. Laird

laird@umich.edu

The University of Michigan, 1101 Beal Avenue
Ann Arbor, MI 48109 USA

Abstract
Hierarchical task decomposition is a method used in many agent systems to organize
agent knowledge. This work shows how the combination of a hierarchy and persistent
assertions of knowledge can lead to difficulty in maintaining logical consistency in asserted
knowledge. We explore the problematic consequences of persistent assumptions in the
reasoning process and introduce novel potential solutions. Having implemented one of
the possible solutions, Dynamic Hierarchical Justification, its effectiveness is demonstrated
with an empirical analysis.

1. Introduction
The process of executing a task by dividing it into a series of hierarchically organized subtasks is called hierarchical task decomposition. Hierarchical task decomposition has been
used in a large number of agent systems, including the Adaptive Intelligent Systems architecture (Hayes-Roth, 1990), ATLANTIS (Gat, 1991a), Cypress (Wilkins et al., 1995),
the Entropy Reduction Engine (Bresina, Drummond, & Kedar, 1993), the Procedural Reasoning System (Georgeff & Lansky, 1987), RAPS (Firby, 1987), Soar (Laird, Newell, &
Rosenbloom, 1987; Laird & Rosenbloom, 1990), and Theo (Mitchell, 1990; Mitchell et al.,
1991), and is a cornerstone in belief-desire-intention-based agent implementations (Rao &
Georgeff, 1991; Wooldridge, 2000). Hierarchical task decomposition helps both an agents
knowledge developer and the agent itself manage environmental complexity. For example,
an agent may consider high-level tasks such as find a power source or fly to Miami
independent of low-level subtasks such as go east 10 meters or turn to heading 135.
The low-level tasks can be chosen dynamically based on the currently active high level
tasks and the current situation; thus the high-level task is progressively decomposed into
smaller subtasks. This division of labor simplifies the design of agents, thus reducing their
cost. Additional advantages of hierarchical task decomposition include knowledge sharing
(a low-level subtask can be invoked for many different high-level procedures), modularity
(the decomposition helps insulate subtasks from interaction with other knowledge) and the
naturalness of this representation (Simon, 1969).
Without careful design, it can be difficult to ensure consistent reasoning in agents employing hierarchical task decompositions. By consistency, we mean that reasoning does
not lead to a set of assertions that contains a contradiction. Ensuring consistency becomes
c 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWray & Laird

much more difficult to solve  and thus more costly  as the complexity of an agents
knowledge grows. Although this problem can be solved through careful design of agent
knowledge, such an approach requires an understanding of all possible interactions in the
hierarchy. Thus, the correctness of this solution depends on the skill and vigilance of the
knowledge engineer. Our bias is to seek solutions in which the operation of an agents primitive memories and processes are structured to ensure inconsistencies do not arise. Thus,
we will prefer architectural solutions to knowledge-based ones. Architectural solutions can
guarantee consistency for all tasks and domains, reducing brittleness due to omissions in
task knowledge. Further, while developing an architectural solution may be costly, it should
be less costly than repeatedly developing knowledge-based solutions for different domains.
The following sections describe the inconsistency problem and introduce a space of
solutions to the problem, including two novel solutions. Through both theoretical and
empirical analysis, one of the new solutions, Dynamic Hierarchical Justification, is shown
to provide an efficient architectural solution to the problem of ensuring reasoning consistency
in hierarchical execution.

2. Maintaining Reasoning Consistency in Hierarchical Agents
This section describes the inconsistency problem in greater detail. We review methods
for ensuring consistency in non-hierarchical systems and discuss the limitations of these
approaches in hierarchical systems.
2.1 Consistency in Non-hierarchical Systems
Truth maintenance systems (TMSs) are often used to maintain consistency in non-hierarchical systems (Doyle, 1979; McDermott, 1991; Forbus & deKleer, 1993). An inference engine
uses domain knowledge to create two different kinds of assertions of knowledge in an agents
knowledge base: assumptions and entailments. The inference engine enables assumptions
that it has decided to treat as being true, without requiring that the assertion be justified. Agents often treat environmental percepts as assumptions or unquestioned beliefs
(Shoham, 1993). Entailments are justified assertions. A data structure, the justification,
captures the reasons for asserting the entailment. When the reasons no longer hold (the
entailment is no longer justified), the TMS retracts it from the set of asserted beliefs. Thus,
a TMS automatically manages the assertion and retraction of entailments as an agents
situation changes, ensuring all entailments are consistent with the external environment
and the enabled assumptions.
Careful construction of the domain knowledge is required to ensure that no enabled
assumptions are contradictory. For example, if some assumption is inconsistent with the
current input, then the agent must have domain knowledge that recognizes the situation and
removes the assumption. Thus, when an agent utilizes a TMS, the problem of maintaining
consistency in reasoning is largely one of managing assumptions through the agents domain
knowledge.
Assumptions often reflect hypothetical reasoning about the world (hence assumptions). However, assumptions can be used to represent any persistent feature. Although
researchers have explored structuring the external environment to provide persistent memory (Agre & Horswill, 1997), internal, persistent memory is usually necessary in agent
356

fiEnsuring Consistency in Hierarchical Execution

Assumptions
Entailments
Assumptions
Entailments
Assumptions

Inference
Engine

Entailments
Assumptions

TMS

Entailments
Assumptions
Entailments
Assumptions
Entailments
Assumptions
Entailments

Memory
Hierarchy Maintenance

Figure 1: A hierarchical agent.
domains. For example, persistence is required for hypothetical reasoning, nonmonotonic
revisions of assertions (such as when counting), and remembering.
2.2 Truth Maintenance in Hierarchical Agents
The TMS agent framework introduced above can be extended to hierarchical agent architectures. In such an agent, the inference engine and TMS are more or less identical to those of
a non-hierarchical agent. When the agent initiates a new subtask via dynamic hierarchical
task decomposition, it also creates a new database that will contain assumptions and entailments specific to the subtask. Further decomposition can result in a stack of subtasks, each
containing entailments and assumptions specific to the subtask, as shown in Figure 1. We
consider the creation and deletion of these distinct databases of assertions the sine qua non
of a hierarchical architecture. The architecture decomposes the task not only by identifying
relevant subtasks, but also by dynamically organizing its memory according to the current
decomposition.
A new system component, hierarchy maintenance, is responsible for creating and
destroying the subtask databases when subtasks begin and terminate. When a subtask is
achieved (or determined to be no longer worth pursuing), hierarchy maintenance responds
by immediately removing all the assertions associated with the subtask. This function is of
central importance in hierarchical architectures because it allows the agent to automatically
retract all assertions associated with a terminated subtask, requiring no agent knowledge
to clean up or remove individual assertions associated with the terminated subtask. The
hierarchy maintenance component can efficiently remove the assertions because they are
(conceptually) located in a distinct unit, the subtask database.
357

fiWray & Laird

Subtask1

a11
e11

a12

a14
a13

a15

Subtask2
e14
e12
e17
e15
e19
e13
e18
a21
e16a11 a12
a22
a13

e21

e22

(a14, a15, e14, e19)
PSfrag replacements

Subtask3

a31

e23

a34
a32

e31

e32

(a12, a22, e22)

Hierarchy Maintenance

Figure 2: An example of hierarchy maintenance. Assumptions (a) and entailments (e)
are asserted within subtasks.

An agents hierarchy maintenance function can be employed to help maintain consistency, illustrated notionally in Figure 2. The agent architecture identifies assertions at each
of the higher levels in the hierarchy that led to a new subtask. These assertions together
form a subtask support set. In Figure 2, assertions a 14 , a15 , e14 , and e19 form the support
set for Subtask2 while a12 , a22 , e22 support Subtask3 . These support sets, in effect, form
justifications for subtasks in the hierarchy. When an assertion in a support set is removed
(e.g., a22 ), the agent responds by removing the subtask (Subtask 3 ). While not all hierarchical architectures use architectural processes to create and destroy subtask databases, this
example illustrates how an architectural hierarchical maintenance function can be realized
via a process similar to that of justification in truth maintenance.
Within a specific subtask, reason maintenance can go on as before. However, the hierarchical structure adds a complication to the maintenance of logical consistency. Assumptions
at some level in the hierarchy can be dependent on entailments and assumptions in higher
levels of the hierarchy.1 This dependence relationship is suggested in Figure 1 by the curved
lines extending from one subtask to the one below it. Higher levels of the hierarchy form a
context for reasoning in the local subtask.
For execution agents embedded in dynamic domains, the hierarchical context may
change at almost any time. The changing context is not problematic for entailments; the
1. Assumptions in a lower level subtask are always at least indirectly dependent on the higher level assertions. This observation will be exploited in Section 3.3.

358

fiEnsuring Consistency in Hierarchical Execution

TMS can readily determine dependent context changes and retract affected entailments.
However, changes in higher levels of the hierarchy (such as those deriving from inputs)
may also invalidate the assumptions of lower levels. Without any additional architectural
mechanisms, domain knowledge is required to ensure consistency among assumptions and
the hierarchical context as in non-hierarchical systems. The domain knowledge for ensuring consistency in the assumptions is complicated by the necessity of spanning multiple
(possibly many) subtasks. We refer to such knowledge as across-level consistency knowledge. As described in further detail below, identifying and creating across-level consistency
knowledge is a tedious, costly, and often incomplete process. Across-level knowledge must
explicitly consider the interactions between different subtasks (in different levels of the hierarchy), rather than focus solely on the local subtask, compromising the benefit of the
hierarchical decomposition.
Before continuing, we note that hierarchical architectures should be contrasted with
hierarchical task network (HTN) planners (Sacerdoti, 1975; Erol, Hendler, & Nau, 1994)
and execution-oriented systems that use HTN representations, such as DECAF (Graham
& Decker, 2000) and RETSINA (Sycara, Decker, Pannu, Williamson, & Zeng, 1996). A
planning problem for an HTN planner is represented by an initial task network that can
consist of primitive and non-primitive tasks. The planner uses operators to find a plan
to solve the tasks. Methods allow the planner to match non-primitive tasks with other
task networks that describe how to accomplish the task; thus, methods enable hierarchical
decomposition of the planning problem into a family of connected task networks.
The main difference between HTN systems and hierarchical architectures is that the
planner represents its plan in a single global state. That is, while methods represent decomposition steps, the hierarchical structure of an evolving plan is represented in a blackboardlike database that does not also reflect the structure of the decomposition. The following
sections discuss problems and especially solutions that depend on the hierarchical organization of asserted knowledge during execution, in addition to a hierarchical task decomposition encoded as an agents task knowledge. Thus, the following will not be generally
applicable to HTN-based execution systems. However, HTN systems need to address the
inconsistency problem; Section 5.1.1 examines the consequences of global state with respect
to inconsistency arising from persistence in a hierarchy.
2.3 Failing to Respond to Relevant Changes in Hierarchical Context
As mentioned in the introduction, when an agent fails to respond to a relevant change
in its hierarchical context and leaves a now-inconsistent assumption enabled, the resulting
behavior can become irrational; that is, not consistent with its knowledge. This section
explores how such irrational behavior can arise with several illustrative examples.
2.3.1 The Blocks World
We use a variant of the blocks world to illustrate the inconsistency problem in a domain
familiar to most readers. This domain is an execution domain rather than a planning
domain, which we call the Dynamic Blocks World to reflect this difference from the static
blocks world used in planning. We assume the agent has knowledge to build an ordered
359

fiWray & Laird

Goal: (1 on 2) (2 on 3) (3 on table)
Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

empty
space

empty
space

empty
space

2
3
1

2
3
1

Actual World State

2

3
1

time

2

2
3
1

ent
l ev 3
a
n
r
Exte ks over
c
kno

Actual World State

time

2
3 1

3 1

Actual World State

Figure 3: Failing to respond to relevant changes in hierarchical context in the Dynamic
Blocks World.

tower (1-on-2-on-3) without resorting to planning and uses hierarchical task decomposition
to determine what actions to take as it builds the tower.
In Figure 3, the agent is placing block-2 on the table, in order to reach block-3 and
begin the goal tower. The put-down subtask finds an empty location on the table. The
agent places the empty assertion in the memory associated with the put-down subtask. In
the figure, the space immediately to the left of the gripper was chosen. Whether or not a
space is empty may not be directly observable but may need to be inferred from a number
of other facts in the domain and stored as an assumption in memory. Assume the empty
assertion is an assumption. Now, assume block-3 is suddenly placed underneath block-2.
The result is an inconsistency between the assumption (the location is a good place to put
block-2) and the hierarchical context (the location is no longer a good place to put the
block on the table).
If the agent fails to recognize that block-3 has moved, it will attempt to put block-2
into the same location occupied by block-3. This behavior is irrational, or not consistent
with the agents goals and knowledge (assuming the agent has knowledge that indicates
that blocks should not be placed in positions already occupied by other blocks). The inconsistency arises because the agent has failed to recognize its previously-derived assumption
(empty) is no longer true in the current situation.
Although this example may appear contrived, this specific situation arose in an experimental system developed to explore architecture and learning issues. Of course, it is possible
in such a simple domain to reformulate the task such that the problem does not occur. This
reformulation of the task via changes to or additions of knowledge is exactly the solution
we wish to avoid. That is, we desire that the architecture guarantee consistency between
the hierarchical context and local assumptions such that the architecture provides a priori
constraints (guidance) in the knowledge development process and increased robustness in
execution (via consistency). The conclusion returns to this example to describe how an
360

fiEnsuring Consistency in Hierarchical Execution

patrol
intercept
attack(defensive)
achieveproximity
turntoheading

Figure 4: Decomposition of behavior into subtasks.
architectural solution to the inconsistency problem solves this particular problem  without
requiring any reformulation of the agents task knowledge.
2.3.2 TacAir-Soar
TacAir-Soar agents pilot virtual military aircraft in a complex, real-time computer simulation of tactical combat (Tambe et al., 1995; Jones et al., 1999). The TacAir-Soar domain is
only indirectly accessible (each agent uses simulated aircraft sensor models and can perceive
only what a pilot in a real aircraft would sense), nondeterministic (from the point of view
of the agent, the behavior of other agents cannot be strictly predicted or anticipated), nonepisodic (the decisions an agent makes early in the simulation can impact later options and
capabilities), dynamic (the world changes in real time while the agent is reasoning), and
continuous (individual inputs have continuous values). Domains with these characteristics
are the most difficult ones in which to create and apply agents (Russell & Norvig, 1995).
The domain knowledge of TacAir-Soar agents is organized into over 450 subtasks; during
execution, the resulting hierarchical task decomposition sometimes reaches depths of greater
than 10 subtasks. Each agent can have one of several different mission roles, among them
flying a patrol mission, and acting as a partner or wing to some other agents lead.
Consider a pair of planes on patrol, which have been given specific instructions for
engaging enemy aircraft. When enemy aircraft enter the patrol area, the lead agent decides
to intercept the aircraft. The lead then decomposes the intercept into a series of situationdependent subtasks, which themselves may be further decomposed. For example, Figure 4
shows that the complex task of intercepting an enemy aircraft has been decomposed into a
decision to turn the agents aircraft to a specific heading. The agent turns to this heading
in order to get close enough to the enemy agent (via achieve-proximity) to launch an
attack.
Assume three different kinds of attack can be chosen for an intercept. The first tactic
(scare) is to engage and attempt to scare away enemy planes without using deadly force.
This tactic is selected when the rules of engagement specify that deadly force should not be
used, regardless of the number of aircraft in the area. One of the remaining two tactics will
be chosen when deadly force is allowed. Offensive attack is appropriate when friendly
patrol
intercept
count (enemy)

patrol
intercept
count(friendly)

patrol
intercept
attack(defensive)
achieveproximity
turntoheading

Figure 5: Trace of behavior leading to intercept tactic in TacAir-Soar.
361

fiWray & Laird

Figure 6: Inconsistency due to persistence.

planes outnumber or equal enemy planes. Defensive attack is used when enemy planes
outnumber friendly planes.
Choosing between offensive and defensive attack requires counting the current aircraft
in the area. Figure 5 shows the evolution of an executing example decomposition. The
agent must count relevant enemy and friendly planes. Determining a planes side and
its relevance to the count often requires remembering and is sufficiently complex that entailment of the count is not possible. For instance, non-combatant aircraft should not be
counted, requiring some reasoning about the type of each aircraft. If the agent determines
that enemy planes outnumber friendly ones, the agent selects defensive-attack, leading
to further decomposition.
What happens if an enemy plane flees, thus reducing the actual count of relevant enemy planes by one? The count maintained by the agent is now invalid. Standard TMS
mechanisms are insufficient because the count was asserted as an assumption. If the actual
number of enemy and friendly planes is now equal, then the agent should switch its tactic to offensive attack. Continuing the defensive attack is not consistent with the agents
knowledge. Additionally, other friendly agents participating in the attack may base their
behavior on the expectation that the agent is pursuing an offensive attack. Thus the agent
needs to recognize the inconsistency and remove the current count.
Figure 6 presents a conceptual illustration of the problem. Assumptions are represented
as squares, entailments as circles. The horizontal line represents a hierarchical relationship
between the assertions (i.e., assumptions and entailments) in the hierarchical context (above
the line) and assertions in a local subtask (below the line). The arrowed lines represent
dependence in the creation of an assertion. As in the previous examples, some reasoning
in the subtask may require persistence, leading to the creation of an assumption such as
assumption 1. However, the persistent assertion may still depend on other assertions. This
work focuses on the dependent assertions in the higher level context, such as A, B, C, D,
and E1 in the figure.
Suppose the world changes so that E1 is retracted from memory and E2 is asserted.
Assumption 1 remains in memory. If E 2 would not also lead to 1 (e.g., it could lead to some
new assumption 2, as shown), then 1 is no longer justified and may not be consistent with
the higher level context. Whether or not this potential inconsistency among the assertions
leads to inconsistent behavior depends on the use of assumption 1 in later reasoning.
362

fiEnsuring Consistency in Hierarchical Execution

3. Solutions
Our goal is to develop architectural solutions that allow an agent to support persistent
assumptions and simultaneously avoid inconsistencies across the hierarchical context that
can lead to irrational behavior. Before introducing two new architectural solutions, however,
we examine knowledge-based approaches and their consequences in order to provide further
rationale for the architectural approach.
3.1 Knowledge-based Solutions
Inconsistency can be avoided in hierarchical agents by creating domain knowledge that
recognizes potential inconsistencies and responds by removing assumptions. Many planning and agent systems use explicit domain knowledge to represent knowledge about the
interactions among assertions in the world. For example, the Entropy Reduction Engine
(ERE) (Bresina et al., 1993) is one agent system that relies on this knowledge-based assumption consistency (KBAC). ERE requires domain constraints, or knowledge that describes
the physics of the task domain. Domain constraints identify impossible conditions. For
instance, a domain constraint would indicate that a robot cannot occupy two different
physical locations simultaneously.
In ERE, domain constraints are specifically used to maintain consistency in the current
world model state during execution (Bresina et al., 1993, pp. 166). However, many other
architectures use KBAC as well (perhaps in conjunction with other methods). KBAC
knowledge can be viewed simply as domain knowledge that must be added to the system
to achieve consistent behavior.
KBAC will always be necessary to maintain consistency among the assumptions within
a level of the hierarchy. However, in order to guarantee consistency for assumptions distributed throughout the hierarchy, all possible interactions leading to inconsistency must
be identified throughout the hierarchy. This knowledge engineering problem can add significant cost to agent development. A knowledge designer must not only specify the conditions
under which an assumption is asserted but also all the conditions under which it must be
removed. In the TacAir-Soar interception example, when the enemy plane flees, the agent
requires knowledge that disables all assumptions that depend upon the number of enemy
airplanes. Similarly, in the Dynamic Blocks World, the agent must have knowledge that
recognizes any situation, in any subtask, that should cause the disabling of empty. In both
cases, this KBAC knowledge crosses levels of the hierarchy. A complete KBAC solution
requires that an agents knowledge capture all potential dependencies between assumptions
in a local subtask and any higher levels in the hierarchy.
Although it will be possible to encode complete across-level consistency knowledge for
simple domains, experience in TacAir-Soar and other complex agent systems has convinced
us that KBAC requires significant investments of time and energy. Further, because it is
often not possible to enumerate all conditions under which an assumption must be removed,
agents are also brittle, failing in difficult-to-understand, difficult-to-duplicate ways.
The insufficiency of knowledge-based solutions led us to consider architectural solutions
to the problem. Architectural solutions eliminate the need for domain knowledge encoded
only to address inconsistency between the hierarchical context and assumptions within a
subtask. Thus, the cost of developing individual agents should be reduced. In addition to
363

fiWray & Laird

their generality, by definition, architectural solutions are also complete, and thus able to
guarantee consistency between the hierarchical context and assumptions within a subtask,
at all times, for any agent task. Such completeness should improve the robustness of agent
systems, especially in situations not explicitly anticipated by their designers.
3.2 Assumption Justification
One potential architectural solution to the inconsistency problem is to justify each assumption in the hierarchy with respect to assertions in higher levels of the hierarchy. Assumption
Justification is an extension of the truth maintenance approaches to consistency outlined
previously. Each assumption in the hierarchy is treated as if it were an entailment with
respect to dependent assertions higher in the hierarchy. A new data structure, the assumption justification, is created that captures the reasons in the hierarchical context for
a particular assumption. Locally, an assumption is treated exactly like an assumption in a
non-hierarchical system. However, when the assumption justification is no longer supported
(indicating a change in the dependent hierarchical context), the architecture retracts the
assumption.
Refer again to Figure 2. When the agent asserts a 34 , the architecture builds an assumption justification for the assumption that includes a 22 and a21 . If the agent retracts
a21 , the assumption justification for a 34 is no longer supported and the architecture also
retracts a34 . The architecture ensures reasoning consistency across hierarchy levels because
an assumption persists no longer than the context assertions that led to its creation.
Assumption Justification solves the inconsistency problem because all dependencies in
the hierarchical context are captured in the justification. Within the subtask, domain knowledge is still required to ensure consistency among the enabled assumptions in the subtask.
However, no across-level consistency knowledge is needed. Assumption Justification still
supports local nonmonotonic and hypothetical reasoning. Thus, Assumption Justification
appears to meet functional evaluation criteria. However, in order to assess its impact on
performance, some implementation details must be considered.
3.2.1 Implementing Assumption Justification
Creating assumption justifications requires computing context dependencies for each assumption, similar to the computation of justifications for entailments. 2 Figure 7 outlines
a procedure for computing the assumption justification data structure. This procedure is
invoked when any assertion is created. This procedure creates assumption justifications for
every assertion in a local subtask; that is, for entailments as well as assumptions. This approach allows the architecture to cache context dependencies for each local assertion. The
advantage of this caching is that the architecture can simply concatenate the assumption
justifications of the local assertions contributing directly to the creation of the assumption
2. The Assumption Justification procedure, as presented, requires that the inference engine record a justification for every assertion during the course of processing. In Soar, the architecture in which Assumption
Justification was implemented, these calculations are available from its production rule matcher for both
assumptions and entailments. However, justification calculations for assumptions may not be supported
in other architectures, requiring modifications to the underlying inference engine. Laird and Rosenbloom
(1995) and the Soar Users Manual (Laird, Congdon, & Coulter, 1999) describe the specific mechanisms
of justification creation in Soar.

364

fiEnsuring Consistency in Hierarchical Execution

PROC create new assertion(. . .)
An assumption justification is computed when each new assertion A is
created. Thus, assumption justifications are computed for both
assumptions and entailments.
...
Ajust  create justif ication(. . .)
Justifications can be created via well-known, textbook algorithms
(e.g., Forbus & deKleer, 1993; Russell & Norvig, 1995)
Aaj  make assumption justif ication f or assertion(A)
...
END
PROC make assumption justif ication f or assertion(assertion A)
AJ  NIL
FOR Each assertion j in Ajust , the justification of A

1
IF (Level(j) closer to the root than Level(A))
AJ  append(j, AJ) (add j to the assumption justification)

2
ELSE
(j and A at the same level)
AJ  concatenate(jaj , AJ) (add assumption justification of j to
assumption justification of A)
return AJ, a list of assertions comprising the assumption justification of A
END
PROC Level(assertion A)
Return the subtask level associated with assertion A
Figure 7: A procedure for building assumption justifications.
(in 
2). We chose this caching option for computing assumption justifications over an ondemand implementation that would, when an assumption was created, recursively follow
local dependencies until all context dependencies were determined. The advantage of the
caching implementation is that the context dependencies for any assertion must be computed only once, even when a local assertion contributes to the creation of multiple local
assumptions.
The procedure that creates an assumption justification loops over the assertions in the
justification of a new assertion A. The assertions in the justification can be either context or
local assertions. Context assertions, in 
1, are added to the assumption justification directly.
However, local assertions should not be added to the assumption justification because the
assumption justification should include only context dependencies. For example, the architecture can retract a local assertion for reasons other than a change in the hierarchical
context (e.g., a non-monotonic reasoning step or a change in enabled assumptions in the
subtask) and in these cases, the agent should not necessarily retract dependent assumptions.
Because assumption justifications for all local assertions in the justification have already
been computed (i.e., they are cached, as described above), the assumption justification of a
365

fiWray & Laird

A B C D E

A B C D E

PSfrag replacements

(a)

12

(b)

1

Figure 8: Technical problems with Assumption Justification. In (a), an assumption replaces
another assumption nonmonotonically. In (b), multiple assumption justifications
for the same assumption must be supported.

local assertion j can simply be added to the assumption justification of A, as in 
2. For an
on-demand implementation, the procedure here would recur through local assertions, until
all context dependencies for local assertions contributing to A had been identified.
The worst-case computational complexity of this algorithm is polynomial in the number
of assertions in the subtask. The addition of a higher-level assertion can be done in constanttime (a single pointer reference). However, 
2 must uniquely add context assertions from the
assumption justification of the local assertion. There are at most (n  1) local assumption
justifications whenever the nth assertion is created. Thus, the concatenation needs to be
performed no more than (n1) times for any call to the assumption justification procedure.
This limit provides an upper bound of O(n) on the complexity of the assumption justification
procedure: the worst-case cost of building an individual assumption justification is linear in
the number of assertions, n, in the level. However, the architecture executes the assumption
justification procedure for every assertion in the level. Thus, the worst case cost for building
all the justifications in a particular level is O(1 + 2 . . . + n) or O(n 2 ).
Non-monotonic changes complicate the implementation. The architecture must disable
a replaced assumption, rather than delete it, because the initial assumption may need to
be restored. For example, in Figure 8 (a), assume that the assertion of E leads to both
the assertion of 2 in the local subtask and the retraction of 1 (i.e., 2 is a revision of 1). If
the agent retracts E, Assumption Justification will retract 2, as desired, but it must also
re-enable 1. Thus, assumption 1 must remain available in memory, although disabled.
Figure 8 (b) illustrates a second problem. An assumption can have multiple assumption justifications. These justifications can change as reasoning progresses. Assumption 1
initially depends on assertions A, B, and C in higher levels. Now assume that later in
the processing, the agent removes A, which normally would result in the retraction of 1.
However, in the meantime, the context has changed such that 1 is now also justified by {C,
D, E}. Now when the agent removes A, the architecture should not immediately retract 1
but must determine if 1 is justified from other sources.
An implementation of Assumption Justification in Soar was completed by members of
the Soar research group at the University of Michigan. Experiments using Air-Soar, a flight
simulator domain (Pearson et al., 1993), showed that the overhead of maintaining all prior
assumptions in a level produced a significant negative impact on agent performance. In
this domain, Assumption Justification incurred significant computational cost, requiring at
366

fiEnsuring Consistency in Hierarchical Execution

least 100% more time than the original Air-Soar agent. Further, the number of assumption
justifications maintained within a level continued to grow during execution, for the reasons
explained above. Some subtasks required minutes to execute as the aircraft performed a
maneuver, leading to large (and problematic) increases in the amount of memory required.
Thus, Assumption Justification failed to meet efficiency requirements on both theoretical
and empirical grounds. Although the limitations of Assumption Justification might be improved by developing solutions to its technical problems, we abandoned further exploration
of this approach after such strongly discouraging results.
3.3 Dynamic Hierarchical Justification
Figure 2 introduced the notion of a support set for subtasks. Both the Procedural Reasoning
System (PRS) (Georgeff & Lansky, 1987) and Soar (Laird et al., 1987) use architectural
mechanisms to retract complete levels of a subtask hierarchy when the support set no longer
holds. In this section, we consider a solution that leverages the hierarchy maintenance
function to ensure consistency between assumptions and the higher level context.
A significant disadvantage of the support set in existing systems is that it is fixed. In
Soar, the support set is computed for the initiation of the subtask but is not updated to
reflect reasoning that occurs within the subtask. For example, in Figure 2, suppose that
assumption a34 depends on assumptions a22 and a21 (represented by the dashed, arrowed
lines). The support set does not include a 21 ; this assertion may not have even been present
when Subtask3 was created. When a local assumption depends on an assertion not in the
support set, then a change in that assertion will not directly lead to the retraction of the
assumption (or the subtask). Thus, approaches using such Fixed Hierarchical Justification
(FHJ) still require knowledge-based solutions for consistency. FHJ is discussed further in
Section 5.1.2.
We propose a novel solution, Dynamic Hierarchical Justification (DHJ), that is similar
to Fixed Hierarchical Justification, but dynamically updates the support set as reasoning
progresses. Assumption justifications for individual assumptions are unnecessary. However,
one consequence of this simplification is that a subtask (and all assertions within it) will
be retracted when the dependent context changes. Refer to Figure 2. When a DHJ agent
asserts a34 in Figure 2, the architecture updates the support set for Subtask 3 to include a21 .
Assumption a22 is already a member of the support set and need not be added again. When
any member of the support set for Subtask 3 changes, the architecture retracts the entire
subtask. Thus Dynamic Hierarchical Justification enforces reasoning consistency across the
hierarchy because a subtask persists only as long as all dependent context assertions.
3.3.1 Implementing Dynamic Hierarchical Justification
Figure 9 outlines the procedure for computing the support set in DHJ. As in Assumption
Justification, the architecture can directly add context assertions to the support set 
1.
When the architecture computes the dependencies for a local assertion 
3, the assertion
is marked as having been inspected 
4. Inspected assertions can simply be ignored in the
future 
2, because the architecture has already added the assertions dependencies to the
support set. The architecture will also ignore dependent, local assumptions 
2 because
the dependencies of those assumptions will have already been added to the support set.
367

fiWray & Laird

PROC create new assertion(. . .)
Whenever a new assumption is asserted, the support set is updated
to include any additional context dependencies.
...
Ajust  create justif ication(. . .)
IF A is an assumption
S is the subtask in which A is asserted
Ssupport set  append(Ssupport set , add dependencies to support set(A))
...
END
PROC add dependencies to support set(assertion A)
FOR Each assertion j in Ajust , the justification of A

1
IF {Level(j) closer to the root than Level(A)}
append(j, S) (append context dependency to support set)

2


3

4

ELSEIF {Level(j) same as Level(A) AND
j is NOT an assumption AND
j has not previously been inspected }
S  append(S, add dependencies to support set(j))
(compute support set dependencies for j and add to S)
jinspected  true
(js context dependencies have now been added to the support set)

return S, the list of new dependencies in the support set
END
PROC Level(assertion A)
Return the subtask level associated with assertion A
Figure 9: A procedure for Dynamic Hierarchical Justification.
Because DHJ needs to inspect any local assertion only once, context dependencies are
computed on-demand, rather than cached as in Assumption Justification. Condition 
2 will
be true whenever there is a local entailment whose context dependencies have not yet been
computed. These dependencies are determined by calling add dependencies to support set
recursively. Recursive instantiations of add dependencies to support set each receive a local
assertion in the justification of an uninspected entailment, j, and return a list comprising
the context dependencies of j. The return value is then appended to the support set S in
the prior instantiation of add dependencies to support set.
The recursive call to add dependencies to support set at 
3 is the only non-constant time
operation in the procedure. It must be made only once for any assertion j i and thus the
worst case complexity to compute the dependencies is linear in the number of assertions in
the level, as in Assumption Justification. However, DHJ requires only a single inspection of
any individual assertion, rather than repeated inspections for each new assumption as in As368

fiEnsuring Consistency in Hierarchical Execution

sumption Justification. Thus the architecture needs to call add dependencies to support set at most n times for any subtask consisting of n assertions, and the worst case cost of
updating the support set in a level remains O(n). This reduction in complexity potentially makes Dynamic Hierarchical Justification a more efficient solution than Assumption
Justification, especially as the number of local assertions increases.
Additionally, the two technical problems outlined for Assumption Justification do not
impact DHJ. DHJ never needs to restore a previous assumption. When a dependency
changes, the architecture retracts the entire level. Thus, DHJ can immediately delete
replaced assumptions from memory. DHJ collects all dependencies for assumptions, so
there is no need to switch from one justification to another. In Figure 8 (b), dependencies
A, B, C, D, and E are all added to the support set. These simplifications can make the
support set overly specific but reduce the memory and computation overhead required by
Dynamic Hierarchical Justification.
DHJ retractions will sometimes be followed by the regeneration of the subtask and
the re-assertion of reasoning that was retracted. For example, if the enemy plane fled as
described in the TacAir-Soar scenario, DHJ would retract the entire level associated with
the counting subtask. The count would then need to be re-started from the beginning.
Section 3.4.3 examines potential problems introduced by interruption and regeneration.
The cost incurred through the regeneration of previously-derived assertions is the primary
drawback of Dynamic Hierarchical Justification.
3.4 Implications of Dynamic Hierarchical Justification
Dynamic Hierarchical Justification solves the specific problem of maintaining reasoning
consistency in a hierarchy, guaranteeing consistency and utilizing an efficient algorithm.
The heuristic DHJ employs assumes that assumptions are so closely associated with their
subtasks that retracting subtasks is nearly equivalent to retracting individual assumptions.
This section explores the implications of this heuristic, focusing on task decompositions, the
impact on the agents ability to use persistent assumptions, and the feasibility of interrupting
an agent (with a subtask retraction) in the midst of reasoning.
3.4.1 The Influence of the Task Decomposition
An agents reasoning can be viewed as knowledge search (Newell, 1990). From this perspective, the inconsistency problem is failure to backtrack in knowledge search. The world
changes, leading to changes in the agent hierarchy. The agent must retract some of the
knowledge it has previously asserted, so it should backtrack to a knowledge state consistent
with the world state.3 Each solution can be described in terms of the way its achieves (or
avoids) backtracking in the knowledge search. For instance, KBAC leads to knowledge-based
backtracking, in which the KBAC knowledge tells the agent how to correct its assumptions
given the current situation.
3. Obviously, this world state will usually be different than the agents initial state and it is often impossible
to return to a prior state in an execution system. We use backtrack in this section to refer to the
retraction of asserted execution knowledge such that any remaining asserted knowledge is consistent with
the currently perceived world state.

369

fiWray & Laird

A B C D E F G H

(a)

1

A B C D E
(b)

2

1

2

Figure 10: Examples of (a) disjoint dependencies and (b) intersecting assumption dependencies.

Assumption Justification is a form of dependency-directed backtracking (Stallman &
Sussman, 1977). In dependency-directed backtracking, regardless of the chronological order
in which an architecture makes assertions, the architecture can identify and retract those
assertions that contributed to failure in a search and retain all other assertions. In Assumption Justification, the architecture retracts only those assumptions that are directly affected
by a change in the context. Assumptions created later in the processing, not dependent
on the change, are unaffected. Consider the examples in Figure 10. In (a), assumptions
1 and 2 each depend upon disjoint sets of assertions. With Assumption Justification, removal of any assertion in 1s assumption justification will result in the retraction of 1; 2 is
unchanged, even if the architecture asserted 2 after 1.
Dynamic Hierarchical Justification is similar to backjumping (Gaschnig, 1979). Backjumping heuristically determines the state to which a current search should backtrack or
backjump. The heuristics used by backjumping are based on syntactic features of the
problem. For instance, in constraint satisfaction problems, the backjumping algorithm
identifies which variable assignments are related to other variable assignments via the constraints specified in the problem definition. When a violation is discovered, the algorithm
backtracks to the most recent, related variable (Dechter, 1990). Intervening variable assignments are discarded. In DHJ, when an assertion in the hierarchy changes, the system
backjumps in its knowledge search to the highest subtask in the hierarchy not dependent
on the change. In Figure 10 (a) all dependent assertions are collected in the support set for
the subtask. If any of the higher level assertions change, the entire subtask is removed.
When using DHJ, as in backjumping, some previous knowledge search may need to
be repeated after backtracking. Assume the removal of the subtask in Figure 10 (a) was
due to a change in A. If a similar subtask is reinitiated, assumption 2 may need to be
regenerated. This regeneration is unnecessary because 2 did not need to be retracted
to avoid inconsistency. Under Dynamic Hierarchical Justification, the agent retracts all
reasoning in the dependent subtask (and all lower levels in the hierarchy); assertions not
dependent on the change in the context can also be removed. Thus, like backjumping, DHJ
uses a syntactic feature of reasoning (decomposition into subtasks) to choose a backtracking
point and this backtracking is not always as conservative as possible.
Although the subtask decomposition is a syntactic feature of the knowledge search, it
is a strongly principled one, reflecting a semantic analysis of the task by a knowledge designer. Hierarchical task decomposition is based on the premise that tasks can be broken
down into discrete units that have little interaction with other units; they are nearly decom370

fiEnsuring Consistency in Hierarchical Execution

posable (Simon, 1969). Thus, the goal of a hierarchical decomposition is to separate mostly
independent subtasks from one another. A consequence of this separation is that dependencies in higher levels will be limited as much as possible (interaction between subtasks should
be minimized) and the dependencies among the assertions in any particular subtask will be
shared (otherwise, the subtask could be subdivided into two or more independent subtasks).
Of course, it is often possible to decompose a given task in many different ways. In most
cases the domain imposes minimal constraint and the knowledge engineer has significant
latitude in crafting the task decomposition.
In the situation illustrated in Figure 10, (b) would be a more complete decomposition
of a task by the knowledge engineer than (a), assuming the two alternatives represent a
decomposition of the same task. In (b), the number of dependent assertions does not necessarily grow as a function of the number of assumptions in the local level, while in (a) it
does. Further, in (a), two independent assumptions are being pursued. These assumptions
could potentially be inferred in separate subtasks in an alternate decomposition. In (b),
on the other hand, the assumptions in the subtask are closely tied together in terms of
their dependencies and thus better asserted within the same subtask. Because the dependencies of assumptions 1 and 2 have considerable overlap in (b), Assumption Justification
pays a high overhead cost to track individual assumptions because (most) everything in
the local subtask would be removed simultaneously if assertions B, C, or D changed. Because DHJ incurs no such overhead, DHJ is a better choice when the intersection between
assumption dependencies is high. Task knowledge structured more like the situation in
(b), rather than (a) would lead to few unnecessary retractions. Because (b) appears to
better reflect well-decomposed tasks, Dynamic Hierarchical Justification will constrain the
knowledge development process and improve the resulting decompositions. Consequently,
nearly-decomposed tasks should allow DHJ to avoid most unnecessary regenerations while
avoiding the processing overhead of Assumption Justification.
3.4.2 Limiting Persistence under DHJ
DHJ limits persistence in subtasks, resulting in assumptions that are not as persistent as
assumptions in typical truth maintenance systems. This section explores the consequences
of these limitations to determine if DHJ architectures 4 can still provide the persistence
necessary for agent execution (Section 2.1).
DHJ will retract a subtask when a potential inconsistency could impact hypothetical and
recursive reasoning like counting. Consider the aircraft classification and counting example.
Perhaps an aircrafts altitude contributes to a hypothetical classification of the aircraft
(e.g., particular altitude and speed combinations might suggest a reconnaissance aircraft).
The agent would create assumptions locally that depend on this aircrafts altitude. If the
altitude (or an altitude boundary) changes, then the assumption should be retracted. This
retraction is required to avoid inconsistency. If the contacts altitude no longer suggests that
it is a reconnaissance aircraft, then the assumption that depended on that assertion should
be removed. DHJ captures these dependencies and performs the retraction. The agent now
4. For clarity, and because Assumption Justification has already been eliminated as a candidate solution,
the following discussion focuses exclusively on DHJ. However, Assumption Justification limits persistence
similarly.

371

fiWray & Laird

has the opportunity to reconsider the classification of the aircraft, or pursue other tasks if
the classification is no longer important.
DHJ will also retract a subtask if an assumption was created in the local subtask for
the purpose of remembering some input (or elaboration of input). For example, if an agent
needed to remember a particular aircrafts altitude at a particular point in time, then that
assumption cannot be stored in a local subtask. DHJ limits persistence in such a way that
remembering within a local subtask is generally impossible.
In order to remember previous situations, assumptions can be asserted in the root task.
Any assumption asserted in this level will never be retracted because there are no higher level
dependencies (assuming percepts are associated with the top level and not a higher input
level, as in Theo, Mitchell et al., 1991). The primary drawback of this requirement for
remembering is that remembered items are no longer local to the subtask that created them,
requiring additional domain knowledge to manage remembered assumptions. However,
remembering already requires domain knowledge; it is not possible to remember an assertion
regardless of its dependencies and also be able to retract it architecturally.
These examples show that Dynamic Hierarchical Justification still allows all forms of
persistence, but trades capturing dependencies for nonmonotonic assumptions in local subtasks with remembering assumptions in the root task, where no dependencies are captured.
Because DHJ forces remembered items into the root task, it also suggests that a fundamental aspect of this root task should be managing these remembered assumptions. We view
this requirement as a positive consequence of DHJ, because it forces knowledge engineers
to better recognize the reasons for creating an assumption (e.g., remembering vs. a hypothetical) and circumscribes remembering so that we can now develop or adopt functional
or temporal theories to manage assumptions created for remembering (e.g., Allen, 1991;
Altmann & Gray, 2002).
3.4.3 Recovery from Interruption with DHJ
Dynamic Hierarchical Justification makes an agent more reactive to its environment, ensuring that relevant changes in the environment lead to the retraction of any dependent
subtasks. DHJ imposes an automatic interruption of the agent for a subtask retraction,
without evaluating the state of the system first. Although automatic interruption increases
the reactivity of the system, it can lead to difficulties if there is no way to override it.
In this section we examine two cases where uncontrolled interruption can cause problems.
The problems arise because DHJ biases the system to be reactive; that is, to respond automatically to changes in the environment without deliberation. However, in both cases,
additional agent knowledge can overcome that bias and make the system more deliberate
and avoid uncontrolled interruption.
The first problem arises when there is a sequence of actions that must be completed
without interruption in order for a subgoal to be achieved. If the processing is interrupted,
then it is possible, because of the dynamics of the world, that the task cannot be resumed.
For example, imagine an aircraft nearing the point where it can launch a missile at a target.
When the task is interrupted and then resumed, the aircrafts position may have changed
enough, relative to the target, that additional steering commands are necessary before the
372

fiEnsuring Consistency in Hierarchical Execution

missile can be launched. In this case, it may be preferable not to interrupt the original
launch sequence once it has begun.
Consider two possible approaches to achieving this capability with Dynamic Hierarchical
Justification architectures. The first is to move the processing to the root task. Because
the root task is not interrupted, the processing will not be interrupted. However, this
approach greatly restricts how a task can be hierarchically decomposed and thus should be
considered only as a last resort. The second approach is to add new reasoning for the task
that freezes the external situation with respect to additional reasoning in the subtask.
The new processing initiates the execution of the subtask and creates persistent structures
in the root task. These persistent structures represent a deliberate commitment to not
being interrupted. The remaining processing in the subtask accesses only these structures
in the execution of the task. Thus, because they are persistent, even if there are changes
to the surrounding situation that would have interrupted the subtask, its processing is
now insensitive to those changes and interruption is prevented. This approach also requires
additional reasoning to recognize completion of the uninterruptible behavior and remove the
persistent structures built by the initial subtask. This reasoning reflects a deliberate act,
signaling that the commitment no longer holds. In the abstract, together these additions
provide a mechanism for overcoming automatic interruption. The disadvantage of this
approach is that, as part of the system design, those subgoals that cannot be interrupted
must be identified beforehand. For those subtasks, additional agent knowledge must be
implemented to create and remove encapsulations of any dynamic data.
A more critical problem for DHJ is the Wesson Oil problem: when someone is cooking
dinner and a higher-priority activity suddenly occurs (a hurt child), the cook should turn off
the stove (a cleanup procedure) before leaving for the hospital (Gat, 1991b). This problem
occurs when there is a change in the hierarchical context at a level far from the terminal
level of the hierarchy. In this situation, similar tasks may not be resumed or initiated
following the interruption. The agent must therefore recognize whether cleanup of the
external and/or internal states is necessary, and, if so, perform that cleanup. Even with
DHJ, the agent can still behave appropriately if it has the right knowledge. In particular,
the agent must be able to recognize partially completed tasks (like cooking dinner) and be
able to select cleanup actions specific to the task state (like turning off a stove burner).
Because DHJ requires all remembered assumptions to be asserted in the root level of the
hierarchy, this recognition task has internal state available; it need not try to reconstruct
that state from the external environment alone. However, it does require some analysis
of the task domain(s) by a knowledge engineer so that any interruptible activity requiring
cleanup include triggering assertions for cleanup in the root task.
This work was prompted by a desire for architectural solutions to inconsistency, yet
maintaining consistency efficiently can lead to interruptions, which, under DHJ, requires
knowledge-based solutions to problems arising from automatic interruption. 5 However, most
of the requirements imposed by DHJ are positive consequences. Subtask retractions and
observed recovery in the development process help define what must be remembered in the
root task for cleanup, which is significantly different than the laborious process of debugging
5. Dynamic Hierarchical Justification could also be used as a trigger for meta-level deliberation rather than
immediate subtask retraction. It would then possibly provide an architectural solution to the question
of when to deliberate about potential inconsistency for intention reconsideration (see Section 5.2).

373

fiWray & Laird

agent programs that are failing due to inconsistency. In theory, Dynamic Hierarchical
Justification imposes requirements for handling interruptions that do pose serious questions
about its overall utility. In practice, we have not found addressing these questions to be a
problem in a variety of recent agent implementations using the Soar-DHJ architecture (e.g.,
Laird, 2001; Wray et al., 2002).

4. Empirical Evaluation of Dynamic Hierarchical Justification
Architectural mechanisms like DHJ must be efficient. We have demonstrated that the
algorithm itself is efficient, but the question of its impact on the overall behavior generation
capability of an agent remains an open question due to interruption and regeneration. Given
the complexity of both agent-based systems and the domains in which they are applied,
analytical evaluations must be extremely narrow in scope, and even then require specialized
techniques (Wooldridge, 2000). This section instead pursues an empirical evaluation of
Dynamic Hierarchical Justification, focusing on efficiency and responsiveness in two domains
at extremes in the continua of agent domain characteristics. Because the architectural
solution to inconsistency was motivated by the cost (and incompleteness) of knowledgebased solutions, knowledge development costs will also be estimated.
4.1 Methodological Issues
Dynamic Hierarchical Justification is a general solution, applicable in a wide range of agent
tasks. In order to evaluate such a solution, a number of methodological issues must be
addressed. The following describes three important issues and the choices made for this
evaluation.
4.1.1 Relative vs. Absolute Evaluation
What constitutes good or poor cost and performance evaluations? In general, an
absolute evaluation of performance and cost is difficult because the task itself, in addition
to the agents knowledge and architecture, determines overall cost and performance results.
We circumvent this problem by making relative comparisons between agents using the
original, Fixed Hierarchical Justification Soar architecture (FHJ agents) and new agents
(DHJ agents). The FHJ agents provide cost and performance benchmarks, obviating the
need for absolute evaluations.
4.1.2 Addressing Multiple Degrees of Freedom in Agent Design
Even when architecture and task are fixed, many different functional agents can be developed. How can one know if comparative results are valid and general if the experimenter
has control over both benchmarks and new agents?
DHJ agents will be compared to agents previously implemented by others. Such systems
will provide good performance targets, because they were optimized for performance, and
will minimize bias, because they were developed independently.
FHJ systems were used as fixed benchmarks, and were not modified. DHJ agents use the
identical task decompositions employed by the FHJ agents and the same initial knowledge
base. We observed opportunities to improve performance in the DHJ agents by modifying
374

fiEnsuring Consistency in Hierarchical Execution

either the task decomposition or re-designing significant portions of the agent knowledge
base. However, agent knowledge was modified only when necessary for correct behavior, in
order to ensure that DHJ agents remained tightly constrained by their FHJ counterparts,
thus limiting bias in the evaluation.
4.1.3 The Choice of Representative Tasks
This evaluation will be limited to execution agents in the Dynamic Blocks World and in
a reduced-knowledge version of TacAir-Soar (micro-TacAir-Soar). The choice of only a
few tasks or domains is a considerable drawback of benchmarks (Hanks, Pollack, & Cohen, 1993). Although these choices were motivated primarily by the availability of domains
with pre-existing FHJ agents, the two domains do represent opposite extremes for many
domain characteristics. Micro-TacAir-Soar, like TacAir-Soar, is inaccessible, nondeterministic, dynamic, and continuous, while the Dynamic Blocks World simulator used in the
experiments is accessible, deterministic, static and discrete. The primary motivation for
using the Dynamic Blocks World, which is less representative of typical agent tasks than
Micro-TacAir-Soar, is to assess the cost of employing DHJ in a domain where a priori it
appears it would not be useful (although Section 6 suggests DHJ can prove useful even
in relatively static domains). Thus, the Dynamic Blocks World will provide a baseline for
the actual cost of deploying the algorithm, even though little benefit is expected from its
deployment in this domain.
4.2 Evaluation Hypotheses
Although specific expectations will differ in different domains, differences in the dimensions
of knowledge cost and performance can be anticipated when comparing DHJ agents to
baseline agents. The following discusses the expectations and the metric(s) used for each
dimension.
4.2.1 Knowledge Engineering Cost
Knowledge engineering effort in DHJ agents should decrease in comparison to previously
developed agents. Knowledge in Soar is represented with production rules. Each production
represents a single, independent knowledge unit. We assume the addition of more productions represents an increase in cost and measure knowledge cost by counting the number of
productions in each type of agent. The number of productions, of course, provides only a
coarse metric of cost because the complexity of individual productions varies significantly.
However, the productions that will be removed in DHJ agents are often the most difficult
ones to create. Therefore, the difference in number of productions is probably a conservative
metric for knowledge cost in DHJ.
4.2.2 Performance: Efficiency and Responsiveness
In general, overall performance should change little in DHJ agents, as compared to FHJ
counterparts. Although Dynamic Hierarchical Justification does add a new architectural
mechanism, the algorithm itself is efficient and should not contribute to significant differences in performance. Further, less domain knowledge will need to be asserted because all
375

fiWray & Laird

across-level consistency knowledge is now incorporated in the architecture. Thus, if applying across-level KBAC knowledge represented a significant expense in the overall cost of
executing a task, DHJ agents might perform better than FHJ agents.
There are two specific exceptions to this expectation. First, in domains where consistency knowledge is (mostly) unnecessary for task performance, FHJ agents may perform
better than DHJ agents. For example, the Dynamic Blocks World requires little consistency knowledge but the DHJ architecture will still update the support set, even though
few inconsistency-causing context changes should arise.
Second, if regeneration is problematic, overall performance will suffer. In DHJ, whenever
the dependent context changes, a subtask will be retracted. If the change does not lead
to a different choice of subtask, the subtask will be necessarily regenerated. Thus, under
DHJ, some subtask regeneration will occur, and, if regeneration is significant, performance
degradation will result.
CPU execution time provides a simple, single dimension of gross performance. The CPU
time reported for individual experiments reflects the time the agent spends reasoning and
initiating actions rather than the time it takes to execute those actions in the environment.
Decisions: In Soar, subtasks correspond to the selection of operators and subgoals for
implementing operators. The selection of an operator is called a decision. When Soar
selects an operator, it tries to apply the operator. Soar reaches an impasse when it cannot
apply a newly selected operator. These non-primitive operators lead to the generation of
a subgoal in the subsequent decision. For example, Soar selects the put-down operator
in one decision and creates a subgoal to implement put-down in the subsequent decision.
Together, these two steps constitute the notion of a subtask in Soar.
The number of decisions can thus be used as an indication of the number of subtasks
undertaken for a task. In FHJ, a subtask was generally never interrupted until it terminated
(either successfully or unsuccessfully). In DHJ, subtasks will be interrupted whenever a
dependent change occurs. Thus, decisions should increase in DHJ agents because subtasks
will be interrupted and re-started. Further, if decisions increase substantially (suggesting
significant regeneration), overall performance will degrade.
Production Firings: A production rule fires when its conditions match and its result is
applied to the current situation. Production firings should decrease in DHJ for two reasons.
First, any across-level consistency knowledge that was previously used in FHJ agents will no
longer be necessary (or represented); therefore, this knowledge will not be accessed. Second,
any reasoning that occurred after inconsistency arose in FHJ agents will be interrupted and
eliminated. However, production firings will increase if significant regeneration is necessary.
4.3 Empirical Evaluation in the Blocks World
Agents in this Dynamic Blocks World domain have execution knowledge to transform any
initial configuration of three blocks into an ordered tower using a simulated gripper arm.
The table in this simulation has a width of nine blocks. The agents task goal is always to
build the 1-on-2-on-3 tower. Each agent built a tower from each of the resulting 981 unique,
non-goal, initial configurations of blocks. Table 1 summarizes the results of these tasks. As
expected, total knowledge decreased. Overall performance improved. Decisions increased,
as expected, but the number of rule firings increased as well, which was not anticipated.
376

fiEnsuring Consistency in Hierarchical Execution

Rules
Decision Avg.
Avg. Rule Firings
Avg. CPU Time (ms)

FHJ
x
s.d.
188

87.1
20.9
720.3 153.5
413.1 121.6

DHJ
x
s.d.
175

141.1
38.7
855.6 199.6
391.6 114.0

Table 1: Summary of knowledge and performance data from the Blocks World. The agents
performed the tower-building task for each of 981 configurations. Task order was
randomly determined.

4.3.1 Knowledge Differences
Total knowledge decreased about 7% in the DHJ agent. This small reduction is consistent
with expectation. The aggregate comparison is misleading because knowledge was both
added (16 productions) and deleted (29).
Removing Consistency Knowledge: In Soar, the subtask operator and subgoal are
terminated separately. Soar monitors impasse-causing assertions to determine if a subgoal
(such as the subtask goal) should be removed via FHJ. However, the removal of a subtask
operator requires knowledge. The original, FHJ architecture treats the initiation of an
operator as a persistent assumption and requires knowledge to recognize when a selected
operator should be interrupted or terminated. This knowledge can be categorized as consistency knowledge because it determines the time at which a subtask should be terminated,
even when the initiating conditions for the subtask no longer hold.
In DHJ, only the effects of operators are persistent; all other assertions are entailments
of the situation. Thus, the initiation of a subtask is now treated as an entailment and
a subtask remains selected only as long as the initiation conditions for the subtask hold.
This change removes the need for knowledge to terminate the subtask: when the subtask
initiation conditions are no longer true, the subtask is automatically retracted. Thus,
termination knowledge was removed for all subtask operators.
Filling Gaps in Domain Knowledge: The persistence of subtasks in the original architecture allows FHJ agents to ignore large parts of the state space in their domain knowledge.
For example, the knowledge that initiates stack and put-on-table subtasks assumes that
the gripper is currently not holding a block. As these tasks are executed, the gripper, of
course, does grasp individual blocks. The conditions for initiating stack or put-on-table
when holding a block were ignored in the original domain knowledge.
The DHJ agent now requires knowledge to determine which subtasks it should choose
when holding blocks, because subtasks can be interrupted while the agent still holds a block.
16 productions were necessary, primarily for the stack and put-on-table operators. It is
important to note that this knowledge is necessary domain knowledge. FHJ agents could
not solve any problem in which they began a task holding a block because they lacked
domain knowledge for these states. These additions are thus a positive consequence of
DHJ. The architectures enforcement of consistency revealed gaps in domain knowledge.
377

fiWray & Laird

4.3.2 Performance Differences
Somewhat surprisingly, overall performance of the DHJ agents (measured in CPU time) improves slightly in comparison to the FHJ agents, even though both decisions and production
firings increase. Each of the Soar-specific performance metrics are considered individually
below, and then the overall performance improvement is considered.
Decisions: FHJ agents, on average, made considerably fewer decisions than DHJ agents.
The difference was consistent across every task. These additional decisions result from the
removal and subsequent regeneration of subtasks. For example, when the agent picks up a
block in pursuit of a stack task, the selection of the stack task must be regenerated. The
knowledge in the DHJ agents could be modified to avoid testing specific configurations of
blocks and thus avoid many of these regenerations.
Production Firings: The number of production firings also increased in the Blocks World.
The increase in production firings can be attributed to the knowledge added to the system
and the regeneration of subtasks that made the additions necessary. The relative increase in
number of production firings (19%) was much smaller than the increase in decisions (62%).
The smaller difference can be attributed to the productions that were removed (and thus
did not fire).
CPU Time: Generally, when production firings increase in Soar, an increase in CPU
time is expected. However, CPU time in DHJ decreased slightly in comparison to FHJ
even though production firings increased. To explain this result, some additional aspects of
Soars processing must be considered.
The match cost of a production is not constant but grows linearly with the number of
tokens, partial instantiations of the production (Tambe, 1991). Each token indicates what
conditions in the production have matched and the variable bindings for those conditions.
Thus, each token represents a node in a search over the agents memory for matching
instantiation(s) of the production. The more specific a productions conditions are, the more
constrained the search through memory, and thus it costs less to generate the instantiation.
The new productions added to the DHJ Blocks World agent were more specific to
the agents memory (i.e., its external and internal state) than the productions removed.
Further, simply having fewer total productions also can reduce the amount of total search
in memory.6 An informal inspection of the match time and tokens for several FHJ and
DHJ runs showed that the number of tokens decreased in DHJ by 10-15%. This reduction
in token activity is the primary source of improvement in Dynamic Blocks World DHJ
agent CPU time. This improvement, of course, is not a general result and provides no
guarantee that in some other task or domain the cost of matching will not increase rather
than decrease.

6. The RETE algorithm (Forgy, 1979) shares condition elements across different productions. Thus, the
removal of productions only decreases the total search if removed productions contain condition elements
not appearing in the remaining productions. We did not perform an exhaustive analysis of the condition
elements to determine if the removed productions reduce the number of unique condition elements in
the RETE network.

378

fiEnsuring Consistency in Hierarchical Execution

4.4 Empirical Evaluation in TacAir-Soar
Converting TacAir-Soar to the DHJ architecture would be very expensive, requiring many
months of effort. DHJ agents were instead developed for a research and instruction version
of TacAir-Soar, Micro-TacAir-Soar (TAS). TAS agents use the TacAir-Soar simulation
environment (ModSAF) and interface but have knowledge to fly only a few missions, resulting in an order of magnitude decrease in the number of productions in the agents. However,
TAS uses the same tactics and doctrine for its missions as TacAir-Soar.
In TAS, a team of two agents (lead and wing) fly the patrol mission described
previously. They engage any hostile aircraft that are headed toward them and are within
a specific range. The lead agents primary role is to fly the patrol route and intercept
enemy planes. The wings responsibility is to fly in formation with the lead. Because the
total knowledge is significantly reduced, converting TAS DHJ agents should be relatively
inexpensive. However, the results should be representative of TacAir-Soar because TAS
retains the complexity and dynamics of TacAir-Soar.
The patrol mission has no clearly-defined task termination condition like the Dynamic
Blocks World. To address this problem, each agent in the simulation executes for ten
minutes of simulator time. During this time, each agent has the opportunity to take off,
fly in formation with its partner on patrol, intercept one enemy agent, and return to patrol
after the intercept. In an actual TacAir-Soar scenario, these activities would normally be
separated by much larger time scales. However, an agent spends much of its time on a
patrol mission simply monitoring the situation (waiting), rather than taking new actions.
Ten minutes of simulated time proved to be brief enough that overall behavior was not
dominated by wait-states, while also providing time for a natural flow of events.
When running for a fixed period of time, an increase in the number of decisions can be
attributed to regeneration or simply an improvement in decision cycle time. We avoid this
potential confusion by running the simulator with a constant cycle time. In this mode, each
simulator update represents 67 milliseconds of simulated time. Because each agent now
runs for a fixed period of time with fixed updates, each FHJ and DHJ agent will execute
the same number of decisions. Any problems due to regeneration will be apparent in the
number of rule firings and degradation in responsiveness. Additionally, the general results
do not change significantly if the scenarios are executed with the real-time mode normally
used for TacAir-Soar agents. The fixed cycle simply eliminates some variability.
Although the patrol scenario was designed to minimize variation from run to run, the
TAS simulator is inherently stochastic and the specific actions taken by an agent and the
time course of those actions varies when the same task is repeated. To control for this variation, each scenario was run for the lead and wing agents approximately 50 times. Logging
and data collection significantly impacted CPU time and other performance statistics. In
order to control for this effect, we actually ran each scenario 99 times, randomly choosing
one agent (lead or wing) to perform logging functions (and discarding its performance measures). The other agent performed no logging functions. Data from logging agents was used
to create Figure 9. The performance measures of the no logging agents were recorded at
the conclusion of each scenario and are summarized in Table 2.
379

fiWray & Laird

Lead Agent
Rules
Number of runs (n)
Decisions
Outputs
Rule Firings
CPU Time (msec)

FHJ
591
43
x s.d.
8974
0.0
109.1 6.71
2438 122
1683 301

x
8974
142.8
2064
1030

DHJ
539
53
s.d.
0.0
7.03
81.1
242

Wing Agent
FHJ
DHJ
591
539
56
46
x s.d.
x s.d.
8958
0.0
8958
0.0
1704 42.7
869 12.8
16540 398
6321 104
12576 861
2175 389

Table 2: Summary of TAS run data.
4.4.1 Improving Task Decompositions
TacAir-Soar DHJ agents required extensive knowledge revision. Such revision was not
unexpected. For instance, unlike the Dynamic Blocks World, TAS agents remember many
percepts, such as the last known location of an enemy aircraft. As previously described,
assertions for remembering must now be located in the root level of the hierarchy, thus
requiring some knowledge revision. However, other problems were discovered. In some
cases, FHJ agents took advantage of inconsistency in asserted knowledge. In other words, the
FHJ agent not only allowed inconsistency in the assertions but actually depended on those
inconsistencies to apply new knowledge. There were two major categories of this knowledge.
Within-level consistency knowledge recognized specific inconsistencies (e.g., retraction of
the proposal for the subtask) as a trigger for actions such as clean up of the subtask
state. Complex subtasks allowed the non-interruptible execution of a complex procedure
regardless of the continuing acceptability of the subtask. In both cases, agent knowledge
was modified to remove any dependence on inconsistency. Appendix A provides further
explanation of the original knowledge and subsequent changes. Section 4.4.3 summarizes
the changes quantitatively.
4.4.2 Results
Table 2 lists average data for the FHJ and DHJ lead and wing agents for the patrol/intercept
scenario after the modifications to the DHJ agents knowledge base were completed. The
results in this domain are consistent with expectations: total knowledge decreases, rule
firings decrease and performance improves, substantially so for the DHJ wing agent. The
following sections explore each of these results in greater detail.
4.4.3 Knowledge Differences
Table 3 quantifies the changes to the Soar production rules described above. 7 Modifications include deletions, additions and changes. A rule was considered changed only if
its conditions changed slightly, but it made the same type of computation for the same
subtask. For example, most of the changed within-level consistency knowledge now refers
7. The DHJ agent data was generated with a knowledge base that included some changes to accommodate
learning (Wray, 1998) and these changes are included in the table for completeness. The presence of
these rules in the knowledge base has negligible impact on the performance data reported here.

380

fiAcross-level
Consistency

Remembering

Within-level
Consistency

Complex
Subtasks

Learning

Miscellaneous

Ensuring Consistency in Hierarchical Execution

44
0

36
32

9
5

10
21

4
0

8
1

0

33

8

0

24

0

FHJ Agent:
Deletions:
Additions:
DHJ Agent:
Additional Changes:

TOTALS
591
(111)
59
539
65

Table 3: Quantitative summary of changes to production rules in the FHJ agent knowledge
base for DHJ agents.

to an entailed structure rather than one created as an assumption, but that structure is
located in the same subtask. This somewhat restrictive definition of a change inflates the
addition and deletion accounting. In many cases a production was deleted and then immediately added to a different subtask. For example, the productions that manipulate
motor commands were all moved from local subtasks to the highest subtask. Almost all
the additions and deletions in the Remembering category can be attributed to this move,
which required no synthesis of new production knowledge.
Total knowledge required for the DHJ agents decreased. This approximately 9% reduction was achieved by making some type of modification to about 40% of the FHJ agent
rules, and may seem a modest gain, given the conversion cost. However, this cost is an
artifact of the chosen methodology. Had the DHJ agents been constructed in this domain
without previously existing FHJ agents, at least a 9% decrease in the total knowledge would
be expected. This result thus suggests a reduction in the cost of the agent design. The
high conversion cost does suggest that converting a much larger system, like TacAir-Soar,
would probably be very costly. On the other hand, the modifications were made evident by
identifiable regenerations in the architecture. Thus, the 235 total changes made to the FHJ
knowledge base were much easier to make than constructing a similar number of rules.
4.4.4 Performance Differences
As the performance results in Table 2 show, DHJ agents improved in performance relative to
their FHJ peers. However, the improvements of the lead and wing agents was substantially
different. Differences in the tasks of lead and wing pilots led to the differences in relative
improvements.
Lead and Wing Agents: The lead and wing agent share the same knowledge base but
perform different tasks in the TAS scenario. 8 These differences lead to differences in their
8. The agents share the same knowledge base because they can dynamically swap roles during execution.
For instance, if the lead exhausts its long-range missiles, it will order the wing to take over the lead role,
and then take the role of wing itself.

381

fiWray & Laird

2000

DHJ Lead
DHJ Wing
FHJ Lead
FHJ Wing

Cumulative Outputs

1500

1000

500
intercept

launch missile

patrol turns

resume patrol
0
0

100

200

300

400

500

600

Time (sec)

Figure 11: Cumulative outputs over the course of one ten minute scenario for DHJ (black)
and FHJ (gray) agents. Cumulative outputs for lead agents are represented with
solid lines, wing agents with dashed lines.

absolute performance. Recall that the leads primary responsibility is to fly the patrol route
and intercept enemy aircraft. On the other hand, the wings primary mission role is to follow
the lead. These different tasks require different responses in the agents.
An agents overall reasoning activity is often correlated with its output activity; that
is, the commands it sends to the external environment to take action in it. Figure 11
summarizes the output activity of two pairs of lead and wing agents (FHJ & DHJ) over the
course of a ten-minute scenario. The output activity of both leads is mostly concentrated
at a few places over the course of the scenario (take-off, intercept, launch-missile,
and when resuming patrol following the intercept). The wings most concentrated output
activity occurs when the leads turn to a new leg of the patrol and the wings must follow
the lead through a 180 degree turn. In the remainder of this section, we focus only on DHJ
agents to contrast lead and wing agent behavior. The discussion of the performance metrics
will examine differences between FHJ and DHJ leads and wings.
The lead actually spends most of the scenario waiting, with short bursts of reasoning
and output activity occurring at tactically important junctures in the scenario. On patrol,
the lead flies straight and makes a decision to turn when it reaches the end of a patrol
leg. The lead monitors the environment and searches for enemy planes. This search is
382

fiEnsuring Consistency in Hierarchical Execution

(mostly) passive; the agents radar notifies the agent if any new entities have been detected.
After detecting and classifying an enemy plane as a potential threat, the lead commits to an
intercept. The lead immediately makes a number of course, speed, and altitude adjustments,
based on the tactical situation. These actions are evident in the figure by the pulse labeled
intercept. The lead spends most of the time in the intercept closing the distance between
the aircraft to get within weapon range, again having to maneuver very little and thus
requiring few actions in the environment (thus the relatively flat slope following intercept).
When the agent reaches missile range of the enemy plane, the lead executes a number of
actions very quickly. The lead steers the plane into a launch window for the missile, pushes
the fire button, waits for the missile to clear, and then determines a course to maintain
radar contact as the missile flies to its target (at launch-missile). Once the intercept has
been completed, the lead resumes its patrol task. Again, it issues a large number of output
commands in a short period of time. These examples show that the leads reasoning focuses
primarily on reacting to discrete changes in the tactical situation (patrol leg ended, enemy
in range, etc.) and the behavior generally requires little continuous adjustment.
The execution of the wings follow-leader task, on the other hand, requires reaction to
continuous change in the leads position in order to maintain formation. Position corrections
require observing the leads position, recognizing an undesired separation in the formation,
and then responding by adjusting speed, course, altitude, etc. Because the wing is following
the lead throughout the scenario, it is executing this position maintenance knowledge almost
constantly. When the lead is flying straight and level, as on a patrol leg, the wings task does
not require the generation of many outputs. In Figure 11, these periods of little activity
are evident in the periodic flat segments in the wings cumulative outputs. When the lead
begins a maneuver (e.g., a turn), the wing must maintain the formation throughout the
maneuver. During a turn the wing generates many motor commands as it follows the lead.
Because the turn takes a few seconds to complete, the outputs increase gradually over the
course of the turn, as can be seen in the figure. Thus, the wing periodically encounters
a dynamic situation that requires significant reasoning and motor responses. Further, the
response to this change is not discrete, as in the lead, but occurs continuously over the
course of the leads maneuver.
These differences in the tasks for the two agents account for the relatively large absolute
differences in the performance metrics between the lead and wing agents. Because the wings
are adjusting their positions relative to the leads, they issue many more output commands
than the leads, which requires many more inferences to determine what those commands
should be.
Decisions: The differences between decisions in the lead and wing is due to an artifact of
the data collection. The lead agents ran for an extra second after the wings halted in order
to initiate data collection.
Production Firings: In both the lead and wing agents, production firings decrease. However, the wings production firings decrease by 62%, while in the lead, the decrease is only
15%. One reason for the large improvement in the DHJ wing is due to the elimination of
some redundant output commands in the FHJ agents. The FHJ wing sometimes issues the
same motor command more than once. The reason for this duplication is that a specific motor command is computed locally, and is thus not available to other subtasks. In some cases,
two subtasks may issue the same motor command. When the command is stored locally, a
383

fiWray & Laird

command may be issued again because the agent cannot recognize that the command has
already been issued by another subtask. Because motor commands are remembered in the
top subtask by DHJ agents, they can be inspected by all subtasks. The DHJ wing thus
never issues a redundant motor command. The large relative decrease in outputs in the
wing agent from FHJ to DHJ (Figure 11) can be attributed to this improvement. Production firings decrease with the decrease in output activity because most reasoning activity
in the wing concerns reacting to the leads maneuvers.
In contrast to the wing, the leads average number of outputs actually increases. Regeneration is the source of these additional outputs. In a few situations, a DHJ agents
subtask for adjusting heading, speed or altitude can get updated repeatedly in a highly
dynamic situation (e.g., a hard turn). The FHJ agent uses subtask knowledge to decide if
the current output command needs to be updated. However, in DHJ, the subtask may be
retracted due to dependence on a changing value (e.g., current heading). When the subtask is regenerated following a retraction, the lead may generate a slightly different motor
command. For example, the lead might decide to turn to heading 90.1 instead of 90.2 .
This decision causes the generation of a new output command that would not have been
re-issued in FHJ agents and accounts for the small increase in outputs. It also suggests
that without the self-imposed constraint of the methodology, the knowledge base could be
further modified to avoid this regeneration and further decrease production firings.
Although the large magnitude of the improvement in the wing is primarily due to remembering motor commands, both agents also needed less consistency knowledge and thus
accessed less knowledge while performing the same task. The agents perform the same tasks
using less knowledge.
CPU Time: CPU time decreases in both the DHJ lead and wing agents. The improvement
in the lead (39%) is about half the improvement in the wing (81%). These differences are due
primarily to the decrease in production firings. There are fewer production firings and thus
fewer instantiations to generate, leading to improvements in CPU time. Match time also
improved, contributing to the overall performance improvement. 9 The larger improvements
in CPU time as compared to production firings improvements (39% vs. 15% in the lead,
81% vs. 62% in the wing) might be attributable to decreases in both the number of rule
firings and match time. Again, these results offer no guarantee that match time will always
decrease with DHJ. It is important to note, however, in two very different domains DHJ
reduces total knowledge and further constrains the remaining knowledge. The architecture
then leveraged these small differences for improved overall performance.
4.4.5 Differences in Responsiveness
Because CPU time decreases in the DHJ agents, responsiveness should generally improve.
However, because some agent knowledge has been split into several different subtasks, some
actions may not be initiated as quickly as would be initiated by the FHJ agent. In this
section, we explore differences in responsiveness in one of these situations.
9. As in the Dynamic Blocks World, these trends are based on a few observations of the data, rather than
a significant analysis. In particular, in TAS, data for the number of tokens generated was not collected.
The results reported here are consistent with the expectation that the token activity falls in DHJ agents,
as compared to FHJ agents.

384

fiEnsuring Consistency in Hierarchical Execution

FHJ
DHJ

Avg. In-Range Time
(sec)
161.816
162.048

Avg. Launch Time
(sec)
162.084
162.993

Reaction Time
(sec)
.268
.945

n
95
99

Table 4: A comparison of average reaction times for launching a missile in TAS.
When an enemy plane comes in range, the agent executes a series of actions, leading to
the firing of a missile. Reaction time is the difference between the time at which an enemy
agent comes in range and the time when the agent actually pushes the fire button to launch
a missile. This reaction time is one measure of the agents responsiveness. As Table 4 shows,
the FHJ agent is able to launch the missile in just over a quarter of a second. However,
the DHJ agent is about three-and-a-half times slower than the FHJ agent in launching the
missile, taking almost a full second, on average.
Split subtasks, regeneration, and subtask selection all contribute to the increase in
reaction time. Splitting a subtask with n steps, which may have all been executed in a
single decision previously, may now take n decisions in the DHJ agent. Only a few actions
are necessary for launching a missile so one would expect an increase of, at most, a few
hundred milliseconds for this change. However, by dividing subtasks into separate steps,
the sequential series of actions can be interrupted. In particular, a number of regenerations
occur under the launch-missile subtask as the agent prepares to fire the missile in a highly
dynamic situation. The agent sometimes chooses to undertake a similar action because the
situation has changed enough that a slightly different action might be necessary, as described
above. The result is that the DHJ agents are taking more accurate aim than the FHJ agents,
as they are responding more quickly to the dynamics of the environment. This aiming,
however takes more time, although the increase in time is not tactically significant (i.e.,
enemy planes were not escaping that were previously hit by FHJ agents).
Some additional re-engineering of the knowledge would improve the reaction time (e.g.,
as described in Section 3.4.3). However, decreases in responsiveness will be difficult to avoid,
in general. Dynamic Hierarchical Justification requires that subtasks with different dependencies be initiated and terminated separately, or risk unnecessary regeneration. However,
by splitting complex tasks into separate subtasks, individual actions are delayed both because the subtasks are now separate procedures, and because the selection for a particular
subtask in the series can be postponed when additional subtask choices are available.
4.5 Summary of Empirical Evaluations
Figure 12 summarizes results from the Dynamic Blocks World and TAS. In both domains,
DHJ agents require fewer total productions, suggesting a decrease in knowledge cost. Performance is roughly the same in the Dynamic Blocks World and for the lead agents in TAS.
The DHJ wing agents show a much greater improvement in overall performance, which is due
both to DHJ and to changes in knowledge. These results suggest that Dynamic Hierarchical
Justification can be expected to reduce engineering effort and not degrade performance in
a variety of domains, simple and complex. However, response time in some situations may
decrease.
385

fiWray & Laird

600
500

Productions

400
300
200
FHJ Lead
FHJ Wing
FHJ DBW

100

DHJ Lead
DHJ Wing
DHJ DBW

0
0

1

2

3

4

10

11

12

13

14

CPU Time (sec)

Figure 12: Mean CPU Time vs. knowledge in productions for FHJ (black) and DHJ (gray)
agents in the Dynamic Blocks World and TAS. The graph includes the actual
distribution of CPU time for each agent as well as the mean for each agent.
Means for the Dynamic Blocks World agents are illustrated with squares, TAS
lead agents with triangles, and TAS wing agents with diamonds.

5. Discussion
Solutions other than KBAC and the new solutions introduced here can be developed for
the inconsistency problem (Wray, 1998). We briefly introduce a few additional solutions,
and also consider the relationship of Dynamic Hierarchical Justification to intention reconsideration in belief-desire-intention agents and belief revision.
5.1 Other solutions to inconsistency across the hierarchy
In this section, we review other existing architectural solutions to the problem of inconsistency arising from persistence in a hierarchy of assertions.
5.1.1 Limiting Persistence
One obvious approach to eliminating inconsistency arising from persistence is to disallow
persistent assumptions altogether. This approach was adopted in Theo (Mitchell et al.,
1991). All reasoning in Theo is entailed from sensors; only perceptual inputs are unjustified.
Theo cannot reason non-monotonically about any particular world state; only the world can
change non-monotonically. Thus, Theo cannot generally remember previous inputs.
386

fiEnsuring Consistency in Hierarchical Execution

Another possible limitation would be to restrict all assumptions to a single memory
(global state), or, equivalently, allow assumptions only in the root level of the hierarchy
in a hierarchical architecture. This solution ensures that the hierarchical context is always
consistent (all assertions within an associated subtask are entailments) and also allows
persistence. Because HTN execution systems such as RETSINA and DECAF, mentioned
previously, have only a global state, they obviously do not suffer from inconsistency over
a hierarchy. However, the interactions between persistent assertions and new information
derived from sensors will be a problem in systems with global state.
RETSINA has recently adopted rationale-based monitoring (Veloso, Pollack, & Cox,
1998) to identify environmental changes that could impact a currently executing task network (Paolucci et al., 1999). Rationale-based monitoring uses the structure of plan knowledge (in this case, plan operators, including task networks) to alleviate inconsistency. Monitors for relevant world features are created dynamically as planning progresses by identifying
pre-conditions in operators and instantiating them via a straightforward taxonomy of monitor types (e.g., a monitor for quantified conditions). The collection of monitors form a plan
rationale, reasons that support a planners decisions (Veloso et al., 1998). Plan rationales
are thus similar to the justifications used in truth maintenance. Monitors are activated
when a pre-condition element in the world changes. They then inform the planner of the
change, and the planner can then deliberate about whether the change should impact the
plan under construction and, if so, consider appropriate repairs.
Rationale-based monitoring is similar to Dynamic Hierarchical Justification, especially
because they both leverage the structures of their (different) underlying task representations to provide consistency. However, there are two important differences. First, because
DHJ identifies the specific subtask impacted by a change, it does not require deliberation
to determine the impact of the change; immediate return to a consistent knowledge state
is possible. When a monitor is activated in rationale-based monitoring, the planner must
first determine where and how the plan is affected, which can require deliberation. Second,
because monitors trigger deliberation, rather than automatically retracting reasoning, an
agent using rationale-based monitoring can determine if the plan should be repaired and
how. DHJ (as implemented) does not offer this flexibility; retraction is automatic. Automatic retraction assumes the cost of retrieving (or regenerating) pre-existing plan knowledge
is less costly than deliberation to determine if/how the plan can be revised. Because plan
modification can be as expensive as plan generation (Nebel & Koehler, 1995), this assumption is reasonable. However, invoking a deliberate revision process could circumvent
potential problems arising from recovery from interruption (Section 3.4.3).
5.1.2 Fixed Hierarchical Justification
As mentioned previously, both the pre-DHJ version of Soar and the Procedural Reasoning
System (PRS) (Georgeff & Lansky, 1987) use Fixed Hierarchical Justification to retract
complete levels of the hierarchy when the support set no longer holds. In PRS, the support
set consists of a set of context elements that must hold during the execution of the subtask.
These elements are defined by a knowledge engineer. Fixed Hierarchical Justification offers a
complete solution to the inconsistency problem if context references within the reasoning in
a subtask are limited to the support set. This approach guarantees consistency. However,
387

fiWray & Laird

it requires that the knowledge designer identify all potentially relevant features used in
reasoning within the subtask. Additionally, the resulting system may be overly sensitive
to the features in the support set if those features only rarely impact reasoning, leading to
unnecessary regeneration.
Fixed Hierarchical Justification requires less explicit consistency knowledge than knowledgebased solutions. However, KBAC knowledge is still required if access to the whole task hierarchy is possible. Thus, an agents ability to make subtask-specific reactions to unexpected
changes in the environment is limited by the knowledge designers ability to anticipate and
explicitly encode the consequences of those changes.
5.2 Intention Reconsideration
In the belief-desire-intention (BDI) model of agency, an intention represents a commitment
to achieving a goal (Rao & Georgeff, 1991; Wooldridge, 2000). An intention is thus similar
to the instantiation of a subtask in a hierarchical architecture.
Dynamic Hierarchical Justification can be viewed as a partial implementation of intention reconsideration (Schut & Wooldridge, 2000, 2001). Intention reconsideration is the
process of determining when an agent should abandon its intentions (due to goal achievement, recognition of failure, or recognition that the intention itself is no longer desired).
Dynamic Hierarchical Justification is only a partial implementation of intention reconsideration because it is only able to capture syntactic features of the problem solving (i.e.,
the identification of dependencies via the support set) to determine when to reconsider an
intention. Situations that require deliberation to determine that an intention should be
abandoned are not captured in DHJ.10 Schut & Wooldridge (2001) describe an initial attempt to allow the run-time determination of reconsideration policies. An optimal policy
would maximize the likelihood that deliberate intention reconsideration actually leads to
abandoning an intention (i.e., the agent reconsiders when reconsideration is necessary). In
contrast, Dynamic Hierarchical Justification offers a low-cost, always available, domain general process for abandoning intentions, but cannot automatically identify reconsiderations
requiring semantic analysis of the problem state.
In BDI models, agents can choose to execute their current action plans with or without
reconsidering their current intentions first. Kinny and Georgeff (1991) showed that, in more
static domains, bold agents that never reconsider their intentions perform more effectively
than cautious agents that always reconsider before executing a plan step. The opposite
is true in highly dynamic domains: cautious agents out perform bold ones. Both Soar
and PRS can be described as being cautious via Fixed Hierarchical Justification. That is,
at each plan step, the architectures determine if elements in the support set remain asserted
before executing the step. FHJ approaches are, in effect, more bold than they appear,
because they do not reconsider intentions when assertions have changed in the dependent
context, but not in the support set. Dynamic Hierarchical Justification provides more
cautious agents, because it ensures that the agents reconsideration function takes into
account all context dependencies for subtask reasoning. From the perspective of intention
10. DHJ does not preclude deliberate reconsideration. However, Soar (as the testbed for the exploration of
DHJ) does not provide an architectural solution for deliberate reconsideration. Thus, these situations
will be addressed through knowledge and the deliberative processes of the architecture.

388

fiEnsuring Consistency in Hierarchical Execution

reconsideration, the problems introduced by more dynamic domains prompted us to explore
more cautious solutions.
The results of the empirical analysis were somewhat consistent with those of Kinny &
Georgeff. The cautious DHJ agents performed better than less cautious FHJ agents in
the highly dynamic TacAir-Soar domain. In the Dynamic Blocks World, the performance
differences were more equivocal. In comparison to FHJ the number of new intentions
increased with Dynamic Hierarchical Justification (measured as Soar decisions). While
there was a slight overall performance improvement with DHJ, it was due to improvements
in the match time of productions, a Soar-specific measure that likely will not generalize
to other systems. These results suggest that DHJ is possibly overly cautious in static
domains. However, because Dynamic Hierarchical Justification did not present a significant
performance cost and unexpectedly played a constructive role in agent execution even in
the static domain, DHJ seems warranted in both static and dynamic domains.
5.3 Belief Revision
Belief Revision refers to the process of changing beliefs to accommodate newly acquired
information. The inconsistency problem is an example of the need for revision in asserted
beliefs: some change in the hierarchical context (deriving ultimately from perceived changes
in the world) leads to a situation in which a currently asserted assumption would not (necessarily) be regenerated if it were re-derived. Theories of belief revision identify functions
that can be used to update a belief set so that it remains consistent.
The best known theory of belief revision is the AGM theory (Alchourron, Gardenfors,
& Makinson, 1985; Gardenfors, 1988, 1992). AGM is a coherence theory, meaning that
changes to beliefs are determined based on mutual coherence with one another. This approach contrasts with the foundations approach, in which justifications (reasons) determine
when/how to revise a belief set. Obviously, Dynamic Hierarchical Justification is an extension to the foundations approach to belief revision. However, as the foundations and
coherence approaches can be reconciled (Doyle, 1994), in this section we explore the repercussions of Dynamic Hierarchical Justification in the context of the AGM theory of belief
revision.
In AGM theory, when a new sentence is presented to a database of sentences representing
the current knowledge state, an agent is faced with the task of revising its knowledge base
via one of three processes: expansion (adding sentences to the knowledge base), contraction
(removing sentences from the knowledge base) and revision (a combination of expansions
and contractions). AGM theory emphasizes making minimal changes to a knowledge base
and epistemic entrenchment, a notion of the usefulness of a sentence within the database.
AGM theory prefers that sentences with high epistemic entrenchment (relative to other
sentences) are retained during revision.
Comparing Dynamic Hierarchical Justification to Assumption Justification suggests that
it is sometimes cheaper to remove a subtask (and all asserted beliefs associated with that
subtask) than it is to compute the minimal revision with Assumption Justification. In
the context of belief revision, this result is not surprising, since it has been shown that
computing a minimal revision to a knowledge base can be computationally harder than
deduction (Eiter & Gottlob, 1992). This theoretical result has led to applications that
389

fiWray & Laird

compute belief updates via incremental derivations of a belief state, rather than via belief
revision (Kurien & Nayak, 2000).
The power of the heuristic approach used by DHJ over the analytic solution follows from
the characteristics outlined in Section 3.4.1: the hierarchical structure and organization of
the agent assertions and the efficiency of the underlying reasoning system to regenerate any
unnecessarily removed assertions. Assumptions (persistent beliefs) are associated with particular subtasks in hierarchical architectures. A change in perception (an epistemic input)
leads to a revision. Rather than determining the minimal revision, DHJ uses a heuristic
that, in this context, says that persistent beliefs in a subtask have similar epistemic entrenchment to the subtask/intention itself. In some cases, this heuristic will be incorrect,
leading to regeneration, but, when correct, it provides a much simpler mechanism for revision. Gardenfors (1988) anticipates such conclusions, suggesting that systems possessing
additional internal structure (as compared to the the relatively unstructured belief sets of
AGM theory) may provide additional constraints for orderings of epistemic entrenchment.

6. Conclusion
The empirical results from both the Dynamic Blocks World and TAS domains were consistent with expectations: knowledge engineering cost decreased and overall performance in
DHJ was roughly the same (or slightly improved) in comparison to independently-developed
FHJ benchmarks. Development cost decreases because the designer is freed from the task
of creating across-level consistency knowledge. One drawback of DHJ is that responsiveness
can degrade when regeneration occurs.
DHJ has been incorporated into the currently released version of Soar (Soar 8) for over
3 years and the experience of users further confirms that development cost decreases. It
is partly true that developers need a deeper understanding of the architecture to realize
this benefit. However, DHJ removes the need for the encoding of across-level consistency
knowledge, which has proven difficult to understand and encode in many systems. DHJ also
makes understanding the role of assumptions in Soar systems more straightforward, by imposing design and development constraints. For instance, the knowledge designer must now
think about why, when, and where persistence should be used in the agent. Once the knowledge designer determines the functional role of some persistent assumption, DHJ guides the
development of the knowledge necessary for that assumption. For a nonmonotonic or hypothetical assumption, no knowledge must be created that looks outside the subtask in
order to ensure consistency (i.e., no across-level knowledge is necessary). Assumptions for
remembering must be asserted in the root level of the hierarchy, and knowledge must be
created to manage the remembered assumption. Functions of the root task now include
monitoring, updating, and removing remembered assumptions (we are developing domaingeneral methods for managing these remembered assumptions to further reduce cost). Thus,
while DHJ does increase the complexity of the architecture, it makes design decisions more
explicit and manageable than previous KBAC approaches.
Regeneration, seemingly one of the drawbacks of DHJ, also contributes to decreased
knowledge development costs. Regeneration serves as a debugging tool, allowing immediate
localization of problem areas in the domain knowledge (and its specific decomposition).
This debugging aid contrasts with previous knowledge development in which inconsistency
390

fiEnsuring Consistency in Hierarchical Execution

often became evident only in irrational behavior, making it often difficult to determine the
actual source of a problem. Thus, in addition to reducing total knowledge necessary for
some task, Dynamic Hierarchical Justification might also reduce the cost per knowledge
unit when creating agent knowledge by localizing problems via regeneration. However, if
it is the case that some domain cannot be decomposed into nearly decomposable subunits,
regeneration could be debilitating.
Another positive consequence of DHJ is that an agent may behave more robustly in novel
situations not anticipated by the knowledge engineer. For example, as a simple experiment,
the FHJ and DHJ Dynamic Blocks World agents were placed in the situation described in
Figure 3. The FHJ agent fails when the block moves because it lacks knowledge to recognize
moving blocks; the knowledge designer assumed a static domain. With the same knowledge,
however, the DHJ agent responds to this situation gracefully. In the specific situation
in Figure 3, the DHJ agent immediately retracts the put-on-table(3) subtask, because
block-3 is on the table, and thus the selection of that subtask is no longer consistent with
the current situation. The agent then chooses stack(2,3) and decomposes this subtask
into actions to put block-2 on block-3. If a new block (e.g., block-4) is placed in the
empty space below block-2, the architecture responds by retracting the subtask goal for
put-down(2) (i.e., the subtask that contains the empty assumption). It then begins to search
for empty spaces in order to continue its attempt to put block-2 on the table. Because the
architecture, rather than agent knowledge, ensures consistency across the hierarchy, DHJ
agents should be less brittle in situations not explicitly anticipated in agent design.
DHJ also provides a solution to the problem of learning rules with non-contemporaneous
constraints (Wray, Laird, & Jones, 1996). Non-contemporaneous constraints arise when
temporally distinct assertions (e.g., red light, green light) are collected in a single learned
rule via knowledge compilation. A rule with non-contemporaneous constraints will not lead
to inappropriate behavior but rather will never apply. This problem makes it difficult to
use straightforward explanation-based learning approaches to operationalize agent execution
knowledge. Non-contemporaneous constraints arise when the architecture creates persistent
assumptions that can become inconsistent with the hierarchical context (Wray et al., 1996).
Because DHJ never allows such inconsistency, it solves the non-contemporaneous problem.
For instance, agents in both the Dynamic Blocks World and TAS were able to learn
unproblematically in the new architecture, with no/little knowledge re-design. Wray (1998)
provides additional details and an empirical assessment of the learning.
Dynamic Hierarchical Justification operates at a higher level of granularity than Assumption Justification or knowledge-based solution methods, trading fine-grained consistency for lower computational cost. This higher level of abstraction does introduce additional cost in execution. In particular, necessary regeneration led to some redundancy in
knowledge search in both the Dynamic Blocks World and TAS agents. Although overall efficiency improved, some of the improvement was due to improvements in the average
match cost of productions, which cannot be guaranteed in all domains or in other architectures. Further, Dynamic Hierarchical Justification requires that complex subtasks be
split into distinct subtasks. This requirement improves the knowledge decomposition and
reduces regeneration in performance but can reduce responsiveness. However, with the
straightforward compilation of reasoning in subtasks that DHJ enables, the reduction in
responsiveness can be overcome with learning (Wray, 1998).
391

fiWray & Laird

Although the implementation and evaluation of DHJ was limited to Soar, we attempted
to reduce the specificity of the results to Soar in two ways. First, we identified the problems that across-level consistency knowledge introduces in knowledge-based approaches: it
is expensive to develop, degrades the modularity and simplicity of the hierarchical representation, and is only as robust as the knowledge designers imagination. When agents are
developed in sufficiently complex domains, the expense of creating this knowledge will grow
prohibitive. This cost may lead additional researchers to consider architectural assurances
of consistency. Second, Dynamic Hierarchical Justification gains its power via the structure
of hierarchically decomposed tasks. Although specific implementations may differ for other
agent architectures, the heuristic simplifications employed by DHJ should transfer to any
architecture utilizing a hierarchical organization of memory for task decomposition. Dynamic Hierarchical Justification is an efficient, architectural solution that ensures reasoning
consistency across the hierarchy in agents employing hierarchical task decompositions. This
solution allows agents to act more reliably in complex, dynamic environments while more
fully realizing low cost agent development via hierarchical task decomposition.

Acknowledgments
This work would not have been possible without those who contributed directly to the
development and evaluation of Dynamic Hierarchical Justification. Scott Huffman, John
Laird and Mark Portelli implemented Assumption Justification in Soar. Ron Chong implemented a precursor to DHJ. Randy Jones, John Laird, and Frank Koss developed TacAirSoar. Sayan Bhattacharyya, Randy Jones, Doug Pearson, Peter Wiemer-Hastings, and
other members of the Soar group at the University of Michigan contributed to the development of the Dynamic Blocks World simulator. The anonymous reviewers provided valuable,
constructive comments on earlier versions of the manuscript. This work was supported in
part by a University of Michigan Rackham Graduate School Pre-doctoral fellowship, contract N00014-92-K-2015 from the Advanced Systems Technology Office of DARPA and
NRL, and contract N6600I-95-C-6013 from the Advanced Systems Technology Office of
DARPA and the Naval Command and Ocean Surveillance Center, RDT&E division. Portions of this work were presented at the 15 th National Conference on Artificial Intelligence
in Madison, Wisconsin.

Appendix A: Improving Task Decompositions
This appendix describes in detail the changes that were made to the TAS agent knowledge
for DHJ.
Remembering: Figure 4 showed an agent computing a new heading as a subtask of the
achieve-proximity subtask. This calculation usually depends upon the current heading.
When the agent generates the command to turn, the heading changes soon thereafter. In
this situation, the DHJ agent must remember that it has already made a decision to turn
to a new heading by placing the assumption that reflects the new heading in the top level.
If it places the assumption in the local level, then the new current heading will trigger the
removal of turn-to-heading and then regeneration of the subtask (if the agent determines
that it still needs to turn to some new heading).
392

fiEnsuring Consistency in Hierarchical Execution

In the FHJ agents, all output commands (such as turn to some specific heading) were
asserted as assumptions in the local subtask. The DHJ agents knowledge was changed to
issue output commands directly to the output interface (which, in Soar, is always part of
the highest subtask in the hierarchy). No unnecessary regeneration now occurs because
the agent remembers all motor commands and generates a new one only when a different
output is necessary. This change, of course, requires consistency knowledge because the
motor commands are unjustified and thus must be explicitly removed, as is true for any
remembered knowledge with DHJ.
Within-level Consistency Knowledge: Dynamic Hierarchical Justification, like all solutions to the across-level consistency problem, still requires consistency knowledge within
an individual subtask. Some of this knowledge in the FHJ agents is used to remove intermediate results in the execution of a subtask. This clean up knowledge allows the agent
to remove local assertions that contributed to some terminating subtask and thus avoid the
(mis)use of these assertions in later reasoning.
As an example, consider the achieve-proximity subtask. This subtask is used in
a number of different situations when an agent needs to get closer to another agent. If
the wing strays too far from the lead, it may invoke achieve-proximity to get back into
formation with the lead. The lead uses achieve-proximity to get close enough to an enemy
aircraft to launch a missile. The subtask requires many local computations as the agent
reasons about what heading it should take to get closer to another aircraft. The specific
computation depends on what information is available about the other aircraft. When the
wing is pursuing the lead, it may know the leads heading and thus calculate a collision
course to maximize the rate of convergence. Sometimes the other agents heading is not
available. In this case, the agent simply moves toward the current location of the other agent.
These local computations are stored in the local subtask. When achieve-proximity is
terminated in the FHJ agent, the agent removes the local structure. Removing the structure
is important both because it interrupts entailment of the local structure (e.g., calculation of
the current collision course) and guarantees that if the agent decides to achieve-proximity
with a different aircraft, supporting data structures are properly initialized. This knowledge
thus maintains consistency in the local subtask by removing the local structure when the
achieve-proximity subtask is no longer selected.
The FHJ agent could recognize when it was going to remove a subtask. The termination
conditions in FHJ agents acted as a signal to the within-level consistency knowledge. The
knowledge that removes the local structure for achieve-proximity can be summarized as:
if the achieve-proximity operator is selected, but its initiation conditions no longer hold,
then remove the local achieve-proximity data structure. Thus, the FHJ agent uses a
recognition of an inconsistency in the assertions to trigger the activation of this within-level
consistency knowledge.
When the subtasks initiating conditions are no longer supported in the DHJ agents, the
selected subtask is removed immediately. Thus, the DHJ agent never has the opportunity
to apply the FHJ agents within-level consistency knowledge. The failure to utilize this
knowledge led to a number of problems, including more regenerations than expected.
To solve this problem, the local subtask data structure was created as an entailment of
the initiation conditions of the subtask itself. When the subtask initiation conditions no
longer held, both the subtask selection and the local structure are immediately removed by
393

fiWray & Laird

the architecture, requiring no additional knowledge. Thus, this change obviated the need
for some within-level consistency knowledge. However, the local data structure may need
to be regenerated if a subtask is temporarily displaced. For instance, the FHJ within-level
consistency knowledge could determine under what conditions the local structure should be
removed. The DHJ solution has lost that flexibility.
Subtasks with Complex Actions: FHJ agents can execute a number of actions in rapid
succession, regardless of any inconsistency in the local assertions. A single subtask operator
can be initiated in a situation representing the conditions under which to apply the first
action in a sequence, and terminated when the last step in the sequence has applied. If
some intermediate step invalidates the initiation conditions, the subtask still executes the
actions.
Consider the process of launching a missile. An actual missile launch requires only
the push of a button, assuming that previous steps such as selecting the target and an
appropriate missile have been accomplished beforehand. After pushing the fire button, the
pilot must fly straight and level for a few seconds while the missile rockets ignite and launch
the missile into flight. Once the missile has cleared the aircraft, the agent supports the
missile by keeping radar contact with the target. In FHJ agents, the push-fire-button
subtask includes both the act of pushing the fire button and counting while the missile clears
the aircraft. These tasks have different and mutually exclusive dependencies. The initiation
condition for push-fire-button requires that no missile is already launched. However, the
subsequent counting requires monitoring the newly launched missile.
DHJ agents using the FHJ knowledge base always remove the push-fire-button subtask as soon as the missile is perceived to be in the air, interrupting the complete procedure.
Regeneration of the push-fire-button subtask occurs because the agent never waits for the
missile to clear and thus never realizes that the missile just launched needs to be supported.
The DHJ agent unsuccessfully fires all available missiles at the enemy plane.
Pushing the fire button and waiting for the missile to clear are independent tasks
which happen to arise in serial order in the domain. We enforced this independence by
creating a new subtask, wait-for-missile-to-clear, which depends only on having a
newly launched missile in the air. The DHJ agent now pushes the fire button, selects
wait-for-missile-to-clear to count a few seconds before taking any other action, and
then supports the missile if it clears successfully.
This solution reduces regeneration and improves behavior quality but it does have a
non-trivial cost. Whenever a subtask is split, the effects of subtask actions no longer occur
in rapid succession within a decision. Instead, the effect of the first subtask occurs in one
decision, the effect of the second subtask in the second decision, etc. Thus, this solution
can compromise responsiveness.

References
Agre, P. E., & Horswill, I. (1997). Lifeworld analysis. Journal of Artificial Intelligence
Research, 6, 111145.
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. Journal of Symbolic Logic, 50 (2),
510530.
394

fiEnsuring Consistency in Hierarchical Execution

Allen, J. F. (1991). Time and time again. International Journal of Intelligent Systems,
6 (4), 341355.
Altmann, E. M., & Gray, W. D. (2002). Forgetting to remember: The functional relationship
of decay and interference. Psychological Science, 13, 2733.
Bresina, J., Drummond, M., & Kedar, S. (1993). Reactive, integrated systems pose new
problems for machine learning. In Minton, S. (Ed.), Machine Learning Methods for
Planning, pp. 159195. Morgan Kaufmann, San Francisco, CA.
Dechter, R. (1990). Enhancement schemes for constraint processing: Backjumping, learning
and cutset decomposition. Artificial Intelligence, 41, 273312.
Doyle, J. (1979). A truth maintenance system. Artificial Intelligence, 12, 231272.
Doyle, J. (1994). Reason maintenance and belief revision. In Gardenfors, P. (Ed.), Belief
Revision, pp. 2951. Cambridge University Press, Cambridge, UK.
Eiter, T., & Gottlob, G. (1992). On the complexity of propositional knowledge base revision,
updates, and counterfactuals. Artificial Intelligence, 57, 227270.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity and expressivity. In
Proceedings of the 12th National Conference on Artificial Intelligence, pp. 11231128.
Firby, R. J. (1987). An investigation into reactive planning in complex domains. In Proceedings of the 6th National Conference on Artificial Intelligence, pp. 202206.
Forbus, K. D., & deKleer, J. (1993). Building Problem Solvers. MIT Press, Cambridge,
MA.
Forgy, C. L. (1979). On the Efficient Implementation of Production Systems. Ph.D. thesis,
Computer Science Department, Carnegie-Mellon University.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
MIT Press, Cambridge, MA.
Gardenfors, P. (1992). Belief revision. In Pettorossi, A. (Ed.), Meta-Programming in Logic.
Springer-Verlag, Berlin, Germany.
Gaschnig, J. (1979). Performance measurement and analysis of certain search algorithms.
Tech. rep. CMU-CS-79-124, Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.
Gat, E. (1991a). Integrating planning and reacting in a heterogeneous asynchronous architecture for mobile robots. SIGART BULLETIN, 2, 7174.
Gat, E. (1991b). Reliable, Goal-directed Control of Autonomous Mobile Robots. Ph.D.
thesis, Virginia Polytechnic Institute and State University, Blacksburg, VA.
Georgeff, M., & Lansky, A. L. (1987). Reactive reasoning and planning. In Proceedings of
the 6th National Conference on Artificial Intelligence, pp. 677682.
Graham, J., & Decker, K. (2000). Towards a distributed, environment-centered agent framework. In Wooldridge, M., & Lesperance, Y. (Eds.), Lecture Notes in Artificial Intelligence: Agent Theories, Architectures, and Languages VI (ATAL-99). Springer-Verlag,
Berlin.
395

fiWray & Laird

Hanks, S., Pollack, M., & Cohen, P. R. (1993). Benchmarks, test beds, controlled experimentation and the design of agent architectures. AI Magazine, 14, 1742.
Hayes-Roth, B. (1990). An architecture for adaptive intelligent systems. In Workshop on
Innovative Approaches to Planning, Scheduling and Control, pp. 422432.
Jones, R. M., Laird, J. E., Neilsen, P. E., Coulter, K. J., Kenny, P., & Koss, F. V. (1999).
Automated intelligent pilots for combat flight simulation. AI Magazine, 20 (1), 2741.
Kinny, D., & Georgeff, M. (1991). Commitment and effectiveness of situated agents. In
Proceedings of the 12th International Joint Conference on Artificial Intelligence, pp.
8288.
Kurien, J., & Nayak, P. P. (2000). Back to the future for consistency-based trajectory
tracking. In Proceedings of the 17th National Conference on Artificial Intelligence,
pp. 370377.
Laird, J. E. (2001). It knows what you are going to do: Adding anticipation to a Quakebot.
In Proceedings of the 5th International Conference on Autonomous Agents, pp. 385
392.
Laird, J. E., Congdon, C. B., & Coulter, K. J. (1999). Soar users manual version 8.2.
Manual, Department of Electrical Engineering and Computer Science, University of
Michigan, http://ai.eecs.umiuch.edu/soar/docs.html.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: An architecture for general
intelligence. Artificial Intelligence, 33, 164.
Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, and learning
in Soar for external environments. In Proceedings of the 8 th National Conference on
Artificial Intelligence, pp. 10221029.
Laird, J. E., & Rosenbloom, P. S. (1995). The evolution of the Soar cognitive architecture.
In Steier, D., & Mitchell, T. (Eds.), Mind Matters: Contributions to Cognitive and
Computer Science in Honor of Allen Newell. Lawrence Erlbaum Associates, Hillsdale,
NJ.
McDermott, D. (1991). A general framework for reason maintenance. Artificial Intelligence,
50, 289329.
Mitchell, T. M., Allen, J., Chalasani, P., Cheng, J., Etzioni, O., Ringuette, M., & Schlimmer,
J. C. (1991). Theo: A framework for self-improving systems. In VanLehn, K. (Ed.),
Architectures for Intelligence, chap. 12, pp. 323355. Lawrence Erlbaum Associates,
Hillsdale, NJ.
Mitchell, T. M. (1990). Becoming increasingly reactive. In Proceedings of the 8 th National
Conference on Artificial Intelligence, pp. 10511058.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: A theoretical and
empirical analysis. Artificial Intelligence, 76, 427454.
Newell, A. (1990). Unified Theories of Cognition. Harvard University Press, Cambridge,
MA.
396

fiEnsuring Consistency in Hierarchical Execution

Paolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (1999). A planning component for RETSINA agents. In Wooldridge, M., & Lesperance, Y. (Eds.), Lecture Notes
in Artificial Intelligence: Agent Theories, Architectures, and Languages VI (ATAL99), pp. 147161, Berlin. Springer-Verlag.
Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. (1993). A
symbolic solution to intelligent real-time control. Robotics and Autonomous Systems,
11, 279291.
Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within a BDI-architecture.
In Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, pp. 471484.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall,
Upper Saddle River, NJ.
Sacerdoti, E. D. (1975). The nonlinear nature of plans. In Proceedings of the 4 th International Joint Conference on Artificial Intelligence, pp. 206214.
Schut, M., & Wooldridge, M. (2000). Intention reconsideration in complex environments. In
Proceedings of the 4th International Conference on Autonomous Agents, pp. 209216.
Schut, M., & Wooldridge, M. (2001). Principles of intention reconsideration. In Proceedings
of the 5th International Conference on Autonomous Agents, pp. 340347.
Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 5192.
Simon, H. A. (1969). The Sciences of the Artificial. MIT Press, Cambridge, MA.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning and dependency-directed
backtracking in a system for computer aided circuit analysis. Artificial Intelligence,
9 (2), 135196.
Sycara, K., Decker, K., Pannu, A., Williamson, M., & Zeng, D. (1996). Distributed intelligent agents. IEEE Expert, 11 (6), 3646.
Tambe, M. (1991). Eliminating Combinatorics from Production Match. Ph.D. thesis,
Carnegie-Mellon University. (Also published as Technical Report CMU-CS-91-150,
Computer Science Department, Carnegie Mellon University.).
Tambe, M., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E., Rosenbloom, P. S., &
Schwamb, K. (1995). Intelligent agents for interactive simulation environments. AI
Magazine, 16 (1), 1539.
Veloso, M. M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring for planning in dynamic environments. In Proceedings of the 4 th International Conference on
Artificial Intelligence Planning Systems, pp. 171180.
Wilkins, D. E., Myers, K. L., Lowrance, J. D., & Wesley, L. P. (1995). Planning and reacting
in uncertain and dynamic environments. Journal of Experimental and Theoretical
Artificial Intelligence, 7 (1), 197227.
Wooldridge, M. (2000). Reasoning about Rational Agents. MIT Press, Cambridge, MA.
Wray, R. E. (1998). Ensuring Reasoning Consistency in Hierarchical Architectures. Ph.D.
thesis, University of Michigan. Also published as University of Michigan Technical
Report CSE-TR-379-98.
397

fiWray & Laird

Wray, R. E., & Laird, J. (1998). Maintaining consistency in hierarchical reasoning. In
Proceedings of the 15th National Conference on Artificial Intelligence, pp. 928935.
Wray, R. E., Laird, J., & Jones, R. M. (1996). Compilation of non-contemporaneous constraints. In Proceedings of the 13th National Conference on Artificial Intelligence, pp.
771778.
Wray, R. E., Laird, J. E., Nuxoll, A., & Jones, R. M. (2002). Intelligent opponents for virtual
reality trainers. In Proceedings of the Interservice/Industry Training, Simulation and
Education Conference (I/ITSEC) 2002.

398

fiJournal of Artificial Intelligence Research 19 (2003) 209-242

Submitted 12/02; published 9/03

Decision-Theoretic Bidding Based on Learned Density
Models in Simultaneous, Interacting Auctions

pstone@cs.utexas.edu

Peter Stone

Dept. of Computer Sciences, The University of Texas at Austin
1 University Station C0500, Austin, Texas 78712-1188 USA

Robert E. Schapire

schapire@cs.princeton.edu

Michael L. Littman

mlittman@cs.rutgers.edu

Department of Computer Science, Princeton University
35 Olden Street, Princeton, NJ 08544 USA
Dept. of Computer Science, Rutgers University
Piscataway, NJ 08854-8019 USA

janos@pobox.com

Janos A. Csirik

D. E. Shaw & Co.
120 W 45th St, New York, NY 10036 USA

David McAllester

Toyota Technological Institute at Chicago
1427 East 60th Street, Chicago IL, 60637 USA

mcallester@tti-chicago.edu

Abstract

Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous
bidding agents to bid in multiple simultaneous auctions for interacting goods. A core
component of our approach learns a model of the empirical price dynamics based on past
data and uses the model to analytically calculate, to the greatest extent possible, optimal
bids. We introduce a new and general boosting-based algorithm for conditional density
estimation problems of this kind, i.e., supervised learning problems in which the goal is to
estimate the entire conditional distribution of the real-valued label. This approach is fully
implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition
(TAC-01). We present experiments demonstrating the effectiveness of our boosting-based
price predictor relative to several reasonable alternatives.
1. Introduction

Auctions are an increasingly popular method for transacting business, especially over the
Internet. In an auction for a single good, it is straightforward to create automated bidding
strategies|an agent could keep bidding until reaching a target reserve price, or it could
monitor the auction and place a winning bid just before the closing time (known as sniping).
When bidding for multiple interacting goods in simultaneous auctions, on the other
hand, agents must be able to reason about uncertainty and make complex value assessments. For example, an agent bidding on one's behalf in separate auctions for a camera and
ash may end up buying the ash and then not being able to find an affordable camera.
Alternatively, if bidding for the same good in several auctions, it may purchase two ashes
when only one was needed.
c 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiStone, Schapire, Littman, Csirik, & McAllester

This article makes three main contributions. The first contribution is a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for
interacting goods. We start with the observation that the key challenge in auctions is the
prediction of eventual prices of goods: with complete knowledge of eventual prices, there
are direct methods for determining the optimal bids to place. Our guiding principle is to
have the agent model its uncertainty in eventual prices and, to the greatest extent possible,
analytically calculate optimal bids.
To attack the price prediction problem, we propose a machine-learning approach: gather
examples of previous auctions and the prices paid in them, then use machine-learning methods to predict these prices based on available features in the auction. Moreover, for our
strategy, we needed to be able to model the uncertainty associated with predicted prices;
in other words, we needed to be able to sample from a predicted distribution of prices
given the current state of the game. This can be viewed as a conditional density estimation
problem, that is, a supervised learning problem in which the goal is to estimate the entire
distribution of a real-valued label given a description of current conditions, typically in the
form of a feature vector. The second main contribution of this article is a new algorithm
for solving such general problems based on boosting (Freund & Schapire, 1997; Schapire &
Singer, 1999).
The third contribution of this article is a complete description of a prototype implementation of our approach in the form of
, a top-scoring agent1 in the second Trading
Agent Competition (TAC-01) that was held in Tampa Bay, FL on October 14, 2001 (Wellman, Greenwald, Stone, & Wurman, 2003a). The TAC domain was the main motivation
for the innovations reported here.
builds on top of
(Stone, Littman,
Singh, & Kearns, 2001), the top-scoring agent at TAC-00, but introduces a fundamentally
new approach to creating autonomous bidding agents.
We present details of
as an instantiation of its underlying principles that we
believe have applications in a wide variety of bidding situations.
uses a predictive, data-driven approach to bidding based on expected marginal values of all available
goods. In this article, we present empirical results demonstrating the robustness and effectiveness of
's adaptive strategy. We also report on
's performance at
TAC-01 and TAC-02 and reect on some of the key issues raised during the competitions.
The remainder of the article is organized as follows. In Section 2, we present our
general approach to bidding for multiple interacting goods in simultaneous auctions. In
Section 3, we summarize TAC, the substrate domain for our work. Section 4 describes our
boosting-based price predictor. In Section 5, we give the details of
. In Section 6,
we present empirical results including a summary of
's performance in TAC01, controlled experiments isolating the successful aspects of
, and controlled
experiments illustrating some of the lessons learned during the competition. A discussion
and summary of related work is provided in Sections 7 and 8.
ATTac-2001

ATTac-2001

ATTac-2000

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

2. General Approach

In a wide variety of decision-theoretic settings, it is useful to be able to evaluate hypothetical
situations. In computer chess, for example, a static board evaluator is used to heuristically
1. Top-scoring by one metric, and second place by another.

210

fiDecision-Theoretic Bidding with Learned Density Models

measure which player is ahead and by how much in a given board situation. The scenario
is similar in auction domains, and our bidding agent
uses a situation evaluator,
analogous to the static board evaluator, which estimates the agent's expected profit in a
hypothetical future situation. This \profit predictor" has a wide variety of uses in the agent.
For example, to determine the value of an item, the agent compares the predicted profit
assuming the item is already owned to the predicted profit assuming that the item is not
available.
Given prices for goods, one can often compute a set of purchases and an allocation that
maximizes profit.2 Similarly, if closing prices are known, they can be treated as fixed, and
optimal bids can be computed (bid high for anything you want to buy). So, one natural
profit predictor is simply to calculate the profit of optimal purchases under fixed predicted
prices. (The predicted prices can, of course, be different in different situations, e.g., previous
closing prices can be relevant to predicting future closing prices.)
A more sophisticated approach to profit prediction is to construct a model of the probability distribution over possible future prices and to place bids that maximize expected
profit. An approximate solution to this dicult optimization problem can be created by
stochastically sampling possible prices and computing a profit prediction as above for each
sampled price. A sampling-based scheme for profit prediction is important for modeling
uncertainty and the value of gaining information, i.e., reducing the price uncertainty.
Section 2.1 formalizes this latter approach within a simplified sequential auction model.
This abstraction illustrates some of the decision-making issues in our full sampling-based
approach presented in Section 2.2. The full setting that our approach addresses is considerably more complex than the abstract model, but our simplifying assumptions allow us to
focus on a core challenge of the full scenario. Our guiding principle is to make decisiontheoretically optimal decisions given profit predictions for hypothetical future situations.3
ATTac-2001

2.1 Simplified Abstraction

In the simple model, there are n items to be auctioned off in sequence (first item 0, then
item 1, etc.). The bidder must place a bid r for each item i, and after each bid, a closing
price y is chosen for the corresponding item from a distribution specific to the item. If the
bid matches or exceeds the closing price, r  y , the bidder holds item i, h = 1. Otherwise,
the bidder does not hold the item, h = 0. The bidder's utility v(H ) is a function of its
final vector of holdings H = (h0 ; : : : ; h 1 ) and its cost is a function of the holdings and
the vector of closing prices, H  Y . We will formalize the problem of optimal bid selection
and develop a series of approximations to make the problem solvable.
i

i

i

i

i

i

n

2. The problem is computationally dicult in general, but has been solved effectively in the non-trivial
TAC setting (Greenwald & Boyan, 2001; Stone et al., 2001).
3. An alternative approach would be to abstractly calculate the Bayes-Nash equilibrium (Harsanyi, 1968)
for the game and play the optimal strategy. We dismissed this approach because of its intractability in
realistically complex situations, including TAC. Furthermore, even if we were able to approximate the
equilibrium strategy, it is reasonable to assume that our opponents would not play optimal strategies.
Thus, we could gain additional advantage by tuning our approach to our opponents' actual behavior as
observed in the earlier rounds, which is essentially the strategy we adopted.

211

fiStone, Schapire, Littman, Csirik, & McAllester
2.1.1 Exact Value

What is the value of the auction, that is, the bidder's expected profit (utility minus cost)
for bidding optimally for the rest of the auction? If a bidder knows this value, it can make
its next bid to be one that maximizes its expected profit. The value is a function of the
bidder's current holdings H and the current item to bid on, i. It can be expressed as
value(i; H ) = max E max
E +1 : : : max E 1 (v (G + H ) G  Y );
(1)
1
+1
yi

ri

yi

ri

rn

yn

where the components of G are the new holdings as a result of additional winnings g 
 y . Note that H only has non-zero entries for items that have already been sold
(8j  i; H = 0) and G only has non-zero entries for items that have yet to be sold
(8j < i; G = 0). Note also that G and Y are fully specified when the g and y variables
(for j  i) are bound sequentially by the expectation and maximization operators. The
idea here is that the bids r through r 1 are chosen to maximize value in the context of
the possible closing prices y .
Equation 1 is closely related to the equations defining the value of a finite-horizon partially observable Markov decision process (Papadimitriou & Tsitsiklis, 1987) or a stochastic
satisfiability expression (Littman, Majercik, & Pitassi, 2001). Like these other problems,
the sequential auction problem is computationally intractable for suciently general representations of v() (specifically, linear functions of the holdings are not expressive enough to
achieve intractability while arbitrary nonlinear functions are).
j

rj

j

j

j

j

i

j

n

j

2.1.2 Approximate Value by Reordering

There are three major sources of intractability in Equation 1|the alternation of the maximization and expectation operators (allowing decisions to be conditioned on an exponential
number of possible sets of holdings), the large number of maximizations (forcing an exponential number of decisions to be considered), and the large number of expectations
(resulting in sums over an exponential number of variable settings).
We attack the problem of interleaved operators by moving all but the first of the maximizations inside the expectations, resulting in an expression that approximates the value:
(2)
value-est(i; H ) = max E E +1 : : : E 1 max
: : : max(v (G + H ) G  Y ):
1
+1
ri

yi

yi

yn

ri

rn

Because the choices for bids r +1 through r 1 appear more deeply nested than the bindings
for the closing prices y through y 1, they cease to be bids altogether, and instead represent
decisions as to whether to purchase goods at given prices. Let G = opt(H; i; Y ) be a vector
representing the optimal number of goods to purchase at the prices specified by the vector
Y given the current holdings H starting from auction i. Conceptually, this can be computed
by evaluating
opt(H; i; Y ) = argmax (v(G + H ) H  Y ):
(3)
1
Thus, Equation 2 can be written:
0
0
0
value-est(i; H ) = max E
1 (v (opt(H ; i + 1; Y ) + H ) opt(H ; i + 1; Y )  Y ) (4)
i

i

n

n

gi ;:::;gn

ri

yi ;:::;yn

212

fiDecision-Theoretic Bidding with Learned Density Models

where H 0 is identical to H except the i-th component reects whether item i is won|r  y .
Note that there is a further approximation that can be made by computing the expected
prices (as point values) before solving the optimization problem. This approach corresponds
to further swapping the expectations towards the core of the equation:
value-est(i; H ) = max(v(opt(H 0 ; i + 1; E ) + H 0) opt(H 0; i + 1; E )  E ) (5)
i

ev

Y

ri

Y

i

Y

where E = E [y +1 ; : : : ; y 1], the vector of expected costs of the goods. In the remainder
of the article, we refer to methods that use this further approximation from Equation 5 as
expected value approaches for reasons that will be come apparent shortly.
The technique of swapping maximization and expectation operators was previously used
by Hauskrecht (1997) to generate a bound for solving partially observable Markov decision
processes. The decrease of uncertainty when decisions are made makes this approximation
an upper bound on the true value of the auction: value-est  value. The tightness of the approximations in Equations 2 and 5 depends on the true distributions of the expected prices.
For example, if the prices were known in advance with certainty, then both approximations
are exact.
Y

i

n

2.1.3 Approximate Bidding

Given a vector of costs Y , the optimization problem opt(H; i; Y ) in Equation 4 is still NPhard (assuming the representation of the utility function v() is suciently complex). For
many representations of v(), the optimization problem can be cast as an integer linear program and approximated by using the fractional relaxation instead of the exact optimization
problem. This is precisely the approach we have adopted in ATTac (Stone et al., 2001).
2.1.4 Approximation via Sampling

Even assuming that opt(H; i; Y ) can be solved in unit time, a literal interpretation of Equation 4 says we'll need to solve this optimization problem for an exponential number of cost
vectors (or even more if the probability distributions Pr(y ) are continuous). Kearns, Mansour, and Ng (1999) showed that values of partially observable Markov decision processes
could be estimated accurately by sampling trajectories instead of exactly computing sums.
Littman et al. (2001) did the same for stochastic satisfiability expressions. Applying this
idea to Equation 4 leads to the following algorithm.
1. Generate a set S of vectors of closing costs Y according to the product distribution
Pr(y )      Pr(y 1).
2. For each of these samples, calculate opt(H 0; i +1; Y ) as defined above and average the
results, resulting in the approximation
X
value-est (i; H ) = max (v(opt(H 0; i + 1; Y ) + H 0) opt(H 0 ; i + 1; Y )  Y )=jS j: (6)
j

i

n

s

ri

Y

2

S

This expression converges to value-est with increasing sample size.
A remaining challenge in evaluating Equation 6 is computing the real-valued bid r that
maximizes the value. Note that we want to buy item i precisely at those closing prices for
i

213

fiStone, Schapire, Littman, Csirik, & McAllester

which the value of having the item (minus its cost) exceeds the value of not having the item;
this maximizes profit. Thus, to make a positive profit, we are willing to pay up to, but not
more than, the difference in value of having the item and not having the item.
Formally, let H be the vector of current holdings and H be the holdings modified to
reect winning item i. Let G (Y ) = opt(H ; i+1; Y ), the optimal set of purchases assuming
item i was won, and G(Y ) = opt(H; i+1; Y ) the optimal set of purchases assuming otherwise
(except in cases of ambiguity, we write simply G and G for G (Y ) and G(Y ) respectively).
We want to select r to achieve the equivalence
X
X
r y 
(v(G + H ) G  Y )=jS j y  (v(G + H ) G  Y )=jS j: (7)
w

w

w

w

w

i

i

i

Y

Setting

ri

w

2

w

i

S

=

Y

X
Y

2

([v(G + H )
w

2

S

 ] [v(G + H )

Gw Y

 ]) j j

(8)

G Y =S:

S

achieves the equivalence desired in Equation 7, as can be verified by substitution, and
therefore bidding the average difference between holding and not holding the item maximizes
the value.4
2.2 The Full Approach

Leveraging from the preceding analysis, we define our sampling-based approach to profit
prediction in general simultaneous, multi-unit auctions for interacting goods. In this scenario, let there be n simultaneous, multi-unit auctions for interacting goods a0 ; : : : ; a 1 .
The auctions might close at different times and these times are not, in general, known in
advance to the bidders. When an auction closes, let us assume that the m units available
are distributed irrevocably to the m highest bidders, who each need to pay the price bid
by the mth highest bidder. This scenario corresponds to an mth price ascending auction.5
Note that the same bidder may place multiple bids in an auction, and thereby potentially
win multiple units. We assume that after the auction closes, the bidders will no longer have
any opportunity to acquire additional copies of the goods sold in that auction (i.e., there
is no aftermarket).
Our approach is based upon five assumptions. For G = (g0 ; : : : ; g 1 ) 2 , let v(G) 2
represent the value derived by the agent if it owns g units of the commodity being sold in
auction a . Note that v is independent of the costs of the commodities. Note further that
this representation allows for interacting goods of all kinds, including complementarity and
substitutability.6 The assumptions of our approach are as follows:
1. Closing prices are somewhat, but only somewhat, predictable. That is, given a set
of input features X , for each auction a , there exists a sampling rule that outputs a
n

n

IN

n

IR

i

i

i

4. Note that the strategy for choosing ri in Equation 8 does not exploit the fact that the sample S contains
only a finite set of possibilities for yi , which might make it more robust to inaccuracies in the sampling.
5. For large enough m it is practically the same as the more ecient m + 1st auction. We use the mth
price model because that is what is used in TAC's hotel auctions.
6. Goods are considered complementary if their value as a package is greater than the sum of their individual
values; goods are considered substitutable if their value as a package is less than the sum of their
individual values.

214

fiDecision-Theoretic Bidding with Learned Density Models

closing price y according to a probability distribution of predicted closing prices for
a.
2. Given a vector of holdings H = (h0 ; : : : ; h 1 ) where h 2 represents the quantity
of the commodity being sold in auction a that are already owned by the agent, and
given a vector of fixed closing prices Y = (y0; : : : ; y 1), there exists a tractable
procedure opt(H; Y ) to determine the optimal set of purchases (g0 ; : : : ; g 1 ) where
g 2 represents the number of goods to be purchased in auction i such that
v (opt(H; Y ) + H ) opt(H; Y )  Y  v (G + H ) G  Y
for all G 2 . This procedure corresponds to the optimization problem opt(H; i; Y )
in Equation 3.
3. An individual agent's bids do not have an appreciable effect on the economy (large
population assumption).
4. The agent is free to change existing bids in auctions that have not yet closed.
5. Future decisions are made in the presence of complete price information. This assumption corresponds to the operator reordering approximation from the previous
section.
While these assumptions are not all true in general, they can be reasonable enough approximations to be the basis for an effective strategy.
By Assumption 3, the price predictor can generate predicted prices prior to considering
one's bids. Thus, we can sample from these distributions to produce complete sets of closing
prices of all goods.
For each good under consideration, we assume that it is the next one to close. If a
different auction closes first, we can then revise our bids later (Assumption 4). Thus, we
would like to bid exactly the good's expected marginal utility to us. That is, we bid the
difference between the expected utilities attainable with and without the good. To compute
these expectations, we simply average the utilities of having and not having the good under
different price samples as in Equation 8. This strategy rests on Assumption 5 in that we
assume that bidding the good's current expected marginal utility cannot adversely affect
our future actions, for instance by impacting our future space of possible bids. Note that as
time proceeds, the price distributions change in response to the observed price trajectories,
thus causing the agent to continually revise its bids.
Table 1 shows pseudo-code for the entire algorithm. A fully detailed description of an
instantiation of this approach is given in Section 5.
i

i

n

i

IN

i

n

n

i

IN

n

IN

2.3 Example

Consider a camera and a ash with interacting values to an agent as shown in Table 2.
Further, consider that the agent estimates that the camera will sell for $40 with probability
25%, $70 with probability 50%, and $95 with probability 25%. Consider the question of
what the agent should bid for the ash (in auction a0). The decision pertaining to the
camera would be made via a similar analysis.
215

fiStone, Schapire, Littman, Csirik, & McAllester



Let H = (h0 ; : : : ; hn 1 ) be the agent's current holdings in each of the n auctions.
For i = 0 to n

1 (assume auction i is next to close):

{ total-diff = 0
{ counter = 0
{ As time permits:



For each auction aj ; j 6= i, generate a predicted price sample yj . Let Y =
(y0 ; : : : ; yi 1 ; 1; yi+1 ; : : : ; yn 1 ).
 Let Hw = (h0 ; : : : ; hi 1 ; hi + 1; hi+1 ; : : : ; hn 1 ), the vector of holdings if the agent
wins a unit in auction ai .
 Compute Gw = opt(Hw ; Y ), the optimal set of purchases if the agent wins a unit in
auction ai . Note that no additional units of the good will be purchased, since the
i-th component of Y is 1.
 Compute G = opt(H; Y ), the optimal set of purchases if the agent never acquires
any additional units in the auction ai and prices are set to Y .
 diff = [v(Gw + H ) Gw  Y ] [v(G + H ) G  Y ]
 total-diff = total-diff + diff
 counter = counter + 1
{ r = total-diff=counter
{ Bid r in auction ai .

Table 1: The decision-theoretic algorithm for bidding in simultaneous, multi-unit, interacting auctions.
utility

$50
10
100
0
Table 2: The table of values for all combination of camera and ash in our example.
camera alone
ash alone
both
neither

First, the agent samples from the distribution of possible camera prices. When the price
of the camera (sold in auction a1) is $70 in the sample:
 H = (0; 0); H = (1; 0); Y = (1; 70)
 G = opt(H ; Y ) is the best set of purchases the agent can make with the ash, and
assuming the camera costs $70. In this case, the only two options are buying the
camera or not. Buying the camera yields a profit of 100 70 = 30. Not buying the
camera yields a profit of 10 0 = 10. Thus, G = (0; 1), and [v(G + H ) G  Y ] =
v (1; 1) (0; 1)  (1; 70) = 100 70.
 Similarly G = (0; 0) (since if the ash is not owned, buying the camera yields a profit of
50 70 = 20, and not buying it yields a profit of 0 0 = 0) and [v(G + H ) G  Y ] = 0.
w

w

w

w

216

w

w

fiDecision-Theoretic Bidding with Learned Density Models

 val = 30 0 = 30.

Similarly, when the camera is predicted to cost $40, val = 60 10 = 50; and when the camera
is predicted to cost $95, val = 10 0 = 10. Thus, we expect that 50% of the camera price
samples will suggest a ash value of $30, while 25% will lead to a value of $50 and the other
25% will lead to a value of $10. Thus, the agent will bid :5  30 + :25  50 + :25  10 = $30
for the ash.
Notice that in this analysis of what to bid for the ash, the actual closing price of
the ash is irrelevant. The proper bid depends only on the predicted price of the camera.
To determine the proper bid for the camera, a similar analysis would be done using the
predicted price distribution of the ash.
3. TAC

We instantiated our approach as an entry in the second Trading Agent Competition (TAC),
as described in this section. Building on the success of TAC-00 held in July 2000 (Wellman,
Wurman, O'Malley, Bangera, Lin, Reeves, & Walsh, 2001), TAC-01 included 19 agents from
9 countries (Wellman et al., 2003a). A key feature of TAC is that it required autonomous
bidding agents to buy and sell multiple interacting goods in auctions of different types. It
is designed as a benchmark problem in the complex and rapidly advancing domain of emarketplaces, motivating researchers to apply unique approaches to a common task. By
providing a clear-cut objective function, TAC also allows the competitors to focus their
attention on the computational and game-theoretic aspects of the problem and leave aside
the modeling and model validation issues that invariably loom large in real applications of
automated agents to auctions (see Rothkopf & Harstad, 1994). Another feature of TAC
is that it provides an academic forum for open comparison of agent bidding strategies in
a complex scenario, as opposed to other complex scenarios, such as trading in real stock
markets, in which practitioners are (understandably) reluctant to share their technologies.
A TAC game instance pits eight autonomous bidding agents against one another. Each
TAC agent is a simulated travel agent with eight clients, each of whom would like to travel
from TACtown to Tampa and back again during a 5-day period. Each client is characterized
by a random set of preferences for the possible arrival and departure dates, hotel rooms, and
entertainment tickets. To satisfy a client, an agent must construct a travel package for that
client by purchasing airline tickets to and from TACtown and securing hotel reservations; it
is possible to obtain additional bonuses by providing entertainment tickets as well. A TAC
agent's score in a game instance is the difference between the sum of its clients' utilities for
the packages they receive and the agent's total expenditure. We provide selected details
about the game next; for full details on the design and mechanisms of the TAC server and
TAC game, see http://www.sics.se/tac.
TAC agents buy ights, hotel rooms and entertainment tickets through auctions run
from the TAC server at the University of Michigan. Each game instance lasts 12 minutes
and includes a total of 28 auctions of 3 different types.
Flights (8 auctions): There is a separate auction for each type of airline ticket: to Tampa
(inights) on days 1{4 and from Tampa (outights) on days 2{5. There is an unlimited
supply of airline tickets, and every 24{32 seconds their ask price changes by from $10
217

fiStone, Schapire, Littman, Csirik, & McAllester

to $x. x increases linearly over the course of a game from 10 to y, where y 2 [10; 90]
is chosen uniformly at random for each auction, and is unknown to the bidders. In
all cases, tickets are priced between $150 and $800. When the server receives a bid at
or above the ask price, the transaction is cleared immediately at the ask price and no
resale is allowed.
Hotel Rooms (8): There are two different types of hotel rooms|the Tampa Towers (TT)
and the Shoreline Shanties (SS)|each of which has 16 rooms available on days 1{4.
The rooms are sold in a 16th-price ascending (English) auction, meaning that for each
of the 8 types of hotel rooms, the 16 highest bidders get the rooms at the 16th highest
price. For example, if there are 15 bids for TT on day 2 at $300, 2 bids at $150, and
any number of lower bids, the rooms are sold for $150 to the 15 high bidders plus
one of the $150 bidders (earliest received bid). The ask price is the current 16thhighest bid and transactions clear only when the auction closes. Thus, agents have
no knowledge of, for example, the current highest bid. New bids must be higher than
the current ask price. No bid withdrawal or resale is allowed, though the price of bids
may be lowered provided the agent does not reduce the number of rooms it would win
were the auction to close. One randomly chosen hotel auction closes at minutes 4{11
of the 12-minute game. Ask prices are changed only on the minute.
Entertainment Tickets (12): Alligator wrestling, amusement park, and museum tickets
are each sold for days 1{4 in continuous double auctions. Here, agents can buy and
sell tickets, with transactions clearing immediately when one agent places a buy bid
at a price at least as high as another agent's sell price. Unlike the other auction
types in which the goods are sold from a centralized stock, each agent starts with a
(skewed) random endowment of entertainment tickets. The prices sent to agents are
the bid-ask spreads, i.e., the highest current bid price and the lowest current ask price
(due to immediate clears, ask price is always greater than bid price). In this case, bid
withdrawal and ticket resale are both permitted. Each agent gets blocks of 4 tickets
of 2 types, 2 tickets of another 2 types, and no tickets of the other 8 types.
In addition to unpredictable market prices, other sources of variability from game instance to game instance are the client profiles assigned to the agents and the random initial
allotment of entertainment tickets. Each TAC agent has eight clients with randomly assigned travel preferences. Clients have parameters for ideal arrival day, IAD (1{4); ideal
departure day, IDD (2{5); hotel premium, HP ($50{$150); and entertainment values, EV
($0{$200) for each type of entertainment ticket.
The utility obtained by a client is determined by the travel package that it is given in
combination with its preferences. To obtain a non-zero utility, the client must be assigned
a feasible travel package consisting of an inight on some arrival day AD, an outight on a
departure day DD, and hotel rooms of the same type (TT or SS) for the days in between
(days d such that AD  d < DD). At most one entertainment ticket of each type can be
assigned, and no more than one on each day. Given a feasible package, the client's utility
is defined as
1000 travelPenalty + hotelBonus + funBonus
where
218

fiDecision-Theoretic Bidding with Learned Density Models

= 100(jAD IAD j + jDD IDD j)
hotelBonus = HP if the client is in the TT, 0 otherwise.
funBonus = sum of EVs for assigned entertainment tickets.
A TAC agent's score is the sum of its clients' utilities in the optimal allocation of
its goods (computed by the TAC server) minus its expenditures. The client preferences,
allocations, and resulting utilities from a sample game are shown in Tables 3 and 4.
Client IAD IDD HP AW AP MU
1 Day 2 Day 5 73 175 34 24
2 Day 1 Day 3 125 113 124 57
3 Day 4 Day 5 73 157 12 177
4 Day 1 Day 2 102 50 67 49
5 Day 1 Day 3 75 12 135 110
6 Day 2 Day 4 86 197 8 59
7 Day 1 Day 5 90 56 197 162
8 Day 1 Day 3 50 79 92 136
Table 3:
's client preferences from an actual game. AW, AP, and MU are EVs
for alligator wrestling, amusement park, and museum respectively.




travelPenalty

ATTac-2001

Client AD DD Hotel
Ent'ment
Utility
1 Day 2 Day 5 SS
AW4
1175
2 Day 1 Day 2 TT
AW1
1138
3 Day 3 Day 5 SS
MU3, AW4
1234
4 Day 1 Day 2 TT
None
1102
5 Day 1 Day 2 TT
AP1
1110
6 Day 2 Day 3 TT
AW2
1183
7 Day 1 Day 5 SS AF2, AW3, MU4 1415
8 Day 1 Day 2 TT
MU1
1086
Table 4:
's client allocations and utilities from the same actual game as that in
Table 3. Client 1's \4" under \Ent'ment" indicates on day 4.
ATTac-2001

The rules of TAC-01 are largely identical to those of TAC-00, with three important
exceptions:
1. In TAC-00, ight prices did not tend to increase;
2. In TAC-00, hotel auctions usually all closed at the end of the game;
3. In TAC-00, entertainment tickets were distributed uniformly to all agents
While relatively minor on the surface, these changes significantly enriched the strategic
complexity of the game. Stone and Greenwald (2003) detail agent strategies from TAC-00.
219

fiStone, Schapire, Littman, Csirik, & McAllester

TAC-01 was organized as a series of four competition phases, culminating with the
semifinals and finals on October 14, 2001 at the EC-01 conference in Tampa, Florida. First,
the qualifying round, consisting of about 270 games per agent, served to select the 16
agents that would participate in the semifinals. Second, the seeding round, consisting of
about 315 games per agent, was used to divide these agents into two groups of eight. After
the semifinals, on the morning of the 14th consisting of 11 games in each group, four teams
from each group were selected to compete in the finals during that same afternoon. The
finals are summarized in Section 6.
TAC is not designed to be fully realistic in the sense that an agent from TAC is not
immediately deployable in the real world. For one thing, it is unrealistic to assume that an
agent would have complete, reliable access to all clients' utility functions (or even that the
client would!); typically, some sort of preference elicitation procedure would be required (e.g.
Boutilier, 2002). For another, the auction mechanisms are somewhat contrived for the
purposes of creating an interesting, yet relatively simple game. However, each mechanism
is representative of a class of auctions that is used in the real world. And it is not dicult
to imagine a future in which agents do need to bid in decentralized, related, yet varying
auctions for similarly complex packages of goods.
4. Hotel Price Prediction

As discussed earlier, a central part of our strategy depends on the ability to predict prices,
particularly hotel prices, at various points in the game. To do this as accurately as possible,
we used machine-learning techniques that would examine the hotel prices actually paid
in previous games to predict prices in future games. This section discusses this part of
our strategy in detail, including a new boosting-based algorithm for conditional density
estimation.
There is bound to be considerable uncertainty regarding hotel prices since these depend
on many unknown factors, such as the time at which the hotel room will close, who the
other agents are, what kind of clients have been assigned to each agent, etc. Thus, exactly
predicting the price of a hotel room is hopeless. Instead, we regard the closing price as a
random variable that we need to estimate, conditional on our current state of knowledge
(i.e., number of minutes remaining in the game, ask price of each hotel, ight prices, etc.).
We might then attempt to predict this variable's conditional expected value. However,
our strategy requires that we not only predict expected value, but that we also be able to
estimate the entire conditional distribution so that we can sample hotel prices.
To set this up as a learning problem, we gathered a set of training examples from
previously played games. We defined a set of features for describing each example that
together are meant to comprise a snap-shot of all the relevant information available at the
time each prediction is made. All of the features we used are real valued; a couple of the
features can have a special value ? indicating \value unknown." We used the following
basic features:
 The number of minutes remaining in the game.
 The price of each hotel room, i.e., the current ask price for rooms that have not closed
or the actual selling price for rooms that have closed.
220

fiDecision-Theoretic Bidding with Learned Density Models

 The closing time of each hotel room. Note that this feature is defined even for rooms

that have not yet closed, as explained below.
 The prices of each of the ights.
To this basic list, we added a number of redundant variations, which we thought might help
the learning algorithm:
 The closing price of hotel rooms that have closed (or ? if the room has not yet closed).
 The current ask price of hotel rooms that have not closed (or ? if the room has already
closed).
 The closing time of each hotel room minus the closing time of the room whose price
we are trying to predict.
 The number of minutes from the current time until each hotel room closes.
During the seeding rounds, it was impossible to know during play who our opponents
were, although this information was available at the end of each game, and therefore during
training. During the semifinals and finals, we did know the identities of all our competitors.
Therefore, in preparation for the semifinals and finals, we added the following features:
 The number of players playing (ordinarily eight, but sometimes fewer, for instance if
one or more players crashed).
 A bit for each player indicating whether or not that player participated in this game.
We trained specialized predictors for predicting the price of each type of hotel room.
In other words, one predictor was specialized for predicting only the price of TT on day
1, another for predicting SS on day 2, etc. This would seem to require eight separate
predictors. However, the tournament game is naturally symmetric about its middle in the
sense that we can create an equivalent game by exchanging the hotel rooms on days 1 and
2 with those on days 4 and 3 (respectively), and by exchanging the inbound ights on
days 1, 2, 3 and 4 with the outbound ights on days 5, 4, 3 and 2 (respectively). Thus,
with appropriate transformations, the outer days (1 and 4) can be treated equivalently, and
likewise for the inner days (2 and 3), reducing the number of specialized predictors by half.
We also created specialized predictors for predicting in the first minute after ight prices
had been quoted but prior to receiving any hotel price information. Thus, a total of eight
specialized predictors were built (for each combination of TT versus SS, inner versus outer
day, and first minute versus not first minute).
We trained our predictors to predict not the actual closing price of each room per se,
but rather how much the price would increase, i.e., the difference between the closing price
and the current price. We thought that this might be an easier quantity to predict, and,
because our predictor never outputs a negative number when trained on nonnegative data,
this approach also ensures that we never predict a closing price below the current bid.
From each of the previously played games, we were able to extract many examples.
Specifically, for each minute of the game and for each room that had not yet closed, we
221

fiStone, Schapire, Littman, Csirik, & McAllester

extracted the values of all of the features described above at that moment in the game, plus
the actual closing price of the room (which we are trying to predict).
Note that during training, there is no problem extracting the closing times of all of the
rooms. During the actual play of a game, we do not know the closing times of rooms that
have not yet closed. However, we do know the exact probability distribution for closing
times of all of the rooms that have not yet closed. Therefore, to sample a vector of hotel
prices, we can first sample according to this distribution over closing times, and then use
our predictor to sample hotel prices using these sampled closing times.
4.1 The Learning Algorithm

Having described how we set up the learning problem, we are now ready to describe the
learning algorithm that we used. Briey, we solved this learning problem by first reducing to
a multiclass, multi-label classification problem (or alternatively a multiple logistic regression
problem), and then applying boosting techniques developed by Schapire and Singer (1999,
2000) combined with a modification of boosting algorithms for logistic regression proposed
by Collins, Schapire and Singer (2002). The result is a new machine-learning algorithm for
solving conditional density estimation problems, described in detail in the remainder of this
section. Table 5 shows pseudo-code for the entire algorithm.
Abstractly, we are given pairs (x1 ; y1 ); : : : ; (x ; y ) where each x belongs to a space X
and each y is in R . In our case, the x 's are the auction-specific feature vectors described
above; for some n, X  (R [f?g) . Each target quantity y is the difference between closing
price and current price. Given a new x, our goal is to estimate the conditional distribution
of y given x.
We proceed with the working assumption that all training and test examples (x; y) are
i.i.d. (i.e, drawn independently from identical distributions). Although this assumption is
false in our case (since the agents, including ours, are changing over time), it seems like a
reasonable approximation that greatly reduces the diculty of the learning task.
Our first step is to reduce the estimation problem to a classification problem by breaking
the range of the y 's into bins:
[b0; b1 ); [b1 ; b2 ); : : : ; [b ; b +1]
for some breakpoints b0 < b1 <    < b  b +1 where for our problem, we chose k = 50.7
The endpoints b0 and b +1 are chosen to be the smallest and largest y values observed
during training. We choose the remaining breakpoints b1; : : : ; b so that roughly an equal
number of training labels y fall into each bin. (More technically, breakpoints are chosen so
that the entropy of the distribution of bin frequencies is maximized.)
For each of the breakpoints b (j = 1; : : : ; k), our learning algorithm attempts to estimate
the probability that a new y (given x) will be at least b . Given such estimates p for each
b , we can then estimate the probability that y is in the bin [b ; b +1 ) by p +1 p (and
we can then use a constant density within each bin). We thus have reduced the problem
to one of estimating multiple conditional Bernoulli variables corresponding to the event
m

i

m

i

i

n

i

i

k

k

k

k

i

k

k

i

j

j

j

j

j

j

j

j

7. We did not experiment with varying k, but expect that the algorithm is not sensitive to it for suciently
large values of k.

222

fiDecision-Theoretic Bidding with Learned Density Models
m ; ym ) where xi 2 X , yi 2 R
positive integers k and T

Input: (x1 ; y1 ); : : : ; (x

b0 < b1 <    < bk+1 where
= mini yi
bk+1 = maxi yi
Pk
b1 ; : : : ; bk chosen to minimize
q ln qj where q0 ; : : : ; qk are fraction of yi 's in
j =0 j
[b0 ; b1 ); [b1 ; b2 ); : : : ; [bk ; bk+1 ] (using dynamic programing)

Compute breakpoints:





b0

Boosting:



for t = 1; : : : T :
1
compute weights Wt (i; j ) =
1 + esj (yi )ft (xi ;j )
where sj (y ) is as in Eq. (10)
use Wt to obtain base function ht : X  f1; : : : ; k g ! R minimizing
m
k
X
X
sj (yi )ht (xi ;j )
Wt (i; j )e
over all decision rules ht considered. The decision rules
i=1 j =1

can take any form. In our work, we use \decision stumps," or simple thresholds on one
of the features.

Output sampling rule:




let f =

X
T

ht
t=1

let f 0 = (f + f )=2 where
( ) = maxff (x; j 0 ) : j  j 0  k g
f (x; j )
= minff (x; j 0 ) : 1  j 0  j g
f x; j



to sample, given x 2 X
1
let pj =
1 + e f (x;j )
let p0 = 1; pk+1 = 0
choose j 2 f0; : : : ; k g randomly with probability pj
choose y uniformly at random from [bj ; bj +1 ]
output y
0

pj +1

Table 5: The boosting-based algorithm for conditional density estimation.
y

 b , and for this, we use a logistic regression algorithm based on boosting techniques as

described by Collins et al. (2002).
Our learning algorithm constructs a real-valued function f : X  f1; : : : ; kg ! R with
the interpretation that
1
(9)
1 + exp( f (x; j ))
j

is our estimate of the probability that y  b , given x. The negative log likelihood of the
conditional Bernoulli variable corresponding to y being above or below b is then
j

i



ln 1 + e

( ) (

sj yi f xi ;j

223

j

)



fiStone, Schapire, Littman, Csirik, & McAllester

where

(

( ) = +11 ifif yy < bb .
(10)
We attempt to minimize this quantity for all training examples (x ; y ) and all breakpoints b . Specifically, we try to find a function f minimizing
sj y

j
j

i

i

j

XX
m

i

k

=1 j =1



ln 1 + e

( ) (

sj yi f xi ;j



) :

We use a boosting-like algorithm described by Collins et al. (2002) for minimizing objective
functions of exactly this form. Specifically, we build the function f in rounds. On each
round t, we add a new base function h : X  f1; : : : ; kg ! R . Let
t

ft

=

X1
t

0 =1

ht0

t

be the accumulating sum. Following Collins, Schapire and Singer, to construct each h , we
first let
1
W (i; j ) =
(
1+e ) ( )
be a set of weights on example-breakpoint pairs. We then choose h to minimize
t

t

sj yi ft xi ;j

t

XX
m

( )

k

=1 j =1

Wt i; j e

( ) (

sj yi ht xi ;j

(11)

)

i

over some space of \simple" base functions h . For this work, we considered all \decision
stumps" h of the form
8
>
< A if (x)  
h(x; j ) =
B if (x) < 
>
: C if (x) =?
where () is one of the features described above, and , A , B and C are all real numbers.
In other words, such an h simply compares one feature  to a threshold  and returns a
vector of numbers h(x; ) that depends only on whether (x) is unknown (?), or above
or below . Schapire and Singer (2000) show how to eciently search for the best such
h over all possible choices of ,  , A , B and C . (We also employed their technique for
\smoothing" A , B and C .)
When computed by this sort of iterative procedure, Collins et al. (2002) prove the
asymptotic convergence of f to the minimum of the objective function in Equation (11)
over all linear combinations of the base functions. For this problem, we fixed the number
of rounds to T = 300. Let f = f +1 be the final predictor.
As noted above, given a new feature vector x, we compute p as in Equation (9) to be
our estimate for the probability that y  b , and we let p0 = 1 and p +1 = 0. For this to
make sense, we need p1  p2      p , or equivalently, f (x; 1)  f (x; 2)      f (x; k), a
condition that may not hold for the learned function f . To force this condition, we replace
f by a reasonable (albeit heuristic) approximation f 0 that is nonincreasing in j , namely,
t

j

j

j

j

j

j

j

j

j

j

j

j

t

T

j

j

k

224

k

fiDecision-Theoretic Bidding with Learned Density Models

= (f + f )=2 where f (respectively, f ) is the pointwise minimum (respectively, maximum)
of all nonincreasing functions g that everywhere upper bound f (respectively, lower bound
f ).
With this modified function f 0, we can compute modified probabilities p . To sample
a single point according to the estimated distribution on R associated with f 0, we choose
bin [b ; b +1) with probability p p +1, and then select a point from this bin uniformly at
random. Expected value according to this distribution is easily computed as


X
b +1 + b
:
(p p +1)
2
=0
Although we present results using this algorithm in the trading agent context, we did
not test its performance on more general learning problems, nor did we compare to other
methods for conditional density estimation, such as those studied by Stone (1994). This
clearly should be an area for future research.
f0

j

j

j

j

j

k

j

j

j

j

j

5. ATTac-2001

Having described hotel price prediction in detail, we now present the remaining details of
's algorithm. We begin with a brief description of the goods allocator, which
is used as a subroutine throughout the algorithm. We then present the algorithm in a
top-down fashion.
ATTac-2001

5.1 Starting Point

A core subproblem for TAC agents is the allocation problem: finding the most profitable
allocation of goods to clients, G, given a set of owned goods and prices for all other goods.
The allocation problem corresponds to finding opt(H; i; Y ) in Equation 3. We denote the
value of G (i.e., the score one would attain with G) as v(G ). The general allocation
problem is NP-complete, as it is equivalent to the set-packing problem (Garey & Johnson,
1979). However it can be solved tractably in TAC via integer linear programming (Stone
et al., 2001).
The solution to the integer linear program is a value-maximizing allocation of owned
resources to clients along with a list of resources that need to be purchased. Using the linear
programming package \LPsolve",
is usually able to find the globally optimal
solution in under 0.01 seconds on a 650 MHz Pentium II. However, since integer linear
programming is an NP-complete problem, some inputs can lead to a great deal of search
over the integrality constraints, and therefore significantly longer solution times. When only
v (G ) is needed (as opposed to G itself), the upper bound produced by LPsolve prior to
the search over the integrality constraints, known as the LP relaxation, can be used as an
estimate. The LP relaxation can always be generated very quickly.
Note that this is not by any means the only possible formulation of the allocation
problem. Greenwald and Boyan (2001) studied a fast, heuristic search variant and found
that it performed extremely well on a collection of large, random allocation problems. Stone
et al. (2001) used a randomized greedy strategy as a fallback for the cases in which the linear
program took too long to solve.
ATTac-2001

225

fiStone, Schapire, Littman, Csirik, & McAllester
5.2 Overview

Table 6 shows a high-level overview of
the remainder of this section.

. The italicized portions are described in

ATTac-2001

When the first ight quotes are posted:
 Compute G with current holdings and expected prices
 Buy the ights in G for which expected cost of postponing commitment exceeds the expected
benefit of postponing commitment

Starting 1 minute before each hotel close:
 Compute G with current holdings and expected prices
 Buy the ights in G for which expected cost of postponing commitment exceeds expected



benefit of postponing commitment (30 seconds)
Bid hotel room expected marginal values given holdings, new ights, and expected hotel
purchases (30 seconds)

Last minute: Buy remaining ights as needed by G
In parallel (continuously): Buy/sell entertainment tickets based on their expected values

Table 6:

's high-level algorithm. The italicized portions are described in the
remainder of this section.
ATTac-2001

5.3 Cost of Additional Rooms

Our hotel price predictor described in Section 4 assumes that
's bids do not affect
the ultimate closing price (Assumption 3 from Section 2). This assumption holds in a large
economy. However in TAC, each hotel auction involved 8 agents competing for 16 hotel
rooms. Therefore, the actions of each agent had an appreciable effect on the clearing price:
the more hotel rooms an agent attempted to purchase, the higher the clearing price would
be, all other things being equal. This effect needed to be taken into account when solving
the basic allocation problem.
The simplified model used by
assumed that the nth highest bid in a hotel
auction was roughly proportional to c (over the appropriate range of n) for some c  1.
Thus, if the predictor gave a price of p,
only used this for purchasing two hotel
rooms (the \fair" share of a single agent of the 16 rooms), and adjusted prices for other
quantities of rooms by using c.
For example,
would consider the cost of obtaining 4 rooms to be 4pc2 . One or
two rooms each cost p, but 3 each cost pc, 4 each cost pc2 , 5 each cost pc3, etc. So in total, 2
rooms cost 2p, while 4 cost 4pc2 . The reasoning behind this procedure is that if
buys two rooms | its fair share given that there are 16 rooms and 8 agents, then the 16th
highest bid (
's 2 bids in addition to 14 others) sets the price. But if
bids on an additional unit, the previous 15th highest bid becomes the price-setting bid: the
price for all rooms sold goes up from p to pc.
The constant c was calculated from the data of several hundred games during the seeding
round. In each hotel auction, the ratio of the 14th and 18th highest bids (reecting the
ATTac-2001

ATTac-2001
n

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

226

fiDecision-Theoretic Bidding with Learned Density Models

most relevant range of n) was taken as an estimate of c4, and the (geometric) mean of the
resulting estimates was taken to obtain c = 1:35.
The LP allocator takes these price estimates into account when computing G by assigning higher costs to larger purchase volumes, thus tending to spread out
's
demand over the different hotel auctions.
In
, a few heuristics were applied to the above procedure to improve stability
and to avoid pathological behavior: prices below $1 were replaced by $1 in estimating c;
c = 1 was used for purchasing fewer than two hotel rooms; hotel rooms were divided into
early closing and late closing (and cheap and expensive) ones, and the c values from the
corresponding subsets of auctions of the seeding rounds were used in each case.
ATTac-2001

ATTac-2001

5.4 Hotel Expected Marginal Values

Using the hotel price prediction module as described in Section 4, coupled with a model of
its own effect on the economy,
is equipped to determine its bids for hotel rooms.
Every minute, for each hotel auction that is still open,
assumes that auction
will close next and computes the marginal value of that hotel room given the predicted
closing prices of the other rooms. If the auction does not close next, then it assumes
that it will have a chance to revise its bids. Since these predicted prices are represented
as distributions of possible future prices,
samples from these distributions and
averages the marginal values to obtain an expected marginal value. Using the full minute
between closing times for computation (or 30 seconds if there are still ights to consider
too),
divides the available time among the different open hotel auctions and
generates as many price samples as possible for each hotel room. In the end,
bids the expected marginal values for each of the rooms.
The algorithm is described precisely and with explanation in Table 7.
One additional complication regarding hotel auctions is that, contrary to one of our
assumptions in Section 2.2 (Assumption 4), bids are not fully retractable: they can only
be changed to $1 above the current ask price. In the case that there are current active
bids for goods that
no longer wants that are less than $1 above the current ask
price, it may be advantageous to refrain from changing the bid in the hopes that the ask
price will surpass them: that is, the current bid may have a higher expected value than
the best possible new bid. To address this issue,
samples from the learned price
distribution to find the average expected values of the current and potential bids, and only
enters a new bid in the case that the potential bid is better.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

5.5 Expected Cost/Benefit of Postponing Commitment

makes ight bidding decisions based on a cost-benefit analysis: in particular,
computes the incremental cost of postponing bidding for a particular ight versus
the value of delaying commitment. In this section, we describe the determination of the
cost of postponing the bid.
Due to diculties that compounded with more sophisticated approaches,
used the following very simple model for estimating the price of a ight ticket at a given
future time. It is evident from the formulation that|given y|the expected price increase
from time 0 to time t was very nearly of the form M t2 for some M . It was also clear that
ATTac-2001

ATTac-2001

ATTac-2001

227

fiStone, Schapire, Littman, Csirik, & McAllester



For each hotel (in order of increasing expected price):




The value of the ith copy of the room is the mean of Vi



Repeat until time bound
1. Generate a random hotel closing order (only over open hotels)
2. Sample closing prices from predicted hotel price distributions
3. Given these closing prices, compute V0 ; V1 ; : : : Vn

Vi  v (G ) if owning i of the hotel
Estimate v (G ) with LP relaxation
Assume that no additional hotel rooms of this type can be bought
For other hotels, assume outstanding bids above sampled price are already owned
(i.e., they cannot be withdrawn).
Note that V0  V1  : : :  Vn : the values are monotonically increasing since having
more goods cannot be worse in terms of possible allocations.
Vi

1

over all the samples.

Note further that V1 V0  V2 V1 : : :  Vn Vn 1 : the value differences are monotonically
decreasing since each additional room will be assigned to the client who can derive the most
value from it.
Bid for one room at the value of the ith copy of the room for all i such that the value is
at least as much as the current price. Due to the monotonicity noted in the step above, no
matter what the closing price, the desired number of rooms at that price will be purchased.

Table 7: The algorithm for generating bids for hotel rooms.
as long as the price did not hit the artificial boundaries at $150 and $800, the constant M
must depend linearly on y 10. This linear dependence coecient was then estimated from
several hundred ight price evolutions during the qualifying round. Thus, for this constant
m, the expected price increase from time t to time T was m(T 2 t2 )(y 10). When a
price prediction was needed, this formula was first used for the first and most recent actual
price observations to obtain a guess for y, and then this y was used in the formula again to
estimate the future price. No change was predicted if the formula yielded a price decrease.
This approach suffers from systemic biases of various kinds (mainly due to the fact that
the variance of price changes gets relatively smaller over longer periods of time), but was
thought to be accurate enough for its use, which was to predict whether or not the ticket
can be expected to get significantly more expensive over the next few minutes.
In practice, during TAC-01,
started with the ight-lookahead parameter set
to 3 (i.e., cost of postponing is the average of the predicted ight costs 1, 2, and 3 minutes
in the future). However, this parameter was changed to 2 by the end of the finals in order
to cause
to delay its ight commitments further.
ATTac-2001

ATTac-2001

5.5.1 Expected Benefit of Postponing Commitment

Fundamentally, the benefit of postponing commitments to ights is that additional information about the eventual hotel prices becomes known. Thus, the benefit of postponing
commitment is computed by sampling possible future price vectors and determining, on
average, how much better the agent could do if it bought a different ight instead of the
one in question. If it is optimal to buy the ight in all future scenarios, then there is no
value in delaying the commitment and the ight is purchased immediately. However, if
228

fiDecision-Theoretic Bidding with Learned Density Models

there are many scenarios in which the ight is not the best one to get, the purchase is more
likely to be delayed.
The algorithm for determining the benefit of postponing commitment is similar to that
for determining the marginal value of hotel rooms. It is detailed, with explanation, in
Table 8.
 Assume we're considering buying n ights of a given type
 Repeat until time bound

1. Generate a random hotel closing order (open hotels)
2. Sample closing prices from predicted price distributions (open hotels)
3. Given these closing prices compute V0; V1 ; : : : V
V = v (G ) if forced to buy i of the ight
Estimate v(G) with LP relaxation
Assume more ights can be bought at the current price
Note that V0  V1  : : :  V since it is never worse to retain extra exibility.
 The value of waiting to buy copy i is the mean of V V 1 over all the samples. If
all price samples lead to the conclusion that the ith ight should be bought, then
V = V 1 and there is no benefit to postponing commitment.
Table 8: The algorithm for generating value of postponing ight commitments.
n

i

n

i

i

i

i

5.6 Entertainment Expected Values

The core of
's entertainment-ticket-bidding strategy is again a calculation of the
expected marginal values of each ticket. For each ticket,
computes the expected
value of having one more and one fewer of the ticket. These calculations give bounds on the
bid and ask prices it is willing to post. The actual bid and ask prices are a linear function of
time remaining in the game:
settles for a smaller and smaller profit from ticket
transactions as the game goes on. Details of the functions of bid and ask price as a function
of game time and ticket value remained unchanged from
(Stone et al., 2001).
Details of the entertainment-ticket expected marginal utility calculations are given in
Table 9.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2000

6. Results

This section presents empirical results demonstrating the effectiveness of the
strategy. First, we summarize its performance in the 2001 and 2002 Trading Agent Competitions (TACs). These summaries provide evidence of the strategy's overall effectiveness,
but, due to the small number of games in the competitions, are anecdotal rather than scientifically conclusive. We then present controlled experiments that provide more conclusive
evidence of the utility of our decision theoretic and learning approaches embedded within
.

ATTac-2001

ATTac-2001

229

fiStone, Schapire, Littman, Csirik, & McAllester

 Assume n of a given ticket type are currently owned
 Repeat until time bound

1. Generate a random hotel closing order (open hotels)
2. Sample closing prices from predicted price distributions (open hotels)
3. Given these closing prices compute V 1; V ; V +1
V = v (G ) if own i of the ticket
Estimate v(G) with LP relaxation
Assume no other tickets can be bought or sold
Note that V 1  V  V +1 since it is never worse to own extra tickets.
 The value of buying a ticket is the mean of V +1 V over all the samples; the value
of selling is the mean of V V 1.
 Since tickets are considered sequentially, if the determined buy or sell bid leads to a
price that would clear according to the current quotes, assume the transaction goes
through before computing the values of buying and selling other ticket types.
Table 9: The algorithm for generating value of entertainment tickets.
n

n

n

i

n

n

n

n

n

n

n

6.1 TAC-01 Competition

Of the 19 teams that entered the qualifying round,
was one of eight agents
to make it to the finals on the afternoon of October 14th, 2001. The finals consisted of
24 games among the same eight agents. Right from the beginning, it became clear that
(Fritschi & Dorer, 2002) was the team to beat in the finals. They jumped to an
early lead in the first two games, and by eight games into the round, they were more than
135 points per game ahead of their closest competitor (
He & Jennings,
2002). 16 games into the round, they were more than 250 points ahead of their two closest
competitors (
and
).
From that point,
, which was continually retraining its price predictors based
on recent games, began making a comeback. By the time the last game was to be played,
it was only an average of 22 points per game behind
. It thus needed to beat
by 514 points in the final game to overtake it, well within the margins observed
in individual game instances. As the game completed,
's score of 3979 was one
of the first scores to be posted by the server. The other agents' scores were reported one
by one, until only the
score was left. After agonizing seconds (at least for us),
the TAC server posted a final game score of 4626, resulting in a win for
.
After the competition, the TAC team at the University of Michigan conducted a regression analysis of the effects of the client profiles on agent scores. Using data from the seeding
rounds, it was determined that agents did better when their clients had:
1. fewer total preferred travel days;
2. higher total entertainment values; and
3. a higher ratio of outer days (1 and 4) to inner (2 and 3) in preferred trip intervals.
ATTac-2001

livingagents

SouthamptonTAC,

ATTac-2001

whitebear

ATTac-2001

livingagents

livingagents

ATTac-2001

livingagents

livingagents

230

fiDecision-Theoretic Bidding with Learned Density Models

Based on these significant measures, the games in the finals could be handicapped based
on each agents' aggregate client profiles. Doing so indicated that
' clients were
much easier to satisfy than those of
, giving
the highest handicapped
score. The final scores, as well as the handicapped scores, are shown in Table 10. Complete
results and aliations are available from http://tac.eecs.umich.edu.
Agent
Mean Handicapped score
3622
4154
3670
4094
3513
3931
3421
3909
3352
3812
3074
3766
3253
3679
2859
3338
Table 10: Scores during the finals. Each agent played 24 games. Southampton's score was
adversely affected by a game in which their agent crashed after buying many
ights but no hotels, leading to a loss of over 3000 points. Discarding that game
results in an average score of 3531.
livingagents

ATTac-2001

ATTac-2001

ATTac-2001
livingagents
whitebear
Urlaub01
Retsina

CaiserSose

SouthamptonTAC
TacsMan

6.2 TAC-02 Competition

A year after the TAC-01 competition,
was re-entered in the TAC-02 competition
using the models trained at the end of TAC-01. Specifically, the price predictors were left
unchanged throughout (no learning). The seeding round included 19 agents, each playing
440 games over the course of about 2 weeks.
was the top-scoring agent in this
round, as shown in Table 11. Scores in the seeding round were weighted so as to emphasize
later results over earlier results: scores on day n of the seeding round were given a weight
of n. This practice was designed to encourage experimentation early in the round. The
ocial ranking in the competitions was based on the mean score after ignoring each agent's
worst 10 results so as to allow for occasional program crashes and network problems.
On the one hand, it is striking that
was able to finish so strongly in a field
of agents that had presumably improved over the course of the year. On the other hand,
most agents were being tuned, for better and for worse, while
was consistent
throughout. In particular, we are told that
experimented with its approach
during the later days of the round, perhaps causing it to fall out of the lead (by weighted
score) in the end. During the 14-game semifinal heat,
, which was now restored
with its learning capability and retrained over the data from the 2002 seeding round, finished
6th out of 8 thereby failing to reach the finals.
There are a number of possible reasons for this sudden failure. One relatively mundane explanation is that the agent had to change computational environments between the
seeding rounds and the finals, and there may have been a bug or computational resource
constraint introduced. Another possibility is that due to the small number of games in
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

SouthamptonTAC

ATTac-2001

231

fiStone, Schapire, Littman, Csirik, & McAllester

Agent

Mean Weighted, dropped worst 10
3050
3131
3100
3129
2980
3118
3018
3091
2998
3055
2952
3000
2945
2966
2738
2855
Table 11: Top 8 scores during the seeding round of TAC-02. Each agent played 440 games,
with its worst 10 games ignored when computing the rankings.
ATTac-2001

SouthamptonTAC
UMBCTAC

livingagents
cuhk

Thalis

whitebear
RoxyBot

the semifinals,
simply got unlucky with respect to clients and the interaction
of opponent strategies. However, it is also plausible that the training data from the 2002
qualifying and seeding round data was less representative of the 2002 finals than the was the
training data from 2001; and/or that the competing agents improved significantly over the
seeding round while
remained unchanged. The TAC team at the University of
Michigan has done a study of the price predictors of several 2002 TAC agents that suggests
that the bug hypothesis is most plausible: the
predictor from 2001 outperforms
all other predictors from 2002 on the data from the 2002 semifinals and finals; and one other
agent that uses the 2002 data did produce good predictions based on that data (Wellman,
Reeves, Lochner, & Vorobeychik, 2003b).8
ATTac-2001

ATTac-2001

ATTac-2001

6.3 Controlled Experiments

's success in the TAC-01 competition demonstrates its effectiveness as a complete
system. However, since the competing agents differed along several dimensions, the competition results cannot isolate the successful approaches. In this section, we report controlled
experiments designed to test the ecacy of
's machine-learning approach to price
prediction.
ATTac-2001

ATTac-2001

6.3.1 Varying the Predictor

In the first set of experiments, we attempted to determine how the quality of
's
hotel price predictions affects its performance. To this end, we devised seven price prediction
schemes, varying considerably in sophistication and inspired by approaches taken by other
TAC competitors, and incorporated these schemes into our agent. We then played these
seven agents against one another repeatedly, with regular retraining as described below.
Following are the seven hotel prediction schemes that we used, in decreasing order of
sophistication:
ATTac-2001

8. Indeed, in the TAC-03 competition, ATTac-2001 was entered using the trained models from 2001, and it
won the competition, suggesting further that the failure in 2002 was due to a problem with the learned
models that were used during the finals in 2002.

232

fiDecision-Theoretic Bidding with Learned Density Models

: This is the \full-strength" agent based on boosting that was used during
the tournament. (The s denotes sampling.)

: This agent samples prices from the empirical distribution of prices from
previously played games, conditioned only on the closing time of the hotel room (a
subset of of the features used by
). In other words, it collects all historical
hotel prices and breaks them down by the time at which the hotel closed (as well as
room type, as usual). The price predictor then simply samples from the collection of
prices corresponding to the given closing time.

: This agent samples prices from the empirical distribution of prices from
previously played games, without regard to the closing time of the hotel room (but
still broken down by room type). It uses a subset of the features used by
.

,
,
: These agents predict in the same way as
their corresponding predictors above, but instead of returning a random sample from
the estimated distribution of hotel prices, they deterministically return the expected
value of the distribution. (The ev denotes expected value, as introduced in Section 2.1.)

: This agent uses a very simple predictor that always predicts that the hotel
room will close at its current price.
In every case, whenever the price predictor returns a price that is below the current price,
we replace it with the current price (since prices cannot go down).
In our experiments, we added as an eighth agent
, inspired by the
agent.
used
to predict closing prices, determined an optimal set of
purchases, and then placed bids for these goods at suciently high prices to ensure that
they would be purchased ($1001 for all hotel rooms, just as
did in TAC-01) right
after the first ight quotes. It then never revised these bids.
Each of these agents require training, i.e., data from previously played games. However,
we are faced with a sort of \chicken and egg" problem: to run the agents, we need to
first train the agents using data from games in which they were involved, but to get this
kind of data, we need to first run the agents. To get around this problem, we ran the
agents in phases. In Phase I, which consisted of 126 games, we used training data from
the seeding, semifinals and finals rounds of TAC-01. In Phase II, lasting 157 games, we
retrained the agents once every six hours using all of the data from the seeding, semifinals
and finals rounds as well as all of the games played in Phase II. Finally, in Phase III, lasting
622 games, we continued to retrain the agents once every six hours, but now using only
data from games played during Phases I and II, and not including data from the seeding,
semifinals and finals rounds.
Table 12 shows how the agents performed in each of these phases. Much of what we
observe in this table is consistent with our expectations. The more sophisticated boostingbased agents (
and
) clearly dominated the agents based on simpler
prediction schemes. Moreover, with continued training, these agents improved markedly
relative to
. We also see the performance of the simplest agent,
, which


ATTac-2001s

Cond'lMeans

ATTac-2001s

SimpleMeans

Cond'lMeans

ATTac-2001ev

Cond'lMeanev

SimpleMeanev

CurrentBid

EarlyBidder

EarlyBidder

livingagents

SimpleMeanev

livingagents

ATTac-2001s

ATTac-2001ev

EarlyBidder

CurrentBid

233

fiStone, Schapire, Littman, Csirik, & McAllester
Agent
ATTac-2001ev
ATTac-2001s
EarlyBidder
SimpleMeanev
SimpleMeans
Cond'lMeanev
Cond'lMeans
CurrentBid

Phase I
105:2  49:5
27:8  42:1
140:3  38:6
28:8  45:1
72:0  47:5
8:6  41:2
147:5  35:6
33:7  52:4

Relative Score
Phase II
131:6  47:7 (2)
86:1  44:7 (3)
152:8  43:4 (1)
53:9  40:1 (5)
71:6  42:8 (6)
3:5  37:5 (4)
91:4  41:9 (7)
157:1  54:8 (8)

(2)
(3)
(1)
(5)
(7)
(4)
(8)
(6)

Phase III
166:2  20:8
122:3  19:4
117:0  18:0
11:5  21:7
44:1  18:2
60:1  19:7
91:1  17:6
198:8  26:0

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Table 12: The average relative scores ( standard deviation) for eight agents in the three
phases of our controlled experiment in which the hotel prediction algorithm was
varied. The relative score of an agent is its score minus the average score of all
agents in that game. The agent's rank within each phase is shown in parentheses.
does not employ any kind of training, significantly decline relative to the other data-driven
agents.
On the other hand, there are some phenomena in this table that were very surprising
to us. Most surprising was the failure of sampling to help. Our strategy relies heavily
not only on estimating hotel prices, but also taking samples from the distribution of hotel
prices. Yet these results indicate that using expected hotel price, rather than price samples,
consistently performs better. We speculate that this may be because an insucient number
of samples are being used (due to computational limitations) so that the numbers derived
from these samples have too high a variance. Another possibility is that the method of using
samples to estimate scores consistently overestimates the expected score because it assumes
the agent can behave with perfect knowledge for each individual sample|a property of our
approximation scheme. Finally, as our algorithm uses sampling at several different points
(computing hotel expected values, deciding when to buy ights, pricing entertainment tickets, etc.), it is quite possible that sampling is beneficial for some decisions while detrimental
for others. For example, when directly comparing versions of the algorithm with sampling
used at only subsets of the decision points, the data suggests that sampling for the hotel
decisions is most beneficial, while sampling for the ights and entertainment tickets is neutral at best, and possibly detrimental. This result is not surprising given that the sampling
approach is motivated primarily by the task of bidding for hotels.
We were also surprised that
and
eventually performed worse
than the less sophisticated
and
. One possible explanation is that
the simpler model happens to give predictions that are just as good as the more complicated
model, perhaps because closing time is not terribly informative, or perhaps because the
adjustment to price based on current price is more significant. Other things being equal,
the simpler model has the advantage that its statistics are based on all of the price data,
regardless of closing time, whereas the conditional model makes each prediction based on
only an eighth of the data (since there are eight possible closing times, each equally likely).
In addition to agent performance, it is possible to measure the inaccuracy of the eventual
predictions, at least for the non-sampling agents. For these agents, we measured the root
Cond'lMeans

SimpleMeans

Cond'lMeanev

SimpleMeanev

234

fiDecision-Theoretic Bidding with Learned Density Models

mean squared error of the predictions made in Phase III. These were: 56.0 for
,
66.6 for
, 69.8 for
and 71.3 for
. Thus, we see that
the lower the error of the predictions (according to this measure), the higher the score
(correlation R = 0:88).
ATTac-2001ev

SimpleMeanev

6.3.2

ATTac-2001

vs.

CurrentBid

Cond'lMeanev

EarlyBidder

In a sense, the two agents that finished at the top of the standings in TAC-01 represented
opposite ends of a spectrum. The
agent uses a simple open-loop strategy, committing to a set of desired goods right at the beginning of the game, while
uses
a closed-loop, adaptive strategy.
The open-loop strategy relies on the other agents to stabilize the economy and create
consistent final prices. In particular, if all eight agents are open loop and place very high
bids for the goods they want, many of the prices will skyrocket, evaporating any potential
profit. Thus, a set of open-loop agents would tend to get negative scores|the open-loop
strategy is a parasite, in a manner of speaking. Table 13 shows the results of running 27
games with 7 copies of the open-loop
and one of
. Although motivated
by
, in actuality it is identical to
except that it uses
and
it places all of its ight and hotel bids immediately after the first ight quotes. It bids
only for the hotels that appear in G at that time. All hotel bids are for $1001. In the
experiments, one copy of
is included for comparison. The price predictors are
all from Phase I in the preceding experiments.
's high bidding strategy backfires
and it ends up overpaying significantly for its goods. As our experiments above indicate,
may improve even further if it is allowed to train on the games of the on-going
experiment as well.
livingagents

ATTac-2001

EarlyBidder

livingagents

ATTac-2001

ATTac-2001

SimpleMeanev

ATTac-2001s

EarlyBidder

ATTac-2001

Agent
ATTac-2001

(7)

EarlyBidder

Score
2431  464
4880  337

Utility
8909  264
9870  34

Table 13: The results of running
against 7 copies of
over the course
of 27 games.
achieves high utility, but overpays significantly, resulting
in low scores.
ATTac-2001

EarlyBidder

EarlyBidder

The open-loop strategy has the advantage of buying a minimal set of goods. That is, it
never buys more than it can use. On the other hand, it is susceptible to unexpected prices
in that it can get stuck paying arbitrarily high prices for the hotel rooms it has decided to
buy.
Notice in Table 13 that the average utility of the
's clients is significantly
greater than that of
's clients. Thus, the difference in score is accounted for
entirely by the cost of the goods.
ends up paying exorbitant prices, while
generally steers clear of the more expensive hotels. Its clients' utility suffers, but the
cost-savings are well worth it.
Compared to the open-loop strategy,
's strategy is relatively stable against
itself. Its main drawback is that as it changes its decision about what goods it wants and as
EarlyBidder

ATTac-2001

EarlyBidder

ATTac-

2001

ATTac-2001

235

fiStone, Schapire, Littman, Csirik, & McAllester

it may also buy goods to hedge against possible price changes, it can end up getting stuck
paying for some goods that are ultimately useless to any of its clients.
Table 14 shows the results of 7 copies of
playing against each other and
one copy of the
. Again, training is from the seeding round and finals of TAC01: the agents don't adapt during the experiment. Included in this experiment are three
variants of
, each with a different ight-lookahead parameter (from the section
on \cost of postponing ight commitments"). There were three copies each of the agents
with ight-lookahead set to 2 and 3 (
(2) and
(3), respectively), and
one
agent with ight-lookahead set to 4 (
(4)).
ATTac-2001

EarlyBidder

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

Agent
EarlyBidder

(2)
(3)
ATTac-2001(4)
ATTac-2001
ATTac-2001

Score
2869  69
2614  38
2570  39
2494  68

Utility
10079  55
9671  32
9641  32
9613  55

Table 14: The results of running the
against 7 copies of
course of 197 games. The three different versions of
different ight-lookaheads.
EarlyBidder

over the
had slightly

ATTac-2001

ATTac-2001

From the results in Table 14 it is clear that
does better when committing
to its ight purchases later in the game (
(2) as opposed to
(4)). In
comparison with Table 13, the economy represented here does significantly better overall.
That is, having many copies of
in the economy does not cause them to suffer.
However, in this economy,
is able to invade. It gets a significantly higher utility
for its clients and only pays slightly more than the
agents (as computed by
utility minus score).9
The results in this section suggest that the variance of the closing prices is the largest
determining factor between the effectiveness of the two strategies (assuming nobody else
is using the open-loop strategy). We speculate that with large price variances, the closedloop strategy (
) should do better, but with small price variances, the open-loop
strategy could do better.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

EarlyBidder

ATTac-2001

ATTac-2001

7. Discussion

The open-loop and closed-loop strategies of the previous section differ in their handling of
price uctuation. A fundamental way of taking price uctuation into account is to place
\safe bids." A very high bid exposes an agent to the danger of buying something at a
ridiculously high price. If prices are in fact stable then high bids are safe. But if prices
uctuate, then high bids, such as the bids of the stable-price strategy, are risky. In TAC,
hotel rooms are sold in a Vickrey-style nth price action. There is a separate auction for
each day of each hotel and these auctions are done sequentially. Although the order of the
auctions is randomized, and not known to the agent, when placing bids in one of these
9. We suspect that were the agents allowed to retrain over the course of the experiments, ATTac-2001
would end up improving, as we saw in Phase III of the previous set of experiments. Were this to occur,
it is possible that EarlyBidder would no longer be able to invade.

236

fiDecision-Theoretic Bidding with Learned Density Models

auctions the agent assumes that auction will close next. We assumed in the design of our
agent that our bids in one auction do not affect prices in other auctions. This assumption
is not strictly true, but in a large economy one expects that the bids of a single individual
have a limited effect on prices. Furthermore, the price most affected by a bid is the price
of the item being bid on; the effect on other auctions seems less direct and perhaps more
limited. Assuming bids in one auction do not affect prices in another, the optimal bidding
strategy is the standard strategy for a Vickrey auction|the bid for an item should be equal
to its utility to the bidder. So, to place a Vickrey-optimal bid, one must be able to estimate
the utility of an item. The utility of owning an item is simply the expected final score
assuming one owns the item minus the expected final score assuming one does not own the
item. So, the problem of computing a Vickrey-optimal bid can be reduced to the problem
of predicting final scores for two alternative game situations. We use two score prediction
procedures, which we call the stable-price score predictor (corresponding to Equation 5)
and the unstable-price score predictor (Equation 4).
The Stable-Price Score Predictor. The stable-price score predictor first estimates
the expected prices in the rest of the game using whatever information is available in the
given game situation. It then computes the value achieved by optimal purchases under the
estimated prices. In an economy with stable prices, this estimate will be quite accurate|
if we make the optimal purchases for the expected price then, if the prices are near our
estimates, our performance will also be near the estimated value.
The Unstable-Price Score Predictor. Stable-price score prediction does not take
into account the ability of the agent to react to changes in price as the game progresses. Suppose a given room is often cheap but is sometimes expensive. If the agent can first determine
the price of the room, and then plan for that price, the agent will do better than guessing
the price ahead of time and sticking to the purchases dictated by that price. The unstable
price predictor uses a model of the distribution of possible prices. It repeatedly samples
prices from this distribution, computes the stable-price score prediction under the sampled
price, and then takes the average of these stable-price scores over the various price samples.
This score prediction algorithm is similar to the algorithm used in Ginsberg's (2001) quite
successful computer bridge program where the score is predicted by sampling the possible
hands of the opponent and, for each sample, computing the score of optimal play in the case
where all players have complete information (double dummy play). While this approach
has a simple intuitive motivation, it is clearly imperfect. The unstable-price score predictor
assumes both that future decisions are made in the presence of complete price information,
and that the agent is free to change existing bids in auctions that have not yet closed. Both
of these assumptions are only approximately true at best. Ways of compensating for the
imperfections in score prediction were described in Section 5.
Buy Now or Decide Later. The trading agent must decide what airline tickets to buy
and when to buy them. In deciding whether to buy an airline ticket, the agent can compare
the predicted score in the situation where it owns the airline ticket with the predicted score
in the situation where it does not own the airline ticket but may buy it later. Airline tickets
tend to increase in price, so if the agent knows that a certain ticket is needed it should buy
it as soon as possible. But whether or not a given ticket is desirable may depend on the
price of hotel rooms, which may become clearer as the game progresses. If airline tickets
did not increase in price, as was the case in TAC-00, then they should be bought at the
237

fiStone, Schapire, Littman, Csirik, & McAllester

last possible moment (Stone et al., 2001). To determine whether an airline ticket should be
bought now or not, one can compare the predicted score in the situation where one has just
bought the ticket at its current price with the predicted score in the situation where the
price of the ticket is somewhat higher but has not yet been bought. It is interesting to note
that if one uses the stable-price score predictor for both of these predictions, and the ticket
is purchased in the optimal allocation under the current price estimate, then the predicted
score for buying the ticket now will always be higher|increasing the price of the ticket can
only reduce the score. However, the unstable-price score predictor can yield an advantage
for delaying the purchase. This advantage comes from the fact that buying the ticket may
be optimal under some prices but not optimal under others. If the ticket has not yet been
bought, then the score will be higher for those sampled prices where the ticket should not
be bought. This corresponds to the intuition that in certain cases the purchase should be
delayed until more information is available.
Our guiding principle in the design of the agent was, to the greatest extent possible, to
have the agent analytically calculate optimal actions. A key component of these calculations
is the score predictor, based either on a single estimated assignment of prices or on a model of
the probability distribution over assignments of prices. Both score predictors, though clearly
imperfect, seem useful. Of these two predictors, only the unstable-price predictor can be
used to quantitatively estimate the value of postponing a decision until more information is
available. The accuracy of price estimation is clearly of central importance. Future research
will undoubtedly focus on ways of improving both price modeling and score prediction based
on price modeling.
8. Related and Future Work

Although there has been a good deal of research on auction theory, especially from the
perspective of auction mechanisms (Klemperer, 1999), studies of autonomous bidding agents
and their interactions are relatively few and recent. TAC is one example. FM97.6 is
another auction test-bed, which is based on fishmarket auctions (Rodriguez-Aguilar, Martin,
Noriega, Garcia, & Sierra, 1998). Automatic bidding agents have also been created in this
domain (Gimenez-Funes, Godo, Rodriguez-Aguiolar, & Garcia-Calves, 1998). There have
been a number of studies of agents bidding for a single good in multiple auctions (Ito,
Fukuta, Shintani, & Sycara, 2000; Anthony, Hall, Dang, & Jennings, 2001; Preist, Bartolini,
& Phillips, 2001).
A notable auction-based competition that was held prior to TAC was the Santa Fe
Double Auction Tournament (Rust, Miller, & Palmer, 1992). This auction involved several
agents competing in a single continuous double auction similar to the TAC entertainment
ticket auctions. As analyzed by Tesauro and Das (2001), this tournament was won by a
parasite strategy that, like
as described in Section 6.3, relied on other agents to
find a stable price and then took advantage of it to gain an advantage. In that case, the
advantage was gained by waiting until the last minute to bid, a strategy commonly known
as sniping.
TAC-01 was the second iteration of the Trading Agent Competition. The rules of TAC01 are largely identical to those of TAC-00, with three important exceptions:
1. In TAC-00, ight prices did not tend to increase;
livingagents

238

fiDecision-Theoretic Bidding with Learned Density Models

2. In TAC-00, hotel auctions usually all closed at the end of the game;
3. In TAC-00, entertainment tickets were distributed uniformly to all agents
While minor on the surface, the differences significantly enriched the strategic complexity
of the game. In TAC-00, most of the designers discovered that a dominant strategy was
to defer all serious bidding to the end of the game. A result, the focus was on solving
the allocation problem, with most agents using a greedy, heuristic approach. Since the
hotel auctions closed at the end of the game, timing issues were also important, with
significant advantages going to agents that were able to bid in response to last-second price
quotes (Stone & Greenwald, 2003). Nonetheless, many techniques developed in 2000 were
relevant to the 2001 competition: the agent strategies put forth in TAC-00 were important
precursors to the second year's field, for instance as pointed out in Section 5.1.
Predicting hotel clearing prices was perhaps the most interesting aspect of TAC agent
strategies in TAC-01, especially in relation to TAC-00 where the last-minute bidding created
essentially a sealed-bid auction. As indicated by our experiments described in Section 6.3,
there are many possible approaches to this hotel price estimation problem, and the approach
chosen can have a significant impact on the agent's performance. Among those observed
in TAC-01 are the following (Wellman, Greenwald, Stone, & Wurman, 2002), associated in
some cases with the price-predictor variant in our experiments that was motivated by it.
1. Just use the current price quote p (
).
2. Adjust based on historic data. For example, if  is the average historical difference
between clearing price and price at time t, then the predicted clearing price is p + .
3. Predict by fitting a curve to the sequence of ask prices seen in the current game.
4. Predict based on closing price data for that hotel in past games (
,
).
5. Same as above, but condition on hotel closing time, recognizing that the closing
sequence will inuence the relative prices.
6. Same as above, but condition on full ordering of hotel closings, or which hotels are
open or closed at a particular point (
,
).
7. Learn a mapping from features of the current game (including current prices) to closing
prices based on historic data (
,
).
8. Hand-construct rules based on observations about associations between abstract features.
Having demonstrated
's success at bidding in simultaneous auctions for multiple interacting goods in the TAC domain, we extended our approach to apply it to the
U.S. Federal Communications Commission (FCC) spectrum auctions domain (Weber, 1997).
The FCC holds spectrum auctions to sell radio bandwidth to telecommunications companies. Licenses entitle their owners to use a specified radio spectrum band within a specified
geographical area, or market. Typically several licenses are auctioned off simultaneously
with bidders placing independent bids for each license. The most recent auction brought in
t

CurrentBid

t

t

SimpleMeanev

Cond'lMeanev

ATTac-2001s

ATTac-2001

239

Cond'lMeans

ATTac-2001ev

t

SimpleMeans

fiStone, Schapire, Littman, Csirik, & McAllester

over $16 billion dollars. In a detailed simulation of this domain (Csirik, Littman, Singh, &
Stone, 2001), we discovered a novel, successful bidding strategy in this domain that allows
the bidders to increase their profits significantly over a reasonable default strategy (Reitsma,
Stone, Csirik, & Littman, 2002).
Our ongoing research agenda includes applying our approach to other similar domains.
We particularly expect the boosting approach to price prediction and the decision-theoretic
reasoning over price distributions to transfer to other domains. Other candidate real-world
domains include electricity auctions, supply chains, and perhaps even travel booking on
public e-commerce sites.
Acknowledgements

This work was partially supported by the United States{Israel Binational Science Foundation (BSF), grant number 1999038. Thanks to the TAC team at the University of Michigan
for providing the infrastructure and support required to run many of our experiments.
Thanks to Ronggang Yu at the University of Texas at Austin for running one of the experiments mentioned in the article. Most of this research was conducted while all of the
authors were at AT&T Labs | Research.
References

Anthony, P., Hall, W., Dang, V. D., & Jennings, N. R. (2001). Autonomous agents for
participating in multiple on-line auctions. In Proceedings of the IJCAI-2001 Workshop
on E-Business and the Intelligent Web Seattle, WA.
Boutilier, C. (2002). A pomdp formulation of preference elicitation problems. In Proceedings
of the Eighteenth National Conference on Artificial Intelligence, pp. 239{246.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48 (1/2/3).
Csirik, J. A., Littman, M. L., Singh, S., & Stone, P. (2001). FAucS: An FCC spectrum
auction simulator for autonomous bidding agents. In Fiege, L., Muhl, G., & Wilhelm,
U. (Eds.), Electronic Commerce: Proceedings of the Second International Workshop,
pp. 139{151 Heidelberg, Germany. Springer Verlag.
Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55 (1),
119{139.
Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering for successful TAC
participation. In First International Joint Conference on Autonomous Agents and
Multi-Agent Systems Bologna.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the
Theory of NP-completeness. Freeman, San Francisco, CA.
240

fiDecision-Theoretic Bidding with Learned Density Models

Gimenez-Funes, E., Godo, L., Rodriguez-Aguiolar, J. A., & Garcia-Calves, P. (1998). Designing bidding strategies for trading agents in electronic auctions. In Proceedings of
the Third International Conference on Multi-Agent Systems, pp. 136{143.
Ginsberg, M. L. (2001). GIB: Imperfect information in a computationally challenging game.
JAIR, 14, 303{358.
Greenwald, A., & Boyan, J. (2001). Bidding algorithms for simultaneous auctions. In
Proceedings of Third ACM Conference on E-Commerce, pp. 115{124 Tampa, FL.
Harsanyi, J. (1967{1968). Games with incomplete information played by bayesian players.
Management Science, 14, 159{182,320{334,486{502.
Hauskrecht, M. (1997). Incremental methods for computing bounds in partially observable
Markov decision processes. In Proceedings of the Fourteenth National Conference on
Artificial Intelligence, pp. 734{739.
He, M., & Jennings, N. R. (2002). SouthamptonTAC: Designing a successful trading agent.
In Fifteenth European Conference on Artificial Intelligence Lyon, France.
Ito, T., Fukuta, N., Shintani, T., & Sycara, K. (2000). Biddingbot: a multiagent support
system for cooperative bidding in multiple auctions. In Proceedings of the Fourth
International Conference on MultiAgent Systems, pp. 399{400.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for nearoptimal planning in large Markov decision processes. In Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence (IJCAI-99), pp. 1324{1331.
Klemperer, P. (1999). Auction theory: A guide to the literature. Journal of Economic
Surveys, 13 (3), 227{86.
Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic Boolean satisfiability.
Journal of Automated Reasoning, 27 (3), 251{296.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of Operations Research, 12 (3), 441{450.
Preist, C., Bartolini, C., & Phillips, I. (2001). Algorithm design for agents which participate in multiple simultaneous auctions. In Agent Mediated Electronic Commerce III
(LNAI), pp. 139{154. Springer-Verlag, Berlin.
Reitsma, P. S. A., Stone, P., Csirik, J. A., & Littman, M. L. (2002). Self-enforcing strategic
demand reduction. In Agent Mediated Electronic Commerce IV: Designing Mechanisms and Systems, Vol. 2531 of Lecture Notes in Artificial Intelligence, pp. 289{306.
Springer Verlag.
Rodriguez-Aguilar, J. A., Martin, F. J., Noriega, P., Garcia, P., & Sierra, C. (1998). Towards
a test-bed for trading agents in electronic auction markets. AI Communications, 11 (1),
5{19.
241

fiStone, Schapire, Littman, Csirik, & McAllester

Rothkopf, M. H., & Harstad, R. M. (1994). Modeling competitive bidding: A critical essay.
Management Science, 40 (3), 364{384.
Rust, J., Miller, J., & Palmer, R. (1992). Behavior of trading automata in a computerized
double auction market. In Friedman, D., & Rust, J. (Eds.), The Double Auction
Market: Institutions, Theories, and Evidence. Addison-Wesley, Redwood City, CA.
Schapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using confidence-rated
predictions. Machine Learning, 37 (3), 297{336.
Schapire, R. E., & Singer, Y. (2000). BoosTexter: A boosting-based system for text categorization. Machine Learning, 39 (2/3), 135{168.
Stone, C. J. (1994). The use of polynomial splines and their tensor products in multivariate
function estimation. The Annals of Statistics, 22 (1), 118{184.
Stone, P., & Greenwald, A. (2003). The first international trading agent competition:
Autonomous bidding agents. Electronic Commerce Research. To appear.
Stone, P., Littman, M. L., Singh, S., & Kearns, M. (2001). ATTac-2000: An adaptive
autonomous bidding agent. Journal of Artificial Intelligence Research, 15, 189{206.
Tesauro, G., & Das, R. (2001). High-performance bidding agents for the continuous double
auction. In Third ACM Conference on Electronic Commerce, pp. 206{209.
Weber, R. J. (1997). Making more from less: Strategic demand reduction in the FCC
spectrum auctions. Journal of Economics and Management Strategy, 6 (3), 529{548.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2002). The 2001 trading agent
competition. In Proceedings of the Fourteenth Innovative Applications of Artificial
Intelligence Conference, pp. 935{941.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003a). The 2001 trading
agent competition. Electronic Markets, 13 (1), 4{12.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2003b). Price prediction
in a trading agent competition. Tech. rep., University of Michigan.
Wellman, M. P., Wurman, P. R., O'Malley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh,
W. E. (2001). A trading agent competition. IEEE Internet Computing, 5 (2), 43{51.

242

fiJournal of Artificial Intelligence Research 19 (2003) 73-138

Submitted 12/02; published 8/03

Optimal Schedules for Parallelizing Anytime Algorithms:
The Case of Shared Resources
Lev Finkelstein
Shaul Markovitch
Ehud Rivlin

lev@cs.technion.ac.il
shaulm@cs.technion.ac.il
ehudr@cs.technion.ac.il

Computer Science Department
Technion - Israel Institute of Technology
Haifa 32000, Israel

Abstract
The performance of anytime algorithms can be improved by simultaneously solving
several instances of algorithm-problem pairs. These pairs may include different instances
of a problem (such as starting from a different initial state), different algorithms (if several
alternatives exist), or several runs of the same algorithm (for non-deterministic algorithms).
In this paper we present a methodology for designing an optimal scheduling policy based
on the statistical characteristics of the algorithms involved. We formally analyze the case
where the processes share resources (a single-processor model), and provide an algorithm
for optimal scheduling. We analyze, theoretically and empirically, the behavior of our
scheduling algorithm for various distribution types. Finally, we present empirical results of
applying our scheduling algorithm to the Latin Square problem.

1. Introduction
Assume that our task is to learn a concept with a predefined success rate, measured on a
given test set. Assume that we can use two alternative learning algorithms, one which learns
fast but requires some preprocessing, and another which works more slowly but requires no
preprocessing. Can we possibly benefit from using both learning algorithms in parallel to
solve one learning task on a single-processor machine?
Another area of application is that of constraint satisfaction problems. Assume that
a student tries to decide between two elective courses by trying to schedule each of them
with the set of her compulsory courses. Should the student try to solve the two sets of
constraints sequentially or should the two computations be somehow interleaved?
Assume now that a crawler searches for a specific page in a site. If we had more than one
starting point, the process could be speeded up by simultaneous application of the crawler
from a few (or all) of them. However, what would be the optimal strategy if the bandwidth
were restricted?
What do the above examples have in common?
 There are potential benefits to be gained from the uncertainty in the amount of
resources that will be required to solve more than one instance of the algorithmproblem pair. We can use different algorithms (in the first example) and different
problems (in the last two examples). For non-deterministic algorithms, we can also
use different runs of the same algorithm.
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiFinkelstein, Markovitch & Rivlin

 Each process is executed with the purpose of satisfying a given goal predicate. The
task is considered accomplished when one of the runs succeeds.
 If the goal predicate is satisfied at time t  , then it is also satisfied at any time t > t  .
This property is equivalent to utility monotonicity of anytime algorithms (Dean &
Boddy, 1988; Horvitz, 1987), where solution quality is restricted to Boolean values.
Our objective is to provide a schedule that minimizes the expected cost, possibly under
some constraints (for example, processes may share resources). Such problem definition is
typical for rational-bounded reasoning (Simon, 1982; Russell & Wefald, 1991). This problem
resembles those faced by contract algorithms (Russell & Zilberstein, 1991; Zilberstein, 1993).
There, given the allocated resources, the task is to construct an algorithm providing a
solution of the highest quality. In our case, given quality requirements, the task is to
construct an algorithm that solves the problem using minimal resources.
There are several research works that deal with similar problems. Simple parallelization,
with no information exchange between the processes, may speed up the process due to
high diversity in solution times. For example, Knight (1993) showed that using many
reactive agents employing RTA* search (Korf, 1990) is more beneficial than using a single
deliberative agent. Another example is the work of Yokoo and Kitamura (1996), who used
several search agents in parallel, with agent rearrangement after preallotted periods of time.
Janakiram, Agrawal, and Mehrotra (1988) showed that for many common distributions of
solution time, simple parallelization leads to at most linear speedup. One exception is the
family of heavy-tailed distributions (Gomes, Selman, & Kautz, 1998) for which it is possible
to obtain superlinear speedup by simple parallelization.
A superlinear speedup can also be obtained when we have access to the internal structure
of the processes involved. For example, Clearwater, Hogg, and Huberman (1992) reported
superlinear speedup for cryptarithmetic problems as a result of information exchange between the processes. Another example is the works of Kumar and Rao (Rao & Kumar,
1987; Kumar & Rao, 1987; Rao & Kumar, 1993), devoted to parallelizing standard search
algorithms, where superlinear speedup is obtained by dividing the search space.
An interesting domain-independent approach is based on portfolio construction (Huberman, Lukose, & Hogg, 1997; Gomes & Selman, 1997). In this approach, a different
amount of resources is allotted to each process. This can reduce both expected resource
consumption and its variance.
In the case of non-deterministic algorithms, another way to benefit from solution time
diversity is to restart the same algorithm in attempt to switch to a better trajectory. Such
a framework was analyzed in detail by Luby, Sinclair, and Zuckerman (1993) for the case of
a single processor and by Luby and Ertel (1994) for the multiprocessor case. In particular,
it was proven that for a single processor, the optimal strategy is to periodically restart the
algorithm after a constant amount of time until the solution is found. This strategy was
successfully applied to combinatorial search problems by Gomes, Selman, and Kautz (1998).
There are several settings, however, where the restart strategy is not optimal. If the
goal is to schedule a number of runs of a single non-deterministic algorithm, such that
this number is limited due to the nature of the problem (for example, robotic search),
the restart strategy is applicable but not optimal. A special case of the above settings is
scheduling a number of runs of a deterministic algorithm with a finite set of available initial
74

fiOptimal Schedules for Parallelizing Anytime Algorithms

configurations (inputs). Finally, the case where the goal is to schedule a set of algorithms
different from each other is out of the scope of the restart strategy.
The goal of this research is to develop a methodology for designing an optimal scheduling
policy for any number of instances of algorithm-problem pairs, where the algorithms can
be either deterministic or non-deterministic. We present a formal framework for scheduling parallel anytime algorithms for the case where the processes share resources (a singleprocessor model), based on the statistical characteristics of the algorithms involved. The
framework assumes that we know the probability of the goal condition to be satisfied as a
function of time (a performance profile (Simon, 1955; Boddy & Dean, 1994) restricted to
Boolean quality values). We analyze the properties of optimal schedules for the suspendresume model, where allocation of resources is performed on mutual exclusion basis, and
show that in most cases an extension of the framework to intensity control, where resources
may be allocated simultaneously and proportionately to multiple demands, does not yield
better schedules. We also present an algorithm for building optimal schedules. Finally, we
demonstrate experimental results for the optimal schedules.

2. Motivation
Before starting the formal discussion, we would like to illustrate how different scheduling
strategies can affect the performance of a system of two search processes. The first example
has a very simple setup which allows us to perform a full analysis. In the second example,
we show quantitative results for a real CSP problem.
2.1 Scheduling DFS Search Processes
Assume DFS with random tie-breaking is applied to a simple search space shown in Figure 1,
but that only two runs of the algorithm are allowed 1 . There is a very large number of
paths to the goal, half of them of length 10, quarter of them of length 40, and quarter of
them of length 160. When one of the processes finds the solution, the task is considered
accomplished.
10
40
10
160

A

B
10
40
10
160

Figure 1: A simple search task: two DFS-based agents search for a path from A to B. Scheduling
the processes may reduce costs.
1. Such a limit can follow, for example, from physical constraints, such as for the problem of robotic search.
For unlimited number of runs the optimal results would be provided by the restart strategy.

75

fiFinkelstein, Markovitch & Rivlin

We consider a single-processor system, where the two processes cannot run simultaneously. Let us denote the processes by A 1 and A2 , and by L1 and L2 the actual path lengths
for A1 and A2 respectively for the particular run.
The application of a single processes (without loss of generality, A 1 ) gives us the expected
execution time of 1/2  10 + 1/4  40 + 1/4  160 = 55, as is shown in Figure 2.
t
@
1/2
L1 = 10, cost = 10

@

@

t
1/2

L1 = 40, cost = 40 t

1/2
@

@
@t L1 6= 10
@
@
1/2
@
@
@
@t

L1 = 160, cost = 160

Figure 2: Path lengths, probabilities and costs for running a single process
We can improve the performance by simulating a simultaneous execution of two processes. For this purpose, we allow each of the processes to expand a single node, and to
switch to the other process (without loss of generality, A 1 starts first). In this case, the
expected execution time is 1/2  19 + 1/4  20 + 1/8  79 + 1/16  80 + 1/16  319 = 49.3125,
as is shown in Figure 3.
Finally, if we know the distribution of path lengths, we can allow A 1 to open 10 nodes;
if A1 fails, we can stop it and allow A2 to open 10 nodes; if A2 fails as well, we can allow
A1 to open the next 30 nodes, and so forth. In this scenario, A 1 and A2 switch after 10
and 40 nodes (if both processes fail to find a solution after 40 nodes, it is guaranteed to be
found by A1 after 160 nodes). This scheme is shown in Figure 4, and the expected time is
1/2  10 + 1/4  20 + 1/8  50 + 1/16  80 + 1/16  200 = 33.75.
2.2 The Latin Square Example
The task in the Latin Square problem is to place N symbols on an N  N square such
that each symbol appears only once in each row and each column. An example is shown in
Figure 5.
A more interesting problem arises when the square is partially filled. The problem in
this case may be solvable (see the left side of Figure 6) or unsolvable (see the right side
of Figure 6). The problem of satisfiability of a partially filled Latin Square is a typical
constraint-satisfaction problem. We consider a slight variation of this task. Let us assume
that two partially filled squares are available, and we need to decide whether at least one of
them is solvable. We assume that we are allocated a single processor. We attempt to speed
up the time of finding a solution by starting to solve the two problems from two different
initial configurations in parallel.
Each of the processes employs a deterministic heuristic DFS with the First-Fail heuristic (Gomes & Selman, 1997). We consider 10%-filled 20  20 Latin Squares. The behavior
of a single process measured on a set of 50,000 randomly generated samples is shown in
76

fiOptimal Schedules for Parallelizing Anytime Algorithms

t0
t
@
1/2
L1 = 10,
L2  10,
cost = 19

t0 = 0
@

@

t1
t

1/2
@

t
@
@t1
@

1/2
L1  40,
L2 = 10,
cost = 20

@

L1  40,
L2  10
@

t1 = 19

1/2
@

L1  40,
t
@
@t2 L2  40
@
@ 1/2
1/2
@
@
L1 = 160,
t3
t
@
t
@t3
L2  40
@
@ 1/2
1/2
@
L1 = 160,
@
t4
t
L2 = 40,
@
t
@t4
cost = 80

t2
t

L1 = 40,
L2  40,
cost = 79

t2 = 20

t3 = 79

L1 = 160,
L2 = 160

t4 = 80

1
L1 = 160,
L2 = 160,
cost = 319

t5
t

A2
A1
...
0

1

2

3

4

5

6

7

8

9

10

Figure 3: Path lengths, probabilities and costs for simulating a simultaneous execution

77

t5 = 319

fiFinkelstein, Markovitch & Rivlin

t0
t
@
1/2
L1 = 10,
L2  10,
cost = 10

t0 = 0
@

@

t1
t

1/2
@

t
@
@t1
@

1/2
L1  40,
L2 = 10,
cost = 20

t2
t

L1 = 40,
L2  40,
cost = 50

@

L1  40,
L2  10
@

t1 = 10

1/2
@

L1  40,
t
@
@t2 L2  40
@
@ 1/2
1/2
@
@
L1 = 160,
t3
t
@
@t3
t
L2  40
@
@ 1/2
1/2
@
L1 = 160,
@
t4
t
L2 = 40,
@
@t4
t
cost = 80

t2 = 20

t3 = 50

L1 = 160,
L2 = 160

t4 = 80

1
L1 = 160,
L2 = 160,
cost = 200

t5
t

t5 = 200

A2
A1
t0

t1

t2

t3

t4

t5

Figure 4: Path lengths, probabilities and costs for the interleaved execution

1
3
5
2
4

2
4
1
3
5

3
5
2
4
1

4
1
3
5
2

5
2
4
1
3

Figure 5: An example of a 5  5 Latin Square.

78

fiOptimal Schedules for Parallelizing Anytime Algorithms

1

1
4

5

? 4 5

1
2

2
3

1
1 2

4

Figure 6: An example of solvable (to the left) and unsolvable (to the right) prefilled 5  5 Latin
Squares.

Figure 7. Figure 7(a) shows the probability of finding a solution as a function of the number
of search steps, and Figure 7(b) shows the corresponding distribution density. Assume that
0.8

0.05
0.045

0.7

0.04
0.6
0.035
0.03

0.4

f(t)

F(t)

0.5

0.025
0.02

0.3

0.015
0.2
0.01
0.1

0
300

0.005

400

500

600

700

800

900

0
300

1000

400

500

600

t

700

800

900

1000

t

(a)

(b)

Figure 7: The behavior of DFS with the First-Fail heuristic on 10%-filled 20  20 Latin Squares.
(a) The probability of finding a solution as a function of the number of search steps;
(b) The corresponding distribution density.

each run is limited to 25,000 search steps (only 88.6% of the problems are solvable under this
condition). If we apply the algorithm only on one of the available two initial configurations,
the average number of search steps is 3777. If we run two processes in parallel (alternating
after each step), we obtain a result of 1358 steps. If we allow a single switch at the optimal
point (an analogue of the restart technique (Luby et al., 1993; Gomes et al., 1998) for two
processes), we get 1376 steps on average (the optimal point is after 1311 steps). Finally, if
we interleave the processes, switching at the points corresponding to 679, 3072, and 10208
of total steps, the average number of steps is 1177. The above results were averaged over a
test set of 25,000 pairs of initial configurations.
The last sequence of switch points is an optimal schedule for the process with behavior
described by the graphs in Figure 7. In the rest of the paper we present an algorithm for
deriving such optimal schedules.
79

fiFinkelstein, Markovitch & Rivlin

3. A Framework for Parallelization Scheduling
In this section we formalize the intuitive description of parallelization scheduling. The first
part of this framework is similar to our framework presented in (Finkelstein & Markovitch,
2001).
Let S be a set of states, t be a time variable with non-negative real values, and A be a
random process such that each realization (trajectory) A(t) of A represents a mapping from
R+ to S. Let X0 be a random variable defined over S. Since an algorithm Alg starting
from an initial state S0 corresponds to a single trajectory (for deterministic algorithms), or
to a set of trajectories with an associated distribution (for non-deterministic algorithms),
the pair hX0 , Algi, where X0 stands for the initial state, can be viewed as a random process.
Drawing a trajectory for such a process corresponds, without loss of generality, to a twostep procedure: first an initial state S 0 is drawn for X0 , and then a trajectory A(t) starting
from S0 is drawn for Alg. Thus, the source of randomness is either the randomness of the
initial state, or the randomness of the algorithm (which can come from the algorithm itself
or from the environment), or both.
Let S   S be a designated set of states, and G : S  {0, 1} be the characteristic
function of S  called the goal predicate. The behavior of a trajectory A(t) of A with respect
cA (t). We say
to the goal predicate G can be written as G(A(t)), which we denote by G
cA (t) is a non-decreasing function for each
that A is monotonic over G if and only if G
cA (t) is a step function with at most
trajectory A(t) of A. Under the above assumptions G
one discontinuity point.
Let A be monotonic over G. From the definitions above we can see that the behavior
of G for each trajectory A(t) of A can be described by a single point b
tA,G , the first point
cA (t) = 1}. If G
cA (t) is always 0,
after which the goal predicate is true, i.e, b
tA,G = inf t {t|G
b
we say that tA,G is not defined. Therefore, we can define a random variable, which for each
trajectory A(t) of A with b
tA,G defined, corresponds to b
tA,G . The behavior of this variable
can be described by its distribution function F (t). At the points where F (t) is differentiable,
we use the probability density f (t) = F 0 (t).
It is important to note that in practice not every trajectory of A leads to the goal
predicate satisfaction even after infinitely large time. That means that the set of the trajectories where b
tA,G is undefined is not necessarily of measure zero. That is why we define
the probability of success p as the probability of A(t) to have b
tA,G defined2 . For the Latin
Square example described in Section 2.2, the probability of success is 0.886, and the graphs
in Figure 7 correspond to pF (t) and pf (t).
Assume now that we have a system of n random processes A 1 , . . . An with corresponding distribution functions F1 , . . . , Fn and goal predicates G1 , . . . , Gn . If the distribution
functions Fi and Fj are identical, we refer to Ai and Aj as F -equivalent.
We define a schedule of the system as a set of binary functions { i }, where at each
moment t, the i-th process is active if  i (t) = 1 and idle otherwise. We refer to this scheme
as suspend-resume scheduling. A possible generalization of this framework is to extend
the suspend/resume control to a more refined mechanism that allows us to determine the
2. Another way to express the possibility that a process will not reach a goal state is to use F (t) that
approach 1  p when t  . We prefer to use p explicitly because the distribution function must meet
the requirement limt F (t) = 1.

80

fiOptimal Schedules for Parallelizing Anytime Algorithms

intensity with which each process acts. For software processes, this means varying the
fraction of CPU utilization; for tasks like robot navigation this implies changing the speed
of the robots. Mathematically, using intensity control is equivalent to replacing the binary
functions i (t) with continuous functions with a range between zero and one 3 .
Note that scheduling makes the term time ambiguous. On one hand, we have the
subjective time for each process, consumed only when the process is active. This kind of
time corresponds to some resource consumed by the process. On the other hand, we have
an objective time measured from the point of view of an external observer. The distribution
function Fi (t) of each process is defined over its subjective time, while the cost function (see
below) may use both kinds of times. Since we are using several processes, all the formulas
in this paper are based on the objective time.
Let us denote by i (t) the total time that process i has been active before t. By
definition,
Z
t

i (x)dx.

i (t) =

(1)

0

In practice i (t) provides the mapping from the objective time t to the subjective time of
the i-th process, and we refer to these functions as subjective schedule functions. Since  i
can be obtained from i by differentiation, we often describe schedules by { i } instead of
{i }.
The processes {Ai } with goal predicates {Gi } running under schedules {i }Wresult in
a new process A, with a goal predicate G. G is the disjunction of G i (G(t) = i Gi (t)),
and therefore A is monotonic over G. We denote the distribution function of the corresponding random variable by Fn (t, 1 , . . . , n ), and the corresponding distribution density
by fn (t, 1 , . . . , n ).
Assume that we are given a monotonic non-decreasing cost function u(t, t 1 , . . . , tn ), which
depends on the objective time t and the subjective times per process t i . We also assume
that u(0, t1 , . . . , tn ) = 0. Since the subjective times can be calculated by  i (t), we actually
have u = u(t, 1 (t), . . . , n (t)).
The expected cost of schedule {i } can be expressed, therefore, as4
Z +
Eu (1 , . . . , n ) =
u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt
(2)
0

(for the sake of readability, we omit t in  i (t)). Under the suspend-resume model assumptions, i must be differentiable (except for a countable set of process switch points) and
have derivatives of 0 or 1 that would ensure correct values for  i . Under intensity control
assumptions, the derivatives of i must lie between 0 and 1.
We consider two alternative setups for resource sharing between the processes:
1. The processes share resources on a mutual exclusion basis. That means that exactly
one process can be active at each moment, and the processes will be active one after
another until the goal is reached by one of them. In this case the sum of derivatives
3. A special case of such a setup using constant intensities was described by Huberman, Lukose, and
Hogg (1997).
4. The generalization to the case where the probability of success p is not 1 is considered at the end of the
next section.

81

fiFinkelstein, Markovitch & Rivlin

of i is always one5 . The case of shared resources corresponds to the case of several
processes running on a single processor.
2. The processes are fully independent: there are no additional constraints on  i . This
case corresponds to n independent processes running on n processors.
Our goal is to find a schedule which minimizes the expected cost (2) under the corresponding
constraints. The current paper is devoted to the case of shared processes. The case of
independent resources was studied in (Finkelstein, Markovitch, & Rivlin, 2002).
The scheduled algorithms considered in this framework can be viewed as anytime algorithms. The behavior of anytime algorithms is usually characterized by their performance
profile  the expected quality of the algorithm output as a function of the alloted resources.
The goal predicate G can be viewed as a quality function with two possible values, and thus
the distribution function F (t) meets the definition of performance profile, where time plays
the role of resource.

4. Suspend-Resume Based Scheduling
In this section we consider the case of suspend-resume based control ( i are continuous
functions with derivatives 0 or 1).
Claim 1 The expressions for the goal-time distribution F n (t, 1 , . . . , n ) and the expected
cost Eu (1 , . . . , n ) are as follows6 :
Fn (t, 1 , . . . , n ) = 1 
Eu (1 , . . . , n ) =

Z

0

+

u0t

+

n
Y
i=1

n
X

(1  Fi (i )),

(3)

n
Y

(4)

i0 u0i

i=1

!

i=1

(1  Fi (i ))dt.

Proof: Let ti be the time it would take the i-th process to meet the goal if acted alone
(if the process fails to reach the goal, we consider t i = ). Let t be the time it takes
the system of n processes to reach the goal. In this case, t  is distributed according to
Fn (t, 1 , . . . , n ), and ti are distributed according to Fi (t). Thus, because the processes,
given a schedule, are independent, we obtain
Fn (t, 1 , . . . , n ) = P (t  t) = 1  P (t > t) = 1  P (t1 > 1 (t))  . . .  P (tn > n (t)) =
n
Y
(1  Fi (i (t))),
1  (1  F1 (1 (t)))  . . .  (1  Fn (n (t))) = 1 
i=1

which corresponds to (3). Since F (t) is a distribution over time, we assume F (t) = 0 for
t  0.
5. This fact is obvious for the case of suspend-resume control, and for intensity control it is reflected in
Lemma 3.
6. u0t and u0i stand for partial derivatives of u by t and by i respectively.

82

fiOptimal Schedules for Parallelizing Anytime Algorithms

The average cost function will therefore be
Z +
Eu (1 , . . . , n ) =
u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt =
0
Z +

u(t, 1 , . . . , n )d(1  Fn (t, 1 , . . . , n )) =
0

 u(t, 1 , . . . , n )(1 

Fn (t, 1 , . . . , n ))|
0

+

Z

+

0

n

du(t, 1 , . . . , n ) Y
(1  Fi (i ))dt.
dt
i=1

Since u(0, 1 , . . . , n ) = 0 and Fn (, 1 , . . . , n ) = 1, the first term in the last expression
is 0. Besides, since the full derivative of u by t can be written as
n

X
du(t, 1 , . . . , n )
i0 u0i ,
= u0t +
dt
i=1

we obtain
Eu (1 , . . . , n ) =

Z

+

u0t +

0

n
X
i=1

i0 u0i

!

n
Y
i=1

(1  Fi (i ))dt,

which completes the proof.
Q.E.D.
Note that in the case of i (t) = t and Fi (t) = F (t) for all i (parallel application of n
F -equivalent processes), we obtain the formula presented in (Janakiram et al., 1988), i.e.,
Fn (t) = 1  (1  F (t))n .
In the rest of this section we show a formal solution (necessary conditions and an algorithm) for the framework with shared resources. We start with two processes and present
the formulas and the algorithm, and then generalize the solution for an arbitrary number
of processes. For the case of two processes, we only assume that u is differentiable.
For the more elaborated setup of n processes, we assume that the total cost is a linear
combination of the objective time and all the subjective times, and the subjective times are
of the same weight:
n
X
i (t).
(5)
u(t, 1 , . . . , n ) = at + b
i=1

Since time is consumed if and only if there is an active process, and the trivial case where
all the processes are idle may be ignored, we obtain (without loss of generality)
Eu (1 , . . . , n ) =

Z

0

n
Y

(1  Fj (j ))dt  min .

(6)

j=1

This assumption is made to keep the expressions more readable. The solution process
remains the same for the general form of u.
4.1 Necessary Conditions for an Optimal Solution for Two Processes
Let A1 and A2 be two processes sharing a resource. While working, one process locks
the resource, and the other is necessarily idle. We can show that such dependency yields
83

fiFinkelstein, Markovitch & Rivlin

a strong constraint on the behavior of the process, allowing the building of an effective
algorithm for solving the minimization problem.
For the suspend-resume model, therefore, only two states of the system are possible:
A1 is active and A2 is idle (S1 ); and A1 is idle and A2 is active (S2 ). We ignore the
case where both processes are idle, since removing such a state from the schedule will not
increase the cost. Therefore, the system continuously alternates between the two states:
S1  S2  S1  S2  . . .. We call the time interval corresponding to each pair hS 1 , S2 i
a phase and denote phase k by k . If we denote the process switch points by t i , the phase
k corresponds to [t2k2 , t2k ]. See Figure 8 for an illustration.
t
s 2k3

S2

k1

s

t2k2

t2k1
s

S1

S2

k

s

t2k t2k+1
s
S1
S2
k+1

t
s 2k+2
S1

t2k+3
s
k+2

Figure 8: Notations for times, states and phases for two processes
By this scheme, A1 is active in the intervals [t0 , t1 ], [t2 , t3 ], . . . , [t2k , t2k+1 ], . . . , and
A2 is active in the intervals [t1 , t2 ], [t3 , t4 ], . . . , [t2k+1 , t2k+2 ], . . . .
Let us denote by 2k1 the total time that A1 has been active before t2k1 , and by
2k the total time that A2 has been active before t2k . By phase definition, 2k1 and 2k
correspond to the cumulative time spent in phases 1 to k in states S 1 and S2 respectively.
There exists a one-to-one correspondence between the sequences  i and ti :
i + i+1 = ti+1 .

(7)

Moreover, by definition of i we have
1 (t2k1 ) = 1 (t2k ) = 2k1 ,
2 (t2k ) = 2 (t2k+1 ) = 2k .

(8)

Under the process switch scheme as defined above, the subjective schedule functions  1
and 2 in time intervals [t2k , t2k+1 ] (state S1 of phase k+1 ) have the form
1 (t) = t  t2k + 1 (t2k ) = t  t2k + 2k1 = t  2k ,
2 (t) = 2 (t2k ) = 2k .

(9)

Similarly, in the intervals [t2k+1 , t2k+2 ] (state S2 of phase k+1 ), the subjective schedule
functions are defined as
1 (t) = 1 (t2k+1 ) = 2k+1 ,
2 (t) = t  t2k+1 + 2 (t2k+1 ) = t  t2k+1 + 2k = t  2k+1 .
Let us denote
v(t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u01 (t1 + t2 , t1 , t2 ) + u02 (t1 + t2 , t1 , t2 )
84

(10)

fiOptimal Schedules for Parallelizing Anytime Algorithms

and
vi (t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u0i (t1 + t2 , t1 , t2 ).
To provide an optimal solution for the suspend/resume model, we may split (4) to phases
k and write it as
Eu (1 , . . . , n ) =

 Z
X

t2k

k=1 t2k2

v(1 , 2 )(1  F1 (1 ))(1  F2 (2 ))dt.

(11)

The last expression may be rewritten as
Eu (1 , . . . , n ) =
 Z t2k+1
X
v(1 , 2 )(1  F1 (1 ))(1  F2 (2 ))dt+
k=0 t2k
 Z t2k+2
X
k=0 t2k+1

(12)

v(1 , 2 )(1  F1 (1 ))(1  F2 (2 ))dt.

Using (9) on interval [t2k , t2k+1 ], performing substitution x = t  2k , and using (7), we
obtain
Z

t2k+1

t2k
Z t2k+1

v(1 , 2 )(1  F1 (1 ))(1  F2 (2 ))dt =

v1 (t
t2k
Z t2k+1 2k
t2k 2k
Z 2k+1
2k1

 2k , 2k )(1  F1 (t  2k ))(1  F2 (2k ))dt =

(13)

v1 (x, 2k )(1  F1 (x))(1  F2 (2k ))dx =

v1 (x, 2k )(1  F1 (x))(1  F2 (2k ))dx.

Similarly, for the interval [t2k+1 , t2k+2 ] we have
Z

t2k+2

t2k+1
Z t2k+2

v(1 , 2 )(1  F1 (1 ))(1  F2 (2 ))dt =

v2 (2k+1 , t
t2k+1
Z t2k+2 2k+1
t2k+1 2k+1
Z 2k+2
2k

 2k+1 )(1  F1 (2k+1 ))(1  F2 (t  2k+1 ))dt =

v2 (2k+1 , x)(1  F1 (2k+1 ))(1  F2 (x))dx =

v2 (2k+1 , x)(1  F1 (2k+1 ))(1  F2 (x))dx.
85

(14)

fiFinkelstein, Markovitch & Rivlin

Substituting (13) and (14) into (12), we obtain a new form for the minimization problem:
Eu (1 , . . . , n ) =
"
Z

X
(1  F2 (2k ))
k=0

(1  F1 (2k+1 ))

Z

2k+1
2k1

v1 (x, 2k )(1  F1 (x))dx +

2k+2

2k

(15)



v2 (2k+1 , x)(1  F2 (x))dx  min

(for the sake of generality, we assume  1 = 0).
The minimization problem (15) is equivalent to the original problem (4), and the dependency between their solutions is described by (9) and (10). The only constraint for the new
problem follows from the fact that the processes are alternating for non-negative periods of
time:

0 = 0 < 2  . . .  2n  . . .
(16)
1 < 3  . . .  2n+1  . . .
The expression (15) reaches its optimal values either when
dEu
= 0 for k = 1, . . . , n, . . . ,
dk

(17)

or on the border described by (16). However, for two processes we can, without loss of
generality, ignore the border case. Indeed, assume that  i = i+2 for some i > 1 (one of the
processes skips its turn). We can construct a new schedule by removing  i+1 and i+2 :
1 , . . . , i1 , i , i+3 , i+4 , i+5 , . . .
It is easy to see that the process described by this schedule is exactly the same process as
described by the original one, but the singularity point has been removed.
Thus, at each step the time spent by the processes is determined by (17). We can see
that 2k appears in three subsequent terms of E u (1 , . . . , n ):

. . . + (1  F1 (2k1 ))
(1  F2 (2k ))

Z

2k+1

Z

2k
2k2

v2 (2k1 , x)(1  F2 (x))dx+

v1 (x, 2k )(1
2k1
Z 2k+2

(1  F1 (2k+1 ))

2k

 F1 (x))dx+

v2 (2k+1 , x)(1  F2 (x))dx + . . . .
86

fiOptimal Schedules for Parallelizing Anytime Algorithms

Differentiating (15) by 2k , therefore, yields
dEu
= v2 (2k1 , 2k )(1  F1 (2k1 ))(1  F2 (2k ))
d2k
Z 2k+1
f2 (2k )
v1 (x, 2k )(1  F1 (x))dx+
2k1

(1  F2 (2k ))

Z

2k+1

2k1

v1
(x, 2k )(1  F1 (x))dx
t2

v2 (2k+1 , 2k )(1  F1 (2k+1 ))(1  F2 (2k )) =

(1  F2 (2k ))(v2 (2k1 , 2k )(1  F1 (2k1 ))  v2 (2k+1 , 2k )(1  F1 (2k+1 ))
Z 2k+1
f2 (2k )
v1 (x, 2k )(1  F1 (x))dx+
2k1

(1  F2 (2k ))

Z

2k+1

2k1

v1
(x, 2k )(1  F1 (x))dx.
t2

A similar expression can be derived by differentiating (15) by  2k+1 . Combining these
expressions with (17) gives us the following theorem:
Theorem 1 (The chain theorem for two processes)
The value for i+1 for i  2 can be computed for given i1 and i using the formulas
f2 (2k )
v2 (2k1 , 2k )(1  F1 (2k1 ))  v2 (2k+1 , 2k )(1  F1 (2k+1 ))
+
=
R 2k+1
1  F2 (2k )
2k1 v1 (x, 2k )(1  F1 (x))dx
R 2k+1 v1
2k1 t2 (x, 2k )(1  F1 (x))dx
, i = 2k + 1,
R 2k+1
2k1 v1 (x, 2k )(1  F1 (x))dx

v1 (2k , 2k+1 )(1  F2 (2k ))  v1 (2k+2 , 2k+1 )(1  F2 (2k+2 ))
f1 (2k+1 )
+
=
R 2k+2
1  F1 (2k+1 )
v
(
,
x)(1

F
(x))dx
2
2
2k+1
2k
R 2k+2 v2
2k
t1 (2k+1 , x)(1  F2 (x))dx
, i = 2k + 2.
R 2k+2
v
(
,
x)(1

F
(x))dx
2
2
2k+1
2k

(18)

(19)

Corollary 1 For the linear cost function (5), the value for  i+1 for i  2 can be computed
for given i1 and i using the formulas
f2 (2k )
F1 (2k+1 )  F1 (2k1 )
, i = 2k + 1,
= R
2k+1
1  F2 (2k )
(1  F1 (x))dx

(20)

2k1

f1 (2k+1 )
F2 (2k+2 )  F2 (2k )
, i = 2k + 2.
= R
2k+2
1  F1 (2k+1 )
(1  F2 (x))dx

(21)

2k

The proof follows immediately from the fact that v i (t1 , t2 ) = a + b.
Theorem 1 allows us to formulate an algorithm for building an optimal solution. This
algorithm is presented in the next subsection.
87

fiFinkelstein, Markovitch & Rivlin

4.2 Optimal Solution for Two Processes: an Algorithm
The goal of the scheduling algorithm is to minimize the expression (15)
Eu (1 , . . . , n ) =
"
Z

X
(1  F2 (2k ))
k=0

(1  F1 (2k+1 ))
under the constraints



Z

2k+1
2k1

v1 (x, 2k )(1  F1 (x))dx +


2k+2

2k

v2 (2k+1 , x)(1  F2 (x))dx  min

0 = 0 < 2  . . .  2n  . . .
1 < 3  . . .  2n+1  . . . .

Assume that A1 acts first (1 > 0). From Theorem 1 we can see that the values of
0 = 0 and 1 determine the set of possible values for  2 , the values of 1 and 2 determine
the possible values for 3 , and so on.
Therefore, a non-zero value for 1 provides us with a tree of possible values of  k . The
branching factor of this tree is determined by the number of roots of (18) and (19). Each
possible sequence 1 , 2 , . . . can be evaluated using (15).
For the cases where the total time is limited as discussed in Section 4.5, or where the
series in that expression converge, e.g., when each process has a finite cost of finding a
solution, the algorithm stops after a finite number of points. In some cases, however, such
as for extremely heavy-tailed distributions, it is possible that the above series diverge. To
ensure a finite number of iterations in such cases, we set an upper limit on the maximal
expected cost.
Another limit is added for the probability of failure. Since t i = i1 + i , the probability
that both runs would not be able to find a solution after t i is
(1  F1 (i1 ))(1  F2 (i )).
Therefore, if the difference
(1  F1 (i1 ))(1  F2 (i ))  (1  p1 )(1  p2 )
becomes small enough, we can conclude that both runs failed to find a solution and stop
the execution.
For each value of 1 we can find the best sequence using one of the standard search
algorithms, such as Branch-and-Bound. Let us denote the value of the best sequence for
each 1 by Eu (1 ). Performing global optimization of E u (1 ) by 1 provides us with an
optimal solution for the case where A 1 acts first. Note that the value of 1 may also be 0
(A2 acts first), so we need to compare the value obtained by optimization of  1 with the
value obtained by optimization of  2 where 1 = 0.
The flow of the algorithm is illustrated in Figure 9, the formal scheme is presented in
Figure 10, and the description of the main routine (realized by the DFS Branch and Bound
method) in Figure 11.
88

fiOptimal Schedules for Parallelizing Anytime Algorithms

The algorithm considers two main branches, one for A 1 and one for A2 , and they are
processed by procedure minimize sequence by f irst point (Figure 10). At each step, we
initialize the array of  values, and pass it, through the procedure build optimal sequence,
to the recursive procedure df sbnb, which represents the core of the algorithm (Figure 11).
The df sbnb procedure, shown in Figure 11, acts as follows. It obtains as an input the
array of  values, the cost involved up to the current moment, and the best value reached till
now. If the cost exceeds this value, the procedure performs a classical Branch-and-Bound
cutoff (lines 1-2).
The inner loop (lines 4-19) corresponds to different roots of the expressions (18)
and (19). The new value of  corresponding to  k is calculated by the procedure
calculate next zeta (line 5), and it cannot exceed the previously found root saved in
last zeta (for the first iteration, last zeta is initialized to  k2 ), lines 3 and 8. Lines 67 correspond to the case where the lower bound passed to calculate next zeta exceeds the
maximal available time, and in this case the procedure is stopped.
After the new possible value of  is found, the procedure updates the current cost (line
9), and the stopping criteria mentioned above are validated for the new array of  values,
which is denoted as a concatenation of the old array and the new value of  (line 10). If
the task is accomplished, the cost is verified versus the best known value (which is updated
if necessary), and the procedure returns (lines 10-16). Otherwise,  is temporarily added
to the array of , and the Branch-and-Bound procedure is called recursively for calculation
k+1 .
When the whole tree is traversed (except the cutoffs), the best known cost is returned
(line 20). The corresponding array of  is the required solution.
Figure 13 shows a trace of a single Branch-and-Bound run for the example shown in
Section 2.2 starting with the optimal value of  1 . The optimal schedule derived from the the
run is 679, 2393, 7815, 17184 with expected cost of 1216.49 steps. The scheduling points are
given in subjective times. Using objective (total) time the schedule can be written as 679,
3072, 10208, and 25000. In this particular run there were no Branch-and-Bound cutoffs due
to the small number of roots of (18) and (19).
4.3 Necessary Conditions for an Optimal Solution for n Processes
In this section we generalize our solution from the case of two processes to the case of n
processes.
Assume that we have n processes A1 , . . . , An using shared resources. One of the possible
ways to present a schedule is to use a sequence
h(Ai1 , t1 ), (Ai2 , t2 ), . . . , (Aij , tj ), . . .i,
where Aij is the j-th active process, and tj is the time allocated for this invocation of A ij .
To simplify the formalization of the problem, however, we use the following alternative
representation. First, we allow t j to be 0, which makes possible it to represent every
schedule as
h(A1 , t1 ), (A2 , t2 ), . . . , (An , tn ), (A1 , tn+1 ), (A2 , tn+2 ), . . . , (An , t2n ), . . .i.
89

fiFinkelstein, Markovitch & Rivlin


+


















x
Q

Get optimal schedule costs for
Q 1 = 0 and for 1 6= 0, and
Q
Q
return
the best value with
Q
Q
the corresponding
schedule
Q
Q

A1 acts first

Q

Q
s
Q

A2 acts first

?

?

...

Minimization by 1
P

QP
   S QPPP




PP
S Q
1 trials

 
Q
PP

S
)

q
P

Q


Q
S
+

s
Q
/

w
S
h
?

 J
 
J 
 
J


J


^
Jh 
/

=

h
h
x
?



J



 J




J 




J




J^



h
x
h
x
x
?
.?



?
Jh 
@


J
B


 J
 B@

J



 B @
@
J



B

@
J^
/

R
@
? 	
?
J




BN


by minimization procedure
Root of Branch and Bound tree
2 satisfying (19) for k = 0
Branch and Bound nodes
3 satisfying (18) for k = 1
Branch and Bound nodes
4 satisfying (19) for k = 1
.....

h

Branch and Bound non-leaf nodes

x

Leaf nodes (terminating condition satisfied) and cutoff nodes (expected result is
worse than the already known). The cost is calculated in accordance with (15).
Figure 9: The flow of the algorithm for constructing optimal schedules for 2 processes

90

fiOptimal Schedules for Parallelizing Anytime Algorithms

procedure optimize
Input: F1 (t), F2 (t) (performance profiles).
Output: An optimal sequence and its value.
[sequence1 , val1 ]  minimize sequence by f irst point(A 1 )
[sequence2 , val2 ]  minimize sequence by f irst point(A 2 )
if val1 < val2 then
return [sequence1 , val1 ]
else
return [sequence2 , val2 ]
end
end
procedure minimize sequence by f irst point(process)
zetas[1]  0
zetas[0]  0
if process = A2 then
zetas[1]  0
end
Using one of the standard minimization methods, find zetas,
minimizing the value of the function build optimal sequence(zetas),
and the corresponding cost.
end

Figure 10: Procedure optimize builds an optimal sequence for the case when A 1 starts, an optimal sequence for the case when A2 starts, compares the results, and returns the best
one. Procedure minimize sequence by f irst point returns an optimal sequence and its
value.

91

fiFinkelstein, Markovitch & Rivlin

procedure build optimal sequence(zetas)
curr cost  calculate cost(zetas)
return df sbnb(zetas, curr cost, M AX V ALU E)
end
procedure df sbnb(zetas, curr cost, thresh)
1:
if (curr cost  thresh) then
// Cutoff
2:
return M AX V ALU E
3:
last value  zetas[length(zetas)  2]
// The previous time value
4:
repeat
5:
  calculate next zeta(zetas, last value)
6:
if ( = last value) then
// Skip
7:
return thresh
8:
last value  
9:
delta cost  calculate partial cost(zetas, )
10:
if (task accomplished([zetas || ])) then
// Leaf
11:
if (curr cost + delta cost < thresh) then
12:
optimal zetas  [zetas || ]
13:
thresh  curr cost + delta cost
14:
end
15:
return thresh
16:
end
17:
tmp result  df sbnb([zetas || ], curr cost + delta cost, thresh)
18:
thresh = min(thresh, tmp result)
19: end
// repeat
20: return thresh
end
Figure 11: Procedure build optimal sequence, given the prefix of the time sequence, restores the
optimal sequence with this prefix using the DFS Branch and Bound search algorithm,
and returns the sequence itself and its value. [x || y] stands for concatenation x and y.
Auxiliary functions are shown in Figure 12.

92

fiOptimal Schedules for Parallelizing Anytime Algorithms

1. calculate cost(zetas) computes the cost of the sequence (or its part) in accordance
with (15),
2. calculate partial cost(zetas, ) computes the additional cost obtained by adding
 to the sequence,
3. calculate next zeta(zetas, last value) uses (18) or (19) to calculate the value of
the next  that is greater than last value. If no such a solution exists, the
maximal time value is returned,
4. task accomplished(zetas) returns true when the task may be considered to be
accomplished (e.g., either maximal possible time is over, or the probability of
error is negligible, or the upper limit on the cost is exceeded).
Figure 12: Auxiliary functions used in the optimal schedule algorithm

1

679.0

2

379.4

3

24620.6

2393.0

7815.4

u=2664.54

4

24321.0
u=1534.06

22607.0
u=1265.67

17184.6
u=1216.49

Figure 13: A trace of a single run of the Branch-and-Bound procedure starting with the optimal
value of 1 .

93

fiFinkelstein, Markovitch & Rivlin

Therefore, the system alternates between n states S 1  S2  . . .  Sn  S1  . . ., where
the state Si corresponds to the situation where A i is active and the rest of the processes
are idle. The time spent in the k-th invocation of S i is tkn+i .
As in the case of two processes, we call the time interval corresponding to the sequence
of states S1  S2  . . .  Sn a phase and denote phase k by k . We denote the process
switch points of k by t1k , t2k , . . . , tnk , where
tik

=

k1
X

tnj+i.

j=0

0
i
Process Ai is active in phase k in the interval [t i1
k , tk ], and the entire phase lasts from t k
to tnk . The corresponding scheme is shown in Figure 14.

s

n1
tk1

Sn
k1

tn = t0k
s k1
S1

t1
sk

S2

t2
sk

...
k

tn1
tn = t0k+1 t1k+1
sk
sk
s
Sn
S1
k+1

Figure 14: Notations for times, states and phases for n processes
To simplify the following discussion, we would like to allow indices i in t ik to be less than
0 or greater than n. For this purpose, we denote
mod n
,
tik = tik+bi/nc

(22)

i
and the index of the process active in the interval [t i1
k , tk ] we denote by #i. For i mod n 6=
0 we obtain #i = i mod n, while for i mod n = 0 we have #i = n. Notation (22) claims
that the shift by n in the upper index is equivalent to the shift by 1 in the phase number:

ti+n
= tik+1 .
k
As in the case of two processes, we denote by  ki the total time that A#i has been active
up to tik . ki corresponds to the cumulative time spent in phases 1 to k in state S #i , and
there is a one-to-one correspondence between the sequences of  ki and tik :
i
ki  k1
= tik  ti1
k ,
n1
X
j=0

kij = tik for i  n.

(23)
(24)

The first equation corresponds to the fact that the time between t i1
and tik is accumulated
k
into the  values of process A#i , while the second equation claims that at each switch the
objective time of the system is equal to the sum of the subjective times of each process. For
the sake of uniformity we also denote
n
1
= . . . = 1
= 00 = 0.
1

94

fiOptimal Schedules for Parallelizing Anytime Algorithms

i
By construction of ki we can see, that at time interval [ti1
k , tk ] the subjective time of process
Aj has the following form:
 j
j = 1, . . . , i  1,
 k ,
i1
i
(t  tk ) + k1 , j = i,
(25)
j (t) =
 j
k1 ,
j = i + 1, . . . , n.

The subjective time functions for a system with 3 processes are illustrated in Figure 15.
6 (t)

1 (t)
21
22
23
12

#

11

#

#

#

#

#

#

#

#
2 (t)
3 (t)

13
t01

t11

t21

t31

t12

t22

t32

t

Figure 15: Subjective time functions for a system with 3 processes

To find an optimal schedule for a system with n processes, we need to minimize the
expression given by (6). The only constraints are the monotonicity of the sequence of  for
each process i:
i
ki  k+1
for each k, i.
(26)
Given the expressions for j , we can prove the following lemma:
Lemma 1 For a system of n processes, the expression for the expected cost (6) can be
rewritten as
Z i
n i+n1
 X
Y
X
k
j
(1  Fi (x))dx.
(27)
(1  F#j (k1 ))
Eu (1 , . . . , n , . . .) =
i
k1

k=0 i=1 j=i+1

The proof is given in Appendix A.1.
This lemma makes it possible to prove the chain theorem for an arbitrary number of
processes:
95

fiFinkelstein, Markovitch & Rivlin

l1
l1 , or can be
Theorem 2 (The chain theorem) The value for  m+1
may either be m
computed given the previous 2n  2 values of  using the formula

1

l )
fl (m
l )
 Fl (m

=

l+n1
Y
j=l+1

l1
X

j
(1  F#j (m1
)) 

i+n1
Y

i=ln+1 j=i+1
#j6=l

j
(1  F#j (m
))

l+n1
Y
j=l+1

Z

j
(1  F#j (m
))

i
m+1

i
m

(28)
(1  F#i (x))dx

The proof of the theorem is given in Appendix A.2.
4.4 Optimal Solution for n Processes: an Algorithm
The goal of the presented algorithm is to minimize the expression (27)
Eu (1 , . . . , n , . . .) =

n i+n1
 X
Y
X
k=0 i=1 j=i+1

(1 

j
F#j (k1
))

Z

ki
i
k1

(1  Fi (x))dx

under the constraints
i
ki  k+1
for each k, i.

As in the case of two processes, assume that A 1 acts first. By Theorem 2, given 2n  2
values of 
10 , 11 , . . . , 1n , 21 , 22 , 2n3 ,
we can determine all the possibilities for the value of  2n2 (either 1n2 if the process skips
its turn, or one of the roots of (28)). Given the values up to  2n2 , we can determine the
values for 2n1 , and so on.
The idea of the algorithm is similar to the algorithm for two processes. The first 2n  2
variables (including 10 = 0) determine the tree of possible values for . Optimization over
2n3 first variables, therefore, provides us with an optimal schedule (as before, we compare
the results for the case where the first k < n variables are 0). The only difference from the
case of two processes is that a process may skip its turn. However, we can ignore the case
when all the processes skip their turn, since we can remove such a loop from the schedule.
The scheme of the algorithm is presented in Figure 16, and the description of the main
routine (realized by the DFS Branch and Bound method) is presented in Figure 17.
4.5 Optimal Solution in the Case of Additional Constraints
Assume now that the problem has additional constraints: the solution time is limited by T
and the probability of success of the i-th process p i is not necessarily 1.
It is possible to show that the expressions for the distribution function and the expected
cost have almost the same form as in the regular framework:
Claim 2 Let the system solution time be limited by T , and let p i be the probability of success
for the i-th process. Then the expressions for the goal-time distribution and expected cost
96

fiOptimal Schedules for Parallelizing Anytime Algorithms

Procedure optimize builds n optimal schedules (each process may start first), compares
the results, and returns the best one
procedure optimize
best val  M AX V ALU E
best sequence  
loop for i from 1 to n do
[sequence, val]  minimize sequence by f irst points(i)
if (val < best val) then
best val  val
best sequence  sequence
end
return [best sequence, best val]
end
// Procedure minimize sequence by f irst points gets as a parameter
// the index of a process which starts, and returns an optimal
// sequence and its value
procedure minimize sequence by f irst points(process to start)
loop for i from 0 to n  1
zetas[i]  0
end
loop for i from 1 to process to start  1
zetas[i]  0
end
Using one of the standard minimization methods, find zetas,
minimizing the value of the function build optimal sequence(zetas).
end

Figure 16: An algorithm for finding an optimal schedule for n processes. The result contains the
i mod n
vector of i , such that i = 0i = bi/nc
.

97

fiFinkelstein, Markovitch & Rivlin

procedure build optimal sequence(zetas)
curr cost  calculate cost(zetas)
return df sbnb(zetas, curr cost, M AX V ALU E, 0)
end
procedure df sbnb(zetas, curr cost, thresh, nskip)
if (curr cost  thresh) then
return M AX V ALU E
// Cutoff
// The previous time value for the current process
last value  zetas[length(zetas)  n]
repeat
  calculate next zeta(zetas, last value)
if ( = last value) then
// Skip
break loop
last value  
delta cost  calculate partial cost(zetas, )
// Leaf
if (task accomplished([zetas || ])) then
if (curr cost + delta cost < thresh) then
optimal zetas  [zetas || ]
thresh  curr cost + delta cost
end
break loop
end
tmp result  df sbnb([zetas || ], curr cost + delta cost, thresh, 0)
thresh = min(thresh, tmp result)
end
// repeat
if (nskip < n  1) then
// Skip is possible
zeta  zetas[length(zetas)  n]
tmp result  df sbnb([zetas || ], curr cost, thresh, nskip + 1)
thresh = min(thresh, tmp result)
end
return thresh
end
Figure 17: Procedure build optimal sequence, given the prefix of time sequence, restores the optimal sequence with this prefix using the DFS Branch and Bound search algorithm, and
returns the sequence itself and its value. [x || y] stands for concatenation x and y. The
auxiliary functions used are similar to their counterparts in Figure 12, but deal with n
processes instead of 2.

98

fiOptimal Schedules for Parallelizing Anytime Algorithms

are as follows:
Fn (t, 1 , . . . , n ) = 1 
Eu (1 , . . . , n ) =

Z

T

0

n
Y
i=1

(1  pi Fi (i )) (for t  T ),

u0t +

n
X

i0 u0i

i=1

!

n
Y
i=1

(1  pi Fi (i ))dt.

(29)
(30)

The proof is similar to the proof of Claim 1.
This claim shows that all the formulas used in the previous sections are valid for the
current settings, with three differences:
1. We use pj Fj instead of Fj and pj fj instead of fj .
2. All the integrals are from 0 to T instead of from 0 to .
3. All time variables are limited by T .
The first two conditions may be easily incorporated into all the algorithms. The last condition implies additional changes in the chain theorems and the algorithms. The chain
theorem for n processes now becomes:
j
Theorem 3 The value for kj can either be k1
, or it can be computed given the previous
2n  2 values of  using formula (28), or it can be calculated by the formula

kj

=T 

n1
X

kjl .

(31)

l=1

The first two alternatives are similar to Theorem 2, while the third one corresponds to the
boundary condition given by Equation (24). This third alternative adds one more branch
to the DFS Branch and Bound algorithm; the rest of the algorithm remains unchanged.
Similar changes in the algorithms are performed in the case of the maximal allowed
time Ti per process. In practice, we always use this limitation, setting T i such that the
probability for Ai to reach the goal after Ti , pi (1  Fi (Ti )), becomes negligible.

5. Process Scheduling by Intensity Control
In this section we analyze the problem of optimal scheduling for the case of intensity control, which is equivalent to replacing the binary scheduling functions  i (t) with continuous
functions with a range between 0 and 1. In this paper we assume a linear cost function of
the form (5). We believe, however, that similar analysis is applicable to the setup with any
differentiable u.
It is easy to see that all the formulas for the distribution function and the expected cost
from Claim 1 are still valid under intensity control settings.
For the linear cost function (5), the minimization problem has the form
! n
Z 
n
X
Y
i0
Eu (1 , . . . , n ) =
a+b
(1  Fj (j ))dt  min .
(32)
0

i=1

99

j=1

fiFinkelstein, Markovitch & Rivlin

Without loss of generality, we can assume a + b = 1. This leads to the equivalent minimization problem
! n
Z 
n
Y
X
0
(1  Fj (j ))dt  min,
(33)
i
(1  c) + c
Eu (1 , . . . , n ) =
0

i=1

j=1

where c = b/(a + b) can be viewed as a normalized resource weight. The constraints,
however, are more complicated than for the suspend/resume model:
1. As before, i must be continuous, and i (0) = i0 (0) = 0 (at the beginning all the
processes are idle).
2. We assume i to have a partially-continuous derivative  i0 , and this derivative should
lie between 0 and 1. This requirement follows from the definition of intensity and
the fact that i0 = i : no process can work for a negative amount of time, and no
process can work with the intensity greater than the one allowed. Since we consider
a framework with shared resources, and the total intensity is limited, we have an
additional constraint: the sum of all the derivatives  i0 at any time point cannot
exceed 1.
Thus, this optimization problem has the following boundary conditions:
i (0) = 0, i0 (0) = 0 for i = 1, . . . , n,
0  i0  1 for i = 1, . . . , n,
n
X
0
i0  1.

(34)

i=1

We are looking for a set of functions { i } that provide a solution to minimization
problem (33) under constraints (34).
Let g(t, 1 , . . . , n , 10 , . . . , n0 ) be a function under the integral sign of (33):
! n
n
Y
X
0
0
0
(1  Fj (j )).
(35)
g(t, 1 , . . . , n , 1 , . . . , n ) = (1  c) + c
i
i=1

j=1

A traditional method for solving problems of this type is to use the Euler-Lagrange necessary
conditions: a set of functions 1 , . . . , n provides a weak (local) minimum to the functional
Z 
Eu (1 , . . . , n ) =
g(t, 1 , . . . , n , 10 , . . . , n0 )dt
0

only if 1 , . . . , n satisfy a system of equations of the form
g0 k 

d 0
g 0 = 0.
dt k

(36)

We can prove the following lemma:
Lemma 2 The Euler-Lagrange conditions for minimization problem (33) yield two strong
invariants:
100

fiOptimal Schedules for Parallelizing Anytime Algorithms

1. For processes k1 and k2 for which k1 and k2 are not on the border described by (34),
the distribution and density functions satisfy
fk1 (k1 )
fk2 (k2 )
=
.
1  Fk1 (k1 )
1  Fk2 (k2 )

(37)

2. If the schedules of all the processes are not on the border described by (34), then either
c = 1 or fk (k ) = 0 for each k.
The proof of the lemma is given in Appendix A.3. The above lemma provides necessary
conditions for a local minimum in the inner points described by constraints (34). These
conditions, however, are very restricting. Therefore, we look for more general conditions,
suitable for boundary points as well 7 .
We start with the following lemma:
Lemma 3 If an optimal solution for minimization problem (33) under constraints (34)
exists, then there exists an optimal solution  1 , . . . , n , such that at each time t all the
resources are consumed, i.e.,
n
X
(38)
i0 (t) = 1.
t
i=1

In the case where time cost is not zero (c 6= 1), the equality above is a necessary condition
for solution optimality.
The proof of the lemma is given in Appendix A.4.

Corollary 2 Under intensity control settings, as in the case of suspend-resume settings,
minimization problem (33) has the form (6), i.e.
Eu (1 , . . . , n ) =

Z

0

n
Y

(1  Fj (j ))dt  min .

j=1

Lemma 3 corresponds to our intuition: if a resource is available, it should be used.
Without loss of generality, we restrict our discussion to schedules satisfying (38), even in
the case where time cost is zero. This leads to the following invariant:
t

n
X

i (t) = t.

(39)

i=1

Assume now that we have two F -equivalent processes A 1 and A2 with density function
f (t) satisfying the normal distribution law with mean value m. Let t 1 and t2 be the
cumulative time consumed by each of the processes at time t, i.e.,  1 (t) = t1 and 2 (t) = t2 .
The question is, which process should be active at t (or should they be active in parallel
with partial intensities)?
7. Note also that even if the conditions above hold, they do not necessarily provide the optimal solution.
Moreover, problems in variation calculus do not necessarily have a minimum, since there is no analogue
for the Weierstrass theorem for continuous functions on a closed set.

101

fiFinkelstein, Markovitch & Rivlin

0.4

0.4

0.35

0.35

0.3

0.3

0.25

0.25

0.2

0.2

f(t)

f(t)

Without loss of generality, t1 < t2 , which means that the first process is required to
cover a larger area to succeed: 1  F (t 1 ) > 1  F (t2 ). This supports a policy that at time t
activates the second process. This policy is further supported if A 1 has a lower distribution
density, f1 (t1 ) < f2 (t2 ), as illustrated in Figure 18(a). If, however, the first process has a
higher density, as illustrated in Figure 18(b), it is not clear which of the two processes should
be activated at time t. What is the optimal policy in the general case 8 ? The answer relies

0.15

0.15

0.1

0.1

0.05

0.05

0

0

1

2

t1

3

t2

4

5
t

6

7

8

9

0

10

0

1

2

3

(a)

4

5
t

6

t1

7

t2

8

9

10

(b)

Figure 18: (a) Process A1 (currently at t1 ) has lower density and larger area to cover, and therefore
is inferior. (b) Process A1 has lower density, but smaller area to cover, and the decision
is unclear.

heavily on the functions that appear in (37). These functions, described by the equation
hk (t) =

fk (t)
,
1  Fk (t)

(40)

are known as hazard functions, and they play a very important role in the following theorem
describing necessary conditions for optimal schedules.
Theorem 4 Let the set of functions { i } be a solution of minimization problem (6) under
constraints (34). Let t0 be a point where the hazard functions of all the processes h i (i (t))
are continuous, and let Ak be the process active at t0 (k0 (t0 ) > 0), such that for any other
process Ai
hi (i (t0 )) < hk (k (t0 )).
(41)
Then at t0 process k consumes all the resources, i.e.  k0 (t0 ) = 1.
The proof of the theorem is given in Appendix A.5.
By Theorem 4 and Equation (37), intensity control may only be useful when hazard
functions of at least two processes are equal. However, even in this case the equilibrium
is not always stable. Assume that within some interval [t 0 , t00 ] processes Ai and Aj are
working with partial intensity, which implies h i (i (t)) = hj (j (t)). Assume now that both
8. Analysis of normal distribution given in Section 6.3 shows that the optimal policy in the example above
is to give all the resources to process A2 in both cases.

102

fiOptimal Schedules for Parallelizing Anytime Algorithms

hi (t) and hj (t) are monotonically increasing. If at some moment t we give a priority to one
of the processes, it will obtain a higher value of the hazard function, and will get all the
subsequent resources. The only case of stable equilibrium is when h i (i (t)) and hj (j (t))
are monotonically decreasing functions or constants.
The intuitive discussion above is formulated in the following theorem:
Theorem 5 An active process will remain active and consume all resources as long as its
hazard function is monotonically increasing.
The proof is given in Appendix A.6.
This theorem imply the important corollary:
Corollary 3 If the hazard function of one of the processes is greater than or equal to that
of the others at t = 0 and is monotonically increasing by t, this process should be the only
one to be activated.
We can conclude that the extension of the suspend-resume model to intensity control in
many cases does not increase the power of the model and is beneficial only for monotonically
decreasing hazard functions. If no time cost is taken into account (c = 1), however, the
intensity control permits us to connect the two concepts: that of the model with shared
resources and that of the model with independent agents:
Theorem 6 If no time cost is taken into account (c = 1), the model with shared resources
under intensity control settings is equivalent to the model with independent processes under
suspend-resume control settings. Namely, given a suspend-resume solution for the model
with independent processes, we may reconstruct an intensity-based solution with the same
cost for the model with shared resources and vice versa.
The proof of the theorem is given in Appendix A.7.
Theorem 4 claims that if the process with the maximal value of h k (k (t)) is active, it
will take all the resources. Why, then, would we not always choose the process with the
highest value of hk (k (t)) to be active? It turns out that such a strategy is not optimal.
Let us consider two processes with the distribution densities shown in Figure 19(a). The
corresponding values of the hazard functions are shown in Figure 19(b). If we were using
the above strategy, A2 would be the only active process. Indeed, at time t = 0, h 2 (2 (0)) >
h1 (1 (0)), which would lead to the activation of A 2 . After that moment, A1 would remain
idle and its hazard function remain 0. This strategy would result in an expected time of 2.
If, on the other hand, we would have activated A 1 only, the result would be an expected
time of 1.5. Thus, although h1 (1 (0)) < h2 (2 (0)), it is better to give all the resources to
A1 from the beginning due to its superiority in the future.
A more elaborate example is shown in Figure 20. It corresponds to the case of two
processes that are not F -equivalent, one of which is a linear combination of two normal
distributions, f (t) = 0.5fN (0.6,0.2) (t) + 0.5fN (4.0,2.0) (t), where fN (,) (t) is the distribution
density of normal distribution with mean value  and standard deviation , and the second
process is uniformly distributed in [1.5, 2.5]. Activating A 1 only results in 0.5  0.6 + 0.5 
4.0 = 2.3, activating A2 only results in an expected time of 2.0, while activating A 1 for time
1.2 followed by activating A2 results in (approximately) 0.6  0.5 + (1.2 + 2.0)  0.5 = 1.9.
103

fiFinkelstein, Markovitch & Rivlin

1.2

5

f1(t)
f2(t)

1

h1(t)
h2(t)

4

h(t) (truncated)

f(t)

0.8

0.6

3

2

0.4

1

0.2

0

0

1

2

3

4

0

5

0

1

2

t

3

4

5

t

(a)

(b)

Figure 19: The density function and the hazard function for two processes. Although h 1 (1 (0)) <
h2 (2 (0)), it is better to give all the resources to A1 .

The best solution is, therefore, to start the execution by activating A 1 , and at some point t0
transfer the control to A2 . In this case we interrupt an active process with a greater value
of hazard function, preferring an idle process with a zero value of hazard function (since
h1 (1 (t0 )) > h2 (2 (t0 )) = 0).
1.2

5

f1(t)
f2(t)

1

h1(t)
h2(t)

4

h(t) (truncated)

f(t)

0.8

0.6

3

2

0.4

1

0.2

0

0

2

4

6

8

0

10

t

0

2

4

6

8

10

t

(a)

(b)

Figure 20: The density function and the hazard function for two processes. The best solution is to
start with A1 , and at some point interrupt it in favor of A2 , although the latter has a
zero hazard function.

These examples show that a straightforward use of hazard functions for building optimal
schedules can be very problematic. However, since the suspend-resume model is a specific
case of the intensity control model, the hazard functions still may be useful for understanding
the behavior of optimal schedules, and this is used in the next section.
104

fiOptimal Schedules for Parallelizing Anytime Algorithms

6. Optimal Scheduling for Standard Distributions
In this section we present the results of the optimal scheduling strategy for a system of
processes whose performance profiles meet one of the well-known distributions: uniform,
exponential, normal and lognormal. Then we show the results for processes with bimodal
and multimodal distribution functions.
We have implemented three scheduling policies for two agents:
1. Sequential strategy, which schedules the processes one after another, initiating the
second process when the probability that the first one will find a solution becomes
negligible. For processes that are not F -equivalent, we choose the best order of process
invocation.
2. Simultaneous strategy, which simulates a simultaneous execution of both processes.
3. Optimal strategy, which is an implementation of the algorithm described in Section 4.2.
In the rest of this section we compare these three strategies, when no deadline is given, and
the processes are stopped when the probability that they can still find a solution becomes
negligible.
Our goal is to compare different scheduling strategies and not to analyze the behavior of
the processes. Absolute quantitative measurements, such as average cost, are very process
dependent, and therefore are not appropriate for scheduling strategy evaluation. We therefore would like to normalize the results of the application of different scheduling methods to
minimize the effect of the process behavior. In the case of F -equivalent processes, a good
candidate for the normalization coefficient is the expected time of the individual process.
For processes that are not F -equivalent, however, the decision is not straightforward, and
therefore we use the results of the sequential strategy as the normalization factor.
We define the relative quality qref (S) of strategy S with respect to strategy S ref as
qref (S) = 1 

u(S)
,
u(Sref )

(42)

where u(S) is the average cost of strategy S. This measurement corresponds to the gain
(maybe negative) of strategy S relative to the reference strategy. In this section we use the
sequential strategy as our reference strategy.
6.1 Uniform Distribution
Assume that the goal-time distribution of the processes meets the uniform law over the
interval [t0 , T ], i.e., has distribution functions

if t < t0 ,
 0
(t  t0 )/(T  t0 ) if t  [t0 , T ],
(43)
F (t) =

1
if t > T
and density functions

f (t) =



0
if t 6 [t0 , T ],
1/(T  t0 ) if t  [t0 , T ].
105

(44)

fiFinkelstein, Markovitch & Rivlin

The density function of a process uniformly distributed in [0, 1] is shown in Figure 21(a).
The hazard function of the uniform distribution has the form

if t < t0 ,
 0
1/(T  t0 )
1
h(t) =
(45)
=
if t  [t0 , T ],

1  (t  t0 )/(T  t0 )
T t
which is a monotonically increasing function. By Corollary 3, only one process will be
active, and the optimal strategy should be equivalent to the sequential strategy. If the
processes are not F -equivalent, the problem can be solved by choosing the process with the
minimal expected time.
A more interesting setup involves a uniformly distributed process that is not guaranteed
to find a solution. This case corresponds to a probability of success p that is less than 1. As
it was claimed in Section 4.5, the corresponding distribution and density function should
be multiplied by p. As a result, the hazard function becomes
h(t) =

(

0

p
(T  t0 )  p(t  t0 )

if t < t0 ,
if t  [t0 , T ].

(46)

This function is still monotonically increasing by t, and the conclusions remain the same.
The graphs for hazard functions of processes uniformly distributed in [0, 1] with probability
of success of 0.5, 0.8 and 1 are shown in Figure 21(b).
1.2

10

f(t)

1

h(t) for p = 1.0
h(t) for p = 0.8
h(t) for p = 0.5

8

h(t) (truncated)

f(t)

0.8

0.6

6

4

0.4

2

0.2

0

0

0.2

0.4

0.6
t

0.8

1

0

1.2

0

0.2

0.4

(a)

0.6
t

0.8

1

1.2

(b)

Figure 21: (a) The density function of a process, uniformly distributed in [0, 1], (b) hazard functions
for processes uniformly distributed in [0, 1] with probability of success of 0.5, 0.8 and 1.

6.2 Exponential Distribution
The exponential distribution is described by the density function

0
if t  0
f (t) =
t
e
if t > 0,
106

(47)

fiOptimal Schedules for Parallelizing Anytime Algorithms

and the distribution function has the form

0
if t  0
F (t) =
t
1e
if t > 0.

(48)

Substituting these expressions into (6) gives
Eu (1 , . . . , n ) =

Z

0

n
Y

(1  Fj (j ))dt =

j=1

Z



e

Pn

j=1

j j (t)

dt.

0

For a system with F -equivalent processes, by Lemma 3
n
X

j j (t) = 

n
X

j (t) = t,

j=1

j=1

and therefore
Eu (1 , . . . , n ) =

Z


0

et dt =

1
.


Thus, for a system with F -equivalent processes all the schedules are equivalent. This interesting fact is reflected also in the behavior of the hazard function, which is constant:
h(t)  .
However, if the probability of success is smaller than 1, the hazard function becomes a
monotonically decreasing function:
h(t) =

pet
p
.
=
t
1  p(1  e )
p + (1  p)et

Such processes should work simultaneously (with identical intensities for F -equivalent processes, and with intensities maintaining the equilibrium of hazard functions otherwise), since
each process which has been idle for a while has an advantage over its working teammate.
Figure 22(a) shows the density function of an exponentially distributed process with
 = 1. The graphs for the hazard functions of processes exponentially distributed with
 = 1 and probability of success of 0.5, 0.8 and 1 are shown in Figure 22(b).
Let us consider a somewhat more elaborate example, involving processes that are not
F -equivalent. Assume that we have two learning systems, both with an exponential-like
performance profile typical of such systems. We also assume that one of the systems requires
a delay for preprocessing but works faster. Thus, we assume that the first system has a
distribution density f1 (t) = 1 e1 t , and the second one has a density f2 (t) = 2 e2 (tt2 ) ,
such that 1 < 2 (the second is faster), and t2 > 0 (it also has a delay). Assume that both
learning systems are deterministic over a given set of examples, and that they may fail to
learn the concept with the same probability of 1  p = 0.5. The graphs for the density and
hazard functions of the two systems are shown in Figure 23.
We applied the optimal scheduling algorithm of Section 4.2 for the values  1 = 3,
2 = 10, and t2 = 5. The optimal schedule is to activate the first system for 1.15136 time
units, then (if it found no solution) to activate the second system for 5.77652 time units.
107

fiFinkelstein, Markovitch & Rivlin

1.2

f(t)

1

1

0.8

0.8
h(t) (truncated)

f(t)

1.2

0.6

0.6

0.4

0.4

0.2

0.2

0

0

2

4

6

8

0

10

h(t) for p = 1.0
h(t) for p = 0.8
h(t) for p = 0.5

0

2

4

6

t

t

(a)

(b)

8

10

Figure 22: (a) The density function of a process, exponentially distributed with  = 1, (b) hazard
functions for processes exponentially distributed with  = 1 and probability of success
of 0.5, 0.8 and 1.

5

3

f1(t)
f2(t)

h1(t)
h2(t)

2.5

4

2

f(t)

h(t) (truncated)

3

2

1.5

1

1

0

0.5

0

2

4

6

8

0

10

t

0

2

4

6

8

10

t

(a)

(b)

Figure 23: (a) Density and (b) hazard functions for two exponentially distributed systems, with
different values of  and time shift.

108

fiOptimal Schedules for Parallelizing Anytime Algorithms

Then the first system will run for additional 3.22276 time units, and finally the second
system will run for 0.53572 time units. If at this point no solution has been found, both
systems have failed with a probability of 1  10 6 each.
Figure 24(a) shows the relative quality of the simultaneous and optimal scheduling
strategies as a function of t2 for p = 0.8 (for 10000 simulated examples). For large values
of t2 the benefit of switching from the first algorithm to the second decreases, and this is
reflected in the relative quality of the optimal strategy. The simultaneous strategy, as we
can see, is beneficial only for relatively small values of t 2 .
Figure 24(b) reflects the behavior of the strategies for a fixed value of t 2 = 5.0 as a
function of probability of success p. The simultaneous strategy is inferior, and its quality
decreases while p increases. Indeed, when the probability of success is 1, running the second
algorithm and the first one simultaneously will be a waste of time. On the other hand, the
optimal strategy has a positive benefit, which means that the resulting schedules are not
trivial.
40

40

Optimal
Simultaneous

35

20

30
25

0
Relative quality (in percent)

Relative quality (in percent)

Optimal
Simultaneous

20
15
10
5

-20

-40

-60

0
-5

-80

-10
-15

1

2

3

4

5
6
Delay of the second system

7

8

9

-100
0.1

10

(a)

0.2

0.3

0.4

0.5
0.6
Probability of success

0.7

0.8

0.9

1

(b)

Figure 24: Learning systems: Relative quality of optimal and simultaneous scheduling strategies
(a) as a function of t2 for fixed p = 0.8, and (b) as a function of p for fixed t2 = 5.

6.3 Normal Distribution
The normal distribution with mean value m and deviation  is described by the density
function
(tm)2
1
(49)
f (t) = 
e 22 ,
2
and its distribution function is
Z t
(xm)2
1
F (t) = 
(50)
e 22 dx.
2 
Since we use t0 = 0, we should have used a truncated normal distribution with a distribution
density
(tm)2
1
1
e 22 ,

(1  )
2
109

fiFinkelstein, Markovitch & Rivlin

and a distribution function


Z t
(xm)2
1
1

e 22 dx   ,
 
1
2 
where
1
= 
2

Z

0

e

(xm)2
2 2

dx,



but if m is large enough,  may be considered to be 0. The density function of a normally
distributed process with m = 5 and  = 1 is shown in Figure 25(a).
The hazard function of a normal distribution is monotonically increasing, which leads
to the same conclusions as for a uniform distribution. However, a probability of success of
less than 1 completely changes the behavior of the hazard function: after some point, it
starts to decrease. The graphs for hazard functions of processes normally distributed with
a mean value of 5, standard deviation of 1 and probabilities of success of 0.5, 0.8 and 1 are
shown in Figure 25(b).
0.4

6

f(t)

h(t) for p = 1.0
h(t) for p = 0.8
h(t) for p = 0.5

0.35
5
0.3
4
h(t) (truncated)

f(t)

0.25

0.2

3

0.15
2
0.1
1
0.05

0

0

2

4

6

8

0

10

t

0

2

4

6

8

10

t

(a)

(b)

Figure 25: (a) The density function of a normally distributed process, with m = 5 and  = 1, (b)
hazard functions for normally distributed processes with m = 5 and  = 1, with the
probabilities of success of 0.5, 0.8 and 1.

As in the previous example, we now consider a case of two processes that are not F equivalent, running with the same deviation  = 1 and the same probability of success
p. The first process is assumed to have m 1 = 1, while the second process is started with
some delay m. The relative quality for 10000 simulated examples is shown in Figure 26.
Figure 26(a) shows the relative quality as a function of m for p = 0.8; Figure 26(b) shows
the relative quality as a function of p for m = 2. Unlike exponential distribution, the gain
for this example for the optimal strategy is rather small.
6.4 Lognormal Distribution
The random variable X is lognormally distributed, if ln X is normally distributed. The
density function and the distribution function with the corresponding parameters m and 
110

fiOptimal Schedules for Parallelizing Anytime Algorithms

10

20

Optimal schedule
Simultaneous

0

Relative quality (in percent)

Relative quality (in percent)

0

-10

-20

-30

-40

-50

Optimal schedule
Simultaneous

-20

-40

-60

-80

0

1

2

3
4
Delay of the second process

5

-100
0.1

6

0.2

0.3

(a)

0.4

0.5
0.6
Probability of success

0.7

0.8

0.9

1

(b)

Figure 26: Normal distribution: relative quality (a) as a function of m for fixed p = 0.8, and (b)
as a function of p for fixed m = 2.

can be written as
(log(t)m)2
1
e 22 ,
f (t) = 
t 2
Z log(t)
(xm)2
1
e 22 dx.
F (t) = 
2 

(51)
(52)

Lognormal distribution plays a significant role in AI applications since in many cases search
time is distributed under the lognormal law. The density function of the lognormal distribution with mean value of log(5.0) and standard deviation of 1.0 is shown in Figure 27(a),
and the hazard functions for different values of p are shown in Figure 27(b). Let us consider
a simulated experiment similar to its analogue for normal distribution. We consider two
processes that are not F -equivalent, with the parameters  = 1 and the same probability of
success p. The first process is assumed to have m 1 = 1, while the second process is started
with some delay, such that m2  m1 = m > 0. The relative quality for 10000 simulated
examples is shown in Figure 28. Figure 28(a) shows the relative quality as a function of
m for p = 0.8; Figure 28(b) shows the relative quality as a function of p for m = 2. The
graphs show that for small values of m both the optimal and the simultaneous strategy
have a significant benefit over the sequential one. However, for larger values, the performance of the optimal strategy approaches the performance of the sequential strategy, while
the simultaneous strategy becomes inferior.
6.5 Bimodal and Multimodal Density Functions
Experiments show that in the case of F -equivalent processes with a unimodal distribution
function, the sequential strategy is often optimal. In this section we consider less trivial
distributions.
111

fiFinkelstein, Markovitch & Rivlin

0.14

0.18

f(t)

h(t) for p = 1.0
h(t) for p = 0.8
h(t) for p = 0.5

0.16

0.12

0.14
0.1

h(t) (truncated)

0.12

f(t)

0.08

0.06

0.1
0.08
0.06

0.04
0.04
0.02

0

0.02

0

5

10

15

20

25
t

30

35

40

45

0

50

0

5

10

15

(a)

20

25
t

30

35

40

45

50

(b)

Figure 27: (a) Density function for lognormal distribution with mean value of log(5.0) and standard
deviation of 1.0 and (b) hazard functions for lognormally distributed processes with
mean value of log(5.0), standard deviation of 1, and the probabilities of success of 0.5,
0.8 and 1.

60

40

Optimal schedule
Simultaneous

Optimal schedule
Simultaneous

50
20

Relative quality (in percent)

Relative quality (in percent)

40

30

20

10

0

-20

-40

0
-60
-10

-20

0

0.5

1

1.5
2
2.5
Delay of the second process

3

3.5

-80
0.1

4

(a)

0.2

0.3

0.4

0.5
0.6
Probability of success

0.7

0.8

0.9

1

(b)

Figure 28: Lognormal distribution: relative quality (a) as a function of m for fixed p = 0.8, and
(b) as a function of p for fixed m = 2.

112

fiOptimal Schedules for Parallelizing Anytime Algorithms

Assume first that we have a non-deterministic algorithm with a performance profile
expressed by a linear combination of two normal distributions with the same deviation:
f (t) = 0.5fN (1 ,) + 0.5fN (2 ,) .
An example of the density and hazard functions of such distributions with  1 = 2, 2 = 5,
and  = 0.5 is given in Figure 29.
0.35

0.9

f(t)

h(t) for p = 1.0

0.8

0.3

0.7
0.25

h(t) (truncated)

0.6

f(t)

0.2

0.15

0.5
0.4
0.3

0.1
0.2
0.05

0

0.1

0

1

2

3

4
t

5

6

7

0

8

0

1

2

3

4
t

(a)

5

6

7

8

(b)

Figure 29: (a) Density function and (b) hazard function for a process distributed according to
the density function f (t) = 0.5fN (2,0.5) + 0.5fN (5,0.5) with the probability of success of
p = 0.8.

Assume that we invoke two runs of this algorithm with fixed values of  1 = 2,  = 0.5,
and p = 0.8, and the free variable 2 . Figure 30 shows how the relative quality of the
scheduling strategies is influenced by the distance between the peaks,  2  1 . The results
correspond to the intuitive claim that the larger distance between the peaks, the more
attractive the optimal and the simultaneous strategies become.
25

Optimal
Simultaneous

20

Relative quality (in percent)

15
10
5
0
-5
-10
-15
-20
-25

0

2

4

6

8

10

12

14

16

Distance between the peaks

Figure 30: Bimodal distribution: relative quality as a function of the distance between the peaks.

113

fiFinkelstein, Markovitch & Rivlin

Now let us see how the number of peaks of the density function affects the scheduling
quality. We consider a case of partial uniform distribution, where the density is distributed
over k identical peaks of length 1 placed symmetrically in the time interval from 0 to 100.
(Thus, the density function will be equal to 1/k when t belongs to one of such peaks, and
0 otherwise.) In this experiment we have chosen p = 1.
Figure 31 shows the relative quality of the system as a function of k, obtained for
10000 randomly generated examples. We can see from the results, that the simultaneous
strategy is inferior, due to the valleys in the distribution function. The optimal strategy
returns schedules where the processes switch after each peak, but the relative quality of the
schedules decreases as the number of peaks increases.
50

Optimal
Simultaneous

40

Relative quality (in percent)

30

20

10

0

-10

-20

-30

2

3

4

5

6
Number of peaks

7

8

9

10

Figure 31: Multimodal distribution: relative quality as a function of the number of peaks.

7. Experiments: Using Optimal Scheduling for the Latin Square Problem
To test the performance of our algorithm in a realistic domain, we applied it to the Latin
Square problem described in Section 2.2. We assume that we are given a Latin Square
problem with two initial configurations, and a fully deterministic algorithm with distribution
function and distribution density shown in Figure 7.
We compare the performance of the schedule produced by our algorithm to the performance of the sequential and simultaneous strategies described in Section 6. In addition,
we test a schedule which runs the processes one after another, allowing a single switch at
the optimal point (an analogue of the restart technique for two processes). We refer to this
schedule as a single-point restart schedule.
Note that the case of two initial configurations corresponds to the case of two processes
in our framework. In general, we could think of a set of n initial configurations that would
correspond to n processes. For sufficiently large n, the restart strategy where each restart
starts with a different initial configuration, becomes close to optimal.
Our experiments were performed for different values of N , with 10% of the square precolored. The performance profile was induced based on a run of 50, 000 instances, and the
remaining 50, 000 instances were used as 25, 000 testing pairs. All the schedules were applied
114

fiOptimal Schedules for Parallelizing Anytime Algorithms

with a fixed deadline T , which corresponds to the maximal allowed number of generated
nodes.
Since the results of the sequential strategy in this type of problems are much worse
than the results of other strategies for sufficiently large values of T , we instead used the
simultaneous strategy as the reference in the relative quality measure.
30

Optimal
Single-point restart

25

Relative quality (in percent)

20
15
10
5
0
-5
-10
-15

0

5000

10000

15000

20000
25000
30000
Maximal available time

35000

40000

45000

50000

Figure 32: Relative quality as a function of maximal allowed time T

Figure 32 shows how maximal available time T (the x axis) influences the quality of the
schedules (the y axis), where the simultaneous strategy has been used as a reference.
For small values of T , both single-point restart and the optimal strategy have about a
25% gain over the simultaneous strategy, since they produce schedules which are close to
the sequential one. However, when available time T increases, the benefit of parallelization
becomes more significant, and the simultaneous strategy overcomes the single-point restart
strategy. The relative quality of the optimal schedule also decreases when T increases, since
the resulting schedule contains more switches between the two problem instances being
solved.
Figure 33 illustrates how the optimal and single-point restart schedules relate to the
simultaneous schedule for different size Latin Squares (given T = 25, 000). The initial gain
of both strategies is about 50%. However, for the problems with N = 20 the single-point
restart strategy becomes worse than the simultaneous one. For larger sizes the probability
of solving the Latin Square problem with a time limit of 25, 000 steps becomes smaller and
smaller, and the benefit of the optimal strategy also approaches zero.
115

fiFinkelstein, Markovitch & Rivlin

50

Optimal
Single-point restart

Relative quality (in percent)

40

30

20

10

0

-10

5

10

15

20
Lain Square size

25

30

35

Figure 33: Relative quality as a function of the size of the Latin Square

8. Combining Restart and Scheduling Policies
Luby, Sinclair, and Zuckerman (1993) showed that the restart strategy is optimal if an
infinite number of identical runs are available. When this number is limited, the restart
strategy is not optimal. Sometimes, however, we have a mixed situation. Assume that we
have two initial states, a non-deterministic algorithm, and a linear time cost. On one hand,
we can perform restarts of a run corresponding to one of the initial states. On the other
hand, we can switch between the runs corresponding to the two initial states. What would
be an optimal policy in this case?
The expected time of a run based on a single initial state is
1
E(t ) =
F (t )


Z

t

0

(1  F (t))dt,

(53)

where t is the restart point and F (t) is the distribution function. This formula is obtained
by a simple summation of the geometric series with coefficient 1F (t  ), and is a continuous
form of the formula given by Luby, Sinclair, and Zuckerman (1993). Minimization of (53)
by t gives us the optimal restart point.
Assume first that the sequence of restarts on a single initial state is a process interruptible only at the restart points. Since the probability of failure of i successive restarts
is (1  F (t ))i , this process is exponentially distributed. Thus, the problem is reduced to
scheduling of two exponentially distributed processes. According to the analysis in Section 6.2, all schedules are equivalent if the problems corresponding to the two initial states
116

fiOptimal Schedules for Parallelizing Anytime Algorithms

are solvable. Otherwise, the optimal policy is to alternate between the two processes at
each restart point.
A more interesting case is when we allow rescheduling at any time point. In general,
it is not beneficial to switch between the processes in non-restart points (otherwise these
rescheduling points would have been chosen for restart). Such rescheduling, however, can
be beneficial if the cost associated with restarts is higher than the rescheduling cost 9 .
Let us assume that each restart has a constant cost C. Similarly to (53), we can write
the expected cost of a policy performing restarts at point t  as
E(t ) =

1
F (t )

Z

0

t

(1  F (t))dt +

1  F (t )
C,
F (t )2

(54)

where the second term corresponds to the series
0 + C(1  F (t )) + 2C(1  F (t ))2 + . . .
Let t and t be the optimal restart points for the setups with and without associated costs
respectively. t should be greater than t due to the restart cost.
Let us consider the following schedule: the first process runs for t  , then the second
process runs for t , then the first process runs (with no restart) for additional t   t , then
the second process runs for additional t   t . Then the first process restarts and runs for
t and so forth.
Let us compare the expected time of such schedule with the time of the pure restart
policy, where the first process runs for t  , then the second process runs for t , then the
first process restarts and runs for t  and so forth.
Similarly to (15), the expected time of the first schedule in the interval [0, 2t  ] can be
written as
Z

t



(1  F (t))dt + (1  F (t ))

Z

t

(1  F (t))dt+
Z t
Z t


(1  F (t ))
(1  F (t))dt + (1  F (t ))
(1  F (t))dt.

Esched =

0

0

t

t

On the other hand, the expected time of the second schedule in the same interval is

Esimple =

Z

t

(1  F (t))dt + (1  F (t ))
0
Z t
(2  F (t ))
(1  F (t))dt.

Z

t
0

(1  F (t))dt =

0

9. An example for such setup is robotic search, where returning the robot to the initial position is more
expensive than suspending and resuming the robot.

117

fiFinkelstein, Markovitch & Rivlin

Esched can be rewritten as
Z t
Z t

(1  F (t))dt+
(1  F (t))dt + (1  F (t ))
Esched =
0
0
Z t
Z t
(1  F (t))dt =
(1  F (t))dt  (1  F (t ))
(1  F (t ))
0
0
Z t
Z t



(1  F (t))dt =
(1  F (t))dt + (2  F (t )  F (t ))
F (t )
0
0
Z t
Z t


(1  F (t))dt + Esimple .
(1  F (t))dt  F (t )
F (t )
0

0

Thus, we obtain


Esimple  Esched = F (t )
F (t )F (t )

1
F (t )

Z

0

Z

t
0



(1  F (t))dt  F (t )

t

(1  F (t))dt 

1
F (t )

Z

t
0

Z

t
0

(1  F (t))dt =
!

(1  F (t))dt ,

and since t provides minimum for (53), the last expression is positive, which means that
scheduling improves a simple restart policy.
Note, that we do not claim that the proposed scheduling policy is optimal  our example
just shows that the pure restart strategy is not optimal. There should be an optimal
combination interleaving restarts on the global level and scheduling on the local level, but
finding this combination is left for future research.

9. Conclusions
In this work we present an algorithm for optimal scheduling of anytime algorithms with
shared resources. We first introduce a formal framework for representing and analyzing
scheduling strategies. We begin by analyzing the case where the only allowed scheduling operations are suspending and resuming processes. We prove necessary conditions for
schedule optimality and present an algorithm for building optimal schedules that is based on
those conditions. We then analyze the more general case where the scheduler can increase or
decrease the intensity of the scheduled processes. We prove necessary conditions and show
that intensity control is only rarely needed. We then analyze, theoretically and empirically,
the behavior of our scheduling algorithm for various distribution types. Finally, we present
empirical results of applying our scheduling algorithm to the Latin Square problem.
The results show that the optimal strategy indeed outperforms other scheduling strategies. For lognormal distribution, we showed an improvement of more than 50% over the
naive sequential strategy. In general, our algorithm is particularly beneficial for heavy-tailed
distributions, but even for exponential distribution we show a benefit of more than 35%.
In some cases, however, simple scheduling strategies yield results similar to those obtained by our algorithm. For example, the optimal schedule for uniform distribution is to
apply one of the processes with no switch. When the probability to succeed within the given
time limit approaches 1, this simple scheduling strategy also becomes close to optimal, at
118

fiOptimal Schedules for Parallelizing Anytime Algorithms

least for unimodal distributions with no strong skew towards zero. On the other hand,
when the probability of success approaches zero, another simple strategy that applies the
processes simultaneously becomes close to optimal.
Such a behavior meets the intuition. For heavy-tailed distributions, switching between
the runs is promising because the chance to be on a bad trajectory is high enough. The
same is correct for distributions with low probability of success. However, if the probability
to be on a bad trajectory is too high, the best strategy is to switch between the runs as
fast as possible, which is equivalent to the simultaneous strategy. On the other hand, if the
distribution is too skewed to the right, often there is no sense to switch between the runs,
since the new run should pay a high penalty before it reaches the promising distribution
area. In general, when the user is certain that the particular application falls under one of
the categories above, the cost of calculating the optimal schedule can be saved.
The high complexity of computation is one of the potential weaknesses of the presented
algorithm. This complexity can be represented as a multiplication of three factors: function
minimization, Branch-and-Bound search, and solving Equations (18) and (19) for the case
of two agents or Equation (28) for the general case. For two agents, the only exponential component is the Branch-and-Bound search. We found, however, that in practice the
branching factor, which is roughly the number of roots of the equations above, is rather
small, while the depth of the search tree can be controlled by iterative-deepening strategies.
For an arbitrary number of agents, function minimization may also be exponential. In practice, however, it depends on the behavior of the minimized function and the minimization
algorithm.
Since the optimal schedule is static and can be applied to a large number of problem
instances, its computation is beneficial even when associated with high cost. Moreover, in
some applications (such as robotic search) the computational cost can be outweighed by
the gain obtained from a single invocation.
The previous work most related to our research is the restart framework (Luby et al.,
1993). The most important difference between our algorithm and the restart policy is the
ability to handle the cases where the number of runs is limited, or where different algorithms
are involved. When only one algorithm is available and the number of runs is infinite, the
restart strategy is optimal. However, as we have shown in Section 8, some problems may
benefit from the combination of these two approaches.
Our algorithm assumes the availability of the performance profiles of the processes. Such
performance profiles can be derived analytically using theoretical models of the processes
or empirically from previous experience with solving similar problems. Online learning of
performance profiles, which could expand the applicability of the proposed framework, is a
subject of ongoing research.
The framework presented here can be used for a wide range of applications. In the introduction we presented three examples. The first example describes two alternative learning
algorithms working in parallel. The behavior of such algorithms is usually exponential, and
the analysis for such setup is given in Section 6.2. The second example is a CSP problem
with two alternative initial configurations, which is analogous to the Latin Square example
of Sections 2.2 and 7. The last example includes crawling processes with a limited shared
bandwidth. Unlike the first two examples, this setup falls under the framework of intensity
control described in Section 5.
119

fiFinkelstein, Markovitch & Rivlin

Similar schemes may be applied for more elaborate setups:
 Scheduling a system of n anytime algorithms, where the overall cost of the system is
defined as the maximal cost of its components (unlike the analysis in Section 4, this
function is not differentiable);
 Scheduling with non-zero process switch costs;
 Providing dynamic scheduling algorithms able to handle changes in the environment;
 Building effective algorithms for the case of several resources of different types, e.g.,
multiprocessor systems.

Appendix A. Formal Proofs
A.1 Proof of Lemma 1
The claim of the lemma is as follows:
For a system of n processes, the expression for the expected cost (6) can be rewritten as
Z i
 X
n i+n1
X
Y
k
j
(1  Fi (x))dx.
(55)
Eu (1 , . . . , n , . . .) =
))
(1  F#j (k1
i
k1

k=0 i=1 j=i+1

i
Proof: Splitting the whole integration range [0, ) to the intervals [t i1
k , tk ] yields
the following expression:
Z Y
n
n Z ti Y
 X
n
X
k
(1  Fj (j ))dt.
(56)
(1  Fj (j ))dt =
Eu (1 , . . . , n ) =
0

k=0 i=1

j=1

ti1
k
j=1

By (25), we can rewrite the inner integral as
Z ti Y
n
k
(1  Fj (j )) =
ti1
k
j=1

Z

tik
ti1
k




i1
Y

i1
Y

j=1

j=i+1n

i
)) 
(1  Fj (kj ))  (1  Fi (t  ti1
+ k1
k

(1  F#j (kj ))

Z

tik
ti1
k

n
Y

j=i+1



j
(1  Fj (k1
)) dt =

(57)

i
+ k1
))dt.
(1  Fi (t  ti1
k

i
Substituting x for t  ti1
+ k1
and using (23), we obtain
k
Z ti
i1
Y
k
j
i
(1  F#j (k ))
(1  Fi (t  ti1
+ k1
))dt =
k
ti1
k

j=i+1n
i+n1
Y
j=i+1

i+n1
Y
j=i+1

(1 

j
F#j (k1
))

(1 

j
F#j (k1
))

Z

Z

i
tik ti1
k +k1

i
k1

ki
i
k1

(1  Fi (x))dx =

(1  Fi (x))dx.

120

(58)

fiOptimal Schedules for Parallelizing Anytime Algorithms

Combining (56), (57) and (58) gives us (55).
Q.E.D.

A.2 Proof of the Chain Theorem for n Processes
The chain theorem claim is as follows:
l1
l1 , or can be computed given the previous 2n  2
The value for m+1
may either be m
values of  using the formula

1

l )
fl (m
l )
 Fl (m

l+n1
Y
j=l+1

=

l1
X

j
(1  F#j (m1
)) 

i+n1
Y

i=ln+1 j=i+1
#j6=l

l+n1
Y

j
(1  F#j (m
))

j=l+1

Z

j
(1  F#j (m
))

i
m+1

i
m

(59)
(1  F#i (x))dx

Proof: By Lemma 1, the expression we want to minimize is described by the equation
Eu (1 , . . . , n , . . .) =

 X
n i+n1
Y
X
k=0 i=1 j=i+1

j
(1  F#j (k1
))

Z

ki
i
k1

(1  Fi (x))dx.

(60)

The expression above reaches its optimal values either when
dEu
= 0 for j = 1, . . . , n, . . . ,
dj

(61)

or on the border described by (26).
Reaching the optimal values on the border corresponds to the first alternative described
in the theorem. Let us now consider a case when the derivative of E u by j is 0.
l , where 0  l  n  1. Let us see which
Each variable j may be presented as mn+l = m
l is participating in.
summation terms of (60) m
l may be a lower bound of the integral from (60). This happens when k = m + 1
1. m
and i = l. The corresponding term is

S0 =

l+n1
Y
j=l+1

and

(1 

j
F#j (m
))

Z

l
m+1
l
m

(1  Fl (x))dx,

l+n1
Y
dS0
l
j
=
(1

F
(
))

(1  F#j (m
)).
l m
l
dm
j=l+1

l may be an upper bound of the same integral, which happens when k = m and
2. m
i = l. The corresponding term is

Sl =

l+n1
Y
j=l+1

(1 

j
F#j (m1
))

121

Z

l
m
l
m1

(1  Fl (x))dx,

fiFinkelstein, Markovitch & Rivlin

and

l+n1
Y
dSl
j
l
(1  F#j (m1
)).
=
(1

F
(
))

l m
l
dm
j=l+1

l may participate in the product
3. Finally, m
i+n1
Y
j=i+1

j
(1  F#j (k1
)).

For i = 1 . . . l  1, this may happen when k = m + 1 and j = l, and the corresponding
term is
Z i
i+n1
Y
m+1
j
(1  Fi (x))dx,
(1  F#j (m ))
Si =
i
m

j=i+1

with the derivative

dSi
l
= fl (m
)
l
dm

i+n1
Y

(1 

j
F#j (m
))

j=i+1,#j6=l

Z

i
m+1
i
m

(1  Fi (x))dx.

For i = l + 1 . . . n, k = m and j = l + n. The corresponding term is
Si =

i+n1
Y

(1

j=i+1

j
 F#j (m1
))

Z

i
m
i
m1

(1  Fi (x))dx,

with the derivative
dSi
l
= fl (m
)
l
dm

i+n1
Y

j=i+1,#j6=l

(1 

j
F#j (m1
))

Z

i
m
i
m1

(1  Fi (x))dx.

l appears only in the integral, there is no other possibility for  l to appear
Since for i = l, m
m
in the expression, and therefore
n
dEu X dSi
=
.
l
l
dm
dm
i=0

The right-hand side of the sum above can be written as follows:
n
X
dSi
=
l
dm
i=0

(1 
l1
X

l
Fl (m
))

j=l+1

l
fl (m
)

i=1

n
X

i=l+1

l+n1
Y

i+n1
Y

(1 

j=i+1,#j6=l
l
fl (m
)

i+n1
Y

j
F#j (m
))

j
(1  F#j (m
))

(1 

l+n1
Y

j
(1  F#j (m1
)) 

+ (1 

l
Fl (m
))

Z

(1  Fi (x))dx 

i
m+1
i
m

j
F#j (m1
))

j=i+1,#j6=l

122

Z

j=l+1

i
m
i
m1

(1  Fi (x))dx.

(62)

fiOptimal Schedules for Parallelizing Anytime Algorithms

However,
n
X

i+n1
Y

j
F#j (m1
))

(1 

i=l+1 j=i+1,#j6=l
0
X

i+n1
Y

(1 

j
F#j (m
))

i=ln+1 j=i+1,#j6=l

Z

i
m
i
m1

Z

(1  Fi (x))dx =

i
m+1
i
m

(1  Fi (x))dx.

(63)

Substituting (63) into (62), we obtain
n
X
dSi
=
l
dm
i=0



l
(1  Fl (m
)) 
l
fl (m
)

l1
X

l+n1
Y
j=l+1

j
(1  F#j (m1
)) 

i+n1
Y

(1 

j
F#j (m
))

i=ln+1 j=i+1,#j6=l

l+n1
Y
j=l+1

Z



j
(1  F#j (m
)) 

i
m+1
i
m

(1  Fi (x))dx.

(64)

l ) were 0, that would mean that the goal has been reached with the probability
If 1  Fl (m
of 1, and further scheduling would be redundant. Otherwise, expression in (64) is 0 when

1

l )
fl (m
l )
 Fl (m

l+n1
Y
j=l+1

=

l1
X

(1 

j
F#j (m1
))

i+n1
Y

(1

i=ln+1 j=i+1,#j6=l



l+n1
Y
j=l+1

j
 F#j (m
))

Z

j
(1  F#j (m
))

i
m+1

i
m

,
(1  F#i (x))dx

which is equivalent to (59).
l1
l+1
= n(m+1)+l1 ),
= n(m1)+l+1 to m+1
Equation (59) includes 2n  1 variables ( m1
l1
providing an implicit dependency of  m+1 on the remaining 2n  2 variables.
Q.E.D.

A.3 Proof of Lemma 2
The claim of the lemma is as follows:
The Euler-Lagrange conditions for the minimization problem (33) yield two strong invariants:
1. For processes k1 and k2 for which k1 and k2 are not on the border described by (34),
the distribution and density functions satisfy
fk2 (k2 )
fk1 (k1 )
=
.
1  Fk1 (k1 )
1  Fk2 (k2 )
123

(65)

fiFinkelstein, Markovitch & Rivlin

2. If the schedules of all the processes are not on the border described by (34), then either
c = 1 or fk (k ) = 0 for each k.
Proof: Let g(t, 1 , . . . , n , 10 , . . . , n0 ) be the function under the integral sign of (33):
g(t, 1 , . . . , n , 10 , . . . , n0 )

(1  c) + c

=

n
X

i0

i=1

!

n
Y

(1  Fj (j )).

(66)

j=1

A necessary condition of Euler-Lagrange claims that a set of functions  1 , . . . , n provides
a weak (local) minimum to the functional
Eu (1 , . . . , n ) =

Z



0

g(t, 1 , . . . , n , 10 , . . . , n0 )dt

only if these functions satisfy a system of equations of the form
g0 k 

d 0
g 0 = 0.
dt k

In our case,
g0 k
and

=  (1  c) + c

n
X

i0

i=1

!

fk (k )

(67)

Y

j6=k

(1  Fj (j )),

n
n
X
Y
d Y
d 0
g 0 = c
(1  Fj (j )) = c
l0 fl (l ) (1  Fj (j )).
dt k
dt
j=1

l=1

(68)

(69)

j6=l

Substituting the last expression into (67), we obtain
g0 1

=

g0 2

= ... =

g0 n

= c

n
X

l0 fl (l )

l=1

Y
j6=l

(1  Fj (j )),

and by (68) for every k1 and k2
fk1 (k1 )

Y

j6=k1

(1  Fj (j )) = fk2 (k2 )

Y

j6=k2

(1  Fj (j )).

We can ignore the case where one of the terms 1  F j (j ) is 0. Indeed, this is possible only
if the goal is reached by process j with probability of 1, and in this case no optimization is
needed. Therefore, we obtain
fk1 (k1 )(1  Fk2 (k2 )) = fk2 (k2 )(1  Fk1 (k1 )),
which is equivalent to (65).
124

(70)

fiOptimal Schedules for Parallelizing Anytime Algorithms

Let us show now the correctness of the second invariant. By (69) and (65), we obtain
n

X
Y
d 0
l0 fl (l ) (1  Fj (j )) =
g 0 =  c
dt k
c

l=1
n
X

l=1
n
X

j6=l

l0

n
fl (l ) Y
(1  Fj (j )) =
1  Fl (l )
j=1
n
Y

fk (k )
(1  Fj (j )) =
1  Fk (k )
j=1
l=1
!
n
X
Y
i0 fk (k )
c
(1  Fj (j )).
c

l0

i=1

j6=k

By (36) we get
g0 k

d
 g0 0 = 
dt k
+c

(1  c) + c
n
X
i=1

i0

!

n
X

i0

i=1

fk (k )

 (1  c)fk (k )

Y

!

fk (k )

Y

j6=k

j6=k

Y

j6=k

(1  Fj (j ))

(1  Fj (j )) =

(1  Fj (j )) = 0.

Since we ignore the case when (1  Fj (j )) = 0, the second invariant is correct.
Q.E.D.

A.4 Proof of Lemma 3
The claim of the lemma is as follows:
If an optimal solution exists, then there exists an optimal solution  1 , . . . , n , such that
at each time t all the resources are consumed, i.e.,
t

n
X

i0 (t) = 1.

i=1

In the case where time cost is not zero (c 6= 1), the equality above is a necessary condition
for solution optimality.
Proof: We know that {i } provide a minimum for the expression (33)
! n
Z 
n
X
Y
i0
(1  c) + c
(1  Fj (j ))dt.
0

i=1

j=1

Let us assume that in some time interval [t 0 , t1 ], {i } do not satisfy the lemmas constraints.
However, it is possible to use the same amount of resources more effectively. Let us consider
125

fiFinkelstein, Markovitch & Rivlin

a linear time warp (t) = t +  on the time interval [t 0 , t1 ], satisfying (t0 ) = t0 . From
the last condition, it follows that  = t 0 (1  ). Let t01 be a point where (t) achieves t1 ,
i.e., t01 = t0 + (t1  t0 )/. Let us consider a set of new objective schedule functions 
e i (t) of
the form

t  t0 ,
 i (t),
i (t + ),
t0  t  t01 ,

ei (t) =

i (t + t1  t01 ), t > t01 .
Thus, 
ei (t) behaves as i (t) before t0 , as i (t) with a time shift after t01 , and as a linearly
speeded up version of i (t) in the interval [t0 , t01 ]. Since (t0 ) = t0 and (t01 ) = t1 , 
ei (t) is
continuous at the points t0 and t01 .

ei0 (t) is equal to i0 (t) within the interval [t0 , t1 ], and to i0 (t) outside this interval. By
the contradiction assumption, i do not meet the lemma constraints in [t 0 , t1 ], and thus we
can take
1
P
> 1,
=
maxt[t0 ,t1 ] ni=1 i0 (t)

leading to valid functions 
ei0 (t). Using 
ei (t) in (33), we obtain
! n
Z 
n
X
Y
0

ei (t)
(1  c) + c
(1  Fj (e
j (t)))dt =
Eu (e
1 , . . . , 
en ) =
0

Z

i=1

t0

(1  c) + c

0

Z

t01

t0

Z

(1  c) + c

i0 (t)

i=1

(1  c) + c



t01

n
X

n
X

!

i0 (t

j=1

n
Y

j=1

(1  Fj (j (t)))dt +

+ )

i=1

n
X

i0 (t

i=1

+ t1 

!

n
Y

(1  Fj (j (t + )))dt +

j=1

t01 )

!

n
Y

(1  Fj (j (t + t1  t01 )))dt.

j=1

By substituting x = t +  in the second term of the last sum, and x = t + t 1  t01 in the
third term, we obtain
! n
Z t0
n
X
Y
Eu (e
1 , . . . , 
en ) =
(1  c) + c
i0 (t)
(1  Fj (j (t)))dt +
0

Z

i=1

t1

t0

Z



t1

1c
+c


n
X

i0 (x)

i=1

(1  c) + c

Eu (1 , . . . , n ) 

n
X

i0 (x)

i=1

Z

t1

t0

!

j=1

n
Y

(1  Fj (j (x)))dx +

j=1

!

n
Y

j=1

(1  Fj (j (x)))dx =



1
(1  c) 1 


Since  > 1, the last term is non-negative, and therefore
Eu (e
1 , . . . , 
en )  Eu (1 , . . . , n ),
126

Y
n

(1  Fj (j ))dt.

j=1

fiOptimal Schedules for Parallelizing Anytime Algorithms

meaning that the set {e
i } provides a solution of at least the same quality as { i }. If c 6= 1,
this contradicts to the optimality of the original schedule, and if c = 1, the new schedule
will also be optimal.
Q.E.D.

A.5 Proof of Theorem 4
The claim of the theorem is as follows:
Let the set of functions {i } be a solution of minimization problem (6) under constraints (34). Let t0 be a point where the hazard functions of all the processes h i (i (t)) are
continuous, and let Ak be the process active at t0 (k0 (t0 ) > 0), such that for any other
process Ai
hi (i (t0 )) < hk (k (t0 )).
(71)
Then at t0 process k consumes all the resources, i.e.  k0 (t0 ) = 1.
Proof: First we want to prove the theorem for the case of two processes, and then
to generalize the proof to the case of n processes. Assume that  1 (t) and 2 (t) provide the
optimal solution, and at some point t 0 10 (t0 ) > 0 and
f2 (2 (t0 ))
f1 (1 (t0 ))
>
.
1  F1 (1 (t0 ))
1  F2 (2 (t0 ))

(72)

From the continuity of the functions h i (t) in the point t0 , it follows that there exists some
neighborhood U (t0 ) of t0 , such that for each two points t0 , t00 in this neighborhood h1 (t0 ) >
h2 (t00 ), i.e.,
f1 (1 (t0 ))
f2 (2 (t00 ))
min
>
max
.
(73)
t0 U (t0 ) 1  F1 (1 (t0 ))
t00 U (t0 ) 1  F2 (2 (t00 ))
Let us consider some interval [t0 , t1 ]  U (t0 ). In order to make the proof more readable, we
introduce the following notation (for this proof only):
 We denote 1 (t) by (t). By Lemma 3, 2 (t) = t  (t).
 We denote (t0 ) by  0 and (t1 ) by  1 .
In the interval [t0 , t1 ] the first process obtains  1   0 resources, and the second process
obtains (t1 t0 )( 1  0 ) resources. Let us consider a special resource distribution 
e, which
first gives all the resources to the first process, and then to the second process, keeping the
same quantity of resources as :

(t),
t  t0 ,



t  t 0 +  0 , t0  t  t 0 +  1   0 ,

e(t) =
1 ,
t0 +  1   0  t  t 1



(t),
t  t1 .

It is easy to see that (t) is continuous at the points t 0 , t1 , and t0 +  1   0 . We want to
show that, unless the first process consumes all the resources at the beginning, the schedule
produced by 
e outperforms the schedule produced by .
127

fiFinkelstein, Markovitch & Rivlin

Let t = t0 +  1   0 , which corresponds to the time when the first process would have
consumed all its resources had it been working with the maximal intensity. First, we want
to show that in the interval [t0 , t ]
(1  F1 ((t)))(1  F2 (t  (t)))  (1  F1 (t  t0 +  0 ))(1  F2 (t0   0 )).

(74)

(t) = (t  t0 +  0 )  (t).

(75)

Let
The inequality (74) becomes
(1F1 (tt0 + 0 (t)))(1F2 (t0  0 +(t)))  (1F1 (tt0 + 0 ))(1F2 (t0  0 )). (76)
Let us find a value of x = (t) that provides the minimum to the left-hand side of (76) for
the fixed t. Let us denote
G(x) = (1  F1 (t  t0 +  0  x))(1  F2 (t0   0 + x)).
Then,
G0 (x) = f1 (t  t0 +  0  x))(1  F2 (t0   0 + x))  f2 (t0   0 + x)(1  F1 (t  t0 +  0  x)).
Since a valid (t) in the interval [t 0 , t1 ] obtains values between  0 and  1 , by (75) we have
t  t0 +  0  x  [ 0 ,  1 ],

t0   0 + x  [t0   0 , t1   1 ].

Therefore, there exist t0 , t00  [t0 , t1 ], such that 1 (t0 ) = (t0 ) = t  t0 +  0  x and 2 (t00 ) =
t00  (t00 ) = t0   0 + x. By (73) we obtain G0 (x) > 0, meaning that G(x) monotonically
increases. Besides, by (75) we have x = (t)  0 (since  0 (t)  1), and therefore G(x)
obtains its minimal value when x = 0. Therefore, if we denote by Ran(t) the set of valid
values for (t),
(1  F1 ())(1  F2 (t  )) = (1  F1 (t  t0 +  0  (t)))(1  F2 (t0   0 + (t))) 
min (1  F1 (t  t0 +  0  x))(1  F2 (t0   0 + x)) =

xRan(t)

(1  F1 (t  t0 +  0 ))(1  F2 (t0   0 )),
and the strict equality occurs if and only if (t) = t  t 0 +  0 . Thus,
(1  F1 ())(1  F2 (t  ))  (1  F1 (e
 ))(1  F2 (t  
e))

for t  [t0 , t ].
Let us show now the correctness of the same statement in the interval [t  , t1 ], which is
equivalent to the inequality
(1  F1 ((t)))(1  F2 (t  (t)))  (1  F1 ( 1 ))(1  F2 (t   1 )).

(77)

The proof is similar. Let
(t) =  1  (t).
128

(78)

fiOptimal Schedules for Parallelizing Anytime Algorithms

The inequality (77) becomes
(1  F1 ( 1  (t)))(1  F2 (t   1 + (t)))  (1  F1 ( 1 ))(1  F2 (t   1 )).

(79)

As before, we find a value of x = (t) that provides the minimum to the left-hand side
of (79)
G(x) = (1  F1 ( 1  x))(1  F2 (t   1 + x)).
The derivative of G(x) is
G0 (x) = f1 ( 1  x))(1  F2 (t   1 + x))  f2 (t   1 + x)(1  F1 ( 1  x)),
and since a valid (t) in the interval [t 0 , t1 ] obtains values between  0 and  1 , by (78) we
have
 1  x  [ 0 ,  1 ],

t   1 + x  [t0   0 , t1   1 ].

Therefore, there exist t0 , t00  [t0 , t1 ], such that 1 (t0 ) = (t0 ) =  1  x and 2 (t00 ) =
t00  (t00 ) = t   1 + x. By (73), G0 (x) > 0, and therefore G(x) monotonically increases.
Since x =  1  (t)  0, G(x)  G(0). Thus, for t  [t  , t1 ],
(1  F1 ())(1  F2 (t  )) = (1  F1 ( 1  (t)))(1  F2 (t   1 + (t))) 

min (1  F1 ( 1  x))(1  F2 (t   1 + x)) = (1  F1 ( 1 ))(1  F2 (t   1 )),

xRan(t)

and the strict equality occurs if and only if (t) =  1 .
Combining this result with the previous one, we obtain that
(1  F1 ())(1  F2 (t  ))  (1  F1 (e
 ))(1  F2 (t  
e))

holds for every t  [t0 , t1 ]. Since 
e(t) behaves as (t) outside this interval, E u ()  Eu (e
 ).
Besides, since the equality is obtained if and only if   
e, and since E u () is optimal, we
obtain that   
e, and therefore the first process will take all the resources in some interval
[t0 , t1 ].
The proof for n processes is exactly the same. Let { i } provide the optimal solution,
and at the point t0 there is process k, such that for each j 6= k
hk (k (t0 )) > hj (j (t0 )).
From the continuity of the functions h i (i (t)) in the point t0 , it follows that there exists
some neighborhood U (t0 ) of t0 , such that
min hk (k (t0 )) > max max hi (i (t00 )).
i6=k t00 U (t0 )

t0 U (t0 )

Let us take any process l 6= k, and let
y(t) = k (t) + l (t).
129

(80)

fiFinkelstein, Markovitch & Rivlin

Now we can repeat the above proof while substituting y(t) instead of t under the function
sign:

k (t),



y(t)  y(t0 ) + k (t0 ),

ek (t) =
 (t ),


 k 1
k (t),

y(t)  y(t0 ),
y(t0 )  y(t)  y(t0 ) + k (t1 )  k (t0 ),
y(t0 ) + k (t1 )  k (t0 )  y(t)  y(t1 ),
y(t)  y(t1 ).

The substitution above produces a valid schedule due to the monotonicity of y(t). The rest
of the proof remains unchanged.
Q.E.D.

A.6 Proof of Theorem 5
The claim of the theorem is as follows:
An active process will remain active and consume all resources as long as its hazard
function is monotonically increasing.
Proof: The proof is by contradiction. Let { j } form an optimal schedule. Assume
that at some point t1 process Ak is suspended, while its hazard function h k (k (t1 )) is
monotonically increasing at t1 .
Let us assume first that at some point t 2 process Ak becomes active again. Since we do
not consider the case of making process active at a single point, there exists some  > 0,
such that Ak is active in the intervals [t1  , t1 ] and [t2 , t2 + ]. Ak has been stopped
at a point of monotonicity of the hazard function, and therefore, by Theorem 4, in these
intervals Ak is the only active process. We consider two alternative scenarios. In the first
one, we allow Ak to be active for additional  time starting at t 1 (i.e., shifting its idle
period by ), while in the second we suspend A k by  earlier.
For the first scenario, the scheduling functions have the following form:

k (t),
t  t1 ,




(t
)
+
(t

t
),
t1  t  t1 + ,
1
k 1
ka (t) =
 (t  ) +  = k (t1 ) + , t1 +   t  t2 + ,


 k
k (t),
t  t2 + ;

t  t1 ,

 j (t),

j (t1 ),
t1  t  t1 + ,
ja (t) =

(t

),
t

1 +   t  t2 + ,

 j
j (t),
t  t2 + .

(81)

(82)

It is possible to see that these scheduling functions are continuous and satisfy invariant (39),
which makes this set a suitable candidate for optimality.
130

fiOptimal Schedules for Parallelizing Anytime Algorithms

Substituting these values of  into (6), we obtain
Eu (1a , . . . , na )
Z

t1 +

Z

(1  Fj (j (t)))dt+

j=1

t2 +

t1 +

0

Z

0

n
t1 Y

(1  Fk (k (t1 ) + (t  t1 )))

t1

Z

=

Z

(1  Fk (k (t1 ) + ))

n
t1 Y

j=1

(1  Fj (j (t)))dt +

t2

(1  Fk (k (t1 ) + ))

t1

Y

j6=k

Y

j6=k

Y

j6=k

Y

j6=k

(1  Fj (j (t1 )))dt+

(1  Fj (j (t  )))dt +

(1  Fj (j (t1 )))

Z

(1  Fj (j (t)))dt +

Z



n
Y

(1  Fj (j (t)))dt =

t2 + j=1



(1  Fk (k (t1 ) + x))dx+

0

Z

n
Y



t2 + j=1

(1  Fj (j (t)))dt.

Subtracting Eu (1 , . . . , n ) given by (6) from Eu (1a , . . . , na ), we get
Eu (1 , . . . , n )  Eu (1a , . . . , na ) =
Z t2
Y
[(1  Fk (k (t)))  (1  Fk (k (t1 ) + ))]
(1  Fj (j (t)))dt+
t1

Z

j6=k

n
t2 + Y

t2

(1  Fj (j (t)))dt 

j=1

Y

j6=k

(1  Fj (j (t1 )))

Z

(83)



(1  Fk (k (t1 ) + x))dx.

0

Let us consider the first term of the last equation. Since in the interval [t 1 , t2 ] k (t) = k (t1 ),
in this interval
(1  Fk (k (t)))  (1  Fk (k (t1 ) + )) = (1  Fk (k (t1 )))  (1  Fk (k (t1 ) + )) =
Z 
Z 
Z 
hk (k (t1 ) + x)(1  Fk (k (t1 ) + x))dx.
fk (k (t1 ) + x)dx =
d(1  Fk (k (t1 ) + x)) =

0

0

0

Due to monotonicity of hk (k ) in t1 ,
(1  Fk (k (t)))  (1  Fk (k (t1 ) + )) =
Z 
Z
hk (k (t1 ) + x)(1  Fk (k (t1 ) + x))dx > hk (k (t1 ))
0

which leads to
Z t2
t1

[(1  Fk (k (t)))  (1  Fk (k (t1 ) + ))]

hk (k (t1 ))

Z


0

(1  Fk (k (t1 ) + x))dx
131

Z

t2

Y

j6=k

Y

t1 j6=k


0

(1  Fk (k (t1 ) + x))dx,

(1  Fj (j (t)))dt >

(1  Fj (j (t)))dt.

(84)

fiFinkelstein, Markovitch & Rivlin

Let us now consider the second term of (83). Since in the interval [t 2 , t2 + ] only Ak is
active, in this interval

j (t2 ),
j 6= k,
j (t) =
k (t1 ) + (t  t2 ), j = k.
Thus,
Z

n
t2 + Y

t2

j=1

(1  Fj (j (t)))dt =

Y

j6=k

Z

(1  Fj (j (t2 )))



(1  Fk (k (t1 ) + x))dx.

0

(85)

Substituting (84) and (85) into (83), we obtain
Z

Eu (1a , . . . , na )



(1  Fk (k (t1 ) + x))dx 
Eu (1 , . . . , n ) 
>
0


(86)
Z t2 Y
Y
Y
hk (k (t1 ))

(1  Fj (j (t)))dt +
(1  Fj (j (t2 ))) 
(1  Fj (j (t1 ))) .
t1 j6=k

j6=k

j6=k

The proof for the second scenario, where A k is suspended for , is similar. For this
scenario, the scheduling functions  k (t) and j (t) for j 6= k can be represented as follows:

k (t),
t  t1  ,



k (t1  ) = k (t1 )  ,
t1    t  t2  ,
ki (t) =
(87)

(t

)
+
(t

(t

))
=

(t
)
+
(t

t
),
t2    t  t 2 ,

2
2
k 1

 k 1
k (t),
t  t2 ;

j (t),
t  t1  ,




(t
+
),
t
j
1    t  t2  ,
(88)
ji (t) =
 (t ),
t2    t  t 2 ,


 j 2
j (t),
t  t2 .
As before, these scheduling functions are continuous and satisfy invariant (39).
Substituting  i into (6), we obtain
Eu (1i , . . . , ni )
Z

t2 

t1 

Z

0

Z

0

n
t1  Y

(1  Fj (j (t)))dt+

j=1

(1  Fk (k (t1 )  ))

t2

t2 

Z

=

Z

Y

j6=k

(1  Fk (k (t1 ) + (t  t2 )))

n
t1  Y

(1  Fj (j (t)))dt +

j=1

t2

t1

(1  Fj (j (t + )))dt+

(1  Fk (k (t1 )  ))

Y

j6=k

Y

j6=k

Y

j6=k

(1  Fj (j (t2 )))dt +

(1  Fj (j (t2 )))

(1  Fj (j (t)))dt +
132

Z

Z

Z


0



n
Y

t2

j=1

(1  Fj (j (t)))dt =

(1  Fk (k (t1 )  x))dx+
n
Y

(1  Fj (j (t)))dt.

t2 + j=1

fiOptimal Schedules for Parallelizing Anytime Algorithms

Subtracting Eu (1 , . . . , n ) given by (6) from Eu (1i , . . . , ni ), we get
Eu (1 , . . . , n )  Eu (1i , . . . , ni ) =
Z t2
Y
[(1  Fk (k (t)))  (1  Fk (k (t1 )  ))]
(1  Fj (j (t)))dt+
t1

Z

j6=k

t1

n
Y

t1  j=1

(1  Fj (j (t)))dt 

Y

j6=k

(1  Fj (j (t2 )))

Z

(89)



0

(1  Fk (k (t1 )  x))dx.

As in the first scenario, in the interval [t 1 , t2 ]
(1  Fk (k (t)))  (1  Fk (k (t1 )  )) = (1  Fk (k (t1 )))  (1  Fk (k (t1 )  )) =
Z 0
Z 0
fk (k (t1 ) + x)dx =
d(1  Fk (k (t1 ) + x)) = 




Z



0

fk (k (t1 )  x)dx = 

Z





hk (k (t1 )  x)(1  Fk (k (t1 )  x))dx.

0

Due to monotonicity of hk (k ) in t1 ,
(1  Fk (k (t)))  (1  Fk (k (t1 )  )) =
Z 
Z

hk (k (t1 )  x)(1  Fk (k (t1 )  x))dx > hk (k (t1 ))
0

which leads to
Z t2
t1

[(1  Fk (k (t)))  (1  Fk (k (t1 )  ))]

 hk (k (t1 ))

Z


0

(1  Fk (k (t1 )  x))dx

Z

t2

Y

j6=k

Y

t1 j6=k


0

(1  Fk (k (t1 )  x))dx,

(1  Fj (j (t)))dt >
(90)
(1  Fj (j (t)))dt.

The transformations for the second term of (89) are also similar to the previous scenario.
Since in the interval [t1  , t1 ] only Ak is active, in this interval

j (t1 ),
j 6= k,
j (t) =
k (t1 )  (t1  t), j = k.
Thus,
Z

t1

n
Y

t1  j=1

(1  Fj (j (t)))dt =

Y

j6=k

(1  Fj (j (t1 )))

Z


0

(1  Fk (k (t1 )  x))dx.

(91)

Substituting (90) and (91) into (89), we obtain
Z 
i
i
Eu (1 , . . . , n )  Eu (1 , . . . , n ) >
(1  Fk (k (t1 )  x))dx 
0


Z t2 Y
Y
Y
(1  Fj (j (t1 ))) .
(1  Fj (j (t2 ))) 
(1  Fj (j (t)))dt +
 hk (k (t1 ))
t1 j6=k

j6=k

j6=k

(92)

133

fiFinkelstein, Markovitch & Rivlin

By (86) and (92),
sign(Eu (1 , . . . , n )  Eu (1a , . . . , na )) = sign(Eu (1 , . . . , n )  Eu (1i , . . . , ni )),

(93)

and therefore one of these scenarios leads to better schedule, which contradicts the optimality of the original one.
The proof for the case where control does not return to A k at all is exactly the same
and is omitted here. Informally, it can be viewed as replacing t 2 by  in all the formulas
above, and the results are the same. same results.
Q.E.D.

A.7 Proof of Theorem 6
The claim of the theorem is as follows:
If no time cost is taken into account (c = 1), the model with shared resources under intensity control settings is equivalent to the model with independent processes under
suspend-resume control settings. Namely, given a suspend-resume solution for the model
with independent processes, we may reconstruct an intensity-based solution with the same
cost for the model with shared resources and vice versa.

Proof: Let Eshared
be the optimal value for the framework with shared resources,

and Eindependent be the optimal value for the framework with independent processes. Since
c = 1, the two problems minimize the same expression
! n
Z  X
n
Y
Eu (1 , . . . , n ) =
i0
(1  Fj (j ))dt  min,
(94)
0

i=1

j=1

and each set {i } satisfying the resource sharing constraints automatically satisfies the
process independence constraints, we obtain


Eindependent
 Eshared
.

Let us prove that


.
Eshared
 Eindependent

Assume that a set of functions 1 , 2 , . . . , n is an optimal solution for the problem with
independent processes, i.e.,

Eu (1 , . . . , n ) = Eindependent
.

We want to construct a set of functions {ei } satisfying the resource sharing constrains, such
that
Eu (f
1 , . . . , 
fn ) = Eu (1 , . . . , n ).
Let us consider a set of discontinuity points of  i0

T = {t|i : i0 (t  ) 6= i0 (t + )}.
134

fiOptimal Schedules for Parallelizing Anytime Algorithms

In our model this set is countable, and we can write it as a sorted sequence t 0 = 0 < t1 <
. . . < tk < . . .. The expected schedule cost in this case will have a form
Eu (1 , . . . , n ) =


X

Euj (1 , . . . , n ),

j=0

where
Euj (1 , . . . , n ) =

Z

n
X

tj+1
tj

i=1

i0

!

n
Y
l=1

(1  Fl (l ))dt.

We want to construct the functions ei incrementally. For each time interval [t j , tj+1 ] we
define a corresponding point tej and a set of functions ei , such that
! n
Z tg
n
j+1
Y
X
g
(1  Fl (el ))dt = Eu (1 , . . . n ).
el 0
E
1 , . . . , 
fn ) =
u (f
j

j

tej

l=1

l=1

Let us denote ij = i (tj ) and f
ei (tj ). At the beginning, f
ij = 
i0 = 0 for each i, and
0 < j, and 
0
defined
for
j
e
(t)
defined
on each interval
te0 = 0. Assume now that we have tf
i
j
tej and ej on [tej , tg
[f
tj 0 , t]
j+1 ].
j 0 +1 ]. Let us show how to
Pdefine
n
0
By definition of tj , k = l=1 l (t) is a constant for t  [tj , tj+1 ]. Since {i } satisfy
suspend-resume constraints, exactly k  1 processes are active in this interval, each with
full intensity. Without loss of generality, the active processes are A 1 , A2 , . . . , Ak , and
Z tj+1 Y
n
Euj (1 , . . . , n ) = k
(1  Fl (l ))dt =
tj

k
k

n
Y

(1  Fl (lj ))

l=k+1
n
Y

(1  Fl (lj ))

l=k+1

Z

Z

tj+1

tj

0

l=1
k
Y
l=1

(1  Fl (t  tj + lj ))dt =

n
tj+1 tj Y
l=1

(1  Fl (x + lj ))dx.

Let tg
ei (t) on the segment [tej , tg
j+1 = tej + k(tj+1  tj ), and let us define 
j+1 ] as follows:

0
(t  tej )/k + f
ij , i > 0 for t  [tj , tj+1 ]
ei (t) =
(95)
f
otherwise.
ij ,

In this case, on this segment

n
X
l=1

ei 0 (t) = 1,

which means that the ei satisfy the resource sharing constraints. By definition,
tg
j+1  tej = k(tj+1  tj ),

and therefore for processes active on [t j , tj+1 ] we obtain

^
f
i,j+1  
ij =

tg
j+1  tej
= tj+1  tj = i,j+1  ij .
k
135

(96)

fiFinkelstein, Markovitch & Rivlin

For processes idle on [tj , tj+1 ] the same equality holds as well:

^
f
i,j+1  
ij = 0 = i,j+1  ij ,

and since ei (t) = 0 we obtain the invariant

f
ij = ij .

(97)

The average cost for the new schedules may be represented as
! n
Z tg
n
j+1
X
Y
g
el 0
E
1 , . . . , 
fn ) =
(1  Fl (el ))dt =
u (f
j

n
Y

l=k+1

(1  Fl (f
lj ))

Z

tej

l=1
k
g
tj+1 Y

tej

l=1

l=1

(1  Fl ((t  tej )/k + f
lj ))dt.

Substituting x = (t  tej )/k and using (95), (96) and (97), we obtain
g
E
1 , . . . , 
fn ) = k
uj (f
k

n
Y

n
Y

l=k+1

(1  Fl (lj ))dt

l=k+1

Euj (1 , . . . , n ).

(1  Fl (f
lj ))

Z

0

k
tj+1 tj Y
l=1

Z

0

k
(tg
j+1 tej )/k Y
l=1

(1  Fl (x + f
lj ))dx =

(1  Fl (x + lj ))dx =

From the last equation, it immediately follows that
Eu (f
1 , . . . , 
fn ) =


X
j=0

g
E
1 , . . . , 
fn ) =
uj (f


X

Euj (1 , . . . , n ) = Eu (1 , . . . , n ),

j=0

which completes the proof.
Q.E.D.

References
Boddy, M., & Dean, T. (1994). Decision-theoretic deliberation scheduling for problem
solving in time-constrained environments. Artificial Intelligence, 67 (2), 245286.
Clearwater, S. H., Hogg, T., & Huberman, B. A. (1992). Cooperative problem solving. In
Huberman, B. (Ed.), Computation: The Micro and Macro View, pp. 3370. World
Scientific, Singapore.
Dean, T., & Boddy, M. (1988). An analysis of time-dependent planning. In Proceedings
of the Seventh National Conference on Artificial Intelligence (AAAI-88), pp. 4954,
Saint Paul, Minnesota, USA. AAAI Press/MIT Press.
Finkelstein, L., & Markovitch, S. (2001). Optimal schedules for monitoring anytime algorithms. Artificial Intelligence, 126, 63108.
136

fiOptimal Schedules for Parallelizing Anytime Algorithms

Finkelstein, L., Markovitch, S., & Rivlin, E. (2002). Optimal schedules for parallelizing
anytime algorithms: The case of independent processes. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, pp. 719724, Edmonton, Alberta, Canada.
Gomes, C. P., & Selman, B. (1997). Algorithm portfolio design: Theory vs. practice. In
Proceedings of UAI-97, pp. 190197, San Francisco. Morgan Kaufmann.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through randomization. In Proceedings of the 15th National Conference on Artificial Intelligence
(AAAI-98), pp. 431437, Menlo Park. AAAI Press.
Horvitz, E. (1987). Reasoning about beliefs and actions under computational resource
constraints. In Proceedings of the Third Workshop on Uncertainty in Artificial Intelligence, pp. 429444, Seattle, Washington.
Huberman, B. A., Lukose, R. M., & Hogg, T. (1997). An economic approach to hard
computational problems. Science, 275, 5154.
Janakiram, V. K., Agrawal, D. P., & Mehrotra, R. (1988). A randomized parallel backtracking algorithm. IEEE Transactions on Computers, 37 (12), 16651676.
Knight, K. (1993). Are many reactive agents better than a few deliberative ones. In
Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence,
pp. 432437, Chambery, France. Morgan Kaufmann.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.
Kumar, V., & Rao, V. N. (1987). Parallel depth-first search on multiprocessors. part II:
Analysis. International Journal of Parallel Programming, 16 (6), 501519.
Luby, M., & Ertel, W. (1994). Optimal parallelization of Las Vegas algorithms. In Proceedings of the Annual Symposium on the Theoretical Aspects of Computer Science
(STACS 94), pp. 463474, Berlin, Germany. Springer.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Rao, V. N., & Kumar, V. (1987). Parallel depth-first search on multiprocessors. part I:
Implementation. International Journal of Parallel Programming, 16 (6), 479499.
Rao, V. N., & Kumar, V. (1993). On the efficiency of parallel backtracking. IEEE Transactions on Parallel and Distributed Systems, 4 (4), 427437.
Russell, S., & Wefald, E. (1991). Do the Right Thing: Studies in Limited Rationality. The
MIT Press, Cambridge, Massachusetts.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. In Proceedings of the
Twelfth National Joint Conference on Artificial Intelligence (IJCAI-91), pp. 212217,
Sydney. Morgan Kaufmann.
Simon, H. A. (1982). Models of Bounded Rationality. MIT Press.
Simon, H. A. (1955). A behavioral model of rational choice. Quarterly Journal of Economics,
69, 99118.
137

fiFinkelstein, Markovitch & Rivlin

Yokoo, M., & Kitamura, Y. (1996). Multiagent real-time-A* with selection: Introducing
competition in cooperative search. In Proceedings of the Second International Conference on Multiagent Systems (ICMAS-96), pp. 409416.
Zilberstein, S. (1993). Operational Rationality Through Compilation of Anytime Algorithms.
Ph.D. thesis, Computer Science Division, University of California, Berkeley.

138

fi     	
ff fi
 	
fi  fi    

# $ % & ' (' ) * +
4

   
		   !   
  "

, + + & - (' % * $ . / 0 ($ ' * 12 3

5 + - $ 16 7 % 8 $ - 4 9 9 & + % 0 :

; < = >= ? @ A BCDE C =

F G H I J H K L M N OF P Q OH M ORS

T UVW X Y Z[ \ ]^Z_ ]Z ` ZW a[YV Z_ Y
b Z_ c dX [^U_ e_ ^f Z[g ^Yh
b ZZ[c\i Zf aj k g [a Zl mn o p q
r < s t > u> = = > = t < vw x

J y Nz { L G y F y | RM N ON|H K I y G } O{ } Q

~ a ]X lYh U k _ X g Y[^al  _ ^_ ZZ[^_ a_   a_ a ZV Z_ Y
Z]i _ ^U_
 a^ a   p p pj k g [a Zl

      
                                 

  

                                          
                                           
                                             
                                           
                                             
                                      

           
                                                    
                                                      
                                                  
                                                         
    

                                            

                                         

           

                                              
                                                      
                                            

        

                                                         
              

                                       

                                                     
                                      
                                                     
                                                         
                                                               
                                                   
                                                       
                                                   
                                                     


fi     fifi

  	
  

         
  

  
	    

fiGHIJH K

  { K K { K z y S| 

                                                      
               



                                     

                                                            
                                                            
                                                         
                                

 

  

               

                                             
                                                      
            
                                                   
                                                            
              

 

                                      

                                                  
                                                           
                                       
              
 

                       

           

	
fffi

	
 fffi

      

            

                                       

                                                       
                                                       
        

      

                        

                                                   
                                                     
                                      
                        

             
	
 fffi        

                    

                                                     
                   

          
                                                         
       

 @   C E >  >  = w  < = s
                                   

  

            

                                                      
        

      

                                      

                                                
                         

     



                  

                                               



                                                 



          

       



      

                              

            
           

                                         

       
    

!" #$ %&'() *+,-. '/$012314 5$ 612/7489 4(/':&4(5' 3&9 14 &4 ;1<<) =8( ;1 ;5<< $/( $11> (:54 '&?&=5<5(@"
AB

 

           

fi{ H G K RK P | y

       



 y y G } RK H| {  I I RM R{ K | S

                   



     

                

                                                       
                            

 	 
 ff fi    

                                               



                                                  
 



  

                             
 ff                                    ff        	  
                                              

                        
    

 

                  



                                  



                                

                



                                                      
            
                                             
                                                    
                                                    



                            

                          

                                 



                    

     

 >  = w < =  

 	   ff    ff  


             





    

 	   ff    ff  





   

    

w C ! >  C E > s "       # $        % 
    %              &  '    (

 ) B < * C *  v s w +
      # 
0 (

 

uB C = s w < =
   

.

 &  ' 

  % 

        

,- = + w  < = " ./ # ff . ff  ff 0       1 1  &  '     
3    &  '   &  '           &  '   &  ' 

  2      

                

  

 4 5 

         

          

                                                       

 45

                                                  

                    

  

                                   

                                                    





                                                          
                                         

  

                                                 

                                                    
                                                
                                            
       

 45

 45

                           

     

         

                      6                                
                                      

 7  8 9 :

     

 45

                                                    
                                                      
                                                          
             

A;

fiGHIJH K

           

.

  { K K { K z y S| 

                                   

  

                    

                          

    

.

                              

.

                     

                                    

 

 

        

                                                      
                                                   
                 

.

                       

                                 

   

       

                                            
                  
         



  

     

 

  & '

                 

                                           

                

 7 

                     '   

                                  
                                

45

                 

                                                     

      %                                  
 # ff  ff 	 ff                                                
                 	           #                   
        #   # ff   	   
 ff   # ff  ff 	 ff           #  	        # ff   
fi
        2   

2  &7 

             



#

 @  s s - E  w  < = s   < E  v>   w 

C=

  w  E C v  w 

                                                
                                                     
                                                         
   

       

                                            

                                     

               

                                                      
                        
                                                       
                                                     
                

                                

fi  



                                                      
                                             



               '                    



 



              

 4 5                                                 
                                      7  1  &  '     4 5
                                                   


                                                    
              

45

      

                       
            

  



  

 45

                               

 #                
2  &7                

                             



         



                       

            &

A



#

     

      

 # 

fi y y G } RK H| {  I I RM R{ K | S

{ H G K RK P | y

#

       

                                            

                                                           
                                          
                                                    
    

 45

                                               

45

                                         



         

                                                       
                                                      
   





                                               



    

 

         

 

           



              

                                                      
                          



  7             
 

.







  

               

#

45

.   

           



             

 





       

         
 # ff  ff .         
   	        # ff  ff .      

         

#     
     #      

       

                          
 



                  

                   

                               
    

 &  

              

                  

     

 	    # ff .0




 0   # ff  ff .0 



    

                                       

   6         

                                       



            

 4 5                   
  &'    &    &                     

                               



                 &

    
ff  ff 
 ff  ff    

                           
     



                 ff

  ff        	 
 	      

                      

                                           

.   

.





       

                              fi                    

     
 
                                                  

.





                                                      



            
                  
u t > < B > E               &   &'     &    &     &      % 
 &  '      7              2        &&      2       &'     &
1 7        7  1            (
                                                          
                         



" /29 &<<@) ;1 ;5<< &4489 1 (:&( (:121 54 & '/$4(&$(   !) 48': (:&( 5% (:1 $89=12 /% &'(5/$4 &0&5<&=<1 (/
&31$(  54  ) (:1$ 5( 54 '/9 9 /$<@  $/;$ (:&( &$@ &31$( '&$ :&01 & 9 /4( /%  !" # $ ! % & ' (() &'(5/$4"
* (:12 ;&@4 /% %/29 &<565$3 (:54 '/$'1?( &21 (21&(1> 459 5<&2<@"
A+

fiGHIJH K

  { K K { K z y S| 

                       

 < B < vvC B              &   &'     &
7               2        &&   
       (

) B< <D "

   &   

  &   

    



  %     &&          

                                                      

                                                   



                                       

                                                   
                                                        


 45

                                                

                                     



                  

                                                        
                   





                                       

 4 5        
                                       .         
          ff ff  ff 
                                     4 5 
           



                       

                                   



    	 
 	 

                  
                        

 4 5

 
  

        

                                                   
                 

            
       

	
 fffi

	
fffi

                                     

                                              

                                                  

                                                          
                                                    
          

                               

                  

  

  

  

       

                           

  

                                                    





                                                       
                        

Cs> " < E E <=

<B

>B



 
   





              



<  > B  < =w C + w < = s @

                        

          
                            
                     
          

	
fffi

	
fffi

  

                      

                

 4 5

     

                                       

                              
                                

                 

    

                 

    

                                                          
       

                                

A

              

fi{ H G K RK P | y

 y y G } RK H| {  I I RM R{ K | S

                             
                     

	
 fffi 

	
 fffi

                   

                         

       
                                                      
                                                       
                                                     
                                                      
                                                       
    

 C s >  "  + < E E < = v



= < = < B

C + w < =  s > w s x > < D C vv C ! > =w s @

 > B = !

<  > B w t > C ! > =w s



C=



= <  v>

!>

<D wt>

                                    

                                                         
                                   



  

                    

                                                        
                                                     
                         



               

  

          

                              
        

  



 

 

           

                 ff  

                                        

                                                



                                



                                

  

      

	
 fffi 

 

                                            

                           

  

    

                  

	
fffi

              

          ff                                        
                
                                                     
                                                        
                          

  

      ff                     

                                   
                                                
    

 C s >  " ) > B D> + w





r < = w<B =! @

                                

                                                     
                                                          
               

                        

                           

           

                          

                                                     
        

A

fiGHIJH K

  { K K { K z y S| 

                                                      
                               

 C s > "  = < = C + w < =



s>w s x>s @

                              

                                                          
                                                            
                                  



                    

 0

 0        	
 fffi                                        
 0                       
 ff                             

 
  

            



                                        

                    

                                                           
                                                           
        





   

                                         

                       



                

        

                                                         
                                                         
                                                   
                                                       
                                   
              
              

              

  

0

                 

                   

                                          

                                                             
                                                  
    
                       
        




         



                          
          









 

                            

      

                         



0

     




 

	

   

             ff           

    

                                     

                                                          
                                    
 

              

                                                           
                                         
 ff  
                                  



                 

     
 ff                    



                                

 	
fffi

      

                     

                                                       
       ff                                   

	
 fffi



 0

          

                                 
 ff          

  ff 

 





 ff   

	 
 ff                     





    

              
 

                                                         

A	

fi{ H G K RK P | y

 y y G } RK H| {  I I RM R{ K | S

                                         








 ff   	                               

	 
ff

                         




   


               

                           

                                       
       



          



                                            
                                     
 ff         

         

                
 ff                                    
 
                                                      
                            
    




                

                                       





       

         

                                                   
                                
              

fi
fi

                 





fi 

Cs> "


t CB>

  < v = < E C v * < - = 





< = C +w < =s>w s x>s @

                  

                                                       
                               





#

                     

                                                      
         
  





#

 #


                    



                      

                                                  

                                                           
                                                     
                          
 



         



                                           

                                                  
                             











  

                 

                                                        

  

                                                 
fi       
  '        fi                            
fi
         



          

 C s > " ) < v = < E C v * < - = 






< = C+w < = s>w s x>s

 -=

= < = E

  = ! w  E > @

      

                                                   
      

	
fffi

                                        

                                



                     

                                                          
                                                      
             

       

             

                                  



	
fffi 

               

                  

                                                   ff  

                       
                      
                      

                                  
                       

.

       

                                  

                                                      
                                                   

A





      

fiGHIJH K

  { K K { K z y S| 

                                      

              

                                                 
                                                      
                 

                                                       
                                            


  

              

                                                        

                             
                     





                         

             

0

        





                                                           


0

                     

        ff               




                            

	    ff                            

            ff  
 0                

                                  

   

                                  

   

                
                  



  

                          



       

                                                           
                                                        
                                                      
            

45

                          

                            

       

	
 fffi    

  

          

                                                
                                                   
                                                         
                            

        

                   

                                                   
           

       

                                       

                                                    
                                                           
                 
                                                 
                       

   

                               

                                                      
       

         

                                       

                                                           
                                                       
                                                        
                                     
                                                     





            

                                          

                                                         
                                                     

B

fi y y G } RK H| {  I I RM R{ K | S

{ H G K RK P | y

  

                             
  

  



              

                          

                              

                                                          



                   
                   



                      



        

                                    



                                                

    

                                                    
                                         

                                                        
                                                        
                                                       
                                                     
             



                                

                  



                                  

                    
                 
          

' 





                              

             



                          



  

                                     
                    




  

   



          

                                                       
                                                   



   

                                                           
                         

         

5

    5      

               	 
  
            

45

          5      
4   5              

         

                          

     

        

	
 fffi

       

	
 fffi

              

                                                          
                                                
        
            

45 

 	 #  ff    ff # ff                
                       fi      

         

 	   ff    ff  



                                            
                               



   

#

fi # ff   

       

                

                     

./ # ff  ff .     
#     .                     
                           

                                 
                        
           

#

./ # ff  ff 

       

./ 

      

                                                         

                 

BA

fiGHIJH K

    

                       

    

        

	
fffi

  

  { K K { K z y S| 

        

  

 45

                     

                    

                                 

 0

0

            

                                          

                            

	
 fffi

                      

                                                         
                                
                                                   
                                 

                  

                                                       
                                                    

       

 

fi  

   fi # ff   

                          

                          

                        

  

fi   	

ff

    

  

? = w C v x > "             



     

  ff    ff   

 4 5       0                 #  ff #  ff    ff # ff  

#  ff    ff # ff                     ff    ff  
                       #                            fi  

    fi # ff   	 fi           #               ./ #  ff  ff #  	     

 	  ff    ff                          ./ #  ff  ff #  	       	   


  

                                            
  %   7    %         

        

7

 

  %                   

                                                
                                                   
   
;>

 >Cw "
 <E  -w>

  + w "                                      
                                     %  
C =  -   C w > "                                  
C=

       

 * s> B>

#

         





            

          
# ff           

       
      


         

                     

#

#

    

                



                                       



	

   

        

ff

    ff




     	 ff  '  


                 

  %                                  

                         
              
u t><B>E

1

 

                      

 1  8 9 : %                ( 
   	 
 	       
9         &      %     7          1' ff  ff     
   &       7       2  1&  1'  7    &    1'
fi . ff  ff   (     % 



     (

    

	
fffi


 



BB

fi{ H G K RK P | y

 y y G } RK H| {  I I RM R{ K | S

  1 1  &  '    &       
   	
 fffi  &    %  &&  
fi . ff  ff     %     7  1        &'     &   ff  ff  ff  


    

	
fffi

                      

   
 (

 







   

 4 5     
         

          

                                          
                             



 7   

      

45

          

                                                   
                      

                                   

                                          

         

                                                            

 45

                       

                        



                                                        
                         



  

  

    

   

:    







    

      

                          

         &   

 



    

  3  &

    &&      

     



      

           



     

                              
  7   &  8     
         

                          



     
                                                         
             

      

:    

         
         

         

   


	  

     



 



8 7&     


       

                      

8     

 

 



                                    
:       
              &      
ff 8 
  fi           
    

                 

8    

        


   

                                                

(

     (  

     

 

   
     

8    

   

:  (

  



  

    &(


    
                                      

(

 



8    

 


            

                                    

                        

:  (

:  (

 8 


 

   (    (    (               

   

                                                     
:  (     (    7  &        :  
 '     ff  :   

 

                 

B;

fi