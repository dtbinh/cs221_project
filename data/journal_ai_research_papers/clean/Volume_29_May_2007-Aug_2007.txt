Journal of Artificial Intelligence Research 29 (2007) 105-151

Submitted 05/06; published 06/07

Combination Strategies for Semantic Role Labeling
Mihai Surdeanu
Llus Marquez
Xavier Carreras
Pere R. Comas

surdeanu@lsi.upc.edu
lluism@lsi.upc.edu
carreras@lsi.upc.edu
pcomas@lsi.upc.edu

Technical University of Catalonia,
C/ Jordi Girona, 1-3
08034 Barcelona, SPAIN

Abstract
This paper introduces and analyzes a battery of inference models for the problem of semantic role labeling: one based on constraint satisfaction, and several strategies that model
the inference as a meta-learning problem using discriminative classiers. These classiers
are developed with a rich set of novel features that encode proposition and sentence-level
information. To our knowledge, this is the rst work that: (a) performs a thorough analysis of learning-based inference models for semantic role labeling, and (b) compares several
inference strategies in this context. We evaluate the proposed inference strategies in the
framework of the CoNLL-2005 shared task using only automatically-generated syntactic
information. The extensive experimental evaluation and analysis indicates that all the
proposed inference strategies are successful they all outperform the current best results
reported in the CoNLL-2005 evaluation exercise but each of the proposed approaches has
its advantages and disadvantages. Several important traits of a state-of-the-art SRL combination strategy emerge from this analysis: (i) individual models should be combined at the
granularity of candidate arguments rather than at the granularity of complete solutions;
(ii) the best combination strategy uses an inference model based in learning; and (iii) the
learning-based inference benets from max-margin classiers and global feedback.

1. Introduction
Natural Language Understanding (NLU) is a subeld of Articial Intelligence (AI) that
deals with the extraction of the semantic information available in natural language texts.
This knowledge is used to develop high-level applications requiring textual and document
understanding, such as Question Answering or Information Extraction. NLU is a complex
AI-complete problem that needs to venture well beyond the syntactic analysis of natural
language texts. While the state of the art in NLU is still far from reaching its goals, recent
research has made important progress in a subtask of NLU: Semantic Role Labeling. The
task of Semantic Role Labeling (SRL) is the process of detecting basic event structures such
as who did what to whom, when and where. See Figure 1 for a sample sentence annotated
with such an event frame.
1.1 Motivation
SRL has received considerable interest in the past few years (Gildea & Jurafsky, 2002;
Surdeanu, Harabagiu, Williams, & Aarseth, 2003; Xue & Palmer, 2004; Pradhan, Hac
2007
AI Access Foundation. All rights reserved.

fiSurdeanu, Marquez, Carreras, & Comas

cioglu, Krugler, Ward, Martin, & Jurafsky, 2005a; Carreras & Marquez, 2005). It was
shown that the identication of such event frames has a signicant contribution for many
NLU applications such as Information Extraction (Surdeanu et al., 2003), Question Answering (Narayanan & Harabagiu, 2004), Machine Translation (Boas, 2002), Summarization (Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, & Popowich, 2005), and Coreference
Resolution (Ponzetto & Strube, 2006b, 2006a).
From a syntactic perspective, most machine-learning SRL approaches can be classied
in one of two classes: approaches that take advantage of complete syntactic analysis of text,
pioneered by Gildea and Jurafsky (2002), and approaches that use partial syntactic analysis,
championed by previous evaluations performed within the Conference on Computational
Natural Language Learning (CoNLL) (Carreras & Marquez, 2004, 2005). The wisdom
extracted from the rst representation indicates that full syntactic analysis has a signicant
contribution to SRL performance, when using hand-corrected syntactic information (Gildea
& Palmer, 2002). On the other hand, when only automatically-generated syntax is available,
the quality of the information provided through full syntax decreases because the state-ofthe-art of full parsing is less robust and performs worse than the tools used for partial
syntactic analysis. Under such real-world conditions, the dierence between the two SRL
approaches (with full or partial syntax) is not that high. More interestingly, the two SRL
strategies perform better for different semantic roles. For example, models that use full
syntax recognize agent and theme roles better, whereas models based on partial syntax are
better at recognizing explicit patient roles, which tend to be farther from the predicate and
accumulate more parsing errors (Marquez, Comas, Gimenez, & Catala, 2005).
1.2 Approach
In this article we explore the implications of the above observations by studying strategies
for combining the output of several independent SRL systems, which take advantage of
dierent syntactic views of the text. In a given sentence, our combination models receive
labeled arguments from individual systems, and produce an overall argument structure
for the corresponding sentence. The proposed combination strategies exploit several levels
of information: local and global features (from individual models) and constraints on the
argument structure. In this work, we investigate three dierent approaches:
 The rst combination model has no parameters to estimate; it only makes use of the
argument probabilities output by the individual models and constraints over argument
structures to build the overall solution for each sentence. We call this model inference
with constraint satisfaction.
 The second approach implements a cascaded inference model with local learning: rst,
for each type of argument, a classier trained oine decides whether a candidate is
or is not a nal argument. Next, the candidates that passed the previous step are
combined into a solution consistent with the constraints over argument structures.
We refer to this model as inference with local learning.
 The third inference model is global: a number of online ranking functions, one for
each argument type, are trained to score argument candidates so that the correct
argument structure for the complete sentence is globally ranked at the top. We call
this model inference with global learning.
106

fiCombination Strategies for Semantic Role Labeling

S

NP

NP

VP
NP

PP

The luxury auto maker last year sold 1,214 cars in the U.S.
A0
Agent

AMTMP
Temporal
Marker

P

A1
Predicate Object

AMLOC
Locative
Marker

Figure 1: Sample sentence from the PropBank corpus.
The proposed combination strategies are general and do not depend on the way in which
candidate arguments are collected. We empirically prove it by experimenting not only
with individual SRL systems developed in house, but also with the 10 best systems at the
CoNLL-2005 shared task evaluation.
1.3 Contribution
The work introduced in this paper has several novel points. To our knowledge, this is
the rst work that thoroughly explores an inference model based on meta-learning (the
second and third inference models introduced) in the context of SRL. We investigate metalearning combination strategies based on rich, global representations in the form of local
and global features, and in the form of structural constraints of solutions. Our empirical
analysis indicates that these combination strategies outperform the current state of the
art. Note that all the combination strategies proposed in this paper are not re-ranking
approaches (Haghighi, Toutanova, & Manning, 2005; Collins, 2000). Whereas re-ranking
selects the overall best solution from a pool of complete solutions of the individual models,
our combination approaches combine candidate arguments, or incomplete solutions, from
different individual models. We show that our approach has better potential, i.e., the upper
limit on the F1 score is higher and performance is better on several corpora.
A second novelty of this paper is that it performs a comparative analysis of several combination strategies for SRL, using the same framework i.e., the same pool of
candidates and the same evaluation methodology. While a large number of combination
approaches have been previously analyzed in the context of SRL or in the larger context of
predicting structures in natural language texts e.g., inference based on constraint satisfaction (Koomen, Punyakanok, Roth, & Yih, 2005; Roth & Yih, 2005), inference based in
local learning (Marquez et al., 2005), re-ranking (Collins, 2000; Haghighi et al., 2005) etc.
it is still not clear which strategy performs best for semantic role labeling. In this paper we
107

fiSurdeanu, Marquez, Carreras, & Comas

provide empirical answers to several important questions in this respect. For example, is a
combination strategy based on constraint satisfaction better than an inference model based
on learning? Or, how important is global feedback in the learning-based inference model?
Our analysis indicates that the following issues are important traits of a state-of-the-art
combination SRL system: (i) the individual models are combined at argument granularity
rather than at the granularity of complete solutions (typical of re-ranking); (ii) the best
combination strategy uses an inference model based in learning; and (iii) the learning-based
inference benets from max-margin classiers and global feedback.
The paper is organized as follows. Section 2 introduces the semantic corpora used for
training and evaluation. Section 3 overviews the proposed combination approaches. The
individual SRL models are introduced in Section 4 and evaluated in Section 5. Section 6
lists the features used by the three combination models introduced in this paper. The
combination models themselves are described in Section 7. Section 8 introduces an empirical analysis of the proposed combination methods. Section 9 reviews related work and
Section 10 concludes the paper.

2. Semantic Corpora
In this paper we have used PropBank, an approximately one-million-word corpus annotated
with predicate-argument structures (Palmer, Gildea, & Kingsbury, 2005). To date, PropBank addresses only predicates lexicalized by verbs. Besides predicate-argument structures,
PropBank contains full syntactic analysis of its sentences, because it extends the Wall Street
Journal (WSJ) part of the Penn Treebank, a corpus that was previously annotated with
syntactic information (Marcus, Santorini, & Marcinkiewicz, 1994).
For any given predicate, a survey was carried out to determine the predicate usage, and,
if required, the usages were divided into major senses. However, the senses are divided
more on syntactic grounds than semantic, following the assumption that syntactic frames
are a direct reection of underlying semantics. The arguments of each predicate are numbered sequentially from A0 to A5. Generally, A0 stands for agent, A1 for theme or direct
object, and A2 for indirect object, benefactive or instrument, but semantics tend to be verb
specic. Additionally, predicates might have adjunctive arguments, referred to as AMs. For
example, AM-LOC indicates a locative and AM-TMP indicates a temporal. Figure 1 shows a
sample PropBank sentence where one predicate (sold) has 4 arguments. Both regular and
adjunctive arguments can be discontinuous, in which case the trailing argument fragments
are prexed by C-, e.g., [A1 Both funds] are [predicate expected] [CA1 to begin operation
around March 1]. Finally, PropBank contains argument references (typically pronominal),
which share the same label with the actual argument prexed with R-.1
In this paper we do not use any syntactic information from the Penn Treebank. Instead,
we develop our models using automatically-generated syntax and named-entity (NE) labels,
made available by the CoNLL-2005 shared task evaluation (Carreras & Marquez, 2005).
From the CoNLL data, we use the syntactic trees generated by the Charniak parser (Char1. In the original PropBank annotations, co-referenced arguments appear as a single item, with no differentiation between the referent and the reference. Here we use the version of the data used in the CoNLL
shared tasks, where reference arguments were automatically separated from their corresponding referents
with simple pattern-matching rules.

108

fiCombination Strategies for Semantic Role Labeling

niak, 2000) to develop two individual models based on full syntactic analysis, and the chunk
i.e., basic syntactic phrase labels and clause boundaries to construct a partial-syntax
model. All individual models use the provided NE labels.
Switching from hand-corrected to automatically-generated syntactic information means
that the PropBank assumption that each argument (or argument fragment for discontinuous arguments) maps to one syntactic phrase no longer holds, due to errors of the syntactic
processors. Our analysis of the PropBank data indicates that only 91.36% of the semantic
arguments can be matched to exactly one phrase generated by the Charniak parser. Essentially, this means that SRL approaches that make the assumption that each semantic
argument maps to one syntactic construct can not recognize almost 9% of the arguments.
The same statement can be made about approaches based on partial syntax with the caveat
that in this setup arguments have to match a sequence of chunks. However, one expects
that the degree of compatibility between syntactic chunks and semantic arguments is higher
due to the ner granularity of the syntactic elements and because chunking algorithms perform better than full parsing algorithms. Indeed, our analysis of the same PropBank data
supports this observation: 95.67% of the semantic arguments can be matched to a sequence
of chunks generated by the CoNLL syntactic chunker.
Following the CoNLL-2005 setting we evaluated our system not only on PropBank but
also on a fresh test set, derived from the Brown corpus. This second evaluation allows us
to investigate the robustness of the proposed combination models.

3. Overview of the Combination Strategies
In this paper we introduce and analyze three combination strategies for the problem of
semantic role labeling. The three combination strategies are implemented on a shared
framework detailed in Figure 2 which consists of several stages: (a) generation of candidate arguments, (b) candidate scoring, and nally (c) inference. For clarity, we describe
rst the proposed combination framework, i.e., the vertical ow in Figure 2. Then, we move
to an overview of the three combination methodologies, shown horizontally in Figure 2.
In the candidate generation step, we merge the solutions of three individual SRL models
into a unique pool of candidate arguments. The individual SRL models range from complete
reliance on full parsing to using only partial syntactic information. For example, Model 1
is developed as a sequential tagger (using the B-I-O tagging scheme) with only partial
syntactic information (basic phrases and clause boundaries), whereas Model 3 uses full
syntactic analysis of the text and handles only arguments that map into exactly one syntactic
constituent. We detail the individual SRL models in Section 4 and empirically evaluate them
in Section 5.
In the candidate scoring phrase, we re-score all candidate arguments using both local
information, e.g., the syntactic structure of the candidate argument, and global information,
e.g., how many individual models have generated similar candidate arguments. We describe
all the features used for candidate scoring in Section 6.
Finally, in the inference stage the combination models search for the best solution that
is consistent with the domain constraints, e.g., two arguments for the same predicate cannot
overlap or embed, a predicate may not have more than one core argument (A0-5), etc.
109

fiSurdeanu, Marquez, Carreras, & Comas

Reliance on full syntax

Model 1

Model 2

Model 3

Candidate
Generation
Candidate Argument
Pool

Constraint
Satisfaction
Engine

Solution

Inference with
Constraint Satisfaction

Learning
(batch)

Learning
(online)

Dynamic
Programming
Engine

Dynamic
Programming
Engine

Solution

Candidate
Scoring

Inference

Solution

Inference with
Local Learning

Inference with
Global Learning

Figure 2: Overview of the proposed combination strategies.

All the combination approaches proposed in this paper share the same candidate argument pool. This guarantees that the results obtained by the dierent strategies on the
same corpus are comparable. On the other hand, even though the candidate generation
step is shared, the three combination methodologies dier signicantly in their scoring and
inference models.
The rst combination strategy analyzed, inference with constraint satisfaction, skips the
candidate scoring step completely and uses instead the probabilities output by the individual SRL models for each candidate argument. If the individual models raw activations are
not actual probabilities we convert them to probabilities using the softmax function (Bishop,
1995), before passing them to the inference component. The inference is implemented using
a Constraint Satisfaction model that searches for the solution that maximizes a certain
compatibility function. The compatibility function models not only the probability of the
global solution but also the consistency of the solution according to the domain constraints.
This combination strategy is based on the technique presented by Koomen et al. (2005).
The main dierence between the two systems is in the candidate generation step: we use
three independent individual SRL models, whereas Komen et al. used the same SRL model
110

fiCombination Strategies for Semantic Role Labeling

trained on dierent syntactic views of the data, i.e., the top parse trees generated by the
Charniak and Collins parsers (Charniak, 2000; Collins, 1999). Furthermore, we take our
argument candidates from the set of complete solutions generated by the individual models, whereas Komen et al. take them from dierent syntactic trees, before constructing any
complete solution. The obvious advantage of the inference model with Constraint Satisfaction is that it is unsupervised: no learning is necessary for candidate scoring, because the
scores of the individual models are used. On the other hand, the Constraint Satisfaction
model requires that the individual models provide raw activations, and, moreover, that the
raw activations be convertible to true probabilities.
The second combination strategy proposed in this article, inference with local learning,
re-scores all candidates in the pool using a set of binary discriminative classiers. The
classiers assign to each argument a score measuring the condence that the argument
is part of the correct, global solution. The classiers are trained in batch mode and are
completely decoupled from the inference module. The inference component is implemented
using a CKY-based dynamic programming algorithm (Younger, 1967). The main advantage
of this strategy is that candidates are re-scored using signicantly more information than
what is available to each individual model. For example, we incorporate features that count
the number of individual systems that generated the given candidate argument, several
types of overlaps with candidate arguments of the same predicate and also with arguments
of other predicates, structural information based on both full and partial syntax, etc. We
describe the rich feature set used for the scoring of candidate arguments in Section 6. Also,
this combination approach does not depend on the argument probabilities of the individual
SRL models (but can incorporate them as features, if available). This combination approach
is more complex than the previous strategy because it has an additional step that requires
supervised learning: candidate scoring. Nevertheless, this does not mean that additional
corpus is necessary: using cross validation, the candidate scoring classiers can be trained
on the same corpus used to train the individual SRL models. Moreover, we show in Section 8
that we obtain excellent performance even when the candidate scoring classiers are trained
on signicantly less data than the individual SRL models.
Finally, the inference strategy with global learning investigates the contribution of global
information to the inference model based on learning. This strategy incorporates global
information in the previous inference model in two ways. First and most importantly, candidate scoring is now trained online with global feedback from the inference component. In
other words, the online learning algorithm corrects the mistakes found when comparing the
correct solution with the one generated after inference. Second, we integrate global information in the actual inference component: instead of performing inference for each proposition
independently, we now do it for the whole sentence at once. This allows implementation of
additional global domain constraints, e.g., arguments attached to dierent predicates can
not overlap.
All the combination strategies proposed are described in detail in Section 7 and evaluated
in Section 8.
111

fiSurdeanu, Marquez, Carreras, & Comas

4. Individual SRL Models
This section introduces the three individual SRL models used by all the combination strategies discussed in this paper. The rst two models are variations of the same algorithm: they
both model the SRL problem as a sequential tagging task, where each semantic argument
is matched to a sequence of non-embedding phrases, but Model 1 uses only partial syntax
(chunks and clause boundaries), whereas Model 2 uses full syntax. The third model takes a
more traditional approach by assuming that there exists a one-to-one mapping between
semantic arguments and syntactic phrases.
It is important to note that all the combination strategies introduced later in the paper
are independent of the individual SRL models used. In fact, in Section 8 we describe
experiments that use not only these individual models but also the best performing SRL
systems at the CoNLL-2005 evaluation (Carreras & Marquez, 2005). Nevertheless, we
choose to focus mainly on the individual SRL approaches presented in this section for
completeness and to show that state-of-the-art performance is possible with relatively simple
SRL models.
4.1 Models 1 and 2
These models approach SRL as a sequential tagging task. In a pre-processing step, the
input syntactic structures are traversed in order to select a subset of constituents organized
sequentially (i.e., non embedding). The output of this process is a sequential tokenization of
the input sentence for each of the verb predicates. Labeling these tokens with appropriate
tags allows us to codify the complete argument structure of each predicate in the sentence.
More precisely, given a verb predicate, the sequential tokens are selected as follows:
First, the input sentence is split into disjoint sequential segments using as markers for
segment start/end the verb position and the boundaries of all the clauses that include
the corresponding predicate constituent. Second, for each segment, the set of top-most
non-overlapping syntactic constituents completely falling inside the segment are selected
as tokens. Finally, these tokens are labeled with B-I-O tags, depending if they are at the
beginning, inside, or outside of a predicate argument. Note that this strategy provides a set
of sequential tokens covering the complete sentence. Also, it is independent of the syntactic
annotation explored, assuming it provides clause boundaries.
Consider the example in Figure 3, which depicts the PropBank annotation of two verb
predicates of a sentence (release and hope) and the corresponding partial and full parse
trees. Since both verbs are in the main clause of the sentence, only two segments of the
sentence are considered for both predicates, i.e., those dening the left and right contexts
of the verbs ([w1 :Others, ..., w3 :just] and [w5 :from, ..., w20 :big-time] for predicate release,
and [w1 :Others, ..., w8 :,] and [w10 :the, ..., w20 :big-time] for the predicate hope). Figure 4
shows the resulting tokenization for both predicates and the two alternative syntactic structures. In this case, the correct argument annotation can be recovered in all cases, assuming
perfect labeling of the tokens.
It is worth noting that the resulting number of tokens to annotate is much lower than
the number of words in all cases. Also, the codications coming from full parsing have
substantially fewer tokens than those coming from partial parsing. For example, for the
predicate hope, the dierence in number of tokens between the two syntactic views is
112

fiCombination Strategies for Semantic Role Labeling

Clause
NP

I

6

VP
Clause

VP
1

NP

3

ADVP

4

II

NP

PP

VP

,

,

5

2

Others , just

released from the majors , hope the senior league

will be their bridge back into the bigtime.

Clause
8

1

NP

I

3

ADVP

2 , II
Others , just

A1

AMTMP

III

VP

IV 4

PP

V

5

NP

VI

7

VP

NP

,
6 VII
released from the majors , hope the senior league
P
A0

Clause

VP

NP

VIII

ADVP PP

NP

will be their bridge back into the bigtime.

A2
P

A1

Figure 3: Annotation of an example sentence with two alternative syntactic structures. The
lower tree corresponds to a partial parsing annotation (PP) with base chunks and
clause structure, while the upper represents a full parse tree (FP). Semantic roles
for two predicates (release and hope) are also provided for the sentence. The
encircled nodes in both trees correspond to the selected nodes by the process
of sequential tokenization of the sentence. We mark the selected nodes for the
predicate release with Western numerals and the nodes selected for hope with
Roman numerals. See Figure 4 for more details.

particularly large (8 vs. 2 tokens). Obviously, the coarser the token granularity, the easier
the problem of assigning correct output labelings (i.e., there are less tokens to label and
also the long-distance relations among sentence constituents can be better captured). On
the other hand, a coarser granularity tends to introduce more unrecoverable errors in the
pre-processing stage. There is a clear trade-o, which is dicult to solve in advance. By
using the two models in a combination scheme we can take advantage of the diverse sentence
tokenizations (see Sections 7 and 8).
Compared to the more common tree node labeling approaches (e.g., the following
Model 3), the B-I-O annotation of tokens has the advantage of permitting to correctly annotate some arguments that do not match a unique syntactic constituent. On the bad side,
the heuristic pre-selection of only some candidate nodes for each predicate, i.e., the nodes
that sequentially cover the sentence, makes the number of unrecoverable errors higher. Another source of errors common to all strategies are the errors introduced by real partial/full
parsers. We have calculated that due to syntactic errors introduced in the pre-processing
stage, the upper-bound recall gures are 95.67% for Model 1 and 90.32% for Model 2 using
the datasets dened in Section 8.
113

fiSurdeanu, Marquez, Carreras, & Comas

words
1: Others
2: ,
3: just
4: released
5: from
6: the
7: majors
8: ,
9: hope
10: the
11: senior
12: league
13: will
14: be
15: their
16: bridge
17: back
18: into
19: the
20: big-time

releasePP
1: B A1
2: O
3: B AM-TMP

4: B A2
5: I A2
6: O
7: O

tokens
releaseFP
hopePP
1: B A1
I: B A0
2: O
II: I A0
3: B AM-TMP
III: I A0

IV: I A0
V: I A0
4: B A2
VI: I A0
5: O

hopeFP

I: B A0

VII: I A0




VIII: B A1

II: B A1

6: O
8: O

Figure 4: Sequential tokenization of the sentence in Figure 3 according to the two syntactic
views and predicates (PP stands for partial parsing and FP for full parsing). The
sentence and semantic role annotations are vertically displayed. Each token is
numbered with the indexes that appear in the tree nodes of Figure 3 and contains
the B-I-O annotation needed to codify the proper semantic role structure.

Approaching SRL as a sequential tagging task is not new. Hacioglu, Pradhan, Ward,
Martin, and Jurafsky (2004) presented a system based on sequential tagging of base chunks
with B-I-O labels, which was the best performing SRL system at the CoNLL-2004 shared
task (Carreras & Marquez, 2004). The novelty of our approach resides in the fact that the
sequence of syntactic tokens to label is extracted from a hierarchical syntactic annotation
(either a partial or a full parse tree) and it is not restricted to base chunks (i.e., a token
may correspond to a complex syntactic phrase or even a clause).
4.1.1 Features
Once the tokens selected are labeled with B-I-O tags, they are converted into training
examples by considering a rich set of features, mainly borrowed from state-of-the-art systems (Gildea & Jurafsky, 2002; Carreras, Marquez, & Chrupala, 2004; Xue & Palmer,
2004). These features codify properties from: (a) the focus token, (b) the target predicate,
(c) the sentence fragment between the token and predicate, and (d) the dynamic context,
i.e., B-I-O labels previously generated. We describe these four feature sets next.2
2. Features extracted from partial parsing and Named Entities are common to Model 1 and 2, while features
coming from full parse trees only apply to Model 2.

114

fiCombination Strategies for Semantic Role Labeling

Constituent structure features:
 Constituent type and head: extracted using the head-word rules of Collins (1999).
If the rst element is a PP chunk, then the head of the rst NP is extracted. For
example, the type of the constituent in the U.S. in Figure 1 is PP, but its head is
U.S. instead of in.
 First and last words and POS tags of the constituent, e.g., in/IN and U.S./NNP
for the constituent in the U.S. in Figure 1.
 POS sequence: if it is less than 5 tags long, e.g., INDTNNP for the above sample
constituent.
 2/3/4-grams of the POS sequence.
 Bag-of-words of nouns, adjectives, and adverbs. For example, the bag-of-nouns for
the constituent The luxury auto maker is {luxury, auto, maker}.
 TOP sequence: sequence of types of the top-most syntactic elements in the constituent
(if it is less than 5 elements long). In the case of full parsing this corresponds to the
right-hand side of the rule expanding the constituent node. For example, the TOP
sequence for the constituent in the U.S. is INNP.
 2/3/4-grams of the TOP sequence.
 Governing category as described by Gildea and Jurafsky (2002), which indicates if NP
arguments are dominated by a sentence (typical for subjects) or a verb phrase (typical
for objects). For example, the governing category for the constituent 1,214 cars in
Figure 1 is VP, which hints that its corresponding semantic role will be object.
 NamedEntity, indicating if the constituent embeds or strictly matches a named entity
along with its type. For example, the constituent in the U.S. embeds a locative
named entity: U.S..
 TMP, indicating if the constituent embeds or strictly matches a temporal keyword
(automatically extracted from AM-TMP arguments of the training set). Among the most
common temporal cue words extracted are: year, yesterday, week, month,
etc. We used a total of 109 cue words.
 Previous and following words and POS tag of the constituent. For example, the
previous word for the constituent last year in Figure 1 is maker/NN, and the next
one is sold/VBD.
 The same features characterizing focus constituents are extracted for the two previous
and following tokens, provided they are inside the boundaries of the current segment.
Predicate structure features:
 Predicate form, lemma, and POS tag, e.g., sold, sell, and VBD for the predicate in
Figure 1.
 Chunk type and cardinality of verb phrase in which verb is included: single-word or
multi-word. For example, the predicate in Figure 1 is included in a single-word VP
chunk.
115

fiSurdeanu, Marquez, Carreras, & Comas

 The predicate voice. We distinguish ve voice types: active, passive, copulative,
innitive, and progressive.
 Binary ag indicating if the verb is a start/end of a clause.
 Sub-categorization rule, i.e., the phrase structure rule that expands the predicates
immediate parent, e.g., S  NP NP VP for the predicate in Figure 1.
Predicate-constituent features:
 Relative position, distance in words and chunks, and level of embedding (in number of
clause-levels) with respect to the constituent. For example, the constituent in the
U.S. in Figure 1 appears after the predicate, at a distance of 2 words or 1 chunk,
and its level of embedding is 0.
 Constituent path as described by Gildea and Jurafsky (2002) and all 3/4/5-grams of
path constituents beginning at the verb predicate or ending at the constituent. For
example, the syntactic path between the constituent The luxury auto maker and
the predicate sold in Figure 1 is NP  S  VP  VBD.
 Partial parsing path as described by Carreras et al. (2004) and all 3/4/5-grams of path
elements beginning at the verb predicate or ending at the constituent. For example,
the path NP + PP + NP + S  VP  VBD indicates that from the current NP token
to the predicate there are PP, NP, and S constituents to the right (positive sign) at
the same level of the token and then the path descends through the clause and a VP
to nd the predicate. The dierence from the previous constituent path is that we do
not have up arrows anymore but we introduce horizontal (left/right) movements at
the same syntactic level.
 Syntactic frame as described by Xue and Palmer (2004). The syntactic frame captures
the overall sentence structure using the predicate and the constituent as pivots. For
example, the syntactic frame for the predicate sold and the constituent in the
U.S. is NPNPVPNPPP, with the current predicate and constituent emphasized.
Knowing that there are other noun phrases before the predicate lowers the probability
that this constituent serves as an agent (or A0).
Dynamic features:
 BIOtag of the previous token. When training, the correct labels of the left context
are used. When testing, this feature is dynamically codied as the tag previously
assigned by the SRL tagger.
4.1.2 Learning Algorithm and Sequence Tagging
We used generalized AdaBoost with real-valued weak classiers (Schapire & Singer, 1999) as
the base learning algorithm. Our version of the algorithm learns xed-depth small decision
trees as weak rules, which are then combined in the ensemble constructed by AdaBoost.
We implemented a simple one-vs-all decomposition to address multi-class classication. In
this way, a separate binary classier has to be learned for each B-X and I-X argument label
plus an extra classier for the O decision.
116

fiCombination Strategies for Semantic Role Labeling

AdaBoost binary classiers are then used for labeling test sequences, from left to right,
using a recurrent sliding window approach with information about the tags assigned to the
preceding tokens. As explained in the previous list of features, left tags already assigned
are dynamically codied as features. Empirically, we found that the optimal left context to
be taken into account reduces to only the previous token.
We tested two dierent tagging procedures. First, a greedy left-to-right assignment of
the best scored label for each token. Second, a Viterbi search of the label sequence that
maximizes the probability of the complete sequence. In this case, the classiers predictions
were converted into probabilities using the softmax function described in Section 7.1. No
signicant improvements were obtained from the latter. We selected the former, which is
faster, as our basic tagging algorithm for the experiments.
Finally, this tagging model enforces three basic constraints: (a) the B-I-O output labeling must codify a correct structure; (b) arguments cannot overlap with clause nor chunk
boundaries; and (c) for each verb, A0-5 arguments not present in PropBank frames (taking
the union of all rolesets for the dierent verb senses) are not considered.
4.2 Model 3
The third individual SRL model makes the strong assumption that each predicate argument
maps to one syntactic constituent. For example, in Figure 1 A0 maps to a noun phrase,
AM-LOC maps to a prepositional phrase, etc. This assumption holds well on hand-corrected
parse trees and simplies signicantly the SRL process because only one syntactic constituent has to be correctly classied in order to recognize one semantic argument. On the
other hand, this approach is limited when using automatically-generated syntactic trees. For
example, only 91.36% of the arguments can be mapped to one of the syntactic constituents
produced by the Charniak parser.
Using a bottom-up approach, Model 3 maps each argument to the rst syntactic constituent that has the exact same boundaries and then climbs as high as possible in the
tree across unary production chains. We currently ignore all arguments that do not map
to a single syntactic constituent. The argument-constituent mapping is performed on the
training set as preprocessing step. Figure 1 shows a mapping example between the semantic
arguments of one verb and the corresponding sentence syntactic structure.
Once the mapping process completes, Model 3 extracts a rich set of lexical, syntactic,
and semantic features. Most of these features are inspired from previous work in parsing
and SRL (Collins, 1999; Gildea & Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al.,
2005a). We describe the complete feature set implemented in Model 3 next.
4.2.1 Features
Similarly to Models 1 and 2 we group the features in three categories, based on the properties
they codify: (a) the argument constituent, (b) the target predicate, and (c) the relation
between the constituent and predicate syntactic constituents.
Constituent structure features:
 The syntactic label of the candidate constituent.

 The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag.
117

fiSurdeanu, Marquez, Carreras, & Comas

 The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE
label. Content words, which add informative lexicalized information dierent from the
head word, were detected using the heuristics of Surdeanu et al. (2003). For example,
the head word of the verb phrase had placed is the auxiliary verb had, whereas
the content word is placed. Similarly, the content word of prepositional phrases is
not the preposition itself (which is selected as the head word), but rather the head
word of the attached phrase, e.g., U.S. for the prepositional phrase in the U.S..
 The first and last constituent words and their POS tags.
 NE labels included in the candidate phrase.
 Binary features to indicate the presence of temporal cue words, i.e., words that appear
often in AM-TMP phrases in training. We used the same list of temporal cue words as
Models 1 and 2.
 For each Treebank syntactic label we added a feature to indicate the number of such
labels included in the candidate phrase.
 The TOP sequence of the constituent (constructed similarly to Model 2).
 The phrase label, head word and POS tag of the constituent parent, left sibling, and
right sibling.
Predicate structure features:
 The predicate word and lemma.
 The predicate voice. Same denition as Models 1 and 2.
 A binary feature to indicate if the predicate is frequent (i.e., it appears more than
twice in the training data) or not.
 Sub-categorization rule. Same denition as Models 1 and 2.
Predicate-constituent features:
 The path in the syntactic tree between the argument phrase and the predicate as
a chain of syntactic labels along with the traversal direction (up or down). It is
computed similarly to Model 2.
 The length of the above syntactic path.
 The number of clauses (S* phrases) in the path. We store the overall clause count
and also the number of clauses in the ascending and descending part of the path.
 The number of verb phrases (VP) in the path. Similarly to the above feature, we store
three numbers: overall verb count, and the verb count in the ascending/descending
part of the path.
 Generalized syntactic paths. We generalize the path in the syntactic tree, when it
appears with more than 3 elements, using two templates: (a) Arg  Ancestor  Ni 
Pred, where Arg is the argument label, Pred is the predicate label, Ancestor is the
label of the common ancestor, and Ni is instantiated with each of the labels between
118

fiCombination Strategies for Semantic Role Labeling

Pred and Ancestor in the full path; and (b) Arg  Ni  Ancestor  Pred, where Ni is
instantiated with each of the labels between Arg and Ancestor in the full path. For
example, in the path NP  S  VP  SBAR  S  VP the argument label is the rst NP, the
predicate label is the last VP, and the common ancestors label is the rst S. Hence,
using the last template, this path is generalized to the following three features: NP 
S  VP  VP, NP  S  SBAR  VP, and NP  S  S  VP. This generalization reduces the
sparsity of the complete constituent-predicate path feature using a dierent strategy
than Models 1 and 2, which implement a n-gram based approach.
 The subsumption count, i.e., the dierence between the depths in the syntactic tree
of the argument and predicate constituents. This value is 0 if the two phrases share
the same parent.
 The governing category, similar to Models 1 and 2.

 The surface distance between the predicate and the argument phrases encoded as:
the number of tokens, verb terminals (VB*), commas, and coordinations (CC) between the argument and predicate phrases, and a binary feature to indicate if the two
constituents are adjacent. For example, the surface distance between the argument
candidate Others and the predicate hope in the Figure 3 example: Others, just
released from the majors, hope the senior league... is 7 tokens, 1 verb, 2 commas,
and 0 coordinations. These features, originally proposed by Collins (1999) for his dependency parsing model, capture robust, syntax-independent information about the
sentence structure. For example, a constituent is unlikely to be the argument of a
verb if another verb appears between the two phrases.

 A binary feature to indicate if the argument starts with a predicate particle, i.e., a
token seen with the RP* POS tag and directly attached to the predicate in training.
The motivation for this feature is to avoid the inclusion of predicate particles in the
argument constituent. For example, without this feature, a SRL system will tend to
incorrectly include the predicate particle in the argument for the text: take [A1 over
the organization], because the marked text is commonly incorrectly parsed as a
prepositional phrase and a large number of prepositional phrases directly attached to
a verb are arguments for the corresponding predicate.
4.2.2 Classifier
Similarly to Models 1 and 2, Model 3 trains one-vs-all classiers using AdaBoost for the most
common argument labels. To reduce the sample space, Model 3 selects training examples
(both positive and negative) only from: (a) the rst clause that includes the predicate, or
(b) from phrases that appear to the left of the predicate in the sentence. More than 98%
of the argument constituents fall into one of these classes.
At prediction time the classiers are combined using a simple greedy technique that
iteratively assigns to each predicate the argument classied with the highest condence. For
each predicate we consider as candidates all AM attributes, but only numbered attributes
indicated in the corresponding PropBank frame. Additionally, this greedy strategy enforces
a limited number of domain knowledge constraints in the generated solution: (a) arguments
can not overlap in any form, (b) no duplicate arguments are allowed for A0-5, and (c) each
119

fiSurdeanu, Marquez, Carreras, & Comas

predicate can have numbered arguments, i.e., A0-5, only from the subset present in its
PropBank frame. These constraints are somewhat dierent from the constraints used by
Models 1 and 2: (i) Model 3 does not use the B-I-O representation hence the constraint
that the B-I-O labeling be correct does not apply; and (ii) Models 1 and 2 do not enforce
the constraint that numbered arguments can not be duplicated because its implementation
is not straightforward in this architecture.

5. Performance of the Individual Models
In this section we analyze the performance of the three individual SRL models proposed.
Our three SRL systems were trained using the complete CoNLL-2005 training set (PropBank/Treebank sections 2 to 21). To avoid the overtting of the syntactic processors i.e.,
part-of-speech tagger, chunker, and Charniaks full parser we partitioned the PropBank
training set into ve folds and for each fold we used the output of the syntactic processors
that were trained on the other four folds. The models were tuned on a separate development partition (Treebank section 24) and evaluated on two corpora: (a) Treebank section
23, which consists of Wall Street Journal (WSJ) documents, and (b) on three sections of the
Brown corpus, semantically annotated by the PropBank team for the CoNLL-2005 shared
task evaluation.
All the classiers for our individual models were developed using AdaBoost with decision trees of depth 4 (i.e., each branch may represent a conjunction of at most 4 basic
features). Each classication model was trained for up to 2,000 rounds. We applied some
simplications to keep training times and memory requirements inside admissible bounds:
(a) we have trained only the most frequent argument labels: top 41 for Model 1, top 35
for Model 2, and top 24 for Model 3; (b) we discarded all features occurring less than 15
times in the training set, and (c) for each Model 3 classier, we have limited the number of
negative training samples to the rst 500,000 negative samples extracted in the PropBank
traversal3 .
Table 1 summarizes the results of the three models on the WSJ and Brown corpora.
We include the percentage of perfect propositions detected by each model (PProps),
i.e., predicates recognized with all their arguments, the overall precision, recall, and F1
measure4 . The results summarized in Table 1 indicate that all individual systems have
a solid performance. Although none of them would rank in the top 3 in the CoNLL2005 evaluation (Carreras & Marquez, 2005), their performance is comparable to the best
individual systems presented at that evaluation exercise5 . Consistently with other systems
evaluated on the Brown corpus, all our models experience a severe performance drop in this
corpus, due to the lower performance of the linguistic processors.
As expected, the models based on full parsing (2 and 3) perform better than the model
based on partial syntax. But, interestingly, the dierence is not large (e.g., less than 2 points
3. The distribution of samples for the Model 3 classifiers is very biased towards negative samples because, in
the worst case, any syntactic constituent in the same sentence with the predicate is a potential argument.
4. The significance intervals for the F1 measure have been obtained using bootstrap resampling (Noreen,
1989). F1 rates outside of these intervals are assumed to be significantly different from the related F1
rate (p < 0.05).
5. The best performing SRL systems at CoNLL were a combination of several subsystems. See section 9
for details.

120

fiCombination Strategies for Semantic Role Labeling

WSJ
Model 1
Model 2
Model 3
Brown
Model 1
Model 2
Model 3

PProps
48.45%
52.04%
45.28%

Precision
78.76%
79.65%
80.32%

Recall
72.44%
74.92%
72.95%

F1
75.47 0.8
77.21 0.8
76.46 0.6

30.85%
36.44%
29.48%

67.72%
71.82%
72.41%

58.29%
64.03%
59.67%

62.65 2.1
67.70 1.9
65.42 2.1

Table 1: Overall results of the individual models in the WSJ and Brown test sets.
Model 1 F1
Model 2 F1
Model 3 F1

A0
83.37
86.65
86.14

A1
75.13
77.06
75.83

A2
67.33
65.04
65.55

A3
61.92
62.72
65.26

A4
72.73
72.43
73.85

Table 2: F1 scores of the individual systems for the A04 arguments in the WSJ test.
in F1 in the WSJ corpus), evincing that having base syntactic chunks and clause boundaries
is enough to obtain competitive performance. More importantly, the full-parsing models are
not always better than the partial-syntax model. Table 2 lists the F1 measure for the three
models for the rst ve numbered arguments. Table 2 shows that Model 2, our overall
best performing individual system, achieves the best F-measure for A0 and A1 (typically
subjects and direct objects), but Model 1, the partial-syntax model, performs best for
the A2 (typically indirect objects, instruments, or benefactives). The explanation for this
behavior is that indirect objects tend to be farther from their predicates and accumulate
more parsing errors. From the models based on full syntax, Model 2 has better recall
whereas Model 3 has better precision, because Model 3 lters out all candidate arguments
that do not match a single syntactic constituent. Generally, Table 2 shows that all models
have strong and weak points. This is further justication for our focus on combination
strategies that combine several independent models.

6. Features of the Combination Models
As detailed in Section 3, in this paper we analyze two classes of combination strategies for
the problem of semantic role labeling: (a) an inference model with constraint satisfaction,
which nds the set of candidate arguments that maximizes a global cost function, and (b)
two inference strategies based on learning, where candidates are scored and ranked using
discriminative classiers. From the perspective of the feature space, the main dierence
between these two types of combination models is that the input of the rst combination strategy is limited to the argument probabilities produced by the individual systems,
whereas the last class of combination approaches incorporates a much larger feature set in
their ranking classiers. For robustness, in this paper we use only features that are extracted from the solutions provided by the individual systems, hence are independent of the
121

fiSurdeanu, Marquez, Carreras, & Comas

1111
0000
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
A0

A0

V

V

A1

A1

V

A1

A2

A4

M1

M2

M3

Figure 5: Sample solutions proposed for the same predicate by three individual SRL models:
M1, M2 and M3. Argument candidates are displayed vertically for each system.

individual models6 . We describe all these features next. All examples given in this section
are based on Figures 5 and 6.
Voting features  these features quantify the votes received by each argument from the
individual systems. This set includes the following features:
 The label of the candidate argument, e.g., A0 for the rst argument proposed by system
M1 in Figure 5.
 The number of systems that generated an argument with this label and span. For the
example shown in Figure 5, this feature has value 1 for the argument A0 proposed by
M1 and 2 for M1s A1, because system M2 proposed the same argument.
 The unique ids of all the systems that generated an argument with this label and
span, e.g., M1 and M2 for the argument A1 proposed by M1 or M2 in Figure 5.
 The argument sequence for this predicate for all the systems that generated an argument with this label and span. For example, the argument sequence generated by
system M1 for the proposition illustrated in Figure 5 is: A0 - V - A1 - A2. This feature attempts to capture information at proposition level, e.g., a combination model
might learn to trust model M1 more for the argument sequence A0 - V - A1 - A2,
M2 for another sequence, etc.
Same-predicate overlap features  these features measure the overlap between dierent
arguments produced by the individual SRL models for the same predicate:
6. With the exception of the argument probabilities, which are required by the constraint satisfaction model.

122

fiCombination Strategies for Semantic Role Labeling

 The number and unique ids of all the systems that generated an argument with the
same span but different label. For the example shown in Figure 5, these features have
values 1 and M2 for the argument A2 proposed by M1, because model M2 proposed
argument A4 with the same span.
 The number and unique ids of all the systems that generated an argument included
in the current argument. For the candidate argument A0 proposed by model M1 in
Figure 5, these features have values 1 and M3, because M3 generated argument A0,
which is included in M1s A0.
 In the same spirit, we generate the number and unique ids of all the systems that
generated an argument that contains the current argument, and the number and
unique ids of all the systems that generated an argument that overlaps  but does
not include nor contain  the current argument.
Other-predicate overlap features  these features quantify the overlap between dierent arguments produced by the individual SRL models for other predicates. We generate the
same features as the previous feature group, with the dierence that we now compare arguments generated for dierent predicates. The motivation for these overlap features is that,
according to the PropBank annotations, no form of overlap is allowed among arguments
attached to the same predicate, and only inclusion or containment is permitted between
arguments assigned to dierent predicates. The overlap features are meant to detect when
these domain constraints are not satised by a candidate argument, which is an indication,
if the evidence is strong, that the candidate is incorrect.
Partial-syntax features  these features codify the structure of the argument and the
distance between the argument and the predicate using only partial syntactic information,
i.e., chunks and clause boundaries (see Figure 6 for an example). Note that these features
are inherently dierent from the features used by Model 1, because Model 1 evaluates each
individual chunk part of a candidate argument, whereas here we codify properties of the
complete argument constituent. We describe the partial-syntax features below.
 Length in tokens and chunks of the argument constituent, e.g., 4 and 1 for argument
A0 in Figure 6.
 The sequence of chunks included in the argument constituent, e.g., PP NP for the
argument AM-LOC in Figure 6. If the chunk sequence is too large, we store n-grams of
length 10 for the start and end of the sequence.
 The sequence of clause boundaries, i.e., clause beginning or ending, included in the
argument constituent.
 The named entity types included in the argument constituent, e.g., LOCATION for the
AM-LOC argument in Figure 6.
 Position of the argument: before/after the predicate in the sentence, e.g., after for A1
in Figure 6.
 A Boolean ag to indicate if the argument constituent is adjacent to the predicate,
e.g., false for A0 and true for A1 in Figure 6.
123

fiSurdeanu, Marquez, Carreras, & Comas

Clause

NP

NP

VP

NP

PP

NP

The luxury auto maker last year sold 1,214 cars in the U.S.
A0

AMTMP

P

A1

AMLOC

Figure 6: Sample proposition with partial syntactic information.
 The sequence of chunks between the argument constituent and the predicate, e.g., the
chunk sequence between the predicate and the argument AM-LOC in Figure 6 is: NP.
Similarly to the above chunk sequence feature, if the sequence is too large, we store
starting and ending n-grams.
 The number of chunks between the predicate and the argument, e.g., 1 for AM-LOC in
Figure 6.
 The sequence of clause boundaries between the argument constituent and the predicate.
 The clause subsumption count, i.e., the dierence between the depths in the clause
tree of the argument and predicate constituents. This value is 0 if the two phrases are
included in the same clause.
Full-syntax features  these features codify the structure of the argument constituent,
the predicate, and the distance between the two using full syntactic information. The
full-syntax features are replicated from Model 3 (see Section 4.2), which assumes that a
one-to-one mapping from semantic constituents to syntactic phrases exists. Unlike Model 3
which ignores arguments that can not be matched against a syntactic constituent, if such
an exact mapping does not exist due to the inclusion of candidates from Models 1 and 2, we
generate an approximate mapping from the unmapped semantic constituent to the largest
phrase that is included in the given span and has the same left boundary as the semantic constituent. This heuristic guarantees that we capture at least some of the semantic
constituents syntactic structure.
The motivation for the partial and full-syntax features is to learn the preferences of
the individual SRL models. For example, with these features a combination classier might
learn to trust model M1 for arguments that are closer than 3 chunks to the predicate, model
M2 when the predicate-argument syntactic path is NP  S  VP  SBAR  S  VP, etc.
Individual systems argument probabilities  each individual model outputs a condence score for each of their proposed arguments. These scores are converted into probabilities using the softmax function as described in detail in Section 7.1. The combination
strategy based on constraint satisfaction (Section 7.1) uses these probabilities as they are,
while the other two strategies based on meta-learning (Section 7.2) have to discretize the
probabilities to include them as features. To do so, each probability value is matched to
124

fiCombination Strategies for Semantic Role Labeling

one of ve probability intervals and the corresponding interval is used as the feature. The
probability intervals are dynamically constructed for each argument label and each individual system such that the corresponding system predictions for this argument label are
uniformly distributed across the intervals.
In Section 8.4 we empirically analyze the contribution of each of these proposed feature
sets to the performance of our best combination model.

7. Combination Strategies
In this section we detail the combination strategies proposed in this paper: (a) a combination
model with constraint satisfaction, which aims at nding the set of candidate arguments
that maximizes a global cost function, and (b) two combination models with inference based
on learning, where candidates are scored and ranked using discriminative classiers. In the
previous section we described the complete feature set made available to all approaches.
Here we focus on the machine learning paradigm deployed by each of the combination
models.
7.1 Inference with Constraint Satisfaction
The Constraint Satisfaction model selects a subset of candidate arguments that maximizes
a compatibility function subject to the fulllment of a set of structural constraints that
ensure consistency of the solution. The compatibility function is based on the probabilities
given by individual SRL models to the candidate arguments. In this work we use Integer
Linear Programming to solve the constraint satisfaction problem. This approach was rst
proposed by Roth and Yih (2004) and applied to semantic role labeling by Punyakanok,
Roth, Yih, and Zimak (2004), Koomen et al. (2005), among others. We follow the setting
of Komen et al., which is taken as a reference.
As a rst step, the scores from each model are normalized into probabilities. The scores
yielded by the classiers are signed and unbounded real numbers, but experimental evidence
shows that the condence in the predictions (taken as the absolute value of the raw scores)
correlates well with the classication accuracy. Thus, the softmax function (Bishop, 1995)
is used to convert the set of unbounded scores into probabilities. If there are k possible
output labels for a given argument and sco(li ) denotes the score of label li output by a
xed SRL model, then the estimated probability for this label is:
esco(li )
p(li ) = Pk
sco(lj )
j=1 e
The  parameter of the above formula can be empirically adjusted to avoid overly skewed
probability distributions and to normalize the scores of the three individual models to a
similar range of values. See more details about our experimental setting in Section 8.1.
Candidate selection is performed via Integer Linear Programming (ILP). The program
goal is to maximize a compatibility function modeling the global condence of the selected
set of candidates, subject to a set of linear constraints. All the variables involved in the
task take integer values and may appear in rst degree polynomials only.
An abstract ILP process can be described in a simple fashion as: given a set of variables V = {v1 , . . . , vn }, it aims to maximize the global compatibility of a label assignment
125

fiSurdeanu, Marquez, Carreras, & Comas

{l1 , . . . , ln } to these variables. A local compatibility function cv (l) denes the compatibility
of assigning label l to variable v. The global compatibility function C(l1 , . . . , ln ) is taken
as the sum of each local assignment compatibility, so the goal of the ILP process can be
written as:
argmax C(l1 , . . . , ln ) = argmax
l1 ,...,ln

l1 ,...,ln

n
X

cvi (li )

i=1

where the constraints are described in a set of accompanying integer linear equations involving the variables of the problem.
If one wants to codify soft constraints instead of hard, there is the possibility of considering them as a penalty component in the compatibility function. In this case, each constraint
r  R can be seen as a function which takes the current label assignment and outputs a
real number, which is 0 when the constraint is satised and a positive number when not,
indicating the penalty imposed to the compatibility function. The new expression of the
compatibility function to maximize is:
C(l1 , . . . , ln ) =

n
X
i=1

cvi (li ) 

X

r(l1 , . . . , ln )

rR

Note that the hard constraints can also be simulated in this setting by making them output
a very large positive number when they are violated.
In our particular problem, we have a binary-valued variable vi for each of the N argument candidates generated by the SRL models, i.e., li labels are in {0, 1}. Given a label
assignment, the arguments with li = 1 are selected to form the solution, while the others
(those where li = 0) are ltered out. For each variable vi , we also have the probability
values, pij , calculated from the score of model j on argument i, according to the softmax
formula described above7 . In a rst approach, the compatibility function cv (li ) equals to
P
8
( M
j=1 pij )li , where the number of models, M , is 3 in our case .
Under this denition, maximizing the compatibility function is equivalent to maximizing
the sum of the probabilities given by the models to the argument candidates considered in
the solution. Since this function is always positive, the global score increases directly with
the number of selected candidates. As a consequence, the model is biased towards the
maximization of the number of candidates included in the solution (e.g., tending to select
a lot of small non-overlapping arguments). Following Koomen et al. (2005), this bias can
be corrected by adding a new score oi , which sums to the compatibility function when the
i-th candidate is not selected in the solution. The global compatibility function needs to be
rewritten to encompass this new information. Formalized as an ILP equation, it looks like:
argmax C(l1 , . . . , lN ) = argmax

L{0,1}N

L{0,1}N

M
N X
X

(

i=1 j=1

pij )li + oi (1  li )

7. If model j does not propose argument i then we consider pij = 0.
8. Instead of accumulating the probabilities of all models for a given candidate argument, one could consider
a different variable for each model prediction and introduce a constraint forcing all these variables to
take the same value at the end of the optimization problem. The two alternatives are equivalent.

126

fiCombination Strategies for Semantic Role Labeling

where the constraints are expressed in separated integer linear equations. It is not possible
to dene a priori the value of oi . Komen et al. used a validation corpus to empirically
estimate a constant value for all oi (i.e., independent from the argument candidate)9 . We
will use exactly the same solution of working with a single constant value, to which we will
refer as O.
Regarding the consistency constraints, we have considered the following six:
1. Two candidate arguments for the same verb can not overlap nor embed.
2. A verb may not have two core arguments with the same type label A0-A5.
3. If there is an argument R-X for a verb, there has to be also an X argument for the
same verb.
4. If there is an argument C-X for a verb, there has to be also an X argument before the
C-X for the same verb.
5. Arguments from two dierent verbs can not overlap, but they can embed.
6. Two dierent verbs can not share the same AM-X, R-AM-X or C-X arguments.
Constraints 14 are also included in our reference work (Punyakanok et al., 2004). No
other constraints from that paper need to be checked here since each individual model
outputs only consistent solutions. Constraints 5 and 6, which restrict the set of compatible
arguments among dierent predicates in the sentence, are original to this work. In the
Integer Linear Programming setting the constraints are written as inequalities. For example,
if Ai is the argument label of the i-th candidate and Vi its verb predicate, constraint number
P
2 is written as: (Ai =a  Vi =v) li  1, for a given verb v and argument label a. The other
constraints have similar translations into inequalities.
Constraint satisfaction optimization will be applied in two dierent ways to obtain
the complete output annotation of a sentence. In the rst one, we proceed verb by verb
independently to nd their best selection of candidate arguments using only constraints
1 through 4. We call this approach local optimization. In the second scenario all the
candidate arguments in the sentence are considered at once and constraints 1 through 6 are
enforced. We will refer to this second strategy as global optimization. In both scenarios the
compatibility function will be the same, but constraints need some rewriting in the global
scenario because they have to include information about the concrete predicate.
In Section 8.3 we will extensively evaluate the presented inference model based on Constraint Satisfaction, and we will describe some experiments covering the following topics: (a)
the contribution of each of the proposed constraints; (b) the performance of local vs. global
optimization; and (c) the precisionrecall tradeo by varying the value of the bias-correction
parameter.
7.2 Inference Based On Learning
This combination model consists of two stages: a candidate scoring phase, which scores
candidate arguments in the pool using a series of discriminative classiers, and an inference
stage, which selects the best overall solution that is consistent with the domain constraints.
9. Instead of working with a constant, one could try to set the oi value for each candidate, taking into
account some contextual features of the candidate. We plan to explore this option in the near future.

127

fiSurdeanu, Marquez, Carreras, & Comas

The rst and most important component of this combination strategy is the candidate
scoring module, which assigns to each candidate argument a score equal to the condence
that this argument is part of the global solution. It is formed by discriminative functions,
one for each role label. Below, we devise two dierent strategies to train the discriminative
functions.
After scoring candidate arguments, the nal global solution is built by the inference
module, which looks for the best scored argument structure that satises the domain specic
constraints. Here, a global solution is a subset of candidate arguments, and its score is
dened as the sum of condence values of the arguments that form it. We currently consider
three constraints to determine which solutions are valid:
(a) Candidate arguments for the same predicate can not overlap nor embed.
(b) In a predicate, no duplicate arguments are allowed for the numbered arguments A0-5.
(c) Arguments of a predicate can be embedded within arguments of other predicates but
they can not overlap.
The set of constraints can be extended with any other rules, but in our particular case, we
know that some constraints, e.g., providing only arguments indicated in the corresponding PropBank frame, are already guaranteed by the individual models, and others, e.g.,
constraints 3 and 4 in the previous sub-section, have no positive impact on the overall
performance (see Section 8.3 for the empirical analysis). The inference algorithm we use
is a bottom-up CKY-based dynamic programming strategy (Younger, 1967). It builds the
solution that maximizes the sum of argument condences while satisfying the constraints,
in cubic time.
Next, we describe two dierent strategies to train the functions that score candidate
arguments. The rst is a local strategy: each function is trained as a binary batch classier,
independently of the combination process which enforces the domain constraints. The
second is a global strategy: functions are trained as online rankers, taking into account the
interactions that take place during the combination process to decide between one argument
or another.
In both training strategies, the discriminative functions employ the same representation of arguments, using the complete feature set described in Section 6 (we analyze the
contribution of each feature group in Section 8). Our intuition was that the rich feature
space introduced in Section 6 should allow the gathering of sucient statistics for robust
scoring of the candidate arguments. For example, the scoring classiers might learn that
a candidate is to be trusted if: (a) two individual systems proposed it, (b) if its label is
A2 and it was generated by Model 1, or (c) if it was proposed by Model 2 within a certain
argument sequence.
7.2.1 Learning Local Classifiers
This combination process follows a cascaded architecture, in which the learning component
is decoupled from the inference module. In particular, the training strategy consists of
training a binary classier for each role label. The target of each label-based classier is to
determine whether a candidate argument actually belongs to the correct proposition of the
corresponding predicate, and to output a condence value for this decision.
128

fiCombination Strategies for Semantic Role Labeling

The specic training strategy is as follows. The training data consists of a pool of
labeled candidate arguments (proposed by individual systems). Each candidate is either
positive, in that it is actually a correct argument of some sentence, or negative, if it is
not correct. The strategy trains a binary classier for each role label l, independently of
other labels. To do so, it concentrates on the candidate arguments of the data that have
label l. This forms a dataset for binary classication, specic to the label l. With it, a
binary classier can be trained using any of the existing techniques for binary classication,
with the only requirement that our combination strategy needs condence values with each
binary prediction. In Section 8 we provide experiments using SVMs to train such local
classiers.
In all, each classier is trained independently of other classiers and the inference module. Looking globally at the combination process, each classier can be seen as an argument
ltering component that decides which candidates are actual arguments using a much richer
representation than the individual models. In this context, the inference engine is used as a
conict resolution engine, to ensure that the combined solutions are valid argument structures for sentences.
7.2.2 Learning Global Rankers
This combination process couples learning and inference, i.e., the scoring functions are
trained to behave accurately within the inference module. In other words, the training
strategy here is global: the target is to train a global function that maps a set of argument
candidates for a sentence into a valid argument structure. In our setting, the global function
is a composition of scoring functions one for each label, same as the previous strategy.
Unlike the previous strategy, which is completely decoupled from the inference engine, here
the policy to map a set of candidates into a solution is that determined by the inference
engine.
In recent years, research has been very active in global learning methods for tagging,
parsing and, in general, structure prediction problems (Collins, 2002; Taskar, Guestrin, &
Koller, 2003; Taskar, Klein, Collins, Koller, & Manning, 2004; Tsochantaridis, Hofmann,
Joachims, & Altun, 2004). In this article, we make use of the simplest technique for global
learning: an online learning approach that uses Perceptron (Collins, 2002). The general
idea of the algorithm is similar to the original Perceptron (Rosenblatt, 1958): correcting
the mistakes of a linear predictor made while visiting training examples, in an additive
manner. The key point for learning global rankers relies on the criteria that determines
what is a mistake for the function being trained, an idea that has been exploited in a
similar way in multiclass and ranking scenarios by Crammer and Singer (2003a, 2003b).
The Perceptron algorithm in our combination system works as follows (pseudocode
of the algorithm is given in Figure 7). Let 1 . . . L be the possible role labels, and let
W = {w1 . . . wL } be the set of parameter vectors of the scoring functions, one for each
label. Perceptron initializes the vectors in W to zero, and then proceeds to cycle through
the training examples, visiting one at a time. In our case, a training example is a pair (y, A),
where y is the correct solution of the example and A is the set of candidate arguments for
it. Note that both y and A are sets of labeled arguments, and thus we can make use of
the set dierence. We will note as a a particular argument, as l the label of a, and as
129

fiSurdeanu, Marquez, Carreras, & Comas

Initialization: for each wl  W do wl = 0
Training :
for t = 1 . . . T do
for each training example (y, A) do
y = Inference(A, W)
for each a  y \ y do
let l be the label of a
wl = wl + (a)
for each a  y \ y do
let l be the label of a
wl = wl  (a)
Output: W
Figure 7: Perceptron Global Learning Algorithm

(a) the vector of features described in Section 6. With each example, Perceptron performs
two steps. First, it predicts the optimal solution y according to the current setting of
W. Note that the prediction strategy employs the complete combination model, including
the inference component. Second, Perceptron corrects the vectors in W according to the
mistakes seen in y: arguments with label l seen in y and not in y are promoted in vector
wl ; on the other hand, arguments in y and not in y are demoted in wl . This correction rule
moves the scoring vectors towards missing arguments, and away from predicted arguments
that are not correct. It is guaranteed that, as Perceptron visits more and more examples,
this feedback rule will improve the accuracy of the global combination function when the
feature space is almost linearly separable (Freund & Schapire, 1999; Collins, 2002).
In all, this training strategy is global because the mistakes that Perceptron corrects are
those that arise when comparing the predicted structure with the correct one. In contrast,
a local strategy identies mistakes looking individually at the sign of scoring predictions: if
some candidate argument is (is not) in the correct solution and the current scorers predict
a negative (positive) condence value, then the corresponding scorer is corrected with that
candidate argument. Note that this is the same criteria used to generate training data for
classiers trained locally. In Section 8 we compare these approaches empirically.
As a nal note, for simplicity we have described Perceptron in its most simple form.
However, the Perceptron version we use in the experiments reported in Section 8 incorporates two well-known extensions: kernels and averaging (Freund & Schapire, 1999; Collins
& Duy, 2002). Similar to SVM, Perceptron is a kernel method. That is, it can be represented in dual form, and the dot product between example vectors can be generalized by
a kernel function that exploits richer representations. On the other hand, averaging is a
technique that increases the robustness of predictions during testing. In the original form,
test predictions are computed with the parameters that result from the training process.
In the averaged version, test predictions are computed with an average of all parameter
vectors that are generated during training, after every update. Details of the technique can
be found in the original article of Freund & Schapire.
130

fiCombination Strategies for Semantic Role Labeling

1

Development
Brown
WSJ

0.98
0.96

Acuracy

0.94
0.92
0.9
0.88
0.86
0.84
0.82
0

10

20

30

40

50

60

70

80

90

100

Reject Rate (%)

Figure 8: Rejection curves of the estimated output probabilities of the individual models.

8. Experimental Results
In this section we analyze the performance of the three combination strategies previously
described: (a) inference with constraint satisfaction, (b) learning-based inference with local
rankers, and (c) learning-based inference with global rankers. For the bulk of the experiments we use candidate arguments generated by the three individual SRL models described
in Section 4 and evaluated in Section 5.
8.1 Experimental Settings
All combination strategies (with one exception, detailed below) were trained using the
complete CoNLL-2005 training set (PropBank/Treebank sections 2 to 21). To minimize the
overtting of the individual SRL models on the training data, we partitioned the training
corpus into ve folds and for each fold we used the output of the individual models when
trained on the remaining four folds. The models were tuned on a separate development
partition (Treebank section 24) and evaluated on two corpora: (a) Treebank section 23,
and (b) on the three annotated sections of the Brown corpus.
For the constraint satisfaction model, we converted the scores of the output arguments of
the three SRL models into probabilities using the softmax function explained in Section 7.1.
The development set (section 24) was used to tune the  parameter of the softmax formula
to a nal value of 0.1 for all models. In order to assess the quality of this procedure, we
plot in Figure 8 the rejection curves of the estimated output probabilities with respect to
classication accuracy on the development and test sets (WSJ and Brown). To calculate
these plots, the probability estimates of all three models are put together in a set and sorted
in decreasing order. At a certain level of rejection (n%), the curve in Figure 8 plots the
percentage of correct arguments when the lowest scoring n% subset is rejected. With few
131

fiSurdeanu, Marquez, Carreras, & Comas

exceptions, the curves are increasing and smooth, indicating a good correlation between
probability estimates and classication accuracy.
As a last experiment, in Section 8.6 we analyze the behavior of the proposed combination
strategies when the candidate pool is signicantly larger. For this experiment we used the
top 10 best performing systems at the CoNLL-2005 shared task evaluation. In this setup
there are two signicant dierences from the experiments that used our in-house individual
systems: (a) we had access only to the systems outputs on the PropBank development
section and on the two test sections, and (b) the argument probabilities of the individual
models were not available. Thus, instead of the usual training set, we had to train our
combination models on the PropBank development section with a smaller feature set. Note
also that the development set is only 3.45% of the size of the regular training set. We
evaluated the resulting combination models on the same two testing sections: WSJ and
Brown.
8.2 Lower and Upper Bounds of the Combination Strategies
Before we venture into the evaluation of the combination strategies, we explore the lower
and upper bounds of the combinations models on the given corpus and individual models.
This analysis is important in order to understand the potential of the proposed approach
and to see how close we actually are to realizing it.
The performance upper bound is calculated with an oracle combination system with
a perfect ltering classier that selects only correct candidate arguments and discards all
others. For comparison purposes, we have implemented a second oracle system that simulates a re-ranking approach: for each predicate it selects the candidate frame i.e., the
complete set of arguments for the corresponding predicate proposed by a single model
with the highest F1 score. Table 3 lists the results obtained on the WSJ and Brown corpora
by these two oracle systems using all three individual models. The combination system
is the oracle that simulates the combination strategies proposed in this paper, which break
candidate frames and work with individual candidate arguments. Note that the precision of
this oracle combination system is not 100% because in the case of discontinuous arguments,
fragments that pass the oracle lter are considered incorrect by the scorer when the corresponding argument is not complete, e.g., an argument A1 appears without the continuation
C-A1. The re-ranking columns list the results of the second oracle system, which selects
entire candidate frames.
Table 3 indicates that the upper limit of the combination approaches proposed in this
paper is relatively high: the F1 of the combination oracle system is over 14 points higher
than our best individual system in the WSJ test set, and over 17 points higher in the
Brown corpus (see Table 1). Furthermore, our analysis indicates that the potential of
our combination strategy is higher than that of re-ranking strategies, which are limited to
the performance of the best complete frame in the candidate pool. By allowing the recombination of arguments from the individual candidate solutions this threshold is raised
signicantly: over 6 F1 points in WSJ and over 9 F1 points in Brown.
Table 4 lists the distribution of the candidate arguments from the individual models in
the selection performed by the combination oracle system. For conciseness, we list only the
core numbered arguments and we focus on the WSJ corpus.  of 3 indicates the percent132

fiCombination Strategies for Semantic Role Labeling

WSJ
Brown

PProps
70.76%
51.87%

Combination
Precision Recall
99.12%
85.22%
99.63%
74.32%

F1
91.64
85.14

PProps
63.51%
45.02%

Re-Ranking
Precision Recall
88.08%
82.84%
80.80%
71.70%

F1
85.38
75.98

Table 3: Performance upper limits detected by the two oracle systems.
A0
A1
A2
A3
A4

 of 3
80.45%
69.82%
56.04%
56.03%
65.85%

 of 2
12.10%
17.83%
22.32%
21.55%
20.73%

Model 1
3.47%
7.45%
12.20%
12.93%
6.10%

Model 2
2.14%
2.77%
4.95%
5.17%
2.44%

Model 3
1.84%
2.13%
4.49%
4.31%
4.88%

Table 4: Distribution of the individual systems arguments in the upper limit selection, for
A0A4 in the WSJ test set.

age of correct arguments where all 3 models agreed,  of 2 indicates the percentage of
correct arguments where any 2 models agreed, and the other columns indicate the percentage of correct arguments detected by a single model. Table 4 indicates that, as expected,
two or more individual models agreed on a large percentage of the correct arguments. Nevertheless, a signicant number of correct arguments, e.g., over 22% of A3, come from a single
individual system. This proves that, in order to achieve maximum performance, one has
to look beyond simple voting strategies that favor arguments with high agreement between
individual systems.
We propose two lower bounds for the performance of the combination models using two
baseline systems:
 The rst baseline is recall-oriented: it merges all the arguments generated by the
individual systems. For conict resolution, the baseline uses an approximate inference
algorithm consisting of two steps: (i) candidate arguments are sorted using a radix
sort that orders the candidate arguments in descending order of: (a) number of models
that agreed on this argument, (b) argument length in tokens, and (c) performance of
the individual system10 ; (ii) Candidates are iteratively appended to the global solution
only if they do not violate any of the domain constraints with the arguments already
selected.
 The second baseline is precision-oriented: it considers only arguments where all three
individual systems agreed. For conict resolution it uses the same strategy as the
previous baseline system.
Table 5 shows the performance of these two baseline models. As expected, the precisionoriented baseline obtains a precision signicantly higher than the best individual model
(Table 1), but its recall suers because the individual models do not agree on a fairly large
number of candidate arguments. The recall-oriented baseline is more balanced: as expected
the recall is higher than any individual model and the precision does not drop too much
10. This combination produced the highest-scoring baseline model.

133

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
baseline
baseline
Brown
baseline
baseline

recall
precision

PProps
53.71%
35.43%

Prec.
78.09%
92.49%

Recall
78.77%
60.48%

F1
78.43 0.8
73.14 0.9

recall
precision

36.94%
20.52%

68.57%
88.74%

66.05%
46.35%

67.29
60.89

2.0
2.1

Table 5: Performance of the baseline models on the WSJ and Brown test sets.
because the inference strategy lters out many unlikely candidates. Overall, the recalloriented baseline performs best, with an F1 1.22 points higher than the best individual
model on the WSJ corpus, and 0.41 points lower on the Brown corpus.
8.3 Performance of the Combination System with Constraint Satisfaction
In the Constraint Satisfaction setting the arguments output by individual Models 1, 2, and
3 are recombined into an expected better solution that satises a set of constraints. We
have run the inference model based on Constraint Satisfaction described in Section 7.1 using
the Xpress-MP ILP solver11 . The main results are summarized in Table 6. The variants
presented in that table are the following: Pred-by-pred stands for local optimization,
which processes each verb predicate independently from others, while Full sentence stands
for global optimization, i.e., resolving all the verb predicates of the sentence at the same
time. The column labeled Constraints shows the particular constraints applied at each
conguration. The O column presents the value of the parameter for correcting the bias
towards candidate overgeneration. Concrete values are empirically set to maximize the F1
measure on the development set. O = 0 corresponds to a setting in which no bias correction
is applied.
Some clear conclusions can be drawn from Table 6. First, we observe that any optimization variant obtains F1 results above both the individual systems (Table 1) and the
baseline combination schemes (Table 5). The best combination model scores 2.61 F1 points
in WSJ and 1.49 in Brown higher than the best individual system. Taking into account
that no learning is performed, it is clear that Constraint Satisfaction is a simple yet formal
setting that achieves good results.
A somewhat surprising result is that all performance improvements come from constraints 1 and 2 (i.e., no overlapping nor embedding among arguments of the same verb,
and no repetition of core arguments in the same verb). Constraints 3 and 4 are harmful,
while the sentence-level constraints (5 and 6) have no impact on the overall performance12 .
Our analysis of the proposed constraints yielded the following explanations:
 Constraint number 3 prevents the assignment of an R-X argument when the referred
argument X is not present. This makes the inference miss some easy R-X arguments
11. Xpress-MP is a Dash Optimization product that is free for academic usage.
12. In Section 8.5 we will see that when the learning strategy incorporates global feedback, performing a
sentence-level inference is slightly better than proceeding predicate by predicate.

134

fiCombination Strategies for Semantic Role Labeling

WSJ
Pred-by-pred

Full sentence

Brown
Full sentence

Constraints
1
1+2
1+2+3
1+2+4
1+2+3+4
1+2+5
1+2+6
1+2+5+6
1+2+5+6

O
0.30
0.30
0.25
0.30
0.30
0.30
0.30
0.30
0

PProps
52.29%
52.52%
52.31%
51.40%
51.19%
52.53%
52.48%
52.50%
54.49%

Precision
84.20%
84.61%
84.34%
84.13%
83.86%
84.63%
84.64%
84.65%
78.74%

Recall
75.64%
75.53%
75.48%
75.04%
74.99%
73.53%
75.51%
75.51%
79.78%

F1
79.69 0.8
79.81 0.6
79.67 0.7
79.32 0.8
79.18 0.7
79.82 0.7
79.81 0.8
79.82 0.6
79.26 0.7

1+2+5+6
1+2+5+6

0.30
0

35.70%
38.06%

78.18%
69.80%

62.06%
67.85%

69.19 2.1
68.81 2.2

Table 6: Results, on WSJ and Brown test sets, obtained by multiple variants of the constraint satisfaction approach

when the X argument is not correctly identied (e.g., constituents that start with
{that, which, who} followed by a verb are always R-A0). Furthermore, this constraint
presents a lot of exceptions: up to 18.75% of the R-X arguments in the WSJ test set do
not have the referred argument X (e.g., when in the law tells them when to do so),
therefore the hard application of constraint 3 prevents the selection of some correct
R-X candidates. The ocial evaluation script from CoNLL-2005 (srl-eval) does not
require this constraint to be satised to consider a solution consistent.
 The srl-eval script requires that constraint number 4 (i.e., a C-X tag is not accepted
without a preceding X argument) be fullled for a candidate solution to be considered
consistent. But when it nds a solution violating the constraint its behavior is to
convert the rst C-X (without a preceding X) into X. It turns out that this simple
post-processing strategy is better than forcing a coherent solution in the inference step
because it allows to recover from the error when an argument has been completely
recognized but labeled only with C-X tags.
 Regarding sentence-level constraints, we observed that in our setting, inference using
local constraints (1+2) rarely produces a solution with inconsistencies at sentence
level.13 This makes constraint 5 useless since it is almost never violated. Constraint
number 6 (i.e., no sharing of AMs among dierent verbs) is more ad-hoc and represents
a less universal principle in SRL. The number of exceptions to that constraint, in
the WSJ test set, is 3.0% for the gold-standard data and 4.8% in the output of
the inference that uses only local constraints (1+2). Forcing the fulllment of this
constraint makes the inference process commit as many errors as corrections, making
its eect negligible.
13. This fact is partly explained by the small number of overlapping arguments in the candidate pool
produced by the three individual models.

135

fiSurdeanu, Marquez, Carreras, & Comas

95
90

95

Precision
Recall
F1

90

80

80
%

85

%

85

Precision
Recall
F1

75

75

70

70

65

65

60
-0.4

-0.2

0

0.2

0.4

0.6

0.8

60
-0.4

1

Value of O

-0.2

0

0.2

0.4

0.6

0.8

1

Value of O

Figure 9: Precision-Recall plots, with respect to the bias correcting parameter (O), for the
WSJ development and test sets (left and right plots, respectively).

Considering that some of the constraints are not universal, i.e., exceptions exist in the
gold standard, it seems reasonable to convert them into soft constraints. This can be done
by precomputing their compatibility from corpora counts using, for instance, point-wise
mutual information, and incorporating its eect in the compatibility function as explained in
section 7.1. This softening could, in principle, increase the overall recall of the combination.
Unfortunately, our initial experiments showed no dierences between the hard and soft
variants.
Finally, the dierences between the optimized values of the bias correcting parameter
and O = 0 are clearly explained by observing precision and recall values. The default
version tends to overgenerate argument assignments, which implies a higher recall at a cost
of a lower precision. On the contrary, the F1 optimized variant is more conservative and
needs more evidence to select a candidate. As a result, the precision is higher but the recall
is lower. A side eect of being restrictive with argument assignments, is that the number
of correctly annotated complete propositions is also lower in the optimized setting.
The preference for a high-precision vs. a high-recall system is mostly task-dependant.
It is interesting to note that in this constraint satisfaction setting, adjusting the precision
recall tradeo can be easily done by varying the value of the bias correcting score. In
Figure 9, we plot the precisionrecall curves with respect to dierent values of the O parameter (the optimization is done using constraints 1, 2, 5, and 6). As expected, high values
of O promote precision and demote recall, while lower values of O do just the contrary. Also,
we see that there is a wide range of values for which the combined F1 measure is almost
constant (the approximate intervals are marked using vertical lines), making it possible to
select dierent recall and precision values with a global performance (F1 ) near to the optimal. Parenthetically, note also that the optimal value estimated on the development set
(O = 0.3) generalizes very well to the WSJ test set.
136

fiCombination Strategies for Semantic Role Labeling

WSJ
Models
Models
Models
Models
Brown
Models
Models
Models
Models

1+2
1+3
2+3
1+2+3

PProps
49.28%
48.26%
49.36%
51.66%

Prec.
87.39%
86.80%
86.63%
87.47%

Recall
72.88%
73.20%
73.03%
74.67%

F1
79.48 0.6
79.42 0.6
79.25 0.7
80.56 0.6

F1 improvement
+2.27
+2.96
+2.04
+3.35

1+2
1+3
2+3
1+2+3

34.33%
31.22%
32.84%
34.33%

81.14%
80.43%
80.90%
81.75%

60.86%
59.07%
60.31%
61.32%

69.55 2.0
68.11 1.9
69.11 2.1
70.08 2.1

+1.85
+2.69
+1.41
+2.38

Table 7: Overall results of the learning-based inference with local rankers on the WSJ and
Brown test sets.

8.4 Performance of the Combination System with Local Rankers
We implemented the candidate-scoring classiers for this combination strategy using Support Vector Machines (SVM) with polynomial kernels of degree 2, which performed slightly
better than other types of SVMs or AdaBoost. We have implemented the SVM classiers
with the SVMlight software14 . Outside of changing the default kernel to polynomial we
have not modied the default parameters. For the experiments reported in this section,
we trained models for all 4 possible combinations of our 3 individual systems, using the
complete feature set introduced in Section 6. The dynamic programming engine used for
the actual inference processes each predicate independently (similar to the Pred-by-pred
approach in the previous sub-section).
Table 7 summarizes the performance of the combined systems on the WSJ and Brown
corpora. Table 7 indicates that our combination strategy is always successful: the results
of all combination systems improve upon their individual models (Table 1) and their F1
scores are always better than the baselines (Table 5). The last column in the table shows
the F1 improvement of the combination model w.r.t. the best individual model in each set.
As expected, the highest scoring combined system includes all three individual models. Its
F1 measure is 3.35 points higher than the best individual model (Model 2) in the WSJ test
set and 2.38 points higher in the Brown test set. Note that with any combination of two
individual systems we outperform the current state of the art (see Section 9 for details). This
is empirical proof that robust and successful combination strategies for the SRL problem
are possible. Table 7 also indicates that, even though the partial parsing model (Model 1) is
the worst performing individual model, its contribution to the ensemble is very important,
indicating that the information it provides is indeed complementary to the other models.
For instance, in WSJ the performance of the combination of the two best individual models
(Models 2+3) is worse than the combinations using model 1 (Models 1+2 and 1+3).
14. http://svmlight.joachims.org/

137

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
FS1
+ FS2
+ FS3
+ FS4
+ FS5
+ FS6
Brown
FS1
+ FS2
+ FS3
+ FS4
+ FS5
+ FS6

PProps
50.24%
50.39%
51.22%
50.66%
51.38%
51.66%

Prec.
86.47%
86.41%
86.13%
86.67%
87.21%
87.47%

Recall
73.51%
73.68%
74.35%
74.10%
74.61%
74.67%

F1
79.47 0.7
79.54 0.6
79.80 0.7
79.89 0.7
80.42 0.6
80.56 0.6

32.21%
32.84%
33.33%
33.33%
34.08%
34.33%

80.12%
80.80%
80.29%
81.10%
81.76%
81.75%

59.44%
59.94%
60.82%
60.50%
61.14%
61.32%

68.25 2.0
68.83 2.2
69.21 2.0
69.30 2.1
69.96 2.2
70.08 1.9

Table 8: Feature analysis for the learning-based inference with local rankers.
Due to its simple architecture i.e., no feedback from the conict resolution component
to candidate ltering this inference model is a good framework to study the contribution of
the features proposed in Section 6. For this study we group the features into 6 sets: FS1 the
voting features, FS2 the overlap features with arguments for the same predicate, FS3 the
overlap features with arguments for other predicates, FS4 the partial-syntax features, FS5
the full-syntax features, and FS6 the probabilities generated by the individual systems
for the candidate arguments. Using these sets we constructed 6 combination models by
increasing the number of features made available to the argument ltering classiers, e.g.,
the rst system uses only FS1, the second system adds FS2 to the rst systems features,
FS3 is added for the third system, etc. Table 8 lists the performance of these 6 systems
for the two test corpora. This empirical analysis indicates that the feature sets with the
highest contribution are:
 FS1, which boosts the F1 score of the combined system 2.26 points (WSJ) and 0.55
points (Brown) over our best individual system. This is yet another empirical proof
that voting is a successful combination strategy.
 FS5, with a contribution of 0.53 points (WSJ) and 0.66 points (Brown) to the F1
score. These numbers indicate that the ltering classier is capable of learning some
of the preferences of the individual models for certain syntactic structures.
 FS3, which contributes 0.26 points (WSJ) and 0.38 points (Brown) to the F1 score.
These results promote the idea that information about the overall sentence structure,
in our case inter-predicate relations, can be successfully used for the problem of SRL.
To our knowledge, this is novel.
All the proposed features have a positive contribution to the performance of the combined
system. Overall, we achieve an F1 score that is 1.12 points (WSJ) and 2.33 points (Brown)
higher than the best performing combined system at the CoNLL-2005 shared task evaluation
(see Section 9 for details).
138

fiCombination Strategies for Semantic Role Labeling

8.5 Performance of the Combination System with Global Rankers
In this section we report experiments with the global Perceptron algorithm described in
Section 7.2.2, that globally trains the scoring functions as rankers. Similar to the local
SVM models, we use polynomial kernels of degree 2. Furthermore, the predictions at test
time used averages of the parameter vectors, following the technique of Freund and Schapire
(1999).
We were interested in two main aspects. First, we evaluate the eect of training the
scoring functions with Perceptron using two dierent update rules, one global and the other
local. The global feedback rule, detailed in Section 7.2.2, corrects the mistakes found when
comparing the correct argument structure with the one that results from the inference (this
is noted as global feedback). In contrast, the local feedback rule corrects the mistakes
found before inference, when each candidate argument is handled independently, ignoring
the global argument structure generated (this is noted as local feedback). Second, we
analyze the eect of using dierent constraints in the inference module. To this extent,
we congured the inference module in two ways. The rst processes the predicates of a
sentence independently, and thus might select overlapping arguments of dierent predicates,
which is incorrect according to the domain constraints (this one is noted as Pred-by-pred
inference). The second processes all predicates jointly, and enforces a hierarchical structure
of arguments, where arguments never overlap, and arguments of a predicate are allowed to
embed arguments of other predicates (this is noted as Full sentence inference). From this
perspective, the model with local update and Pred-by-pred inference is almost identical
to the local combination strategy described in Section 8.4, with the unique dierence that
here we use Perceptron instead of SVM. This apparently minute dierence turns out to
be signicant for our empirical analysis because it allows us to measure the contribution
of both SVM margin maximization and global feedback to the classier-based combination
strategy (see Section 8.7).
We trained four dierent models: with local or global feedback, and with predicate-bypredicate or joint inference. Each model was trained for 5 epochs on the training data,
and evaluated on the development data after each training epoch. We selected the best
performing point on development, and evaluated the models on the test data. Table 9
reports the results on test data.
Looking at results, a rst impression is that the dierence in F1 measure is not very
signicant among dierent congurations. However, some observations can be pointed out.
Global methods achieve much better recall gures, whereas local methods prioritize the
precision of the system. Overall, global methods achieve a more balanced tradeo between
precision and recall, which contributes to a better F1 measure.
Looking at Pred-by-pred versus Full sentence inference, it can be seen that only
the global methods are sensitive to the dierence. Note that a local model is trained
independently of the inference module. Thus, adding more constraints to the inference
engine does not change the parameters of the local model. At testing time, the dierent
inference congurations do not aect the results. In contrast, the global models are trained
dependently of the inference module. When moving from Pred-by-pred to Full sentence
inference, consistency is enforced between argument structures of dierent predicates, and
this benets both the precision and recall of the method. The global learning algorithm
139

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
Pred-by-pred, local
Full sentence, local
Pred-by-pred, global
Full sentence, global
Brown
Pred-by-pred, local
Full sentence, local
Pred-by-pred, global
Full sentence, global

PProps
50.71%
50.67%
53.45%
53.81%

Prec.
86.80%
86.80%
84.66%
84.84%

Recall
74.31%
74.29%
76.19%
76.30%

F1
80.07 0.7
80.06 0.7
80.20 0.7
80.34 0.6

33.33%
33.33%
35.20%
35.95%

80.62%
80.67%
77.65%
77.91%

60.77%
60.77%
62.70%
63.02%

69.30 1.9
69.32 2.0
69.38 1.9
69.68 2.0

Table 9: Test results of the combination system with global rankers. Four congurations are
evaluated, that combine Pred-by-pred or Full sentence inference with local
or global feedback.

improves both in precision and recall when coupled with a joint inference process that
considers more constraints in the solution.
Nevertheless, the combination system with local SVM classiers, presented in the previous section, achieves marginally better F1 score than the global learning method (80.56% vs.
80.34% in WSJ). This is explained by the dierent machine learning algorithms (we discuss
this issue in detail in Section 8.7). The better F1 score is accomplished by a much better
precision in the local approach (87.47% vs. 84.84% in WSJ), whereas the recall is lower in
the local than in the global approach (74.67% vs. 76.30% in WSJ). On the other hand, the
global strategy produces more completely-correct annotations (see the PProps column)
than any of the local strategies investigated (see Tables 9 and 7). This is to be expected,
considering that the global strategy optimizes a sentence-level cost function. Somewhat
surprisingly, the number of perfect propositions generated by the global strategy is lower
than the number of perfect propositions produced by the constraint-satisfaction approach.
We discuss this result in Section 8.7.
8.6 Scalability of the Combination Strategies
All the combination experiments reported up to this point used the candidate arguments
generated by the three individual SRL models introduced in Section 4. While these experiments do provide an empirical comparison of the three inference models proposed, they do
not answer an obvious scalability question: how do the proposed combination approaches
scale when the number of candidate arguments increases but their quality diminishes? We
are mainly interested in answering this question for the last two combination models (which
use inference based on learning with local or global rankers) for two reasons: (a) they
performed better than the constraint satisfaction model in our previous experiments, and
(b) because they have no requirements on the individual SRL systems outputs unlike
the constraint satisfaction model which requires the argument probabilities of the individual models they can be coupled to pools of candidates generated by any individual SRL
model.
140

fiCombination Strategies for Semantic Role Labeling

koomen
pradhan+
haghighi
marquez
pradhan

surdeanu
tsai
che
moschitti
tjongkimsang
yi

ozgencil

WSJ
Prec.
Recall
82.28%
76.78%
82.95% 74.75%
79.54% 77.39%
79.55%
76.45%






50.14%

81.97%

73.27%

77.37

36.44%

73.73%

61.51%

67.07






45.28%
45.43%
47.81%
47.66%
45.85%

80.32%
82.77%
80.48%
76.55%
79.03%

72.95%
70.90%
72.79%
75.24%
72.03%

76.46
76.38
76.44
75.89
75.37

29.48%
30.47%
31.84%
30.85%
28.36%

72.41%
73.21%
71.13%
65.92%
70.45%

59.67%
59.49%
59.99%
61.83%
60.13%

65.42
65.64
65.09
63.81
64.88







F1
79.44
78.63
78.45
77.97

PProps
32.34%
38.93%
37.06%
36.44%

Brown
Prec.
Recall
73.38%
62.93%
74.49% 63.30%
70.24% 65.37%
70.79%
64.35%

PProps
53.79%
52.61%
56.52%
51.85%

F1
67.75
68.44
67.71
67.42

47.50%

77.51%

72.97%

75.17

31.09%

67.88%

59.03%

63.14

46.19%

74.66%

74.21%

74.44

31.47%

65.52%

62.93%

64.20

Table 10: Performance of the best systems at CoNLL-2005. The pradhan+ contains postevaluation improvements. The top 5 systems are actually combination models
themselves. The second column marks with  the systems not used in our evaluation: pradhan, which was replaced by its improved version pradhan+, and
yi, due to format errors in the submitted data.

For this scalability analysis, we use as individual SRL models the top 10 systems at the
CoNLL-2005 shared task evaluation. Table 10 summarizes the performance of these systems
on the same two test corpora used in our previous experiments. As Table 10 indicates, the
performance of these systems varies widely: there is a dierence of 5 F1 points in the WSJ
corpus and of over 4 F1 points in the Brown corpus between the best and the worst system
in this set.
For the combination experiments we generated 5 candidate pools using the top 2, 4, 6,

8, and 10 individual systems labeled with
in Table 10. We had to make two changes
to the experimental setup used in the rst part of this section: (a) we trained our combined models on the PropBank development section because we did not have access to the
individual systems outputs on the PropBank training partition; and (b) from the feature
set introduced in Section 6 we did not use the individual systems argument probabilities
because the raw activations of the individual models classiers were not available. Note
that under these settings the size of the training corpus is 10 times smaller than the size of
the training set used in the previous experiments.
Table 11 shows the upper limits of these setups using the combination and reranking oracle systems introduced in Section 8.2. Besides performance numbers, we also
list in Table 11 the average number of candidates per sentence for each setup, i.e., number
of unique candidate arguments (# Args./Sent.) for the combination oracle and number
of unique candidate frames (# Frames/Sent.) for the re-ranking oracle. Table 12 lists
the performance of our combined models with both local feedback (Section 7.2.1) and global
feedback (Section 7.2.2). The combination strategy with global rankers uses joint inference
and global feedback (see the description in the previous sub-section).
141

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
C2
C4
C6
C8
C10
Brown
C2
C4
C6
C8
C10

# Args./Sent.
8.53
9.78
10.23
10.74
11.33
7.42
8.99
9.62
10.24
10.86

Combination
Prec.
Recall
99.34%
82.71%
99.47%
87.26%
99.47%
88.02%
99.48%
88.63%
99.50% 89.02%
99.62%
99.65%
99.65%
99.66%
99.66%

71.34%
77.58%
79.38%
80.52%
81.72%

F1
90.27
92.96
93.39
93.75
93.97

Re-Ranking
# Frames/Sent.
Prec.
3.16
88.63%
4.44
91.08%
7.21
92.14%
8.11
92.88%
8.97
93.31%

83.14
87.24
88.37
89.08
89.80

3.02
4.55
7.09
8.19
9.21

82.45%
86.01%
88.19%
88.95%
89.65%

Recall
81.77%
86.12%
86.57%
87.33%
87.71%

F1
85.07
88.53
89.27
90.02
90.42

70.37%
75.98%
76.80%
78.04%
79.19%

75.94
80.68
82.10
83.14
84.10

Table 11: Performance upper limits determined by the oracle systems on the 10 best systems at CoNLL-2005. Ck stands for combination of the top k systems from
Table 10. # Args./Sent. indicates the average number of candidate arguments
per sentence for the combination oracle; # Frames/Sent. indicates the average
number of candidate frames per sentence for the re-ranking oracle. The latter
can be larger than the number of systems in the combination because on average
there are multiple predicates per sentence.

WSJ
C2
C4
C6
C8
C10
Brown
C2
C4
C6
C8
C10

PProps
50.69%
55.14%
54.85%
54.36%
53.90%

Local
Prec.
86.60%
86.67%
87.45%
87.49%
87.48%

ranker
Recall
73.90%
76.63%
76.34%
76.12%
75.81%

F1
79.750.7
81.380.7
81.520.6
81.410.6
81.230.6

PProps
52.74
54.95
55.21
55.00
54.76

Global ranker
Prec.
Recall
84.07% 75.38%
84.00% 77.19%
84.24% 77.41%
84.42% 77.10%
84.02% 77.44%

F1
79.490.7
80.450.7
80.680.7
80.590.7
80.600.7

32.71%
35.95%
35.32%
35.95%
36.32%

79.56%
80.27%
80.94%
81.98%
82.61%

60.45%
63.16%
62.24%
61.87%
61.97%

68.701.8
70.692.0
70.371.8
70.522.2
70.812.0

35.32
39.30
37.44
38.43
37.44

74.88%
75.63%
76.12%
76.40%
75.94%

68.092.0
69.592.2
69.882.0
69.702.2
69.862.0

62.43%
64.45%
64.58%
64.08%
64.68%

Table 12: Local versus global ranking for combinations of the 10 best systems at CoNLL2005. Ck stands for combination of the top k systems from Table 10.

We can draw several conclusions from these experiments. First, the performance upper limit of re-ranking is always lower than that that of the argument-based combination
strategy, even when the number of candidates is large. For example, when all 10 individual
models are used, the F1 upper limit of our approach in the Brown corpus is 89.80 whereas
the F1 upper limit for re-ranking is 84.10. However, the enhanced potential of our combination approach does not imply a signicant increase in computational cost: Table 11 shows
142

fiCombination Strategies for Semantic Role Labeling

that the number of candidate arguments that must be handled by combination approaches
is not that much higher than the number of candidate frames input to the re-ranking system, especially when the number of individual models is high. For example, when all 10
individual models are used, the combination approaches must process around 11 arguments
per sentence, whereas re-ranking approaches must handle approximately 9 frames per sentence. The intuition behind this relatively small dierence in computational cost is that,
even though the number of arguments is signicantly larger than the number of frames,
the dierence between the number of unique candidates for the two approaches is not high
because the probability of repeated arguments is higher than the probability of repeated
frames.
The second conclusion is that all our combination models boost the performance of the
corresponding individual systems. For example, the best 4-system combination achieves an
F1 score approximately 2 points higher than the best individual model in both the WSJ
and Brown corpus. As expected, the combination models reach a performance plateau
around 4-6 individual systems, when the quality of the individual models starts to drop
signicantly. Nevertheless, considering that the top 4 individual systems use combination
strategies themselves and the amount of training data for this experiment was quite small,
these results show the good potential of the combination models analyzed in this paper.
The third observation is that the relation previously observed between local and global
rankers holds: our combination model with local rankers has better precision, but the model
with global rankers always has better recall and generally better PProps score. Overall,
the model with local rankers obtains better F1 scores and scales better as the number
of individual systems increases. We discuss these dierences in more detail in the next
sub-section.
Finally, Table 11 indicates that the potential recall of this experiment (shown in the
left-most block in the table) is higher than the potential recall when combining our three
individual SRL systems (see Table 3): 3.8% higher in the WSJ test set, and 7.4% higher in
the Brown test set. This was expected, considering that both the number and the quality
of the candidate arguments in this last experiment is higher. However, even after this
improvement, the potential recall of our combination strategies is far from 100%. Thus,
combining the solutions of the N best state-of-the-art SRL systems still does not have
the potential to properly solve the SRL problem. Future work should focus on recallboosting strategies, e.g., using candidate arguments of the individual systems before the
individual complete solutions are generated, because in this step many candidate arguments
are eliminated.
8.7 Discussion
The experimental results presented in this section indicate that all proposed combination
strategies are successful: all three combination models provide statistically signicant improvements over the individual models and the baselines in all setups. An immediate (but
somewhat shallow) comparison of the three combination strategies investigated indicates
that: (a) the best combination strategy for the SRL problem is a max-margin local metalearner; (b) the global ranking approach for the meta-learner is important but it does not
143

fiSurdeanu, Marquez, Carreras, & Comas

have the same contribution as a max-margin strategy; and (c) the constraint-satisfaction
model performs the worst of all the strategies tried.
However, in most experiments the dierences between the combination approaches investigated are small. A more reasonable observation is that each combination strategy has
its own advantages and disadvantages and dierent approaches are suitable for dierent
applications and data. We discuss these dierences below.
If the argument probabilities of individual systems are available, the combination model
based on constraint satisfaction is an attractive choice: it is a simple, unsupervised strategy that obtains competitive performance. Furthermore, the constraint satisfaction model
provides an elegant and customizable framework to tune the balance between precision and
recall (see Section 8.3). With this framework we currently obtain the highest recall of all
combination models: 3.48% higher than the best recall obtained by the meta-learning approaches on the WSJ corpus, and 4.83% higher than the meta-learning models on the Brown
corpus. The higher recall implies also higher percentage of predicates that are completely
correctly annotated: the best PProps numbers in Table 6 are the best of all combination
strategies. The cause for this high dierence in recall in favor of the constraint satisfaction
approach is that the candidate scoring of the learning-based inference acts implicitly as a
lter: all candidates whose score i.e., the classier condence that the candidate is part of
the correct solution is negative are discarded, which negatively aects the overall recall.
Hence, constraint satisfaction is a better solution for SRL-based NLP applications which
require that predicate-argument frames be extracted with high recall. For example, in Information Extraction, predicate-argument tuples are ltered with subsequent high-precision,
domain-specic constraints (Surdeanu et al., 2003), hence it is paramount that the SRL
model have high recall.
Nevertheless, in many cases the argument probabilities of the individual SRL models are
not available, either because the models do not generate them, e.g., rule-based systems, or
because the individual models are available only as black boxes, which do not oer access to
internal information. Under these conditions, we showed that combination strategies based
on meta-learning are a viable alternative. In fact, these approaches obtain the highest
F1 scores (see Section 8.4) and obtain excellent performance even with small amounts of
training data (see Section 8.6). As previously mentioned, because candidate scoring acts
as lter, the learning-based inference tends to favor precision over recall: their precision
is 2.82% higher than the best precision of the constraint-satisfaction models in the WSJ
corpus, and 3.57% higher in the Brown corpus. This preference for precision over recall is
more pronounced in the learning-based inference with local rankers (Section 8.4) than in
the inference model with global rankers (Section 8.5). Our hypothesis for what causes the
global-ranking model to be less precision-biased is that in this conguration the ratio of
errors on positive versus negative samples is more balanced. Thinking in the strategy that
Perceptron follows, a local approach updates at every candidate with incorrect prediction
sign, whereas a global approach only updates at candidates that should or should not be in
the complete solution, after enforcing the domain constraints. In other words, the number
of negative updates which drives the precision bias is reduced in the global approach,
because some of the false positives generated by the ranking classiers are eliminated by
the domain constraints. Thus, because candidate scoring is trained to optimize accuracy,
144

fiCombination Strategies for Semantic Role Labeling

WSJ
global feedback
max margin
Brown
global feedback
max margin

PProps
+3.10%
+0.95%

Prec.
-1.96%
+0.67%

Recall
+1.99%
+0.36%

F1
+0.27
+0.49

+2.62%
+1.00%

-2.71%
+1.13%

+2.25%
+0.55%

+0.38
+0.78

Table 13: Contribution of global feedback and max margin to the learning-based inference.
The baseline is the Pred-by-pred, local model in Table 9.

fewer candidate arguments will be eliminated by the meta-learner with global rankers, which
translates into a better balance between precision and recall.
Another important conclusion of our analysis of global versus local ranking for the
learning-based inference is that a max-margin approach for the candidate scoring classiers
is more important than having global feedback for inference. In fact, considering that the
only dierence between the model with predicate-by-predicate inference with local feedback
in Section 8.5 (Pred-by-pred, local) versus the best model in Section 8.4 (+FS6) is that the
latter uses SVM classiers whereas the former uses Perceptron, we can compute the exact
contribution of both max margin and global feedback15 . For convenience, we summarize this
analysis in Table 13. That table indicates that max margin yields a consistent improvement
of both precision and recall, whereas the contribution of global feedback is more in reducing
the dierence between precision and recall by boosting recall and decreasing precision. The
benet of max-margin classiers is even more evident in Table 12, which shows that the
local-ranking model with max-margin classiers generalizes better than the global-ranking
model when the amount of training data is reduced signicantly.
Even though in this paper we have analyzed several combination approaches with three
independent implementations, the proposed models are in fact compatible with each other.
Various combinations of the proposed strategies are immediately possible. For example, the
constraint satisfaction model can be applied on the output probabilities of the candidate
scoring component introduced in Section 7.2. Such a model eliminates the dependency
on the output scores of the individual SRL models but retains all the advantages of the
constraint satisfaction model, e.g., the formal framework to tune the balance between precision and recall. Another possible combination of the approaches introduced in this paper
is to use max-margin classiers in the learning-based inference with global feedback, e.g.,
by using a global training method for margin maximization such as SVMstruct (Tsochantaridis et al., 2004). This model would indeed have an increased training time16 , but could
leverage the advantages of both max-margin classiers and inference with global feedback
(summarized in Table 13). Finally, another attractive approach is stacking, i.e., N levels of chained meta-learning. For example, we could cascade the learning-based inference
model with global rankers, which boosts recall, with the learning-based inference with local
rankers, which favors precision.
15. The contribution of global feedback is given by the model with joint inference and global feedback (Full
sentence, global) in Section 8.5.
16. This was the main reason why we chose Perceptron for the proposed online strategies.

145

fiSurdeanu, Marquez, Carreras, & Comas

9. Related Work
The 4 best performing systems at the CoNLL-2005 shared task included a combination of
dierent base subsystems to increase robustness and to gain coverage and independence
from parse errors. Therefore, they are closely related to the work of this paper. The rst
four rows in Table 10 summarize their results under exactly the same experimental setting
as the one used in this paper.
Koomen et al. (2005) used a 2 layer architecture close to ours. The pool of candidates is
generated by: (a) running a full syntax SRL system on alternative input information (Collins
parsing, and 5-best trees from Charniaks parser), and (b): taking all candidates that pass
a lter from the set of dierent parse trees. The combination of candidates is performed
in an elegant global inference procedure as constraint satisfaction, which, formulated as
Integer Linear Programming, is solved eciently. This is dierent from our work, where
we break complete solutions from any number of SRL systems and we also investigate
a meta-learning combination approach in addition to the ILP inference. Koomen et al.s
system was the best performing system at CoNLL-2005 (see Table 10).
Haghighi et al. (2005) implemented double re-ranking on top of several outputs from a
base SRL model. The re-ranking is performed, rst, on a set of n-best solutions obtained
by the base system run on a single parse tree, and, then, on the set of best-candidates
coming from the n-best parse trees. This was the second-best system at CoNLL-2005
(third row in Table 10). Compared to our decomposition and re-combination approach,
the re-ranking setting has the advantage of allowing the denition of global features that
apply to complete candidate solutions. According to a follow-up work by the same authors
(Toutanova, Haghighi, & Manning, 2005), these global features are the source of the major
performance improvements of the re-ranking system. In contrast, we focus more on features
that exploit the redundancy between the individual models, e.g., overlap between individual
candidate arguments, and we add global information at frame level only from the complete
solutions provided by individual models. The main drawback of re-ranking compared to our
approach is that the dierent individual solutions can not be combined because re-ranking
is forced to select a complete candidate solution. This implies that its overall performance
strongly depends on the ability of the base model to generate the complete correct solution
in the set of n-best candidates. This drawback is evident in the lower performance upper
limit of the re-ranking approach (see Tables 3 and 11) and in the performance of the actual
system our best combination strategy achieves an F1 score over 2 points higher than
Haghighi et al. in both WSJ and Brown17 .
Finally, Pradhan, Hacioglu, Ward, Martin, and Jurafsky (2005b) followed a stacking
approach by learning two individual systems based on full syntax, whose outputs are used to
generate features to feed the training stage of a nal chunk-by-chunk SRL system. Although
the ne granularity of the chunking-based system allows to recover from parsing errors, we
nd this combination scheme quite ad-hoc because it forces to break argument candidates
into chunks in the last stage.
17. Recently, Yih and Toutanova (2006) reported improved numbers for this system: 80.32 F1 for WSJ and
68.81 for Brown. However, these numbers are not directly comparable with the systems presented in
this paper because they fixed a significant bug in the representation of quotes in the input data, a bug
that is still present in our data.

146

fiCombination Strategies for Semantic Role Labeling

Outside of the CoNLL shared task evaluation, Roth and Yih (2005) reached the conclusion that the quality of the local argument classiers is more important than the global
feedback from the inference component. This is also one of the conclusions drawn by this paper. Our contribution is that we have shown that this hypothesis holds in a more complex
framework: combination of several state-of-the-art individual models, whereas Roth and
Yih experimented with a single individual model, only numbered arguments, and a slightly
simplied problem representation: B-I-O over basic chunks. Additionally, our more detailed
experiments allowed us to show clearly that the contribution of max margin is higher than
that of global learning in several corpora and for several combinations of individual systems.
Punyakanok, Roth, and Yih (2005) showed that the performance of individual SRL
models (particularly argument identication) is signicantly improved when full parsing is
used and argument boundaries are restricted to match syntactic constituents (similarly to
our Model 3). We believe that the approach used by our Models 1 and 2, where candidate
arguments do not have to match a single syntactic constituent, has increased robustness
because it has a built-in mechanism to handle some syntax errors, when an argument constituent is incorrectly fragmented into multiple phrases. Our empirical results support this
claim: Model 2 performs better than both Model 3 and the models proposed by Punyakanok
et al. A second advantage of the strategy proposed in this paper is that the same model
can be deployed using full syntax (Model 2) or partial syntax (Model 1).
Pradhan, Ward, Hacioglu, Martin, and Jurafsky (2005c) implement a SRL combination
strategy at constituent level that, similarly to our approach, combines dierent syntactic
views of the data based on full and partial syntactic analysis. However, unlike our approach,
Pradhan et al.s work uses only a simple greedy inference strategy based on the probabilities
of the candidate arguments, whereas in this paper we introduce and analyze three dierent
combination algorithms. Our analysis yielded a combination system that outperforms the
current state of the art.
Previous work in the more general eld of predicting structures in natural language
texts has indicated that the combination of several individual models improves overall performance in the given task. Collins (2000) rst proposed a learning layer based on ranking
to improve the performance of a generative syntactic parser. In that approach, a reranker
was trained to select the best solution from a pool of solutions produced by the generative
parser. In doing so, the reranker dealt with complete parse trees, and represented them with
rich features that exploited dependencies not considered in the generative method. On the
other hand, it was computationally feasible to train the reranker, because the base method
reduced the number of possible parse trees for a sentence from an exponential number (w.r.t.
sentence length) to a few tens. More recently, global discriminative learning methods for
predicting structures have been proposed (Laerty, McCallum, & Pereira, 2001; Collins,
2002, 2004; Taskar et al., 2003, 2004; Tsochantaridis et al., 2004). All of them train a
single discriminative ranking function to detect structures in a sentence. A major property
of these methods is that they model the problem discriminatively, so that arbitrary and
rich representations of structures can be used. Furthermore, the training process in these
methods is global, in that parameters are set to maximize measures not only related to
local accuracies (i.e., on recognizing parts of a structure), but also related to the global
accuracy (i.e., on recognizing complete structures). In this article, the use of global and
rich representations is also a major motivation.
147

fiSurdeanu, Marquez, Carreras, & Comas

10. Conclusions
This paper introduces and analyzes three combination strategies in the context of semantic
role labeling: the rst model implements an inference strategy with constraint satisfaction
using integer linear programming, the second uses inference based on learning where the
candidates are scored using discriminative classiers using only local information, and the
third and last inference model builds on the previous strategy by adding global feedback
from the conict resolution component to the ranking classiers. The meta-learners used
by the inference process are developed with a rich set of features that includes voting
statistics i.e., how many individual systems proposed a candidate argument overlap
with arguments from the same and other predicates in the sentence, structure and distance
information coded using partial and full syntax, and probabilities from the individual SRL
models (if available). To our knowledge, this is the rst work that: (a) introduces a thorough
inference model based on learning for semantic role labeling, and (b) performs a comparative
analysis of several inference strategies in the context of SRL.
The results presented suggest that the strategy of decomposing individual solutions and
performing a learning-based re-combination for constructing the nal solution has advantages over other approaches, e.g., re-ranking a set of complete candidate solutions. Of
course, this is a task-dependant conclusion. In the case of semantic role labeling, our approach is relatively simple since the re-combination of argument candidates has to fulll
only a few set of structural constraints to generate a consistent solution. If the target structure is more complex (e.g., a full parse tree) the re-combination step might be too complex
from both the learning and search perspectives.
Our evaluation indicates that all proposed combination approaches are successful: they
all provide signicant improvements over the best individual model and several baseline
combination algorithms in all setups. Out of the three combination strategies investigated,
the best F1 score is obtained by the learning-based inference using max-margin classiers.
While all the proposed approaches have their own advantages and drawbacks (see Section 8.7
for a detailed discussion of dierences among the proposed inference models) several important features of a state-of-the-art SRL combination strategy emerge from this analysis:
(i) individual models should be combined at the granularity of candidate arguments rather
than at the granularity of complete solutions or frames; (ii) the best combination strategy
uses an inference model based in learning; (iii) the learning-based inference benets from
max-margin classiers and global feedback, and (iv) the inference at sentence level (i.e.,
considering all predicates at the same time) proves only slightly useful when the learning is
performed also globally, using feedback from the complete solution after inference.
Last but not least, the results obtained with the best combination strategy developed
in this work outperform the current state of the art. These results are empirical proof that
a SRL system with good performance can be built by combining a small number (three in
our experiments) of relatively simple SRL models.

Acknowledgments
We would like to thank the JAIR reviewers for their valuable comments.
This research has been partially supported by the European Commission (CHIL project,
148

fiCombination Strategies for Semantic Role Labeling

IP-506909; PASCAL Network, IST-2002-506778) and the Spanish Ministry of Education
and Science (TRANGRAM, TIN2004-07925-C03-02). Mihai Surdeanu is a research fellow
within the Ramon y Cajal program of the Spanish Ministry of Education and Science. We
are also grateful to Dash Optimization for the free academic use of Xpress-MP.

References
Bishop, C. (1995). Neural Networks for Pattern Recognition. Oxford University Press.
Boas, H. C. (2002). Bilingual framenet dictionaries for machine translation. In Proceedings
of LREC 2002.
Carreras, X., & Marquez, L. (2004). Introduction to the CoNLL-2004 shared task: Semantic
role labeling. In Proceedings of CoNLL 2004.
Carreras, X., & Marquez, L. (2005). Introduction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of CoNLL-2005.
Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition of propositional
arguments with perceptrons. In Proceedings of CoNLL 2004 Shared Task.
Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of NAACL.
Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD
Dissertation, University of Pennsylvania.
Collins, M. (2000). Discriminative reranking for natural language parsing. In Proceedings
of the 17th International Conference on Machine Learning, ICML-00, Stanford, CA
USA.
Collins, M. (2002). Discriminative training methods for hidden markov models: Theory and
experiments with perceptron algorithms. In Proceedings of the SIGDAT Conference
on Empirical Methods in Natural Language Processing, EMNLP-02.
Collins, M. (2004). Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Bunt, H., Carroll, J., & Satta, G. (Eds.), New
Developments in Parsing Technology, chap. 2. Kluwer.
Collins, M., & Duy, N. (2002). New ranking algorithms for parsing and tagging: Kernels
over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics, ACL02.
Crammer, K., & Singer, Y. (2003a). A family of additive online algorithms for category
ranking. Journal of Machine Learning Research, 3, 10251058.
Crammer, K., & Singer, Y. (2003b). Ultraconservative online algorithms for multiclass
problems. Journal of Machine Learning Research, 3, 951991.
Freund, Y., & Schapire, R. E. (1999). Large margin classication using the perceptron
algorithm. Machine Learning, 37 (3), 277296.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational
Linguistics, 28 (3).
149

fiSurdeanu, Marquez, Carreras, & Comas

Gildea, D., & Palmer, M. (2002). The necessity of syntactic parsing for predicate argument
recognition. In Proceedings of the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02).
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & Jurafsky, D. (2004). Semantic
role labeling by tagging syntactic chunks. In Proceedings of the 8th Conference on
Computational Natural Language Learning (CoNLL-2004).
Haghighi, A., Toutanova, K., & Manning, C. (2005). A joint model for semantic role labeling.
In Proceedings of CoNLL-2005 Shared Task.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W. (2005). Generalized inference with
multiple semantic role labeling systems. In Proceedings of CoNLL-2005 Shared Task.
Laerty, J., McCallum, A., & Pereira, F. (2001). Conditonal random elds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th
International Conference on Machine Learning, ICML-01.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1994). Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics, 19 (2).
Marquez, L., Comas, P., Gimenez, J., & Catala, N. (2005). Semantic role labeling as
sequential tagging. In Proceedings of CoNLL-2005 Shared Task.
Melli, G., Wang, Y., Liu, Y., Kashani, M. M., Shi, Z., Gu, B., Sarkar, A., & Popowich,
F. (2005). Description of SQUASH, the SFU question answering summary handler
for the DUC-2005 summarization task. In Proceedings of Document Understanding
Workshop, HLT/EMNLP Annual Meeting.
Narayanan, S., & Harabagiu, S. (2004). Question answering based on semantic structures.
In International Conference on Computational Linguistics (COLING 2004).
Noreen, E. W. (1989). Computer-Intensive Methods for Testing Hypotheses. John Wiley &
Sons.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The Proposition Bank: An annotated
corpus of semantic roles. Computational Linguistics, 31 (1).
Ponzetto, S. P., & Strube, M. (2006a). Exploiting semantic role labeling, wordnet and
wikipedia for coreference resolution. In Proceedings of the Human Language Technolgy
Conference of the North American Chapter of the Association for Computational Linguistics.
Ponzetto, S. P., & Strube, M. (2006b). Semantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th Meeting of the European Chapter
of the Association for Computational Linguistics.
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin, J. H., & Jurafsky, D. (2005a).
Support vector learning for semantic argument classication. Machine Learning, 60,
1139.
Pradhan, S., Hacioglu, K., Ward, W., Martin, J. H., & Jurafsky, D. (2005b). Semantic role
chunking combining complementary syntactic views. In Proceedings of CoNLL-2005.
150

fiCombination Strategies for Semantic Role Labeling

Pradhan, S., Ward, W., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2005c). Semantic role
labeling using dierent syntactic views. In Proceedings of the 43rd Annual Conference
of the Association for Computational Linguistics.
Punyakanok, V., Roth, D., & Yih, W. (2005). The necessity of syntactic parsing for semantic role labeling. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI).
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the International Conference on
Computational Linguistics (COLING04).
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and
organization in the brain. Psychological Review, 65, 386407.
Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in
natural language tasks. In Proceedings of the Annual Conference on Computational
Natural Language Learning (CoNLL-2004), pp. 18, Boston, MA.
Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random
elds. In Proceedings of the International Conference on Machine Learning (ICML).
Schapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using condence-rated
predictions. Machine Learning, 37 (3).
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argument
structures for information extraction. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL 2003).
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-Margin Markov Networks. In Proceedings
of the 17th Annual Conference on Neural Information Processing Systems, NIPS-03,
Vancouver, Canada.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C. (2004). Max-margin parsing.
In Proceedings of the EMNLP-2004.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pp. 589596, Ann Arbor, MI, USA. Association for
Computational Linguistics.
Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector machine
learning for interdependent and structured output spaces. In Proceedings of the 21st
International Conference on Machine Learning, ICML-04.
Xue, N., & Palmer, M. (2004). Calibrating features for semantic role labeling. In Proceedings
of EMNLP-2004.
Yih, S. W., & Toutanova, K. (2006). Automatic semantic role labeling. In Tutorial of
the Human Language Technolgy Conference of the North American Chapter of the
Association for Computational Linguistics.
Younger, D. H. (1967). Recognition and parsing of context-free languages in n3 time.
Information and Control, 10 (2), 189208.

151

fiJournal of Artificial Intelligence Research 29 (2007) 353389

Submitted 9/06; published 8/07

Answer Sets for Logic Programs with Arbitrary Abstract
Constraint Atoms
Tran Cao Son
Enrico Pontelli
Phan Huy Tu

tson@cs.nmsu.edu
epontell@cs.nmsu.edu
tphan@cs.nmsu.edu

Computer Science Department
New Mexico State University
Las Cruces, NM 88003, USA

Abstract
In this paper, we present two alternative approaches to defining answer sets for logic
programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches
generalize the fixpoint-based and the level mapping based answer set semantics of normal
logic programs to the case of logic programs with arbitrary types of c-atoms. The results
are four different answer set definitions which are equivalent when applied to normal logic
programs.
The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which
differ from each other in the treatment of negation-as-failure (naf ) atoms, make use of an
immediate consequence operator to perform answer set checking, whose definition relies on
the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations.
The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of
programs with c-atoms. As for the case of fixpoint-based semantics, the difference between
these two definitions is rooted in the treatment of naf atoms.
We prove that answer sets by reduct (resp. by complement) are equivalent to weakly
(resp. strongly) well-supported models of a program, thus generalizing the theorem on
the correspondence between stable models and well-supported models of a normal logic
program to the class of programs with c-atoms.
We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set
semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with
aggregates presented in the literature.

1. Introduction and Motivation
Logic programming under the answer set semantics has been introduced as an attractive
and suitable knowledge representation language for AI research (Baral, 2005), as it offers
several desirable properties for this type of applications. Among other things, the language
is declarative and it has a simple syntax; it naturally supports non-monotonic reasoning,
and it is sufficiently expressive for representing several classes of problems (e.g., normal logic
programs capture the class of NP-complete problems); it has solid theoretical foundations
with a large body of building block results (e.g., equivalence between programs, systematic
c
2007
AI Access Foundation. All rights reserved.

fiSon, Pontelli, & Tu

program development, relationships to other non-monotonic formalisms), which is extremely
useful in the development and validation of large knowledge bases; it also has a large number
of efficient computational tools. For further discussion of these issues, the interested reader
is referred to the book of Baral (2003), the overview paper of Gelfond and Leone (2002),
the paper of Marek and Truszczynski (1999), and the paper of Niemela (1999).
A large number of extensions of logic programming, aimed at improving its usability in
the context of knowledge representation and reasoning, have been proposed. The Smodels
system introduces weight and cardinality constraint atoms to facilitate the encoding of
constraints on atom definitions (Simons, Niemela, & Soininen, 2002). These constructs
can be generalized to aggregates; aggregates have been extensively studied in the general
context of logic programming by the work of (see, e.g., Kemp & Stuckey, 1991; Mumick,
Pirahesh, & Ramakrishnan, 1990; Gelder, 1992), and further developed in recent years (see,
e.g., DellArmi, Faber, Ielpa, Leone, & Pfeifer, 2003; Denecker, Pelov, & Bruynooghe, 2001;
Elkabani, Pontelli, & Son, 2004; Faber, Leone, & Pfeifer, 2004; Gelfond, 2002; Pelov, 2004;
Son & Pontelli, 2007). Both dlv (Eiter, Leone, Mateis, Pfeifer, & Scarcello, 1998) and
Smodels have been extended to deal with various classes of aggregates (DellArmi et al.,
2003; Elkabani, Pontelli, & Son, 2005). The semantics of these extensions have been defined
either indirectly, by translating programs with these extensions to normal logic programs,
or directly, by providing new characterizations of the concept of answer sets for programs
with such extensions.
Each of the above mentioned extensions to logic programming has been introduced to
facilitate the representation of a desirable type of knowledge in logic programming. As
such, it is not a surprise that the focus has been on the definition of the semantics and
little has been done to investigate the basic building block results for the new classes of
logic programs. In this context, the study of a uniform framework covering various classes of
extensions will provide us with several benefits. For example, to prove (or disprove) whether
a basic building block result (e.g., splitting theorem) can be extended to the new classes of
logic programs, we will need to prove (or disprove) this result only once; new results in the
study of the generic framework are applicable to the study of one of the aforementioned
extensions; etc. Naturally, for these studies to be possible, a uniform framework whose
semantical definition exhibits the behavior of various extensions of logic programming, needs
to be developed. The main goal in this paper is to address this issue.
The concept of logic programs with abstract constraint atoms (or c-atoms) has been
introduced by Marek, Remmel, and Truszczynski as an elegant theoretical framework for
investigating, in a uniform fashion, various extensions of logic programming, including cardinality constraint atoms, weight constraint atoms, and more general forms of aggregates
(Marek & Remmel, 2004; Marek & Truszczynski, 2004). Intuitively, a c-atom A represents
a constraint on models of the program containing Aand the description of A includes an
explicit description of what conditions each interpretation has to meet in order to satisfy
A. This view is very general, and it can be shown to subsume the description of traditional classes of aggregates (e.g., Sum, Count, Min, etc.).1 Thus, programs with weight
constraint atoms or other aggregates can be represented as logic programs with c-atoms.
1. One could also argue that c-atoms and general aggregates capture analogous notions.

354

fiLogic Programs with Arbitrary Abstract Constraint Atoms

The first explicit definition of answer sets for positive programs with arbitrary c-atoms
(i.e., programs without the negation-as-failure operator)called programs with set constraints or SC-programshas been introduced in the work of Marek and Remmel (2004).
In this work answer sets for programs with c-atoms are defined by extending the notion of
answer sets for programs with weight constraint atoms proposed in the work of Niemela,
Simons, and Soininen (1999). Nevertheless, this approach provides, in certain cases, unintuitive answer sets (see, e.g., Examples 7 and 20). In particular, the approach of Marek
and Remmel does not naturally capture some of the well-agreed semantics for aggregates.
One of our main goals in this paper is to investigate alternative solutions to the problem of
characterizing answer sets for programs with arbitrary c-atoms. Our aim is to match the
semantics provided in the more recent literature for monotone c-atoms, and to avoid the
pitfalls of the approach developed in the work of Marek and Remmel (2004).
The concept of answer sets for programs with c-atoms has been later revisited by Marek
and Truszczynski (2004), focusing on answer sets for programs with monotone constraint
atoms, where a c-atom A is monotone if, for each pair of interpretations I and I 0 with
I  I 0 , we have that I satisfies A implies that I 0 satisfies A. This proposal has been
further extended to the case of disjunctive logic programs with monotone c-atoms (Pelov &
Truszczynski, 2004). In another paper (Liu & Truszczynski, 2005b), it is extended to deal
with convex c-atoms where a c-atom A is convex if, for every pair of interpretations I and J
with I  J, we have that I and J satisfy A implies that I 0 satisfies A for every I  I 0  J.
This paper also proves several properties of programs with monotone and convex c-atoms.
It is shown that many well-known properties of standard logic programming under answer
set semantics are preserved in the case of programs with monotone c-atoms.
The main advantage of focusing on monotone c-atoms lies in that monotonicity provides
a relatively simpler way for defining answer sets of logic programs with c-atoms. On the
other hand, this restriction does not allow several important classes of problems to be
directly expressed. For example2 , the aggregate atom Min({X | p(X)}) > 2 cannot be
viewed as a monotone aggregate atomsince monotonic extensions of the definition of p
might make the aggregate false; e.g., the aggregate is true if {p(3)} is the definition of p, but
it becomes false if we consider a definition containing {p(3), p(1)}. Similarly, the cardinality
constraint atom 1 {a, b} 1 is not a monotone constraint. Neither of these two examples can
be directly encoded using monotone c-atoms.
The studies in Marek and Remmel (2004), Marek and Truszczynski (2004) and in Liu
and Truszczynski (2005b) lead to the following question: what are the alternatives to the
approach to defining answer sets of programs with arbitrary c-atoms developed by Marek and
Remmel (2004)? Furthermore, will these alternativesif any capture the semantics of
programs with monotone c-atoms proposed by Marek and Truszczynski (2004) and avoid
the pitfalls of the notion of answer sets for arbitrary c-atoms in Marek and Remmel (2004)?
We present two equivalent approaches for defining answer sets for logic programs with
arbitrary c-atoms.
 The first approach is inspired by the notion of conditional satisfactionoriginally
developed in Son and Pontelli (2007)to characterize the semantics of logic programs
2. Although variables appear in the definition of aggregates, they are locally quantified. As such, an
aggregate literal is nothing but a shorthand of a collection of ground terms.

355

fiSon, Pontelli, & Tu

with aggregates. We generalize this notion to the case of programs with c-atoms. The
generalization turns out to be significantly more intuitive and easier to understand
than the original definition in Son and Pontelli (2007). Using this notion, we define
an immediate consequence operator TP for answer set checking.
 The second approach is inspired by the notion of well-supportedness, proposed by
Fages (1994) for normal logic programs.
The approaches are very intuitive, and, we believe, they improve over the only other semantics proposed for logic programs with arbitrary c-atoms in Marek and Remmel (2004).
We show that the newly defined semantics coincide with the previously introduced
semantics in Marek and Truszczynski (2004) in the case of programs with monotone c-atoms,
and they extend the original stable model semantics for normal logic programs. We discuss
different approaches for treating negation-as-failure c-atoms. We also relate our definitions
to several semantics for logic programs with aggregates, since the notion of c-atom can
be used to encode arbitrary aggregates. These results show that the proposed framework
naturally subsumes many existing treatments of aggregates in logic programming.
In summary, the main contributions of the paper are:
 A new notion of fixpoint answer set for programs with arbitrary c-atoms, which is
inspired by the fixpoint construction proposed in Son and Pontelli (2007) (but simpler)
and which differs significantly from the only proposal for programs with arbitrary catoms in Marek and Remmel (2004); this will lead to two different definitions of answer
sets (answer set by reduct and answer set by complement);
 A generalization of the notion of well-supported models in Fages (1994) to programs
with arbitrary c-atoms, whichto the best of our knowledgehas not been investigated by any other researchers, which leads to the notions of weakly and strongly
well-supported models;
 A result showing that the set of answer sets by reduct (resp. by complement) is
equivalent to the set of weakly (resp. strongly) well-supported models; and
 A number of results showing that the newly defined notions of answer sets capture
the answer set semantics of various extensions to logic programming, in those cases
all the previously proposed semantics agree.
The rest of this paper is organized as follows. Section 2 presents preliminary definitions,
including the syntax of the language of logic programming with c-atoms, the basic notion of
satisfaction, and the notion of answer set for programs with monotone c-atoms in Marek and
Truszczynski (2004) and for positive programs with arbitrary c-atoms in Marek and Remmel
(2004). Section 3 presents our first approach to defining answer sets for logic programs with
arbitrary c-atoms based on a fixpoint operator, while Section 4 introduces an alternative
definition based on well-supportedness. Section 5 extends the semantics to programs with
arbitrary c-atoms in the head of rules. Section 6 relates the semantics presented in this paper
with early work on abstract constraint atoms and aggregates. Section 7 provides conclusions
and future work. Proofs of theorems and propositions are deferred to the appendix.
356

fiLogic Programs with Arbitrary Abstract Constraint Atoms

2. PreliminariesLogic Programs with Abstract Constraint Atoms
We follow the syntax used in Liu and Truszczynski (2005b) to define programs with abstract
constraint atoms. Throughout the paper, we assume a fixed propositional language L with
a countable set A of propositional atoms.
2.1 Syntax
An abstract constraint atom (or c-atom) is an expression of the form (D, C), where D  A is
a set of atoms (the domain of the c-atom), and C is a collection of sets of atoms belonging to
D, i.e., C  2D (the solutions of the c-atom). Intuitively, a c-atom (D, C) is a constraint on
the set of atoms D, and C represents its admissible solutions. Given a c-atom A = (D, C),
we use Ad and Ac to denote D and C, respectively.
A c-atom of the form ({p}, {{p}}) is called an elementary c-atom and will be simply
written as p. A c-atom of the form (A, ), representing a constraint which does not admit
any solutions, will be denoted by . A c-atom A is said to be monotone if for every
X  Y  Ad , X  Ac implies that Y  Ac .
A rule is of the form
A  A1 , . . . , Ak , not Ak+1 , . . . , not An

(1)

where A, Aj s are c-atoms. The literals not Aj (k < j  n) are called negation-as-failure
c-atoms (or naf-atoms). For a rule r of the form (1), we define:
 head(r) = A,
 pos(r) = {A1 , . . . , Ak },
 neg(r) = {Ak+1 , . . . , An },
 body(r) = {A1 , . . . , Ak , not Ak+1 , . . . , not An }.
For a program P , hset(P ) denotes the set rP head(r)d .
We recognize special types of rules:
1. A rule r is positive if neg(r) = ;
2. A rule r is basic if head(r) is an elementary c-atom;
3. A rule r is a constraint rule if head(r) = .
A logic program with c-atoms (or logic program, for simplicity)3 is a set of rules. A program
P is called a basic program if each rule r  P is a basic or a constraint rule. P is said to be
positive if every rule in P is positive. P is monotone (resp. naf-monotone) if each c-atom
occurring in P (resp. in a naf-atom in P ) is monotone. Clearly, a monotone program is
also naf-monotone.
3. Whenever we want to refer to traditional logic programs (without c-atoms), we will explicitly talk about
normal logic programs.

357

fiSon, Pontelli, & Tu

2.2 Models and Satisfaction
In this subsection, we introduce the basic definitions for the study of logic programs with
constraints. We will begin with the definition of the satisfaction of c-atoms. We then
introduce the notion of a model of programs with c-atoms.
2.2.1 Satisfaction of C-Atoms
A set of atoms S  A satisfies a c-atom A, denoted by S |= A, if Ad  S  Ac . S satisfies
not A, denoted by S |= not A, if Ad  S 6 Ac .
It has been shown in Marek and Remmel (2004) and in Marek and Truszczynski (2004)
that the notion of c-atom is more general than extended atoms such as cardinality constraint atoms and aggregate atoms; thus, c-atoms can be used to conveniently represent
weight constraints, cardinality constraints (Simons et al., 2002), and various other classes
of aggregates, such as maximal cardinality constraints. For example,
 Let us consider an arbitrary choice atom of the form L{p1 , . . . , pk , notq1 , . . . , notqh }U ;
this can be represented by the c-atom (A, S) where:
 A = {p1 , . . . , pk , q1 , . . . , qh }
 S = { T  A | L  |(T  {p1 , . . . , pk })  ({q1 , . . . , qh } \ T )|  U }
 Let us consider an arbitrary aggregate of the form F {v | p(v)}  V where F is a set
function (e.g., Sum, Avg), V is a number, and  is a comparison operation (e.g.,
, >, 6=). This can be represented by the c-atom (A, S), where:
 A = {p(a) | p(a)  A}
 S = {T | T  A, F (T )  V }
Example 1 Let us consider the aggregate sum({X | p(X)})  1, defined in a language
where A = {p(1), p(2)}. From the considerations above, we have that this aggregate can
be represented by the c-atom ({p1 ), p(2)}, S) where
S = {T | T  {p(1), p(2)}, sum(T )  1} = {, {p(1)}, {p(2), p(1)}}
2
Example 2 Let us consider the cardinality constraint atom 1 {p(1), p(1)} 1. This can be
represented by the c-atom ({p(1), p(1)}, S) where
S = { T | T  {p(1), p(1)}, 1  |(T  {p(1), p(1)}|  1 } = {{p(1)}, {p(1)}}
2
C-atoms allow us to compactly represent properties that would require complex propositional combinations of traditional aggregates. E.g., a condition like either all elements
or no elements of the set {a, b, c, d} are true can be simply written as the single c-atom
({a, b, c, d}, {, {a, b, c, d}}). Further motivations behind the use of c-atoms can be found in
Marek and Remmel (2004) and Marek and Truszczynski (2004).
In the rest of the paper, we will often use in our examples the notation of cardinality
constraint atoms, weight constraint atoms, or general aggregate atoms instead of c-atoms,
whenever no confusion is possible.
358

fiLogic Programs with Arbitrary Abstract Constraint Atoms

2.2.2 Models
A set of atoms S satisfies the body of a rule r of the form (1), denoted by S |= body(r),
if S |= Ai for i = 1, . . . , k and S |= not Aj for j = k + 1, . . . , n. S satisfies a rule r if it
satisfies head(r) or if it does not satisfy body(r).
A set of atoms S is a model of a program P if S satisfies every rule of P . M is a minimal
model of P if it is a model of P and there is no proper subset of M which is also a model
of P . In particular, programs may have more than one minimal model (see, for example,
Example 5).
Given a program P , a set of atoms S is said to support an atom a  A if there exists
some rule r in P and X  head(r)c such that the following conditions are met:
 S |= body(r),
 X  S, and
 a  X.
Example 3 Let P1 4 be the program
p(a)
p(b)
p(c)
q



 q
 Count({X | p(X)}) > 2

The aggregate notation Count({X|p(X)}) > 2 represents the c-atom (D, {D}) where
D = {p(a), p(b), p(c)}. P1 has two models:
M1 = {p(a), p(b), p(c), q}

M2 = {p(a), p(b)}

M2 is a minimal model of P1 , while M1 is not.

2

Example 4 Let P2 be the program
p(1)

p(1)  p(2)
p(2)
 Sum({X | p(X)})  1
The aggregate notation Sum({X|p(X)})1 represents the c-atom (D, C) where
D = {p(1), p(2), p(1)}
and
C = {{p(1)}, {p(2)}, {p(1), p(2)}, {p(2), p(1)}, {p(1), p(2), p(1)}}
Because of the first rule, any model of P2 will need to contain {p(1)}. It is easy to see that
{p(1), p(1)} and {p(1), p(2), p(1)} are models of P2 but {p(1), p(2)} is not a model of P2 .
2
Example 5 Let P3 be the program
p  ({q}, {})
q  ({p}, {})
P3 has three models {p}, {q}, and {p, q}, of which {p} and {q} are minimal.
4. Remember that the notation p is a short form for the c-atom ({p}, {{p}}).

359

2

fiSon, Pontelli, & Tu

2.3 Previously Proposed Semantics
In this section, we will overview the semantical characterizations for programs with c-atoms
proposed in the existing literature. In particular, we will review the notion of answer sets
for monotone programs (i.e., program that contain only monotone c-atoms), as defined in
Marek and Truszczynski (2004). A formal comparison between these semantics and the
novel approach we propose in this paper is described in Section 6.
Given a set of atoms S, a rule r is applicable in S if S |= body(r). The set of applicable
rules in S is denoted by P (S). A set S 0 is nondeterministically one-step provable from S by
means of P if S 0  hset(P (S)) and S 0 |= head(r) for every r  P (S). The nondeterministic
A
one-step provability operator TPnd is a function from 2A to 22 such that for every S  A,
TPnd (S) consists of all sets S 0 that are nondeterministically one-step provable from S by
means of P .
A P -computation is a sequence t = (Xn )n=0,1,2,... where X0 =  and for every nonnegative integer n,
(i) Xn  Xn+1 , and
(ii) Xn+1  TPnd (Xn )
St = 
n=0 Xi is called the result of the computation t. A set of atoms S is a derivable model
of P if there exists a P -computation t such that S = St . The Gelfond-Lifschitz reduct for
normal logic programs is generalized to monotone programs as follows.
Definition 1 Let P be a monotone program. For a set of atoms M , the reduct of P with
respect to M , denoted by P M , is obtained from P by
1. removing from P every rule containing in the body a literal not A such that M |= A;
and
2. removing all literals of the form not A from the remaining rules.
Answer sets for monotone programs are defined next.
Definition 2 A set of atoms M is an answer set of a monotone program P if M is a
derivable model of the reduct P M .
The next example shows that, for programs with non-monotone c-atoms, Definition 2 is, in
general, not applicable.
Example 6 Consider the program P3 from Example 5. We can check that this program
does not allow the construction of any P3 -computation. In fact, TPnd
() = {{p, q}} and
3
nd
TP3 ({p, q}) = {}. Hence, {p} would not be an answer set of P3 (according to Definition 2)
since it is not a derivable model of the reduct of P3 with respect to {p} (which is P3 ).
On the other hand, it is easy to see that P3 is intuitively equivalent to the normal logic
program {p  not q, q  not p}. As such, P3 should accept {p} as one of its answer sets.
2
The main reason for the inapplicability of Definition 2 lies in that the nondeterministic onestep provability operator TPnd might become non-monotone in the presence of non-monotone
c-atoms.
360

fiLogic Programs with Arbitrary Abstract Constraint Atoms

2.4 Answer Sets for Positive Programs
Positive programs are characterized by the lack of negation-as-failure atoms. Positive programs with arbitrary c-atoms have been investigated in Marek and Remmel (2004), under
the name of SC-programs. Let us briefly review the notion of answer sets for SC-programs
which, in turn, is a generalization of the notion of answer sets for logic programs with weight
constraints, as presented in Niemela et al. (1999). A detailed comparison between the approach in Marek and Remmel (2004) and our work is given in Section 6.
b is the c-atom
For a c-atom A, the closure of A, denoted by A,
( Ad , {Y | Y  Ad , Z. (Z  Ac , Z  Y )} )
Intuitively, the closure is constructed by including all the supersets of the existing solutions.
b A rule of the form (1) is a Horn-rule if
A c-atom A is said to be closed if A = A.
(i) its head is an elementary c-atom; and (ii) each c-atom in the body is a closed c-atom.
A SC-program P is said to be a Horn SC-program if each rule in P is a Horn-rule. The
one-step provability operator, defined by TP (X) = {a | r  P, head(r) = a, X |= body(r)},
associated to a Horn SC-program P is monotone. Hence, every Horn SC-program P has
a least fixpoint M P which is the only minimal model of P (w.r.t. set inclusion). Given a
set of atoms M and a SC-program P , the NSS-reduct of P with respect to M , denoted by
N SS(P, M ), is obtained from P by
(i) removing all rules whose body is not satisfied by M ; and
(ii) replacing each rule
A  e1 , . . . , en , A1 , . . . , Am
where ei s are elementary c-atoms and Aj s are non-elementary c-atoms by the set of
rules
c1 , . . . , A
d
{a  e1 , . . . , en , A
m | a  Ad  M }
A model S of a program P is an answer set of P if it is the least fixpoint of the one-step
provability operator of the program N SS(P, S), i.e., S = M N SS(P,S) . It sometimes yields
answer sets that are not accepted by other extensions to logic programming. The next
example illustrates this point.
Example 7 Consider the program P4 :
c 
a  ({a, c}, {, {a, c}})
We have that M1 = {c} and M2 = {a, c} are models of P4 . Furthermore, N SS(P4 , M1 ) is
the program
c 
and N SS(P4 , M2 ) consists of the rules
c 
a  ({a, c}, {, {a}, {c}, {a, c})}
361

fiSon, Pontelli, & Tu

It is easy to see that M1 = M N SS(P,M1 ) and M2 = M N SS(P,M2 ) . Thus, observe that P4 has
a non-minimal answer set {a, c} according to Marek and Remmel (2004). Note that the
program P4 can be viewed as the following program with aggregates
c 
a  Count({a, c}) 6= 1
which does not have {a, c} as an answer set under most of the proposed semantics for
aggregates (Denecker et al., 2001; Faber et al., 2004; Ferraris, 2005; Pelov, 2004; Son &
Pontelli, 2007). Furthermore, all these approaches accept {c} as the only answer set of this
program.
2

3. Answer Sets for Basic Programs: A Fixpoint Based Approach
In this section, we define the notion of answer sets of basic programs. In this approach, we
follow the traditional way for defining answer sets of logic programs, i.e., by:
1. first characterizing the semantics of positive programs (Definition 4), and then
2. extending it to deal with naf-atoms (Definitions 7 and 8).
3.1 Answer Sets for Basic Positive Programs
Example 5 shows that a basic positive program might have more than one minimal model.
This leads us to define a TP -like operator for answer set checking, whose construction is
based on the following observation.
Observation 1 Let P be a propositional normal logic program (i.e., without
c-atoms)5 and let R, S be two sets of atoms. Given a set of atoms M , we define
the operator TP (R, S) and the monotone sequence of interpretations hIiM ii=0 as
follows.
)
(
r  P : head(r) = a,
TP (R, S) = a
pos(r)  R, neg(r)  S = 
I0M = 

M = T (I M , M )
Ii+1
P i

(i  0)

Let us denote with IM the limit of this sequence of interpretations. It is possible
to prove that M is an answer set of P w.r.t. Gelfond and Lifschitz (1988) iff
M = IM .
As we can see from the above observation, the (modified) consequence operator TP takes
two sets of atoms, R and S, as its arguments, and generates one set of atoms which could
be viewed as the consequences of P given that R is true and S is assumed to be an answer
set of P . It is easy to see that TP is monotone w.r.t. its first argument, i.e., if R  V , then
TP (R, S)  TP (V, S). Thus, the sequence hIjM ij=0 is monotone and converges to IM for a
given S. We will next show how TP can be generalized to programs with c-atoms.
5. For a rule r from a normal logic program P ,
a  a1 , . . . , an , not b1 , . . . , not bm
head(r), pos(r), and neg(r) denote a, {a1 , . . . , an }, and {b1 , . . . , bm }, respectively.

362

fiLogic Programs with Arbitrary Abstract Constraint Atoms

Observe that the definition of TP requires that pos(r)  R or, equivalently, R |= pos(r).
For normal logic programs, this is sufficient to guarantee the monotonicity of TP (, S). If
this definition is naively generalized to the case of programs with c-atoms, the monotonicity
of TP (., S) is guaranteed only under certain circumstances, e.g., when c-atoms in pos(r) are
monotone. To deal with arbitrary c-atoms, we need to introduce the notion of conditional
satisfaction of a c-atom.
Definition 3 (Conditional Satisfaction) Let M and S be sets of atoms. The set S
conditionally satisfies a c-atom A w.r.t. M , denoted by S |=M A, if
1. S |= A and,
2. for every I  Ad such that S  Ad  I and I  M  Ad , we have that I  Ac .
Observe that this notion of conditional satisfaction has been inspired by the conditional
satisfaction used to characterize aggregates in Son and Pontelli (2007), but it is significantly
simpler.
We say that S conditionally satisfies a set of c-atoms V w.r.t. M , denoted by S |=M V ,
if S |=M A for every A  V . Intuitively, S |=M V implies that S 0 |= V for every S 0 such that
S  S 0  M . Thus, conditional satisfaction ensures that if the body of a rule is satisfied in
S then it is also satisfied in S 0 , provided that S  S 0  M . This allows us to generalize the
operator TP defined in Observation 1 as follows. For a set of atoms S and a positive basic
program P , let
o

n

TP (S, M ) = a

r  P : S |=M pos(r), head(r) = ({a}, {{a}})

The following proposition holds.
Proposition 1 Let M be a model of P , and let S  U  M .
TP (U, M )  M.

Then TP (S, M ) 

Let TP0 (, M ) =  and, for i  0, let
TPi+1 (, M ) = TP (TPi (, M ), M )
Then, the following corollary is a natural consequence of Proposition 1.
Corollary 1 Let P be a positive, basic program and M be a model of P . Then, we have
TP0 (, M )  TP1 (, M )  . . .  M
The above corollary implies that the sequence hTPi (, M )i
i=0 is monotone and limited (w.r.t.
set inclusion) by M . Therefore, it converges to a fixpoint. We denote this fixpoint by
TP (, M ).
Definition 4 Let M be a model of a basic positive program P . M is an answer set of P
iff M = TP (, M ).
363

fiSon, Pontelli, & Tu

Observe that the constraint rules present in P (i.e., rules whose head is ) do not contribute
to the construction performed by TP ; nevertheless, the requirement that M should be a
model of P implies that all the constraint rules will have to be satisfied by each answer set.
We illustrate Definition 4 in the next examples.
Example 8 Consider the program P1 from Example 3.
 M1 = {p(a), p(b)} is an answer set of P1 since:
TP01 (, M1 ) = 
TP11 (, M1 ) = {p(a), p(b)} = M1
TP21 (, M1 ) = TP1 ({p(a), p(b)}, M1 ) = M1
 M2 = {p(a), p(b), p(c), q} is not an answer set of P1 , since:
TP01 (, M2 ) = 
TP11 (, M2 ) = {p(a), p(b)} = M1
TP21 (, M2 ) = TP1 ({p(a), p(b)}, M2 ) = M1
2
Example 9 Consider again the program P3 (Example 5). Let M1 = {p} and M2 = {q}.
We have that
TP03 (, M1 ) = 
TP03 (, M2 ) = 
TP13 (, M1 ) = {p} = M1
TP13 (, M2 ) = {q} = M2
Thus, both {p} and {q} are answer sets of P3 . On the other hand, for M = {p, q}, we have
that TP13 (, {p, q}) =  because  6|=M ({q}, {}) and  6|=M ({p}, {}). Hence, {p, q} is not
an answer set of P3 .
2
We conclude this section by observing that the answer sets obtained from the above construction are minimal models.
Corollary 2 Let P be a positive basic program and M be an answer set of P . Then, M is
a minimal model of P .
The next example shows that not every positive program has an answer set.
Example 10 Consider P2 (Example 4). Since answer sets of positive programs are minimal
models (Corollary 2) and M = {p(1), p(1)} is the only minimal model of P2 , we have that
M is the only possible answer set of P2 . Since
TP02 (, M ) = 
TP12 (, M ) = {p(1)}
TP22 (, M ) = TP2 ({p(1)}, M ) = {p(1)}
we can conclude that M is not an answer set of P2 . Thus, P2 does not have answer sets. 2
The example highlights that supportedness, in our approach, is not a sufficient condition for
being an answer setM 0 = {p(1), p(1), p(2)} is a supported model, but it is not accepted
as an answer set. The reason for rejecting M 0 is the fact that the element p(2) is essentially
self-supporting itself (cyclically) in M 0 . Note that M 0 is rejected, as an answer set, in most
approaches to aggregates in logic programminge.g., the approach in Faber et al. (2004)
rejects M 0 for not being a minimal model of the FLP-reduct of the program.
364

fiLogic Programs with Arbitrary Abstract Constraint Atoms

3.2 Answer Sets for Basic Programs
We will now define answer sets for basic programs, i.e., programs with elementary c-atoms
in the head of the rules, and rule bodies composed of c-atoms and naf-atoms.
In the literature, two main approaches have been considered to deal with negation of aggregates and of other complex atoms. Various extensions of logic programming (e.g., weight
constraints in Simons et al. (2002) and aggregates in Faber et al. (2004)) support negationas-failure atoms by replacing each naf-atom not A with a c-atom A0 , where A0 is obtained
from A by replacing the predicate relation of A with its negation. For example, following
this approach, the negated cardinality constraint atom not 1 {a, b} 1 can be replaced by
({a, b}, {, {a, b}}). Similarly, the negated aggregate atom not Sum({X | p(X)}) 6= 5 can
be replaced by Sum({X | p(X)}) = 5.
On the other hand, other researchers (see, e.g., Marek & Truszczynski, 2004; Ferraris,
2005) have suggested to handle naf-atoms by using a form of program reductin the same
spirit as in Gelfond and Lifschitz (1988).
Following these perspectives, we study two different approaches for dealing with nafatoms, described in the next two subsections. It is worth mentioning that both approaches
coincide in the case of monotone programs (Proposition 2).
3.2.1 Negation-as-Failure by Complement
In this approach, we treat a naf-atom not A by replacing it with its complement. We define
the notion of complement of a c-atom as follows.
Definition 5 The complement A of a c-atom A is the c-atom (Ad , 2Ad \ Ac ).
We next define the complement of a program P .
Definition 6 Given a basic program P , we define C(P ) (the complement of P ) to be the
program obtained from P by replacing each occurrence of not A in P with the complement
of A.
The program C(P ) is a basic positive program, whose answer sets have been defined in
Definition 4. This allows us to define the notion of answer sets of basic programs as follows.
Definition 7 A set of atoms M is an answer set by complement of a basic program P iff
it is an answer set of C(P ).
It is easy to see that each answer set of a program P is indeed a minimal model of P .
Example 11 Let us consider the program P5 , which consists of the following rules:
a 
c 

not ({a, b}, {{a, b}})

The complement of P5 is
a 
c  ({a, b}, {, {a}, {b}})
which has {a, c} as its only answer set. Thus, {a, c} is an answer set by complement of P5 .
2
365

fiSon, Pontelli, & Tu

Example 12 Let P6 be the program
c  not 1{a, b}1
a  c
b  a
We have that C(P6 ) is the program
c  ({a, b}, {, {a, b}})
a  c
b  a
This program does not have an answer set (w.r.t. Definition 4); thus P6 does not have an
answer set by complement.
2
3.2.2 Negation-as-Failure by Reduction
Another approach for dealing with naf-atoms is to adapt the Gelfond-Lifschitz reduction
of normal logic programs (Gelfond & Lifschitz, 1988) to programs with c-atomsthis approach has been considered in Marek and Truszczynski (2004) and Ferraris (2005). We can
generalize this approach to programs with arbitrary c-atoms as follows. For a basic program
P and a set of atoms M , the reduct of P w.r.t. M (P M ) is the set of rules obtained by
1. removing all rules containing not A s.t. M |= A; and
2. removing all not A from the remaining rules.
The program P M is a positive basic program. Thus, we can define answer sets for P as
follows:
Definition 8 A set of atoms M is an answer set by reduct of P iff M is an answer set of
P M (w.r.t. Definition 4).
Example 13 Let us reconsider the program P5 from Example 11 and let us consider M =
{a, c}. If we perform a reduct, we are left with the rules
a 
c 
whose minimal model is M itself. Thus, M is an answer set by reduct of the program P5 .
2
The next example shows that this approach can lead to different answer sets than the case
of negation by complement (for non-monotone programs).
Example 14 Consider the program P6 from Example 12. Let M = {a, b, c}. The reduct
of P6 w.r.t. M is the program
c 
a  c
b  a
which has M as its answer set, i.e., M is an answer set by reduct of P6 .
366

2

fiLogic Programs with Arbitrary Abstract Constraint Atoms

One consequence of the negation by reduct approach is the fact that it might lead to nonminimal answer setsin the presence of non-monotone atoms. For instance, if we replace
the atom Count({X | p(X)}) > 2 in P1 with not Count({X | p(X)})  2, the new
program is (by replacing the aggregate with a c-atom):
p(a) 
p(b) 
p(c)  q
q

 not



(

{p(a), p(b), p(c)},

, {p(a)}, {p(b)}, {p(c)},
{p(a), p(b)}, {p(b), p(c)}, {p(a), p(c)}

)!

This program admits the following two interpretations as answer sets by reduct: M1 =
{p(a), p(b), p(c), q} and M2 = {p(a), p(b)}. Since M2  M1 , we have that a non-minimal
answer set exists.
This result indicates that, for certain programs with c-atoms, there might be different
ways to treat naf-atoms, leading to different semantical characterizations. This problem
has been mentioned in Ferraris (2005). Investigating other methodologies for dealing with
naf-atoms is an interesting topic of research, that we plan to pursue in the future.
3.3 Properties of Answer Sets of Basic Programs
We will now show that the notion of answer sets for basic programs with c-atoms is a natural
generalization of the notions of answer sets for normal logic programs. We prove that
answer sets of basic positive programs are minimal and supported models and characterize
situations in which these properties hold for basic programs. We begin with a result stating
that, for the class of naf-monotone programs, the two approaches for dealing with naf-atoms
coincide.
Proposition 2 Let P be a basic program. Each answer set by complement of P is an
answer set by reduct of P . Furthermore, if P is naf-monotone, then each answer set by
reduct of P is also an answer set by complement of P .
The above proposition implies that, in general, the negation-as-failure by complement approach is more skeptical than the negation-as-failure by reduct approach, in that it may
accept fewer answer sets.6 Furthermore, Examples 12 and 14 show that a minimal (w.r.t.
set inclusion) answer set by reduct is not necessarily an answer set by complement of a
program.
Let P be a normal logic program (without c-atoms) and let c-atom(P ) be the program obtained by replacing each occurrence of an atom a in P with ({a}, {{a}}). Since
({a}, {{a}}) is a monotone c-atom, c-atom(P ) is a monotone program. Proposition 2 implies that, for c-atom(P ), answer sets by reduct and answer sets by complement coincide.
In the next proposition, we prove that the notion of answer sets for programs with c-atoms
preserves the notion of answer set for normal logic programs, in the following sense.
6. Note that we use the term skeptical to indicate acceptance of fewer models, which is somewhat different
than the use of the term in model theory.

367

fiSon, Pontelli, & Tu

Proposition 3 (Preserving Answer Sets) For a normal logic program P , M is an answer set (by complement or by reduct) of c-atom(P ) iff M is an answer set of P (w.r.t. the
definition in Gelfond and Lifschitz (1988)).
The above proposition, together with Proposition 2, implies that normal logic programs can
be represented by positive basic programs. This is stated in the following corollary.
Corollary 3 Every answer set of a normal logic program P is an answer set of
C(c-atom(P )) and vice versa.
In the next proposition, we study the minimality and supportedness properties of answer
sets of basic programs.
Proposition 4 (Minimality of Answer Sets) The following properties hold:
1. Every answer set by complement of a basic program P is a minimal model of P .
2. Every answer set by reduct of a basic, naf-monotone program P is a minimal model
of P .
3. Every answer set (by complement/reduct) of a basic program P supports each of its
members.

4. Answer Sets for Basic Programs: A Level Mapping Based Approach
The definition of answer sets provided in the previous section can be viewed as a generalization of the answer set semantics for normal logic programsin the sense that it relies
on a fixpoint operator, defined for positive programs. In this section, we discuss another
approach for defining answer sets for programs with c-atoms, which is based on the notion
of well-supported models.
The notion of well-supported models for normal logic programs was introduced in Fages
(1994), and it provides an interesting alternative characterization of answer sets. Intuitively,
a model M of a program P is a well-supported model iff there exists a level mapping, from
atoms in M to the set of positive integers, such that each atom a  M is supported by a
rule r, whose body is satisfied by M and the level of each positive atom in body(r) is strictly
smaller than the level of a.7 Fages proved that answer sets are well-supported models and
vice versa (Fages, 1994). The notion of well-supportedness has been extended to deal with
dynamic logic programs in Banti, Alferes, Brogi, and Hitzler (2005). Level mapping has
also been used as an effective tool to analyze different semantics of logic programs in a
uniform way (Hitzler & Wendt, 2005).
In what follows, we will show that the notion of well-supported models can be effectively
applied to programs with c-atoms. A key to the formulation of this notion is the answer to
the following question:
what is the level of a c-atom A given a set of atoms M and a level mapping L of M ?
On one hand, one might argue that the level mapping of A should be defined independently
from the mapping of the other atoms, being A an atom itself. On the other hand, it is
7. This implicitly means that pos(r)  M and neg(r)  M = , i.e., naf-atoms are dealt with by reduct.

368

fiLogic Programs with Arbitrary Abstract Constraint Atoms

reasonable to assume that the level of A depends on the levels of the atoms in Ad , since the
satisfaction of A (w.r.t. a given interpretation) depends on the satisfaction of the elements
in Ad . The fact that every existing semantics of programs with c-atoms evaluates the truth
value of a c-atom A based on the truth value assigned to elements of Ad stipulates us to
adopt the second view.
It is worth to mention that this view also allows us to avoid undesirable circular justifications of elements of a well-supported model: if we follow the first view, the program P7
consisting of the following rules
a  b
b  a
a  ({a, b}, {, {a, b}})
would have {a, b} as a well-supported model in which a, b, and ({a, b}, {, {a, b}}) are
supported by ({a, b}, {, {a, b}}), a, and {a, b} respectively. This means that a is true
because both a and b are true, i.e., there is a circular justification for a w.r.t. the model
{a, b}.
Let M be a set of atoms, ` be a mapping from M to positive integers, and let A be a
c-atom. We define H(X) = max({`(a) | a  X}), and
L(A, M ) = min({H(X) | X  Ac , X  M, X |=M A}).
Intuitively, the level of each atom is given by the smallest of the levels of the solutions of
the atom compatible with M where the level of a solution is given by the maximum level of
atoms in the solution. We assume that max() = 0, while min() is undefined. We will now
introduce two different notions of well-supported models. The first notion, called weakly
well-supported models, is a straightforward generalization of the definition given in Fages
(1994)in that it ignores the naf-atoms. The second notion,, called strongly well-supported
models, does take into consideration the naf-atoms in its definition.
Definition 9 (Weakly Well-Supported Model) Let P be a basic program. A model M
of P is said to be weakly well-supported iff there exists a level mapping ` such that, for
each b  M , P contains a rule r with head(r) = ({b}, {{b}}), M |= body(r), and for each
A  pos(r), L(A, M ) is defined and l(b) > L(A, M ).
We illustrate this definition in the next example.
Example 15 Let us consider again the program P5 and the set of atoms M = {a, b}. Let
A = ({a, b}, {, {a, b}}). Obviously, M is a model of P5 . Assume that M is a weakly wellsupported model of P5 . This means that there exists a mapping ` from M to the set of
positive integers satisfying the condition of Definition 9. Since b  M and there is only one
rule in P5 with b as head, we can conclude that `(b) > `(a). Observe that  6|=M A and
{a, b} |=M A. Thus, by the definition of L(A, M ), we have that
L(A, M ) = max({`(a), `(b)}) = `(b).
This implies that there exists no rule in P5 , which satisfies the condition of Definition 9 and
has a as its head. In other words, M is not a weakly well-supported model of P5 .
2
369

fiSon, Pontelli, & Tu

The next proposition generalizes Fages result to answer sets by reduct for programs with
c-atoms.
Proposition 5 A set of atoms M is an answer set by reduct of a basic program P iff it is
a weakly well-supported model of P .
As we have seen in the previous section, different ways to deal with naf-atoms lead to
different semantics for basic programs with c-atoms. To take into consideration the fact
that naf-atoms can be dealt with by complement, we develop an alternative generalization
of Fagess definition of a well-supported model to programs with abstract c-atoms as follows.
Definition 10 (Strongly Well-Supported Model) Let P be a basic program. A model
M of P is said to be strongly well-supported iff there exists a level mapping ` such that,
for each b  M , P contains a rule r with head(r) = ({b}, {{b}}), M |= body(r), for each
A  pos(r), L(A, M ) is defined and `(b) > L(A, M ), and for each A  neg(r), L(A, M ) is
defined and `(b) > L(A, M ),
Using Proposition 5 and Proposition 2, we can easily show that the following result holds.
Proposition 6 A set of atoms M is an answer set by complement of a basic program P
iff it is a strongly well-supported model of C(P ).
The above two propositions, together with Proposition 2, lead to the following corollary.
Corollary 4 For every naf-monotone basic program P , each weakly well-supported model
of P is also a strongly well-supported model of P and vice versa.
As we have discussed in the previous section, each normal logic program P can be easily
translated into a monotone basic program with c-atoms of the form ({a}, {{a}}), c-atom(P ).
Thus, Corollary 4 indicates that the notion of weakly/strongly well-supported model is
indeed a generalization of Fagess definition of well-supported model to programs with catoms.

5. Answer Sets for General Programs
General programs are programs with non-elementary c-atoms in the head. The usefulness
of rules with non-elementary c-atoms in the head, in the form of a weight constraint or an
aggregate, has been discussed in Ferraris (2005), Simons et al. (2002) and in Son, Pontelli,
and Elkabani (2006). For example, a simple atom8
Count({X | taken(X, ai)})  10
can be used to represent the constraint that no more than 10 students can take the AI class.
The next example shows how the 3-coloring problem of a graph G can be represented using
c-atoms.
8. Recall that aggregates are special form of c-atoms.

370

fiLogic Programs with Arbitrary Abstract Constraint Atoms

Example 16 Let the three colors be red (r), blue (b), and green (g). The program contains
the following rules:
 the set of atoms edge(u, v) for every edge (u, v) of G,
 for each vertex u of G, the following rule:
({color(u, b), color(u, r), color(u, g)}, {{color(u, b)}, {color(u, r)}, {color(u, g)}}) 
which states that u must be assigned one and only one of the colors red, blue, or
green.
 for each edge (u, v) of G, three rules representing the constraint that u and v must
have different color:
  color(u, r), color(v, r), edge(u, v)
  color(u, b), color(v, b), edge(u, v)
  color(u, g), color(v, g), edge(u, v)
2
We note that, with the exception of the proposals in Ferraris (2005), Son, Pontelli, and
Elkabani (2006), other approaches to defining answer sets of logic programs with aggregates
do not deal with programs with aggregates in the head. On the other hand, weight constraint
and choice atoms are allowed in the head (Simons et al., 2002). Similarly, c-atoms are
considered as head of rules in the framework of logic programs with c-atoms by Marek and
Remmel (2004) and by Marek and Truszczynski (2004).
In this section, we define answer sets for general programsi.e., programs where the
rule heads can be arbitrary c-atoms. Our approach is to convert a program P with c-atoms
in the head into a collection of basic programs, whose answer sets are defined as answer sets
of P . To simplify the presentation, we will talk about an answer set of a basic program to
refer to either an answer set by complement, an answer set by reduct, or a well-supported
model of the program. The distinction will be stated clearly whenever it is needed.
Let P be a program, r  P , and let M be a model of P . The instance of r w.r.t. M ,
denoted by inst(r, M ) is defined as follows
(

inst(r, M ) =

{b  body(r) | b  M  head(r)d }


M  head(r)d  head(r)c
otherwise

The instance of P w.r.t. M , denoted by inst(P, M ), is the program
inst(P, M ) =

[

inst(r, M )

rP

It is easy to see that the instance of P w.r.t. M is a basic program. This allows us to define
answer sets of general programs as follows.
Definition 11 Let P be a general program. M is an answer set of P iff M is a model of
P and M is an answer set of inst(P, M ).
371

fiSon, Pontelli, & Tu

This definition is illustrated in the next examples.
Example 17 Let P8 be the program consisting of a single fact:
({a, b}, {{a}, {b}}) 
Intuitively, P8 is the choice atom 1 {a, b} 1 in the notation of Smodels.
This program has two models, {a} and {b}. The instance inst(P8 , {a}) contains the
single fact
a
whose only answer set is {a}. Similarly, the instance inst(P8 , {b}) is the single fact
b
whose only answer set is {b}. Thus, P8 has the two answer sets {a} and {b}.

2

The next example shows that in the presence of non-elementary c-atoms in the head, answer
sets might not be minimal.
Example 18 Let P9 be the program consisting of the following rules:
({a, b}, {{a}, {b}, {a, b}}) 
c  b
Intuitively, the first rule of P9 is the cardinality constraint 1 {a, b} 2 in the notation of
Smodels. This program has four models: M1 = {a}, M2 = {b, c}, M3 = {a, b, c}, and
M4 = {a, c}. The instance inst(P9 , M1 ) contains the single fact
a
whose only answer set is M1 . Thus, M1 is an answer set of P9 .
If we consider M3 , the corresponding instance inst(P9 , M3 ) contains the rules
a 
b 
c  b
whose only answer set is M3 . This shows that M3 is another answer set of P9 .
Similarly, one can show that M2 is also an answer set of P9 .
The instance inst(P9 , M4 ) is the program
a 
c  b
which has {a} as its only answer set. Hence, M4 is not an answer set of P9 . Thus, P9 has
three answer sets, M1 , M2 , and M3 . In particular, observe that M1  M3 .
2
Observe that if P is a basic program then P is its unique instance. As such, the notion
of answer sets for general programs is a generalization of the notion of answer sets for
basic programs. It can be shown that Proposition 2 also holds for general programs. The
relationship between the notion of answer set for general programs and the definition given
in Marek and Remmel (2004) and other extensions to logic programming is discussed in the
next section.
372

fiLogic Programs with Arbitrary Abstract Constraint Atoms

6. Related Work and Discussion
In this section, we relate our work to some recently proposed extensions of logic programming, and discuss a possible method for computing answer sets of programs with c-atoms
using available answer set solvers.
6.1 Related Work
The concept of logic programs with c-atoms, as used in this paper, has been originally introduced in Marek and Remmel (2004) and in Marek and Truszczynski (2004)in particular,
programs with c-atoms have been named SC-programs in Marek and Remmel (2004).9 The
Example 7 shows that our semantical characterization differs from the approach adopted
in Marek and Remmel (2004). In particular, our approach guarantees that answer sets for
basic programs are minimal, while that is not the case for the approach described in Marek
and Remmel (2004). Consider another example:
Example 19 Consider the program P10
a 
c 
d  ({a, c, d}, {{a}, {a, c, d}})
According to our characterization, this program has only one answer set, M1 = {a, c}. If
we consider the approach described in Marek and Remmel (2004), then we can verify that
M2 = {a, c, d} is an answer set since the NSS-reduct of P10 with respect to M2 is
a 
c 
d  ({a, c, d}, {{a}, {a, c}, {a, d}, {a, c, d}})
and the least fixpoint of the one-step provability operator is {a, c, d}.

2

In this type of examples, it seems hard to justify the presence of d in the answer set of the
original program. We suspect that the replacement of a c-atom by its closure, used in the
NSS-reduct, might be the reason for the acceptance of unintuitive answer sets in Marek and
Remmel (2004). The following proposition states that our approach is more skeptical than
the approach of Marek and Remmel (2004).
Proposition 7 Let P be a positive program. If a set of atoms M is an answer set of P
w.r.t. Definition 11 then it is an answer set of P w.r.t. Marek and Remmel (2004).
The syntax of logic programs with c-atoms, as used in this paper, is also used in Liu
and Truszczynski (2005b) and in Liu and Truszczynski (2005a). One of the main differences
between our work and the work of Marek and Truszczynski (2004) is that we consider
arbitrary c-atoms, while the proposal of Marek and Truszczynski (2004) focuses on monotone
9. Although naf-atoms are not allowed in the definition of SC-programs, the authors suggest that naf-atoms
can be replaced by their complement.

373

fiSon, Pontelli, & Tu

(and convex) c-atoms. The framework introduced in this paper can be easily extended to
disjunctive logic programs considered in Pelov and Truszczynski (2004).
The immediate consequence operator TP proposed in this paper is different from the
nondeterministic one-step provability operator, TPnd , adopted in Marek and Truszczynski
(2004), in that TP is deterministic and it is applied only to basic positive programs. In Marek
and Truszczynski (2004) and in Liu and Truszczynski (2005b), the researchers investigate
how several properties of normal logic programs (e.g., strong equivalence) hold in the semantics of programs with monotone c-atoms of Marek and Truszczynski (2004). We have
not directly studied such properties in the context of our semantical characterization; nevertheless, as we will see later, Proposition 8 implies that the results proved in Liu and
Truszczynski (2005b) are immediately applicable to our semantic characterization for the
class of monotone programs. We do, however, focus on the use of well-supported models
and level mapping in studying answer sets for programs with c-atoms, an approach that
has not been used before for programs with c-atoms.
We will next present a result that shows that our approach to define answer sets for
monotone programs coincides with that of Marek and Truszczynski (2004).
Proposition 8 Let P be a monotone program. A set of atoms M is an answer set of P
w.r.t. Definition 11 iff M is a stable model of P w.r.t. Marek and Truszczynski (2004).
As discussed earlier, c-atoms can be used to represent several extensions of logic programs,
among them weight constraints and aggregates. Intuitively, an aggregate atom  (see, e.g.,
Elkabani et al., 2004; Faber et al., 2004) can be encoded as a c-atom (D, C), where D
consists of all atoms occurring in the set expression of  and C  2D is such that every
X  C satisfies  (see Examples 3-4). As indicated in Marek and Truszczynski (2004),
many of the previous proposals dealing with aggregates do not allow aggregates to occur in
the head of rules. Here, instead, we consider programs with c-atoms in the head.
With regards to naf-atoms, some proposals (see, e.g., Elkabani et al., 2004) do not allow
aggregates to occur in naf-atoms. The proposal in Faber et al. (2004) treats naf-atoms by
complement, although a reduction is used in defining the semantics, while Ferraris (2005)
argues that, under different logics, naf-atoms might require different treatments.
We will now present some propositions which relate our work to the recent works on
aggregates. We can prove10 :
Proposition 9 For a program with monotone aggregates P , M is an answer set of P iff it
is an answer set of P w.r.t. Faber et al. (2004) and Ferraris (2005).
The proposal presented in Pelov (2004) and in Denecker et al. (2001) deals with aggregates
by using approximation theory and three-valued logic, building the semantics on the threevalued immediate consequence operator aggr
, which maps three-valued interpretations into
P
three-valued interpretations of the program. This operator can be viewed as an operator
which maps pairs of set of atoms (R, S) where R  S into pairs of set of atoms (R0 , S 0 ) with
R0  S 0 . The authors show that the ultimate approximate aggregates provide the most
precise semantics for logic programs with aggregates. Let us denote with 1 (R, S) and
2 (R, S)) the two components of aggr
(R, S), i.e., aggr
(R, M ) = (1 (R, M ), 2 (R, M )).
P
P
aggr
The next proposition relates TP to P .
10. Abusing the notation, we use a single symbol to denote a program in different notations.

374

fiLogic Programs with Arbitrary Abstract Constraint Atoms

Proposition 10 Let P be a positive program with aggregates and R and M be two set of
atoms such that R  M . Then, TP (R, M ) = 1 (R, M ).
The above proposition, together with the fact that the evaluation of the truth value of
aggregate formulas in Denecker et al. (2001) treats naf-atoms by complement, allows us to
conclude that, for a program with aggregates P , answer sets by complement of P (w.r.t.
Definition 4) are ultimate stable models of P (Denecker et al., 2001) and vice versa. This
result, together with the results in Son and Pontelli (2007), allows us to conclude that TP
is a generalization of the immediate consequence operator for aggregates programs in Son
and Pontelli (2007).
Before we conclude the discussion on related work, we would like to point out that
Propositions 7-10 show that the different approaches to dealing with aggregates differ only
for non-monotone programs. The main difference between our approach and others lies in
the skepticism of the TP operator, caused by the notion of conditional satisfaction. We will
illustrate this issue in the next two examples.
Example 20 Consider the program P2 of Example 4. This program does not have an
answer set w.r.t. Definition 4 but has M = {p(1), p(1), p(2)} as an answer set according
to Marek and Remmel (2004). The reason for the unacceptability of M as an answer set
in our approach lies in that the truth value of the aggregate atom Sum({X | p(X)}) could
be either true or false even when p(1) is known to be true. This prevents the third rule to
be applicable and hence the second rule as well. This makes p(1) the fixpoint of the TP2
operator, given that M is considered as an answer set. In other words, we cannot regenerate
M given the programand the skepticism of TP2 is the main reason. We observe that other
approaches (see, e.g., Faber et al., 2004; Ferraris, 2005) do not accept M as an answer set
of P2 as well.
2
The following example shows the difference between our approach and those in Faber et al.
(2004) as well as in Ferraris (2005).
Example 21 Consider the program P
p(1)
 ({p(1), p(1)}, {, {p(1), p(1)}})
p(1)
 p(1)
p(1)  p(1)
Intuitively, the abstract atom A = ({p(1), p(1)}, {, {p(1), p(1)}}) represents the aggregate atom Sum({X | p(X)})  0. This program has two models M1 = {p(1), p(1)} and
M2 = . The approaches in Marek and Remmel (2004), Faber et al. (2004), and Ferraris
(2005) accept M1 as an answer set, while our approach and that of Pelov (2004), Denecker
et al. (2001) do not admit any answer sets. In our approach, TP (, M1 ) =  because 
does not conditionally satisfy A w.r.t M1 since it is not true in every possible extension
of  that leads to M1 , namely it is not true in {p(1)}. In other words, the skepticism
of our approach is again the main reason for the difference between our approach and the
approaches in Faber et al. (2004) and in Ferraris (2005).
2
375

fiSon, Pontelli, & Tu

6.2 Discussion
In this section, we briefly discuss a possible method for computing answer sets of programs
with c-atoms, using off-the-shelf answer set solvers. The method makes use of a transformation similar to the unfolding transformation proposed in Elkabani et al. (2004) for dealing
with aggregates, which has been further studied and implemented in Elkabani et al. (2005).
We begin our discussion with basic positive programs. Given a basic positive program
P and a c-atom A, if Ac 6= , an unfolding of A is an expression of the form
p1 , . . . , pn , not q1 , . . . , not qm
where {p1 , . . . , pn }  Ac and {q1 , . . . , qm } = Ad \ {p1 , . . . , pn }. If Ac = , then , denoting
false, is the unique unfolding of A. Observe that if A = ({a}, {{a}}) then the only unfolding
of A is a. An unfolding of a rule
A0  A1 , . . . , Ak
is a rule obtained by replacing each Ai with one of its unfoldings. unf olding(r) denotes
S
the set of all the unfoldings of a rule r. Let unf olding(P ) = rP unf olding(r). Clearly,
unf olding(P ) is a normal logic program if P is a basic positive program. We can show that
M is an answer set of P iff M is an answer set of unf olding(P ). This indicates that we can
compute answer sets of basic positive programs with c-atoms by (i) computing its unfolding;
and (ii) using available answer set solvers to compute the answer sets of the unfolded
program. Following this approach, the main additional cost for computing answer sets of a
basic positive program is the cost incurred during the unfolding process. Theoretically, this
can be very costly as for each rule r, we have that |unf olding(r)| = Abody(r) |Ac |, where
|.| denotes the cardinality of a set. This means that the size of the program unf olding(P )
might be exponential in the size of the original program P . Thus, the additional cost
might be significant. In practice, we can expect that this number is more manageable,
as a rule might contain only a few c-atoms whose set of solutions is reasonably small.
Furthermore, certain techniques can be employed to reduce the size of the unfolding program
(Son, Pontelli, & Elkabani, 2006).
The above method can be easily extended to deal with naf-atoms and general programs.
If answer sets by complement need to be computed, we need to (i) compute the complement
of the program; and (ii) use the above procedure to compute answer sets of the complement.
On the other hand, if answer sets by reduct need to be computed, we will have at hand a
tentative answer set M . The reduction of the program with respect to M can be computed,
and the unfolding can then be applied to verify whether M is an answer set of the reduct.
Observe that the complement or a reduct of a program can be easily computed, and it does
not increase the size of the program. As such, the main cost for computing answer sets
of general programs following this approach is still the cost of the unfolding. So far, in
our study on programs with aggregates (a special type of c-atoms), we did not encounter
unmanageable situations (Son, Pontelli, & Elkabani, 2006).
Observe that the specification of a c-atom requires the enumeration of its domain and
solutions, whose size can be exponential in the size of the set of atoms of the program.
This does not mean that an explicit representation of c-atoms needs to be used. In most
cases, c-atoms can be replaced by aggregate literals. Because of this, several complexity
376

fiLogic Programs with Arbitrary Abstract Constraint Atoms

results for programs with aggregates (see, e.g., Pelov, 2004; Son & Pontelli, 2007) can be
extended to logic programs with c-atoms. For example, we can easily show that the problem
of determining whether a logic program has an answer set or not is at least NPco-NP .
However, for programs with c-atoms representable by standard aggregate functions, except
those of the form Sum(.) 6= value and Avg(.) 6= value, the problem of determining whether
or not a program has an answer set remains NP-complete.

7. Conclusions and Future Work
In this paper, we explored a general logic programming framework based on the use of arbitrary constraint atoms. The proposed approach provides a characterization which is more
in line with existing semantics of logic programming with aggregates than the characterization proposed in Marek and Remmel (2004). We provided two alternative characterizations
of answer set semantics for programs with arbitrary constraint atoms, the first based on
a fixpoint operator, which generalizes the immediate consequence operator for traditional
logic programs, and the second built on a generalization of the notion of well-supported
models of Fages (1994).
Within each characterization of answer set, we investigated two methodologies for treating naf-atoms and we identified the class of naf-monotone programs, on which the two
approaches for dealing with naf-atoms coincide. We also proved that the newly proposed
semantics coincides with the semantics proposed in Marek and Truszczynski (2004) for
monotone programs. Finally, we related our work to other proposals on logic programs
with aggregates and discussed a possible method for computing answer sets of programs
with abstract constraint atoms using available answer set solvers.
The proposal has some unexplored aspects. The proposed approach is rather skeptical
in the identification of answer setswhile the approach in Marek and Remmel (2004) is
overly credulous. We believe that these two approaches represent the two extremes of
a continuum that needs to be explored. In particular, we believe it is possible to identify
intermediate approaches simply by modifying the notion of conditional satisfaction. Work
is in progress to explore these alternatives.
Acknowledgment
The authors wish to thank the anonymous reviewers for their insightful comments. The
research has been partially supported by NSF grants HRD-0420407, CNS-0454066, and
CNS-0220590. An extended abstract of this paper appeared in the Proceedings of the
Twenty-First National Conference on Artificial Intelligence, 2006.

Appendix A
First, we will show some lemmas that will be used for the proofs of propositions.
Lemma 1 Let S  U  M 0  M be sets of atoms and A be an abstract constraint atom.
Then, S |=M A implies U |=M 0 A.
377

fiSon, Pontelli, & Tu

Proof. S |=M A implies that
{I | I  Ad , S  Ad  I  M  Ad }  Ac .
Together with the fact S  U  M 0  M , we have that
{I | I  Ad , U  Ad  I  M 0  Ad }  Ac .
This implies that U |=M 0 A.

2

Lemma 2 For two sets of atoms M 0  M and a monotone c-atom A, if M 0 |= A (resp.
M 0 |= A) and M |= A (resp. M |= A) then M 0 |=M A (resp. M 0 |=M A). (Recall that A
denotes the complement of A.)
Proof.
1. Let us assume that M 0 |= A and M |= A. From the monotonicity of A, we can
conclude that, for every S, if M 0  S  M , we have that S |= A. As a result, we have
M 0 |=M A.
2. Let us assume that M 0 |= A and M |= A. Let us assume, by contradiction, that
M 0 6|=M A. Since we already know that M 0 |= A, this implies that there exists
S  Ad , M 0  Ad  S  M  Ad , such that S 6 2Ad \ Ac , i.e., S  Ac . Since A
is monotone and S  M , we have that M |= A. This is a contradiction, since we
initially assumed that M |= A.
2
Proposition 1. Let M be a model of P , and let S  U  M . Then
TP (S, M )  TP (U, M )  M.
Proof.
1. From Lemma 1, the assumption that S  U  M , and the definition of TP , we have
that TP (S, M )  TP (U, M ).
2. Let us now show that TP (U, M )  M . Consider an atom a  TP (U, M ). We need to
show that a  M . From the definition of the TP operator, there is a rule r such that
head(r) = ({a}, {{a}}) and U |=M pos(r). But observe that, for each A  pos(r), if
U |=M A then we will have that M |= A (Definition 3). Thus, we can conclude that
M |= pos(r). Since the program is positive and M is known to be a model of P , we
must have that M |= head(r), thus a  M .
2
Corollary 2 Let P be a positive basic program and M be an answer set of P . Then, M is
a minimal model of P .
Proof. M is a model of P since it is an answer set of P (Definition 4). Thus, we need to
prove that M is indeed a minimal model of P . Suppose that there exists M 0  M such that
378

fiLogic Programs with Arbitrary Abstract Constraint Atoms

M 0 is a model of P . Proposition 1 and Lemma 1 imply that T k (, M )  T k (, M 0 )  M 0 for
every k. Since M is an answer set, we have that M  M 0 . This contradicts the assumption
that M 0  M .
2
Proposition 2. Let P be a basic program. Each answer set by complement of P is an
answer set by reduct of P . Furthermore, if P is naf-monotone, then each an answer set by
reduct of P is also an answer set by complement of P .
Proof. Let us start by showing that answer sets by complement are also answer sets by
reduct. Let M be a model and let us denote with P1 = C(P ) and let P2 = P M . Using the
fact that if S |=M A then M 6|= A we can easily prove by induction that the following result
holds:
TP1 (, M )  TP2 (, M )
(2)
and if P is a naf-monotone program then
TPi 1 (, M ) = TPi 2 (, M )

(3)

If M is an answer set by complement then we have M = TP1 (, M ). Furthermore,
TP2 (, M )  M (Proposition 1). This implies that M is an answer set of P by reduct
as well.
If P is naf-monotone, using Equation (3) and the fact that M is an answer set of P2 we
can conclude that M is an answer set by complement of P .
2
Proposition 3. For a normal logic program P , M is an answer set (by complement or by
reduct) of c-atom(P ) iff M is an answer set of P (w.r.t. Definition in Gelfond and Lifschitz
(1988)).
Proof. For convenience, in this proof, we will refer to answer sets defined in Gelfond
and Lifschitz (1988) as GL-answer sets. Because of the monotonicity of c-atom(P ) and
Proposition 2, it suffices to show that M is an answer set of P iff M is an answer set by
reduct of c-atom(P ).
Let us consider the case where P is a positive program. It follows from Observation 1
and the fact that S |=M ({a}, {{a}}) iff a  S that the operator TP (., .) for P (defined in
Observation 1) coincides with the operator TP (., .) for c-atom(P ). Hence, M is an answer
set of c-atom(P ) iff M is a GL-answer set of P .
Now suppose that P is an arbitrary normal logic program. Let GL(P, M ) be the
Gelfond-Lifschitzs reduct of P w.r.t. M . Since M |= ({a}, {{a}}) iff a  M , we have
that c-atom(P )M = c-atom(GL(P, M )). Using the result for positive program, we have
that M is a GL-answer set of P iff M is an answer set by reduct of c-atom(P ).
2
Proposition 4.
1. Every answer set by complement of a basic program P is a minimal model of P .
2. Every an answer set by reduct of a basic, naf-monotone program P is a minimal model
of P .
3. Every answer set (by complement/reduct) of a basic program P supports each of its
members.
379

fiSon, Pontelli, & Tu

Proof.
1. Notice that for a set of atoms M and an abstract constraint atom A, M |= not A iff
M |= A. This implies that M is a model of P iff M is a model of C(P ). From this
and from Corollary 2, we can conclude that if M is an answer set by complement of
P , then it is a minimal model of P .
2. Let us assume that P is a naf-monotone basic program, and let M be an answer set
by reduct of P . From Proposition 2, M is also an answer set of P by complement.
The previous result implies that M is a minimal model of P .
3. It follows from Proposition 2 that it is enough to prove the conclusion for answer sets
by reduct of P . Let M be an answer set by reduct of P . From the definition, we
have that M = TPM (, M ). This implies that, if a  M , then there exists i such that
a  TPi M (, M ). In turn, this allows us to identify a rule
({a}, {{a}})  A1 , . . . , Ak , not Ak+1 , . . . , not An
such that M 6|= Aj for k + 1  j  n and TPi M (, M ) |=M Ai for 1  i  k. In
particular, M |= Ai for 1  i  k. We can easily conclude that the given rule
supports a.
2
Proof of Proposition 5
Let us start by proving the following lemma:
Lemma 3 Let P be a positive, basic program, and let M be a weakly well-supported model
of P . Let l be a mapping that satisfies the desired properties of weakly well-supportedness
l(a)+1
of M . For every atom a, a  M implies a  TP
(, M ).
Proof. First, observe that, for an atom a  M , we have
L(({a}, {{a}}), M ) = l(a).
Let us prove the lemma by induction on l(a).
1. Base Case: Consider a  M such that l(a) = 0. Clearly, we have that P must contain
the rule
({a}, {{a}}) 
hence, a  TP1 (, M ).
2. Inductive Step: Assume that the result holds for every atom b such that 0  l(b) < k.
Consider an atom a  M such that l(a) = k. We will show that a  TPk+1 (, M ).
Since M is a weakly well-supported model of P , there exists a rule
({a}, {{a}})  A1 , . . . , An
380

fiLogic Programs with Arbitrary Abstract Constraint Atoms

in P such that L(Ai , M ) is defined and l(a) > L(Ai , M ) for every 1  i  n.
Let S = TPk (, M ). For each i  {1, . . . , n}, since L(Ai , M ) is defined, there is X  M ,
X  (Ai )c such that X |=M Ai and L(Ai , M ) = H(X). Hence, we have k = l(a) >
L(Ai , M ) = H(X). From the inductive hypothesis, since X  M , we can conclude
that X  S. On the other hand, we already proved (Corollary 1) that
TP0 (, M )  . . .  TPk (, M ) = S  . . .  M.
As a result, we have that X  S  M .
From Lemma 1, since X |=M Ai , this implies that S |=M Ai . Accordingly, we have
a  TPk+1 (, M ).
2
We can now proceed with the proof of the proposition.
Proposition 5. A set M of atoms is an answer set by reduct of a basic program P iff it is
a weakly well-supported model of P .
Proof. We will prove the proposition in two steps. We first prove that the result holds for
positive programs and then extend it to the case of arbitrary basic programs.
 P is a positive program.
1. : Suppose M is an answer set of P . Corollary 2 implies that M is a model of
P . Thus, it suffices to find a level mapping satisfies l the condition of Definition
9. For each atom a, let
(

l(a) =

min{k | a  TPk (, M )} if a  M
0
otherwise

Clearly, l is well defined. We will show that l is indeed the mapping satisfying
the properties of Definition 9.
Let us consider an atom a  M and let k = l(a). Clearly, k > 0 since TP0 (, M ) =
. So, we have that a  TPk (, M ) but a 6 S = TPk1 (, M )  M . There are two
cases:
(a) P contains a rule
({a}, {{a}}) 
In this case, the condition on l for the atom a is trivially satisfied.
(b) P contains a rule r of the form
({a}, {{a}})  A1 , . . . , An
such that S |=M Ai for 1  i  n.
Consider an integer 1  i  n. Let X = S  (Ai )d . By the definition of
conditional satisfaction, we have that X  (Ai )c . It is easy to check that
X |=M Ai . In addition, we have X  M . As a result, L(Ai , M ) is defined.
Furthermore, we have L(Ai , M )  H(X)  H(S) < k = l(a). This shows
that the condition on l for a is also satisfied in this case.
381

fiSon, Pontelli, & Tu

The above two cases allow us to conclude that l satisfies the condition of Definition 9, i.e., M is a weakly well-supported model of P .
2. : Suppose M is a weakly well-supported model of P . Due to Lemma 3,
we have that M  TP (, M ). On the other hand, from Corollary 1, we have
TP (, M )  M . Consequently, we have M = TP (, M ), which implies that M
is an answer set of P .
 P is an arbitrary basic program. It is easy to see that a set of atoms M is a
weakly well-supported model of P iff M is a weakly well-supported model of P M .
From the previous result, this means that M is an answer set by reduct of P iff M is
a weakly well-supported model of P .
2
Proposition 7. Let P be a positive program. If a set of atoms M is an answer set of P
w.r.t. Definition 11 then it is an answer set of P w.r.t. Marek and Remmel (2004).
Proof.
 Consider the case that P is a basic program. Since N SS(P, M ) is a monotone positive programs, the least fixpoint of the one-step provability operator TN SS(P,M ) (.)
coincides with the least fixpoint of our extended immediate consequence operator
TN SS(P,M ) (., .) (see also Proposition 8). Furthermore, we can easily verify that
TP (, M ) = TNSS(P,M ) (, M ) holds if M is an answer set w.r.t. Definition 11. These
two observations imply the conclusion of the proposition.
 We now consider the case that P is general positive program. Without loss of generality, we can assume that P does not contain any constraints. Let Q = inst(P, M ).
We have that for a rule r0  Q if and only if there exists some rule r  P such
that M |= head(r), r0 = a  body(r), and a  head(r)d  M . This implies that
N SS(P, M ) = N SS(Q, M ). Since M is an answer set of Q (w.r.t. Definition 11),
we conclude that it is also an answer set of Q w.r.t. Marek and Remmel (2004) (the
basic case) which implies that M is also an answer set of P w.r.t. Marek and Remmel
(2004)
2
Proposition 8. Let P be a monotone program. A set of atoms M is an answer set of P
w.r.t. Definition 11 iff M is a stable model of P w.r.t. Marek and Truszczynski (2004).
Proof. Let us start by showing the validity of the result for positive programs. Let us
assume that P is a positive program. Without loss of generality, we assume that P does
not contain any constraints.
1. : Let M be an answer set of P . From Definition 11, we have that M is a model
of P and M is an answer set of Q = inst(P, M ).
For every non-negative integer i, let
Mi = TQi (, M )
382

fiLogic Programs with Arbitrary Abstract Constraint Atoms

Because M is an answer set of Q, by definition, we have
M = TQ (, M )
To show that M is a stable model of P w.r.t. Marek and Truszczynski (2004), all we
need to do is to prove that the sequence hMi i
i=0 is a P computation. We do so by
proving that (i) Mi  Mi+1 and (ii) for every r  P (Mi ) 11 , Mi+1 |= head(r), and
(iii) Mi+1  hset(P (Mi )).
(i) Follows from Corollary 1.
(ii) Consider a rule r  P (Mi ). By the definition of P (Mi ), we have that Mi |=
body(r). Because P is monotone and Mi  M , it follows that M |= body(r) and
Mi |=M body(r).
Let X = M  head(r)d . By the definition of inst(r, M ), we have that
inst(r, M ) = {b  body(r) | b  X}  Q
As Mi |=M body(r), for every r0  inst(r, M ), Mi |=M body(r0 ). By the definition
of Mi+1 , it follows that head(r0 )  Mi+1 . Hence, X  Mi+1 . Since X  head(r)d ,
this implies that
X  Mi+1  head(r)d
On the other hand, because Mi+1  M , we have
Mi+1  head(r)d  X
Accordingly, we have
M  head(r)d = X = Mi+1  head(r)d

(4)

On the other hand, because M is a model of P and M |= body(r), we have
M |= head(r). Therefore,
M  head(r)d  head(r)c

(5)

From (4) and (5), we have Mi+1  head(r)d  head(r)c , i.e., Mi+1 |= head(r).
(iii) Let a be an atom in Mi+1 . From the definition of Mi+1 it is easy to see that Q
must contain a rule r0 whose head is a and whose body is satisfied by Mi . This
implies that P (Mi ) must contain a rule r such that a  M  head(r)d . It follows
that a  head(r)d  hset(P (Mi )). Accordingly, we have Mi+1  hset(P (Mi )).
2. : Let M be a stable model of P according to Marek and Truszczynski (2004) and
let hXi i
i=0 be the canonical computation for M , i.e.,
X0 = 
11. Recall that P (Mi ) is the set of rules in P whose body is satisfied by Mi .

383

fiSon, Pontelli, & Tu

Xi+1 =

[

head(r)d  M

rP (Xi )

According to Theorem 5 of Marek and Truszczynski (2004), we have
M=

[

Xi

i

Let Q = inst(P, M ). Because M is a stable model of P , it is also a model of P . So,
to prove that M is an answer set of P , we only need to show that it is an answer set
of Q.
Let us construct a sequence of sets of atoms hMi i
i as follows
M0 = 
Mi+1 = TQ (, Mi )
Clearly, to prove M is an answer set of Q, it suffices to show that
Xi = Mi

(6)

Let us prove this by induction.
(a) i = 0: trivial because X0 = M0 = .
(b) Suppose (6) is true for i = k, we will show that it is true for i = k + 1.
Consider an atom a  Xk+1 . By the definition of Xk+1 , there exists a rule r  P
such that a  head(r)d and Xk |= body(r). Since Xk  M and P is monotone,
it follows that M |= body(r). Because a stable model of P is also a model of P ,
we have M |= head(r). As a result, Q contains the following set of rules:
inst(r, M ) = {b  body(r) | b  M  head(r)d }
Because a  head(r)d and a  Xk+1  M , we have a  M  head(r)d . As a
result, the following rule belongs to inst(r, M )
a  body(r)
Because Mk = Xk (inductive hypothesis), we have Mk |= body(r) and thus
Mk |=M body(r) (recall that Mk = Xk  M and body(r) consists of monotone
abstract constraint atoms only). By the definition of Mk+1 , we have a  Mk+1 .
We have shown that for every atom a  Xk+1 , a belongs to Mk+1 . Hence,
Xk+1  Mk+1

(7)

Now, we will show that Mk+1  Xk+1 . Consider an atom b in Mk+1 . By
definition of Mk+1 , there exists a rule r0  Q such that head(r)d = b and Mk |=M
body(r0 ). By the definition of Q this means that there exists a rule r in P such
that M |= head(r)d , body(r) = body(r0 ) and b  M head(r)d . Because Xk = Mk
384

fiLogic Programs with Arbitrary Abstract Constraint Atoms

(inductive hypothesis), from Mk |=M body(r0 ) = body(r), we have Xk |= body(r).
This implies that r  P (Xk ). Hence,
b  M  head(r)d  Xk+1
Therefore we have
Mk+1  Xk+1

(8)

From (7) and (8), we have Xk+1 = Mk+1 .
The above result can be easily extended for programs with negation-as-failure c-atoms. We
omit the proof here.
2
Proof of Proposition 9.
To prove this proposition, a brief review of the approach in Faber et al. (2004) is needed.
The notion of answer set proposed in Faber et al. (2004) is based on a new notion of reduct,
defined as follows. Given a program P and a set of atoms S, the reduct of P with respect to
S, denoted by S P , is obtained by removing from P those rules whose body is not satisfied
by S. In other words,
F LP (P, M ) = {r | r  ground(P ), S |= body(r)}.
The novelty of this reduct is that it does not remove aggregate atoms and negation-as-failure
literals satisfied by S. For a program P , S is a FLP-answer set of P iff it is a minimal
model of F LP (P, S). We will now continue with the proof of the proposition. It is easy to
see that it is enough to consider programs without negation-as-failure c-atoms.
Proposition 9. For a program with monotone aggregates P , M is an answer set of P iff
it is an answer set of P w.r.t. Faber et al. (2004) and Ferraris (2005).
Proof. Due to the equivalent result in Ferraris (2005), it suffices to prove the equivalence
between our approach and that of Faber et al. (2004). Notice that in this paper we are
dealing with ground programs and therefore
1. : Let M be a FLP-answer set of P . We will show that M is an answer set of P
(w.r.t. Definition 4).
Let Q = F LP (P, M ). From the definition of FLP-answer set, M is a minimal model
of Q. Let M 0 = TP (, M ). As M is a model of Q, it is also a model of P . Corollary
1 implies that M 0  M .
Consider r  Q such that M 0 |= body(r) and head(r) = ({a}, {{a}}). From the
definition of Q and the monotonicity of P , we have M |= body(r). It follows from
Lemma 2 that M 0 |=M body(r). Hence, a  M 0 (by the definition of the operator TP ).
This implies that M 0 is a model of Q.
Because of the minimality of M and M 0  M , we have M 0 = M . Hence, M is an
answer set of P .
385

fiSon, Pontelli, & Tu

2. : Let M be an answer set of P . We will prove that M is a FLP-answer set of P
by showing that M is a minimal model of Q = F LP (P, M ).
Let M 0  M be a model of Q. First, we will demonstrate that M 0 is a model of P .
Suppose otherwise, i.e., M 0 is not a model of P . This implies that P contains a rule
r such that head(r) = ({a}, {{a}}) for some atom a, M 0 |= A for A  pos(r), and
a 6 M 0 . Due to the monotonicity of P we have that M |= A for A  pos(r). Hence,
Q contains the rule r. As a result, we have M 0 |= body(r). Thus, a  M 0 because M 0
is a model of Q. This is a contradiction.
We have shown that M 0 is a model of P . On the other hand, by Corollary 2, M is a
minimal model of P . Therefore, we have M  M 0 . Accordingly, we have M 0 = M .
Thus, M is a minimal model of Q, i.e., an FLP-answer set of P .
2
Proof of Proposition 10. Let P be a positive program with aggregates and R and M
be two set of atoms such that R  M . Then, TP (R, M ) = 1 (R, M ) where aggr (R, M ) =
(1 (R, M ), 2 (R, M )).
Proof. In order to prove this result, we will make use of an intermediate step. In Son and
Pontelli (2007), the following concepts for program with aggregates are introduced:
 Given an aggregate A, a solution of A is a pair hS + , S  i, satisfying the following
properties:
 S +  A and S   A,
 S +  S  = , and
 for each I  A where S +  I and I  S  = , we have that I |= A.
 Given two interpretations I, M , an aggregate A is conditionally satisfied w.r.t. I, M
(denoted (I, M ) |= A) if hI  M  Ad , Ad \ M i is a solution of A. For simplicity, we
define also conditional satisfaction for atoms, by saying that a is conditionally satisfied
w.r.t. I, M if a  I.
 Given a positive program with aggregates P and an interpretation M , the aggreP : 2A  2A is defined as:12 K P (I) = {head(r)|r 
gate consequence operator KM
M
P, (I, M ) |= body(r)}.
We wish to show here that, for a positive program with aggregates P and for interpretations
P (I) = T (I, M ). This will allow us to conclude the result of proposition 10, since
I, M , KM
P
P (I) = 1 (I, M ) (Son & Pontelli, 2007).
it has been proved that KM
Observe that, under the condition I  M :
 If a is a standard atom, then I |=M a iff a  I iff (I, M ) |= a.
 Let A be an aggregate.
 Let us assume I |=M A. This means that I |= A and, for each J  Ad s.t.
I  Ad  J  M  Ad , we have that J |= A.
12. The original definition in Son and Pontelli (2007) allows for the use of negative atoms in the body of the
rules, but we omit this for the sake of simplicity.

386

fiLogic Programs with Arbitrary Abstract Constraint Atoms

If we consider J  Ad s.t. I  M  Ad  J and J  (Ad \ M ) = , then
I  M  Ad = I  Ad  J, and J  M  Ad (otherwise, if a  J and a 6 M  Ad ,
then a  Ad \ M , which would violate the condition J  (Ad \ M ) = ). From
the initial assumption that I |=M A, we can conclude that J  Ac . This allows
us to conclude that I |=M A implies (I, M ) |= A.
 Let us assume (I, M ) |= A. This means that, for each J  Ad s.t. I M Ad  J
and J  (Ad \ M ) = , we have that J  Ac .
First of all, note that I  Ad = I  M  Ad , thus I  Ad  Ac i.e., I |= A. Let us
now take some arbitrary J  Ad , where I  Ad  J  M  Ad . Since I  Ad  J,
in particular I  M  Ad  J. Furthermore, J  (Ad \ M ) = , since J  M  Ad .
Thus, from the initial assumption, we have J |= Ac . This allows us to conclude
that (I, M ) |= A implies I |=M A.
These results allows us to conclude that for any element  in the body of a rule of P (either
atom or aggregate), (I, M ) |=  iff I |=M . This allows us to immediately conclude that
P (I) = T (I, M ).
KM
2
P

References
Banti, F., Alferes, J. J., Brogi, A., & Hitzler, P. (2005). The well supported semantics
for multidimensional dynamic logic programs.. In Baral, C., Greco, G., Leone, N., &
Terracina, G. (Eds.), Logic Programming and Nonmonotonic Reasoning, 8th International Conference, LPNMR 2005, Diamante, Italy, September 5-8, 2005, Proceedings,
Vol. 3662 of Lecture Notes in Computer Science, pp. 356368. Springer.
Baral, C. (2003). Knowledge Representation, reasoning, and declarative problem solving
with Answer sets. Cambridge University Press, Cambridge, MA.
Baral, C. (2005). From Knowledge to Intelligence  Building Blocks and Applications..
Invited Talk, AAAI, www.public.asu.edu/~cbaral/aaai05-invited-talk.ppt.
DellArmi, T., Faber, W., Ielpa, G., Leone, N., & Pfeifer, G. (2003). Aggregate Functions in
Disjunctive Logic Programming: Semantics,Complexity,and Implementation in DLV.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI) 2003, pp. 847852.
Denecker, M., Pelov, N., & Bruynooghe, M. (2001). Ultimate well-founded and stable semantics for logic programs with aggregates.. In Codognet, P. (Ed.), Logic Programming,
17th International Conference, ICLP 2001, Paphos, Cyprus, November 26 - December
1, 2001, Proceedings, Vol. 2237 of Lecture Notes in Computer Science, pp. 212226.
Springer.
Eiter, T., Leone, N., Mateis, C., Pfeifer, G., & Scarcello, F. (1998). The KR System dlv:
Progress Report, Comparisons, and Benchmarks. In International Conference on
Principles of Knowledge Representation and Reasoning, pp. 406417.
Elkabani, I., Pontelli, E., & Son, T. C. (2004). Smodels with CLP and its applications: A
simple and effective approach to aggregates in asp.. In Demoen, B., & Lifschitz, V.
(Eds.), Logic Programming, 20th International Conference, ICLP 2004, Saint-Malo,
387

fiSon, Pontelli, & Tu

France, September 6-10, 2004, Proceedings, Vol. 3132 of Lecture Notes in Computer
Science, pp. 7389. Springer.
Elkabani, I., Pontelli, E., & Son, T. C. (2005). SmodelsA - A System for Computing Answer Sets of Logic Programs with Aggregates.. In Baral, C., Greco, G., Leone, N., &
Terracina, G. (Eds.), Logic Programming and Nonmonotonic Reasoning, 8th International Conference, LPNMR 2005, Diamante, Italy, September 5-8, 2005, Proceedings,
Vol. 3662 of Lecture Notes in Computer Science, pp. 427431. Springer.
Faber, W., Leone, N., & Pfeifer, G. (2004). Recursive aggregates in disjunctive logic programs: Semantics and complexity.. In Alferes, J. J. , & Leite, J. A. (Eds.), Logics
in Artificial Intelligence, 9th European Conference, JELIA 2004, Lisbon, Portugal,
September 27-30, 2004, Proceedings, Vol. 3229 of Lecture Notes in Computer Science,
pp. 200212. Springer.
Fages, F. (1994). Consistency of Clarks completion and existence of stable models. Methods
of Logic in Computer Science, pp. 5160.
Ferraris, P. (2005). Answer sets for propositional theories.. In Baral, C., Greco, G., Leone,
N., & Terracina, G. (Eds.), Logic Programming and Nonmonotonic Reasoning, 8th
International Conference, LPNMR 2005, Diamante, Italy, September 5-8, 2005, Proceedings, Vol. 3662 of Lecture Notes in Computer Science, pp. 119131. Springer.
Gelder, A. V. (1992). The well-founded semantics of aggregation.. In Proceedings of the
Eleventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database
Systems, June 2-4, 1992, San Diego, California, pp. 127138. ACM Press.
Gelfond, M., & Leone, N. (2002). Logic programming and knowledge representation  the
A-Prolog perspective. Artificial Intelligence, 138 (1-2), 338.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming.
In Kowalski, R., & Bowen, K. (Eds.), Logic Programming: Proceedings of the Fifth
International Conf. and Symp., pp. 10701080.
Gelfond, M. (2002). Representing Knowledge in A-Prolog. In Kakas, A., & Sadri, F. (Eds.),
Computational Logic: Logic Programming and Beyond, pp. 413451. Springer Verlag.
Hitzler, P., & Wendt, M. (2005). A uniform approach to logic programming semantics.
Theory and Practice of Logic Programming, 5 (1-2), 123159.
Kemp, D. B., & Stuckey, P. J. (1991). Semantics of logic programs with aggregates.. In
Saraswat, V. A , & Ueda, K. (Eds.), Logic Programming, Proceedings of the 1991
International Symposium, San Diego, California, USA, pp. 387-401. MIT Press.
Liu, L., & Truszczynski, M. (2005a). Pbmodels - software to compute stable models by
pseudoboolean solvers.. In Baral, C., Greco, G., Leone, N., & Terracina, G. (Eds.),
Logic Programming and Nonmonotonic Reasoning, 8th International Conference, LPNMR 2005, Diamante, Italy, September 5-8, 2005, Proceedings, Vol. 3662 of Lecture
Notes in Computer Science, pp. 410415.
Liu, L., & Truszczynski, M. (2005b). Properties of programs with monotone and convex
constraints.. In Veloso, M. M., & Kambhampati, S. (Eds.), Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative
388

fiLogic Programs with Arbitrary Abstract Constraint Atoms

Applications of Artificial Intelligence Conference, July 9-13, 2005, Pittsburgh, Pennsylvania, USA, pp. 701706. AAAI Press AAAI Press / The MIT Press.
Marek, V. W., & Remmel, J. B. (2004). Set constraints in logic programming. In Logic
Programming and Nonmonotonic Reasoning, 7th International Conference, LPNMR
2004, Fort Lauderdale, FL, USA, January 6-8, 2004, Proceedings, Vol. 2923 of Lecture
Notes in Computer Science, pp. 167179. Springer Verlag.
Marek, V. W., & Truszczynski, M. (1999). Stable Models as an Alternative Logic Programming Paradigm.. In The Logic Programming Paradigm, Springer Verlag.
Marek, V. W., & Truszczynski, M. (2004). Logic programs with abstract constraint atoms..
In Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence, July 25-29,
2004, San Jose, California, USA. AAAI Press / The MIT Press.
Mumick, I. S., Pirahesh, H., & Ramakrishnan, R. (1990). The magic of duplicates and
aggregates.. In McLeod, D., Sacks-Davis, R., & Schek, H.-J. (Eds.), 16th International
Conference on Very Large Data Bases, August 13-16, 1990, Brisbane, Queensland,
Australia, Proceeding, pp. 264277. Morgan Kaufmann.
Niemela, I., (1999). Logic Programs with Stable Models as a Constraint Programming
Paradigm.. In Annals of Math and AI, 25 (34), 241273.
Niemela, I., Simons, P., & Soininen, T. (1999). Stable model semantics for weight constraint
rules. In Proceedings of the 5th International Conference on on Logic Programming
and Nonmonotonic Reasoning, pp. 315332.
Pelov, N. (2004). Semantic of Logic Programs with Aggregates. Ph.D. thesis, Katholieke
Universiteit Leuven. http://www.cs.kuleuven.ac.be/publicaties/doctoraten/
cw/CW2004_02.abs.html.
Pelov, N., & Truszczynski, M. (2004). Semantics of disjunctive programs with monotone
aggregates  an operator-based approach.. In Delgrande, J. P., & Schaub, T. (Eds.),
10th International Workshop on Non-Monotonic Reasoning (NMR 2004), Whistler,
Canada, June 6-8, 2004, Proceedings, pp. 327334.
Simons, P., Niemela, N., & Soininen, T. (2002). Extending and Implementing the Stable
Model Semantics. Artificial Intelligence, 138 (12), 181234.
Son, T. C., & Pontelli, E. (2007). A Constructive Semantic Characterization of Aggregates
in Answer Set Programming. Theory and Practice of Logic Programming. 7 (03),
355375.
Son, T. C., Pontelli, E., & Elkabani, I. (2006). An Unfolding-Based Semantics for Logic
Programming with Aggregates. Computing Research Repository. cs.SE/0605038.

389

fiJournal of Artificial Intelligence Research 29 (2007) 309-352

Submitted 6/06; published 7/07

Learning Symbolic Models of Stochastic Domains
Hanna M. Pasula
Luke S. Zettlemoyer
Leslie Pack Kaelbling

pasula@csail.mit.edu
lsz@csail.mit.edu
lpk@csail.mit.edu

MIT CSAIL
Cambridge, MA 02139

Abstract
In this article, we work towards the goal of developing agents that can learn to act in
complex worlds. We develop a probabilistic, relational planning rule representation that
compactly models noisy, nondeterministic action effects, and show how such rules can be
effectively learned. Through experiments in simple planning domains and a 3D simulated
blocks world with realistic physics, we demonstrate that this learning algorithm allows
agents to effectively model world dynamics.

1. Introduction
One of the goals of artificial intelligence is to build systems that can act in complex environments as effectively as humans do: to perform everyday human tasks, like making breakfast
or unpacking and putting away the contents of an office. Many of these tasks involve manipulating objects. We pile things up, put objects in boxes and drawers, and arrange them
on shelves. Doing so requires an understanding of how the world works: depending on how
the objects in a pile are arranged and what they are made of, a pile sometimes slips or falls
over; pulling on a drawer usually opens it, but sometimes the drawer sticks; moving a box
does not typically break the items inside it.
Building agents to perform these common tasks is a challenging problem. In this work,
we approach the problem by developing a rule-based representation that such agents can use
to model, and learn, the effects of acting on their environment. Learning allows the agents
to adapt to new environments without requiring humans to hand-craft models, something
humans are notoriously bad at, especially when numeric parametrization is required. The
representation we use is both probabilistic and relational, and includes additional logical
concepts. We present a supervised learning algorithm that uses this representation language
to build a model of action effects given a set of example action executions. By optimizing the
tradeoff between maximizing the likelihood of these examples and minimizing the complexity
of the current hypothesis, this algorithm effectively selects a relational model structure, a
set of model parameters, and a language of new relational concepts that together provide a
compact, yet highly accurate description of action effects.
Any agent that hopes to act in the real world must be an integrated system that perceives
the environment, understands, and commands motors to effect changes to it. Unfortunately,
the current state of the art in reasoning, planning, learning, perception, locomotion, and
manipulation is so far removed from human-level abilities, that we cannot yet contemplate
c
2007
AI Access Foundation. All rights reserved.

fiPasula, Zettlemoyer, & Pack Kaelbling

Figure 1: A three-dimensional blocks world simulation. The world consists of a table, several cubes
of roughly uniform density but of varying size, and a robotic gripper that is moved by
simulated motors.

working in an actual domain of interest. Instead, we choose to work in domains that are its
almost ridiculously simplified proxies.1
One popular such proxy, used since the beginning of work in AI planning (Fikes &
Nilsson, 1971) is a world of stacking blocks. It is typically formalized in some version of
logic, using predicates such as on(a, b) and clear (a) to describe the relationships of the blocks
to one another. Blocks are always very neatly stacked; they dont fall into jumbles. In this
article, we present our work in the context of a slightly less ridiculous version of the blocks
world, one constructed using a three-dimensional rigid-body dynamics simulator (ODE,
2004). An example world configuration is shown in Figure 1. In this simulated blocks
world, blocks vary in size and colour; piles are not always tidy, and may sometimes fall
over; and the gripper works only on medium-sized blocks, and is unreliable even there.
Any approach capable of enabling effective behavior in this domain must handle its noisy,
nondeterministic nature, and nontrivial dynamics, and so should then be able to handle
other domains with similar characteristics.
One strategy for formulating such an approach is to learn models of the worlds dynamics
and then use them for planning different courses of action based on goals that may change
over time. Another strategy is to assume a fixed goal or reward function, and to learn a
policy that optimizes that reward function. In worlds of the complexity we are imagining,
it would be impossible to establish, in advance, an appropriate reaction to every possible
situation; in addition, we expect an agent to have an overall control architecture that is
hierarchical, and for which an individual level in the hierarchy will have changing goals.
For these reasons, we learn a model of the world dynamics, and then use it to make plans
to achieve the goals at hand.
We begin this paper by describing the assumptions that underlie our modeling decisions.
We then describe the syntax and semantics of our modeling language and give an algorithm
1. There is a very reasonable alternative approach, advocated by Brooks (1991), of working in the real world,
with all its natural complexity, but solving problems that are almost ridiculously simplified proxies for
the problems of interest.

310

fiLearning Symbolic Models of Stochastic World Dynamics

for learning models in that language. To validate our models, we introduce a simple planning
algorithm and then provide empirical results demonstrating the utility of the learned models
by showing that we can plan with them. Finally, we survey relevant previous work, and
draw our conclusions.

2. Structured Stochastic Worlds
An agent introduced into a novel world must find the best possible explanation for the
worlds dynamics within the space of possible models it can represent, which is defined by
the agents representation language. The ideal language would be able to compactly model
every action effect the agent might encounter, and no others. Any extra modeling capacity
is wasted and will complicate learning, since the agent will have to consider a larger space of
possible models, and be more likely to overfit its experience. Choosing a good representation
language provides a strong bias for any algorithm that will learn models in that language.
Most languages that have been used to describe deterministic planning models are, at
least on the surface, first order; that is, they abstract over the particular identities of objects, describing the effects of actions in terms of the properties of and relations among
these objects. (They accomplish this by letting an action take arguments, and representing
these arguments as variables.) This representational capacity is crucial for reasons of compactness and generalization: it is usually grossly inefficient to have to describe the behavior
of individual objects.
Much of the original work on probabilistic planning uses the formalism of Markov decision processes, which represents the states of the world individually and atomically (Puterman, 1999). More recently, propositional (factored) representations of dynamics have been
employed (Boyen & Koller, 1998; Guestrin, Koller, Parr, & Venkataraman, 2003), and some
first-order representations have been developed, including probabilistic rules (Blum & Langford, 1999), equivalence classes (Draper, Hanks, & Weld, 1994), and the situation calculus
approach of Boutilier, Reiter, and Price (2001). These representations also make it easy to
articulate and take direct advantage of two useful assumptions about world dynamics: the
frame assumption, which states that, when an agent takes an action in the world, anything
not explicitly changed stays the same, and the outcome assumption, which states that each
action affects the world in a small number of distinct ways, where each possible effect causes
a set of changes to the world that happen together as a single outcome.
We take as our point of departure these probabilistic first-order representations of world
dynamics. These representations have traditionally been applied to domains such as logistics
planning or the traditional, abstract blocks world, which are idealized symbolic abstractions
of an underlying domain. Our goal is to learn models of more realistic worlds, which requires
us to adapt the modeling language to accommodate additional uncertainty and complexity.
We do this by:
 Allowing rules to refer to objects not mentioned in the argument list of the action.
 Relaxing the frame assumption: allowing unmodeled noise changes in the world.
 Extending the language: allowing more complex forms of quantification and the construction of new concepts.
311

fiPasula, Zettlemoyer, & Pack Kaelbling

Action parameterization In traditional representations of action dynamics, any objects
whose properties may be changed as the result of an action must be named in the argument
list of the action. Instead, we define actions so that their parameters describe only those
objects that are free parameters of the action: for example, the block to be picked up, and
the object on which the currently held block is to be placed. However, actions can change
the properties of other objects, ones that are not in their parameter list, and our models
should have some way of determining which objects can be affected. In this paper, we
introduce the use of deictic references to identify these objects. Deictic references (Agre
& Chapman, 1987) identify objects relative to the agent or action being performed. For
example, they can refer to objects such as the thing under the block to be picked up, the
currently held object, or the table that the block accidentally falls onto. We use deictic
references as a mechanism for adding new logical variables to our models, in much the same
way as Benson (1996).
Modeling noise In complex domains, actions affect the world in a variety of ways. We
must learn to model not only the circumstances under which they have reasonable effects,
but also their behavior in unusual situations. This complicates the dynamics, and makes
learning more difficult. Also, because these actions are executed in a physical world, they
are not guaranteed to have a small number of simple effects, and as a result they may violate
the outcomes assumption. In the blocks world this can happen, for example, when a stack
is knocked over. We develop a simple noise mechanism that allows us to partially model
action effects, ignoring ones that are rare or too complicated to model explicitly.
Language extension In traditional symbolic domains, rules are constructed using a
predefined set of observable predicates. However, it is sometimes useful to define additional
predicates whose truth values can be computed based on the predefined ones. This has
been found to be essential for modeling certain advanced planning domains (Edelkamp &
Hoffman, 2004).
In traditional blocks worlds, for example, the usual set of predicates contains on, clear
and inhand. When working in our more realistic, noisy blocks world, we found that these
predicates would not be sufficient to allow the agent to learn an accurate model. For
example, it would be difficult to state that putting a block on a tall stack is likely to cause
the stack to topple without having some concept of stack height, or to state that attempting
to pick up a block that is not clear usually picks up the block on the top of its stack without
having some way of describing the block on top of a stack.
While we could simply add those additional predicates that seem useful to the perceptual
language, hand-engineering an appropriate language every time we tackle a new problem is
difficult, time consuming, and error prone. State-of-the-art planning representations such
as PDDL (Edelkamp & Hoffman, 2004) use a concept language to define new predicates or
concepts in terms of previous, simpler ones. In this paper, we show that concepts can be
learned, much like predicates can be invented in ILP (Khan, Muggleton, & Parson, 1998).
As we will see, all of the traditional blocks world predicates, including inhand and clear, as
well as other useful concepts such as height, are easily defined in terms of on given a simple
concept language (Yoon, Fern, & Givan, 2002).
312

fiLearning Symbolic Models of Stochastic World Dynamics

3. State and Action Representation
Our goal is to learn a model of the state transition dynamics of the world. To do so, we need
to be able to represent the set S of possible states of the world and the set A of possible
actions the agent can take. We represent both of these components using a subset of a
relatively standard first-order logic with equality. The representation of states and actions
is ground during inference, learning, and planning.
We begin by defining a primitive language which includes a set of constants C, a set of
predicates , and a set of functions . There are three types of functions in : traditional
functions, which range over objects; discrete-valued functions, which range over a predefined
discrete set of values; and integer-valued functions, which range over a finite subset of the
integers. All of these primitives can be observed directly in the world. (In this work, we
assume that the environment is completely observable; that is, that the agent is able to
perceive an unambiguous and correct description of the current state.2 ) The constants in
C can be assumed to have intrinsic meaning, or can be viewed as meaningless markers
assigned by the perceptual system, as described in detail below.
3.1 State Representation
States describe all the possible different configurations of the properties of and relations
between objects. Each state describes a particular configuration of these values for all
of the objects in the world, where those individual objects are denoted using constants.
There is no limit on the number of objects in a world configuration, though in our current
formalism there is no mechanism for the creation or deletion of objects as a result of the
world dynamics.
Formally, the state descriptions are conjunctive sentences of the form:
^

^

 tG(C,m())

^

()(t) 

^

(t) =  ,

 tG(C,m())

where m(x) is the arity of predicate or function x, C is the set c1 . . . cn of constants, G(x, a)
is the set of all length a lists of elements from x, () indicates that the predicates may
be optionally negated, and  indicates that functions can be assigned to any value in their
range. In this manner, states list the truth values for all of the possible groundings of the
predicates and functions with the terms. Such a sentence gives a complete specification, in
the vocabulary of  and , of the properties and interrelations of the |C| objects present
in the world. (Note that predicate and function arguments are always constants, and never
terms made using function symbols, so these descriptions are always finite given a finite
language.)
In the rest of this section, we describe the two approaches to denoting objects using the
constants in C, and illustrate them with example conjunctive state sentences.
3.1.1 Intrinsic Constants
The first approach to state descriptions refers to objects using intrinsic constants. Each
intrinsic constant is associated with a particular object and consistently used to denote that
2. This is a very strong, and ultimately indefensible assumption; one of our highest priorities for future
work is to extend this to the case when the environment is partially observable.

313

fiPasula, Zettlemoyer, & Pack Kaelbling

same object. Such constants are useful when the perceptual system has some unique way
to identify perceived objects independent of their attributes and relations to one another.
For example, an internet software agent might have access to universal identifiers that
distinguish the objects it perceives.
As an example, let us consider representing the states of a simple blocks world, using a
language that contains the predicates on, clear, inhand, inhand-nil, block, and table, and the
integer-valued function height. The objects in this world include blocks BLOCK-A and BLOCK-B,
a table TABLE, and a gripper. Blocks can be on other blocks or on the table. A block that
has nothing on it is clear. The gripper can hold one block or be empty. The sentence
inhand-nil  on(BLOCK-A, BLOCK-B)  on(BLOCK-B, TABLE)  on(BLOCK-B, BLOCK-A)
on(BLOCK-A, TABLE)  on(TABLE, BLOCK-A)  on(TABLE, BLOCK-B)  on(TABLE, TABLE)
on(BLOCK-A, BLOCK-A)  on(BLOCK-B, BLOCK-B)  table(TABLE)  table(BLOCK-A)
(1)
table(BLOCK-B)  block(BLOCK-A)  block(BLOCK-B)  block(TABLE)  clear(BLOCK-A)
clear(BLOCK-B)  clear(TABLE)  inhand(BLOCK-A)  inhand(BLOCK-B)
inhand(TABLE)  height(BLOCK-A) = 2  height(BLOCK-B) = 1  height(TABLE) = 0
represents a blocks world where the gripper holds nothing and the two blocks are in a single
stack on the table. Block BLOCK-A is on top of the stack, while BLOCK-B is below BLOCK-A and
on the table TABLE.
Under this encoding, the sentence contains meaningful information about the objects
identities, which can then be used when learning about world dynamics.
3.1.2 Skolem Constants
Alternatively, states can also denote objects using skolem constants. Skolem constants are
arbitrary identifiers that are associated with objects in the world but have no inherent
meaning beyond how they are used in a state description.3 Such constants are useful
when the perceptual system has no way of assigning meaningful identifiers to the objects it
observes. As an example, consider how a robot might build a state description of the room
it finds itself in. We assume that this robot can observe the objects that are present, their
properties, and their relationships to each other. However, when naming the objects, it has
no reason to choose any particular name for any specific object. Instead, it just creates
arbitrary identifiers, skolem constants, and uses them to build the state.
Using skolem constants, we can rewrite Sentence 1 as:
inhand-nil  on(c001, c002)  on(c002, c004)  on(c002, c001)
on(c001, c004)  on(c004, c001)  on(c004, c002)  on(c004, c004)
on(c001, c001)  on(c002, c002)  table(c004)  table(c001)
table(c002)  block(c001)  block(c002)  block(c004)  clear(c001)
clear(c002)  clear(c004)  inhand(c001)  inhand(c002)
inhand(c004)  height(c001) = 2  height(c002) = 1  height(c004) = 0
Here, the perceptual system describes the table and the two blocks using the arbitrary
constants c004, c001, and c002.
3. Skolem constants can be interpreted as skolemizations of existential variables.

314

fiLearning Symbolic Models of Stochastic World Dynamics

From this perspective, states of the world are not isomorphic to interpretations of the
logical language, since there might be many interpretations that satisfy a particular statespecification sentence; these interpretations will be the same up to permutation of the
objects the constants refer to. This occurs because objects are only distinguishable based
on their properties and relations to other objects.
The techniques we develop in this paper are generally applicable to representing and
learning the dynamics of worlds with intrinsic constants or skolem constants. We will
highlight the few cases where this is not true as they are presented. We will also see that
the use of skolem constants is not only more perceptually plausible but also forces us to
create new learning algorithms that abstract object identity more aggressively than previous
work and can improve the quality of learned models.
3.2 Action Representation
Actions are represented as positive literals whose predicates are drawn from a special set, ,
and whose terms are drawn from the set of constants C associated with the world s where
the action is to be executed.
For example, in the simulated blocks world,  contains pickup/1, an action for picking up
blocks, and puton/1, an action for putting down blocks. The action literal pickup(BLOCK-A)
could represent the action where the gripper attempts to pickup the block BLOCK-A in the
state represented in Sentence 1.

4. World Dynamics Representation
We are learning the probabilistic transition dynamics of the world, which can be viewed as
the conditional probability distribution Pr(s0 |s, a), where s, s0  S and a  A. We represent
these dynamics with rules constructed from the basic logic described in Section 3, using
logical variables to abstract the identities of particular objects in the world.
In this section, we begin by describing a traditional representation of deterministic world
dynamics. Next, we present the probabilistic case. Finally, we extend it in the ways we
mentioned in Section 2: by permitting the rules to refer to objects not mentioned in the
action description, by adding in noise, and by extending the language to allow for the
construction of new concepts.
A dynamic rule for action z has the form
x.(x)  z(x)  0 (x) ,
meaning that, for any vector of terms x such that the context  holds of them at the
current time step, taking action z(x) will cause the formula 0 to hold of those same terms
in the next step. The action z(x) must contain every xi  x. We constrain  and 0 to
be conjunctions of literals constructed from primitive predicates and terms xi  x, or from
functions applied to these terms and set equal to a value in their range. In addition,  is
allowed to contain literals constructed from integer-valued functions of a term related to an
integer in their range by greater-than or less-than predicates.
We will say that a rule covers a state s and action a if there exists a substitution 
mapping the variables in x to C (note that there may be fewer variables in x than constants
315

fiPasula, Zettlemoyer, & Pack Kaelbling

in C) such that s |= ((x)) and a = z((x)). That is, there is a substitution of constants
for variables that, when it is applied to the context (x), grounds it so that it is entailed
by the state s and, when applied to the rule action z(x), makes it equal to the action a.
Now, given that the rule covers s and a, what can we say of the subsequent state s0 ?
First, the rule directly specifies that 0 ((x)) holds at the next step. But this may be only
an incomplete specification of the state; we will use the frame assumption to fill in the rest:
s0 = 0 ((x)) 

^

^

l(s, , (x))

 {{(t):tG(C,m())}pos(0 ((x)))}



^

^

l(s, , (x)) ,

 {{(t):tG(C,m())}funct(0 ((x)))}

where l(s, y, t) stands for the literal in s that has predicate or function y and argument
list t, pos(0 ) is the set of literals in 0 with negations ignored, and funct(0 ) is the set of
ground functions in 0 extracted from their equality assignments. This is all to say that
every literal that would be needed to make a complete description of the state but is not
included in 0 ((x)) is retrieved, with its associated truth value or equality assignment,
from s.
In general we will have a set of rules for each action, but we will require their contexts to
be mutually exclusive, so that any given state-action pair is covered by at most one rule; if
it is covered by none, then we will assume that nothing changes. 4 As an example, consider
a small set of rules for picking up blocks,
pickup(X, Y) : inhand-nil, on(X, Y), block(Y), height(Y) < 10
 inhand(X), on(X, Y), clear(Y),
pickup(X, Y) : inhand-nil, on(X, Y), table(Y)
 inhand(X), on(X, Y).
The top line of each rule shows the action followed by context; the next line describes the
effects, or the outcome. According to these two rules, executing pickup(X, Y) changes the
world only when the hand is empty and when X is on Y. The exact set of changes depends
on whether Y is the table, or a block of height nine or less.
4.1 Probabilistic Rules
The deterministic dynamics rules described above allow generalization over objects and
exploitation of the frame assumption, but they are not very well suited for use in highly
stochastic domains. In order to apply them to such domains we will have to extend them to
describe the probability distribution over resulting states, Pr(s0 |s, a). Probabilistic STRIPS
operators (Blum & Langford, 1999) model how an agents actions affect the world around it
by describing how these actions alter the properties of and the relationships between objects
in the world. Each rule specifies a small number of simple action outcomessets of changes
that occur in tandem.
4. Without this restriction, we would need to define some method of choosing between the possibly conflicting predictions of the different covering rules. The simplest way to do so would involve picking one
of the rules, perhaps the most specific one, or the one we are most confident of. (Rule confidence scores
would have to be estimated.)

316

fiLearning Symbolic Models of Stochastic World Dynamics

We can see such probabilistic rules as having the form


 p1

01 (x)
x.(x)  z(x)   . . . . . .

 p
0
n n (x)

,

where p1 . . . pn are positive numbers summing to 1, representing a probability distribution, and 01 . . . 0n are formulas describing the subsequent state, s0 .
Given a state s and action a, we can compute coverage as we did in the deterministic
case. Now, however, given a covering substitution (x), probabilistic rules no longer predict
a unique successor state. Instead, each 01 . . . 0n can be used to construct a new state, just
as we did with the single 0 in the deterministic case. There are n such possible subsequent
states, s0i , each of which will occur with associated probability pi .
The probability that a rule r assigns to moving from state s to state s0 when action a
is taken, Pr(s0 |s, a, r), can be calculated as:

P (s0 |s, a, r) =

X

P (s0 , 0i |s, a, r)

0i r

=

X

P (s0 |0i , s, a, r)P (0i |s, a, r)

(2)

0i r

where P (0i |s, a, r) is pi , and the outcome distribution P (s0 |0i , s, a, r) is a deterministic
distribution that assigns all of its mass to the relevant s0 . If P (s0 |0i , s, a, r) = 1.0, that is,
if s0 is the state that would be constructed given that rule and outcome, we say that the
outcome 0i covers s0 .
In general, it is possible, in this representation, for a subsequent state s0 to be covered
by more than one of the rules outcomes. In that case, the probability of s0 occurring is the
sum of the probabilities of the relevant outcomes. Consider a rule for painting blocks:

paint(X) : inhand(X), block(X)
(



.8 : painted(X), wet
.2 : no change.

When this rule is used to model the transition caused by the action paint(a) in an initial
state that contains wet and painted(a), there is only one possible successor state: the one
where no change occurs, so that wet and painted(a) remain true. Both the outcomes describe
this one successor state, and so we must sum their probabilities to recover that states total
probability.
A set of rules specifies a complete conditional probability distribution Pr(s0 |s, a) in the
following way: if the current state s and action a are covered by exactly one rule, then the
distribution over subsequent states is that prescribed by the rule. If not, then s0 is predicted
to be the same as s with probability 1.0.
317

fiPasula, Zettlemoyer, & Pack Kaelbling

As an example, a probabilistic set of rules for picking up blocks might look as follows:
pickup(X, Y) : inhand-nil, on(X, Y), block(Y), height(Y) < 10
(



.7 : inhand(X), on(X, Y), clear(Y)
.3 : no change

pickup(X, Y) : inhand-nil, on(X, Y), table(Y)
(



.8 : inhand(X), on(X, Y)
.2 : no change

The top line of each rule still shows the action followed by the context; the bracket surrounds
the outcomes and their distribution. The outcomes are the same as before, only now there
is a small chance that they will not occur.
4.2 Deictic Reference
In standard relational representations of action dynamics, a variable denoting an object
whose properties may be changed as the result of an action must be named in the argument
list of the action. This can result in awkwardness even in deterministic situations. For example, the abstract action of picking up a block must take two arguments. In pickup(X, Y),
X is the block to be picked up and Y is the block from which it is to be picked up. This
relationship is encoded by an added condition on(X, Y) in the rules context. That condition does not restrict the applicability of the rule; it exists to guarantee that Y is bound
to the appropriate object. This restriction has been adopted because it means that, given
a grounding of the action, all the variables in the rule are bound, and it is not necessary to
search over all substitutions  that would allow a rule to cover a state. However, it can complicate planning because, in many cases, all ground instances of an operator are considered,
even though most of them are eventually rejected due to violations of the preconditions. In
our example, we would reject all instances violating the on(X, Y) relation in the context.
In more complex domains, this requirement is even more awkward: depending on the
circumstances, taking an action may affect different, varied sets of objects. In blocks worlds
where a block may be on several others, the pickup action may affect the properties of each
of those blocks. To model this without an additional mechanism for referring to objects,
we might have to increase, or even vary, the number of arguments pickup takes.
To handle this more gracefully, we extend the rule formalism to include deictic references
to objects. Each rule may be augmented with a list, D, of deictic references. Each deictic
reference consists of a variable Vi and restriction i which is a set of literals that define
Vi with respect to the variables x in the action and the other Vj such that j < i. These
restrictions are supposed to pick out a single, unique object: if they do notif they pick out
several, or nonethe rule fails to apply. So, to handle the pickup action described above,
the action would have a single argument, pickup(X), and the rule would contain a deictic
variable V with the constraint that on(X, V).
To use rules with deictic references, we must extend our procedure for computing rule
coverage to ensure that all of the deictic references can be resolved. The deictic variables
may be bound simply by starting with bindings for x and working sequentially through the
deictic variables, using their restrictions to determine their unique bindings. If at any point
318

fiLearning Symbolic Models of Stochastic World Dynamics

the binding for a deictic variable is not unique, it fails to refer, and the rule fails to cover
the stateaction pair.
This formulation means that extra variables need not be included in the action specification, which reduces the number of operator instances, and yet, because of the requirement
for unique designation, a substitution can still be quickly discovered while testing coverage.
So, for example, to denote the red block on the table as V2 (assuming that there were
only one table and one such block) we would use the following deictic references:
V1 : table(V1 )
V2 : color (V2 ) = red  block (V2 )  on(V2 , V1 ) .
If there were several, or no, tables in the world, then, under our rule semantics, the first
reference would fail: similarly, the second reference would fail if the number of red blocks
on the unique table represented by V1 were not one.
To give a more action-oriented example, when denoting the block on top of the block
I touched, where touch(Z) was the action, we would use the following deictic reference:
V1 : on(V1 , Z)  block (V1 ) .
A set of deictic probabilistic rules for picking up blocks might look as follows:
pickup(X) :

n

Y : inhand(Y), Z : table(Z)

o

empty context
(



.9 : inhand-nil, inhand(Y), on(Y, Z)
.1 : no change

pickup(X) :

n

Y : block(Y), on(X, Y)

o

inhand-nil, height(Y) < 10
(



.7 : inhand(X), on(X, Y), clear(Y)
.3 : no change

pickup(X) :

n

Y : table(Y), on(X, Y)

o

inhand-nil
(
.8 : inhand(X), on(X, Y)

.2 : no change
The top line of each rule now shows the action followed by the deictic variables, where each
variable is annotated with its restriction. The next line is the context, and the outcomes
and their distribution follow. The first rule applies in situations where there is something in
the gripper, and states that there is a probability of 0.9 that action will cause the gripped
object to fall to the table, and that nothing will change otherwise. The second rule applies
in situations where the object to be picked up is on another block, and states that the
probability of success is 0.7. The third rule applies in situations where the object to be
picked up is on the table and describes a slightly higher success probability, 0.8. Note that
different objects are affected, depending on the state of the world.
319

fiPasula, Zettlemoyer, & Pack Kaelbling

4.3 Adding Noise
Probability models of the type we have seen thus far, ones with a small set of possible
outcomes, are not sufficiently flexible to handle the noise of the real world. There may be a
large number of possible outcomes that are highly unlikely, and reasonably hard to model:
for example, all the configurations that may result when a tall stack of blocks topples. It
would be inappropriate to model such outcomes as impossible, but we dont have the space
or inclination to model each of them as an individual outcome.
So, we will allow our rule representation to account for some results as noise. By definition, noise will be able to represent the outcomes whose probability we havent quantified.
Thus, by allowing noise, we will lose the precision of having a true probability distribution
over next states.
To handle noise, we must change our rules in two ways. First, each rule will have an
additional noise outcome 0noise , with an associated probability P (0noise |s, a, r); now, the
set of outcome probabilities that must sum to 1.0 will include P (0noise |s, a, r) as well as
P (01 |s, a, r) . . . P (0n |s, a, r). However, 0noise will not have an associated list of literals,
since we are declining to model in detail what happens to the world in such cases.
Second, we will create an additional default rule, with an empty context and two outcomes: an empty outcome (which, in combination with the frame assumption, models the
situations where nothing changes), and, again, a noise outcome (modeling all other situations). This rule allows noise to occur in situations where no other specific rule applies; the
probability assigned to the noise outcome in the default rule specifies a kind of background
noise level.
Since we are not explicitly modeling the effects of noise, we can no longer calculate
the transition probability Pr(s0 |s, a, r) using Equation 2: we lack the required distribution
P (s0 |0i , s, a, r) for the noise outcome. Instead, we substitute a worst case constant bound
pmin  P (s0 |0noise , s, a, r). This allows us to bound the transition probability as
P (s0 |s, a, r) = pmin P (0noise |s, a, r) +

X

P (s0 |0i , s, a, r)P (0i |s, a, r)

0i r

 P (s0 |s, a, r).

(3)

Intuitively, pmin assigns a small amount of probability mass to every possible next state
Note that it can take a value higher than the true minimum: it is an approximation.
However, to ensure that the probability model remains well-defined, pmin times the number
of possible states should not exceed 1.0.
In this way, we create a partial model that allows us to ignore unlikely or overly complex
state transitions while still learning and acting effectively. 5
Since these rules include noise and deictic references, we call them Noisy Deictic Rules
(NDRs). In a rather stochastic world, the set of NDRs for picking up blocks might now
s0 .

5. P (s0 |0noise , s, a, r) could be modeled using any well-defined probability distribution describing the noise
of the world, which would give us a full distribution over the next states. The premise here is that
it might be difficult to specify such a distributionin our domain, we would have to ensure that this
distribution does not assign probability to worlds that are impossible, such as worlds where some blocks
are floating in midair. As long as these events are unlikely enough that we would not want to consider
them while planning, it is reasonable to not model them directly.

320

fiLearning Symbolic Models of Stochastic World Dynamics

look as follows:
pickup(X) :

n

Y : inhand(Y), Z : table(Z)

o

empty context


 .6 : inhand-nil, inhand(Y), on(Y, Z)
.1 : no change


 .3 : noise
pickup(X) :

n

Y : block(Y), on(X, Y)

o

inhand-nil, height(Y) < 10




 .7 : inhand(X), on(X, Y), clear(Y)

.1 : no change


 .2 : noise
n

pickup(X) :

Y : table(Y), on(X, Y)

o

inhand-nil


 .8 : inhand(X), on(X, Y)
.1 : no change


 .1 : noise
default (rule:
.6 : no change

.4 : noise
The format of the rules is the same as before, in Section 4.2, except that each rule now
includes an explicit noise outcome. The first three rules are very similar to their old versions,
the only difference being that they model noise. The final rule is the default rule: it states
that, if no other rule applies, the probability of observing a change is 0.4.
Together these rules provide a complete example of the type of rule set that we will learn
in Section 5.1. However, they were written with a fixed modeling language of functions and
predicates. The next section describes how concepts can be used to extend this language.
4.4 Concept Definitions
In addition to the observed primitive predicates, it is often useful to have background
knowledge that defines additional predicates whose truth values can be computed based
on the observations. This has been found to be essential for modeling certain planning
domains (Edelkamp & Hoffman, 2004).
This background knowledge consists of definitions for additional concept predicates
and functions. In this work, we express concept definitions using a concept language
that includes conjunction, existential quantification, universal quantification, transitive closure, and counting. Quantification is used for defining concepts such as inhand(X) :=
block(X)  Y.on(X, Y ). Transitive closure is included in the language via the Kleene star
operator and defines concepts such as above(X, Y ) := on (X, Y ). Finally, counting is included using a special quantifier # which returns the number of objects for which a formula is
true. It is useful for defining integer-valued functions such as height(X) := #Y.above(X, Y ).
321

fiPasula, Zettlemoyer, & Pack Kaelbling

Once defined, concepts enable us to simplify the context and the deictic variable definitions, as well as to restrict them in ways that cannot be described using simple conjunctions.
Note, however, that there is no need to track concept values in the outcomes, since they can
always be computed from the primitives. Therefore, only the rule contexts use a language
enriched by concepts; the outcomes contain primitives.
As an example, here is a deictic noisy rule for attempting to pick up block X, side by
side with the background knowledge necessary when the only primitive predicates are on
and table.

pickup(X) :




 Y : topstack(Y, X), 


Z : on(Y, Z),

 T : table(T)




inhand-nil, height(Y) < 9


.80 : on(Y, Z)


 .10 : on(Y, Z), on(Y, T)


.05 : no change



.05 : noise

clear(X)

:= Y.on(Y, X)

inhand(X) := block(X)  Y.on(X, Y)
inhand-nil := Y.inhand(Y)
above(X, Y)

:= on (X, Y)

(4)

topstack(X, Y) := clear(X)  above(X, Y)
height(X) := #Y.above(X, Y)

The rule is more complicated than the example rules given thus far: it deals with the
situation where the block to be picked up, X, is in the middle of a stack. The deictic variable
Y identifies the (unique) block on top of the stack, the deictic variable Zthe object under
Y , and the deictic variable Tthe table. As might be expected, the gripper succeeds in
lifting Y with a high probability.
The concept definitions include clear(X), defined as There exists no object that is on
X; inhand(X), defined as X is a block that is not on any object; inhand-nil, defined as
There exists no object such that it is in the hand; above(X, Y), defined as the transitive
closure of on(X, Y); topstack(X, Y), defined as X is above Y, and clear; and height(X),
defined as The number of objects that can are below X using a chain of ons. As explained
above, these concepts are used only in the context and the deictic variable definitions, while
outcomes track only the primitive predicates; in fact, only on appears in the outcomes, since
the value of the table predicates never changes.
4.5 Action Models
We combine a set of concept definitions and a set of rules to define an action model. Our
best action models will represent the rule set using NDRs, but, for comparison purposes,
some of our experiments will involve rule sets that use simpler representations, without
noise or deictic references. Moreover, the rule sets will differ in whether they are allowed
to contain constants. The rules presented so far have contained none, neither in their
context nor in the outcomes. This is the only reasonable setup when states contain skolem
constants, as these constants have no inherent meaning and the names they are assigned
will not in general be repeated. However, when states have intrinsic constants, it is perfectly
acceptable to include constants in action models. After all, these constants can be used to
uniquely identify objects in the world.
As we develop a learning algorithm in the next section, we will assume in general that
constants are allowed in the action model, but we will show how simple restrictions within
322

fiLearning Symbolic Models of Stochastic World Dynamics

the algorithm can ensure that the learned models do not contain any. We also show, in
Section 7, that learning action models which are restricted to be free of constants provides
a useful bias that can improve generalization when training with small data sets.

5. Learning Action Models
Now that we have defined rule action models, we will describe how they may be constructed
using a learning algorithm that attempts to return the action model that best explains a set
of example actions and their results. More formally, this algorithm takes a training set E,
where each example is a (s, a, s0 ) triple, and searches for an action model A that maximizes
the likelihood of the action effects seen in E, subject to a penalty on complexity.
Finding A involves two distinct problems: defining a set of concept predicates, and
constructing a rule set R using a language that contains these predicates together with the
directly observable primitive predicates. In this section, we first discuss the second problem,
rule set learning, assuming some fixed set of predicates is provided to the learner. Then,
we present a simple algorithm that discovers new, useful concept predicates.
5.1 Learning Rule Sets
The problem of learning rule sets is, in general, NP-hard (Zettlemoyer, Pasula, & Kaelbling,
2003). Here, we address this problem by using greedy search. We structure the search
hierarchically by identifying two self-contained subproblems: outcome learning, which is a
subproblem of the general rule set search, and parameter estimation, which is a subproblem
of outcome learning. Thus, the overall algorithm involves three levels of greedy search:
an outermost level, LearnRules, which searches through the space of rule sets, often by
constructing new rules, or altering existing ones; a middle level, InduceOutcomes which,
given an incomplete rule consisting of a context, an action, and a set of deictic references, fills
in the rest of the rule; and an innermost level, LearnParameters, which takes a slightly more
complete rule, now lacking only a distribution over the outcomes, and finds the distribution
that optimizes the likelihood of the examples covered by this rule. We present these three
levels starting from the inside out, so that each subroutine is described before the one that
depends on it. Since all three subroutines attempt to maximize the same scoring metric,
we begin by introducing this metric.
5.1.1 The Scoring Metric
A greedy search algorithm must judge which parts of the search space are the most desirable.
Here, this is done with the help of a scoring metric over rule sets,
S(R) =

X

log(P (s0 |s, a, r(s,a) ))  

(s,a,s0 )E

X

P EN (r)

(5)

rR

where r(s,a) is the rule governing the transition occurring when a is performed in s,  is
a scaling parameter, and P EN (r) is a complexity penalty applied to rule r. Thus, S(R)
favors rule sets that maximize the likelihood bound on the data and penalizes rule sets that
are overly complex.
Ideally, P would be the likelihood of the example. However, rules with noise outcomes
cannot assign an exact likelihood so, in their case, we use the lower bound defined in Equa323

fiPasula, Zettlemoyer, & Pack Kaelbling

tion 3 instead. P EN (r) is defined simply as the total number of literals in r. We chose this
penalty for its simplicity, and also because it performed no worse than any other penalty
term we tested in informal experiments. The scaling parameter  is set to 0.5 in our experiments, but it could also be set using cross-validation on a hold-out dataset or some other
principled technique. This metric puts pressure on the model to explain examples using
non-noise outcomes, which increases P , but also has opposing pressure on complexity, via
P EN (r).
If we assume that each state-action pair (s, a) is covered by at most one rule (which, for
any finite set of examples, can be enforced simply by ensuring that each examples stateaction pair is covered by at most one rule) we can rewrite the metric in terms of rules rather
than examples, to give
S(R) =

X

X

rR

(s,a,s0 )Er

log(P (s0 |s, a, r))  P EN (r)

(6)

where Er is the set of examples covered by r. Thus, each rules contribution to S(R) can
be calculated independently of the others.
5.1.2 Learning Parameters
The first of the algorithms described in this section, LearnParameters, takes an incomplete
rule r consisting of an action, a set of deictic references, a context, and a set of outcomes,
and learns the distribution P that maximizes rs score on the examples Er covered by it.
Since the procedure is not allowed to alter the number of literals in the rule, and therefore
cannot affect the complexity penalty term, the optimal distribution is simply the one that
maximizes the log likelihood of Er . In the case of rules with noise outcomes this will be
log(P (s0 |s, a, r))

X

L =

(s,a,s0 )Er



=



log pmin P (0noise |s, a, r) +

X

X

P (s0 |0i , s, a, r)P (0i |s, a, r) .

(7)

0i r

(s,a,s0 )Er

For each non-noise outcome, P (s0 |0i , s, a, r) is one if 0i covers (s, a, s0 ) and zero otherwise.
(In the case of rules without noise outcomes, the sum will be slightly simpler, with the
pmin P (0noise |s, a, r) term missing.)
When every example is covered by a unique outcome, Ls maximum can be expressed
in a closed form. Let the set of examples covered by an outcome 0 be E0 . If we add a
Lagrange multiplier to enforce the constraint that the P (0i |s, a, r) distributions must sum
to 1.0, we will get


L =

X
(s,a,s0 )Er

=

X
E0

log 


X

P (s0 |0i , s, a, r)P (0i |s, a, r) + (

X

0i r

0i

|E0 | log P (0i |s, a, r) + (


X

P (0i |s, a, r)  1.0) .

0i

324

P (0i |s, a, r)  1.0)

fiLearning Symbolic Models of Stochastic World Dynamics

Then, the partial derivative of L with respect to P (0i |s, a, r) will be |E0 |/P (0i |s, a, r)
 and  = |E|, so that P (0i |s, a, r) = |E0i |/|E|. Thus, the parameters can be estimated
by calculating the percentage of the examples that each outcome covers.
However, as we have seen in Section 4.1, it is possible for each example to be covered by
more than one outcome; indeed, when we have a noise outcome, which covers all examples,
this will always be the case. In this situation, the sum over examples cannot be rewritten as
a simple sum of terms each representing a different outcomes and containing only a single
relevant probability: the probabilities of overlapping outcomes remain tied together, no
general closed-form solution exists, and estimating the maximum-likelihood parameters is a
nonlinear programming problem. Fortunately, it is an instance of the well-studied problem
of maximizing a concave function (the log likelihood presented in Equation 7) over a probability simplex. Several gradient ascent algorithms are known for this problem (Bertsekas,
1999); since the function is concave, they are guaranteed to converge to the global maximum.
LearnParameters uses the conditional gradient method, which works by, at each iteration,
moving along the parameter axis with the maximal partial derivative. The step-sizes are
chosen using the Armijo rule (with the parameters s = 1.0,  = 0.1, and  = 0.01.) The
search converges when the improvement in L is very small, less than 106 . We chose this
algorithm because it was easy to implement and converged quickly for all of the experiments
that we tried. However, if problems are found where this method converges too slowly, one
of the many other nonlinear optimization methods, such as a constrained Newtons method,
could be directly applied.
5.1.3 Inducing Outcomes
Given LearnParameters, an algorithm for learning a distribution over outcomes, we can
now consider the problem of taking an incomplete rule r consisting of a context, an action,
and perhaps a set of deictic references, and finding the optimal way to fill in the rest of
the rulethat is, the set of outcomes {01 . . . 0n } and the associated distribution P that
maximize the score
S(r) =

X

log(P (s0 |s, a, r))  P ENo (r),

(s,a,s0 )Er

where Er is the set of examples covered by r, and P ENo (r) is the total number of literals in
the outcomes of r. (S(r) is that factor of the scoring metric in Equation 6 which is due to
rule r, without those aspects of P EN (r) which are fixed for the purposes of this subroutine:
the number of literals in the context.)
In general, outcome induction is NP-hard (Zettlemoyer, Pasula, & Kaelbling, 2003). InduceOutcomes uses greedy search through a restricted subset of possible outcome sets: those
that are proper on the training examples, where an outcome set is proper if every outcome
covers at least one training example. Two operators, described below, move through this
space until there are no more immediate moves that improve the rule score. For each set of
outcomes it considers, InduceOutcomes calls LearnParameters to supply the best P it can.
The initial set of outcomes is created by, for each example, writing down the set of
atoms that changed truth values as a result of the action, and then creating an outcome to
describe every set of changes observed in this way.
325

fiPasula, Zettlemoyer, & Pack Kaelbling

E1
E2
E3
E4

01
02
03
04

= t(c1), h(c2)  h(c1), h(c2)
= h(c1), t(c2)  h(c1), h(c2)
= h(c1), h(c2)  t(c1), t(c2)
= h(c1), h(c2)  h(c1), h(c2)
(a)

= {h(c1)}
= {h(c2)}
= {t(c1), t(c2)}
= {no change}
(b)

Figure 2: (a) Possible training data for learning a set of outcomes. (b) The initial set of
outcomes that would be created from the data in (a) by picking the smallest
outcome that describes each change.

As an example, consider the coins domain. Each coins world contains n coins, which
can be showing either heads or tails. The action flip-coupled, which takes no arguments,
flips all of the coins, half of the time to heads, and otherwise to tails. A set of training
data for learning outcomes with two coins might look like part (a) of Figure 2 where h(C)
stands for heads(C), t(C) stands for heads(C), and s  s0 is part of an (s, a, s0 ) example
where a = flip-coupled. Now suppose that we have suggested a rule for flip-coupled that
has no context or deictic references. Given our data, the initial set of outcomes has the four
entries in part (b) of Figure 2.
If our rule contained variables, either as abstract action arguments or in the deictic
references, InduceOutcomes would introduce those variables into the appropriate places in
the outcome set. This variable introduction is achieved by applying the inverse of the action
substitution to each examples set of changes while computing the initial set of outcomes. 6
So, given a deictic reference C : red(C) which was always found to refer to c1, the only red
coin, our example set of outcomes would contain C wherever it currently contains c1.
Finally, if we disallow the use of constants in our rules, variables become the only way for
outcomes to refer to the objects whose properties have changed. Then, changes containing a
constant which is not referred to by any variable cannot be expressed, and the corresponding
example will have to be covered by the noise outcome.
Outcome Search Operators

InduceOutcomes uses two search operators. The first is an add operator, which picks
a pair of non-contradictory outcomes in the set and creates a new outcome that is their
conjunction. For example, it might pick 01 and 02 and combine them, adding a new
outcome 05 = {h(c1), h(c2)} to the set. The second is a remove operator that drops an
outcome from the set. Outcomes can only be dropped if they were overlapping with other
outcomes on every example they cover, otherwise the outcome set would not remain proper.
(Of course, if the outcome set contains a noise outcome, then every other outcome can be
dropped, since all of its examples are covered by the noise outcome.) Whenever an operator
adds or removes an outcome, LearnParameters is called to find the optimal distribution
6. Thus, InduceOutcomes introduces variables aggressively wherever possible, based on the intuition that
if any of the corresponding objects would be better described by a constant, this will become apparent
through some other training example.

326

fiLearning Symbolic Models of Stochastic World Dynamics

over the new outcome set, which can then be used to calculate the maximum log likelihood
of the data with respect to the new outcome set.
Sometimes, LearnParameters will return zero probabilities for some of the outcomes.
Such outcomes are removed from the outcome set, since they contribute nothing to the
likelihood, and only add to the complexity. This optimization improves the efficiency of the
search.
In the outcomes of Figure 2, 04 can be dropped since it covers only E4 , which is also
covered by both 01 and 02 . The only new outcome that can be created by conjoining the
existing ones is 05 = {h(c1), h(c2)}, which covers E1 , E2 , and E3 . Thus, if 05 is added, then
01 and 02 can be dropped. Adding 05 and dropping 01 , 02 , and 04 creates the outcome
set {03 , 05 }, which is the optimal set of outcomes for the training examples in Figure 2.
Notice that an outcome is always equal to the union of the sets of literals that change in
the training examples it covers. This fact ensures that every proper outcome can be made
by merging outcomes from the initial outcome set. InduceOutcomes can, in theory, find any
set of outcomes.
5.1.4 Learning Rules
Now that we know how to fill in incomplete rules, we will describe LearnRules, the outermost
level of our learning algorithm, which takes a set of examples E and a fixed language of
primitive and derived predicates, and performs a greedy search through the space of rule
sets. More precisely, it searches through the space of proper rule sets, where a rule set R
is defined as proper with respect to a data set E if it includes at most one rule that is
applicable to every example e  E in which some change occurs, and if it does not include
any rules that are applicable to no examples.
The search proceeds as described in the pseudocode in Figure 3. It starts with a rule set
that contains only the default rule. At every step, it takes the current rule set and applies
all its search operators to it to obtain a set of new rule sets. It then selects the rule set R
that maximizes the scoring metric S(R) as defined in Equation 5. Ties in S(R) are broken
randomly.
We will begin by explaining how the search is initialized, then go on to describe the
operators used, and finish by working through a simple example that shows LearnRules in
action.
Rule Set Search Initialization

LearnRules can be initialized with any proper rule set. In this paper, we always initialize
the set with only the noisy default rule. This treats all action effects in the training set as
noise; as the search progresses, the search operators will introduce rules to explain action
effects explicitly. We chose this initial starting point for its simplicity, and because it worked
well in informal experiments. Another strategy would be to start with a very specific rule
set, describing in detail all the examples. Such bottom-up methods have the advantage
of being data-driven, which can help search reach good parts of the search space more
easily. However, as we will show, several of the search operators used by the algorithm
presented here are guided by the training examples, so the algorithm already has this
desirable property. Moreover, this bottom-up method has bad complexity properties in
327

fiPasula, Zettlemoyer, & Pack Kaelbling

LearnRuleSet(E)
Inputs:
Training examples E
Computation:
Initialize rule set R to contain only the default rule
While better rules sets are found
For each search operator O
Create new rule sets with O, RO = O(R, E)
For each rule set R0  RO
If the score improves (S(R0 ) > S(R))
Update the new best rule set, R = R0
Output:
The final rule set R

Figure 3: LearnRuleSet pseudocode. This algorithm performs greedy search through the space of
rule sets. At each step a set of search operators each propose a set of new rule sets. The
highest scoring rule set is selected and used in the next iteration.

situations where a large data set can be described using a relatively simple set of rules,
which is the case we are most interested in.
Rule Set Search Operators

During rule set search, LearnRules repeatedly finds and applies the operator that will
increase the score of the current rule set the most.
Most of the search operators work by creating a new rule or set of rules (usually by
altering an existing rule) and then integrating these new rules into the rule set in a way
that ensures the rule set remains proper. Rule creation involves picking an action z, a
set of deictic references D, and a context , and then calling on the InduceOutcomes
learning algorithm to complete the rule by filling in the 0i s and pi s. (If the new rule covers
no examples, the attempt is abandoned, since adding such a rule cannot help the scoring
metric.) Integration into a rule set involves not just adding the new rules, but also removing
the old rules that cover any of the same examples. This can increase the number of examples
covered by the default rule.
5.1.5 Search Operators
Each search operator O takes as input a rule set R and a set of training examples E, and
creates a set of new rule sets RO to be evaluated by the greedy search loop. There are eleven
search operators. We first describe the most complex operator, ExplainExamples, followed
by the most simple one, DropRules. Then, we present the remaining nine operators, which
all share the common computational framework outlined in Figure 4.
Together, these operators provide many different ways of moving through the space of
possible rule sets. The algorithm can be adapted to learn different types of rule sets (for
example, with and without constants) by restricting the set of search operators used.
328

fiLearning Symbolic Models of Stochastic World Dynamics

OperatorTemplate(R, E)
Inputs:
Rule set R
Training examples E
Computation:
Repeatedly select a rule r  R
Create a copy of the input rule set R0 = R
Create a new set of rules, N , by making changes to r
For each new rule r0  N that covers some examples
Estimate new outcomes for r0 with InduceOutcomes
Add r0 to R0 and remove any rules in R0 that cover any
examples r0 covers
Recompute the set of examples that the default rule in R0
covers and the parameters of this default rule
Add R0 to the return rule sets RO
Output:
The set of rules sets, RO

Figure 4: OperatorTemplate Pseudocode. This algorithm is the basic framework that is used by
six different search operators. Each operator repeatedly selects a rule, uses it to make n
new rules, and integrates those rules into the original rule set to create a new rule set.

 ExplainExamples takes as input a training set E and a rule set R and creates new,
alternative rule sets that contain additional rules modeling the training examples
that were covered by the default rule in R. Figure 5 shows the pseudocode for this
algorithm, which considers each training example E that was covered by the default
rule in R, and executes a three-step procedure. The first step builds a large and
specific rule r0 that describes this example; the second step attempts to trim this rule,
and so generalize it so as to maximize its score, while still ensuring that it covers E;
and the third step creates a new rule set R0 by copying R and integrating the new
rule r0 into this new rule set.
As an illustration, let us consider how steps 1 and 2 of ExplainExamples might be
applied to the training example (s, a, s0 ) = ({on(a, t), on(b, a)}, pickup(b), {on(a, t)}),
when the background knowledge is as defined for Rule 4 in Section 4.4 and constants
are not allowed.
Step 1 builds a rule r. It creates a new variable X to represent the object b in the
action; then, the action substitution becomes  = {X  b}, and the action of r is
set to pickup(X). The context of r is set to the conjunction inhand-nil, inhand(X),
clear(X), height(X) = 2, on(X, X), above(X, X), topstack(X, X). Then, in Step
1.2, ExplainExamples attempts to create deictic references that name the constants
whose properties changed in the example, but which are not already in the action substitution. In this case, the only changed literal is on(b, a), and b is in the substitution,
so C = {a}; a new deictic variable Y is created and restricted, and  is extended to
329

fiPasula, Zettlemoyer, & Pack Kaelbling

ExplainExamples(R, E)
Inputs:
A rule set R
A training set E
Computation:
For each example (s, a, s0 )  E covered by the default rule in R
Step 1: Create a new rule r
Step 1.1: Create an action and context for r
Create new variables to represent the arguments of a
Use them to create a new action substitution 
Set rs action to be  1 (a)
Set rs context to be the conjunction of boolean and equality literals that can
be formed using the variables and the available functions and predicates
(primitive and derived) and that are entailed by s
Step 1.2: Create deictic references for r
Collect the set of constants C whose properties changed from s to s0 , but
which are not in 
For each c  C
Create a new variable v and extend  to map v to c
Create , the conjunction of literals containing v that can be formed using
the available variables, functions, and predicates, and that are entailed by s
Create deictic reference d with variable v and restriction  1 ()
If d uniquely refers to c in s, add it to r
Step 1.3: Complete the rule
Call InduceOutcomes to create the rules outcomes.
Step 2: Trim literals from r
Create a rule set R0 containing r and the default rule
Greedily trim literals from r, ensuring that r still covers (s, a, s0 ) and filling in the
outcomes using InduceOutcomes until R0 s score stops improving
Step 3: Create a new rule set containing r
Create a new rule set R0 = R
Add r to R0 and remove any rules in R0 that cover any examples r covers
Recompute the set of examples that the default rule in R0 covers and the parameters
of this default rule
Add R0 to the return rule sets RO
Output:
A set of rule sets, RO

Figure 5: ExplainExamples Pseudocode. This algorithm attempts to augment the rule set with new
rules covering examples currently handled by the default rule.

330

fiLearning Symbolic Models of Stochastic World Dynamics

be {X  b, Y  a}. Finally, in Step 1.3, the outcome set is created. Assuming that
of the examples for which context applies, nine out of ten end with X being lifted,
and the rest with it falling onto the table, the resulting rule r0 looks as follows:


inhand(Y ), clear(Y), on(X, Y), table(Y) 

pickup(X) : Y : above(X, Y), topstack(X, Y), above(Y, Y)


topstack(Y, Y), on(Y, Y), height(Y) = 1
inhand-nil, inhand(X), clear(X), table(X), height(X) = 2, on(X, X),
above(X,
X), topstack(X, X)

0.9 : on(X, Y)

0.1 : noise

(The falls on table outcome is modeled as noise, since in the absence of constants
the rule has no way of referring to the table.)
In Step 2, ExplainExamples trims this rule to remove the literals that are always true
in the training examples, like on(X, X), and the table()s, and the redundant ones,
like inhand(), clear(Y), and perhaps one of the heights, to give

	
pickup(X) : Y : on(X, Y)
inhand-nil,
clear(X), height(X) = 2

0.9 : on(X, Y)

0.1 : noise

This rules context describes the starting example concisely. Explain Examples will
consider dropping some of the remaining literals, and thereby generalizing the rule so
it applies to examples with different starting states. However, such generalizations do
not necessarily improve the score. While they have smaller contexts, they might end
up creating more outcomes to describe the new examples, so the penalty term is not
guaranteed to improve. The change in the likelihood term will depend on whether the
new examples have higher likelihood under the new rule than under the default rule,
and on whether the old examples have higher likelihood under the old distribution
than under the new one. Quite frequently, the need to cover the new examples will
give the new rule a distribution that is closer to random than before, which will usually
lead to a decrease in likelihood too large to be overcome by the improvement in the
penalty, given the likelihood-penalty trade-off.
Let us assume that, in this case, no predicate can be dropped without worsening the
likelihood. The rule will then be integrated into the rule set as is.
 DropRules cycles through all the rules in the current rule set, and removes each one
in turn from the set. It returns a set of rule sets, each one missing a different rule.
The remaining operators create new rule sets from the input rule set R by repeatedly
choosing a rule r  R and making changes to it to create one or more new rules. These new
rules are then integrated into R, just as in ExplainExamples, to create a new rule set R0 .
Figure 4 shows the the general pseudocode for how this is done. The operators vary in the
way they select rules and the changes they make to them. These variations are described
331

fiPasula, Zettlemoyer, & Pack Kaelbling

for each operator below. (Note that some of the operators, those that deal with deictic
references and constants, are only applicable when the action model representation allows
these features.)
 DropLits selects every rule r  R n times, where n is the number of literals in the
context of r; in other words, it selects each r once for each literal in its context. It
then creates a new rule r0 by removing that literal from rs context; N of Figure 4 is
simply the set containing r0 .
So, the example pickup rule created by ExplainExamples would be selected three times,
once for inhand-nil, once for clear(X), and one for height(X) = 2, and so would create
three new rules (each with a different literal missing), three singleton N sets, and
three candidate new rule sets R0 . Since the newly-created r0 are generalizations of r,
they are certain to cover all of rs examples, and so r will be removed from each of
the R0 s.
The changes suggested by DropLits are therefore exactly the same as those suggested by the trimming search in ExplainExamples, but there is one crucial difference:
DropLits attempts to integrate the new rule into the full rule set, instead of just making a quick comparison to the default rule as in Step 2 of ExplainExamples. This is
because ExplainExamples used the trimming search only as a relatively cheap, local
heuristic allowing it to decide on a rule size, while DropLits uses it to search globally
through the space of rule sets, comparing the contributions of various conflicting rules.
 DropRefs is an operator used only when deictic references are permitted. It selects
each rule r  R once for each deictic reference in r. It then creates a new rule r0 by
removing that deictic reference from r; N is, again, the set containing only r0 .
When applying this operator, the pickup rule would be selected once, for the reference
describing Y , and only one new rule set would be returned: one containing the rule
without Y .
 GeneralizeEquality selects each rule r  R twice for each equality literal in the context
to create two new rules: one where the equality is replaced by a , and one where
it is replaced by a . Each rule will then be integrated into the rule set R, and the
resulting two R0 s returned. Again, these generalized rules are certain to cover all of
rs examples, and so the R0 s will not contain r.
The context of our pickup rule contains one equality literal, height(X) = 1. GeneralizeEquality will attempt to replace this literal with height(X)  1 and height(X)  1.
In a domain containing more than two blocks, this would be likely to yield interesting
generalizations.
 ChangeRanges selects each rule r  R n times for each equality or inequality literal in
the context, where n is the total number of values in the range of each literal. Each
time it selects r it creates a new rule r0 by replacing the numeric value of the chosen
(in)equality with another possible value from the range. Note that it is quite possible
that some of these new rules will cover no examples, and so will be abandoned. The
remaining rules will be integrated into new copies of the rule set as usual.
332

fiLearning Symbolic Models of Stochastic World Dynamics

Thus, if f () ranges over [1 . . . n], ChangeRange would, when applied to a rule containing the inequality f () < i, construct rule sets in which i is replaced by all other
integers in [1 . . . n].
Our pickup rule contains one equality literal, height(X) = 1. In the two-block domain
from which our (s, a, s0 ) example was drawn, height() can take on the values 0, 1, and 2,
so the rule will, again, be selected thrice, and new rules will be created containing the
new equalities. Since the rule constrains X to be on something, the new rule containing
height(X) = 0 can never cover any examples and will certainly be abandoned.
 SplitOnLits selects each rule r  R n times, where n is the number of literals that
are absent from the rules context and deictic references. (The set of absent literals
is obtained by applying the available functions and predicatesboth primitive and
derivedto the terms present in the rule, and removing the literals already present in
the rule from the resulting set.) It then constructs a set of new rules. In the case of
predicate and inequality literals, it creates one rule in which the positive version of the
literal is inserted into the context, and one in which it is the negative version. In the
case of equality literals, it constructs a rule for every possible value the equality could
take. In either case, rules that cover no examples will be dropped. Any remaining
rules corresponding to the one literal are placed in N , and they are then integrated
into the rule set simultaneously.
Note that the newly created rules will, between them, cover all the examples that
start out covered by the original rule and no others, and that these examples will be
split between them.
The list of literals that may be added to the pickup rule consists of inhand(X),
inhand(Y), table(X), table(Y), clear(Y), on(Y, X), on(Y, Y), on(X, X), height(Y) =?,
and all possible applications of above and topstack. These literals do not make for
very interesting examples: adding them to the context will create rules that either
cover no examples at all, and so will be abandoned, or that cover the same set of
examples as the original rule, and so will be rejected for having the same likelihood
but a worse penalty. However, just to illustrate the process, attempting to add in the
height(Y) =? predicate will result in the creation of three new rules with height(Y) = n
in the context, one for each n  [0, 1, 2]. These rules would be added to the rule set
all at once.
 AddLits selects each rule r  R 2n times, where n is the number of predicate-based
literals that are absent from the rules context and deictic references, and the 2 reflects the fact that each literal may be considered in its positive or negative form. It
constructs a new rule for each literal by inserting that literal into the earliest place
in the rule in which its variables are all well-defined: if the literal contains no deictic
variables, this will be the context, otherwise this will be the restriction of the last
deictic variable mentioned in the literal. (So, if V1 and V2 are deictic variables and V1
appears first, on(V1 , V2 ) would be inserted into the restriction of V2 .) The resulting
rule is then integrated into the rule set.
The list of literals that may be added to the pickup rule is much as for SplitOnLits, only
without height(Y) =?. Again, this process will not lead to anything very interesting
333

fiPasula, Zettlemoyer, & Pack Kaelbling

in our example, for the same reason. Just as an illustration, inhand(Y) would be
chosen twice, once as inhand(Y), and added to the context in each case. Since the
context already contains inhand-nil, adding inhand(Y) will be redundant, and adding
inhand(Y) will produce a contradiction, so neither rule will be seriously considered.
 AddRefs is an operator used only when deictic references are permitted. It selects
each rule r  R n times, where n is the number of literals that can be constructed
using the available predicates, the variables in r, and a new variable v. In each case, it
creates a new deictic reference for v, using the current literal to define the restriction,
and adds this deictic reference to the antecendent of r to construct a new rule, which
will then be integrated into the rule set.
Supposing V is the new variable, the list of literals that would be constructed with the
pickup rule consists of inhand(V), clear(V), on(V, X), on(X, V), table(V), on(V, Y),
on(Y, V), on(V, V), and all possible applications of above and topstack (which will
mirror those for on.) They will be used to create deictic references like V : table(V).
(A useful reference here, as it allows the rule to describe the falls on table outcomes
explicitly; this operator is very likely to be accepted at some point in the search.)
 RaiseConstants is an operator used only when constants are permitted. It selects each
rule r  R n times, where n is the number of constants among the arguments of rs
action. For each constant c, it constructs a new rule by creating a new variable and
replacing every occurrence of c with it. It then integrates this new rule into the rule
set.
 SplitVariables is an operator used only when constants are permitted. It selects each
rule r  R n times, where n is the number of variables among the arguments of rs
action. For each variable v, it goes through the examples covered by the rule r and
collects the constants v binds to. Then, it creates a rule for each of these constants by
replacing every occurrence of v with that constant. The rules corresponding to one
variable v are combined in the set N and integrated into the old rule set together.
We have found that all of these operators are consistently used during learning. While
this set of operators is heuristic, it is complete in the sense that every rule set can be
constructed from the initial rule setalthough, of course, there is no guarantee that the
scoring metric will lead the greedy search to the global maximum.
LearnRuless search strategy has one large drawback; the set of learned rules is only
guaranteed to be proper on the training set and not on testing data. New test examples
could be covered by more than one rule. When this happens, we employ an alternative
rule-selection semantics, and return the default rule to model the situation. In this way,
we are essentially saying that we dont know what will happen. However, this is not a
significant problem; the problematic test examples can always be added to a future training
set and used to learn better models. Given a sufficiently large training set, these failures
should be rare.

334

fiLearning Symbolic Models of Stochastic World Dynamics

e1:

B2

B2
B0

puton(B1)

B2
B0

puton(B1)

B0
B1

B1

e3:

r1 :

r2 :

puton(X)
:
(
)
Y : inhand(Y)
T : table(T)
empty
 context

 0.33 : on(Y, T)
0.33 : on(Y, X)


 0.34 : noise

r3 :

puton(X)
:
n
o
Y : inhand(Y)
clear(X)
n
 1.0 : on(Y, X)

B1

B1

e2:

B0

puton(X)
:
(
)
Y : inhand(Y)
Z : on(Z, X)
empty
( context
0.5 : on(Y, Z)

0.5 : noise

B2

B2
puton(B1)

B0 B1

B2
B0 B1

Figure 6: Three training examples in a three blocks world. Each example is paired with an initial
rule that ExplainExamples might create to model it. In each example, the agent is trying
to put block B2 onto block B1.

An Example of Rule Set Learning

As an example, consider how LearnRuleSet might learn a set of rules to model the three
training examples in Figure 6, given the settings of the complexity penalty and noise bound
later used in our experiments:  = 0.5 and pmin = 0.0000001. This pmin is very low for the
three-block domain, since it only has 25 different states, but we use it for consistency.
At initialization, the rule set contains only the default rule; all the changes that occur
in these examples are modeled as noise. Since all examples include change, the default rule
will have a noise probability of 1.0. We now describe the path the greedy search takes.
During the first round of search the ExplainExamples operator suggests adding in new
rules to describe these examples. In general, ExplainExamples tries to construct rules that
are compact, that cover many examples, and that assign a relatively high probability to
each covered example. (The latter means that noise outcomes are to be avoided whenever
possible.) One reasonable set of rules to be suggested is shown on the right-hand side
of Figure 6. Notice that r3 is deterministic, and so high-probability and relatively compact:
e3 has a unique initial state, and ExplainExamples can take advantage of this. Meanwhile,
335

fiPasula, Zettlemoyer, & Pack Kaelbling

e1 and e2 have the same starting state, and so the rules explaining them must cover each
others examples. Thus, noise outcomes are unavoidable in both rules, since they lack the
necessary deictic references. (Deictic variables are created only to describe the objects
whose state changes in the example being explained.)
Now, consider adding one of these rules. There is no guarantee that doing so will
constitute an improvement, since a very high complexity penalty  would make any rule
look bad, while a high pmin would make the default rule look good. To determine what
the best move is, the algorithm compares the scores of the rule sets containing each of the
proposed rules to the score of the initial rule set containing only the default rule. Let us
calculate these scores for the example, starting with the rule set consisting of the rule r1 ,
which covers e1 and e2 , and the default rule rd , which covers the remaining example, and
which therefore has a noise probability of 1.0. We will use Equation 5, and let a rules
complexity be the number of literals in its body: so, in the case of r1 , three. We get:
S(r1 , rd ) =

X

log(P (s0 |s, a, r(s,a) ))  

(s,a,s0 )E

X

P EN (r)

rR

= log(0.5 + 0.5  pmin ) + log(0.5  pmin ) + log(pmin )  P EN (r1 )  P EN (rd )
= log(0.50000005) + log(0.00000005) + log(0.0000001)  0.5  3  0.5  0
= 0.301  7.301  7  1.5
= 16.101
So, the rule set containing r1 has a score of 16.101. Similar calculations show that the
rule sets containing r2 and r3 have scores of 10.443 and 15.5 respectively. Since the
initial rule set has a score of 21, all of these new rule sets are improvements, but the one
containing r2 is best, and will be picked by the greedy search. The new rule set is now:

	
puton(X) : Y : inhand(Y), T : table(T)
empty context

 0.33 : on(Y, T)
0.33 : on(Y, X)


0.34 : noise
default 
rule:
1.0 : no change

0.0 : noise

Notice that all the training examples are covered by a non-default rule. In this situation,
the default rule does not cover any examples and has no probability assigned to the noise
outcome.
At the next step, the search has to decide between altering an existing rule, and introducing another rule to describe an example currently covered by the default rule. Since the
default rule covers no examples, altering the single rule in the rule set is the only option.
The operators most likely to score highly are those that can get rid of that noise outcome,
which is there because the rule has no means of referring to the block above X in e1 . The
appropriate operator is therefore AddRefs, which can introduce a new deictic reference describing that block. Of course, this increases the size of the rule, and so its complexity,
336

fiLearning Symbolic Models of Stochastic World Dynamics

and in addition it means that the rule no longer applies to e3 , leaving that example to
be handled by the default rule. However, the new rule set raises the probabilities of all
the examples enough to compensate for the increase in complexity, and so it ends up with
a score of 10.102, which is a clear improvement on 10.443. This is the highest score
obtainable at this step, so the algorithm alters the rule set to get:
puton(X) :



Y : inhand(Y), T : table(T), Z : on(Z, X)

	

empty context

0.5 : on(Y, Z)

0.5 : on(Y, T)
default 
rule:
0.0 : no change

1.0 : noise

Now that the default rule covers e3 , ExplainExamples has something to work with again.
Adding in r3 will get rid of all noise, and yield a much improved score of 4.602. Again,
this is the biggest improvement that can be made, and the rule set becomes:
puton(X) :



Y : inhand(Y), T : table(T), Z : on(Z, X)

	

empty context

0.5 : on(Y, Z)

0.5 : on(Y, T)

	
puton(X) : Y : inhand(Y)
clear(X)

 1.0 : on(Y, X)
default 
rule:
1.0 : no change

0.0 : noise

Note that this rule could not have been added earlier because e3 was also covered by the
first rule added, r2 , before it was specialized. Thus, adding r3 to the rule set containing r2
would have knocked r2 out, and caused examples e1 and e2 to be explained as noise by the
default rule, which would have reduced the overall score. (It is, however, possible for a rule
to knock out another and yet improve the score: it just requires a more complicated set of
examples.)
Learning continues with more search. Attempts to apply the rule-altering operators to
the current rules will either make them bigger without changing the likelihood, or will lead
to the creation of some noise outcomes. Dropping either rule will add noise probability to
the default rule and lower the score. Since there are no extra examples to be explained,
no operator can improve the score, and the search stops at this rule set. It seems like a
reasonable rule set for this domain: one rule covers what happens when we try to puton a
clear block, and one describes when we try to puton a block that has another block on it.
Ideally, we would like the first rule to generalize to blocks that have something above them,
instead of just on, but to notice that we would need examples containing higher stacks.
337

fiPasula, Zettlemoyer, & Pack Kaelbling

5.1.6 Different Versions of the Algorithm
By making small variations in the LearnRuleSet algorithm, we can learn different types of
rule sets. This will be important for evaluating the algorithm.
To explore the effects of constants in the rules, we will evaluate three different versions
of rule learning: propositional, relational, and deictic. For propositional rule learning, ExplainExamples creates initial trimmed rules with constants but never introduces variables.
None of the search operators that introduce variables are used. Thus, the learned rules
are guaranteed to be propositionalthey cannot generalize across the identities of specific
objects. For relational rule learning, variables are allowed in rule action arguments but the
search operators are not allowed to introduce deictic references. ExplainExamples creates
rules with constants that name objects, as long as those constants do not already have a
variable in the action argument list mapped to them. Finally, for deictic rule learning, no
constants are allowed. We will see that deictic learning provides a strong bias that can
improve generalization.
To demonstrate that the addition of noise and deictic references can result in better
rules, we will learn action models that do not have these enhancements. Again, this can
be done by changing the algorithm in minor ways. To disallow noise, we set the rule noise
probability to zero, which means that we must then constrain outcome sets to contain
an outcome for every example where change was observed; rules that cannot express all
the changes are abandoned. To disallow deictic references, we disable the operators that
introduce them, and have ExplainExamples create an empty deictic reference set.
5.2 Learning Concepts
The contexts and deictic references of NDRs can make use of concept predicates and functions as well as primitive ones. These concepts can be specified by hand, or learned using
a rather simple algorithm, LearnConcepts, which uses LearnRuleSet as a subprocedure for
testing concept usefulness. The algorithm works by constructing increasingly complex concepts, and then running LearnRuleSet and checking what concepts appear in the learned
rules. The first set is created by applying the operators in Figure 7 to literals built with
the original language. Subsequent sets of concepts are constructed using the literals that
proved useful on the latest run; concepts that have been tried before, or that are always
true or always false across all examples, are discarded. The search ends when none of the
new concepts prove useful.
As an example, consider the predicate topstack in a simple blocks world, which could be
discovered as follows. In the first round of learning, the literal on(X1 , X2 ) is used to define
the new predicate n(Y1 , Y2 ) := on (Y1 , Y2 ), which is true when Y1 is stacked above Y2 .
Assuming this new predicate appears in the learned rules, it can then be used in the second
round of learning, to define, among others, m(Z1 , Z2 ) := n(Z1 , Z2 )  clear(Z1 ). By ensuring
that Z1 is clear, this predicate will be true only when Z1 is the highest block in the stack
containing Z2 . This notion of topstack can be used to determining what will happen with
the gripper tries to pick up Z2 . Because it descends from above, it will likely grasp the
block on the top of the stack instead.
Since our concept language is quite rich, overfitting (e.g., by learning concepts that can
be used to identify individual examples) can be a serious problem. We handle this in the
338

fiLearning Symbolic Models of Stochastic World Dynamics

p(X)  n := QY.p(Y )
p(X1 , X2 )  n(Y2 ) := QY1 .p(Y1 , Y2 )
p(X1 , X2 )  n(Y1 ) := QY2 .p(Y1 , Y2 )
p(X1 , X2 )  n(Y1 , Y2 ) := p (Y1 , Y2 )
p(X1 , X2 )  n(Y1 , Y2 ) := p+ (Y1 , Y2 )
p1 (X1 ), p2 (X2 )  n(Y1 ) := p1 (Y1 )  p2 (Y1 )
p1 (X1 ), p2 (X2 , X3 )  n(Y1 , Y2 ) := p1 (Y1 )  p2 (Y1 , Y2 )
p1 (X1 ), p2 (X2 , X3 )  n(Y1 , Y2 ) := p1 (Y1 )  p2 (Y2 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 )  n(Y1 , Y2 ) := p1 (Y1 , Y2 )  p2 (Y1 , Y2 )
p1 (X1 , X2 ), p2 (X3 , X4 )  n(Y1 , Y2 ) := p1 (Y1 , Y2 )  p2 (Y2 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 )  n(Y1 , Y2 ) := p1 (Y1 , Y2 )  p2 (Y1 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 )  n(Y1 , Y2 ) := p1 (Y1 , Y2 )  p2 (Y2 , Y2 )
f (X) = c  n() := #Y.f (Y ) = c
f (X)  c  n() := #Y.f (Y )  c
f (X)  c  n() := #Y.f (Y )  c
Figure 7: Operators used to invent a new predicate n. Each operator takes as input one or more
literals, listed on the left. The ps represent old predicates; f represents an old function;
Q can refer to  or ; and c is a numerical constant. Each operator takes a literal and
returns a concept definition. These operators are applied to all of the literals used in
rules in a rule set to create new predicates.

expected way: by introducing a penalty term, 0 c(R), to create a new scoring metric
S 0 (R) = S(R)  0 c(R)
where c(R) is the number of distinct concepts used in the rule set R and 0 is a scaling
parameter. This new metric S 0 is now used by LearnRuleSet; it avoids overfitting by favoring
rule sets that use fewer derived predicates. (Note that the fact that S 0 cannot be factored by
rule, as S was, does not matter, since the factoring was used only by InduceOutcomes and
LearnParameters, neither of which can change the number of concepts used in the relevant
rule: outcomes contain only primitive predicates.)
5.3 Discussion
The rule set learning challenge addressed in this section is complicated by the need to learn
the structure of the rules, the numeric parameters associated with the outcome distributions,
and the definitions of derived predicates for the modeling language. The LearnConcepts
339

fiPasula, Zettlemoyer, & Pack Kaelbling

algorithm is conceptually simple, and performs this simultaneous learning effectively, as we
will see in the experiments in Section 7.2.
The large number of possible search operators might cause concern about the overall
computational complexity of the LearnRuleSet algorithm. Although this algorithm is expensive, the set of search operators were designed to control this complexity by attempting
keep the number of rules in the current set as small as possible.
At each step of search, the number of new rule sets that are considered depends on the
current set of rules. The ExplainExamples operator creates d new rule sets, where d is the
number of examples covered by the default rule. Since the search starts with a rule set
containing only the default rule, d is initially equal to the number of training examples.
However, ExplainExamples was designed to introduce rules that cover many examples, and
in practice d grows small quickly. All of the other operators can create O(rm) new rule sets,
where r is the number of rules in the current set and m depends on the specific operator.
For example, m could be the number of literals that can be dropped from the context of a
rule by the DropLits operator. Although m can be large, r stays small in practice because
the search starts with only the default rule and the complexity penalty favors small rule
sets.
Because we ensure that the score increases at each search step, the algorithm is guaranteed to converge to a (usually local) optimum. There is not, however, any guarantee about
how quickly it will get there. In practice, we found that the algorithm converged quickly
in the test domains. The LearnRuleSet algorithm never took more that 50 steps and the
LearnConcepts outer loop never cycled more than 5 times. The entire algorithm never took
more than six hours to run on a single processor, although significant effort was made to
cache intermediate computations in the final implementation.
In spite of this, we realize that, as we scale up to more complex domains, this approach
will eventually become prohibitively expensive. We plan to handle this problem by developing new algorithms that learn concepts, rules, and rule parameters in an online manner,
with more directed search operators. However, we leave this more complex approach to
future work.

6. Planning
Some of the experiments in Section 7.2 involve learning models of complex actions where
true models of the dynamics, at the level of relational rules, are not available for evaluation.
Instead, the learned models are evaluated by planning and executing actions. There are
many possible ways to plan. In this work, we explore MDP planning.
A MDP (Puterman, 1999) is a 4-tuple (S, A, T, R). S is the set of possible states, A
is a set of possible actions, and T is a distribution that encodes the transition dynamics of
the world, T (s0 |s, a). Finally, R is a reward signal that maps every state to a real value.
A policy or plan  is a (possibly stochastic) mapping from states to actions. The expected
amount of reward that can be achieved by executing  starting in s is called the value of s
P
i
and defined as V (s) = E[ 
i=0  R(si )|], where the si are the states that can be reached
by  at time i. The discount factor 0   < 1 favors more immediate rewards. The goal
of MDP planning is to find the policy   that will achieve the most reward over time. This
340

fiLearning Symbolic Models of Stochastic World Dynamics

optimal policy can be found by solving the set of Bellman equations,
s, V (s) = R(s) + 

X

T (s0 |s, (a))V (s0 ).

(8)

s0 S

In our application, the action set A and the state set S are defined by the world we are
modeling. A rule set R defines the transition model T and the reward function R is defined
by hand.
Because we will be planning in large domains, it will be difficult to solve the Bellman
equations exactly. As an approximation, we implemented a simple planner based on the
sparse sampling algorithm (Kearns, Mansour, & Ng, 2002). Given a state s, it creates a tree
of states (of predefined depth and branching factor) by sampling forward using a transition
model, computes the value of each node using the Bellman equation, and selects the action
that has the highest value.
We adapt the algorithm to handle noisy outcomes, which do not predict the next state,
by estimating the value of the unknown next state as a fraction of the value of staying in
the same state: i.e., we sample forward as if we had stayed in the same state and then scale
down the value we obtain. Our scaling factor was 0.75, and our depth and branching factor
were both four.
This scaling method is only a guess at what the value of the unknown next state might
be; because noisy rules are partial models, there is no way to compute the value explicitly.
In the future, we would like to explore methods that learn to associate values with noise
outcomes. For example, the value of the outcome where a tower of blocks falls over is
different if the goal is to build a tall stack of blocks than if the goal is to put all of the
blocks on the table.
While this algorithm will not solve hard combinatorial planning problems, it will allow
us to choose actions that maximize relatively simple reward functions. As we will see in
the next section, this is enough to distinguish good models from poor ones. Moreover, the
development of first-order planning techniques is an active field of research (AIPS, 2006).

7. Evaluation
In this section, we demonstrate that the rule learning algorithm is robust on a variety of lownoise domains, and then show that it works in our intrinsically noisy simulated blocks world
domain. We begin by describing our test domains, and then report a series of experiments.
7.1 Domains
The experiments we performed involve learning rules for the domains which are briefly
described in the following sections.
7.1.1 Slippery Gripper
The slippery gripper domain, inspired by the work of Draper et al. (1994), is an abstract,
symbolic blocks world with a simulated robotic arm, which can be used to move the blocks
around on a table, and a nozzle, which can be used to paint the blocks. Painting a block
might cause the gripper to become wet, which makes it more likely that it will fail to
manipulate the blocks successfully; fortunately, a wet gripper can be dried.
341

fiPasula, Zettlemoyer, & Pack Kaelbling

pickup(X, Y ) : on(X, Y ), clear(X),
inhand-nil, block(X), block(Y ), wet,



pickup(X, Y ) : on(X, Y ), clear(X),
inhand-nil, block(X), block(Y ), wet




inhand(X), clear(X), inhand-nil,

 .7 :
on(X, Y ), clear(Y )


 .2 : on(X, TABLE), on(X, Y )
.1 : no change

inhand(X), clear(X), inhand-nil,

 .33 :
on(X, Y ), clear(Y )


 .33 : on(X, TABLE), on(X, Y )
.34 : no change

pickup(X, Y ) : on(X, Y ), clear(X),
inhand-nil, block(X), table(Y ), wet
pickup(X, Y ) : on(X, Y ), clear(X),
inhand-nil, block(X), table(Y ), wet

(


(


inhand(X), clear(X), inhand-nil,
on(X, Y )
.5 : no change

.5 :

inhand(X), clear(X), inhand-nil,
on(X, Y )
.2 : no change

.8 :


inhand-nil, clear(Y), inhand(X),


.7 :


on(X, Y), clear(X)

puton(X, Y) : clear(Y), inhand(X),
on(X, TABLE), clear(X), inhand-nil,

block(Y)

 .2 : inhand(X)


.1 : no change

(
puton(X, TABLE) : inhand(X) 

(

.6 : painted(X)
.1 : painted(X), wet
.3 : no change



.9 : wet
.1 : no change

paint(X) : block(X) 

dry : no context 

on(X, TABLE), clear(X), inhand-nil,
inhand(X)
.2 : no change
.8 :

Figure 8: Eight relational planning rules that model the slippery gripper domain.

Figure 8 shows the set of rules that model this domain. Individual states represent
world objects as intrinsic constants and experimental data is generated by sampling from
the rules. In Section 7.2.1, we will explore how the learning algorithms of Section 5 compare
as the number of training examples is scaled in a single complex world.
7.1.2 Trucks and Drivers
Trucks and drivers is a logistics domain, adapted from the 2002 AIPS international planning
competition (AIPS, 2002). There are four types of constants: trucks, drivers, locations,
and objects. Trucks, drivers and objects can all be at any of the locations. The locations
are connected with paths and links. Drivers can board trucks, exit trucks, and drive trucks
between locations that are linked. Drivers can also walk, without a truck, between locations
that are connected by paths. Finally, objects can be loaded and unloaded from trucks.
A set of rules is shown in Figure 9. Most of the actions are simple rules which succeed or
fail to change the world. However, the walk action has an interesting twist. When drivers
try to walk from one location to another, they succeed most of the time, but some of the
342

fiLearning Symbolic Models of Stochastic World Dynamics

load(O, T, L) :

at(T, L), at(O, L)



.9 : at(O, L), in(O, T )
.1 : no change

unload(O, T, L) :

in(O, T ), at(T, L)



.9 : at(O, L), in(O, T )
.1 : no change

board(D, T, L) :

at(T, L), at(D, L), empty(T )



.9 : at(D, L), driving(D, T ), empty(T )
.1 : no change

disembark(D, T, L) :

at(T, L), driving(D, T )



.9 : driving(D, T ), at(D, L), empty(T )
.1 : no change

drive(T, F L, T L, D) :

driving(D, T ), at(T, F L), link(F L, T L)



.9 : at(T, T L), at(T, F L)
.1 : no change


 .9 : at(D, T L), at(D, F L)

walk(D, F L, T L) :

at(D, F L), path(F L, T L)
 .1 : pick X s.t. path(F L, X)
at(D, X), at(D, F L)

Figure 9: Six rules that encode the world dynamics for the trucks and drivers domain.
time they arrive at a randomly chosen location that is connected by some path to their
origin location.
The representation presented here cannot encode this action efficiently. The best rule
set has a rule for each origin location, with outcomes for every location that the origin is
linked to. Extending the representation to allow actions like walk to be represented as a
single rule is an interesting area for future work.
Like in the slippery gripper domain, individual states represent world objects as intrinsic
constants and experimental data is generated by sampling from the rules. The trucks and
drivers dynamics is more difficult to learn but, as we will see in Section 7.2.1, can be learned
with enough training data.
7.1.3 Simulated Blocks World
To validate the rule extensions in this paper, Section 7.2 presents experiments in a rigid
body, simulated physics blocks world. This section describes the logical interface to the
simulated world. A description of the extra complexities inherent in learning the dynamics
of this world was presented in Section 1.
We now define the interface between the symbolic representation that we use to describe
action dynamics and a physical domain such as the simulated blocks world. The perceptual
system produces states that contain skolem constants. The logical language includes the
binary predicate on(X, Y), which is defined as X exerts a downward force on Y and obtained
by querying the internal state of the simulator, and unary typing predicates table and block.
The actuation system translates actions into sequences of motor commands in the simulator.
Actions always execute, regardless of the state of the world. We define two actions; both
have parameters that allow the agent to specify which objects it intends to manipulate. The
pickup(X) action centers the gripper above X, lowers it until it hits something, grasps, and
raises the gripper. Analogously, the puton(X) action centers the gripper above X, lowers
until it encounters pressure, opens it, and raises it.
343

fiPasula, Zettlemoyer, & Pack Kaelbling

By using a simulator we are sidestepping the difficult pixels-to-predicates problem that
occurs whenever an agent has to map domain observations into an internal representation.
Primitive predicates defined in terms of the internal state of the simulation are simpler and
cleaner than observations of the real world would be. They also make the domain completely
observable: a prerequisite for our learning and planning algorithms. Choosing the set of
predicates to observe is important. It can make the rule learning problem very easy or very
hard, and the difficulty of making this choice is magnified in richer settings. The limited
language described above balances these extremes by providing on, which would be difficult
to derive by other means, but not providing predicates such as inhand and clear, that can
be learned.
7.2 Experiments
This section describes two sets of experiments. First, we compare the learning of deictic,
relational, and propositional rules on the slippery gripper and trucks and drivers data. These
domains are modeled by planning rules, contain intrinsic constants, and are not noisy, and
thus allow us to explore the effect of deictic references and constants in the rules directly.
Then, we describe a set of experiments that learns rules to model data from the simulated
blocks world. This data is inherently noisy and contains skolem constants. As a result, we
focus on evaluating the full algorithm by performing ablation studies that demonstrate that
deictic references, noise outcomes, and concepts are all required for effective learning.
All of the experiments use examples, (s, a, s0 )  E, generated by randomly constructing
a state s, randomly picking the arguments of the action a, and then executing the action in
the state to generate s0 . The distribution used to construct s is biased to guarantee that,
in approximately half of the examples, a has a chance to change the state. This method
of data generation is designed to ensure that the learning algorithms will always have data
which is representative of the entire model that they should learn. Thus, these experiments
ignore the problems an agent would face if it had to generate data by exploring the world.
7.2.1 Learning Rule Sets with No Noise
When we know the model used to generate the data, we can evaluate our model with respect
to a set of similarly generated test examples E by calculating the average variational distance
between the true model P and the estimate P ,
V D(P, P ) =

1 X
|P (E)  P (E)| .
|E| EE

Variational distance is a suitable measure because it clearly favors similar distributions,
and yet is well-defined when a zero probability event is observed. (As can happen when
a non-noisy rule is learned from sparse data and does not have as many outcomes as it
should.)
These comparisons are performed for four actions. The first two, paint and pickup,
are from the slippery gripper domain, while the second two, drive and walk, are from the
trucks and drivers domain. Each action presents different challenges for learning. Paint
is a simple action where more than one outcome can lead to the same successor state (as
described in Section 4.1). Pickup is a complex action that must be represented by more
344

fiLearning Symbolic Models of Stochastic World Dynamics

The Paint Action
0.3
0.25

The Pickup Action
0.35

Propositional
Relational
Deictic

Variational Distance

Variational Distance

0.35

0.2
0.15
0.1
0.05
0

0.3
0.25
0.2
0.15
0.1
0.05
0

100 200 300 400 500 600 700 800 9001000
Training set size

100 200 300 400 500 600 700 800 9001000
Training set size

The Walk Action
0.3
0.25

The Drive Action
0.2

Propositional
Relational
Deictic

Variational Distance

Variational Distance

0.35

Propositional
Relational
Deictic

0.2
0.15
0.1
0.05
0

0.15

Propositional
Relational
Deictic

0.1
0.05
0

100 200 300 400 500 600 700 800 900
Training set size

100 200 300 400 500 600 700 800 900
Training set size

Figure 10: Variational distance as a function of the number of training examples for propositional, relational, and deictic rules. The results are averaged over ten trials of
the experiment. The test set size was 400 examples.

than one planning rule. Drive is a simple action that has four arguments. Finally, walk
is a complicated action uses the path connectivity of the world in its noise model for lost
pedestrians. The slippery gripper actions were performed in a world with four blocks. The
trucks and driver actions were performed in a world with two trucks, two drivers, two
objects, and four locations.
We compare three versions of the algorithm: deictic, which includes the full rules language and does not allow constants; relational, which allows variables and constants but
no deictic references; and propositional, which has constants but no variables. Figure 10
shows the results. The relational learning consistently outperforms propositional learning;
this implies that the variable abstractions are useful. In all cases except for the walk action,
the deictic learner outperforms the relational learner. This result implies that forcing the
rules to only contain variables is preventing overfitting and learning better models. The
results on the walk action are more interesting. Here, the deictic learner cannot actually
represent the optimal rule; it requires a noise model that is too complex. The deictic learner
quickly learns the best rule it can, but the relational and propositional learners eventually
345

fiPasula, Zettlemoyer, & Pack Kaelbling

Learning in the Simulated Blocksworld
18

learned concepts
hand-engineered concepts
without noise outcomes
with a restricted language

Total Reward

16
14
12
10
8
6
200

300

400

500
600
700
Training set size

800

900

1000

Figure 11: The performance of various action model variants as a function of the number of training
examples. All data points were averaged over five planning trials for each of the three
rule sets learned from different training data sets. For comparison, the average reward
for performing no actions is 9.2, and the reward obtained when a human directed the
gripper averaged 16.2.

learn better rule sets because they can use constants to more accurately model the walkers
moving to random locations.
In these experiments, we see that variable abstraction helps to learn from less data, and
that deictic rules, which abstract the most aggressively, perform the best, as long as they
can represent the model to be learned. In the next section, we will only consider deictic
rules, since we will be working in a domain with simulated perception that does not have
access to objects identities and names them using skolem constants.
7.2.2 Learning in the Blocks World Simulator
Our final experiment demonstrates that both noise outcomes and complicated concepts are
necessary to learn good action models for the blocks world simulator.
When the true model is not known, we evaluate the learned model by using it to plan
and estimating the average reward it gets. The reward function we used in simulated blocks
world was the average height of the blocks in the world, and the breadth and depth of the
search for the sampling planner were both four. During learning, we set  to 0.5 and pmin
to 0.0000001.
We tested four action model variants, varying the training set size; the results are
shown in Figure 11. The curve labeled learned concepts represents the full algorithm as
presented in this paper. Its performance approaches that obtained by a human expert,
and is comparable to that of the algorithm labeled hand-engineered concepts that did not
346

fiLearning Symbolic Models of Stochastic World Dynamics

do concept learning, but was, instead, provided with hand-coded versions of the concepts
clear, inhand, inhand-nil, above, topstack, and height. The concept learner discovered all of
these, as well as other useful predicates, e.g., p(X, Y) := clear(Y)  on(Y, X), which we will
call onclear. This could be why its action models outperformed the hand-engineered ones
slightly on small training sets. In domains less well-studied than the blocks world, it might
be less obvious what the useful concepts are; the concept-discovery technique presented here
should prove helpful.
The remaining two model variants obtained rewards comparable to the reward for doing
nothing at all. (The planner did attempt to act during these experiments, it just did a
poor job.) In one variant, we used the same full set of predefined concepts but the rules
could not have noise outcomes. The requirement that they explain every action effect led
to significant overfitting and a decrease in performance. The other rule set was given the
traditional blocks world language, which does not include above, topstack, or height, and
allowed to learn rules with noise outcomes. We also tried a full-language variant where noise
outcomes were allowed, but deictic references were not: the resulting rule sets contained only
a few very noisy rules, and the planner did not attempt to act at all. The poor performance
of these ablated versions of our representation shows that all three of our extensions are
essential for modeling the simulated blocks world domain.
A human agent commanding the gripper to solve the same problem received an average
total reward of 16.2, which was below the theoretical maximum due to unexpected action
outcomes. Thus, the ND rules are performing at near-human levels, suggesting that this
representation is a reasonable one for this problem. It also suggests that our planning
approximations and learning bounds are not limiting performance. Traditional rules, which
face the challenge of modeling all the transitions seen in the data, have a much larger
hypothesis space to consider while learning; it is not surprising that they generalize poorly
and are consistently out-performed by the NDRs.
Informally, we can also report that NDR algorithms execute significantly faster than the
traditional ones. On one standard desktop PC, learning NDRs takes minutes while learning
traditional rules can take hours. Because noisy deictic action models are generally more
compact than traditional ones (they contain fewer rules with fewer outcomes) planning is
much faster as well.
To get a better feel for the types of rules learned, here are two interesting rules produced
by the full algorithm.

pickup(X) :

Y : onclear(X, Y), Z : on(Y, Z),
T : table(T)



inhand-nil, size(X) < 2

 .80 : on(Y, Z)
.10 : on(X, Y)


.10 : on(X, Y), on(Y, T), on(Y, Z)

This rule applies when the empty gripper is asked to pick up a small block X that sits on
top of another block Y. The gripper grabs both with a high probability.

347

fiPasula, Zettlemoyer, & Pack Kaelbling


puton(X) :

Y : topstack(Y, X), Z : inhand(Z),
T : table(T)

size(Y) < 2


 .62 :

.12 :

.04 :



.22 :



on(Z, Y)
on(Z, T)
on(Z, T), on(Y, T), on(Y, X)
noise

This rule applies when the gripper is asked to put its contents, Z, on a block X which is
inside a stack topped by a small block Y. Because placing things on a small block is chancy,
there is a reasonable probability that Z will fall to the table, and a small probability that
Y will follow.

8. Discussion
In this paper, we developed a probabilistic action model representation that is rich enough
to be used to learn models for planning in a physically simulated blocks world. This is a
first step towards defining representations and algorithms that will enable learning in more
complex worlds.
8.1 Related Work
The problem of learning deterministic action models is well studied. Most work in this
area (Shen & Simon, 1989; Gil, 1993, 1994; Wang, 1995) has focused on incrementally
learning planning operators by interacting with simulated worlds. However, all of this work
assumes that the learned models are completely deterministic.
Oates and Cohen (1996) did the earliest work on learning probabilistic planning operators. Their rules are factored and can apply in parallel. However, their representation
is strictly propositional, and allows each rule to contain only a single outcome. In our
previous work, we developed algorithms for learning probabilistic relational planning operators (Pasula, Zettlemoyer, & Kaelbling, 2004). Unfortunately, neither of these probabilistic
algorithms are robust enough to learn in complex, noisy environments like the simulated
blocks world.
One previous system that comes close to this goal is the TRAIL learner (Benson, 1996).
TRAIL learns an extended version of Horn clauses in noisy environments by applying inductive logic programming (ILP) learning techniques that are robust to noise. TRAIL
introduced deictic references that name objects based on their functional relationships to
arguments of the actions. Our deictic references, with their exists-unique quantification
semantics, are a generalization of Bensons original work. Moreover, TRAIL models continuous actions and real-valued fluents, which allows it to represent some of the most complex
models to date, including the knowledge required to pilot a realistic flight simulator. However, the rules that TRAIL learns are in a limited probabilistic representation that can not
represent all possible transition distributions. TRAIL also does not include any mechanisms
for learning new predicates.
348

fiLearning Symbolic Models of Stochastic World Dynamics

All of this work on action model learning has used different versions of greedy search for
rule structure learning, which is closely related to and inspired by the learning with version
spaces of Mitchell (1982) and later ILP work (Lavrac & Dzeroski, 1994). In this paper,
we also explore, for the first time, a new way of moving through the space of rule sets by
using the noise rule as an initial rule set. We have found that this approach works well in
practice, avoiding the need for a hand-selected initial rule set and allowing our algorithm
to learn in significantly more complex environments.
As far as we know, no work on learning action models has explored learning concepts.
In the ILP literature, recent work (Assche, Vens, Blockeel, & Dzeroski, 2004) has shown
that adding concept learning to decision tree learning algorithms improves classification
performance.
Outside of action learning, there exists much related research on learning probabilistic
models with relational or logical structure. A complete discussion is beyond the scope
of this paper, but we present a few highlights. Some work learns representations that are
relational extension of Bayesian networks. For a comprehensive example, see work by Getoor
(2001). Other work extends research in ILP by incorporating probabilistic dependencies.
For example, see the wide range of techniques presented by Kersting (2006). Additionally,
there is recent work on learning Markov logic networks (Richardson & Domingos, 2006; Kok
& Domingos, 2005), which are log-linear models with features that are defined by first-order
logical formulae. The action models and action model learning algorithms in this paper are
designed to represent action effects, a special case of the more general approaches listed
above. As we have discussed in Section 2, by tailoring the representation to match the
model to be learnt, we simplify learning.
Finally, let us consider work related to the NDR action model representation. The most
relevant approach is PPDDL, a representation language for probabilistic planning operators
and problem domains (Younes & Littman, 2004). The NDR representation was partially
inspired by PPDDL operators but includes restrictions to make it easier to learn and extensions, such as noise outcomes, that are required to effectively model the simulated blocks
world. In the future, the algorithms in this paper could be extended to learn full PPDDL
rules. Also, PPDDL planning algorithms (for examples, see the papers from recent planning
competitions) could be adapted to improve the simple planning presented in Section 6. In a
more general sense, NDRs are related to all the other probabilistic relational representations
that are designed to model dependencies across time. For examples, see work on relational
dynamic Bayesian networks (Sanghai, Domingos, & Weld, 2005), which are a specialization of PRMs, and logical hidden Markov models (Kersting, Raedt, & Raiko, 2006), which
come from the ILP research tradition. These approaches make a different set of modeling
assumptions that are not as closely tied to the planning representations that NDR models
extend.
8.2 Future and Ongoing Work
There remains much to be done in the context of learning probabilistic planning rules.
First of all, it is very likely that when this work is applied to additional domains (such as
more realistic robotic applications or dialogue systems) the representation will need to be
adapted, and the search operators adjusted accordingly. Some possible changes mentioned
349

fiPasula, Zettlemoyer, & Pack Kaelbling

in this article include allowing the rules to apply in parallel, so different rules could apply to
different aspects of the state, and extending the outcomes to include quantifiers, so actions
like walk, from the trucks and drivers domain in Section 7.1.2, could be described using a
single rule. A more significant change we intend to pursue is expanding this approach to
handle partial observability, possibly by incorporating some of the techniques from work on
deterministic learning (Amir, 2005). We also hope to make some changes that will make
using the rules easier, such as associating values with the noise outcomes to help a planner
decide whether they should be avoided.
A second research direction involves the development of new algorithms that learn probabilistic operators in an incremental, online manner, similar to the learning setup in the
deterministic case (Shen & Simon, 1989; Gil, 1994; Wang, 1995). This has the potential to
scale our approach to larger domains, and will make it applicable even in situations where it
is difficult to obtain a set of training examples that contains a reasonable sampling of worlds
that are likely to be relevant to the agent. This line of work will require the development
of techniques for effectively exploring the world while learning a model, much as is done in
reinforcement learning. In the longer term, we would like these online algorithms to learn
not only operators and concept predicates, but also useful primitive predicates and motor
actions.

Acknowledgments
This material is based upon work supported in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition
Services Division, under Contract No. NBCHD030010; and in part by DARPA Grant No.
HR0011-04-1-0012 .

References
Agre, P., & Chapman, D. (1987). Pengi: An implementation of a theory of activity. In
Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI).
AIPS (2002). International planning competition. http://www.dur.ac.uk/d.p.long/competition.html.
AIPS (2006). International planning competition. http://www.ldc.usb.ve/bonet/ipc5/.
Amir, E. (2005). Learning partially observable deterministic action models. In Proceedings
of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI).
Assche, A. V., Vens, C., Blockeel, H., & Dzeroski, S. (2004). A random forest approach to
relational learning. In Proceedings of the ICML Workshop on Statistical Relational
Learning and its Connections to Other Fields.
Benson, S. (1996). Learning Action Models for Reactive Autonomous Agents. Ph.D. thesis,
Stanford University.
Bertsekas, D. P. (1999). Nonlinear Programming. Athena Scientific.
Blum, A., & Langford, J. (1999). Probabilistic planning in the graphplan framework. In
Proceedings of the Fifth European Conference on Planning (ECP).
350

fiLearning Symbolic Models of Stochastic World Dynamics

Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for first-order
MDPs. In Proceedings of the Seventeenth International Joint Conference on Artificial
Intelligence (IJCAI).
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Proceedings of the Fourteenth Annual Conference on Uncertainty in AI (UAI).
Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning with information gathering
and contingent execution. In Proceedings of the Second International conference on
AI Planning Systems (AIPS).
Edelkamp, S., & Hoffman, J. (2004). PDDL2.2: The language for the classical part of the 4th
international planning competition. Technical Report 195, Albert-Ludwigs-Universitat,
Freiburg, Germany.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2(2).
Getoor, L. (2001). Learning Statistical Models From Relational Data. Ph.D. thesis, Stanford.
Gil, Y. (1993). Efficient domain-independent experimentation. In Proceedings of the Tenth
International Conference on Machine Learning (ICML).
Gil, Y. (1994). Learning by experimentation: Incremental refinement of incomplete planning domains. In Proceedings of the Eleventh International Conference on Machine
Learning (ICML).
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research (JAIR), 19.
Kearns, M., Mansour, Y., & Ng, A. (2002). A sparse sampling algorithm for near-optimal
planning in large Markov decision processes. Machine Learning (ML), 49(2).
Kersting, K. (2006). An Inductive Logic Programming Approach to Statistical Relational
Learning. IOS Press.
Kersting, K., Raedt, L. D., & Raiko, T. (2006). Logical hidden markov models. Journal of
Artificial Intelligence Research (JAIR), 25.
Khan, K., Muggleton, S., & Parson, R. (1998). Repeat learning using predicate invention.
In International Workshop on Inductive Logic Programming (ILP).
Kok, S., & Domingos, P. (2005). Learning the structure of markov logic networks. In Proceedings of the Twenty Second International Conference on Machine Learning (ICML).
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming Techniques and Applications. Ellis Horwood.
Mitchell, T. M. (1982). Generalization as search. Artificial Intelligence, 18(2).
Oates, T., & Cohen, P. R. (1996). Searching for planning operators with context-dependent
and probabilistic effects. In Proceedings of the Thirteenth National Conference on
Artificial Intelligence (AAAI).
ODE (2004). Open dynamics engine toolkit.. http://opende.sourceforge.net.
351

fiPasula, Zettlemoyer, & Pack Kaelbling

Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2004). Learning probabilistic relational planning rules. In Proceedings of the Fourteenth International Conference on Automated
Planning and Scheduling (ICAPS).
Puterman, M. L. (1999). Markov Decision Processes. John Wiley and Sons, New York.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning (ML),
62.
Sanghai, S., Domingos, P., & Weld, D. (2005). Relational dynamic bayesian networks.
Journal of Artificial Intelligence Research (JAIR), 24.
Shen, W.-M., & Simon, H. A. (1989). Rule creation and rule learning through environmental exploration. In Proceedings of the Eleventh International Joint Conference on
Artificial Intelligence (IJCAI).
Wang, X. (1995). Learning by observation and practice: An incremental approach for planning operator acquisition. In Proceedings of the Twelfth International Conference on
Machine Learning (ICML).
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection for first-order Markov
decision processes. In Proceedings of the Eighteenth Conference on Uncertainty in
Artificial Intelligence (UAI).
Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: An extension to PDDL for expressing
planning domains with probabilistic effects. School of Computer Science, Carnegie
Mellon University, Technical Report CMU-CS-04-167.
Zettlemoyer, L., Pasula, H., & Kaelbling, L. (2003). Learning probabilistic relational planning rules. MIT Tech Report.

352

fiJournal of Artificial Intelligence Research 29 (2007) 191219

Submitted 07/2006; published 06/2007

The Language of Search
Jinbo Huang

jinbo.huang@nicta.com.au

Logic and Computation Program
National ICT Australia

Adnan Darwiche

darwiche@cs.ucla.edu

Computer Science Department
University of California, Los Angeles

Abstract
This paper is concerned with a class of algorithms that perform exhaustive search on
propositional knowledge bases. We show that each of these algorithms defines and generates
a propositional language. Specifically, we show that the trace of a search can be interpreted
as a combinational circuit, and a search algorithm then defines a propositional language
consisting of circuits that are generated across all possible executions of the algorithm. In
particular, we show that several versions of exhaustive DPLL search correspond to such
well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF. By
thus mapping search algorithms to propositional languages, we provide a uniform and
practical framework in which successful search techniques can be harnessed for compilation
of knowledge into various languages of interest, and a new methodology whereby the power
and limitations of search algorithms can be understood by looking up the tractability and
succinctness of the corresponding propositional languages.

1. Introduction
Systematic search algorithms lie at the core of a wide range of automated reasoning systems,
and many of them are based on a procedure where branches of a node in the search tree are
generated by splitting on the possible values of a chosen variable. One prototypical example
is the DPLL algorithm (Davis, Logemann, & Loveland, 1962) for propositional satisfiability
(SAT). Given a propositional formula, the problem of SAT is to determine whether the
formula has a satisfying assignmentan assignment of Boolean values (0 and 1) to the
variables under which the formula evaluates to 1. For example, (x1 = 0, x2 = 1, x3 = 0) is a
satisfying assignment for the following formula: (x1 x2 )(x1 x2 x3 )(x1 x2 x3 ).
To determine the satisfiability of a given formula , DPLL chooses a variable x from the
formula, recursively determines whether  is satisfiable in case x is set to 0, and in case x
is set to 1, and declares  satisfiable precisely when at least one of the two cases results
in a positive answer. In effect, this algorithm performs a systematic search in the space
of variable assignments and terminates either on finding a satisfying assignment, or on
realizing that no such assignment exists.
Despite its simplicity, DPLL has long remained the basis of most SAT solvers that
employ systematic search (Berre & Simon, 2005), and finds natural counterparts in the
more general constraint satisfaction problems, where variables are not restricted to the
Boolean domain. Several decades of sustained research has greatly enhanced the efficiency
and scalability of DPLL-based search algorithms, and today they are routinely used to solve

c
2007
AI Access Foundation. All rights reserved.

fiHuang & Darwiche

practical problems with several million variables (Zhang & Malik, 2002). These algorithms
have been so successful, indeed, that it has become a recent trend in such areas as formal
verification to modify them to produce all solutions of a propositional formula (McMillan,
2002; Chauhan, Clarke, & Kroening, 2003; Grumberg, Schuster, & Yadgar, 2004), as an
alternative to the traditional practice (McMillan, 1993) of converting the formula into an
ordered binary decision diagram (OBDD) (Bryant, 1986). The modifications thus involved
are in such a form that the DPLL search will not terminate on finding the first solution,
but is extended to exhaust the whole search space. For the same formula shown earlier, an
example output of such an exhaustive search could be the following set of three solutions
{(x1 = 0, x2 = 1, x3 = 0), (x1 = 1, x2 = 0, x3 = 0), (x1 = 1, x2 = 1)}. Note that a solution
here is defined as an assignment of Boolean values to some (possibly all) of the variables
that satisfies the formula regardless of the values of the other variables. The last solution
in the above set, for example, represents two satisfying assignments as variable x3 is free to
assume either value.
Producing all solutions of a propositional formula is, of course, only one of the possible
computational tasks for which an exhaustive search is useful. Other such tasks include
counting the number of satisfying assignments of a formula, also known as model counting
(Birnbaum & Lozinskii, 1999; Bayardo & Pehoushek, 2000; Bacchus, Dalmao, & Pitassi,
2003b; Sang, Bacchus, Beame, Kautz, & Pitassi, 2004; Sang, Beame, & Kautz, 2005), and
processing certain types of queries on belief and constraint networks (Dechter & Mateescu,
2004b, 2004a).
In this paper we uncover a fundamental connection between this class of exhaustive
search algorithms, and a group of propositional languages that have been extensively studied in the field of knowledge compilation (Darwiche & Marquis, 2002). Specifically, we show
that an exhaustive search algorithm based on variable splitting, when run on propositional
knowledge bases, defines and generates a propositional language in a very precise sense: The
trace of a single search, when recorded as a graph, can be interpreted as a combinational
circuit that is logically equivalent to the propositional knowledge base on which the search
has run; the search algorithm itself then defines a propositional language consisting of all
circuits that can be generated by legal executions of the algorithm. We show, in particular,
that exhaustive DPLL corresponds to the language of FBDD (free binary decision diagrams)
(Blum, Chandra, & Wegman, 1980), exhaustive DPLL with fixed variable ordering corresponds to the language of OBDD (ordered binary decision diagrams) (Bryant, 1986), and
exhaustive DPLL with decomposition corresponds to a well defined subset of the language
of d-DNNF (deterministic decomposable negation normal form) (Darwiche, 2001).
The establishment of this correspondence supplies a bridge between the field of knowledge compilation, and other areas of automated reasoning, including propositional satisfiability, where search algorithms have been extensively studied. In particular, it leads to the
following two sets of theoretical and practical benefits.
First, we show that the class of search algorithms we have described can be immediately
turned into knowledge compilers for the respective propositional languages they define, by
simply recording the trace of the search. This realization provides a uniform and practical framework in which successful techniques developed in the context of search can be
directly used for compilation of knowledge into various languages of interest. In particular,
we discuss how recent advances in DPLL search, including sophisticated conflict analysis,
192

fiThe Language of Search

dependency-directed backtracking, clause learning, new variable ordering heuristics, and
data structures for faster constraint propagation, can be harnessed for building efficient
practical knowledge compilers.
Second, we show how, by looking up known properties of propositional languages, we are
now able to answer at a fundamental level, and in concrete terms, two important questions
regarding the power and limitations of the same class of search algorithms: What can these
algorithms do? And what can they not? Specifically, we discuss how the tractability of the
propositional language defined by a search algorithm illustrates the power of the algorithm,
and how the succinctness and the constraints of the language illustrate its limitations.
We complement these discussions by relating our results to previous work on knowledge compilation and a recent body of work centering on the notion of AND/OR search
(Dechter & Mateescu, 2004b, 2004a; Marinescu & Dechter, 2005). In particular, we discuss
similarities as well as differences between AND/OR search and d-DNNF compilation.
Finally, we present experimental results on implementations of exhaustive search algorithms that define distinct propositional languages. We use these programs to compile a set
of propositional formulas into the respective languages, both to demonstrate the practicality of this knowledge compilation framework, and to empirically illustrate the variation of
language succinctness in response to the variation of the search strategy.
The remainder of the paper is organized as follows. Section 2 reviews a number of
propositional languages concerned in this work and their theoretical roles and relations in
knowledge compilation. Section 3 discusses the DPLL search algorithm for propositional
satisfiability and its exhaustive extension, and introduces the notion of interpreting the
trace of a search as a combinational circuit. Section 4 is a detailed exposition on mapping
variants of exhaustive DPLL to FBDD, OBDD, and a subset of d-DNNF, as well as techniques involved in transforming these algorithms into practical knowledge compilers for the
respective languages. Section 5 formalizes two fundamental principles relating to the intrinsic power and limitations of a class of exhaustive search algorithms, using recently proposed
model counting algorithms as concrete examples. We relate our results to previous work in
Section 6, present experimental results in Section 7, and conclude in Section 8. Proofs of
all theorems are given in the appendix.

2. Propositional Languages and Their Properties
The study of propositional languages (i.e., representations for propositional theories) has
been a central subject in knowledge compilation, which is concerned with the task of converting a given knowledge base from one language into another so that certain reasoning
tasks become tractable on the compiled knowledge base (Selman & Kautz, 1991; del Val,
1994, 1995; Marquis, 1995; Selman & Kautz, 1996; Cadoli & Donini, 1997; Darwiche &
Marquis, 2002; Darwiche, 2002, 2004; Coste-Marquis, Berre, Letombe, & Marquis, 2005).
After propositional theories are compiled into the language of OBDD, for example, their
equivalence can be tested in time that is polynomial in the sizes of the OBDDs (Meinel &
Theobald, 1998), or constant time if the OBDDs use the same variable order (Bryant, 1986).
More recent applications of compilation using the language of d-DNNF can be found in the
fields of diagnosis (Barrett, 2004, 2005; Huang & Darwiche, 2005a; Elliott & Williams, 2006;
Siddiqi & Huang, 2007), planning (Barrett, 2004; Palacios, Bonet, Darwiche, & Geffner,
193

fiHuang & Darwiche

(a) NNF

(b) Decision Node
or

or
and

and

and

and

or

or

or

and and and and

X

or

and and and and



X



(c) Alternatively
X

A

B

B

A

C

D

D

C





Figure 1: An NNF circuit and a decision node.
2005; Huang, 2006; Bonet & Geffner, 2006), probabilistic reasoning (Chavira & Darwiche,
2005; Chavira, Darwiche, & Jaeger, 2006), and query rewrites in databases (Arvelo, Bonet,
& Vidal, 2006).
In this section we review a set of propositional languages and discuss an established
body of results concerning their tractability and succinctness (Darwiche & Marquis, 2002)
several of these languages will resurface in Section 4 as those to which exhaustive search
algorithms are mapped, and their properties will prove vital to our formalization in Section 5
of two fundamental principles relating to the power and limitations of exhaustive search
algorithms.
2.1 Propositional Languages
Following the conventions of Darwiche and Marquis (2002), we consider graph representations of propositional theories, which allow sharing of subformulas for compactness. Specifically, we consider directed acyclic graphs (DAGs) where each internal node is labeled with
a conjunction (and, ) or disjunction (or, ), and each leaf is labeled with a propositional
literal or constant (true/f alse, or 1/0). It should be clear that such a DAG is effectively a
combinational circuit with and-gates, or-gates, and inverters, where inverters only appear
next to inputs (variables), a property that is characteristic of the Negation Normal Form
(NNF) (Barwise, 1977). We will hence refer to these DAGs as NNF circuits and the set of
all such DAGs as the NNF language. Figure 1a depicts a propositional theory represented
as an NNF circuit. We will next define some interesting subsets of the NNF language.
The popular language of CNF (conjunctive normal form) can now be defined as the
subset of NNF that satisfies (i) flatness: the height of the DAG is at most two; and (ii)
simple-disjunction: any disjunction is over leaf nodes only (i.e., is a clause). Similarly,
DNF (disjunctive normal form) is the subset of NNF that satisfies flatness and simpleconjunction: any conjunction is over leaf nodes only (i.e., is a term).
We consider next a set of nested representations, starting with the DNNF (decomposable
negation normal form) language, which is the set of all NNF circuits satisfying decomposability: conjuncts of any conjunction share no variables. Our next language, d-DNNF,
satisfies both decomposability and determinism: disjuncts of any disjunction are pairwise
logically inconsistent. The NNF circuit shown in Figure 1a, for example, is in d-DNNF;
194

fiThe Language of Search

x1

or
and
or

x2
and

X3

X1

x1

X2

(a) DNNF

x2

x3
x3

x3

x2

0

1

(b) FBDD

x2

0

1

(c) OBDD

Figure 2: A circuit in DNNF, FBDD, and OBDD.
that shown in Figure 2a, by contrast, is in DNNF but not in d-DNNF as neither of the two
disjunction nodes satisfies determinism.
The FBDD language is the subset of d-DNNF where the root of every circuit is a
decision node, which is defined recursively as either a constant (0 or 1) or a disjunction in
the form of Figure 1b where X is a propositional variable and  and  are decision nodes.
Note that an equivalent but more compact drawing of a decision node as Figure 1c is widely
used in the formal verification literature, where FBDDs are equivalently known as BDDs
(binary decision diagrams) that satisfy the test-once property: each variable appears at
most once on any root-to-sink path (Gergov & Meinel, 1994).1 See Figure 2b for an FBDD
example using this more compact drawing.
The OBDD language is the subset of FBDD where all circuits satisfy the ordering
property: variables appear in the same order on all root-to-sink paths (Bryant, 1986). See
Figure 2c for an OBDD example (using the more compact drawing). For a particular
variable order <, we also write OBDD< to denote the corresponding OBDD subset where
all circuits use order <.
2.2 Succinctness and Tractability of Propositional Languages
Given a choice of languages in which a knowledge base may be represented, one needs to
strike a balance between the size of the representation and the support it provides for the
reasoning task at hand, as these two properties of a representation often run counter to
each other. CNF, for example, is often convenient for compactly encoding a knowledge
base since in many applications the behavior of a system can be naturally described as the
conjunction of behaviors of its components. However, few typical reasoning tasks can be
efficiently carried out on CNF representations. There is no efficient algorithm to determine,
for example, whether an arbitrary clause is entailed by a CNF formula. The story changes
when the propositional theory is represented in a language known as PI (prime implicates,
a subset of CNF): By definition PI supports a linear-time clausal entailment test. The
downside is, unfortunately, that PI representations can be exponentially larger than their
CNF equivalents in the worst case (Karnaugh, 1953; Forbus & de Kleer, 1993).
We are therefore interested in formally analyzing the succinctness and tractability of
languages, so that given a required reasoning task, we can choose the most succinct language
1. FBDDs are also known as read-once branching programs (Wegener, 2000).

195

fiHuang & Darwiche

that supports the set of necessary operations in polynomial time. The following is a classical
definition of succinctness:
Definition 1. (Succinctness) Let L1 and L2 be two subsets of NNF. L1 is at least as succinct
as L2 , denoted L1  L2 , iff there exists a polynomial p such that for every circuit   L2 ,
there exists a logically equivalent circuit   L1 where ||  p(||). Here, || and || are
the sizes of  and , respectively.
Intuitively, language L1 is at least as succinct as language L2 if given any circuit in L2 ,
there exists a logically equivalent circuit in L1 whose size does not blow up. One can also
define L1 to be strictly more succinct than L2 , denoted L1 < L2 , if L1  L2 but L2 6 L1 .
The languages we described in Section 2.1 satisfy the following succinctness relations: NNF
< DNNF < d-DNNF < FBDD < OBDD, NNF < CNF, and DNNF < DNF (Darwiche &
Marquis, 2002). Note, however, that L1  L2 does not imply L1 < L2 in general. In other
words, imposing conditions on a representation does not necessarily reduce its succinctness.
One example is smoothness, which requires disjuncts of any disjunction to mention the same
set of variablesit is known that this condition, when imposed on d-DNNF, does not reduce
its succinctness (Darwiche & Marquis, 2002).
We now turn to the tractability of languages, which refers to the set of polynomial-time
operations they support. According to Darwiche and Marquis (2002), one traditionally
distinguishes between two types of operations on circuits in a given language: queries and
transformations. The difference between the two is that queries return information about
circuits, but normally do not change them, while transformations modify circuits to generate
new ones (in the same language).
Some of the known results from Darwiche and Marquis (2002) regarding the tractability of languages are summarized in Table 1 (queries) and Table 2 (transformations). The
abbreviations in the first row of Table 1 stand for the following eight queries, respectively:
Consistency (is the formula satisfiable), Validity (does the formula evaluate to 1 under
all variable assignments), Clausal Entailment (does the formula imply a given clause),
Implicant (is the formula implied by a given term), Equivalence (are the two formulas logically equivalent), Sentential Entailment (does the one formula imply the other),
Model Counting (how many satisfying assignments does the formula have), Model Enumeration (what are the satisfying assignments of the formula). The abbreviations in the
first row of Table 2 stand for the following eight transformations, respectively: Conditioning (setting a set of variables to constants), Forgetting (existentially quantifying a set of
variables), Single-Variable Forgetting (existentially quantifying a single variable), Conjunction (conjoining a set of circuits), Bounded Conjunction (conjoining a bounded
number of circuits), Disjunction (disjoining a set of circuits), Bounded Disjunction
(disjoining a bounded number of circuits), Negation (negating a circuit).
Interestingly, Table 1 offers one explanation for the popularity of OBDDs in formal verification where efficient equivalence testing, among other things, is often critical. Although
more succinct, d-DNNF and FBDD are not known to admit a polynomial-time equivalence
test (a polynomial-time probabilistic equivalence test is possible; see Blum et al., 1980;
Darwiche & Huang, 2002). Note also that although there is no difference between d-DNNF
and FBDD to the extent of this table, the question mark on the equivalence test (EQ) could
eventually be resolved differently for the two languages.
196

fiThe Language of Search

Table 1: Polynomial-time queries supported by a language ( means not supported unless
P=NP and ? means we dont know).
Language
NNF
DNNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF

CO




VA




CE




IM




































EQ


?

?



SE








CT




ME

























Table 2: Polynomial-time transformations supported by a language ( means not supported,  means not supported unless P=NP, and ? means  we dont know).
Language
NNF
DNNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF

CD










FO



SFO











C


BC































C



BC






















C


?







It is also worth pointing out that while tractability with respect to the queries generally improves when the language becomes more restrictive (has more conditions imposed),
tractability with respect to the transformations may not. DNNF, for example, supports a
subset of the queries that are supported by OBDD according to Table 1, and is therefore
less tractable than OBDD from this point of view. However, when it comes to certain
transformations, such as the operation of Forgetting (existential quantification), DNNF becomes more tractable than OBDD according to Table 2. The key reason for this shift of
advantage is that transformations operate on circuits in a given propositional language, and
require the result to be in the same languagethis requirement can become a burden for
the more restrictive language where more conditions need to be satisfied when the result of
the transformation is generated.

197

fiHuang & Darwiche

2.3 A Connection to Be Established
We have summarized and discussed in this section a rich body of known results concerning
properties of various propositional languages. These results have previously been presented
as a guide for the task of selecting a suitable target compilation language in applications
of knowledge compilation. In particular, they suggest that given a reasoning task involving
knowledge compilation, one identify the set of operations required for the task, and then
select the most succinct target compilation language supporting these operations (Darwiche
& Marquis, 2002).
In the following sections of the paper, we wish to establish a fundamental connection
between propositional languages with distinct degrees of succinctness and tractability, and
exhaustive search algorithms running under distinct sets of constraints. Specifically, we
show that the trace of an exhaustive search can be interpreted as a circuit representing a
compilation of the propositional knowledge base on which the search has run, and the search
algorithm itself then defines a propositional language consisting of all its possible traces.
This connection will then serve as a bridge between the field of knowledge compilation
and other areas of automated reasoning in which search algorithms have been extensively
studied, affording two related sets of benefits as follows.
In the first direction, we show how this connection provides a set of practical algorithms
for compilation of knowledge into various languages. Specifically, an exhaustive search
algorithm can be directly turned into a knowledge compiler by recording the trace of its
execution as a graph, and variations on the search algorithm then nicely correspond to
compilers for different propositional languages. Such a framework for knowledge compilation
provides a significant advantage in that many (past as well as future) advances in search
will automatically carry over to knowledge compilation. In particular, we discuss how
knowledge compilers can capitalize on several important recent advances in the DPLL search
for propositional satisfiability, including sophisticated conflict analysis, dependency-directed
backtracking, clause learning, new variable ordering heuristics, and data structures for faster
constraint propagation.
In the second direction, we formulate two principles whereby the intrinsic power and
limitations of a given exhaustive search algorithm can be understood by identifying the
propositional language defined by the search algorithm. Specifically, the tractability of the
language illustrates the power (usefulness) of the search algorithm and the succinctness
and the constraints of the language illustrates its limitations. Using a group of recently
proposed model counters as concrete examples, we show that the search algorithms used
by these model counters are powerful enough to support not just a model counting query,
but all other queries that are known to be tractable for the language of d-DNNF, such as
a probabilistic equivalence test (Darwiche & Huang, 2002). On the other hand, two fundamental limitations can be identified for these same algorithms, as well as other exhaustive
search algorithms based on variable splitting, the first in their traces being restricted to
a subset of d-DNNF, potentially limiting the efficiency of the search, and the second in
their inability to produce traces without determinism, making them overly constrained for
compilation of knowledge into more general languages than d-DNNF, such as DNNF.

198

fiThe Language of Search

We now proceed to uncover the connection between search algorithms and propositional
languages, starting with a systematic search algorithm for propositional satisfiability, its
exhaustive extension, and the notion of the trace of a search.

3. Systematic Search for Satisfiability and Its Exhaustive Extension
In this section we introduce the notion of the trace of a systematic exhaustive search, and
show how the trace can be interpreted as a circuit, which is logically equivalent to (and hence
is a compilation of) the propositional knowledge base on which the search has run. We will
do so in the context of systematically searching for satisfying assignments of a propositional
formula, a major approach to the problem of propositional satisfiability (SAT) that has
come to be known as DPLL (Davis et al., 1962).
3.1 DPLL Search and Its Exhaustive Extension
Algorithm 1 is a summary of DPLL for SAT, which takes a propositional formula in CNF,
and return 1 (0) precisely when the formula is satisfiable (unsatisfiable). It works by recursively doing a case analysis on the assignment to a selected variable (Line 5): The formula
is satisfiable if and only if either case results in a satisfiable subformula (Line 6). The two
subformulas are denote as |x=0 and |x=1 , which result from replacing all occurrences of
x in  with 0 and 1, respectively. In keeping with the rules of Boolean logic, we assume
that if a literal becomes or evaluates to 0 as a result of this variable instantiation, it is
removed from every clause that contains it; and that if a literal becomes or evaluates to 1,
all clauses that contain it are removed. (To facilitate our subsequent discussion of variants
of DPLL, we have omitted the use of unit resolution from the pseudocode. The programs
used in Section 7, however, do employ unit resolution.) In effect, Algorithm 1 performs a
search in the space of variable assignments until it finds one that satisfies the given CNF
formula or realizes that no satisfying assignments exist.
Now consider extending Algorithm 1 so that it will go through the space of all satisfying assignmentsby always exploring both branches of Line 6rather than terminate on finding the first one. Figure 3a depicts the search tree of this exhaustive version of DPLL, under some particular variable ordering, on the following CNF formula:
(x1  x2 )  (x1  x2  x3 )  (x1  x2  x3 ). Note that in drawing the branches of the
search we use a dotted (solid) line to denote setting the variable to 0 (1)we will also refer
to the corresponding child of the search node as the low (high) child.
Algorithm 1 DPLL(CNF: ): returns satisfiability of 
1: if there is an empty clause in  then
2:
return 0
3: if there are no variables in  then
4:
return 1
5: select a variable x of 
6: return DPLL(|x=0 ) or DPLL(|x=1 )

199

fiHuang & Darwiche

or

x1

and
x1
x2

or

x2

unsat

and
sat

x3
unsat

0 x2

unsat

or

and
x2

and
x2
or

x3
sat

and
x1
and
x2 1

or

and and
and and
0 x3 x3 1 0 x3 x3 1
(b) Equivalent NNF circuit

sat

(a) Termination tree

Figure 3: The trace of an exhaustive DPLL search.
The tree depicted in Figure 3a is also known as the termination tree of the search,
as it captures the set of paths through the search space that have been explored at the
termination of the search. In particular, each leaf of the tree labeled with sat gives a
partial variable assignment that satisfies the propositional formula regardless of the values
of any unassigned variables, and the whole tree characterizes precisely the set of all satisfying
assignments, which the algorithm has set out to find and succeeded in finding.
3.2 The Trace of the Search and the Issue of Redundancy
As we have alluded to earlier, we would like to view such a termination tree as the trace of
the search, about which we now make two important observations: First, the trace of the
search as depicted in Figure 3a can be directly translated into a circuit in NNF as depicted
in Figure 3b. All that is involved is to rename sat/unsat to 1/0 and invoke the identity
between Figure 1b and Figure 1c as we described in Section 2.1. Second, this NNF circuit
is logically equivalent to, and hence is a compilation of, the CNF formula on which the
search has run. (Note that this notion of trace is different from that used in earlier work
to establish the power of DPLL as a proof system for unsatisfiable CNF formulas. For
example, in earlier work it was shown that on an unsatisfiable CNF formula, the trace of
DPLL can be converted into a tree-like resolution refutation (Urquhart, 1995).)
These two observations imply that exhaustive DPLL is as powerful as a knowledge
compiler, as long as one takes the (small) trouble of recording its trace. From the viewpoint
of knowledge compilation, however, a search trace recorded in its present form may not be
immediately useful, because it will typically have a size proportional to the amount of work
done to produce it. Answering even a linear-time query (which may require only a single
traversal of the compiled representation) on such a compilation, for example, would be as
if one were running the whole search over again.
This problem can be remedied by first realizing that there is quite a bit of redundancy
in the search trace we have drawn. In Figure 3a, for example, the two subgraphs whose
roots are labeled with x3 are isomorphic to each other and could be merged into one. The
same redundancy, of course, is present in the corresponding portions of the NNF circuit
shown in Figure 3b.
200

fiThe Language of Search

We can distinguish two levels of dealing with the issue of redundancy in the trace. In
the first level, we can remove all redundancy from the trace by reducing it from a tree to
a DAG, with repeated applications of the following two rules: (i) Isomorphic nodes (i.e.,
nodes that have the same label, same low child, and same high child) are merged; (ii) Any
node with identical children is deleted and pointers to it are redirected to either one of its
children (Bryant, 1986). If we apply these reduction rules to the tree of Figure 3a (again
renaming sat/unsat to 1/0), we will get the DAG shown in Figure 2c (in this particular
example the second rule does not apply). Note that instead of performing the reduction at
the end of the search as these two reduction rules suggest, we can do better by integrating
the rules into the trace recording process so that redundant portions of the trace will not
be recorded in the first place. This brings us to a technique known as unique nodes (Brace,
Rudell, & Bryant, 1991; Somenzi, 2004), which we discuss in more detail in the next section.
Removing redundancy in this level ensures that the smallest possible compilation is obtained given a particular execution of the search algorithm; however, it does not improve
the time complexity of the search itself. In Figure 3a, for example, the reason why there
are two isomorphic subgraphs (rooted at nodes labeled with x3 ) in the first place is that the
search has run into equivalent subproblems from different paths. In the general case these
can be nontrivial subproblems, and solving each one of them can be a source of great inefficiency. We therefore refer to the second level of dealing with the issue of redundancy, where
we would like to be able to recognize the equivalence of subproblems and avoid carrying
out the same computation over and over again. This can be done using the technique of
formula caching (Majercik & Littman, 1998), which we also discuss in the following section.

4. The Language of Search
We have established in Section 3 the notion of interpreting the trace of an exhaustive search
as a circuit. In this section we continue our study of these search algorithms by showing how
each defines a propositional language consisting of all its possible traces. We will look at
three algorithms in particular: (i) the original exhaustive DPLL, (ii) exhaustive DPLL with
fixed variable ordering, and (iii) exhaustive DPLL with decomposition. For each algorithm
we will discuss the propositional language it defines, the corresponding knowledge compiler
it provides, as well as issues regarding the efficiency of the knowledge compiler.
4.1 Mapping Exhaustive DPLL to FBDD
We have seen in Section 3 that with the application of reduction rules, the trace of exhaustive
DPLL in our example, depicted in Figure 3a, can be stored more compactly as Figure 2c,
which is none other than a circuit in the language of FBDD (which happens to be also an
OBDD in this case).
We will now formally show that traces of exhaustive DPLL across all possible executions
of the algorithm form a propositional language that is precisely the language of (reduced)
FBDD as defined in Section 2 (from now on we will assume that circuits in FBDD and
OBDD are always given in their reduced form by application of the two reduction rules). In
order to do so we will first need a formalism for explicitly recording the trace of the search
as a graph. For this purpose we introduce Algorithm 2, which is exactly the exhaustive
extension of the original DPLL (Algorithm 1) except that the newly introduced function,
201

fiHuang & Darwiche

get-node (given in Algorithm 3), provides a means of recording the trace of the search
in the form of a DAG. Specifically, get-node will return a decision node (in the form of
Figure 1b) labeled with the first argument, having the second argument as the low child,
and having the third argument as the high child (Lines 2&4 have also been modified to
return the terminal decision nodes, instead of the Boolean constants). Note that, as we
briefly mentioned in Section 3, this algorithm will have its trace recorded directly in the
reduced from, instead of producing redundant nodes to be removed later. This is because
the two reduction rules are built in by means of a unique nodes table, well known in
the BDD community (Brace et al., 1991; Somenzi, 2004). Specifically, all nodes created by
get-node are stored in a hash table and get-node will not create a new node if (i) the node
to be created already exists in the table (that existing node is returned); or (ii) the second
and third arguments are the same (either argument is returned). We can now formally state
our result as follows:
Theorem 1. DAGs returned by Algorithm 2 form the language of (reduced) FBDD.
Theorem 1 immediately provides us with a CNF-to-FBDD compiler, which means that
as soon as the search finishes, we can answer in polynomial time any query about the
propositional theory, as long as that query is known to be tractable for FBDD. According
to Table 1, such queries include consistency, validity, clausal entailment, implicant, model
counting, and model enumeration. According to Blum et al. (1980), one can then also
test the equivalence of two propositional formulas probabilistically in polynomial time after
running Algorithm 2 on both. On the other hand, if a propositional theory given to Algorithm 2 is known to have no polynomial-size representation in FBDD, we can also conclude
that the algorithm will not be able to finish in polynomial time no matter what variable
ordering it uses.
To make Algorithm 2 a practical FBDD compiler, we need to deal with the issue of
redundant computation as briefly mentioned in Section 3. The reason is that, despite
the use of unique nodes which controls the space complexity, Algorithm 2 still has a time
complexity proportional to the size of the tree version of the search trace: Portions of
the DAG can end up being explored multiple times. See Figure 4 for an example, where
two different instantiations of the first three variables lead to the same subformula, which
would then be compiled twice, unnecessarily, by Algorithm 2. To alleviate this problem,
one resorts to the technique of formula caching (Majercik & Littman, 1998).
Algorithm 4 describes the same exhaustive DPLL search, but now with caching. The
result of a recursive call DPLLf () will be stored in a cache (Line 10) before being returned,
indexed by a key (computed on Line 5) identifying ; any subsequent call on some 0 will
Algorithm 2 dpllf (CNF: ): exhaustive DPLL
1: if there is an empty clause in  then
2:
return 0-sink
3: if there are no variables in  then
4:
return 1-sink
5: select variable x of 
6: return get-node(x, dpllf (|x=0 ), dpllf (|x=1 ))
202

fiThe Language of Search

x5

x6

X1

0
x4

x5

x1

x3

x2

x3

x1

x2

1

x6
x4

x5

X2

1

0

X3
x3
x5
x4

X2

X3
1

1

x6
x5 x6

x4

x5

x6
x5 x6

Figure 4: Reaching the same subformula via different paths of the search.
immediately return the existing compilation for  from the cache (Line 7) if 0 is found to
be equivalent to  (by a key comparison on Line 6). (Note that the introduction of caching
does not change the identity of the proposition language defined by the algorithm. In other
words, Theorem 1 applies to Algorithm 4 as well.)
In practice, one normally focuses on efficiently recognizing formulas that are syntactically
identical (i.e., have the same set of clauses). Various methods have been proposed for this
purpose in recent years, starting with Majercik and Littman (1998) who used caching for
probabilistic planning problems, followed by Darwiche (2002) who proposed a concrete
formula caching method in the context of knowledge compilation, then Bacchus, Dalmao,
and Pitassi (2003a) and Sang et al. (2004) in the context of model counting, and then
Darwiche (2004) and Huang and Darwiche (2005b) who proposed further refinements on
the method of Darwiche (2002).
4.2 Mapping Exhaustive DPLL with Fixed Variable Ordering to OBDD
Note that in Algorithm 4, DPLL is free to choose any variable on which to branch (Line 8).
This corresponds to the use of a dynamic variable ordering heuristic in a typical SAT solver,
and is in keeping with the spirit of free binary decision diagrams (FBDD).
Not surprisingly, when one switches from dynamic to static variable ordering, the DAGs
produced by the algorithm will be restricted to a subset of FBDD. Algorithm 5 implements
this change, by taking a particular variable order  as a second argument, and making sure
that this order is enforced when choosing the next branching point (see Line 8). Across all
possible inputs and variable orderings, this algorithm will indeed produce exactly the set of
all circuits in the language of (reduced) OBDD:
Algorithm 3 get-node(int: i, BDD: low, BDD: high)
1: if low is the same as high then
2:
return low
3: if node (i, low, high) exists in unique-table then
4:
return unique-table[(i, low, high)]
5: result = create-bdd-node(i, low, high)
6: unique-table[(i, low, high)] = result
7: return result
203

fiHuang & Darwiche

Theorem 2. DAGs returned by Algorithm 5 form the language of (reduced) OBDD.
We are therefore provided with a CNF-to-OBDD compiler in Algorithm 5, which means
that as soon as the search finishes, we can answer in polynomial time any query about
the propositional theory, as long as that query is known to be tractable for OBDD. Most
notably, we can now test the equivalence of two propositional formulas deterministically in
polynomial time after running Algorithm 5 on both, which we could not with Algorithm 2
or Algorithm 4. On the other hand, if a propositional theory given to Algorithm 5 is
known to have no polynomial-size representation in OBDD, such as the hidden weighted bit
function (Bryant, 1991), we can also conclude that the algorithm will not be able to finish
in polynomial time no matter what variable ordering it uses.
To make Algorithm 5 a practical OBDD compiler, we need again to deal with the issue
of redundant computation. Naturally, any general formula caching method, such as the ones
we described earlier, will be applicable to Algorithm 5. For this more constrained search
algorithm, however, a special method is available where shorter cache keys can be used
to reduce the cost of their manipulation. The reader is referred to Huang and Darwiche
(2005b) for details of this method, which allows one to bound the number of distinct cache
keys, therefore providing both a space and a time complexity bound. In particular, with
this specific caching scheme in force, the space and time complexity of Algorithm 5 was
shown to be exponential only in the cutwidth of the given CNF formula. A variant caching
scheme allows one to show a parallel complexity in terms of the pathwidth (cutwidth and
pathwidth are not comparable).
We emphasize here that Algorithm 5 represents a distinct way of OBDD construction, in
contrast to the standard method widely adopted in formal verification where one recursively
builds OBDDs for components of the system (or propositional formula) to be compiled and
combines them using the Apply operator (Bryant, 1986). A well-known problem with this
latter method is that the intermediate OBDDs that arise in the process can grow so large as
to make further manipulation impossible, even when the final result would have a tractable
size. Considering that the final OBDD is really all that one is after, Algorithm 5 affords
a solution to this problem by building exactly it, no more and no less (although it may
do more work than is linear in the OBDD size, both because inconsistent subproblems do
not contribute to the OBDD size, and because the caching is not complete). An empirical
Algorithm 4 DPLLf (CNF: ): exhaustive DPLL with caching
1: if there is an empty clause in  then
2:
return 0-sink
3: if there are no variables in  then
4:
return 1-sink
5: key = compute-key()
6: if there exists a cache entry (key, result) then
7:
return result
8: select variable x of 
9: result = get-node(x, DPLLf (|x=0 ), DPLLf (|x=1 ))
10: cache-insert(key, result)
11: return result
204

fiThe Language of Search

ABC
ADE
BC
DE

and

A

and
BC

or

A  B  C
A  D  E

and A
and

or

DE

B

D

B

and

D

C

or

and
B D

E
0

or

and
A and

C

1

(a) Mixture of decision and conjunction nodes

or
and

and

B D
E

(b) Equivalent NNF circuit

Figure 5: Trace of exhaustive DPLL with decomposition.
comparison of this compilation algorithm and the traditional OBDD construction method
can be found in Huang and Darwiche (2005b).
4.3 Mapping Exhaustive DPLL with Decomposition to a Subset of d-DNNF
It has been observed, in the particular case of model counting, that the efficiency of exhaustive DPLL can be improved by introducing decomposition, also known as component
analysis (Bayardo & Pehoushek, 2000; Bacchus et al., 2003b; Sang et al., 2004, 2005). The
idea is that when a propositional formula breaks down to a conjunction of disjoint subformulas (i.e., those that do not share variables), each subformula can be processed separately
and the results combined.
Algorithm 6 implements decomposition in exhaustive DPLL by relaxing a constraint on
Algorithm 4: Immediately before Line 8 of Algorithm 4, we need not insist any more that
a case analysis be performed on some variable x of the formula; instead, we will examine
the current formula, and attempt to decompose it (Line 5) into subsets that do not share a
Algorithm 5 DPLLo (CNF: , order: ): exhaustive DPLL with fixed variable ordering
1: if there is an empty clause in  then
2:
return 0-sink
3: if there are no variables in  then
4:
return 1-sink
5: key = compute-key()
6: if there exists a cache entry (key, result) then
7:
return result
8: x = first variable of order  that appears in 
9: result = get-node(x, DPLLo (|x=0 , ), DPLLo (|x=1 , ))
10: cache-insert(key, result)
11: return result
205

fiHuang & Darwiche

variable (we assume that this process is nondeterministic; that is, we do not have to detect
all decomposition points). The search will then run on each of these subformulas separately
and recursively (Lines 79), and the separate subtraces that result are connected by
means of an and-node to indicate that the results of the recursive call are being combined
(Line 10). In case no decomposition is performed (Line 6 fails), we will branch on a selected
variable as in regular DPLL (Lines 14&15).
Figure 5a shows the result of an example execution of this algorithm, where the instantiation of the first variable breaks the CNF formula into two disjoint clauses, which are
processed separately and the results combined as an and-node. Figure 5b shows the same
trace drawn equivalently as an explicit NNF circuit (for ease of viewing constants have been
removed and decision nodes at the bottom compacted into the corresponding literals they
represent).
As we have just witnessed, the use of decomposition in exhaustive DPLL has resulted
in a new type of node in the trace, returned by get-and-node on Line 10 of Algorithm 6.
The old get-node function (Line 15) still returns decision nodes (in a relaxed sense, as their
children now are not necessarily decision nodes) in the form of Figure 1c. The unique nodes
technique can also be extended in a straightforward way so that isomorphic and-nodes, as
well as duplicate children of an and-node, will not be created.
We are now ready to discuss the proposition language defined by Algorithm 6, for which
purpose we first define the following subset of the d-DNNF language, where determinism is
fulfilled by means of decision nodes (again, in the relaxed sense):
Definition 2. The language of decision-DNNF is the set of d-DNNF circuits in which all
disjunction nodes have the form of Figure 1b, or (x  )  (x  ), where x is a variable.2
2. Note that, unlike in an FBDD, here  and  can be either a conjunction or a disjunction node.

Algorithm 6 DPLLd (CNF: ): exhaustive DPLL with decomposition
1: if there is an empty clause in  then
2:
return 0-sink
3: if there are no variables in  then
4:
return 1-sink
5: components = exhaustive partitions of  with disjoint variable sets
6: if |components| > 1 then
7:
conjuncts = {}
8:
for all c  components do
9:
conjuncts = conjuncts  {DPLLd (c )}
10:
return get-and-node(conjuncts)
11: key = compute-key()
12: if there exists a cache entry (key, result) then
13:
return result
14: select variable x of 
15: result = get-node(x, DPLLd (|x=0 ), DPLLd (|x=1 ))
16: cache-insert(key, result)
17: return result
206

fiThe Language of Search

We can now formally state our result (again, we assume that circuits are always given
in their reduced form by application of appropriate reduction rules as described earlier,
although we have allowed redundancy in some of our figures for ease of viewing):
Theorem 3. DAGs returned by Algorithm 6 form the language of (reduced) decision-DNNF.
We are hence provided with a CNF-to-decision-DNNF compiler in Algorithm 6, which
can serve as a d-DNNF compiler in practice since decision-DNNF  d-DNNF. This means
that once the search finishes, we can answer in polynomial time any query about the input
propositional formula, as long as that query is known to be tractable for the language of dDNNF (see Table 1). On the other hand, Algorithm 6 will not be able to finish in polynomial
time on any propositional theory that does not have a polynomial-size representation in dDNNF (and decision-DNNF), no matter what variable ordering and decomposition method
it uses.
Again, one needs to implement some form of formula caching to make Algorithm 6 a
practical compiler. Several caching methods have been proposed for d-DNNF compilation,
the latest and most effective of which appeared in Darwiche (2004). However, we refer the
reader to Darwiche (2001) for a caching scheme that is specific to a decomposition method
based on what is known as dtrees (which we discuss next). This scheme is not as effective
as the one in Darwiche (2004) in that the former may miss some equivalences that would be
caught by the latter, yet it allows one to show that the space and time complexity of Algorithm 6, with this caching scheme in force, will be exponential only in the treewidth of the
CNF formula (as compared to pathwidth and cutwidth in OBDD compilation as discussed
in Section 4.2). Considering that model counting is a linear-time query supported by the
d-DNNF language, the results of Darwiche (2001) also imply that DPLL with decomposition (such as Algorithm 6) can be used to count models with a time and space complexity
that is exponential only in the treewidth of the CNF formula; see Bacchus et al. (2003a) for
an alternative derivation of this complexity result. Interestingly, no similar structure-based
measure of complexity appears to be known for FBDD compilation.
Finally, we would like to briefly discuss a distinction between two possible methods of
decomposition. Algorithm 6 suggests a dynamic notion of decomposition, where disjoint
components will be recognized after each variable split. This dynamic decomposition was
initially proposed and utilized by Bayardo and Pehoushek (2000) for model counting and
adopted by more recent model counters (Sang et al., 2004, 2005). Darwiche (2002, 2004)
proposed another method for performing the decomposition less dynamically by preprocessing the CNF formula to generate a dtree (decomposition tree), which is a binary tree
whose leaves correspond to clauses of the CNF formula. Each node in the dtree defines
a set of variables, called the cutset, whose instantiation is guaranteed to decompose the
CNF formula below that node into disjoint components. The rationale is that the cost of
dynamically computing a partition (Line 5 of Algorithm 6) many times during the search
is now replaced with the lesser cost of computing a static and recursive partition once and
for all. This method of decomposition allows one to provide structure-based computational
guarantees as discussed above. Moreover, the instantiation of variables in each cutset can be
performed dynamically, by utilizing dynamic variable ordering heuristics as typically done
in SAT solvers. The use of dtrees, combined with dynamic variable ordering, leads to an
almost static behavior on highly structured problems, for which cutsets are small. Yet, one
207

fiHuang & Darwiche

sees a more dynamic behavior on less structured problems, such as random 3-SAT, where
the cutsets are relatively large and dynamic variable ordering tends to dominate. Interestingly, the static behavior of dtrees (low overhead) can be orders of magnitude more efficient
than purely dynamic behavior on structured benchmarks, including the ISCAS85 circuits.
On the other hand, the dynamic behavior of dtrees can lead to very competitive results on
unstructured benchmarks, including random 3-SAT. One may obtain results to this effect
by running the model counter of Sang et al. (2004), Cachet Version 1.1, against the d-DNNF
compiler of Darwiche (2004), c2d Version 2.2, on relevant benchmarks. It should be noted
that the two programs differ in other aspects, but the decomposition method appears to be
one of the major differences.
4.4 Harnessing Search Techniques for Knowledge Compilation
Research in recent years has greatly improved the efficiency and scalability of systematic
search methods, particularly those for the problem of propositional satisfiability. Techniques contributing to this improvement include sophisticated conflict analysis, dependencydirected backtracking, clause learning, new variable ordering heuristics, and data structures
for faster constraint propagation, among other things (Marques-Silva & Sakallah, 1996;
Marques-Silva, 1999; Aloul, Markov, & Sakallah, 2001; Moskewicz, Madigan, Zhao, Zhang,
& Malik, 2001; Zhang, Madigan, Moskewicz, & Malik, 2001; Goldberg & Novikov, 2002;
Zhang & Malik, 2002; Heule & van Maaren, 2004). On the other hand, we have described in
this section a uniform framework where systematic search algorithms can be converted into
knowledge compilers by exhausting the search space and recording the trace of the search.
Such a framework affords an opportunity for many of the successful search techniques to
carry over to knowledge compilation. We refer the reader to Bayardo and Pehoushek (2000),
Darwiche (2002, 2004), Huang and Darwiche (2005b), and Sang et al. (2004, 2005) for detailed discussions of issues that arise from the implementation of these techniques when
search is extended to exhaustion, when the trace of the search needs to be stored, and when
decomposition is introduced.
Finally, we note that the efficiency of the search as addressed by these and other techniques (including caching in particular) is an important practical issue, but is orthogonal
to the language generated by the search, the main focus of the present paper. As a simple
example, one may have two versions of Algorithm 5 that have drastically different running
times on the same input because one of them has a much better caching method, but at the
end of the day, they are bound to produce exactly the same OBDD due to the canonicity
of OBDDs. As another example, a learned clause provided by conflict analysis can reduce
the number of search nodes, but of itself will not affect the final DAG (circuit) generated,
because the nodes avoided all correspond to contradictions (the constant 0) and would not
appear in the DAG anyway due to the reduction rules.

5. Power and Limitations of Exhaustive Search Algorithms
In Sections 3 and 4 we have established the notion of interpreting the trace of an exhaustive
search as a circuit, and then mapping the search algorithm itself to a propositional language
consisting of all its possible traces. This notion provides a new perspective on the intrinsic
power and limitations of these search algorithms, which we have illustrated in Section 4 by
208

fiThe Language of Search

or
and

or

and
and

A

B

C
or
and
and

C

or

A B A B
(a) General determinism

A

and
B

(b) Nondeterminism

Figure 6: DPLL is unable to produce general determinism or nondeterminism.
discussing the usefulness of the search algorithms as knowledge compilers and the inherent
inefficiency of the same on certain classes of inputs. In this section we formalize these
concepts and further illustrate them using examples from real implementations of exhaustive
search algorithms.
Consider an arbitrary exhaustive search algorithm based on variable splitting, call it
DPLLx , and suppose that its traces form a propositional language Lx . The intrinsic power
and limitations of DPLLx can then be identified by the following two principles, respectively:
1. If DPLLx runs in polynomial time on a class of formulas, then DPLLx (with its trace
recorded) can answer in polynomial time any query on these formulas that is known
to be tractable for language Lx .
2. DPLLx will not run in polynomial time on formulas for which no polynomial-size
representations exist in Lx .
Take for example the model counters recently proposed by Bayardo and Pehoushek
(2000) and Sang et al. (2004, 2005), which employ the techniques of decomposition and
(the latter also) formula caching. A simple analysis of these model counters shows that
their traces are in the language of decision-DNNF.3 Now consider the query of testing
whether the minimization of a theory  implies a particular clause , min() |= , where
min() is defined as a theory whose models are exactly the minimum-cardinality models of
. This query is at the heart of diagnostic and nonmonotonic reasoning and is known to be
tractable if  is in d-DNNF. Applying the first principle above, and noting that decisionDNNF  d-DNNF, we conclude that this query can be answered in polynomial time for any
class of formulas on which the model counters of Bayardo and Pehoushek (2000) and Sang
et al. (2004, 2005) have a polynomial time complexity. Similarly, a probabilistic equivalence
test can be performed in polynomial time for formulas on which these models counters have
a polynomial time complexity.
As an example of the second principle above, first note that neither of these same
model counters will finish in polynomial time on formulas for which no polynomial-size
representations exist in decision-DNNF. Furthermore, recall that decision-DNNF as defined
in Definition 2 is a strict subset of d-DNNF: Every disjunction in a decision-DNNF circuit
3. See the DDP algorithm of Bayardo and Pehoushek (2000) and Table 1 of Sang et al. (2004). For
example, the variable splitting (Lines 59) in Table 1 of Sang et al. (2004) corresponds to the generation
of a decision node, and the ToComponents function (Line 6 and Line 8) corresponds to the generation
of an and-node that satisfies decomposability.

209

fiHuang & Darwiche

has the form (x  )  (x  ), while d-DNNF allows disjunctions of the form    where
   is logically inconsistent, yet  and  do not contradict each other on any particular
variable (Figure 6a gives one example). Recall also that the model counting query remains
tractable when one generalizes from decision-DNNF to d-DNNF. If decision-DNNF turns
out to be not as succinct as d-DNNF, therefore, one may find another generation of model
counters, as well as d-DNNF compilers that can be exponentially more efficient than the
current ones.
Finally, we note that DPLL traces are inherently bound to be NNF circuits that are
both deterministic and decomposable. Decomposability alone, however, is sufficient for the
tractability of such important tasks as clausal entailment testing, existential quantification of variables, and cardinality-based minimization (Darwiche & Marquis, 2002). DPLL
cannot generate traces in DNNF that are not in d-DNNF (Figure 6b for example), since
variable splitting (the heart of DPLL) amounts to enforcing determinism. It is the property of determinism that provides the power needed to do model counting (#SAT), which
is essential for applications such as probabilistic reasoning. But if one does not need this
power, then one should go beyond DPLL-based procedures; otherwise one would be solving
a harder computational problem than is necessary.

6. Relation to Previous Work
We have employed the notion of the trace of a search in this paper to provide a theoretical
and practical channel between advances in systematic search and those in knowledge compilation. This channel has indeed been active for some time, but implicitly and mostly in one
direction: systematic search algorithms being employed to compile knowledge bases (see,
for example, Darwiche, 2002, 2004; Huang & Darwiche, 2005b; Darwiche, 2005). In fact,
the techniques of variable ordering, decomposition, and caching have all been extensively
used in these bodies of work, just as they are being used (some more recently) in pure search
algorithms.
A key contribution of this paper is then in formally explicating this notion of search
trace, and proposing it as the basis for a systematic framework for compiling knowledge
bases into subsets of NNF. This is to be contrasted with earlier systematic studies on
NNF (Darwiche & Marquis, 2002), as those were concerned with describing the various
properties of compiled NNF representations without delving into the algorithmic nature
of generating them. Another key contribution of this paper is in activating the second
direction of the search/compilation channel: looking at the language membership of search
traces to formally characterize the power and limitations of various search algorithms.
There has been a recent line of work, centering on the notion of AND/OR search, which
is also concerned with understanding the power and limitations of various search techniques,
such as decomposition and caching, by measuring the size of explored search spaces (Dechter
& Mateescu, 2004b, 2004a; Marinescu & Dechter, 2005). The premise of this work is that
while traditional search algorithms (based on branching) can be thought of as exploring an
ORsearch space, more recent search algorithms (employing decomposition) can be thought
of as exploring an AND/ORsearch space. Here, the space of an AND/OR search is characterized by a graph (or tree if no caching is used) with alternating layers of AND-nodes and
OR-nodes, the former representing decomposition and the latter branching. The algorithms
210

fiThe Language of Search

for exploring an AND/ORsearch space therefore exhibit a behavior similar to that of Algorithm 6, used in compiling CNF to d-DNNF (Darwiche, 2002, 2004, 2005)), except that
Algorithm 6 records the AND/ORsearch space explicitly as a circuit. Hence, AND/OR
search and Algorithm 6 share the same limitations we discussed of DPLL: They cannot take
advantage of the general notion of determinism, or rid themselves of determinism altogether.
We note here that the proposed algorithms for AND/OR search proceed by instantiating variables and performing decomposition according to a pseudo tree (Freuder & Quinn,
1985), while earlier work on d-DNNF compilation uses a decomposition tree for those two
tasks (corresponding to the choices on Lines 5 and 14 of Algorithm 6). Pseudo trees and decomposition trees are similar in that they both provide a scheme in which the instantiation
of a certain set of variables will lead to decomposition of the problem. In the framework
proposed in this paper, however, we make no commitment to either the decomposition or
the variable ordering scheme, as they are not relevant to our discussion. One should note
though that a commitment to any particular decomposition or variable ordering scheme
can have significant practical implications. Specifically, a search algorithm whose trace is
in d-DNNF may be performing variable ordering and decomposition in such a way as to
prohibit the possibility of generating certain (space efficient) d-DNNF circuits.
Finally, one can identify a major difference between AND/OR search and d-DNNF
compilation, in terms of their handling of queries: Each execution of an AND/OR search
algorithm answers only a single query, while executing a d-DNNF compilation algorithm
results in a compact structure that can be used repeatedly to answer all queries (for the
same knowledge base) that are known to be tractable. As discussed earlier, traversing a
compiled structure can be potentially much more efficient than repeating the search that
produced it. From this point of view, the separation of search from the actual reasoning
task provides the benefit of amortizing the search effort over a potentially large number
of queries. It also provides, as we have discussed, a systematic methodology by which
independent advances in search can be harnessed to improve the performance of automated
reasoning systems.

7. Experimental Results
By way of experimentation, we ran implementations of Algorithm 5 using the MINCE variable ordering heuristic (Aloul et al., 2001), Algorithm 4 using the VSIDS variable ordering
heuristic (Moskewicz et al., 2001), and Algorithm 6 using static decomposition by hypergraph partitioning (Darwiche & Hopkins, 2001), to compile a set of CNF formulas into
OBDD, FBDD, and d-DNNF, respectively (implementation details of the first and third
programs can be found in Huang & Darwiche, 2005b and Darwiche, 2004). The goal of
these experiments is to show the practicality of the search-based compilation framework
and to illustrate the improvement of language succinctness in response to the relaxation of
constraints on the search process. The benchmarks we used include random 3-CNF and
graph coloring problems from Hoos and Stutzle (2000), and a set of ISCAS89 circuits.
The results of these experiments are shown in Table 3, where the running times are
given in seconds based on a 2.4GHz CPU. The size of the compilation reflects the number
of edges in the NNF DAG. A dash indicates that the compilation did not succeed given
the available memory (4GB) and a 900-second time limit. It can be seen that for most of
211

fiHuang & Darwiche

Table 3: Compiling CNF into OBDD, FBDD, and d-DNNF.
CNF
Name
uf75-01
uf75-02
uf75-03
uf100-01
uf100-02
uf100-03
uf200-01
uf200-02
uf200-03
flat75-1
flat75-2
flat75-3
flat100-1
flat100-2
flat100-3
flat200-1
flat200-2
flat200-3
s820
s832
s838.1
s953
s1196
s1238
s1423
s1488
s1494

Number of
Models
2258
4622
3
314
196
7064
112896
1555776
804085558
24960
774144
25920
684288
245376
11197440
5379314835456
13670940672
15219560448
8388608
8388608
73786976294838206464
35184372088832
4294967296
4294967296
2475880078570760549798248448
16384
16384

OBDD
Size
Time
10320
0.14
22476
0.15
450
0.02
2886
2.22
1554
0.91
12462
0.78
8364
651.04




23784
0.16
13374
0.28
84330
0.29
62298
0.78
88824
1.57
15486
0.15






1372536 72.99
1147272 76.55
87552
0.24
2629464 38.81
4330656 78.26
3181302 158.84


6188427 50.35
3974256 31.67

FBDD
Size
Time
3684
0.02
14778
0.04
450
0.02
2268
0.01
1164
0.07
9924
0.12
7182
35.93
12900
33.72
662238 56.61
10758
0.04
8844
0.04
26472
0.07
37704
0.10
39882
0.30
21072
0.09


134184 7.07
358092 4.13
364698 0.69
362520 0.70


1954752 4.01
4407768 12.49
4375122 12.14


388026 1.14
374760 1.07

d-DNNF
Size
Time
822
0.02
1523
0.03
79
0.01
413
0.02
210
0.04
1363
0.02
262
3.66
744
2.64
86696 10.64
2273
0.01
1838
0.01
4184
0.04
3475
0.03
6554
0.09
2385
0.02
184781 56.86
9859 23.81
9269
3.28
23347
0.07
21395
0.05
12148
0.02
85218
0.26
206830 0.44
293457 0.94
738691 4.75
51883
0.19
55655
0.18

these instances, the compilation was the smallest in d-DNNF, then FBDD, then OBDD; a
similar relation can be observed among the running times. Also, the number of instances
successfully compiled was the largest for d-DNNF, then FBDD, then OBDD. This tracks
well with the theoretical succinctness relations of the three languages. (However, note that
FBDD and d-DNNF are not canonical representations and therefore compilations smaller
than reported here are perfectly possible; smaller OBDD compilations are, of course, also
possible under different variable orderings.)
We close this section by noting that the implementations of these knowledge compilers
bear witness to the advantage of the search-based compilation framework we have described
in Section 4. The first compiler is based on an existing SAT solver (Moskewicz et al.,
2001), and the other two on our own implementation of DPLL, all three benefiting from
techniques that have found success in SAT, including conflict analysis, clause learning, and
data structures for efficient detection of unit clauses.

212

fiThe Language of Search

8. Conclusion
This work is concerned with a class of exhaustive search algorithms that are run on propositional knowledge bases. We proposed a novel methodology whereby the trace of a search is
identified with a combinational circuit and the search algorithm itself is mapped to a propositional language consisting of all its possible traces. This mapping leads a uniform and
practical framework for compilation of propositional knowledge bases into various languages
of interest, and at the same time provides a new perspective on the intrinsic power and limitations of exhaustive search algorithms. As interesting examples, we unveiled the hidden
power of several recent model counters, discussed one of their potential limitations, and
pointed out the inability of this class of algorithms to produce traces without the property
of determinism, which limits their power from a knowledge compilation point of view. We
discussed the generality of some of of our results in relation to recent work on AND/OR
search. Finally, we presented experimental results to demonstrate the practicality of the
search-based knowledge compilation framework and to illustrate the variation of language
succinctness in response to the variation of the search strategy.

Acknowledgments
Parts of this work have been presented in DPLL with a Trace: From SAT to Knowledge
Compilation, in Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), 2005, pages 156162. We thank Rina Dechter and the anonymous reviewers
for their feedback on earlier drafts of this paper. This work was partially supported by
NSF grant IIS-9988543, MURI grant N00014-00-1-0617, and JPL/NASA contract 442511DA-57765. National ICT Australia is funded by the Australian Governments Backing
Australias Ability initiative, in part through the Australian Research Council.

Appendix A
Proof of Theorem 1
We first point out that the recursion is guaranteed to terminate as each recursive call
(Line 6) is accompanied by the disappearance of one variable and eventually either Line 2
or Line 4 will execute. Assuming the compact drawing of decision nodes as in Figure 1c,
we now show that every DAG returned by Algorithm 2 is an FBDD, and that every FBDD
can be generated by some execution of Algorithm 2.
Part I: There are three return statements in Algorithm 2: Lines 2, 4, and 6. The single
nodes returned on Lines 2 and 4 are trivial FBDDs. The DAG returned by get-node on
Line 6, call it G, is a decision node by induction on the level of recursion. Hence it remains
to show that G satisfies the test-once property (equivalent to decomposability in this case as
we discussed in Section 2.1), which is true because on Line 6, variable x has been replaced
with constants before the two recursive calls, and hence cannot appear in the two graphs
that are supplied as the second and third arguments to get-node. Finally, FBDDs returned
by the algorithm are guaranteed to be reduced by the use of the unique nodes technique in
the get-node function (Brace et al., 1991).

213

fiHuang & Darwiche

Part II: Let G denote an arbitrary (reduced) FBDD, and also its root node. Define the
following function (G) that returns a CNF formula (as a set of clauses) for every G:


{the empty clause},




{},
(G) 

 {x  c | c  (G.low)}  {x  c | c  (G.high)},



if G is the 0-sink;
if G is the 1-sink;
otherwise, where node G
is labeled with variable x.

On the last line of the definition above, assume that the literal x (and x) is always appended
to the front of the clause c. Now execute Algorithm 2 on (G), and on Line 5 always choose
the first variable of any clause (the first variable of every clause must be the same). The
DAG returned will be isomorphic to G.
Proof of Theorem 2
We show that every DAG returned by Algorithm 5 is an OBDD, and that every OBDD can
be generated by some execution of Algorithm 5.
Part I: First, any DAG returned by Algorithm 5 is an FBDD as Algorithm 5 is a
restricted version of Algorithm 2. Second, the DAG is an OBDD because any nonterminal
node N in the DAG must be constructed on Line 9, and therefore by virtue of Line 8 satisfies
the following property: The variable x that labels N appears before all other variables in the
two subgraphs below N in the variable order . Finally, OBDDs returned by the algorithm
are guaranteed to be reduced by the use of the unique nodes technique in the get-node
function (Brace et al., 1991).
Part II: Given an arbitrary (reduced) OBDD G, let  be the variable order of G. Now
using (G) as defined in the previous proof, execute Algorithm 5 on ((G), ). The DAG
returned will be isomorphic to G.
Proof of Theorem 3
We show that every DAG returned by Algorithm 6 is a decision-DNNF circuit, and that
every decision-DNNF circuit can be generated by some execution of Algorithm 6.
Part I: Nodes returned by get-and-node on Line 10 are conjunction nodes that satisfy
decomposability because the components identified on Line 5 do not share variables. Nodes
returned by get-node on Line 15 are disjunction nodes that have the form of Figure 1c by
the definition of get-node. Therefore the whole DAG is in decision-DNNF. Finally, decisionDNNF circuits returned by the algorithm are guaranteed to be reduced by the use of the
unique nodes technique in the get-node and get-and-node functions.
Part II: Let G denote an arbitrary (reduced) decision-DNNF circuit. Again assuming the
compact drawing of decision nodes as in Figure 1c, we now expand the previous definition
of (G) as follows:


{the empty clause},




{},


 [

(G) 

(Gi ),


i




{x  c | c  (G.low)}  {x  c | c  (G.high)},



214

if G is the 0-sink;
if G is ^
the 1-sink;
if G is
Gi ;
i

otherwise, where node G
is labeled with variable x.

fiThe Language of Search

As in the previous definition of (G), assume that on the last line of the definition above,
the literal x (and x) is always appended to the front of the clause c. Now, let each literal
of each clause be associated with a (possibly empty) list of colors as follows: The lists
are initially empty for the literals x and x introduced on the last line of the definition; on
the third line of the definition, assign a distinct color to each of the sets being unioned, and
in each set, append the assigned color to the head of the list of colors for the first literal of
every clause. Now execute Algorithm 6 on (G) and resolve the nondeterministic choices
on Line 5 (decomposition) and Line 14 (variable selection) as follows: If the first literal of
any clause has a nonempty list of colors, then (the first literal of every clause must have a
nonempty list of colors) let Line 5 partition the set of clauses according to the first color
of their first literal and remove that color from the first literal of every clause; otherwise,
(the first literal of every clause must mention the same variable) let Line 5 return a single
partition and on Line 14 choose the first variable of any clause. The DAG returned will be
isomorphic to G.

References
Aloul, F., Markov, I., & Sakallah, K. (2001). Faster SAT and smaller BDDs via common
function structure. In International Conference on Computer Aided Design (ICCAD),
pp. 443448.
Arvelo, Y., Bonet, B., & Vidal, M.-E. (2006). Compilation of query-rewriting problems
into tractable fragments of propositional logic. In Proceedings of the 21st National
Conference on Artificial Intelligence (AAAI).
Bacchus, F., Dalmao, S., & Pitassi, T. (2003a). Algorithms and complexity results for
#SAT and Bayesian inference. In 44th Annual IEEE Symposium on Foundations of
Computer Science (FOCS), pp. 340351.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003b). DPLL with caching: A new algorithm for
#SAT and Bayesian inference. Electronic Colloquium on Computational Complexity
(ECCC), 10 (003).
Barrett, A. (2004). From hybrid systems to universal plans via domain compilation. In Proceedings of the 14th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 4451.
Barrett, A. (2005). Model compilation for real-time planning and diagnosis with feedback.
In Proceedings of the 19th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 11951200.
Barwise, J. (Ed.). (1977). Handbook of Mathematical Logic. North-Holland.
Bayardo, R., & Pehoushek, J. (2000). Counting models using connected components. In
Proceedings of the 17th National Conference on Artificial Intelligence (AAAI), pp.
157162.
Berre, D. L., & Simon, L. (2005).
http://www.satcompetition.org/.

The

Annual

SAT

Competitions.

Birnbaum, E., & Lozinskii, E. (1999). The good old Davis-Putnam procedure helps counting
models. Journal of Artificial Intelligence Research, 10, 457477.
215

fiHuang & Darwiche

Blum, M., Chandra, A. K., & Wegman, M. N. (1980). Equivalence of free Boolean graphs
can be decided probabilistically in polynomial time. Information Processing Letters,
10 (2), 8082.
Bonet, B., & Geffner, H. (2006). Heuristics for planning with penalties and rewards using compiled knowledge. In Proceedings of the Tenth International Conference on
Principles of Knowledge Representation and Reasoning (KR), pp. 452462.
Brace, K. S., Rudell, R. L., & Bryant, R. E. (1991). Efficient implementation of a BDD
package. In Proceedings of the 27th Design Automation Conference (DAC), pp. 4045.
Bryant, R. E. (1986). Graph-based algorithms for Boolean function manipulation. IEEE
Transactions on Computers, C-35, 677691.
Bryant, R. E. (1991). On the complexity of VLSI implementations and graph representations
of Boolean functions with application to integer multiplication. IEEE transactions on
Computers, 40, 205213.
Cadoli, M., & Donini, F. M. (1997). A survey on knowledge compilation. AI Communications, 10, 137150.
Chauhan, P., Clarke, E. M., & Kroening, D. (2003). Using SAT based image computation
for reachability analysis. Tech. rep. CMU-CS-03-151, School of Computer Science,
Carnegie Mellon University.
Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks with local structure.
In Proceedings of the 19th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 13061312.
Chavira, M., Darwiche, A., & Jaeger, M. (2006). Compiling relational Bayesian networks
for exact inference. International Journal of Approximate Reasoning, 42 (1-2), 420.
Coste-Marquis, S., Berre, D. L., Letombe, F., & Marquis, P. (2005). Propositional fragments
for knowledge compilation and quantified Boolean formulae.. In Proceedings of the
20th National Conference on Artificial Intelligence (AAAI), pp. 288293.
Darwiche, A. (2001). On the tractability of counting theory models and its application
to belief revision and truth maintenance. Journal of Applied Non-Classical Logics,
11 (1-2), 1134.
Darwiche, A. (2002). A compiler for deterministic decomposable negation normal form.
In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI), pp.
627634.
Darwiche, A. (2004). New advances in compiling CNF into decomposable negation normal form. In Proceedings of the 16th European Conference on Artificial Intelligence
(ECAI), pp. 328332.
Darwiche, A. (2005). The c2d compiler user manual. Tech. rep. D-147, Computer Science
Department, UCLA. http://reasoning.cs.ucla.edu/c2d/.
Darwiche, A., & Hopkins, M. (2001). Using recursive decomposition to construct elimination
orders, jointrees and dtrees. In Trends in Artificial Intelligence, Lecture notes in AI,
2143, pp. 180191. Springer-Verlag.

216

fiThe Language of Search

Darwiche, A., & Huang, J. (2002). Testing equivalence probabilistically. Tech. rep. D-123,
Computer Science Department, UCLA.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial
Intelligence Research, 17, 229264.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem proving.
Journal of the ACM, (5)7, 394397.
Dechter, R., & Mateescu, R. (2004a). The impact of AND/OR search spaces on constraint
satisfaction and counting. In Proceedings of the 10th International Conference on
Principles and Practice of Constraint Programming (CP), pp. 731736.
Dechter, R., & Mateescu, R. (2004b). Mixtures of deterministic-probabilistic networks and
their AND/OR search spaces. In Proceedings of the 20th Conference on Uncertainty
in Artificial Intelligence (UAI), pp. 120129.
del Val, A. (1994). Tractable databases: How to make propositional unit resolution complete through compilation. In Proceedings of the Fourth International Conference on
Principles of Knowledge Representation and Reasoning (KR), pp. 551561.
del Val, A. (1995). An analysis of approximate knowledge compilation. In Proceedings of the
14th International Joint Conference on Artificial Intelligence (IJCAI), pp. 830836.
Elliott, P., & Williams, B. (2006). DNNF-based belief state estimation. In Proceedings of
the 21st National Conference on Artificial Intelligence (AAAI).
Forbus, K. D., & de Kleer, J. (1993). Building Problem Solvers. MIT Press.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage of stable sets of variables in
constraint satisfaction problems. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (IJCAI), pp. 10761078.
Gergov, J., & Meinel, C. (1994). Efficient analysis and manipulation of OBDDs can be
extended to FBDDs. IEEE Transactions on Computers, 43 (10), 11971209.
Goldberg, E., & Novikov, Y. (2002). BerkMin: A fast and robust SAT-solver. In Design
Automation and Test in Europe (DATE), pp. 142149.
Grumberg, O., Schuster, A., & Yadgar, A. (2004). Memory efficient all-solutions SAT solver
and its application for reachability analysis. In Proceedings of the 5th International
Conference on Formal Methods in Computer-Aided Design (FMCAD), pp. 275289.
Heule, M., & van Maaren, H. (2004). Aligning CNF- and equivalence reasoning. In Proceedings of the Seventh International Conference on Theory and Applications of Satisfiability Testing (SAT).
Hoos, H. H., & Stutzle, T. (2000). SATLIB: An online resource for research on SAT. In
I.P.Gent, H.v.Maaren, T.Walsh, editors, SAT 2000, pp. 283292. IOS Press. SATLIB
is available online at www.satlib.org.
Huang, J. (2006). Combining knowledge compilation and search for conformant probabilistic
planning. In Proceedings of the 16th International Conference on Automated Planning
and Scheduling (ICAPS), pp. 253262.

217

fiHuang & Darwiche

Huang, J., & Darwiche, A. (2005a). On compiling system models for faster and more scalable
diagnosis. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI), pp. 300306.
Huang, J., & Darwiche, A. (2005b). Using DPLL for efficient OBDD construction. In
Seventh International Conference on Theory and Applications of Satisfiability Testing,
SAT 2004, Revised Selected Papers, Vol. 3542 of Lecture Notes in Computer Science,
pp. 157172.
Karnaugh, M. (1953). The map method for synthesis of combinational logic circuits. Transactions of the AIEE, 72 (9), 593599.
Majercik, S. M., & Littman, M. L. (1998). Using caching to solve larger probabilistic
planning problems. In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI), pp. 954959.
Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for graphical models.
In Proceedings of the 19th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 224229.
Marques-Silva, J., & Sakallah, K. (1996). GRASPA new search algorithm for satisfiability.
In Proceedings of the International Conference on Computer Aided Design (ICCAD),
pp. 220227.
Marques-Silva, J. (1999). The impact of branching heuristics in propositional satisfiability
algorithms. In Proceedings of the 9th Portuguese Conference on Artificial Intelligence,
pp. 6274.
Marquis, P. (1995). Knowledge compilation using theory prime implicates. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pp. 837
843.
McMillan, K. (1993). Symbolic Model Checking. Kluwer Academic.
McMillan, K. L. (2002). Applying SAT methods in unbounded symbolic model checking.
In Proceedings of the 14th International Conference on Computer Aided Verification
(CAV), pp. 250264.
Meinel, C., & Theobald, T. (1998). Algorithms and Data Structures in VLSI Design: OBDD
Foundations and Applications. Springer.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
an efficient SAT solver. In Proceedings of the 38th Design Automation Conference
(DAC), pp. 530535.
Palacios, H., Bonet, B., Darwiche, A., & Geffner, H. (2005). Pruning conformant plans
by counting models on compiled d-DNNF representations. In Proceedings of the 15th
International Conference on Automated Planning and Scheduling (ICAPS), pp. 141
150.
Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component
caching and clause learning for effective model counting. In Proceedings of the Seventh
International Conference on Theory and Applications of Satisfiability Testing (SAT),
pp. 2028.
218

fiThe Language of Search

Sang, T., Beame, P., & Kautz, H. (2005). Heuristics for fast exact model counting. In
Proceedings of the Eighth International Conference on Theory and Applications of
Satisfiability Testing (SAT), Lecture Notes in Computer Science, pp. 226240.
Selman, B., & Kautz, H. (1991). Knowledge compilation using Horn approximation. In
Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI), pp.
904909.
Selman, B., & Kautz, H. (1996). Knowledge compilation and theory approximation. Journal
of the ACM, 43 (2), 193224.
Siddiqi, S., & Huang, J. (2007). Hierarchical diagnosis of multiple faults. In Proceedings of
the 20th International Joint Conference on Artificial Intelligence (IJCAI), pp. 581
586.
Somenzi, F. (2004).
CUDD: CU Decision Diagram Package, Release 2.4.0.
http://vlsi.colorado.edu/fabio/CUDD/cuddIntro.html.
Urquhart, A. (1995). The complexity of propositional proofs. Bulletin of Symbolic Logic,
1 (4), 425467.
Wegener, I. (2000). Branching Programs and Binary Decision Diagrams: Theory and Applications. SIAM Monographs on Discrete Mathematics and Applications.
Zhang, L., Madigan, C., Moskewicz, M., & Malik, S. (2001). Efficient conflict driven learning
in a Boolean satisfiability solver. In Proceedings of the International Conference on
Computer Aided Design (ICCAD), pp. 279285.
Zhang, L., & Malik, S. (2002). The quest for efficient Boolean satisfiability solvers. In
Proceedings of the 18th International Conference on Automated Deduction (CADE),
Lecture Notes in Computer Science, pp. 295313.

219

fiJournal of Artificial Intelligence Research 29 (2007) 421-489

Submitted 8/06; published 8/07

An Algebraic Graphical Model for Decision with
Uncertainties, Feasibilities, and Utilities
Cedric Pralet

cedric.pralet@onera.fr

ONERA Toulouse, France
2 av. Edouard Belin, 31400 Toulouse

Gerard Verfaillie

gerard.verfaillie@onera.fr

ONERA Toulouse, France
2 av. Edouard Belin, 31400 Toulouse

Thomas Schiex

thomas.schiex@toulouse.inra.fr

INRA Toulouse, France
Chemin de Borde Rouge, 31320 Castanet-Tolosan

Abstract
Numerous formalisms and dedicated algorithms have been designed in the last decades
to model and solve decision making problems. Some formalisms, such as constraint networks, can express simple decision problems, while others are designed to take into account uncertainties, unfeasible decisions, and utilities. Even in a single formalism, several
variants are often proposed to model different types of uncertainty (probability, possibility...) or utility (additive or not). In this article, we introduce an algebraic graphical model
that encompasses a large number of such formalisms: (1) we first adapt previous structures
from Friedman, Chu and Halpern for representing uncertainty, utility, and expected utility
in order to deal with generic forms of sequential decision making; (2) on these structures,
we then introduce composite graphical models that express information via variables linked
by local functions, thanks to conditional independence; (3) on these graphical models,
we finally define a simple class of queries which can represent various scenarios in terms
of observabilities and controllabilities. A natural decision-tree semantics for such queries
is completed by an equivalent operational semantics, which induces generic algorithms.
The proposed framework, called the Plausibility-Feasibility-Utility (PFU) framework, not
only provides a better understanding of the links between existing formalisms, but it also
covers yet unpublished frameworks (such as possibilistic influence diagrams) and unifies
formalisms such as quantified boolean formulas and influence diagrams. Our backtrack
and variable elimination generic algorithms are a first step towards unified algorithms.

1. Introduction
In the last decades, numerous formalisms have been developed to express and solve decision
making problems. In such problems, an agent must make decisions consisting of either
choosing actions and ways to fulfill them (as in action planning, task scheduling, or resource
allocation), or choosing explanations of observed phenomena (as in diagnosis or situation
assessment). These choices may depend on various parameters:
1. uncertainty measures, which we call plausibilities, may describe beliefs about the state
of the environment;
2. preconditions may have to be satisfied for a decision to be feasible;
c
2007
AI Access Foundation. All rights reserved.

fiPralet, Verfaillie, & Schiex

3. the possible states of the environment and decisions do not generally have the same
value from the decision makers point of view. Utilities can be expressed to model costs,
gains, risks, satisfaction degrees, hard requirements, and more generally, preferences;
4. when time is involved, decision processes may be sequential and the environment may
be partially observable. This means that there may be several decision steps, and that
the values of some variables may be observed between two steps, as in chess where
each player plays in turn and can observe the move of the opponent before playing
again;
5. there may be adversarial or collaborative decision makers, each of them controlling a
set of decisions. Hence, a multi-agent aspect can yield partial controllabilities.
Given the plausibilities defined over the states of the environment, the feasibility constraints on the decisions, the utilities defined over the decisions and the states of the environment, and given the possible multiple decision steps, the objective is to provide the decision
maker with optimal decision rules for the decision variables he controls, depending on the
environment and on other agents. To be concise, the class of such problems is denoted as
the class of sequential decision problems with plausibilities, feasibilities, and utilities.
Various formalisms have been designed to cope with problems of this class, sometimes
in a degenerated form (covering only a subset of the features of the general problem):
 formalisms developed in the boolean satisfiability framework: the satisfiability problem (SAT), quantified boolean formulas, stochastic SAT (Littman, Majercik, & Pitassi,
2001), and extended stochastic SAT (Littman et al., 2001);
 formalisms developed in the very close constraint satisfaction framework: constraint
satisfaction problems (CSPs, Mackworth, 1977), valued/semiring CSPs (Bistarelli,
Montanari, Rossi, Schiex, Verfaillie, & Fargier, 1999) (covering classical, fuzzy, additive, lexicographic, probabilistic CSPs), mixed CSPs and probabilistic mixed CSPs
(Fargier, Lang, & Schiex, 1996), quantified CSPs (Bordeaux & Monfroy, 2002), and
stochastic CSPs (Walsh, 2002);
 formalisms developed to represent uncertainty and extended to represent decision
problems under uncertainty: Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993) (also known as Gibbs networks), chain graphs (Frydenberg, 1990), hybrid or mixed networks (Dechter & Larkin, 2001; Dechter & Mateescu,
2004), influence diagrams (Howard & Matheson, 1984), unconstrained (Jensen & Vomlelova, 2002), asymmetric (Smith, Holtzman, & Matheson, 1993; Nielsen & Jensen,
2003), or sequential (Jensen, Nielsen, & Shenoy, 2004) influence diagrams, valuation
networks (Shenoy, 1992), and asymmetric (Shenoy, 2000) or sequential (Demirer &
Shenoy, 2001) valuation networks;
 formalisms developed in the classical planning framework, such as STRIPS planning (Fikes & Nilsson, 1971; Ghallab, Nau, & Traverso, 2004), conformant planning (Goldman & Boddy, 1996), and probabilistic planning (Kushmerick, Hanks, &
Weld, 1995);

422

fiThe PFU Framework

 formalisms such as Markov decision processes (MDPs), probabilistic, possibilistic,
or using Spohns epistemic beliefs (Spohn, 1990; Wilson, 1995; Giang & Shenoy,
2000), factored or not, possibly partially observable (Puterman, 1994; Monahan, 1982;
Sabbadin, 1999; Boutilier, Dean, & Hanks, 1999; Boutilier, Dearden, & Goldszmidt,
2000).
Many of these formalisms present interesting similarities:
 they include variables modeling the state of the environment (environment variables)
or the decisions (decision variables);
 they use sets of functions which, depending on the formalism considered, can model
plausibilities, feasibilities, or utilities;
 they use operators either to combine local information (such as  to aggregate probabilities under independence hypothesis, + to aggregate gains and costs), or to project
a global information (such as + to compute a marginal probability, min or max to
compute an optimal decision).
Even if the meaning of variables, functions, and combination or projection operators
may be specific to each formalism, they can all be seen as graphical models in the sense that
they exploit, implicitly or explicitly, a hypergraph of local functions between variables. This
article shows that it is possible to build a generic algebraic framework subsuming many of
these formalisms by reducing decision making problems to a sequence of so-called variable
eliminations on an aggregation of local functions.
Such a generic framework will be able to provide:
 a better understanding of existing formalisms: a generic framework has an obvious
theoretical and pedagogical interest, since it can bring to light similarities and differences between the formalisms covered and help people of different communities to
communicate on a common basis;
 increased expressive power : a generic framework may be able to capture problems
that cannot be directly modeled in any existing formalism. This increased expressiveness should be reachable by capturing the essential algebraic properties of existing
frameworks;
 generic algorithms: ultimately, besides a generic framework, it should be possible
to define generic algorithms capable of solving problems defined in this framework.
This objective fits into a growing effort to identify common algorithmic approaches
that were developed for solving different AI problems. It may also facilitate crossfertilization by allowing each subsumed framework to reuse algorithmic ideas defined
in another one.
1.0.1 Article Outline
After the introduction of some notations and notions, the article starts by showing, with a
catalog of existing formalisms for decision making, that a generic algebraic framework can
423

fiPralet, Verfaillie, & Schiex

be informally identified. This generic framework, called the Plausibility-Feasibility-Utility
(PFU) framework, is then formally introduced in three steps: (1) algebraic structures capturing plausibilities, feasibilities, and utilities are introduced (Section 4), (2) these algebraic structures are exploited to build a generic form of graphical model (Section 5), and
(3) problems over such graphical models are captured by the notion of queries (Section 6).
The framework is analyzed in Section 7 and generic algorithms are defined in Section 8.
A table recapitulating the main notations used is available in Appendix A and the proofs
of all propositions and theorems appear in Appendix B. A short version of the framework
described in this article has already been published (Pralet, Verfaillie, & Schiex, 2006c).

2. Background Notations and Definitions
The essential objects used in the article are variables, domains, and local functions (called
scoped functions).
Definition 1. The domain of values of a variable x is denoted dom(x) and for every
a  dom(x), (x, a) denotes the assignment of value a to x. By extension, for a set of
variables S, we denote
Q by dom(S) the Cartesian product of the domains of the variables in
S, i.e. dom(S) = xS dom(x). An element A of dom(S) is called an assignment of S.1
If A1 , A2 are assignments of disjoint subsets S1 , S2 , then A1 .A2 , called the concatenation
of A1 and A2 , is the assignment of S1  S2 where variables in S1 are assigned as in A1 and
variables in S2 are assigned as in A2 . If A is an assignment of a set of variables S, the
projection of A onto S 0  S is the assignment of S 0 where variables are assigned to their
value in A.
Definition 2. (Scoped function) A scoped function is a pair (S, ) where S is a set of
variables and  is a function mapping elements in dom(S) to a given set E. In the following,
we will often consider that S is implicit and denote a scoped function (S, ) as  alone. The
set of variables S is called the scope of  and is denoted sc(). If A is an assignment of a
superset of sc() and A0 is the projection of A onto sc(), we define (A) by (A) = (A0 ).
For example, a scoped function  mapping assignments of sc() to elements of the
boolean lattice B = {t, f } is analogous to a constraint describing the subset of dom(sc())
of authorized tuples in constraint networks.
From this, the general notion of graphical model can be defined:
Definition 3. (Graphical model) A graphical model is a pair (V, ) where V = {x1 , . . . , xn }
is a finite set of variables and  = {1 , . . . , m } is a finite set of scoped functions whose
scopes are included in V .
The terminology of graphical models is used here simply because a set of scoped functions
can be represented as a hypergraph that contains one hyperedge per function scope. As
we will see, this hypergraph captures a form of independence (see Section 5) and induces
parameters for the time and space complexity of our algorithms (see Section 8). This
definition of graphical models generalizes the usual one used in statistics, defining a graphical
1. An assignment of S = {x1 , . . . , xk } is actually a set of variable-value pairs {(x1 , a1 ), . . . , (xk , ak )}; we
assume that variables are implicit when using a tuple of values (a1 , . . . , ak )  dom(S).

424

fiThe PFU Framework

model as a (directed or not) graph where the nodes represent random variables and where
the structure captures probabilistic independence relations.
Local scoped functions in a graphical model give a space-tractable definition of a
global function defined by their aggregation. For example, in a Bayesian network (Pearl,
1988) a global probability distribution Px,y,z over x, y, z may be defined as the product
(using operator ) of a set of scoped functions {Px , Py|x , Pz|y }. Local scoped functions
can also facilitate the projection of the information expressed by a graphical model onto a
smaller scope. For example, in order to compute
P a marginal
P probability distribution Py,z
from the previous network, we can computeP x Px,y,z = ( x Px  Py|x )  Pz|y and avoid
taking Pz|y into account. Here the operator
is used to project information onto a smaller
scope by eliminating variable x. Operators used to combine scoped functions will be called
combination operators, while operators used to project information onto smaller scopes will
be called elimination operators.
Definition 4. (Combination) Let 1 , 2 be scoped functions to E1 and E2 respectively. Let
 : E1 E2  E be a binary operator. The combination of 1 and 2 , denoted by 1 2 , is
the scoped function to E with scope sc(1 )sc(2 ) defined by (1 2 )(A) = 1 (A)2 (A)
for all assignments A of sc(1 )  sc(2 ).  is called the combination operator of 1 and
2 .
In the rest of the article, all combination operators will be denoted .
Definition 5. (Elimination) Let  be a scoped function to E. Let op be an on E which
is associative, commutative, and has an identity element . The elimination of variable
x from  with op is a scoped function whose scope is sc()  {x} and whose value for an
assignment A of its scope is (opx )(A) = opadom(x) (A.(x, a)). In this context, op is called
the elimination operator for x. The elimination of a set of variables S = {x1 , . . . , xk } from
 is a function with scope sc()  S defined by opS (A) = opA0 dom(S) (A.A0 ).
P
Hence, when computing x (Px  Py|x  Pz|x ), scoped functions are aggregated using
the combination operator  =  and the information is projected by eliminating x using
the elimination operator +. In this article,  denotes elimination operators.
In some cases, the elimination of a set of variables S with an operator op from a scoped
function  should only be performed on a subset of dom(S) containing assignments that
satisfy some property denoted by a boolean scoped function F . Then, we must compute
for every A  dom(sc()  S) the value opA0 dom(S),F (A0 )=t (A.A0 ). For simplicity and
homogeneity, and in order to always use elimination over dom(S), we can equivalently
truncate  so that elements of dom(S) which do not satisfy F are mapped to a special
value (denoted ) which is itself defined as a new identity for op.
Definition 6. (Truncation operator) The unfeasible value  is a new special element that
is supposed to be outside of the domain E of every elimination operator op : E  E  E.
We explicitly extend every elimination operator op : E  E  E on E  {} by taking the
convention op(, e) = op(e, ) = e for all e  E  {}.
Let {t, f } be the boolean lattice. For any boolean b and any e  E, we define b ? e to be
equal to e if b = t and  otherwise. ? is called the truncation operator.

425

fiPralet, Verfaillie, & Schiex

Given a boolean scoped function F ,  and ? make it possible to write quantities like
opA0 dom(S),F (A0 )=t  as the elimination opS (F ? ).
In order to solve decision problems, one usually wants to compute functions mapping the
available information to a decision. The notion of decision rules will be used to formalize
this:
Definition 7. (Decision rule, policy) A decision rule for a variable x given a set of variables
S 0 is a function  : dom(S 0 )  dom(x) mapping each assignment of S 0 to a value in dom(x).
By extension, a decision rule for a set of variables S given a set of variables S 0 is a function
 : dom(S 0 )  dom(S). A set of decision rules is called a policy.
Definition 8. (Optimal decision rule) Consider a totally -ordered set E, a scoped function
 from dom(sc()) to E, and a set of variables S  sc(). A decision rule  : dom(sc() 
S)  dom(S). is said to be optimal iff, for all (A, A0 )  dom(sc()  S)  dom(S),
(A.(A))  (A.A0 ) (resp. (A.(A))  (A.A0 )). Such a decision rule always exists
when dom(sc()) is finite.
In other words, optimal decision rules are examples of decision rules given by argmin
and argmax (in this article, we consider that optimality on decision rules is always given by
min or max on some totally ordered set).
Definition 9. (Directed Acyclic Graph (DAG)) A directed graph G is a DAG if it contains
no directed cycle. When variables are used as vertices, paG (x) denotes the set of parents of
variable x in G.
Last, [1, n] will denote the set of integers i such that 1  i  n.

3. From Examples of Graphical Models to a Generic Framework
We now present different AI formalisms for expressing and solving decision problems. In
the most simple case, a single decision which maximizes utility is sought. The introduction
of plausibilities (uncertainties), unfeasible actions (feasibilities), and sequential decision
(several decision steps with some observations between decision steps) only appears in the
most sophisticated frameworks. The goal of this section is to show that these formalisms
can all be viewed as graphical models where specific elimination and combination operators
are used.
3.1 Examples of Graphical Models
The examples used cover various AI formalisms, which are briefly described. A wider and
more accurate review of existing graphical models could be provided (Pralet, 2006).
3.1.1 Constraint Networks
Constraint networks (CNs, Mackworth, 1977), often called constraint satisfaction problems
(CSPs), are graphical models (V, ) where the scoped functions in  are constraints mapping
assignments onto {t, f }. The usual query on a CN is to determine the existence of an

426

fiThe PFU Framework

assignment of V that satisfies all constraints. By setting f  t, this decision problem can
be answered by computing:


max   .
(1)
V



If this quantity equals true, then an optimal decision rule for V defines a solution. This
query can be answered by performing eliminations (using max) on a combination of scoped
functions (using ). Replacing the hard constraints in  by soft constraints (boolean scoped
functions being replaced by cost functions) and replacing  by an abstract operator  equal
to +, min, , . . . leads to queries on a valued and totally ordered semiring CN (Bistarelli
et al., 1999).
3.1.2 Bayesian Networks
Bayesian networks (BNs, Pearl, 1988) can model problems with plausibilities expressed as
probabilities. A BN is a graphical model (V, ) in which  is a set of local conditional
probability distributions:  = {Px | paG (x) , x  V }, where G is a DAG with vertices in V .
A BN represents a joint probability distribution P
QV over all variables as a combination of
local conditional probability distributions (PV = xV Px | paG (x) ), just as the combination
of local constraints in a CN defines a global constraint over all variables. One possible query
on a BN is to compute the marginal probability distribution of a variable y  V :
!
X
X
Y
Py =
PV =
Px | paG (x) .
(2)
V {y}

V {y}

xV

Equation 2 corresponds to variable eliminations (with +) on a product of scoped functions.
In other queries on BNs such as MAP (Maximum A Posteriori hypothesis), eliminations
with max are also performed.
3.1.3 Quantified Boolean Formulas and Quantified CNs
Quantified boolean formulas (QBFs) and quantified CNs (Bordeaux & Monfroy, 2002) can
model sequential decision problems. Let x1 , x2 , x3 be boolean variables. A QBF using the
so-called prenex conjunctive normal form looks like (with f  t):
x1 x2 x3 ((x1  x3 )  (x2  x3 )) = max min max((x1  x3 )  (x2  x3 )).
x1

x2

x3

(3)

Thus, the query Is there a value for x1 such that for all values of x2 , there exists a value
for x3 such that the clauses x1  x3 and x2  x3 are satisfied? can be answered as in
Equation 3 using a sequence of eliminations (max over x1 , min over x2 , and max over x3 )
on a conjunction of clauses. In a quantified CN, clauses are replaced by constraints.
3.1.4 Stochastic CNs
A stochastic CN (Walsh, 2002) can model sequential decision problems with probabilities as
plausibilities and hard requirements as utilities, provided that the decisions do not influence
the environment (the so-called contingency assumption). In a stochastic CN, two types of
427

fiPralet, Verfaillie, & Schiex

variables are defined: decision variables di and environment (stochastic) variables sj . A
global probability distribution over the environment variables is expressed as a combination
of local probability distributions. If the environment variables are mutually independent,
these local probability distributions are simply unary probability distributions Psj . Finally,
a stochastic CN defines a set of constraints {C1 , . . . , Cm } mapping tuples of values onto
{0, 1} (instead of {t, f }). This allows constraints to be multiplied with probabilities.
Consider a situation where first two decisions d1 and d2 are made, then an environment
variable s1 is observed, then decisions d3 and d4 are made, while an environment variable s2
remains unobserved. A possible query on a stochastic CN is to compute decision rules for
d1 , d2 , d3 , and d4 which maximize the expected constraint satisfaction, through Equation 4:
Q

X
X
C
(4)
(Ps1  Ps2 ) 
max
max
i[1,m] i .
d1 ,d2

s1

d3 ,d4

s2

The answer to the query defined by Equation 4 is determined by a sequence of eliminations
(max over the decision variables, + over the environment ones) on a combination of scoped
functions (probabilities are combined using , constraints are combined using , since they
are expressed onto {0, 1} instead of {t, f }, and probabilities are combined with constraints
using ).
3.1.5 Influence Diagrams
Influence diagrams (Howard & Matheson, 1984) can model sequential decision problems
with probabilities as plausibilities together with gains and costs as utilities. They can be
seen as an extension of BNs including the notions of decision and utility. More precisely,
an influence diagram is a composite graphical model defined by three sets of variables
organized in a DAG G: (1) a set S of chance variables; for each s  S, a conditional
probability distribution Ps | paG (s) of s given its parents in G is specified; (2) a set D of
decision variables; for each d  D, paG (d) is the set of variables observed before decision
d is made; (3) a set  = {u1 , . . . , um } of utility variables, each of which is associated with
a utility function Ui = UpaG (ui ) of scope paG (ui ). Utility variables P
must be leaves in the
DAG, and the utility functions define a global additive utility UG = i[1,m] Ui .
The usual problem associated with an influence diagram is to compute decision rules
maximizing the global expected utility. If we modify the example used for stochastic CNs
by replacing Ps1 by Ps1 | d2 , Ps2 by Ps2 | d1 ,d3 , and the constraints C1 , . . . , Cm by the additive
utility functions U1 , . . . , Um , then an optimal policy can be obtained by computing optimal
decision rules for d1 , d2 , d3 , and d4 in Equation 5:

X
X
 P
max
max
Ps1 | d2  Ps2 | d1 ,d3 
U
.
(5)
i
i[1,m]
d1 ,d2

s1

d3 ,d4

s2

Again, the answer to the query defined by Equation 5 can be computed by a sequence
of eliminations (alternating max- and sum-eliminations) on a combination of scoped functions (plausibilities combined using , utilities combined using +, plausibilities and utilities
combined using ).

428

fiThe PFU Framework

3.1.6 Valuation Networks
Valuation networks (Shenoy, 1992) can model sequential decision problems with plausibilities, feasibilities, and utilities, where plausibilities are combined using  and where utilities
are additive. A valuation network is composed of several sets of nodes and valuations: (1)
a set D of decision nodes, (2) a set S of chance nodes, (3) a set F of indicator valuations,
which specify unfeasible assignments of decision and chance variables, (4) a set P of probability valuations, which are multiplicative factors of a joint probability distribution over
the chance variables, and (5) a set
P U of utility valuations, representing additive factors
of a joint utility function UG =
Ui U Ui . Arcs between nodes are also used to define
the order in which decisions are made and chance variables are observed. If this order is
d1  d2  s1  d3  d4  s2 , it can be shown that optimal decision rules for d1 , d2 , d3 , d4
are defined through Equation 6:


 


X
X 
Y
X
  Fi ? 
max
max
Pi   
Ui  .
(6)
d1 ,d2

s1

d3 ,d4

Fi F

s2

Pi P

Ui U

Local feasibility constraints are combined using , and combined with other scoped functions using the truncation operator ? (cf. Definition 6).
3.1.7 Finite Horizon Markov Decision Processes
Finite horizon Markov Decision Processes (MDPs, Puterman, 1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000) model sequential decision problems with plausibilities and utilities over a horizon of T time-steps. For every time-step t, a variable st
represents the state of the environment and a variable dt represents the decision made after
observing st . In factored MDPs, several state variables may be used at each time-step.
In a probabilistic finite horizon MDP, plausibilities over the environment are described by
local probability distributions Pst+1 | st ,dt of being in state st+1 given st and dt . The utilities
over states and decisions are local additive rewards Rst ,dt , and boolean functions Fdt | st can
express whether a decision dt is feasible in state st . An optimal policy for each initial state
s1 can be computed by the following equation (which is a bit unusual for defining optimal
policies for a MDP, but is equivalent to the usual form):
!
!


max u max . . . u max
d1

s2

d2

sT

dT


t[1,T ]

Fdt |st

?

p Pst+1 |st ,dt
t[1,T [

pu

u Rst ,dt
t[1,T ]

.

(7)

Plausibilities are combined using p = , utilities are combined using u = +, plausibilities
and utilities are combined using pu = , decision variables are eliminated using max, and
environment variables are eliminated using u = +. The truncation operator ? enables the
elimination operators to ignore unfeasible decisions.
In a pessimistic possibilistic finite horizon MDP (Sabbadin, 1999), probability distributions Pst+1 | st ,dt are replaced by possibility distributions st+1 | st ,dt , rewards Rst ,dt are
replaced by preferences st ,dt , and the operators used are u = p = u = min and
pu : (p, u)  max(1  p, u).

429

fiPralet, Verfaillie, & Schiex

3.2 Towards a Generic Framework
The previous section shows that usual queries in various existing formalisms can be reduced
to a sequence of variable eliminations on a combination of scoped functions.
This observation has led to the definition of algebraic MDPs (Perny, Spanjaard, & Weng,
2005) or to the definition of valuation algebras (Shenoy, 1991; Kolhas, 2003), a generic
algebraic framework in which eliminations can be performed on a combination of scoped
functions. However, valuation algebras use only one combination operator, whereas several
combination operators may be needed to manipulate different types of scoped functions (as
previously shown). Moreover, valuation algebras can deal with only one type of elimination,
whereas several elimination operators may be required for handling the different types of
variables. In valuation networks (Shenoy, 2000), plausibilities are necessarily represented as
probabilities, and eliminations with min cannot be performed. Essentially, a more powerful
framework is needed.
In order to be simple and yet general enough to cover the queries defined by Equations 1
to 7, the generic form we should consider is:
!



!
Sov

 Fi

Fi F

?

p
Pi P

Pi

pu

u
Ui U

Ui

.

(8)

where (1)  is used to combine local feasibilities, p is used to combine plausibilities, u
is used to combine utilities, pu is used to combine plausibilities and utilities, and the
truncation operator ? is used to ignore unfeasible decisions without having to deal with
elimination operations over restricted domains;2 (2) F , P , U are (possibly empty) sets
of local feasibility, plausibility, and utility functions respectively; (3) Sov is an operatorvariable(s) sequence, indicating how to eliminate variables. Sov involves min or max to
eliminate decision variables and an operator u to eliminate environment variables.
Equation 8 is still very informal. To define it formally, and to provide it with clear
semantics, we need to define three key elements:
1. We must capture the essential properties of the combination operators p , u , pu
used respectively to combine plausibilities, utilities, and plausibilities with utilities.
We must also characterize the elimination operators u and p used to project information coming from utilities and plausibilities. These operators will define the
algebraic structure of the PFU (Plausibility-Feasibility-Utility) framework.
2. On this algebraic structure, we must define a generic form of graphical model, involving a set of variables and sets of scoped functions expressing plausibilities, feasibilities,
and utilities (sets P , F , U ). Together, they will define a PFU network. The factored
form offered by such graphical models must also be analyzed in order to understand
when and how it can be applied to concisely represent global functions (using the
notion of conditional independence).
2. In Equation 8, all plausibilities are combined using the same operator p and all utilities are combined
using the same operator u ; we denote such models as composite graphical models because they include
different types of scoped functions (plausibilities, feasibilities, and utilities). Beyond this, Equation 8
also allows for heterogeneous information among each type of scoped functions. For example, in order to
manipulate both probabilities and possibilities, we can use p defined over probability-possibility pairs
by (p1 , 1 ) p (p2 , 2 ) = (p1  p2 , min(1 , 2 )).

430

fiThe PFU Framework

3. Finally, we must define queries on PFU networks capturing interesting decision problems. As Equation 8 shows, such queries will be defined by a sequence Sov of operatorvariable(s) pairs, applied on the combination of the scoped functions in the network.
The fact that the answer to such queries represents meaningful values from the decision theory point of view will be proved by relating approach.
3.3 Summary
We have informally shown that several queries in various formalisms dealing with plausibilities and/or feasibilities and/or utilities reduce to sequences of variable eliminations
applied to combinations of scoped functions, using various operators. They can intuitively
be covered by Equation 8.
The three key elements (an algebraic structure, a PFU network, and a sequence of variable eliminations) needed to formally define and give sense to this equation are introduced
in Sections 4, 5, and 6.

4. The PFU Algebraic Structures
The first element of the PFU framework is an algebraic structure specifying how the information provided by plausibilities, feasibilities, and utilities is combined and synthesized.
This algebraic structure is obtained by adapting previous structures defined by Friedman,
Chu, and Halpern (Friedman & Halpern, 1995; Halpern, 2001; Chu & Halpern, 2003a) for
representing uncertainties and expected utilities.
4.1 Definitions
Definition 10. (E, ~) is a commutative monoid iff E is a set and ~ is a binary operator
on E which is associative (x ~ (y ~ z) = (x ~ y) ~ z), commutative (x ~ y = y ~ x), and
has an identity 1E  E (x ~ 1E = 1E ~ x = x).
Definition 11. (E, , ) is a commutative semiring iff
 (E, ) is a commutative monoid, with an identity denoted 0E ,
 (E, ) is a commutative monoid, with an identity denoted 1E ,
 0E is annihilator for  (x  0E = 0E ),
  distributes over  (x  (y  z) = (x  y)  (x  z)).
Definition 12. Let (Ea , a , a ) be a commutative semiring. Then, (Eb , b , ab ) is a semimodule on (Ea , a , a ) iff
 (Eb , b ) is a commutative monoid, with an identity denoted 0Eb ,
 ab : Ea  Eb  Eb satisfies
 ab distributes over b (a ab (b1 b b2 ) = (a ab b1 ) b (a ab b2 )),
 ab distributes over a ((a1 a a2 ) ab b = (a1 ab b) b (a2 ab b)),
431

fiPralet, Verfaillie, & Schiex

 linearity property: a1 ab (a2 ab b) = (a1 a a2 ) ab b,
 for all b  Eb , 0Ea ab b = 0Eb and 1Ea ab b = b.
Definition 13. Let E be a set with a partial order . An operator ~ on E is monotonic
iff (x  y)  (x ~ z  y ~ z) for all x, y, z  E.
4.2 Plausibility Structure
Various forms of plausibilities exist (Halpern, 2003). The most usual one is probabilities. As
shown previously, for example with Equation 2, probabilities are aggregated using p = 
as a combination operator, and projected using p = + as an elimination operator.
But plausibilities can also be expressed as possibility degrees in [0, 1]. Possibilities are
eliminated using p = max and usually combined using p = min. An interesting case
appears when possibility degrees are booleans describing which states of the environment
are completely possible or impossible. Plausibilities are then combined using p =  and
eliminated using p = .
Another example is Spohns epistemic beliefs, also known as -rankings (kappa rankings, Spohn, 1990; Wilson, 1995; Giang & Shenoy, 2000). In this case, plausibilities are
elements of N  {+} called surprise degrees, 0 is associated with non-surprising situations,
+ is associated with completely surprising (impossible) situations, and more generally a
surprise degree k can be viewed as a probability of k for an infinitesimal . Surprise degrees
are combined using p = + and eliminated using p = min.
To capture these various plausibility modeling frameworks, we start from FriedmanHalperns work on plausibility measures (Friedman & Halpern, 1995; Halpern, 2001). Weydert (1994) and Darwiche-Ginsberg (1992) developed similar approaches.
Friedman-Halperns structure Assume we want to express plausibilities over the assignments of a set of variables S. Each subset of dom(S) is called an event. Friedman and
Halpern (1995) define plausibilities as elements of a set Ep called the plausibility domain.
Ep is equipped with a partial order p and with two special elements 0p and 1p satisfying
0p p p p 1p for all p  Ep . A function P l : 2dom(S)  Ep is a plausibility measure over S
iff it satisfies P l() = 0p , P l(dom(S)) = 1p , and (W1  W2 )  (P l(W1 ) p P l(W2 )). This
means that 0p is associated with impossibility, 1p is associated with the highest plausibility
degree, and the plausibility degree of a set is as least as high as the plausibility degree of
each of its subsets.
Among all plausibility measures, we focus on so-called algebraic conditional plausibility
measures, which use abstract functions p and p which are analogous to + and  for
probabilities. These measures satisfy properties such as decomposability: for all disjoint
events W1 , W2 , P l(W1  W2 ) = P l(W1 ) p P l(W2 ). As  is associative and commutative,
it follows that p is associative and commutative on representations of disjoint events, i.e.
(a p b) p c = a p (b p c) and a p b = b p a if there exist pairwise disjoint sets
W1 , W2 , W3 such that P l(W1 ) = a, P l(W2 ) = b, P l(W3 ) = c. More details are available in
Friedman-Halperns references (Friedman & Halpern, 1995; Halpern, 2001).
Restriction of Friedman-Halperns structure An important aspect in FriedmanHalperns work is that the algebraic properties of p and p hold only on the domains

432

fiThe PFU Framework

of definition of p and p . Although this is sufficient to express and manipulate plausibilities, it can be algorithmically restrictive. Indeed, consider a Bayesian network involving two
that Px1 is a constant
boolean variables {x1 , x2 } and define Px1 ,x2 as Px1  Px2 | x1 . Assume
P
 Px2 | x1 ((x2 , t)) must
factor 0 = 0.5. In order to evaluate Px2 ((x2 , t)), the quantity x1 0 P
be computed. To do so, it is simpler to factor it and compute 0  x1 Px2 | x1 ((x2 , t)). If
Px2 | x1 ((x2 , t).(x1 , t)) = 0.6 and Px2 | x1 ((x2 , t).(x1 , f )) = 0.8, the answer is 0.5(0.6+0.8) =
0.7. Performing 0.6 + 0.8 requires applying addition outside of the range of usual probabilities, for which a p b is defined only if a + b  1, since two probabilities whose sum exceeds
1 cannot be associated with disjoint events.
To take such issues into account, we adapt Friedman-Halperns Ep , p , p so that p
and p become closed in Ep and so that Friedman-Halperns axioms hold in the closed
structure. Once this closure is performed, we obtain a plausibility structure.
Definition 14. A plausibility structure is a tuple (Ep , p , p ) such that
 (Ep , p , p ) is a commutative semiring (identities for p and p are denoted 0p and
1p respectively),
 Ep is equipped with a partial order p such that 0p = min(Ep ) and such that p and
p are monotonic with respect to p .
Elements of Ep are called plausibility degrees
Note that 1p is not necessarily the maximal element of Ep . For probabilities, FriedmanHalperns structure would be ([0, 1], +0 , ), where a +0 b = a + b if a + b  1 and is
undefined otherwise. In order to get closed operators, we take (Ep , p , p ) = (R+ , +, )
and therefore 1p = 1 is not the maximal element in Ep . In some cases, Friedman-Halperns
structure is already closed. This is the case with -rankings (where (Ep , p , p ) = (N 
{+}, min, +)) and with possibilities (where (Ep , p , p ) is typically ([0, 1], max, min),
although other choices like ([0, 1], max, ) are possible).
Given two plausibility structures (Ep , p , p ) and (Ep0 , 0p , 0p ), if we define E = Ep Ep0 ,
(p1 , p01 )  (p2 , p02 ) = (p1 p p2 , p01 0p p02 ) and (p1 , p01 )  (p2 , p02 ) = (p1 p p2 , p01 0p p02 ), then
(E, , ) is a plausibility structure too. This allows us to deal with different kinds of plausibilities (such as probabilities and possibilities) or with families of probability distributions.
4.2.1 From Plausibility Measures to Plausibility Distributions
Let us consider a plausibility measure (Friedman & Halpern, 1995; Halpern, 2001) P l :
2dom(S)  Ep over a set of variables S. Assume that P l(W1  W2 ) = P l(W1 ) p P l(W2 )
for all disjoint sets W1 , W2  2dom(S) , as is the case with Friedman-Halperns algebraic
plausibility measures. This assumption entails that P l(W ) = p AW P l({A}) for all W 
2dom(S) . This holds even for W =  since 0p is the identity for p . Hence, defining
P l({A}) for all complete assignments A of S suffices to describe P l. Moreover, in this
case, the three conditions defining plausibility measures (P l(dom(S)) = 1p , P l() = 0p ,
and (W1  W2 )  (P l(W1 ) p P l(W2 ))) are equivalent to just p Adom(S) P l({A}) = 1p ,
using the monotonicity of p for the third condition. This means that we can deal with
plausibility distributions instead of plausibility measures:

433

fiPralet, Verfaillie, & Schiex

Definition 15. A plausibility distribution over S is a function PS : dom(S)  Ep such
that p Adom(S) PS (A) = 1p .
The normalization condition imposed on plausibility distributions is simply a generalization of the convention that probabilities sum up to 1. It captures the fact that the
disjunction of all the assignments of S has 1p as a plausibility degree.
Proposition 1. A plausibility distribution PS can be extended to give a plausibility distribution PS 0 over every S 0  S, defined by PS 0 = p SS 0 PS .
4.3 Feasibility Structure
Feasibilities define whether a decision is possible or not, and are therefore expressed as
booleans in {t, f }. This set is equipped with the total order bool satisfying f bool t.
Boolean scoped functions expressing feasibilities are combined using the operator ,
since an assignment of decision variables is feasible iff all feasibility functions agree that
this assignment is feasible.
Given a scoped function Fi expressing feasibilities, we can compute whether an assignment A of a set S of variables is feasible according to Fi by computing sc(Fi )S Fi (A), since
A is feasible according to Fi iff one of its extensions over sc(Fi ) is feasible. This means that
the projection of feasibility functions onto a smaller scope uses the elimination operator .
As a result, feasibilities are expressed using the feasibility structure Sf = ({t, f }, , ).
Sf is not only a commutative semiring, but also a plausibility structure. Therefore, all
plausibility notions and properties apply to feasibility. We may therefore speak of feasibility distributions, and the normalization condition S FS = t imposed on a feasibility
distribution FS over S means that at least one decision must be feasible.
4.4 Utility Structure
Utilities express preferences and can take various forms. Typically, utilities can be combined
with +. But utilities can also model priorities combined using min. Also, when utilities
represent hard requirements such as goals to be achieved or properties to be satisfied, they
can be modeled as booleans combined using . More generally, utility degrees are defined as
elements of a set Eu equipped with a partial order u . Smaller utility degrees are associated
with less preferred events. Utility degrees are combined using an operator u which is
assumed to be associative and commutative. This guarantees that combined utilities do
not depend on the way combination is performed. We also assume that u admits an
identity 1u  Eu , representing indifference. This ensures the existence of a default utility
degree when there are no utility scoped functions. These properties are captured in the
following notion of utility structure.
Definition 16. (Eu , u ) is a utility structure iff it is a commutative monoid and Eu is
equipped with a partial order u . Elements of Eu are called utility degrees.
Eu may have a minimum element u representing unacceptable events and which will
be an annihilator for u (the combination of any event with an unacceptable one must be
unacceptable too). u is also usually monotonic. But these properties are not necessary to
establish the forthcoming results.
434

fiThe PFU Framework

The distinction between plausibilities, feasibilities, and utilities is important and can
be justified using algebraic arguments. Since p and u may be different operators (for
example, p =  and u = + in usual probabilities with additive utilities), we must
distinguish plausibilities and utilities. It is also necessary to distinguish feasibilities from
utilities or plausibilities. Indeed, imagine a simple card game involving two players P1 and
P2 , each having three cards: a jack J, a queen Q, and a king K. P1 must first play one
card x  {J, Q, K}, then P2 must play a card y  {J, Q, K}, and last P1 must play a card
z  {J, Q, K}. A rule forbids to play the same card consecutively (feasibility functions Fxy :
x 6= y and Fyz : y 6= z). The goal for P1 is that his two cards x and z have a value strictly
better than P2 s card y. By setting J < Q < K, this requirement corresponds to two utility
functions Uxy : x > y and Uyz : z > y. In order to compute optimal decisions in presence of
unfeasibilities, we must restrict optimizations (eliminations of decision variables with max
or min) to feasible values: instead of maxx miny maxz (Uxy  Uyz ), we must compute:



max
min
max
(Uxy (a, b)  Uyz (b, c))
,
adom(x)

bdom(y),Fxy (a,b)=t

cdom(z),Fyz (b,c)=t

which, by setting f  t, is logically equivalent to


max min Fxy  max (Fyz  (Uxy  Uyz )) .
x

y

z

In the latter quantity, feasibility functions concerning P2 s play (y) are taken into account
using logical connective , so that P2 s unfeasible decisions are ignored in the set of all
scenarios considered. Feasibility functions concerning P1 s last move (z) are taken into account using , so that P1 does not consider scenarios in which he achieves a forbidden move.
Therefore, feasibility functions cannot be handled simply by using the same combination
operator as for utility functions: we need to dissociate what is unfeasible for all decision
makers (unfeasibility is absolute) from what is unacceptable or required for one decision
maker only (utility is relative), i.e. what a decision maker wants from what a decision maker
can do.
At a more general level, for example when Uxy and Uyz are soft requirements or when we
do not know exactly in advance who controls which variable, the logical connectives  and
 cannot be used anymore. In order to ignore unfeasible values in decision variables elimination, we use the truncation operator ? introduced in Definition 6. In order to eliminate a
variable x from a scoped function  while ignoring unfeasibilities indicated by a feasibility
function Fi , we simply perform the elimination of x on (Fi ? ) instead of . This maps
unfeasibilities to value , which is ignored by elimination operators (see Definition 6). On
the example above, if Uxy and Uyz were additive gains and costs, we would compute


max min Fxy ? max (Fyz ? (Uxy + Uyz )) .
x

y

z

4.5 Combining Plausibilities and Utilities via Expected Utility
To define expected utilities, plausibilities and utilities must be combined. Consider a
situation where a utility ui is obtained with a plausibility pi for all i  [1, N ], with

435

fiPralet, Verfaillie, & Schiex

p1 p . . . p pN = 1p . L = ((p1 , u1 ), . . . , (pN , uN )) is classically called a lottery (von Neumann & Morgenstern, 1944). When we speak of expected utility, we implicitly speak of the
expected utility EU (L) of a lottery L.
The standard way to combine plausibilities and utilities is to use the
P probabilistic expected utility (von Neumann & Morgenstern, 1944) defining EU (L) as i[1,N ] (pi  ui ): it
aggregates plausibilities and utilities using the combination operator pu =  and projects
the aggregated information using the elimination operator u = +. However, alternative
definitions exist:
 If plausibilities are possibilities, then EU (L) = mini[1,N ] max(1  pi , ui ) with the
possibilistic pessimistic expected utility (Dubois & Prade, 1995) (i.e. u = min and
pu : (p, u)  max(1p, u)) and EU (L) = maxi[1,N ] min(pi , ui ) with the possibilistic
optimistic expected utility (Dubois & Prade, 1995) (i.e. u = max and pu = min).
 If plausibilities are -rankings and utilities are positive integers (Giang & Shenoy,
2000), then EU (L) = mini[1,N ] (pi + ui ) (i.e. u = min and pu = +).
To generalize these definitions of EU (L), we start from Chu-Halperns work on generalized expected utility (Chu & Halpern, 2003a, 2003b).
Chu-Halperns structure Generalized expected utility is defined in an expectation domain, which is a tuple (Ep , Eu , Eu0 , u , pu ) such that: (1) Ep is a set of plausibility degrees
and Eu is a set of utility degrees; (2) pu : Ep  Eu  Eu0 combines plausibilities with
utilities and satisfies 1p pu u = u; (3) u : Eu0  Eu0  Eu0 is a commutative and associative
operator which can aggregate the information combined using pu .
When a decision problem is additive, i.e. when, for all plausibility degrees p1 , p2 associated with disjoint events, (p1 p p2 ) pu u = (p1 pu u) u (p2 pu u), the generic definition
of the expected utility of a lottery is:
EU (L) = u (pi pu ui ).
i[1,N ]

Classical expectation domains also satisfy additional properties such as u is monotonic and 0p pu u = 0u , where 0u is the identity of u .
Restriction of Chu-Halperns structure for sequential decision making If we
use pu : Ep  Eu  Eu0 and u : Eu0  Eu0  Eu0 to compute expected utilities at
the first decision step, then we need to introduce operators 0pu : Ep  Eu0  Eu00 and
0u : Eu00  Eu00  Eu00 to compute expected utilities at the second decision step. In the end, if
there are T decision steps, we must define T operators pu and T operators u . In order to
avoid the definition of an algebraic structure that would depend on the number of decision
steps, we take Eu = Eu0 and work with only one operator pu : Ep  Eu  Eu and one
operator u : Eu  Eu  Eu .
As for plausibilities, and for the sake of the future algorithms, we restrict Chu-Halperns
expectation domains (Ep , Eu , Eu , u , pu ) so that u and pu become closed and generalize
properties of the initial u and pu . However, this closure is not sufficient to deal with
sequential decision making, because Chu-Halperns expected utility is designed for one-step
decision processes only. This is why we introduce three additional axioms for u and pu :
436

fiThe PFU Framework

 The first axiom is similar to a standard axiom for lotteries (von Neumann & Morgenstern, 1944) defining compound lotteries. It states that if a lottery L2 involves a
utility u with plausibility p2 , and if one of the utilities of a lottery L1 is the expected
utility of L2 with plausibility p1 , then it is as if utility u had been obtained with
plausibility p1 p p2 . This gives the axiom p1 pu (p2 pu u) = (p1 p p2 ) pu u.
 We further require that pu distributes over u . To justify this point, assume that a
lottery L = ((p1 , u1 ), (p2 , u2 )) is obtained with plausibility p. Two different versions
of the contribution of L to the global utility degree can be derived: the first is p pu
((p1 pu u1 ) u (p2 pu u2 )), and the second, which uses compound lotteries, is ((p p
p1 ) pu u1 ) u ((p p p2 ) pu u2 ). We want these two quantities to be equal for all
p, p1 , p2 , u1 , u2 . This can be shown to be equivalent to a simpler property p pu (u1 u
u2 ) = (p pu u1 ) u (p pu u2 ), i.e. that pu distributes over u .
 Finally, we assume that pu is right monotonic (i.e. (u1 u u2 )  (p pu u1 u
p pu u2 )). This means that if an agent prefers (strictly or not) an event ev2 to
another event ev1 , and if both events have the same plausibility degree p, then the
contribution of ev2 to the global expected utility degree must not be lesser than the
contribution of ev1 .
These axioms define the notion of expected utility structure.
Definition 17. Let (Ep , p , p ) be a plausibility structure and let (Eu , u ) be a utility
structure. (Ep , Eu , u , pu ) is an expected utility structure iff
 (Eu , u , pu ) is a semimodule on (Ep , p , p ) (cf. Definition 12),
 u is monotonic for u and pu is right monotonic for u ((u1 u u2 )  (ppu u1 u
p pu u2 )).
Many structures considered in the literature are instances of expected utility structures,
as shown in Proposition 2. The results presented in the remaining of the article hold not only
for these usual expected utility structures, but more generally for all structures satisfying
the axioms specified in Definitions 14, 16, and 17.
Proposition 2. The structures in Table 1 are expected utility structures.
It is possible to define more complex expected utility structures from existing ones. For
example, from two expected utility structures (Ep , Eu , u , pu ) and (Ep0 , Eu0 , 0u , 0pu ), it is
possible to build a compound expected utility structure (Ep  Ep0 , Eu  Eu0 , 00u , 00pu ). This
can be used to deal simultaneously with probabilistic and possibilistic expected utility or
more generally to deal with tuples of expected utilities.
The business dinner example To flesh out these definitions, we consider the following
toy example, which will be referred to in the sequel. It does not correspond to a concrete
real-life problem, but is used for its simplicity. Peter invites John and Mary (a divorced
couple) to a business dinner in order to convince them to invest in his company. Peter
knows that if John is present at the end of the dinner, he will invest $10K. The same holds
for Mary with $50K. Peter knows that John and Mary will not come together (one of them
437

fiPralet, Verfaillie, & Schiex

1
2
3
4
5
6
7
8
9

Ep
R+
R+
[0, 1]
[0, 1]
N  {}
{t, f }
{t, f }
{t, f }
{t, f }

p





bool
bool
bool
bool

p
+
+
max
max
min





p


min
min
+





0p , 1p
0, 1
0, 1
0, 1
0, 1
, 0
f, t
f, t
f, t
f, t

Eu
R  {}
R+
[0, 1]
[0, 1]
N  {}
{t, f }
{t, f }
{t, f }
{t, f }

u





bool
bool
bool
bool

u
+

min
min
+





u
+
+
max
min
min





pu
0u , 1u

0, 0

0, 1
min
0, 1
max(1p, u) 1, 1
+
, 0

f, t

t, t

f, f

t, f

Table 1: Expected utility structures for: 1. probabilistic expected utility with additive
utilities (allows the probabilistic expected utility of a cost or a gain to be computed); 2. probabilistic expected utility with multiplicative utilities, also called
probabilistic expected satisfaction (allows the probability of satisfaction of some
constraints to be computed); 3. possibilistic optimistic expected utility; 4. possibilistic pessimistic expected utility; 5. qualitative utility with -rankings and
with only positive utilities; 6. boolean optimistic expected utility with conjunctive utilities (allows one to know whether there exists a possible world in which all
goals of a set of goals G are satisfied); bool denotes the order on booleans such
that f bool t; 7. boolean pessimistic expected utility with conjunctive utilities
(allows one to know whether in all possible worlds, all goals of a set of goals G are
satisfied); 8. boolean optimistic expected utility with disjunctive utilities (allows
one to know whether there exists a possible world in which at least one goal of a
set of goals G is satisfied); 9. boolean pessimistic expected utility with disjunctive
utilities (allows one to know whether in all possible worlds, at least one goal of a
set of goals G is satisfied).

has to baby-sit their child), that at least one of them will come, and that the case John
comes and Mary does not occurs with a probability of 0.6. As for the menu, Peter can order
fish or meat for the main course, and white or red for the wine. However, the restaurant
does not serve fish and red wine together. John does not like white wine and Mary does not
like meat. If the menu does not suit them, they will leave the dinner. If John comes, Peter
does not want him to leave the dinner because he is his best friend.
Example. The dinner problem uses the expected utility structure representing probabilistic
expected additive utility (row 1 in Table 1): the plausibility structure is (R+ , +, ), u = +,
pu = , and utilities are additive gains ((Eu , u ) = (R  {}, +), with the convention
that u + () = ).
4.6 Relation with Existing Structures
If we compare the algebraic structures defined with existing ones (Friedman & Halpern,
1995; Halpern, 2001; Chu & Halpern, 2003a), we can observe that:

438

fiThe PFU Framework

 The structures defined here are less general than Friedman-Chu-Halperns, since additional axioms are introduced. For example, plausibility structures are not able to
model belief functions (Shafer, 1976), which are not decomposable, whereas this is
possible using Friedman-Halperns plausibility measures (however, the authors are
not aware of existing schemes for decision theory using belief functions). Moreover, for one-step decision processes, Chu-Halperns generalized expected utility is
more general, since it assumes that pu : Ep  Eu  Eu0 whereas we consider
pu : Ep  Eu  Eu .
 Conversely, the structures defined here can deal with multi-step decision processes
whereas Chu-Halperns generalized expected utility is designed for one-shot decision
processes. Beyond this, other axioms, such as the use of closed operators, are essentially motivated by operational reasons. We use a less expressive structure for the
sake of future algorithms (cf. Section 8).
As a set Ep of plausibility degrees and a set Eu of utility degrees are defined, plausibilities and utilities must be cardinal. Purely ordinal approaches such as CP-nets (Boutilier,
Brafman, Domshlak, Hoos, & Poole, 2004), which, like Bayesian networks, exploit the notion of conditional independence to express a network of purely ordinal preference relations,
are not covered.
As pu takes values in Eu , it is implicitly assumed that plausibilities and utilities are
commensurable: works from Fargier and Perny (1999), describing a purely ordinal approach,
where qualitative preferences and plausibilities are not necessarily commensurable, are not
captured either. Also, works from Giang and Shenoy (2005), which satisfy all required
associativity, commutativity, identity, annihilator, and distributivity properties, are not
covered because they implicitly use pu : Ep  Eu  Eu0 with Eu 6= Eu0 (even if the
expected utility EU (L) = (p1 pu u1 ) u (p2 pu u2 ) of a lottery L = ((p1 , u1 ), (p2 , u2 )) stays
in Eu ).
Furthermore, some axioms entail that only distributional plausibilities are covered (the
plausibility of a set of variable assignments is determined by the plausibilities of each covered
complete assignment): Dempster-Shafer belief functions (Shafer, 1976) or Choquet expected
utility (Schmeidler, 1989) are not encompassed. Finally, as only one partial order u on Eu
is defined, it is assumed that the decision makers share the same preferences over utilities.
4.7 Summary
In this section, we have introduced expected utility structures, which are the first element
of the PFU framework. They specify how plausibilities are combined and projected (using
p and p respectively), how utilities are combined (using u ), and how plausibilities and
utilities are aggregated to define generalized expected utility (using u and pu ). The
structure chosen is inspired by Friedman-Chu-Halperns plausibility measures and generalized expected utility. The main differences lie in the addition of axioms to deal with
multi-step decision processes and in the use of extended domains to have closed operators,
motivated by operational reasons.

439

fiPralet, Verfaillie, & Schiex

5. Plausibility-Feasibility-Utility Networks
The second element of the PFU framework is a network of scoped functions Pi , Fi , and
Ui (cf. Equation 8) over a set of variables V . This network defines a compact and structured representation of the state of the environment, of the decisions, and of the global
plausibilities, feasibilities, and utilities which hold over them.
In the rest of the article, a plausibility function denotes a scoped function to Ep (the
set of plausibility degrees), a feasibility function is a scoped function to {t, f } (the set
of feasibility degrees), and a utility function, a scoped function to Eu (the set of utility
degrees).
5.1 Variables
In structured representations, decisions are represented using decision variables, which are
directly controlled by an agent, and the state of the environment is represented by environment variables, which are not directly controlled by an agent. The notion of agent used
here is restricted to cooperative and adversarial decision makers (if there is an uncertainty
on the way a decision maker behaves, then the decisions he controls will be modeled as
environment variables). We use VD to denote the set of decision variables and VE to denote
the set of environment variables. VD and VE form a partition of V .
Example. The dinner problem can be modeled using six variables: bpJ and bpM (value t
or f ), representing Johns and Marys presence at the beginning, epJ and epM (value t or
f ), representing their presence at the end, mc (value f ish or meat), representing the main
course choice, and w (value white or red), representing the wine choice. Thus, we have
VD = {mc, w} and VE = {bpJ , bpM , epJ , epM }.
5.2 Decomposing Plausibilities and Feasibilities into Local Functions
Using combined local functions to represent a global one raises some considerations: how
and when such local functions can be obtained from a global one, and conversely, when
such local functions are directly used, which implicit assumptions on the global function
are made. We now show that all these questions boil down to the notion of conditional
independence. In the following definitions and propositions, (Ep , p , p ) corresponds to a
plausibility structure.
5.2.1 Preliminaries: Generalization of Bayesian Networks Results
Assume that we want to express a global plausibility distribution PS (cf. Definition 15)
as a combination of local plausibility functions Pi . As work on Bayesian networks (Pearl,
1988) has shown, the factorization of a joint distribution is essentially related to the notion of conditional independence. To introduce conditional independence, we first define
conditional plausibility distributions.
Definition 18. A plausibility distribution PS over S is said to be conditionable iff there
exists a set of functions denoted PS1 | S2 (one function for each pair S1 , S2 of disjoint subsets
of S) such that if S1 , S2 , S3 are disjoint subsets of S, then

440

fiThe PFU Framework

(a) for all assignments A of S2 such that PS2 (A) 6= 0p , PS1 | S2 (A) is a plausibility distribution over S1 ,3
(b) PS1 |  = PS1 ,
(c) p S1 PS1 ,S2 | S3 = PS2 | S3 ,
(d) PS1 ,S2 | S3 = PS1 | S2 ,S3 p PS2 | S3 ,
(e) (PS1 ,S2 ,S3 = PS1 | S3 p PS2 | S3 p PS3 )  (PS1 ,S2 | S3 = PS1 | S3 p PS2 | S3 ).
PS1 | S2 is called the conditional plausibility distribution of S1 given S2 .
Condition (a) means that conditional plausibility distributions must be normalized.
Condition (b) means that the information given by an empty set of variables does not
change the plausibilities over the states of the environment. Condition (c) means that
conditional plausibility distributions are consistent from the marginalization point of view.
Condition (d) is the analog of the so-called chain rule with probabilities. Condition (e) is a
kind of weak division axiom.4
Proposition 3 gives simple conditions on a plausibility structure, satisfied in all usual
frameworks, that suffice for plausibility distributions to be conditionable.
Definition 19. A plausibility structure (Ep , p , p ) is called a conditionable plausibility
structure iff it satisfies the axioms:
 if p1 p p2 and p2 6= 0p , then max{p  Ep | p1 = p p p2 } exists and is p 1p ,
 if p1 p p2 , then there exists a unique p  Ep such that p1 = p p p2 ,
 if p1 p p2 , then there exists a unique p  Ep such that p2 = p p p1 .
Proposition 3. If (Ep , p , p ) is a conditionable plausibility structure, then all plausibility distributions are conditionable: it suffices to define PS1 | S2 by PS1 | S2 (A) = max{p 
Ep | PS1 ,S2 (A) = p p PS2 (A)} for all A  dom(S1  S2 ) satisfying PS2 (A) 6= 0p .
The systematic definition of conditional plausibility distributions given in Proposition 3
fits with the usual definitions of conditional distributions, which are, with probabilities,
PS1 | S2 (A) = PS1 ,S2 (A)/PS2 (A), with -rankings, PS1 | S2 (A) = PS1 ,S2 (A)  PS2 (A),
and with possibility degrees combined using min, PS1 | S2 (A) = PS1 ,S2 (A) if PS1 ,S2 (A) <
PS2 (A), 1 otherwise. In the following, every conditioning statement PS1 | S2 for conditionable plausibility structures will refer to the canonical notion of conditioning given in
Proposition 3. Conditional independence can now be defined.
3. To avoid specifying that properties of PS1 | S2 hold only for assignments A of S1  S2 satisfying PS2 (A) 6=
0p , we use expressions such as PS1 | S2 =  to denote A  dom(S1  S2 ), (PS2 (A) 6= 0p ) 
(PS1 | S2 (A) = (A)).
4. Compared to Friedman and Halperns conditional plausibility measures (Friedman & Halpern, 1995;
Halpern, 2001), (c) is the analog of axiom (Alg1), (d) is the analog of axiom (Alg2), (e) is the analog of
axiom (Alg4), and axiom (Alg3) corresponds to the distributivity of p over p .

441

fiPralet, Verfaillie, & Schiex

Definition 20. Let (Ep , p , p ) be a conditionable plausibility structure. Let PS be a
plausibility distribution over S and S1 , S2 , S3 be disjoint subsets of S. S1 is said to be
conditionally independent of S2 given S3 , denoted I(S1 , S2 | S3 ), iff PS1 ,S2 | S3 = PS1 | S3 p
PS2 | S3 .
This means that S1 is conditionally independent of S2 given S3 , iff the problem can be
split into one part depending on S1 and S3 , and another part depending on S2 and S3 .5
This definition satisfies the usual properties of conditional independence, as Proposition 4
shows.
Proposition 4. I(., . | .) satisfies the semigraphoid axioms:
1. symmetry: I(S1 , S2 | S3 )  I(S2 , S1 | S3 ),
2. decomposition: I(S1 , S2  S3 | S4 )  I(S1 , S2 | S4 ),
3. weak union: I(S1 , S2  S3 | S4 )  I(S1 , S2 | S3  S4 ),
4. contraction: (I(S1 , S2 | S4 )  I(S1 , S3 | S2  S4 ))  I(S1 , S2  S3 | S4 ).
Proposition 4 makes it possible to use Bayesian network techniques to express information in a compact way. With Bayesian networks, a DAG of variables is used to represent
conditional independences between the variables (Pearl, 1988). In some cases, such as image
processing and statistical physics, it is more natural to express conditional independences
between sets of variables. If probabilities are used, such situations can be modeled using chain graphs (Frydenberg, 1990). In a chain graph, the DAG defined is not a DAG of
variables, but a DAG of sets of variables, called components. Conditional probability distributions Px | paG (x) of variables are replaced by conditional probability distributions Pc | paG (c)
of components, each Pc | paG (c) being expressed in a factored form c1 c2 . . .ckc . Markov
random fields (Chellappa & Jain, 1993) correspond to the case in which there is Q
a unique
component equal to V , and in which the factored form of PV looks like 1/Z  jJ eHj
(Gibbs distribution).
We now formally introduce DAGs over sets of variables, called DAGs of components,
and then use them to factor plausibility distributions.
Definition 21. A DAG G is said to be a DAG of components over a set of variables S iff
the vertices of G form a partition of S. C(G) denotes the set of components of G. For each
c  C(G), paG (c) denotes the set of variables included in the parents of c in G, and ndG (c)
denotes the set of variables included in the non-descendant components of c in G.
Definition 22. Let (Ep , p , p ) be a conditionable plausibility structure. Let PS be a
plausibility distribution over S and let G be a DAG of components over S. G is said to
be compatible with PS iff I(c, ndG (c)  paG (c) | paG (c)) for all c  C(G) (c is conditionally
independent of its non-descendants given its parents).
5. Definition 20 differs from Halperns, which is S1 is conditionally independent (CI) of S2 given S3 iff
PS1 | S2 ,S3 = PS1 | S3 and PS2 | S1 ,S3 = PS2 | S3 . Halpern (2001) called the definition we adopt noninteractivity (NI) and showed that NI is weaker than CI. This implies that NI is satisfied more often
and may lead to more factorizations. Halpern gave a simple axiom (axiom (Alg4)) under which CI and
NI are equivalent. Though this axiom holds in many usual frameworks, it does not hold with possibility
degrees combined using min, a case covered by the PFU algebraic structure.

442

fiThe PFU Framework

Theorem 1. (Conditional independence and factorization) Let (Ep , p , p ) be a conditionable plausibility structure and let G be a DAG of components over S.
(a) If G is compatible with a plausibility distribution PS over S, then PS = p cC(G) Pc | paG (c) .
(b) If, for all c  C(G), there is a function c,paG (c) such that c,paG (c) (A) is a plausibility
distribution over c for all assignments A of paG (c), then S = p cC(G) c,paG (c) is a
plausibility distribution over S with which G is compatible.
Theorem 1 links conditional independence and factorization. Theorem 1(a) is a generalization of the usual result of Bayesian networks (Pearl, 1988) which says that if a DAG
of variables
is compatible with a probability distribution PS , then PS can be factored as
Q
PS = xS Px | paG (x) . Theorem 1(b) is a generalization of the standard result of Bayesian
networks (Pearl, 1988) which says that, given a DAG G of variables
Q in S, if conditional
probabilities Px | paG (x) are defined for each variable x  S, then xS Px | paG (x) defines a
probability distribution over S with which G is compatible. Both results are generalizations
since they hold for arbitrary plausibility distributions (and not for probability distributions
only). Results similar in spirit are provided by Halpern (2001), who gives some conditions
under which a plausibility measure can be represented by a Bayesian network.
Theorem 1(a) entails that, in order to factor a global plausibility distribution PS , it
suffices to define a DAG of components compatible with it, i.e. to express conditional
independences. To define such a DAG, the following systematic procedure can be used.
The initial DAG of components is an empty DAG G. While C(G) = {c1 , . . . , ck1 } is not a
partition of S, do:
1. Let Sk = c1  . . .  ck1 ; choose a subset ck of the set S  Sk of variables not already
considered.
2. Add ck as a component to G and find a minimal subset pak of Sk such that I(ck , Sk 
pak | pak ). Add edges directed from components containing at least one variable in
pak to ck , so that paG (ck ) = (c{c1 ,...,ck1 })/(cpak 6=) c.
The resulting DAG of components is guaranteed to be compatible with PS , which implies, using Theorem 1(a), that the local functions Pi representing PS can simply be defined
as the functions in the set {Pc | paG (c) , c  C(G)}. In practice, if there is a reasonable notion
of causes and effects, then networks that are smaller or somehow easier to build can be
obtained by using the following two heuristics in order to choose ck at each step of the
procedure above:
(R1) Consider causes before effects: in the dinner problem, this suggests not putting epJ
in ck if its causes bpJ and w are not in Sk .
(R2) Gather in a component variables that are correlated even when all variables in Sk are
assigned : bpJ and bpM are correlated and (R1) does not apply. Indeed, we cannot
say that bpJ has a causal influence on bpM , or that bpM has a causal influence on bpJ ,
since which of Mary or John chooses first if (s)he baby-sits is not specified. We can
even assume that bpJ and bpM are correlated via an unmodeled common cause, such

443

fiPralet, Verfaillie, & Schiex

as a coin toss that determines the baby-sitter. Hence, bpJ and bpM can be put in the
same component c = {bpJ , bpM }.6
We say that (R1) and (R2) build a DAG respecting causality. They must be seen just as
possible mechanisms that help in identifying conditional independences by using the notions
of causes and effects.
All the previous results extending Bayesian networks results to plausibility distributions
also apply to feasibilities. Indeed, the feasibility structure Sf = ({t, f }, , ) is a particular
case of a conditionable plausibility structure, since it satisfies the axioms of Definition 19.
We may therefore speak of conditional feasibility distribution. If S is a set of decision
variables, the construction of a DAG compatible with a feasibility distribution FS leads to
the factorization FS = cC(G) Fc | paG (c) .
5.2.2 Taking the Differenty Types of Variables into Account
The material defined in the previous subsection enables us to factor one plausibility distribution PVE defined over the set VE of environment variables and one feasibility distribution
FVD defined over the set VD of decision variables. However, dealing with just one plausibility
distribution over VE and one feasibility distribution over VD is not sufficient.
Indeed, for plausibilities, decision variables can influence the environment (for example,
the health state of a patient depends on the treatment chosen for him by a doctor). Rather
than expressing one plausibility distribution over VE , we want to express a family of plausibility distributions over VE , one for each assignment of VD . To make this clear, we define
controlled plausibility distributions.
Definition 23. A plausibility distribution over VE controlled by VD (or just a controlled
plausibility distribution), denoted PVE || VD , is a function dom(VE  VD )  Ep , such that
for all assignments AD of VD , PVE || VD (AD ) is a plausibility distribution over VE .
For feasibilities, it goes the other way around: the values of environment variables can
constrain the possible decisions (for example, an unmanned aerial vehicle which is flying
cannot take off). Thus, we want to express a family of feasibility distributions over VD ,
one for each assignment of VE . In other words, we want to express a controlled feasibility
distribution FVD || VE .
In order to directly reuse Theorem 1 for controlled distributions, we introduce the notion
of the completion of a controlled distribution. This allows us to extend a distribution to
the full set of variables V by assigning the same plausibility (resp. feasibility) degree to
every assignment of VD (resp. VE ), and to work with only one plausibility (resp. feasibility)
distribution.
6. Components such as {bpJ , bpM } could be broken by assuming for example that bpM causally influences
bpJ , i.e. that Mary chooses if she baby-sits first. We can (and prefer to) keep the component as
{bpJ , bpM } because, in general, breaking components can increase the scopes of the functions involved.
For example, assume that we want to model plausibilities over variables representing colors of pixels of
an N  N image, such that the color of a pixel probabilistically depends on the colors of its 4 neighbors
only. With a component approach, results of Markov random fields (Chellappa & Jain, 1993) show that
the local functions obtained have scopes of size 5 only, whereas with a component-breaking mechanism,
the size of the largest scope is linear in N .

444

fiThe PFU Framework

Proposition 5. Let (Ep , p , p ) be a conditionable plausibility structure. Then, for all
n  N , there exists a unique p0 such that p i{1,...,n} p0 = 1p .
Definition 24. Let (Ep , p , p ) be a conditionable plausibility structure and let PVE || VD be
a controlled plausibility distribution. Then, the completion of PVE || VD is a function denoted
PVE ,VD , defined by PVE ,VD = PVE || VD p p0 , where p0 is the unique element of Ep such that
p i[1,|dom(VD )|] p0 = 1p (the cardinality of a set  is denoted ||).
In other words, PVE ,VD is defined from PVE || VD by assigning the same plausibility degree
p0 to all assignments of VD . In the case of probability theory, it corresponds to saying that
the assignments of VD are equiprobable. The definition of the completion of a controlled
plausibility distribution could be made more flexible: instead of defining a uniform plausibility distribution over VD , we could define a plausibility distribution such that no assignment
of VD has 0p as a plausibility degree. We arbitrarily choose the uniform distribution, the
goal being just to introduce some prior plausibilities over decision variables, for the sake of
factorization.
Proposition 6. Let PVE ,VD be the completion of a controlled plausibility distribution PVE || VD .
Then, PVE ,VD is a plausibility distribution over VE  VD and PVE | VD = PVE || VD .
As a result, we use PVE | VD to denote PVE || VD (and this is equivalent). Similarly, it is
possible to complete a controlled feasibility distribution FVD || VE .
5.2.3 A First Factorization
Proposition 7 below, entailed by Theorem 1(a), shows how to obtain a first factorization of
PVE | VD and FVD | VE .
Definition 25. A DAG G is a typed DAG of components over VE  VD iff the vertices of
G form a partition of VE  VD such that each element of this partition is a subset of either
VD or VE . Each vertex of G is called a component. The set of components contained in
VE (environment components) is denoted CE (G) and the set of components contained in VD
(decision components) is denoted CD (G).
Proposition 7. Let G be a typed DAG of components over VE  VD . Let Gp be the partial
graph of G induced by the arcs of G incident to environment components. Let Gf be the
partial graph of G induced by the arcs of G incident to decision components. If Gp is
compatible with the completion of PVE || VD (cf. Definition 22) and Gf is compatible with
the completion of FVD || VE , then
PVE | VD =

p Pc | paG (c)
cCE (G)

and FVD | VE =


cCD (G)

Fc | paG (c) .

This allows us to specify local Pi and Fi functions: it suffices to express each Pc | paG (c)
and each Fc | paG (c) to express PVE | VD and FVD | VE in a compact way. In fact, we could have
defined two DAGs, one for the factorization of PVE | VD and the other for the factorization
of FVD | VE , but these two DAGs can actually always be merged as soon as we make the
(undemanding) assumption that it is impossible, given x  VD and y  VE , that both x
influences y and y constrains the possible decision values for x. This assumption ensures
that the union of the two DAGs does not create cycles. We use just one DAG for simplicity.
445

fiPralet, Verfaillie, & Schiex

Example. Consider the dinner problem to illustrate the first factorization step. One way
to obtain G is to use the causality-based reasoning described after Theorem 1. We start
with an empty DAG. As epJ and epM are both effects of bpJ , bpM , w, or mc, they are not
considered in the first component c1 . bpJ can be chosen as a variable to add to c1 , because
we cannot say that bpJ is necessarily an effect of another variable. As previously explained,
bpJ can be a cause of bpM , or an effect of bpM , or bpJ may be correlated with bpM via an
unmodeled cause. As a result, we get c1 = {bpJ , bpM } as a first component. Obviously, c1
has no parents in the DAG because it is the first added component.
Then, as epJ and epM are effects of w or mc, we do not consider epJ or epM in the
second component c2 . Since w is not necessarily an effect of mc, we can add w to c2 . The
dinner problem specifies that ordering fish and red wine simultaneously is not feasible, but
we do not know whether the wine is chosen before or after the main course, i.e. w can
be a cause or an effect of mc. As a result, we take c2 = {mc, w}. As the menu choice is
independent from who is present at the beginning, c2 has no parent in the temporary DAG.
As epJ is a direct effect of bpJ and w only (John leaves the dinner if white wine is
chosen), we can add epJ to c3 . Moreover, epJ is not correlated with epM when c1  c2 =
{bpJ , bpM , mc, w} is assigned. Therefore, we take c3 = {epJ }. Given that epJ depends both
on bpJ and w, c3 gets {bpJ , bpM } and {mc, w} as parents. Finally, c4 = {epM }, and as epM
depends on bpM and mc (Mary leaves if meat is chosen) and is independent of epJ given bpM
and mc, we have that I({epM }, {epJ , bpJ , w} | {bpM , mc}). This entails that {epM } is added
to the DAG with {bpJ , bpM } and {mc, w} as parents. Therefore, we get CD (G) = {{mc, w}}
as the set of decision components and CE (G) = {{bpJ , bpM }, {epJ }, {epM }} as the set of
environment components. The DAG of components is shown in Figure 1a.
Using Proposition 7, we know that the joint probability distribution factors as PVE | VD =
PbpJ ,bpM  PepJ | bpJ ,bpM ,mc,w  PepM | bpJ ,bpM ,mc,w and that the joint feasibility distribution
can be factored as FVD | VE = Fmc,w .
5.2.4 Further Factorization Steps
Proposition 7 provides us with a decomposition of PVE | VD and FVD | VE based on the conditional independence relation I(., . | .) of Definition 20. It may be possible to perform further
factorization steps by factoring each Pc | paG (c) as a set of local plausibility functions Pi and
factoring each Fc | paG (c) as a set of local feasibility functions Fi .
 In some cases, expressing factors of Pc | paG (c) or Fc | paG (c) is quite natural. For example, if p = , if variables in an environment component c = {xi,j | i, j  [1, n]}
without parents represent pixel colors, and if we want to model in Pc that two
adjacent pixels have different colors, it is natural to define a set of binary differ
ence constraints xi,j ,xk,l and
to
factor
P
as
P
=




c
c
x
,x
i[1,n1]
j[1,n]
i,j
i+1,j

i[1,n] j[1,n1] xi,j ,xi,j+1 . Such a decomposition cannot be obtained based only
on the conditional independence relation I(., . | .) of Definition 20.
 In some settings, as in Markov random fields (Chellappa & Jain, 1993), systematic
techniques exist to obtain such factorizations. The Bayesian network community
also offers systematic techniques: with hybrid networks (Dechter & Larkin, 2001),
we can extract the deterministic information contained in conditional probability
distributions. More precisely, a conditional probability distribution Px | paG (x) can
446

fiThe PFU Framework

be expressed as Px | paG (x) = Px | paG (x)  , where  is the 0-1 function defined by

0 if Px | paG (x) (A) = 0
(A) =
. The factorization of Px | paG (x) as Px | paG (x)   can
1 otherwise
be computationally relevant when constraint propagation techniques on 0-1 functions
are used to solve hybrid networks.
 We may use another weaker definition of conditional independence: in valuation-based
systems (Shenoy, 1994), S1 and S2 are said to be conditionally independent given S3
with regard to a function S1 ,S2 ,S3 if this function factors into two scoped functions
with scopes S1  S3 and S2  S3 . This definition is not used for the first factorization
step because it destroys the normalization conditions which may be useful from a
computational point of view.
These additional factorization steps are of interest because decreasing the size of the
scopes of the functions involved or adding redundant information in the problem can be
computationally useful.
For every environment component c, if Pi  F act(c) stands for Pi is a factor of
Pc | paG (c) , the second factorization gives us
Pc | paG (c) =

p
Pi .
Pi F act(c)

As p c Pc | paG (c) = 1p , the Pi functions in F act(c) satisfy the normalization condition
p c (p Pi F act(c) Pi ) = 1p . Their scopes sc(Pi ) are contained in sc(Pc | paG (c) ) = c  paG (c).
For every decision component c, if Fi  F act(c) stands for Fi is a factor of Fc | paG (c) ,
the second factorization gives us
Fc | paG (c) =



Fi F act(c)

Fi .

Given that c Fc | paG (c) = t, the Fi functions in F act(c) satisfy the normalization condition
c Fi F act(c) Fi = t. Moreover, sc(Fi )  c  paG (c).
Other factorizations, which do not decrease the scopes of the functions involved, could
also be exploited. Indeed, each scoped function Pi or Fi can itself have an internal local
structure, as for instance when Pi is a noisy-OR gate (Pearl, 1988) in a Bayesian network,
or in presence of context-specific independence (Boutilier, Friedman, Goldszmidt, & Koller,
1996). Such internal local structures can be made explicit by representing functions with
tools such as Algebraic Decision Diagrams (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo,
& Somenzi, 1993). In the rest of the article, we do not make any assumption on the way
each scoped function is represented.
Example. PbpJ ,bpM can be expressed in terms of a first plausibility function P1 specifying the probability of John and Mary being present at the beginning. P1 is defined
by P1 ((bpJ , t).(bpM , f )) = 0.6, P1 ((bpJ , f ).(bpM , t)) = 0.4, and P1 ((bpJ , t).(bpM , t)) =
P1 ((bpJ , f ).(bpM , f )) = 0. We can also add redundant deterministic information with a second plausibility function P2 defined as the constraint bpJ 6= bpM (P2 (A) = 1 if the constraint
is satisfied, 0 otherwise). We get PbpJ ,bpM = P1 p P2 and F act({bpJ , bpM }) = {P1 , P2 }.

447

fiPralet, Verfaillie, & Schiex

PepJ | bpJ ,bpM ,mc,w can be specified as a combination of two plausibility functions P3 and
P4 . P3 expresses that if John is absent at the beginning, he is absent at the end: P3 is the
hard constraint (bpJ = f )  (epJ = f ) (P3 (A) = 1 if the constraint is satisfied, 0 otherwise).
Then, P4 : (bpJ = t)  ((epJ = t)  (w 6= white)) is a hard constraint specifying that
John leaves iff white wine is chosen. Hence, we have PepJ | bpJ ,bpM ,mc,w = P3 p P4 and
F act({epJ }) = {P3 , P4 }. Similarly, PepM | bpJ ,bpM ,mc,w = P5 p P6 , with P5 , P6 defined as
constraints, and F act({epM }) = {P5 , P6 }.
As for feasibilities, Fmc,w can be specified by a feasibility function F1 expressing that ordering fish with red wine is not allowed: F1 : ((mc = f ish)(w = red)) and F act({mc, w})
= {F1 }. The association of local functions with components appears in Figure 1a.
5.3 Local Utilities
Local utilities can be defined over the states of the environment only (as in the utility of
the health state of a patient), over decisions only (as in the utility of the decision of buying
a car or not), or over the states of the environment and decisions (as in the utility of the
result of a horse race and a bet on the race).7
In order to specify local utilities, one standard approach, used in CSPs and influence
diagrams, is to directly define a set U of local utility functions, modeling preferences or
hard requirements, over decision and environment variables. This set implicitly defines a
global utility UV = uUi U Ui over all variables. If this factored form is obtained from
a global joint utility, one may rely, when u = +, on the work of Fishburn (1982) and
Bacchus-Grove (1995), who introduced a notion of conditional independence for utilities.
No normalization condition is imposed on local utilities.
Example. In the dinner problem, three local utility functions can be defined. A binary
utility function U1 expresses that Peter does not want John to leave the dinner: U1 is
the hard constraint (bpJ = t)  (epJ = t) (U1 (A) = 0 if the constraint is satisfied, 
otherwise). Two unary utility functions U2 and U3 over epJ and epM respectively express the
gains expected from the presences at the end: U2 ((epJ , t)) = 10 and U2 ((epJ , f )) = 0 (John
invests $10K if he is present at the end), while U3 ((epM , t)) = 50 and U3 ((epM , f )) = 0
(Mary invests $50K if she is present at the end). U2 and U3 can be viewed as soft constraints.
All the local functions are represented in a graphical model in Figure 1b.
5.4 Formal Definition of PFU Networks
We can now formally define Plausibility-Feasibility-Utility networks. The definition is justified by the previous construction process, but it holds even if the plausibility structure is
not conditionable.
7. In influence diagrams, special nodes called value nodes are introduced to represent the outcome of
decisions, and one utility function is associated with each of these value nodes (the utility of the outcome).
In the PFU framework, we directly represent such utility functions as scoped functions which hold on
the parents of value nodes. This explicitly express that utility functions are scoped functions, just as
plausibility and feasibility functions. In other words, utility functions are directly utilities of the outcome
of decision and environment variables assignments.

448

fiThe PFU Framework

P4
mc, w

bpJ , bpM
P1 , P 2

F1

w
F1

epJ

epM

P3 , P 4

P5 , P 6

(a)

mc

bpJ

P3

environment

P2 U1

P1

bpM

P5

decision

epJ
U2

plausibility
function

epM

P6

feasibility
function

U3

utility
function

(b)

Figure 1: (a) DAG of components (b) Network of scoped functions.
Definition 26. A Plausibility-Feasibility-Utility network on an expected utility structure
is a tuple N = (V, G, P, F, U ) such that the following conditions hold:
 V = {x1 , x2 , . . .} is a finite set of finite domain variables. V is partitioned into VD
(decision variables) and VE (environment variables);
 G is a typed DAG of components over VE  VD (cf. Definition 25);
 P = {P1 , P2 , . . .} is a finite set of plausibility functions. Each Pi  P is associated with a unique component c  CE (G) such that sc(Pi )  c  paG (c). The set of
Pi  P associated with a component c  CE (G) is denoted F act(c) and must satisfy
p (p Pi F act(c) Pi ) = 1p ;
c

 F = {F1 , F2 , . . .} is a finite set of feasibility functions. Each function Fi is associated
with a unique component c  CD (G) such that sc(Fi )  c  paG (c). The set of
Fi  F associated
 with a component c  CD (G) is denoted F act(c) and must satisfy
 Fi F act(c) Fi = t;
c

 U = {U1 , U2 , . . .} is a finite set of utility functions.
5.5 From PFU Networks to Global Functions
We have seen how to obtain a PFU network expressing a global controlled plausibility
distribution PVE || VD , a global controlled feasibility distribution FVD || VE , and a global utility
UV . Conversely, let N = (V, G, P, F, U ) be a PFU network, i.e. a set of variables, a typed
DAG of components, and sets of scoped functions. Then
 the global function  = p Pi P Pi is a controlled plausibility distribution of VE given
VD . Moreover, by Theorem 1(b), if the plausibility structure is conditionable and if
Gp is the partial DAG of G induced by the arcs incident to environment components,
then Gp is compatible with the completion of ;
 the global function  = Fi F Fi is a controlled feasibility distribution of VD given
VE . Moreover, by Theorem 1(b), if Gf is the partial DAG of G induced by the arcs of
G incident to decision components, then Gf is compatible with the completion of ;
449

fiPralet, Verfaillie, & Schiex

  = uUi U Ui is necessarily a global utility.
We can therefore denote  by PVE || VD ,  by FVD || VE , and  by UV .
5.6 Back to Existing Frameworks
Let us consider the formalisms described in Section 3 again.
 A CSP (hard or soft) can easily be represented as a PFU network N = (V, G, , , U ):
all variables in V are decision variables, G is reduced to a single decision component
containing all variables, and constraints are represented by utility functions. Using
feasibility functions to represent constraints, it would be impossible to represent inconsistent networks because of the normalization conditions on feasibilities. SAT is
modeled similarly; the only difference is that constraints are replaced by clauses.
 The same PFU network as above is used to represent the local functions of a quantified
boolean formula or of a quantified CSP. The differences with CSPs or SAT appear
when we consider queries on the network (see Section 6).
 A Bayesian network can be modeled as N = (V, G, P, , ): all variables in V are
environment variables, G is the DAG of the BN, and P = {Px | paG (x) , x  V }. There
is no feasibility or utility function. A chain graph is also modeled as N = (V, G, P, , ),
with G the DAG of components of the chain graph and P the set of factors of each
Pc | paG (c) .
 A stochastic CSP is represented by a PFU network N = (V, G, P, , U ), where V
is partitioned into VD , the set of decision variables, and VE , the set of stochastic
variables, G is a DAG which depends on the relations between the stochastic variables,
P is the set of probability distributions over the stochastic variables, and U is the set
of constraints.
 An influence diagram can be modeled by N = (V, G, P, , U ) such that VD contains
the decision variables, VE contains the chance variables, G is the DAG of the influence
diagram without the utility nodes and with arcs into random variables only (i.e. we
keep only the so-called influence arcs), and P = {Px | paG (x) , x  VE }. There are no
feasibilities, and one utility function Ui is defined per utility variable u, the scope
of Ui being paG (u). To represent valuation networks, a set F of feasibility functions
is added. Note that the business dinner example could not have been modeled using a standard influence diagram, since influence diagrams cannot model feasibilities
(suitable extensions exist however, Shenoy, 2000).
 A finite horizon probabilistic MDP can be modeled as N = (V, G, P, F, U ). If there are
T time-steps, then VD = {dt , t  [1, T ]}{s1 } and VE = {st , t  [2, T ]};8 G is a DAG of
components such that (a) each component contains one variable, (b) the unique parent
of a decision component {dt } is {st }, (c) the parents of an environment component
{st+1 } are {st } and {dt }; P = {Pst+1 |st ,dt , t  [1, T  1]}, F = {Fdt | st , t  [1, T ]}, and
U = {Rst ,dt , t  [1, T ]}. Modeling a finite horizon possibilistic MDP is similar.
8. As there is no plausibility distribution over the initial state s1 , s1 is not viewed as an environment
variable. This corresponds to the special case where decision variables model problem parameters.

450

fiThe PFU Framework

5.7 Summary
In this section, we have introduced the second element of the PFU framework: a network of
variables linked by local plausibility, feasibility, and utility functions, with a DAG capturing
normalization conditions. The factorization of global plausibilities, feasibilities, and utilities
into scoped functions was linked to conditional independence.

6. Queries on a PFU Network
A query will correspond to a reasoning task on the information expressed by a PFU network.
If decision variables are involved in the PFU network considered, answering a query may
provide decision rules. Examples of informal queries about the dinner problem are
1. What is the best menu choice if Peter does not know who is present at the beginning?
2. What is the best menu choice if Peter knows who is present at the beginning?
3. How should we maximize the expected investment if the restaurant chooses the main
course first and Peter is pessimistic about this choice, then who is present at the
beginning is observed, and last Peter chooses the wine?
Dissociating PFU networks from queries is consistent with the trend in the influence diagram community to relax the so-called information links, as in Unconstrained Influence
Diagrams (Jensen & Vomlelova, 2002) or Limited Memory Influence Diagrams (Lauritzen
& Nilsson, 2001): it explains the intuition that queries do not change the local relations
between variables.
In this section, we define a simple class of queries on PFU networks. We assume that
a sequence of decisions must be performed, and that the order in which decisions and
observations are made is known. We also make a no-forgetting assumption, that is, when
making a decision, an agent is aware of all previous decisions and observations. From
now on, the set of utility degrees Eu is assumed to be totally ordered. This total order
assumption, which holds in most of the standard frameworks, implies that there always
exists an optimal decision rule. See Subsection 6.7 for a discussion of how to extend the
results to a partial order.
Two definitions of the answer to a query are given, the first based on decision trees,
and the second more operational. An equivalence between these two definitions is then
established.
6.1 Query Definition
In order to formulate reasoning tasks on a PFU network, we use a sequence Sov of operatorvariable(s) pairs. This sequence captures different aspects of the query:
 Partial observabilities: Sov specifies the order in which decisions are made and environment variables are observed. If x  VE appears to the left of y  VD (for example
Sov = . . . (u , {x}) . . . (max, {y}) . . .), this means that the value of x is known (observed) when a value for y is chosen. Conversely, Sov = . . . (max, {y}) . . . (u , {x}) . . .
if x is not observed when choosing y.
451

fiPralet, Verfaillie, & Schiex

 Optimistic/pessimistic attitude concerning the decision makers: (max, {y}) is inserted
in the elimination sequence if the decision maker is optimistic about the behavior of
the agent controlling a decision variable y (i.e. if this agent is cooperative), and
(min, {y}) if one is pessimistic (i.e. if the agent controlling y is an adversary). The
operator used for environment variables will always be u , to model that expected
utilities are sought.9
 Parameters of the decision making problem: the set S of variables that are not involved
in Sov are kind of parameters. Their absence indicates that we want to obtain optimal
expected utilities and/or optimal policies for each assignment of S. This is useful in
order to evaluate several scenarios simultaneously.
Example. The sequence corresponding to the informal query: How should we maximize the
expected investment if the restaurant chooses the main course first and Peter is pessimistic
about this choice, then those present at the beginning of the dinner are observed, and last
Peter chooses the wine before knowing who is present at the end? is
Sov = (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }).
It models the fact that: (1) Peter is pessimistic about the main course (min over mc), which
is chosen without observing any variable (no variable to the left of mc in Sov); (2) Peter
makes the best choice of wine (max over w) after the main course has been chosen and
after knowing who is present at the beginning (w appears to the right of mc, bpJ , and bpM
in Sov), but before knowing who is present at the end (w appears to the left of epJ , epM ).
Specifically, bpJ and bpM are partially observable, whereas epJ and epM are unobservable.
If the query becomes What should Peter do if he observes those present at the beginning
of the dinner and then chooses the wine before knowing who is present at the end?, then
the sequence to use is Sov = (u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }). In this case,
variable mc does not appear in the sequence anymore, which means that mc is a parameter
and that an answer for each value of this parameter is sought.
Definition 27. A query on a PFU network is a pair Q = (N , Sov) where N is a PFU
network and Sov = (op1 , S1 )(op2 , S2 )    (opk , Sk ) is a sequence of operator-set of variables
pairs such that
(1) all the Si are disjoint;
(2) either Si  VD and opi = min or max, or Si  VE and opi = u ;
(3) variables not involved in any of the Si , called free variables, are decision variables;
(4) for all variables x, y of different types (one is a decision variable, the other is an
environment variable), if there is a directed path from the component which contains
x to the component which contains y in the DAG of the PFU network N , then x does
not appear to the right of y in Sov, i.e. either x appears to the left of y, or x is a free
variable.
9. If a decision is made by nature and if we have a plausibility distribution on that decision, then this
decision will be viewed as an environment variable.

452

fiThe PFU Framework

Condition (1) ensures that each variable is eliminated at most once. Condition (2) means
that optimal decisions are sought for decision variables (either maximized if the decision
maker who controls a decision variable is cooperative, or minimized if he is adversarial),
whereas expected utilities are sought for environment variables. Condition (3) means that
variables which are not eliminated in Sov act as problem parameters and are viewed as
decision variables. Condition (4) means that if x and y are of different types and x is an
ancestor of y, then x is assigned before y. This ensures that causality is respected for variables of different types: for example, (N , (u , {bpJ , bpM , epJ , epM }).(max, {mc, w})), which
violates condition (4), violates causality since the menu cannot be chosen after knowing who
is present at the end.
Variables appearing in Sov are called quantified variables, by analogy with quantified
boolean formulas. The set of free variables is denoted by Vf r . Notice that the definition of
queries does not prevent an environment variable from being quantified by min or max,
because we may have u = min or u = max. Note also that it is straightforward that for
every PFU network N , there exists at least one query on N without free variables.
For all i  [1, k], we define
 the set l(Si ) of variables appearing in Vf r or to the left of Si in Sov by l(Si ) =
Vf r  (j[1,i1] Sj );
 the set r(Si ) of variables appearing to the right of Si in Sov by r(Si ) = j[i+1,k] Sj .
6.2 Semantic Answer to a Query
In this subsection, we assume that the plausibility structure is conditionable (cf. Definition 19). The controlled plausibility distribution PVE || VD = p Pi P Pi can then be completed (cf. Definition 24) to give a plausibility distribution PVE ,VD over VE  VD . Similarly,
the controlled feasibility distribution FVD || VE = Fi F Fi can be completed to give a feasibility distribution FVE ,VD over VE  VD . We also use the global utility UV = uUi U Ui
defined by the PFU network.
Imagine that we want to answer the query Q = (N , Sov), where N is the network of
the dinner problem and Sov = (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }).
To answer such a query, we can use a decision tree. First, the restaurant chooses the
worst possible main course, taking into account the feasibility distribution of mc. Here,
Fmc ((mc, meat)) = Fmc,w ((mc, meat).(w, white))  Fmc,w ((mc, meat).(w, red)) = t  t =
t. Similarly, Fmc ((mc, f ish)) = t. Both choices are feasible. Then, if A1 denotes the
assignment of mc, the uncertainty over those present at the beginning given the main
course choice is described by the probability distribution PbpJ ,bpM | mc (A1 ). For each possible
assignment A2 of {bpJ , bpM }, i.e. for each A2 such that PbpJ ,bpM | mc (A1 .A2 ) 6= 0p , Peter
chooses the best wine while taking into account the feasibility Fw | mc,bpJ ,bpM (A1 .A2 ): if
the restaurant chooses meat, Peter chooses an optimal value between red and white, and
if the restaurant chooses fish, Peter can choose white wine only. Then, for each feasible
assignment A3 of w, the uncertainty regarding the presence of John and Mary at the end
of the dinner is given by PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 ).
Note that the conditional probabilities used in the decision tree above are not directly
defined by the network. They must be computed from the global distributions. This
computation can be a challenge in large problems.
453

fiPralet, Verfaillie, & Schiex

The utility UV (A1 .A2 .A3 .A4 ) can be associated with each possible complete assignment
A1 .A2 .A3 .A4 of the variables. For each possible assignment A1 .A2 .A3 of {bpJ , bpM , mc, w},
the last stage, i.e. the one in which epJ and epM are assigned,
P can be seen as a lottery (von
Neumann & Morgenstern, 1944) whose expected utility is A4 dom({epJ ,epM }) p(A4 )u(A4 ),
where p(A4 ) = PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 ) and u(A4 ) = UV (A1 .A2 .A3 .A4 ). This
expected utility becomes the reward of the scenario over {bpM , bpJ , mc, w} described by
A1 .A2 .A3 . It provides us with a criterion for choosing an optimal value for w. The step
in which bpJ and bpM are assigned can then be seen as a lottery, which provides us with a
criterion for choosing a worst value for mc. The computation associated with the previously
described process is:
min
A1 dom(mc),Fmc (A1 )=t
X
PbpJ ,bpM | mc (A1 .A2 )
(
A2 dom({bpJ ,bpM }),PbpJ ,bpM

(

max
X

| mc (A1 .A2 )6=0

A3 dom(w),Fw | mc,bpJ ,bpM (A1 .A2 .A3 )=t

(

PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 )

A4  dom({epJ , epM })
PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 ) 6= 0

UV (A1 .A2 .A3 .A4 )))).

Decision rules for the decision variables (argmin and argmax) can be recorded during
the computation. This formulation represents the decision process as a decision tree in
which each internal level corresponds to variable assignments. Arcs associated with the
assignment of a set of decision variables are weighted by the feasibility of the decision given
the previous assignments. Arcs associated with the assignment of environment variables are
weighted by the plausibility degree of the assignment given the previous assignments. Leaf
nodes correspond to the utilities of complete assignments, and a node collects the values of
its children to compute its own value.
6.2.1 Formalization of the Decision Tree Procedure
In order to formalize the decision tree procedure, some technical results are first introduced
in Proposition 8. These results and the definitions preceding them can be skipped for a first
reading.
Definition 28. Let PS1 | S2 be the conditional plausibility distribution of S1 given S2 and let
A  dom(S2 ). The function PS1 | S2 (A) is said to be well-defined iff PS2 (A) 6= 0p . Similarly,
if FS1 | S2 is the conditional feasibility distribution of S1 given S2 , then, for all A  dom(S2 ),
FS1 | S2 (A) is said to be well-defined iff FS2 (A) = t.
Next, the conditioning can be defined directly for controlled plausibility distributions
because for all A  dom(VD ), PVE || VD (A) is a plausibility distribution over VE :
Definition 29. Assume that the plausibility structure used is conditionable. Let PVE || VD
be a controlled plausibility distribution and S, S 0 be two disjoint subsets of VE . We define
conditional controlled plausibility distributions by: for all A  dom(S  S 0  VD ) such that
PS 0 || VD (A) 6= 0p , PS | S 0 || VD (A) = max{p  Ep | PS,S 0 || VD (A) = p p PS 0 || VD (A)}, as in
the canonical definition of conditioning given in Proposition 3. Given a controlled feasi454

fiThe PFU Framework

bility distribution FVD || VE , the definition of conditional controlled feasibility distributions
FS | S 0 || VE for S, S 0 disjoint subsets of VD is similar.
Proposition 8. Assume that the plausibility structure used is conditionable. Let Q =
(N , Sov) be a query where Sov = (op1 , S1 )  (op2 , S2 )    (opk , Sk ). Let Vf r denote the set of
free variables of Q.
(1) If Si  VE and PSi | l(Si ) (A) is well-defined, then there exists at least one A0  dom(Si )
satisfying PSi | l(Si ) (A.A0 ) 6= 0p .
(2) If Si  VD and FSi | l(Si ) (A) is well-defined, then there exists at least one A0  dom(Si )
satisfying FSi | l(Si ) (A.A0 ) = t.
(3) If VE 6=  and Si is the leftmost set of environment variables appearing in Sov, then,
for all A  dom(l(Si )), PSi | l(Si ) (A) is well-defined.
(4) If i, j  [1, k], i < j, Si  VE , Sj  VE , r(Si )  l(Sj )  VD (Sj is the first set of environment variables appearing to the right of Si in Sov), (A, A0 )  dom(l(Si ))dom(Si ),
PSi | l(Si ) (A) is well-defined, and PSi | l(Si ) (A.A0 ) 6= 0p , then, for all A00 extending A.A0
over l(Sj ), PSj | l(Sj ) (A00 ) is well-defined.
(5) If i, j  [1, k], i < j, Si  VD , Sj  VD , r(Si )  l(Sj )  VE (Sj is the first set of
decision variables appearing to the right of Si in Sov), (A, A0 )  dom(l(Si ))dom(Si ),
FSi | l(Si ) (A) is well-defined, and FSi | l(Si ) (A.A0 ) = t, then, for all A00 extending A.A0
over l(Sj ), FSj | l(Sj ) (A00 ) is well-defined.
(6) For all i  [1, k] such that Si  VE , PSi | l(Si ) = PSi | l(Si )VE ||VD .
(7) For all i  [1, k] such that Si  VD , FSi | l(Si ) = FSi | l(Si )VD ||VE .
The technical results of Proposition 8 ensure that, in the following semantic answer to
a query (see Definition 30),
 all quantities PS | l(S) (A.A0 ) and FS | l(S) (A.A0 ) used are defined (thanks to items 3 to
5 in Proposition 8);
 all eliminations over restricted domains are defined because the restricted domains
used are never empty (items 1 and 2 in Proposition 8);
 the conditional distributions used coincide with a conditioning defined directly from
the controlled plausibility and feasibility distributions PVE || VD and FVD || VE (items 6
and 7 in Proposition 8). This is useful because this guarantees that PS | l(S) (A.A0 )
and FS | l(S) (A.A0 ), which a priori require the notion of completion to be written, are
actually independent of the notion of completion, which is arbitrarily added to the
basic information expressed in a PFU network. We use PS | l(S) and FS | l(S) instead
of conditional controlled distributions PS | l(S)VE || VD and FS | l(S)VD || VE for notation
convenience and to explicitly represent that PS | l(S)VE || VD and FS | l(S)VD || VE do not
depend on the assignment of VD  l(S) and VE  l(S) respectively.

455

fiPralet, Verfaillie, & Schiex

Definition 30. The semantic answer Sem-Ans(Q) to a query Q = (N , Sov) is a function
of the set Vf r of free variables of Q defined by10

 if FVf r (A) = f
Sem-Ans(Q)(A) =
Qs(N , Sov, A) otherwise,
with Qs inductively defined by:
(1)

Qs(N , , A) = UV (A)

(2)

Qs(N , (op, S) . Sov, A) =


min
Qs (N , Sov, A.A0 )

0

A  dom(S)



FS|l(S) (A.A0 ) = t




Qs (N , Sov, A.A0 )
max











A0  dom(S)
FS|l(S) (A.A0 ) = t

u

A0  dom(S)
PS|l(S) (A.A0 ) 6= 0p

if (S  VD )  (op = min),
if (S  VD )  (op = max),


PS|l(S) (A.A0 ) pu Qs (N , Sov, A.A0 )

if (S  VE ).

In other words, each step involving decision variables (first two cases) corresponds to an
optimization step among the feasible choices, and each step involving environment variables
(third case) corresponds to a lottery (von Neumann & Morgenstern, 1944) such that the
rewards are the Qs (N , Sov, A.A0 ), and such that the plausibility attributed to a reward is
PS | l(S) (A.A0 ) (the formula looking like ui (pi pu ui ) is the expected utility of this lottery).
When a set of decision variables S is eliminated, a decision rule for S can be recorded, using
an argmax (resp. an argmin) if max (resp. min) is performed.
Example. What is the maximum investment Peter can expect, and which associated decision(s) should he make if he chooses the menu without knowing who will attend? To answer
this question, we can use a query in which bpJ , bpM , epJ , and epM are eliminated before
mc and w to represent the fact that their values are not known when the menu is chosen.
This query is:
(N , (max, {mc, w}).(u , {bpJ , bpM , epJ , epM })).
The answer is $6K, with (mc, meat).(w, red) as a decision. If Peter knows who comes, the
query becomes
(N , (u , {bpJ , bpM }).(max, {mc, w}).(u , {epJ , epM })).
and optimal values for mc and w can depend on bpJ and bpM . The answer is $26K with
a $20K gain from the observability of who is present. The decision rule for {mc, w} is
(mc, meat).(w, red) if John is present and Mary is not, (mc, f ish).(w, white) otherwise.
Consider the query introduced at the beginning of Section 6.1:
(N , (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM })).
The answer is : in the worst main course case, even if Peter chooses the wine, the situation can be unacceptable. In order to compute the expected utility for each menu choice,
we can use a query in which mc and w are free variables:
10.  is the unfeasible value, cf. Definition 6.

456

fiThe PFU Framework

(N , (u , {bpJ , bpM , epJ , epM })).
The answer is a function of {mc, w}. These examples show how queries can capture various
situations in terms of partial observabilities, optimistic/pessimistic attitude, and parameters
in the decision process.
6.3 Operational Answer to a Query
The quantities PS | l(S) (A.A0 ) and FS | l(S) (A.A0 ) involved in the definition of the semantic answer to a query are not directly available from the local functions and can be very expensive
to compute. For instance, with
probabilities, PS | l(S) (A.A0 ) equals PS,l(S) (A.A0 )/Pl(S) (A).
P
0 00
Computing PS,l(S) (A.A0 ) =
A00 dom(V (Sl(S))) PVE ,VD (A.A .A ) typically requires time
exponential in |V  (S  l(S))|. Moreover, such quantities must be computed at each node
of the decision tree. Fortunately, there exists an alternative definition of the answer to
a query, which can be directly expressed using a PFU instance, i.e. the expressed local
plausibility, feasibility, and utility functions.
Definition 31. The operational answer Op-Ans(Q) to a query Q = (N , Sov) is a function
of the free variables of Q: if A is an assignment of the free variables, then (Op-Ans(Q))(A)
is defined inductively as follows:
(Op-Ans(Q))(A) = Qo (N , Sov, A)
Qo(N , (op, S) . Sov, A) = opA0 dom(S) Qo (N , Sov, A.A0 )
!



!
Qo(N , , A) =
 Fi ? p Pi pu u Ui
(A).
Fi F

Pi P

(9)
(10)

Ui U

By Equation 10, if all the problem variables are assigned, the answer to the query is the
combination of the plausibility degree, the feasibility degree, and the utility degree of the
corresponding complete assignment. By Equation 9, if the variables are not all assigned and
(op, S) is the leftmost operator-variable(s) pair in Sov, the answer to the query is obtained
by eliminating S using op as an elimination operator. Again, optimal decision rules for the
decision variables can be recorded if needed, using argmin and argmax. Equivalently, by
considering a sequence of operator-variable(s) pairs as a sequence of variable eliminations,
Op-Ans(Q) can be written:
!
!



Op-Ans(Q) = Sov
 Fi ? p Pi pu u Ui
.
Fi F

Pi P

Ui U

It shows that Op-Ans(Q) actually corresponds to the generic form of Equation 8.
6.4 Equivalence Theorem
Theorem 2 proves that the semantic definition Sem-Ans(Q) gives semantic foundations to
what is computed with the operational definition Op-Ans(Q).
Theorem 2. If the plausibility structure is conditionable, then, for all queries Q on a PFU
network, Sem-Ans(Q) = Op-Ans(Q) and the optimal policies for the decisions are the same
with Sem-Ans(Q) and Op-Ans(Q).
457

fiPralet, Verfaillie, & Schiex

In other words, Theorem 2 shows that it is possible to perform computations in a
completely generic algebraic framework, while providing the result of the computations with
decision-theoretic foundations. Due to this equivalence theorem, Op-Ans(Q) is denoted
simply by Ans(Q) in the following. Note that the operational definition applies even in
a non-conditionable plausibility structure. Giving a decision-theoretic-based semantics to
Op-Ans when the plausibility structure is not conditionable is an open issue.
6.5 Bounded Queries
It may be interesting to relax the problem of computing the exact answer to a query.
Assume that the leftmost operator-variable(s) pair in the sequence Sov is (max, {x}), with
x a decision variable. From the decision maker point of view, computing decision rules
providing an expected utility greater than a given threshold  may be sufficient. This is
the case with the E-MAJSAT problem, defined as Given a boolean formula over a set
of variables V = VD  VE , does there exist an assignment of VD such that the formula is
satisfied for at least half of the assignments of VE ?  Extending the generic PFU framework
to answer such queries is done in Definitions 32 and 33, which introduce bounded queries.
Definition 32. A bounded query B-Q is a triple (N , Sov, ), such that (N , Sov) is a query
and   Eu ( is the threshold).
Definition 33. The answer Ans(B-Q) to a bounded query B-Q = (N , Sov, ) is a boolean
function of the free variables of the unbounded query Q = (N , Sov). For every assignment
A of these free variables,

t if Ans(Q)(A) u 
(Ans(B-Q))(A) =
f otherwise.
As the threshold  may be used to prune the search space during the resolution, computing the answer to a bounded query is easier than computing the answer to an unbounded
one.
6.6 Back to Existing Frameworks
Let us consider again the frameworks of Section 3. Solving a CSP (Equation 1) or a totally
ordered soft CSP corresponds to the query Q = (N , (max, V )), with N the PFU network
corresponding to the CSP and V the set of variables of the CSP. Computing the probability
distribution of a variable y for a Bayesian network (Equation 2) modeled as N corresponds
to Q = (N , (+, V  {y}). These examples are mono-operator queries, involving only one
type of elimination operator.
Consider multi-operator queries. The search for an optimal policy for the stochastic
CSP associated with Equation 4 is captured by a query such as Q = (N , (max, {d1 , d2 })
.(+, {s1 }).(max, {d3 , d4 }).(+, {s2 })). The query on influence diagrams of Equation 5 and
the query on valuation networks of Equation 6 are captured the same way.
For a finite horizon MDP with T time-steps (Equation 7), the query looks like Q =
(N , (max, {d1 }).(u , {s2 }).(max, {d2 }) . . . (u , {sT }).(max, {dT })), where u = + with probabilistic MDPs and u = min with pessimistic possibilistic MDPs. The initial state s1 is a

458

fiThe PFU Framework

free variable. With a quantified CSP or a quantified boolean formula, elimination operators
min and max are used to represent  and .
More formally, we can show:
Theorem 3. Queries and bounded queries can be used to express and solve the following
list of problems:
1. SAT framework: SAT, MAJSAT, E-MAJSAT, quantified boolean formula, stochastic
SAT (SSAT) and extended-SSAT (Littman et al., 2001).
2. CSP (or CN) framework:
 Check consistency for a CSP (Mackworth, 1977); find a solution to a CSP; count
the number of solutions of a CSP.
 Find a solution of a valued CSP (Bistarelli et al., 1999).
 Solve a quantified CSP (Bordeaux & Monfroy, 2002).
 Find a conditional decision or an unconditional decision for a mixed CSP or a
probabilistic mixed CSP (Fargier et al., 1996).
 Find an optimal policy for a stochastic CSP or a policy with a value greater than
a threshold; solve a stochastic COP (Constraint Optimization Problem) (Walsh,
2002).
3. Integer Linear Programming (Schrijver, 1998) with finite domain variables.
4. Search for a solution plan with a length  k in a classical planning problem (STRIPS
planning, Fikes & Nilsson, 1971; Ghallab et al., 2004).
5. Answer classical queries on Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993), and chain graphs (Frydenberg, 1990), with plausibilities
expressed as probabilities, possibilities, or -rankings:
 Compute plausibility distributions.
 MAP (Maximum A Posteriori hypothesis) and MPE (Most Probable Explanation).
 Compute the plausibility of an evidence.
 CPE task for hybrid networks (Dechter & Larkin, 2001) (CPE means CNF Probability Evaluation, a CNF being a formula in Conjunctive Normal Form).
6. Solve an influence diagram (Howard & Matheson, 1984).
7. With a finite horizon, solve a probabilistic MDP, a possibilistic MDP, a MDP based on
-rankings, completely or partially observable (POMDP), factored or not (Puterman,
1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000).

459

fiPralet, Verfaillie, & Schiex

6.7 Towards More Complex Queries
Queries can be made more complex by relaxing some assumptions:
 In the definition of queries, the order u on Eu is assumed to be total. Extending
the results to a partial order is possible if (Eu , u ) defines a lattice (partially ordered
set closed under least upper and greatest lower bounds) and if pu distributes over
the least upper bound lub and greatest lower bound glb (i.e. p pu lub(u1 , u2 ) =
lub(p pu u1 , p pu u2 ) and p pu glb(u1 , u2 ) = glb(p pu u1 , p pu u2 )). This allows
semiring CSPs (Bistarelli et al., 1999) to be captured in the framework. We believe
that other extensions to partial orders on utilities should allow algebraic MDPs (Perny
et al., 2005) to be captured.
 We can try to relax the no-forgetting assumption, as in limited memory influence
diagrams (LIMIDs, Lauritzen & Nilsson, 2001), which show that this can be relevant
for decision processes involving multiple decision makers or memory constraints on
the policy recording. In such cases, optimal decisions can become nondeterministic
(decisions such as choose x = 0 with probability p and x = 1 with probability 1p).
 The order in which decisions are made and environment variables are observed is
total and completely determined by the query. One may wish to compute not only
an optimal policy for the decisions, but also an optimal order in which to perform
decisions, without exactly knowing the steps at which other agents make decisions or
the steps at which observations are made. Work on influence diagrams with unordered
decisions (Jensen & Vomlelova, 2002) is good starting point to try and extend our
work in this direction.
While it should be possible to relax the assumption that variables have a finite domain,
doing this is nontrivial, since transforming u = + into integrals is not straightforward,
and performing min- or max-eliminations over continuous domains requires the guarantee
of existence of a supremum.
6.8 Summary
In Section 6, the last element of the PFU framework, a class of queries on PFU networks, has
been introduced. A decision-tree based definition of the answer to a query has been provided.
The first main result of the section is Theorem 2, which gives theoretical foundations to
another equivalent operational definition, reducing the answer to a query to a sequence of
eliminations on a combination of scoped functions. The latter is best adapted to future
algorithms, because it directly handles the local functions defined by a PFU network. The
second important result is Theorem 3, which shows that many standard queries are PFU
queries. Overall, the PFU framework is captured by Definitions 14, 16, and 17 for algebraic
structures, Definition 26 for PFU networks, and Definitions 27 and 31 for queries.

7. Gains and Costs
A better understanding Theorem 3 shows that many existing frameworks are instances
of the PFU framework. Through this unification, similarities and differences between ex460

fiThe PFU Framework

isting formalisms can be analyzed. For instance, by comparing VCSPs and the optimistic
version of finite horizon possibilistic MDPs through the operational definition of the answer
to a query, it appears that a finite horizon optimistic possibilistic MDP (partially observable
or not) is a fuzzy CSP: both can indeed be represented as a query Q whose operational
answer looks like maxV (min ), where V is a set of variables and  is a set of scoped
functions. Techniques available for solving fuzzy CSPs can then be used to solve finite
horizon optimistic possibilistic MDPs.
From the complexity theory point of view, studying the time and space complexity for
answering queries of the form of Equation 8 can lead to upper bounds on the complexity
for several frameworks simultaneously. One may also try to characterize which properties
lead to a given theoretical complexity.
Increased expressive power The expressive power of PFU networks is the result of
a number of features: (1) flexibility of the plausibility/utility model; (2) flexibility of the
possible networks; (3) flexibility of the queries in terms of situation modeling. This enables
queries on PFU networks to cover generic finite horizon sequential decision problems with
plausibilities, feasibilities, and utilities, cooperative or adversarial decision makers, partial observabilities, and possible parameters in the decision process modeled through free
variables.
As none of the frameworks indicated in Theorem 3 presents such flexibility, for every
subsumed formalism X indicated in Theorem 3, it is possible to find a problem which can
be represented with PFUs but not directly with X. More specifically, compared to influence
diagrams (Howard & Matheson, 1984; Jensen & Vomlelova, 2002; Smith et al., 1993; Nielsen
& Jensen, 2003; Jensen et al., 2004) or valuation networks (VNs, Shenoy, 1992, 2000;
Demirer & Shenoy, 2001), PFUs can deal with more than the probabilistic expected additive
utility and allow us to perform eliminations with min to model the presence of adversarial
agents. Thus, quantified boolean formulas cannot be represented with influence diagrams
or VNs, but are covered by PFU networks (see Theorem 3). Moreover, PFU networks use a
DAG which captures the normalization conditions of plausibilities or feasibilities, whereas
with VNs, this information is lost. Compared to sequential influence diagrams (Jensen
et al., 2004) or sequential VNs (Demirer & Shenoy, 2001), PFUs can express some so-called
asymmetric decision problems (problems in which some variables may not even need to be
considered in a decision process) by adding dummy values to variables.
Actually, some simple problems which can be expressed with PFUs cannot be apparently
directly expressed in other frameworks. The simple instance feasibilities with normalization
conditions + hard requirements is not captured by any of the subsumed frameworks. For
example, using a CSP to model it would result in a loss of the information provided by
the normalization conditions on feasibilities. The same occurs for influence diagrams like sequential decision processes based on possibilistic expected utility, which could be
called possibilistic influence diagrams. Similarly for stochastic CSPs without contingency
assumption.
The cost of greater flexibility and increased expressive power is that the PFU framework
cannot be described as simply and straightforwardly as, for example, constraint networks.
Generic algorithms Section 8 shows that generic algorithms can be built to answer
queries on PFU networks. As previously said, building generic algorithms should facilitate
461

fiPralet, Verfaillie, & Schiex

cross-fertilization in the sense that any of the subsumed formalisms will directly benefit
from the techniques developed on another subsumed formalism. This fits into a growing
effort to generalize resolution methods used for different AI problems. For example, soft
constraint propagation drastically improves the resolution of valued CSPs; integrating such
a tool in a generic algorithm on PFUs could improve the resolution of influence diagrams.
Using abstract operators may enable us to identify algorithmically interesting properties,
or to infer necessary or sufficient conditions for a particular algorithm to be usable.
However, one could argue that some techniques are highly specific to one formalism or
to one type of problem, and that, in this case, dedicated approaches certainly outperform
a generic algorithm. A solution for this can be to characterize the actual properties used
by a dedicated approach, in order to generalize it as much as possible. Moreover, even if
specialized schemes usually improve over generic ones, there exist cases in which general
tools can be more efficient than specialized algorithms, as shown by the use of SAT solvers
for solving STRIPS planning problems (Sang, Beame, & Kautz, 2005).

8. Algorithms
The ability to design generic algorithms is one of the motivations for building the PFU
framework, and some choices are justified by algorithmic considerations. We present generic
algorithms that answer arbitrary PFU queries.
8.1 A Generic Tree Search Algorithm
The operational definition of the answer to a query Q actually defines a naive exponential
time algorithm to compute Ans(Q) using a tree-exploration procedure, with a variable
ordering given by Sov, that collects elementary plausibilities, feasibilities, and utilities.
More precisely, for each assignment A of the free variables of Q, a tree is explored. Each
node in this tree corresponds to a partial assignment of the variables. The value of a leaf
is provided by the combination of the scoped functions of the PFU network, applied to
the complete assignment defined by the path from the root to the leaf. Depending on
the operator used, the value of an internal node is computed by performing a min, max,
or u operation on the values of its children. The root node returns (Ans(Q))(A). The
corresponding pseudo-code is given in Figure 2. For a query (N , Sov), the first call is
TreeSearchAnswerQ(N , Sov). It returns a function of the free variables.
If we assume that every operator returns a result in a constant time, then the time
complexity of the algorithm is O(m  n  ln(d)  dn ), where d stands for the maximum domain
size, n stands for the number of variables of the PFU network, and m stands for the number
of scoped functions.11
The space complexity is polynomial (it can be shown to be linear in the entry data
size). Hence, computing the answer to a bounded query is PSPACE. Moreover, given that
the satisfiability of a QBF is a PSPACE-complete problem which can be expressed as a
bounded query (cf. Theorem 3), it follows that computing the answer to a bounded query
is PSPACE-hard. Being PSPACE and PSPACE-hard, the decision problem that consists of
11. The factor n  ln(d) corresponds to an upper bound on the time needed to get (A) for a scoped function
 represented as a table (of size  dn ).

462

fiThe PFU Framework

TreeSearchAnswerQ((V, G, P, F, U ), Sov)
begin
foreach A  dom(Vf r ) do (A)  AnswerQ((V, G, P, F, U ), Sov, A)
return 
end
AnswerQ((V, G, P, F, U ), Sov, A)
begin
if Sov =  then return ((Fi F Fi ) ? (p Pi P Pi ) pu (uUi U Ui ))(A)
else
(op, S).Sov 0  Sov
choose x  S
if S = {x} then Sov  Sov 0 else Sov  (op, S  {x}).Sov 0
dom  dom(x)
res  
while dom 6=  do
choose a  dom
dom  dom  {a}
res  op (res, AnswerQ((V, G, P, F, U ), Sov, A.(x, a)))
return res
end

Figure 2:
A generic tree search algorithm for answering a query Q
((V, G, P, F, U ), Sov)

=

answering a bounded query is PSPACE-complete. This result is not surprising, but it gives
an idea of the level of expressiveness which can be reached by the PFU framework. More
work is needed to identify subclasses of queries with a lower complexity, although many are
already known.
8.2 A Generic Variable Elimination Algorithm
Quite naturally, a generic variable elimination algorithm (Bertele & Brioschi, 1972; Shenoy,
1991; Dechter, 1999; Kolhas, 2003) can be defined to answer queries on a PFU network.
8.2.1 A First Naive Scheme
This first naive variable elimination algorithm is given in Figure 3. It eliminates variables
from the right to the left of the sequence Sov of the query, whereas with the tree search
procedure, variables are assigned from the left to the right. This right-to-left processing
entails that the algorithm naturally returns a function of the free variables of the query.
The first call is VarElimAnswerQ((V, G, P, F, U ), Sov).
The version presented in Figure 3 is actually a very naive variable elimination scheme
with time and space complexities O(m  n  ln(d)  dn ) and O(m  dn ) respectively: it begins
by combining all the scoped functions before eliminating variables, whereas the interest of
a variable elimination algorithm is primarily to use the factorization into local functions.

463

fiPralet, Verfaillie, & Schiex

VarElimAnswerQ((V, G, P, F, U ), Sov)
begin
0  ((Fi F Fi ) ? (p Pi P Pi ) pu (uUi U Ui ))
while Sov 6=  do
Sov 0 .(op, S)  Sov
choose x  S
if S = {x} then Sov  Sov 0 else Sov  Sov 0 .(op, S  {x})
0  opx 0
return 0
end

Figure 3: A first generic variable elimination algorithm for answering a query Q =
((V, G, P, F, U ), Sov)
8.2.2 Improving the Basic Scheme
The algorithm of Figure 3 works on a unique global function defined by the combination
of all the plausibility, feasibility, and utility functions (first line), whereas a factorization is
available. To improve this scheme, the properties of the algebraic structure can be used.
In the sequel, we denote by +x (resp. x ) a scoped function which has (resp. does not
have) x in its scope. Moreover, we extend every combination operator  on E  {} by
setting   e = e   =  (combining anything with something unfeasible is unfeasible
too).12
First, in order to use the factorization of plausibilities and feasibilities, we can use the
properties below, which come from the right monotonicity of pu , the distributivity of pu
over u , and the definition
truncation operator ?:
 of the x
min
(P
pu U ) = P x pu (minx U )
x




maxx (P x pu U ) = P x pu (maxx U )



ux (P x pu U ) = P x pu (ux U )
minx (F x ? U ) = F x ? (minx U )





maxx (F x ? U ) = F x ? (maxx U )


ux (F x ? U ) = F x ? (ux U ) .
They express that when a variable x is eliminated, it is not necessary to consider plausibility
functions or feasibility functions that do not have x in their scope.
However, it is necessary to add some axioms on the expected utility structure, since
in the general case, an expression such as ux (P +x pu (U x u U +x )) cannot be decomposed. We give two axioms, Ax1 and Ax2, each of which is a sufficient additional condition
to exploit the factorization of utility functions.
(Ax1) (Ep , p ) = (Eu , u ), p = u , p = u = pu
(Ax2) u = u on Eu (but not on Eu  {}).
12. An operator op can be used both as a combination operator between scoped functions and as an elimination operator over some variables. In this case, the extension of op used as a combination operator creates
an operator op0 such that op0 (e, ) = , whereas the extension of op used as an elimination operator
creates an operator op00 such that op00 (e, ) = e. op0 and op00 coincide on E but differ on E  {}.

464

fiThe PFU Framework

Among the cases in Table 1, rows 2, 3, 5, 6 satisfy Ax1, whereas rows 1, 4, 7, 8 satisfy
Ax2. Ax1 andAx2 enable us to write:
minx (F +x ? (U x u U +x )) = U x u (minx F +x ? U +x )
maxx (F +x ? (U x u U +x )) = U x u (maxx F +x ? U +x ) .
and
 x

U u (ux P +x pu U +x ) with Ax1
+x
x
+x
u P pu (U u U ) =
((p x P +x ) pu U x ) u (ux P +x pu U +x ) with Ax2.
x
Hence, when eliminating a variable x, it is not necessary to consider utility functions which
do not have x in their scope.
We present an algorithm when Ax1 is satisfied. When Ax2 holds, working on plausibility/utility pairs (p, u) allows Ax1 to be recovered: this is used, for example, to solve
influence diagrams (Ndilikilikesha, 1994). When Ax1 is satisfied, there is actually only one
set E = Ep = Eu , one order =p =u , one combination operator  = p = u = pu ,
and one elimination operator  = p = u . Rather than express feasibilities on {t, f }, we
can express them on {1E , } by mapping t onto 1E and f onto : this preserves the value
of the answer to a query, since f ? u =   u and t ? u = 1E  u.
The improved variable elimination algorithm is shown in Figure 4. To answer a query
Q = ((V, G, P, F, U ), Sov), the first call is Ax1-VarElimAnswerQ(P  F  U, Sov). It
returns a set of scoped functions whose -combination equals Ans(Q). This time, the
factorization available in a PFU network is exploited, since when eliminating a variable x,
only scoped functions involving x are considered.
Ax1-VarElimAnswerQ(, Sov)
begin
if Sov =  then return 
else
Sov 0 .(op, S)  Sov
choose x  S
if S = {x} then Sov  Sov 0 else Sov  Sov 0 .(op, S  {x})
+x  {   | x  sc()}
0  opx +x 
  (  +x )  {0 }
return Ax1-VarElimAnswerQ(, Sov)
end

Figure 4: Variable elimination algorithm when Ax1 holds (: set of scoped functions)
When Ax1 holds, the algorithm is actually a standard variable elimination algorithm on a
commutative semiring. As for classical variable elimination algorithms, the time complexity
of this algorithm is in O(m  n  ln(d)  dw+1 ), where w is the tree-width (Bodlaender, 1997;
Dechter & Fattah, 2001) of the network of scoped functions, constrained by the elimination
order imposed by Sov. Yet, its space complexity is also exponential in this tree-width.
8.3 Other Approaches
Starting from the generic tree-search algorithm of Section 8.1, bound computations and local
consistencies (Mackworth, 1977; Cooper & Schiex, 2004; Larrosa & Schiex., 2003) can be

465

fiPralet, Verfaillie, & Schiex

integrated in order to prune the search space. Local consistencies can improve the quality of
the bounds thanks to the use of smaller local functions. Techniques coming from quantified
boolean formulas or from game algorithms (such as the -algorithm) can be considered to
more efficiently manage bounds when min and max operators alternate. Caching strategies
exploiting the problem structure (Darwiche, 2001; Jegou & Terrioux, 2003) are also obvious
candidates to improve the basic tree search scheme. Additional axioms Ax1 and Ax2 can
be useful in this direction. Heuristics for the choice of the variable to assign when a pair
(op, S) is encountered, as well as heuristics for the value choices, may also speed up the
search.
In another direction, approximate algorithms using sampling and local search could also
be considered: sampling when eliminations with + (+, and not u ) are performed, local
search when eliminations with min or max are performed.

9. Conclusion
In the last decades, AI has witnessed the design and study of numerous formalisms for
reasoning about decision problems. In this article, we have built a generic framework
to model sequential decision making with plausibilities, feasibilities, and utilities. This
framework covers many existing approaches, including hard, valued, quantified, mixed, and
stochastic CSPs, Bayesian networks, finite horizon probabilistic or possibilistic MDPs, or
influence diagrams. The result is an algebraic framework built upon decision-theoretic
foundations: the PFU framework. The two facets of the PFU framework are explicit in
Theorem 2, which states that the operational definition of the answer to a query is equivalent
to the decision tree-based semantics. This is the result of a design that accounts both for
expressiveness and for computational aspects.
Compared to related works (Shenoy, 1991; Dechter, 1999; Kolhas, 2003), the PFU framework is the only framework which directly deals with different types of variables (decision
and environment variables), different types of local functions (plausibilities, feasibilities,
utilities), and different types of combination and elimination operators.
From an algorithmic point of view, generic algorithms based on tree search and variable
elimination have been described. They prove that the PFU framework is not just an abstraction. The next step is to explore ways of improving these algorithms, so as to generalize
techniques that are used in formalisms subsumed by the PFU framework. Along this line,
a generic approach to query optimization has lead to the definition of original architectures
for answering queries, called multi-operator cluster trees and multi-operator cluster DAGs.
These can be applied to QBFs and other structures compatible with Ax1 (Pralet, Schiex, &
Verfaillie, 2006a), as well as influence diagrams and other structures satisfying Ax2 (Pralet,
Schiex, & Verfaillie, 2006b).

Acknowledgments
We would like to thank Jean-Loup Farges, Jerome Lang, Regis Sabbadin, and the three
anonymous reviewers for useful comments on previous versions of this article. The work

466

fiThe PFU Framework

described in this article was initiated when the first author was at LAAS-CNRS and INRA
Toulouse. It was partially conducted within the EU Integrated Project COGNIRON (The
Cognitive Companion) and funded by the European Commission Division FP6-IST Future
and Emerging Technologies under Contract FP6-002020.

Appendix A. Notations
See Table 2.
Symbol

p
u

p
u
pu
p
u
?


VE
VD
dom(x)
dom(S)
G
paG (x)
ndG (x)
CE (G)
CD (G)
Pi
Fi
Ui
F act(c)
sc()
PS
PS1 | S2
FS
FS1 | S2

M eaning
Elimination operator
Elimination operator on plausibilities
Elimination operator on utilities
Combination operator
Combination operator for plausibilities
Combination operator for utilities
Combination operator between plausibilities and utilities
Partial order on plausibilities
Partial order on utilities
Truncation operator
Unfeasible value

Environment variables
Decision variables
Domain
of values of a variable x
Q
dom(x)
xS
Directed Acyclic Graph (DAG)
Parents of x in the DAG G
Non-descendant of x in the DAG G
Set of environment components of G
Set of decision components of G
Plausibility function
Feasibility function
Utility function
Pi or Fi factors associated with a component c
Scope of a local function 
Plausibility distribution over S
Conditional plausibility distribution of S1 given S2
Feasibility distribution over S
Conditional feasibility distribution of S1 given S2

Sov
Sequence of operator-variable(s) pairs
Sem-Ans(Q) Semantic answer to a query Q (decision trees)
Op-Ans(Q) Operational answer to a query Q
Ans(Q)
Answer to a query Q

Table 2: Notation.
467

fiPralet, Verfaillie, & Schiex

Appendix B. Proofs
Proposition 1 A plausibility distribution PS can be extended to give a plausibility distribution PS 0 over every S 0  S, defined by PS 0 = p SS 0 PS .
Proof. Given that p is associative and commutative, p S 0 PS 0 = p S 0 (p SS 0 PS ) = p S PS = 1p .
Thus, PS 0 : dom(S 0 )  Ep is a plausibility distribution over S 0 .

Proposition 2 The structures presented in Table 1 are expected utility structures.
Proof. It is sufficient to verify each of the required axioms successively.

Proposition 3 If (Ep , p , p ) is a conditionable plausibility structure, then all plausibility distributions are conditionable: it suffices to define PS1 | S2 by PS1 | S2 (A) = max{p 
Ep | PS1 ,S2 (A) = p p PS2 (A)} for all A  dom(S1  S2 ) satisfying PS2 (A) 6= 0p .
Proof. Let PS be a plausibility distribution over S. For all S1 , S2 disjoint subsets of S and for all
A  dom(S1  S2 ) satisfying PS2 (A) 6= 0p , let us define PS1 | S2 (A) = max{p  Ep | PS1 ,S2 (A) =
p p PS2 (A)}. We must show that the PS1 | S2 functions satisfy axioms a, b, c, d, e of Definition 18.
(a) By definition of PS1 | S2 and by distributivity of p over p , we can write
PS2 = p S1 PS1 ,S2 = p S1 (PS1 | S2 p PS2 ) = (p S1 PS1 | S2 ) p PS2 .
As PS2 p PS2 , we can infer that p S1 PS1 | S2 p 1p . Let A2 be an assignment of S2 satisfying
PS2 (A2 ) 6= 0p . Assume that the hypothesis (H): p S1 PS1 | S2 (A2 ) p 1p  holds.
Then, for all A1  dom(S1 ), PS1 ,S2 (A1 .A2 ) p PS2 (A2 ), since if PS1 ,S2 (A1 .A2 ) = PS2 (A2 ),
then PS1 | S2 (A1 .A2 ) = 1p , which implies that p S1 PS1 | S2 (A2 ) p 1p by the monotonicity of
p . Moreover, (H) implies that there exists a unique p  Ep satisfying (p S1 PS1 | S2 (A2 )) p
p = 1p . Combining this equation by PS2 (A2 ) gives PS2 (A2 ) p PS2 (A2 ) p p = PS2 (A2 ), i.e.
PS2 (A2 ) p (1p p p) = PS2 (A2 ). This implies that 1p p p p 1p . Given that 1p p p p 1p
(by monotonicity of p ), we obtain 1p p p = 1p . We analyze two cases.
 If p p 1p , there exists a unique p0 satisfying p0 p p = 1p . As both (p S1 PS1 | S2 (A2 )) p p
= 1p and 1p p p = 1p , this entails that p S1 PS1 | S2 (A2 ) = 1p , which contradicts (H).
 If p = 1p , then 1p p 1p = 1p . This entails that p is idempotent. Let dom0 be a subset
of dom(S1 ) such that p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ). Let A01  dom0 . We can
write:
(
PS1 ,S2 (A01 .A2 ) p (p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 )) = PS2 (A2 )
1
.
PS1 ,S2 (A01 .A2 ) p (p A1 dom0 PS1 ,S2 (A1 .A2 )) = PS2 (A2 ) (as p is idempotent)
As PS1 ,S2 (A01 .A2 ) p PS2 (A2 ), there exists a unique p00  Ep such that PS1 ,S2 (A01 .A2 )p
p00 = PS2 (A2 ). Therefore, p A1 dom0 PS1 ,S2 (A1 .A2 ) = p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 ),
1
which gives p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 ) = PS2 (A2 ).
1

The assumption p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ) holds for dom0 = dom(S1 ). Recursively applying the previous mechanism by removing one assignment in dom0 at each
iteration leads to p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ) with |dom0 | = 1, i.e. it leads to
PS1 ,S2 (A001 .A2 ) = PS2 (A2 ) with dom0 = {A001 }. As a result, we obtain a contradiction.
In both cases, a contradiction with (H) is obtained, so p S1 PS1 | S2 (A2 ) = 1p .
(b) PS1 = PS1 |  p P = PS1 |  p (p S PS ) = PS1 |  p 1p = PS1 |  .
468

fiThe PFU Framework

(d) Let A  dom(S1  S2  S3 ) satisfying PS2 ,S3 (A) 6= 0p . Then, PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p
PS2 | S3 (A) holds, because:
 If PS1 ,S2 ,S3 (A) p PS3 (A), then, there exists a unique p  Ep such that PS1 ,S2 ,S3 (A) =
pp PS3 (A). As both PS1 ,S2 ,S3 (A) = PS1 ,S2 | S3 (A)p PS3 (A) (by definition of PS1 ,S2 | S3 )
and PS1 ,S2 ,S3 (A) = PS1 | S2 ,S3 (A)p PS2 | S3 (A)p PS3 (A) (by definition of PS1 | S2 ,S3 and
PS2 | S3 ), this implies that PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p PS2 | S3 (A).
 Otherwise, PS1 ,S2 ,S3 (A) = PS3 (A). This implies that 1p p PS1 ,S2 | S3 (A) and, as
PS1 ,S2 | S3 (A) p 1p , that PS1 ,S2 | S3 (A) = 1p . Similarly, this entails that PS2 | S3 (A) = 1p
and PS1 | S2 ,S3 (A) = 1p (the monotonicity of p implies that PS1 ,S2 ,S3 (A) = PS2 ,S3 (A) =
PS3 (A)). As 1p = 1p p 1p , we get PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p PS2 | S3 (A).
(c)

p S1 PS1 ,S2 | S3

= p S1 (PS1 | S2 ,S3 p PS2 | S3 ) (using (d))
= (p S1 PS1 | S2 ,S3 ) p PS2 | S3 (because p distributes over p )
= PS2 | S3 (using (a))

(e) Assume that PS1 ,S2 ,S3 = PS1 | S3 p PS2 | S3 p PS3 . Let A  dom(S1  S2  S3 ) such that
PS3 (A) 6= 0p . Then, PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A) holds, because:
 If PS1 ,S2 ,S3 (A) p PS3 (A), there exists a unique p  Ep such that PS1 ,S2 ,S3 (A) =
p p PS3 (A), and therefore PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A).
 Otherwise, we can write PS1 | S3 (A) = PS2 | S3 (A) = PS1 ,S2 | S3 (A) = 1p by using reasoning similar to that of (d), and therefore PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A).

Proposition 4 I(., . | .) satisfies the semigraphoid axioms:
1. symmetry: I(S1 , S2 | S3 )  I(S2 , S1 | S3 ),
2. decomposition: I(S1 , S2  S3 | S4 )  I(S1 , S2 | S4 ),
3. weak union: I(S1 , S2  S3 | S4 )  I(S1 , S2 | S3  S4 ),
4. contraction: (I(S1 , S2 | S4 )  I(S1 , S3 | S2  S4 ))  I(S1 , S2  S3 | S4 ).
Proof.
1. Symmetry axiom: directly satisfied by commutativity of p .
2. Decomposition axiom: assume that I(S1 , S2  S3 | S4 ) holds. Then
PS1 ,S2 | S4 = p S3 PS1 ,S2 ,S3 | S4
= p S3 (PS1 | S4 p PS2 ,S3 | S4 ) (since I(S1 , S2  S3 | S4 ))
= PS1 | S4 p (p S3 PS2 ,S3 | S4 ) (by distributivity of p over p )
= PS1 | S4 p PS2 | S4 .
Thus, I(S1 , S2 | S4 ) holds.
3. Weak union axiom: assume that I(S1 , S2  S3 | S4 ) holds. The decomposition axiom entails
that I(S1 , S3 | S4 ) is also satisfied. Then
PS1 ,S2 ,S3 ,S4 = PS1 ,S2 ,S3 | S4 p PS4 (chain rule)
= PS1 | S4 p PS2 ,S3 | S4 p PS4 (since I(S1 , S2  S3 | S4 ))
= PS1 | S4 p PS3 | S4 p PS4 p PS2 | S3 ,S4 (chain rule)
= PS1 ,S3 | S4 p PS4 p PS2 | S3 ,S4 (since I(S1 , S3 | S4 ))
= PS1 | S3 ,S4 p PS2 | S3 ,S4 p PS3 ,S4 (chain rule).
From axiom (e) in Definition 18, we can infer that PS1 ,S2 | S3 ,S4 = PS1 | S3 ,S4 p PS2 | S3 ,S4 , i.e.
I(S1 , S2 | S3  S4 ) holds.
469

fiPralet, Verfaillie, & Schiex

4. Contraction axiom: assume that I(S1 , S2 | S4 ) and I(S1 , S3 | S2  S4 ) hold. Then
PS1 ,S2 ,S3 | S4 = PS1 ,S3 | S2 ,S4 p PS2 | S4 (chain rule)
= PS1 | S2 ,S4 p PS3 | S2 ,S4 p PS2 | S4 (since I(S1 , S3 | S2  S4 ))
= PS1 ,S2 | S4 p PS3 | S2 ,S4 (chain rule)
= PS1 | S4 p PS2 | S4 p PS3 | S2 ,S4 (since I(S1 , S2 | S4 ))
= PS1 | S4 p PS2 ,S3 | S4 (chain rule).
Thus, I(S1 , S2  S3 | S4 ) holds.

Theorem 1 (Conditional independence and factorization) Let (Ep , p , p ) be a conditionable plausibility structure and let G be a DAG of components over S.
(a) If G is compatible with a plausibility distribution PS over S, then PS = p cC(G) Pc | paG (c) .
(b) If, for all c  C(G), there is a function c,paG (c) such that c,paG (c) (A) is a plausibility
distribution over c for all assignments A of paG (c), then S = p cC(G) c,paG (c) is a
plausibility distribution over S with which G is compatible.
Proof.
(a) First, if |C(G)| = 1, G contains a unique component c1 . Then, p cC(G) Pc | paG (c) = Pc1 |  =
Pc1 : the proposition holds for |C(G)| = 1.
Assume that the proposition holds for all DAGs with n components. Let G be a DAG of
components compatible with a plausibility distribution PS and such that |C(G)| = n + 1.
Let c0 be a component labeling a leaf of G. As G is compatible with PS , we can write
I(c0 , ndG (c0 )  paG (c0 ) | paG (c0 )). As c0 is a leaf, ndG (c0 ) = S  c0 , and consequently
I(c0 , (S  c0 )  paG (c0 ) | paG (c0 )). This means that PSpaG (c0 ) | paG (c0 ) = Pc0 | paG (c0 ) p
P(Sc0 )paG (c0 ) | paG (c0 ) . Combining each side of the equation by PpaG (c0 ) gives
PS = Pc0 | paG (c0 ) p PSc0 .
0
Let G be the DAG obtained from G by deleting the node labeled with c0 . Then for every
component c  C(G0 ), paG0 (c) = paG (c) (since the deleted component c0 is a leaf). Moreover
ndG0 (c) equals either ndG (c) or ndG (c)c0 (again, since the deleted component c0 is a leaf). In
the first case (ndG0 (c) = ndG (c)), the property I(c, ndG (c)  paG (c) | paG (c)) directly implies
I(c, ndG0 (c)paG0 (c) | paG0 (c)). In the second case (ndG0 (c) = ndG (c)c0 ), the decomposition
axiom allows us to write I(c, ndG0 (c)  paG0 (c) | paG0 (c)) from I(c, ndG (c)  paG (c) | paG (c)).
Consequently, G0 is a DAG compatible with PSc0 . As |C(G0 )| = n, the induction hypothesis
gives PSc0 = p cC(G0 ) Pc | paG (c) , which implies that PS = p cC(G) Pc | paG (c) , as desired.
(b) Assume that for every component c, c,paG (c) (A) is a plausibility distribution over c for all
assignments A of paG (c). For |C(G)| = 1, C(G) = {c1 }. Then, S = c1 is a plausibility
distribution over c1 . Moreover, as  |  = 1p , we can write c1  |  = c1 |  p  |  , i.e.
I(c1 ,  | ). Therefore, G is compatible with c1 : the proposition holds for |C(G)| = 1.
Assume that the proposition holds for all DAGs with n components. Consider a DAG G
with n + 1 components. We first show that S is a plausibility distribution over S, i.e.
p S (p cC(G) c,paG (c) ) = 1p . Let c0 be a leaf component in G. As c0 is a leaf, the unique
scoped function whose scope contains a variable in c0 is c0 ,paG (c0 ) . By the distributivity of
p over p , this implies that
p c0 (p cC(G) c,paG (c) ) = (p c0 c0 ,paG (c0 ) ) p (p cC(G){c0 } c,paG (c) ).
Given that c0 ,paG (c0 ) (A) is a plausibility distribution over c0 for all assignments A of paG (c0 ),
470

fiThe PFU Framework

p c0 c0 ,paG (c0 ) = 1p . Consequently,
p c0 (p cC(G) c,paG (c) ) = p cC(G){c0 } c,paG (c) .
Applying the induction hypothesis to the DAG with n components obtained from G by deleting c0 , we can infer that p Sc0 (p cC(G){c0 } c,paG (c) ) = 1p . This allows us to write
p Sc0 (p c0 (p cC(G) c,paG (c) )) = 1p , i.e. p S S = 1p : S is a plausibility distribution
over S. It remains to prove that G is a DAG of components compatible with S . Let c  C(G).
We must show that I(c, ndG (c)  paG (c) | paG (c)) holds. There are two cases:
1. If c = c0 , we must prove that
c0 ,ndG (c0 )paG (c0 ) | paG (c0 ) = c0 | paG (c0 ) p ndG (c0 )paG (c0 ) | paG (c0 ) .
First, note that
c0 ,paG (c0 ) = p S(c0 paG (c0 )) (p cC(G) c,paG (c) )
= (p S(c0 paG (c0 )) (p cC(G){c0 } c,paG (c) )) p c0 ,paG (c0 )
(because p distributes over p and sc(c0 ,paG (c0 ) )  c0  paG (c0 )
= (p SpaG (c0 ) (p cC(G) c,paG (c) )) p c0 ,paG (c0 )
(because p distributes over p and c0 c0 ,paG (c0 ) = 1p )
= paG (c0 ) p c0 ,paG (c0 ) .
From this, it is possible to write:
ndG (c0 )paG (c0 ) | paG (c0 ) p c0 | paG (c0 ) p paG (c0 )
= ndG (c0 )paG (c0 ) | paG (c0 ) p c0 ,paG (c0 )
= ndG (c0 )paG (c0 ) | paG (c0 ) p paG (c0 ) p c0 ,paG (c0 )
= ndG (c0 ) p c0 ,paG (c0 )
= S{c0 } p c0 ,paG (c0 ) (because c0 is a leaf in G)
=

(p cC(G){c0 } c,paG (c) ) p c0 ,paG (c0 )

= p cC(G) c,paG (c)
= S .
Using axiom (e) of Definition 18, this entails that ndG (c0 )paG (c0 ) | paG (c0 ) p c0 | paG (c0 ) =
SpaG (c0 ) | paG (c0 ) , i.e., as S = c0  ndG (c0 ), that I(c0 , ndG (c0 )  paG (c0 ) | paG (c0 )).
2. Otherwise, c 6= c0 . Let G0 be the DAG obtained from G by deleting c0 . G0 contains
n components: by the induction hypothesis, I(c, ndG0 (c)  paG0 (c) | paG0 (c)). As c0
is a leaf in G, we have c0 
/ paG (c), which implies that paG0 (c) = paG (c). Thus,
I(c, ndG0 (c)  paG (c) | paG (c)).
(i) If ndG0 (c) = ndG (c), then it is immediate that I(c, ndG (c)  paG (c) | paG (c)).
(ii) Otherwise, ndG0 (c) 6= ndG (c). As c0 is a leaf in G, this is equivalent to say that
ndG (c) = ndG0 (c)  c0 . This means that c is not an ancestor of c0 , and a fortiori c 
/ paG (c0 ). In the following, the four semigraphoid axioms are used to
prove the required result. From the decomposition axiom, from I(c0 , ndG (c0 ) 
paG (c0 ) | paG (c0 )), and from (c  ndG0 (c))  ndG (c0 ) (because ndG (c0 ) = S 
c0 ), it follows that I(c0 , (c  ndG0 (c))  paG (c0 ) | paG (c0 )), or, in other words, as
c  paG (c0 ) = , that I(c0 , c  (ndG0 (c)  paG (c0 )) | paG (c0 )). Using the weak
union axiom leads to I(c0 , c | (ndG0 (c)  paG (c0 ))  paG (c0 )) and, using the symmetry axiom, to I(c, c0 | (ndG0 (c)  paG (c0 ))  paG (c0 )). As shown previously,
I(c, ndG0 (c)paG (c) | paG (c)). Together with I(c, c0 | (ndG0 (c)paG (c0 ))paG (c0 )),
the contraction axiom implies that I(c, (ndG0 (c)  paG (c))  c0 | paG (c)). As c0 
/
paG (c) and ndG (c) = ndG0 (c)  c0 , this means that I(c, ndG (c)  paG (c)) | paG (c)).
471

fiPralet, Verfaillie, & Schiex

We have proved that G is compatible with S . Consequently, the proposition holds if there
are n + 1 components in G, which ends the proof by induction.

Proposition 5 Let (Ep , p , p ) be a conditionable plausibility structure. Then, for all
n  N , there exists a unique p0 such that p i[1,n] p0 = 1p .
Proof. Let n  N . If p i[1,n] 1p = 1p , then p0 = 1p satisfies the required property. Moreover, in
this case, the distributivity of p over p implies that for all p  Ep , p i[1,n] p = p. Therefore, if
p i[1,n] p = 1p , then p = 1p , which shows that p0 is unique.
Otherwise, p i[1,n] 1p 6= 1p . In this case, as 1p p p i[1,n] 1p by monotonicity of p , we can
write 1p p p i[1,n] 1p . The second item of Definition 19 then implies that there exists a unique
p0  Ep such that 1p = p0 p (p i[1,n] 1p ), i.e. such that 1p = p i[1,n] p0 .

Proposition 6
Let PVE ,VD be the completion of a controlled plausibility distribution
PVE || VD . Then, PVE ,VD is a plausibility distribution over VE  VD and PVE | VD = PVE || VD .
Proof. PVE ,VD = PVE || VD p p0 , where p0 is the element of Ep such that p i[1,|dom(VD )|] p0 = 1p .
Then p VE VD PVE ,VD = p VE VD (PVE || VD p p0 ) = p VD ((p VE PVE || VD ) p p0 ) = p VD p0 =
p i[1,|dom(VD )|] p0 = 1p . This proves that PVE ,VD is a plausibility distribution over VE  VD .
As PVE ,VD = PVE || VD p p0 and PVE ,VD = PVE | VD p PVD , we can write PVE || VD p p0 =
PVE | VD p PVD . Moreover, PVD = p VE PVE ,VD = p VE (PVE || VD p p0 ) = p0 . Thus, PVE || VD p
p0 = PVE | VD p p0 . Summing this equation |dom(VD )| times with p gives PVE || VD = PVE | VD .

Proposition 7 Let G be a typed DAG of components over VE  VD . Let Gp be the partial
graph of G induced by the arcs of G incident to environment components. Let Gf be the
partial graph of G induced by the arcs of G incident to decision components. If Gp is
compatible with the completion of PVE || VD (cf. Definition 22) and Gf is compatible with
the completion of FVD || VE , then
PVE | VD =

p Pc | paG (c)
cCE (G)

and FVD | VE =


cCD (G)

Fc | paG (c) .

Proof. The result is proved only for PVE | VD (the proof for FVD | VE is similar). The completion of
PVE || VD looks like PVE ,VD = PVE || VD p p0 . Gp being compatible with this completion, Theorem 1a
entails that PVE ,VD = p cC(Gp ) Pc | paGp (c) . As the decision components are roots in Gp , we can infer,
by successively eliminating the environment components, that PVD = p VE PVE ,VD = p cCD (Gp ) Pc .

On the other hand, PVD = p VE PVE || VD p p0 = p0 . This proves that p cCD (Gp ) Pc =
p0 . Therefore, PVE ,VD = PVE | VD p p0 = (p cCE (Gp ) Pc | paGp (c) ) p p0 . Summing this equation
|dom(VD )| times with p gives PVE | VD = p cCE (Gp ) Pc | paGp (c) . As CE (Gp ) = CE (G) and paGp (c) =
paG (c) for every c  CE (G), this entails that PVE | VD = p cCE (G) Pc | paG (c) .

Proposition 8 Assume that the plausibility structure used is conditionable. Let Q =
(N , Sov) be a query where Sov = (op1 , S1 )  (op2 , S2 )    (opk , Sk ). Let Vf r denote the set
of free variables of Q.
472

fiThe PFU Framework

(1) If Si  VE and PSi | l(Si ) (A) is well-defined, then there exists at least one A0  dom(Si )
satisfying PSi | l(Si ) (A.A0 ) 6= 0p .
(2) If Si  VD and FSi | l(Si ) (A) is well-defined, then there exists at least one A0  dom(Si )
satisfying FSi | l(Si ) (A.A0 ) = t.
(3) If VE 6=  and Si is the leftmost set of environment variables appearing in Sov, then,
for all A  dom(l(Si )), PSi | l(Si ) (A) is well-defined.
(4) If i, j  [1, k], i < j, Si  VE , Sj  VE , r(Si )l(Sj )  VD (Sj is the first set of environment variables appearing to the right of Si in Sov), (A, A0 )  dom(l(Si ))  dom(Si ),
PSi | l(Si ) (A) is well-defined, and PSi | l(Si ) (A.A0 ) 6= 0p , then, for all A00 extending A.A0
over l(Sj ), PSj | l(Sj ) (A00 ) is well-defined.
(5) If i, j  [1, k], i < j, Si  VD , Sj  VD , r(Si )  l(Sj )  VE (Sj is the first set of
decision variables appearing to the right of Si in Sov), (A, A0 )  dom(l(Si ))dom(Si ),
FSi | l(Si ) (A) is well-defined, and FSi | l(Si ) (A.A0 ) = t, then, for all A00 extending A.A0
over l(Sj ), FSj | l(Sj ) (A00 ) is well-defined.
(6) For all i  [1, k] such that Si  VE , PSi | l(Si ) = PSi | l(Si )VE ||VD .
(7) For all i  [1, k] such that Si  VD , FSi | l(Si ) = FSi | l(Si )VD ||VE .
Proof. We denote by p0 the element in Ep such that the completion of PVE || VD equals PVE || VD  p0 .
Note that p0 6= 0p , since it must satisfy p i[1,|dom(VD )|] p0 = 1p .
Lemma 1. Let (Ep , p , p ) be a conditionable plausibility structure. Then, (p1 p p2 = 0p ) 
((p1 = 0p )  (p2 = 0p )).
Proof. First, if p1 = 0p or p2 = 0p , then p1 p p2 = 0p . Conversely, assume that p1 p p2 = 0p . Then,
if p1 p 0p , the conditionability of the plausibility structure together with p1 p 0p = 0p entails that
p2 = 0p . Similarly, if p2 p 0p , then p1 = 0p . Therefore (p1 p p2 = 0p )  ((p1 = 0p )(p2 = 0p )).
Lemma 2. Assume that the plausibility structure is conditionable. Let S1 , S2 be disjoint subsets of
VE . Then, PS1 | S2 || VD = PS1 | S2 ,VD .
Proof. Note that PS1 ,S2 | VD = PS1 | S2 ,VD p PS2 | VD . Moreover, we can also write PS1 ,S2 | VD =
PS1 ,S2 || VD = PS1 | S2 || VD p PS2 || VD = PS1 | S2 || VD p PS2 | VD . Let A be an assignment of V .
If PS1 ,S2 | VD (A) p PS2 | VD (A), then the conditionability of the plausibility structure entails that
PS1 | S2 ,VD (A) = PS1 | S2 || VD (A). Otherwise, PS1 ,S2 | VD (A) = PS2 | VD (A), which also entails that
PS1 ,S2 || VD (A) = PS2 || VD (A). In this case, PS1 | S2 ,VD (A) = PS1 | S2 || VD (A) = 1p . Therefore,
PS1 | S2 ,VD = PS1 | S2 || VD .
(1) Assume that Si  VE and PSi | l(Si ) (A) is well-defined. Then, PSi | l(Si ) (A) is a plausibility
distribution over Si . Hence, p A0 dom(Si ) PSi | l(Si ) (A.A0 ) = 1p , which implies that there exists
at least one A0  dom(Si ) such that PSi | l(Si ) (A.A0 ) 6= 0p .
(2) Proof similar to point (2).
(3) Assume that VE 6= . Let Si be the leftmost set of environment variables appearing in Sov and
let A  dom(l(Si )). Since l(Si )  VE = , we can write Pl(Si ) (A) = p V l(Si ) PVE ,VD (A) =
p VD l(Si ) (p VE PVE ,VD (A)) = p VD l(Si ) p0 6= 0p . Therefore, PSi | l(Si ) (A) is well-defined.

473

fiPralet, Verfaillie, & Schiex

(6) Let lE (Si ) = l(Si )  VE and lD (Si ) = l(Si )  VD . For a set of variables S, we denote by dG (S)
the set of variables in V that are descendant in the DAG G of at least one variable in S.
First, PSi ,lE (Si ) || VD = p VE (Si lE (Si )) PVE || VD = p VE (Si lE (Si )) (p Pj P Pj ). By definition of a query, variables in VE dG (VD lD (Si )) do not belong to Si lE (Si ) (the environment
variables that are descendants of as-yet-unassigned decision variables are not assigned yet).
Thus, PSi ,lE (Si ) || VD = p VE (Si lE (Si )dG (VD lD (Si ))) (p Pj F act(c),c*VE dG (VD lD (Si )) Pj ).
The last equality is obtained by successively eliminating the environment components included
in dG (VD lD (Si )) (using the normalization conditions). As the scope of a plausibility function
Pj  F act(c) is included in cpaG (c), this equality entails that PSi ,lE (Si ) || VD does not depend
on the assignment of VD  lD (Si ). Morever, PlE (Si ) || VD = Si PSi ,lE (Si ) || VD does not depend
on the assignment of VD  lD (Si ) too. As PSi | lE (Si ) || VD = max{p  Ep | PSi ,lE (Si ) || VD =
p p PlE (Si ) || VD }, this also entails that PSi | lE (Si ) || VD does not depend on the assignment of
VD  lD (Si ). It can be denoted PSi | lE (Si ) || lD (Si ) .
We now show that PSi | l(Si ) = PSi | lE (Si ) || lD (Si ) . First, note that
PSi ,l(Si ) = p VD lD (Si ) PSi ,lE (Si ),VD = p VD lD (Si ) (PSi | lE (Si ),VD p PlE (Si ),VD )
= p VD lD (Si ) (PSi | lE (Si ) || VD p PlE (Si ),VD ) (using Lemma 2)
= PSi | lE (Si ) || VD p (p VD lD (Si ) PlE (Si ),VD )
(since PSi | lE (Si ) || VD does not depend on the assignment of VD  lD (Si ))
= PSi | lE (Si ) || VD p Pl(Si ) .
Let A be an assignment of V .
 If PSi ,l(Si ) (A) p Pl(Si ) (A), then the conditionability of the plausibility structure directly
entails that PSi | l(Si ) (A) = PSi | lE (Si ) || VD (A).
 Otherwise, PSi ,l(Si ) (A) = Pl(Si ) (A). In this case, PSi | l(Si ) (A) = 1p . Next, as V 
l(Si ) = (VD  lD (Si ))  (VE  lE (Si )), observe that Pl(Si ) = p V l(Si ) (PVE || VD p p0 ) =
p VD lD (Si ) (PlE (Si ) || VD p p0 ). Similarly, we have PSi ,l(Si ) = p V (Si l(Si )) (PVE || VD p
p0 ) = p VD lD (Si ) (PSi ,lE (Si ) || VD p p0 ). As PSi ,l(Si ) (A) = Pl(Si ) (A), we can infer that
p VD lD (Si ) (PlE (Si ) || VD (A) p p0 ) = p VD lD (Si ) (PSi ,lE (Si ) || VD (A) p p0 ). As neither
PlE (Si ) || VD nor PSi ,lE (Si ) || VD depends on the assignment of VD lD (Si ), this entails that
PlE (Si ) || VD (A) p (p VD lD (Si ) p0 ) = PSi ,lE (Si ) || VD (A) p (p VD lD (Si ) p0 ). Summing
this equation |dom(lD (Si ))| times gives PSi ,lE (Si ) || VD (A) = PlE (Si ) || VD (A), and thus
PSi | lE (Si ) || VD (A) = 1p = PSi | l(Si ) (A).
(7) Proof similar to point (6).
(4) Let i, j  [1, k] such that i < j, Si  VE , Sj  VE , and r(Si )  l(Sj )  VD (Sj is the first set of
environment variables appearing to the right of Si in Sov). Let (A, A0 )  dom(l(Si ))dom(Si )
such that PSi | l(Si ) (A) is well-defined (i.e. Pl(Si ) (A) 6= 0p ) and PSi | l(Si ) (A.A0 ) 6= 0p . Let A00
be an extension of A.A0 over l(Sj ). We must show that PSj | l(Sj ) (A00 ) is well-defined, i.e.
that Pl(Sj ) (A00 ) 6= 0p . As PSi | l(Si ) (A.A0 ) 6= 0p and Pl(Si ) (A) 6= 0p , Lemma 1 implies that
PSi ,l(Si ) (A.A0 ) 6= 0p . Similarly to the proof of point (6), it is possible to show that Pl(Sj ) does
not depend on the assignment of l(Sj )  (Si  l(Si )). Therefore, for every A00 extending A.A0
over l(Sj ), p l(Sj )(Si l(Si )) Pl(Sj ) (A00 ) 6= 0p , which implies that Pl(Sj ) (A00 ) 6= 0p .
(5) Proof similar to point (4), except that plausibilities are replaced by feasibilities and decision
variables are replaced by environment variables.

474

fiThe PFU Framework

Theorem 2 If the plausibility structure is conditionable, then, for all queries Q on a PFU
network, Sem-Ans(Q) = Op-Ans(Q) and the optimal policies for the decisions are the same
with Sem-Ans(Q) and Op-Ans(Q).
Proof. Let Af r be an assignment of the set of free variables Vf r such that FVf r (Af r ) = f . The
semantic definition gives (Sem-Ans(Q))(Af r ) = . Given that FVf r (Af r ) = V Vf r FVE ,VD (Af r ) =
V Vf r FVD || VE (Af r ) = V Vf r (Fi F Fi (Af r )) (since the completion of FVD || VE gives FVD || VE =
FVD ,VE ), we can infer that for every complete assignment A00 extending Af r , Fi F Fi (A00 ) = f and
(Fi F Fi (A00 )) ? (p Pi P Pi (A00 )) pu (uUi U Ui (A00 )) = . As min(, ) = max(, ) =  u  =
, this entails that (Op-Ans(Q))(Af r ) =  too.
We now analyze the case FVf r (Af r ) = t. We use A00 to denote a complete assignment which
must be considered with the semantic definition. Using the properties:
 p pu min(u1 , u2 ) = min(p pu u1 , p pu u2 ) (right monotonicity of pu ),
 p pu max(u1 , u2 ) = max(p pu u1 , p pu u2 ) (right monotonicity of pu ),
 p pu (u1 u u2 ) = (p pu u1 ) u (p pu u2 ) (distributivity of pu over u ),
 p1 pu (p2 pu u) = (p1 p p2 ) pu u,
we can move all the PSi | l(Si ) (A.A0 ) to get, starting from the semantic definition,
(p i[1,k],Si VE PSi | l(Si ) )(A00 ) pu UV (A00 )
on the right of the elimination operators.
We now prove that this quantity equals PVE | VD (A00 ) pu UV (A00 ). Let S be the rightmost set
of quantified environment variables. The chain rule enables us to write PVE | VD = PS | lE (S),VD p
PlE (S) | VD , where lE (S) = l(S)  VE . Moreover, using Lemma 2 and Proposition 8(6), we can
write PS | lE (S),VD = PS | lE (S) || VD = PS | l(S) . Therefore, PVE | VD = PS | l(S) p PlE (S) | VD . Recursively applying this mechanism leads to: PVE | VD = p i[1,k],Si VE PSi | l(Si ) . Therefore, we obtain
PVE | VD (A00 ) pu UV (A00 ) on the right of the elimination operators.
The semantic definition of the query meaning can be simplified a bit, thanks to Lemma 1. This
lemma implies that conditions like PS | l(S) (A.A0 ) 6= 0p , which are used only when Pl(S) (A) 6= 0p , are
equivalent to PS,l(S) (A.A0 ) 6= 0p , since PS,l(S) (A.A0 ) = PS | l(S) (A.A0 ) p Pl(S) (A). As a result, the
operators uA0 dom(S),PS | l(S) (A.A0 )6=0p can be replaced by uA0 dom(S),PS,l(S) (A.A0 )6=0p . Similarly, in
the eliminations minA0 dom(S),FS | l(S) (A.A0 )=t , the conditions FS | l(S) (A.A0 ) = t can be replaced by
FS,l(S) (A.A0 ) = t. The same holds for the eliminations with maxadom(xi ),FS | l(S) (A.A0 )=t .
We now start from the operational definition and show that it can be reformulated as above. The
operational definition applies a sequence of variable eliminations on the global function (Fi F Fi ) ?
(p Pi P Pi ) pu (Ui U Ui ), which also equals FVD | VE ? PVE | VD pu UV . Let S be the leftmost
set of quantified decision variables. Let A be an assignment of l(S). Assume that S is quantified by min. Let A0  dom(S) such that FS,l(S) (A.A0 ) = f . It can be inferred that for
all complete assignment A00 extending A.A0 , FVE ,VD (A00 ) = f , and consequently FVD | VE (A00 ) =
f . This implies that FVD | VE (A00 ) ? PVE | VD (A00 ) pu UV (A00 ) = . Given that min(, ) =
max(, ) =  u  = , we obtain Qo(N , Sov, A.A0 ) = . As min(d, ) = d, this entails
that minA0 dom(S) Qo(N , Sov, A.A0 ) = minA0 dom(S){A0 } Qo(N , Sov, A.A0 ). Thus, minA0 dom(S)
can be replaced by minA0 dom(S),FS,l(S) (A.A0 )=t (as FVf r (A) = t, there exists at least one assignment
A0  dom(S) such that FS,l(S) (A.A0 ) = t). The same result holds if S is quantified by max. Applying this mechanism to each set of quantified decision variables from the left to the right of Sov,
we obtain that minA0 dom(S) and maxA0 dom(S) can be replaced by minA0 dom(S),FS,l(S) (A.A0 )=t and
maxA0 dom(S),FS,l(S) (A.A0 )=t respectively. Moreover, it can be shown that for every complete assignment A00 considered in the corresponding transformed operational definition, FVD | VE (A00 ) = t. It is
thus possible to replace FVD | VE (A00 ) ? PVE | VD (A00 ) pu UV (A00 ) by PVE | VD (A00 ) pu UV (A00 ).

475

fiPralet, Verfaillie, & Schiex

We now transform each uA0 dom(S) Qo(N , Sov, A.A0 ) so that it looks like the expression in the
semantic definition. Let S be the leftmost set of quantified environment variables. Let A be an assignment of l(S). Let A0  dom(S) be such that PS,l(S) (A.A0 ) = 0p . Then, for all complete assignments
A00 extending A.A0 , PVE | VD (A00 ) = 0p , and thus PVE | VD (A00 ) pu UV (A00 ) = 0u . As min(0u , 0u ) =
max(0u , 0u ) = 0u u 0u = 0u , we obtain Qo(N , Sov, A.A0 ) = 0u . As d u 0u = d, computing uA0 dom(S) Qo(N , Sov, A.A0 ) is equivalent to computing uA0 dom(S){A0 } Qo(N , Sov, A.A0 ).
Thus, uA0 dom(S) can be replaced by uA0 dom(S),PS,l(S) (A.A0 )6=0p (as Pl(S) (A) 6= 0p , there exists
at least one assignment A0  dom(S) satisfying PS,l(S) (A.A0 ) 6= 0p ). Applying this mechanism,
considering each set of quantified environment variables from the left to the right of Sov, we get
uA0 dom(S),PS,l(S) (A,A0 )6=0p instead of uA0 dom(S) .
Consequently, we have found a function  such that Sem-Ans(Q) =  and Op-Ans(Q) = .
Moreover, the optimal policies for the decisions for Sem-Ans(Q) are optimal policies for decisions
for . Indeed, the transformation rules used preserve the set of optimal policies. The same holds
for Op-Ans(Q) and . It entails that Sem-Ans(Q) = Op-Ans(Q), and that the optimal policies for
Sem-Ans(Q) are the same as those for Op-Ans(Q).

Theorem 3 Queries and bounded queries can be used to express and solve the following
nonexhaustive list of problems:
1. SAT framework: SAT, MAJSAT, E-MAJSAT, quantified boolean formula, stochastic
SAT (SSAT) and extended-SSAT (Littman et al., 2001).
2. CSP (or CN) framework:
 Check consistency for a CSP (Mackworth, 1977); find a solution to a CSP; count
the number of solutions of a CSP.
 Find a solution of a valued CSP (Bistarelli et al., 1999).
 Solve a quantified CSP (Bordeaux & Monfroy, 2002).
 Find a conditional decision or an unconditional decision for a mixed CSP or a
probabilistic mixed CSP (Fargier et al., 1996).
 Find an optimal policy for a stochastic CSP or a policy with a value greater than
a threshold; solve a stochastic COP (Constraint Optimization Problem) (Walsh,
2002).
3. Integer Linear Programming (Schrijver, 1998) with finite domain variables.
4. Search for a solution plan with a length  k in a classical planning problem (STRIPS
planning, Fikes & Nilsson, 1971; Ghallab et al., 2004).
5. Answer classical queries on Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993), and chain graphs (Frydenberg, 1990), with plausibilities
expressed as probabilities, possibilities, or -rankings:
 Compute plausibility distributions.
 MAP (Maximum A Posteriori hypothesis) and MPE (Most Probable Explanation).
476

fiThe PFU Framework

 Compute the plausibility of an evidence.
 CPE task for hybrid networks (Dechter & Larkin, 2001) (CPE means CNF Probability Evaluation, a CNF being a formula in Conjunctive Normal Form).
6. Solve an influence diagram (Howard & Matheson, 1984).
7. With a finite horizon, solve a probabilistic MDP, a possibilistic MDP, a MDP based on
-rankings, completely or partially observable (POMDP), factored or not (Puterman,
1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000).
Proof.
Lemma 3. Let (Ep , Eu , u , pu ) be an expected utility structure such that Eu is totally ordered by
u . Let S1 ,S2 be a local function on Eu whose scope is S1  S2 . Then
max
u
S1 ,S2 ((A).A)
:dom(S2 )dom(S1 ) Adom(S2 )

= u max S1 ,S2 .
S2

S1

Moreover,  : dom(S2 )  dom(S1 ) satisfies (maxS1 S1 ,S2 )(A) = S1 ,S2 ((A).A) for all A 
dom(S2 ) iff max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) = uAdom(S2 ) S1 ,S2 ((A).A). In
other words, the two sides of the equality have the same set of optimal policies for S1 .
Proof. Let 0 : dom(S2 )  dom(S1 ) be a function such that
max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) = uAdom(S2 ) S1 ,S2 (0 (A).A).
Given that, for all A  dom(S2 ), S1 ,S2 (0 (A).A) u maxA0 dom(S1 ) S1 ,S2 (A0 .A), the monotonicity
of u entails that uAdom(S2 ) S1 ,S2 (0 (A).A) u uAdom(S2 ) maxA0 dom(S1 ) S1 ,S2 (A0 .A). Thus,
max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) u uS2 maxS1 S1 ,S2 .
On the other hand, let 0 : dom(S2 )  dom(S1 ) be a function such that A  dom(S2 ),
(maxS1 S1 ,S2 )(A) = S1 ,S2 (0 (A).A). Then,
uS2 maxS1 S1 ,S2 =
u
S1 ,S2 (0 (A).A) u
max
u
S1 ,S2 ((A).A).
:dom(S2 )dom(S1 ) Adom(S2 )

Adom(S2 )

The antisymmetry of u implies the required equality. The equality of the set of optimal policies
over S1 is directly implied by the equality.
We now give the proof of the theorem, which uses for some cases the previous lemma.
1. (CSP based problems, Mackworth, 1977 )
Let us consider a CSP over a set of variables V and with a set of constraints {C1 , . . . , Cm }.
(a) (Consistency and solution finding) Consistency can be checked by using the query Q =
(N , (max, V )), where N = (V, G, , , U ) (all variables in V are decision variables, G is
reduced to a unique decision component containing all variables, and U = {C1 , . . . , Cm }),
and where the expected utility structure is boolean optimistic expected conjunctive
utility (row 6 in Table 1). Computing Ans(Q) = maxV (C1  . . .  Cm ) is equivalent to
checking consistency, because Ans(Q) = t iff there exists an assignment of V satisfying
C1 . . .Cm , i.e. iff the CSP is consistent. In order to get a solution when Ans(Q) = t, it
suffices to record an optimal decision rule for V . Integer Linear Programming (Schrijver,
1998) with finite domain variables can be formulated as a CSP.
(b) (Counting the number of solutions) The expected utility structure considered for this
task is probabilistic expected satisfaction (row 2 in Table 1). The PFU network is
N = (V, G, P, , U ), where all variables in V are environment variables, G is a DAG with
a unique component c0 = V , P = {1/0 }, 0 being a constant factor equal to |dom(V )|

477

fiPralet, Verfaillie, & Schiex

such that F act(c0 ) = {0 }, and U = {C1 , . . . , Cm }. Implicitly, 1/0 specifies that the
complete assignments are equiprobable. It enables the normalization
condition for all
P
c  CE (G), p c p Pi F act(c) Pi = 1p  to be satisfied, since V (1/|dom(V )|) = 1. The
query to consider is then Q = (N , (+, V )). It is P
not hard to check that this satisfies the
conditions imposed on queries and Ans(Q) = V (1/0  (C1  . . .  Cm )) gives the
percentage of solutions of the CSP. 0  Ans(Q) gives the number of solutions.
2. (Solving a Valued CSP (VCSP), Bistarelli et al., 1999 )
In order to model this problem, the only difficulty lies in the definition of an expected utility
structure. In a VCSP, a triple (E, ~, ) called a valuation structure is introduced. It satisfies
properties such as (E, ~) is a commutative semigroup,  is a total order on E, and E has
a minimum element denoted >. The expected utility structure to consider is the following
one: (Ep , p , p ) = ({t, f }, , ), (Eu , u ) = (E, ~), and the expected utility structure is
(Ep , Eu , u , pu ), with u = min and pu defined by f alse pu u = > and true pu u =
u (it is not hard to check that this structure is an expected utility structure). Next, the
PFU network is N = (V, G, , , U ), where V is the set of variables of the VCSP, G is a
DAG with only one decision component containing all the variables, and U contains the soft
constraints. The query Q = (min, V ) enables us to find the minimum violation degree of the
soft constraints. A solution for the VCSP is an optimal (argmin) decision rule for V .
3. (Problems from the SAT framework, Littman et al., 2001 )
In the SAT framework, queries on a conjunctive normal form boolean formula  over a set
of variables V = {x1 , . . . , xn } are asked. Let us first prove that an extended SSAT formula can be evaluated with a PFU query. An extended SSAT formula is defined by a
triple (, , q) where  is a boolean formula in conjunctive normal form,  is a threshold
in [0, 1], and q = (q1 x1 ) . . . (qn xn ) is a sequence of quantifier/variable pairs (the quantifiers
are , , or R; the meaning of R appears below). If we take f  t, the value of  under the quantification sequence q, val(, q), is defined recursively by: (i) val(, ) = 1 if 
is t, 0 otherwise; (ii) val(,
(x) q 0 ) = maxx val(, q 0 ); (iii) val(, (x) q 0 ) = minx val(, q 0 );
P
(iv) val(, (Rx) q 0 ) =
0.5
 val(, q 0 ). Intuitively, the last case means that R quantifies
x
boolean variables taking equiprobable values. An extended SSAT formula (, , q) is t iff
val(, q)  . If S denotes the set of variables quantified by R, an equivalent definition of
val(, q) is: (i) val(, ) = 0.5|S| if  is t, 0 otherwise; (ii) val(,
(x) q 0 ) = maxx val(, q 0 );
P
0
0
0
(iii) val(, (x) q ) = minx val(, q ); (iv) val(, (Rx) q ) = x val(, q 0 ). This second definition proves that val(, q) can be computed with the PFU query defined by: (a) expected
utility structure: probabilistic expected satisfaction (row 2 in Table 1); (b) PFU network:
N = (V, G, P, , U ), with V the set of variables of the formula  (the decision variables are
the variables quantified by  or ), G a DAG without arcs, with one decision component per
decision variable and a unique environment component containing all variables quantified by
R, P = {0 }, 0 being a constant factor equal to 0.5|VE | , and U the set of clauses of ; (c)
query: Q = (N , Sov), Sov being obtained from q by replacing , , and R by max, min, and
+ respectively. Then, Ans(Q) = val(, q), which implies that the value of an extended SSAT
formula (, , q) is the value of the bounded query (N , Sov, ).
SSAT is a particular case of extended-SSAT and is therefore covered. SAT, MAJSAT, EMAJSAT, QBF are also particular cases of extended SSAT. As a result, they are instances
of PFU bounded queries. More precisely, SAT corresponds to a bounded query of the form
Q = (N , (max, V ), 1); MAJSAT (given a boolean formula over a set of variables V , is it
satisfied for at least half of the assignments of V ) corresponds to a bounded query of the
form (N , (+, V ), 0.5); E-MAJSAT (given a boolean formula over V = VE  VD , does there
exist an assignment of VD such that the formula is satisfied for at least half of the assignments
of VE ?) corresponds to a bounded query of the form (N , (max, VD ).(+, VE ), 0.5); QBF

478

fiThe PFU Framework

corresponds to a bounded query in which max over existentially quantified variables and min
over universally quantified variables alternate.
4. (Solving a Quantified CSP (QCSP), Bordeaux & Monfroy, 2002 )
A QCSP represents a formula of the form Q1 x1 . . . Qn xn (C1  . . .  Cm ), where each Qi is a
quantifier ( or ) and each Ci is a constraint. The value of a QCSP is defined recursively as
follows: the value of a QCSP without variables (i.e. containing only t, f , and connectives) is
given by the definition of the connectives. A QCSP x qcsp is t iff either qcsp((x, t)) = t or
qcsp((x, f )) = t. Assuming f  t, it gives that x qcsp is t iff maxx qcsp = t. A QCSP x qcsp
is t iff qcsp((x, t)) = t and qcsp((x, f )) = t. Equivalently, x qcsp is t iff minx qcsp = t. It
implies that the value of a QCSP is actually given by the formula op(Q1 )x1 . . . op(Qn )xn (C1 
. . .  Cm ), with op() = max and op() = min. It corresponds to the answer to the query
(N , (op(Q1 ), x1 ). . . . .(op(Qn ), xn )), where N = (V, G, , , U ) (V is the set of variables of the
QCSP, G is a DAG with only one decision component containing all variables, and U is the
set of constraints), and where the expected utility structure is boolean optimistic expected
conjunctive utility (row 6 in Table 1).
5. (Solving a mixed CSP or a probabilistic mixed CSP, Fargier et al., 1996 )
A probabilistic mixed CSP is defined by (i) a set of variables partitioned into a set W of
contingent variables and a set X of decision variables; an assignment AW of W is called
a world and an assignment AX of X is called a decision; (ii) a set C = {C1 , . . . , Cm } of
constraints involving at least one decision variable; (iii) a probability distribution PW over
the worlds; a possible world AW (i.e. such that PW (AW ) > 0) is covered by a decision AX iff
the assignment AW .AX satisfies all the constraints in C.
On one hand, if a decision must be made without knowing the world, the task is to find
an optimal non-conditional decision, i.e. to find an assignment AX of the decision variables
that
is covered by AX . This probability is equal to
P maximizes the probability that the world
P
P
(A
)
=
(P
W
W  C1  . . .  Cm ). As a result, an optimal
AW | (C1 ...Cm )(AX ,AW )=1 W
W
non-conditionalPdecision can be found by recording an optimal decision rule for X for the
formula maxX W (PW  C1  . . .  Cm ). The previous formula actually specifies how to
solve such a problem with PFUs. The algebraic structure is probabilistic expected additive
utility (row 2 in Table 1), the PFU network is N = (V, G, P, , U ), with VD = X, VE = W ,
G a DAG without arc, with one decision component X and a set of environment components
that depends on how PW is specified, P is the set of multiplicative factors that define PW ,
and finally U = {C1 , . . . , Cm }. The query is then Q = (N , (max, X).(+, W )).
On the other hand, if the world is known when the decision is made, the task is to look for an
optimal conditional decision, i.e. to look for a decision rule 0 : dom(W )  dom(X) which
maximizes the probability
P that the world is covered. In other words, the goal is to compute
max:dom(W )dom(X) AW dom(W ) | (C1 ...Cm )(AW .(AW ))=1 PW (AW ) =
P
max:dom(W )dom(X) AW dom(W ) (PW  C1  . . .  Cm ) (AW .(AW )). Due to Lemma 3, it
P
also equals W maxX (PW  C1  . . .  Cm ), and 0 can be found by recording an optimal
decision rule for X. It proves that the query Q = (N , (+, W ).(max, X)) enables us to compute
an optimal conditional decision.
With Mixed CSPs, PW is replaced by a set K of constraints defining the possible worlds. The
goal is then to look for a decision, either conditional or non-conditional, that maximizes the
number of covered worlds. This task is equivalent, ignoring a normalizing constant, to find a
decision that maximizes the percentage of covered worlds. This can be solved using the set
of plausibility functions P = K  {N0 }, with N0 a normalizing constant ensuring that the
normalization condition on plausibilities holds. N0 is the number of possible worlds, but it
does actually not need to be computed, since it is a constant factor and we are only interested
in optimal decisions.
479

fiPralet, Verfaillie, & Schiex

6. (Stochastic CSP (SCSP) and stochastic COP (SCOP), Walsh, 2002 )
Formally, a SCSP is a tuple (V, S, P, C, ), where V is a list of variables (each variable x
having a finite domain dom(x)), S is the set of stochastic variables in V , P = {Ps | s  S}
is a set of probability distributions (in a more advanced version of SCSPs, probabilities over
S may be defined by a Bayesian network; the subsumption result is still valid for this case),
C = {C1 , . . . , Cm } is a set of constraints, and  is a threshold in [0, 1].
A SCSP-policy is a tree with internal nodes labeled with variables. The root is labeled with
the first variable in V , and the parents of the leaves are labeled with the last variable in
V . Nodes labeled with a decision variable have only one child, whereas nodes labeled with
a stochastic variable s have |dom(s)| children. Leaf nodes are labeled with 1 if the complete
assignment they define satisfies all theQconstraints in C, and with 0 otherwise. With each leaf
node can be associated a probability sS Ps (AS ), where AS stands for the assignment of S
implicitly defined by the path from the root to the leaf. The satisfaction of a SCSP-policy
is the sum of the values of the leaves weighted by their probabilities. A SCSP is satisfiable
iff there exists a SCSP-policy with a satisfaction of at least . The optimal satisfaction of a
SCSP is the maximum satisfaction over all SCSP-policies.
For the subsumption proof, we first consider the problem of looking for the optimal satisfaction of a SCSP. In a SCSP-policy, each decision variable x can take one value per
assignment of the set preds (x) of stochastic variables which precede x in the list of variables V . Instead of being described as a tree, a SCSP-policy can be viewed as a set of
x
functions
 =
s (x))  dom(x)), x  V  S}, and its value is val() =
P
Q { : dom(pred
Q
(
P

C
 x (AS ))). The goal is to maximize the previAS dom(S)
sS s
Ci C i )(AS .(
xV S

ous quantity among the sets . Let y be the last decision variable in V , and let y be the set
of local functions y : dom(preds (y))
defining
Y rule for y. Then
X  dom(y)X
Ya decision
(
val() = max
Ps 
Ci )(AS .(  x (AS ))).
max
y
y
y
y
 

 

AS dom(preds (y)) Spreds (y) sS

Ci C

xV S

By
also equals:

P Lemma 3, the
Pprevious quantity
Q
Q
max
P
y
preds (y))
Spreds (y)
sS s 
Ci C Ci . A recursive application of this mechanism shows that the answer Ans(Q) to the query Q described below is equal to the optimal
satisfaction of a SCSP:
 expected utility structure: row 2 in Table 1 (probabilistic expected satisfaction);
 PFU network: N = (V 0 , G, P, , U ), with V 0 the set of variables of the SCSP; VE = S
and VD = V 0  S; G is a DAG without arcs, with one component per variable; P =
{Ps | s  S}; F act({s}) = {Ps }; U is the set of constraints of the SCSP;
 query: Q =(N , Sov), with Sov=t(V ) (V is thelist of variables of the SCSP), t(V ) being
(+, {x}).t(V 00 ) if x  S
.
recursively defined by t() =  and t(x.V 00 ) =
(max, {x}).t(V 00 ) otherwise
An optimal SCSP-policy can be recorded during the evaluation of Ans(Q). The satisfiability
of a SCSP can be answered with the bounded query (N , Sov, ). Again, a corresponding
SCSP-policy can be obtained by recording optimal decision rules.
With Stochastic Constraint Optimization Problem (SCOP), the constraints in C are additive
soft constraints. The subsumption proof is similar.
7. (Classical planning problems (STRIPS planning), Fikes & Nilsson, 1971; Ghallab et al., 2004 )
In order to search for a plan of length lesser than k, we can simply model a classical planning problem as a CSP. Such a transformation is already available in the literature (Ghallab
et al., 2004). However, we can also model a classical planning problem more directly in the
PFU framework. More precisely, the state at one step can be described by a set of boolean

480

fiThe PFU Framework

environment variables. For each step, there is a unique decision variable whose set of values corresponds to the actions available. Plausibility functions are deterministic and link
variables in step t to variables in step t + 1 (these functions simply specify the positive and
negative effects of actions). The initial state is also represented by a plausibility function
which links variables in the first step. Feasibility functions define preconditions for an action
to be feasible. They link variables in a step t to the decision variable of that step. Utility
functions are boolean functions which describe the goal states. They hold over variables in
step k. In order to search for a plan of length lesser than k, the sequence of elimination is a
max-elimination on all variables. The expected utility structure used is the boolean optimistic
expected disjunctive utility.
8. (Influence diagrams, Howard & Matheson, 1984 )
We start from the definition of influence diagrams of Section 3. With each decision variable
d, we can associate a decision rule  d : dom(paG (d))  dom(d). An influence diagram policy
(ID-policy) is a set  = { d | d  D} of decision rules (one for each decision variable). The
value val() of an ID-policy X
 is given
Yby the probabilistic
X expectation of the utility:
((
Ps | paG (s) )  (
Ui ))(AS .(   d (AS ))).
val() =
AS dom(S)

Ui U

sS

dD

To solve an influence diagram, we must compute the maximum value of the previous quantity and find an associated optimal ID-policy. Using Lemma 3 and the DAG structure, it is
possible to show, using the same ideas as in the SCSP subsumption proof, that the optimal
expected utility is given by the answer to the query Q below (associated optimal decision rules
can be recorded during the evaluation of Ans(Q)):
 expected utility structure: row 1 in Table 1 (probabilistic expected additive utility);
 PFU network: N = (V, G0 , P, , U ); V is the set of variables of the influence diagram,
G0 is the DAG obtained from the DAG of the influence diagram by removing utility
nodes and arcs into decision nodes; in G0 , there is one component per variable; P =
{Ps | paG (s) , s  VE } and F act({s}) = {Ps | paG (s) }; U is the set of utility functions
associated with utility nodes.
 PFU query: Q = (N , Sov), with Sov obtained from the DAG of the influence diagram as
follows. Initially, Sov = . In the DAG of an influence diagram, the decisions are totally
ordered. Let d be the first decision variable in the DAG G of the influence diagram (i.e.
the decision variable with no parent decision variable). Then, repeatedly update Sov
by Sov  Sov.(+, paG (d)).(max, {d}) and delete d and the variables in paG (d) from G
until no decision variable remains. Then, perform Sov  Sov.(+, S), where S is the set
of chance variables that have not been deleted from G.
9. (Finite horizon MDPs, Puterman, 1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al.,
1999, 2000 ) In order to prove that the encoding in the PFU framework given in Sections 5.6
and 6.6 actually enables us to solve a T time-steps probabilistic MDP, we start by reminding
the algorithm used to compute an optimal MDP-policy. Usually, a decision rule for dT is
chosen by computing VsT = maxdT RsT ,dT . VsT is the optimal reward which can be obtained
in state sT . AtP
a time-step i  [1, T [, a decision rule for di is chosen by computing Vsi =
maxdi (Rsi ,di + si+1 Psi+1 | si ,di  Vsi+1 ). Last, the optimal expected value of the reward,
which depends on the initial state s1 , is Vs1 .
Let us prove by induction
that forP
all i  [1,
P
Q T  1],
P
Vs1 = maxd1 s2 . . .maxdi si+1 (( k[1,i] Psk+1 | sk ,dk )  (( k[1,i] Rsk ,dk ) + Vsi+1 )).
This proposition holds for i = 1, since P

Vs1 = maxd1 (R
P s1 ,d1 + s2 Ps2 | s1 ,d1  Vs2)
P
= maxd1 s2 (Ps2 | s1 ,d1  (Rs1 ,d1 + Vs2 )) (since s2 Ps2 | s1 ,d1 = 1).
481

fiPralet, Verfaillie, & Schiex

Moreover, if the proposition
holds P
at stepQi  1 (with i > 1), then P
P
Vs1 = maxd1 s2 . . . maxdi1 si (( k[1,i1] Psk+1 | sk ,dk )  (( k[1,i1] Rsk ,dk ) + Vsi )).
Given
P that
P
P
( k[1,i1] Rsk ,dk ) + Vsi = ( k[1,i1] Rsk ,dk ) + maxdi (Rsi ,di + si+1 Psi+1 | si ,di  Vsi+1 )
P
P
= maxdi (( k[1,i] Rsk ,dk ) + si+1 Psi+1 | si ,di  Vsi+1 )
P
P
= maxdi si+1 Psi+1 | si ,di  (( k[1,i] Rsk ,dk ) + Vsi+1 )
P
(the last equality holds since si+1 Psi+1 | si ,di = 1), it can be inferred that
Q
P
( k[1,i1] Psk+1 | sk ,dk )  (( k[1,i1] Rsk ,dk ) + Vsi )
P
Q
P
= maxdi si+1 (( k[1,i] Psk+1 | sk ,dk )  (( k[1,i] Rsk ,dk ) + Vsi+1 )),
which proves that the proposition holds at step i. This proves that it also holds at step T , and
therefore Vs1 = Ans(Q). Furthermore, as each step in the proof preserves the set of optimal
decision rules, an optimal MDP-policy can be recorded during the evaluation of Ans(Q).
We now study the case of partially observable finite horizon MDPs (finite horizon POMDPs).
In a POMDP, we add for each time step t > 1 a conditional probability distribution Pot | st
of making observation ot at time step t given the state st . The value of st remains unobserved. We also assume that a probability distribution Ps1 over the initial state is available.
The subsumption proof for this case is more difficult. We consider the approach of POMDPs
which consists in finding an optimal policy tree. This approach is equivalent to compute,
for each decision variable dt , a decision rule for dt depending on the observations made so
far, i.e. a function dt : dom({o2 , . . . , ot })  dom(dt ). The set of such functions is denoted
dt . A set  = {d1 , . . . , dT } is called a POMDP-policy. The value of a POMDP-policy
is recursively defined as follows. First, the value of the reward at the last decision step,
which depends on the assignment AsT of sT and on the observations O2T made from the
beginning, is V ()sT ,o2 ,...,ot (AsT .O2T ) = RsT ,dT (AsT , dT (O2T )). At a time step i, the
obtained reward depends on the actual state Asi and on the observations O2i made so far.
Its expression is:
V ()si ,o2 ,...,o
P
Pi (Asi .O2i )
= (Rsi ,di + si+1 Psi+1 | si ,di  oi+1 Poi+1 | si+1  V ()si+1 ,o1 ,...,oi+1 )(A)
where A = Asi .di (O2i ).O2i This equation and the recursive formula used to define the
value of a policy tree for a POMDP (Kaelbling, Littman, & Cassandra,P
1998) are equivalent. Finally, the expected reward of the POMDP-policy  is V () = s1 Ps1  V ()s1 .
Solving a finite horizon POMDP consists in computing the optimal expected reward among
all POMDP-policies (i.e. in computing V  = maxd1 ,...,dT V ({d1 , . . . , dT })), as well as
associated optimal decision rules.
Using a proof by induction as in the observable MDP case, it is first possible to prove that
for a problem with T steps, P
P
V  = maxd1 ,...,dT o2 ,...,oT s1 ,...,sT V
Q
Q
P
with V = (Ps1  i[1,T [ Psi+1 | si ,di  i[1,T [ Poi+1 | si+1 )  ( i[1,T ] Rsi ,di ).
From this, a recursive use of Lemma
3 enables
P
P us to infer
Pthat
P
V  = maxd1 o2 maxd2 o3 maxd3 . . . oT maxdT s1 ,...,sT V .
It proves that the query defined below enables us to compute V  as well as an optimal policy:
 algebraic structure: probabilistic expected additive utility (row 1 in Table 1);
 PFU network: N = (V, G, P, , U ); V equals {si | i  [1, T ]}  {oi | i  [2, T ]}  {di | i 
[1, T ]}, with VD = {di | i  [1, T ]}; G is a DAG with one variable per component; a
decision component does not have any parents, an environment component {oi } has
{si } as parent, and a component {si+1 } has {si } and {di } as parents; P = {Ps1 } 
{Psi+1 | si ,di , i  [1, T  1]}  {Poi | si | i  [2, T ]}; F act({s1 }) = {Ps1 }, F act({si+1 }) =
{Psi+1 | si ,di }, and F act({oi }) = {Poi | si }; last, U = {Rsi ,di | i  [1, T ]};
482

fiThe PFU Framework

 PFU query: based on the DAG, a necessary condition for a query to be defined is that
each decision di must appear to the left of the variables in {sk | k  [i + 1, T ]}  {ok | k 
[i + 1, T ]}; the query considered is Q = (N , Sov), with
Sov = (max, d1 ).(+, o2 ).(max, d2 ). . . . .(+, oT ).(max, dT ).(+, {s1 , . . . , sT }).
The proofs for finite horizon (PO)MDPs based on possibilities or on -rankings are similar.
As for the subsumption of factored MDPs, we can first argue that every factored MDP can
be represented as a usual MDP, and therefore as a PFU query on a PFU network. Even if
this is a sufficient argument, we can define a better representation of factored MDPs in the
PFU framework: it corresponds to a representation where the variables describing states are
directly used together with the local plausibility functions and rewards, which can be modeled
by scoped functions (defined as decision trees, binary decision diagrams. . . ).
10. (Queries on Bayesian networks, Pearl, 1988, Markov random fields, Chellappa & Jain, 1993,
and chain graphs, Frydenberg, 1990 )
It suffices to consider chain graphs, since Bayesian networks and Markov random fields are
particular cases of chain graphs. The subsumption proofs are provided for the general case of
plausibility distributions defined on a totally ordered conditionable plausibility structure.
(a) (MAP, MPE, and probability of an evidence) As MPE (Most Probable Explanation) and
the computation of the probability of an evidence are particular cases of MAP (Maximum
A Posteriori hypothesis), it suffices to prove that MAP is subsumed. The probabilistic
MAP problem consists in finding, given a probability distribution PV , a Maximum A
Posteriori explanation to an assignment of a subset O of V which has been observed (also
called evidence). More formally, let D denote the set of variables on which an explanation
is sought and let e denote the observed assignment of O. The MAP problem consists in
finding an assignment A of D such that maxAdom(D) PD | O (A.e) = PD | O (A .e). As
PD | O = PD,O /PO , we can write:
maxAdom(D) PD | O (A.e) = (maxAdom(D) P
PD,O (A.e))/PO (e)
= (maxAdom(D) A0 dom(V (DO)) PV (A.e.A0 ))/PO (e)
P
Thus, computing maxD V (DO) PV (e) is sufficient (the difference lies only in a normalizing constant). This result can be generalized to all totally ordered conditionable
plausibility structures.
Indeed, as p is monotonic, maxAdom(D) PD,O (A.e) = (maxAdom(D) PD | O (A.e)) p
PO (e). If maxAdom(D) PD,O (A.e) p PO (e), then there exists a unique p  Ep such
that maxAdom(D) PD,O (A.e) = p p PO (e). This gives us p = maxAdom(D) PD | O (A.e).
Otherwise, if maxAdom(D) PD,O (A.e) = PO (e), then we can infer that there exists
A  dom(D) such that PD,O (A .e) = PO (e), and therefore PD | O (A .e) = 1p . Thus,
maxAdom(D) PD | O (A.e) = 1p too. This shows that determining maxAdom(D) PD,O (A.e)
gives maxAdom(D) PD | O (A.e).
Moreover, if A  argmax{PD,O (A0 .e), A0  dom(D)}, then max{p  Ep | PD,O (A .e) =
p p PO (e)} p max{p  Ep | PD,O (A.e) = p p PO (e)} for all A  dom(D). Therefore, an optimal assignment of D for maxD PD,O (e) is also an optimal assignment of D
for maxD PD | O (e). As a result, the MAP problem can be reduced to the computation of
maxD PD,O (e) = maxD p V (DO) PV (e) = maxD p V D (PV p O )
where O is the scoped function with scope O such that O (e0 ) = 1p if e0 = e, 0p otherwise. We define a PFU query whose answer is Ans(Q) = maxD p V D (PV p O ):
 the plausibility structure is (Ep , p , p ), the utility structure is (Eu , u ) = (Ep , p ),
and the expected utility structure is (Ep , Eu , u , pu ) = (Ep , Ep , p , p );
 PFU network: the difficulty in the definition of the PFU network lies in the fact that
normalization conditions on components must be satisfied. The idea is that only the
483

fiPralet, Verfaillie, & Schiex

components in which a variable in D  O is involved have to be modified. The PFU
network is N = (V, G, P, , U ); V the set of variables of the chain graph; VD = D
and VE = V D; G is a DAG of components obtained from the DAG G0 of the chain
graph by splitting every component c in which a variable in D  O is involved: such
a component c is transformed into |c| components containing only one variable; all
these |c| components become parents of the child components of c; for a component
{x0 } included in one of these |c| components, if x0  D, then {x0 } is a decision
component; otherwise, {x0 } is an environment component, and we create a plausibility function Pi , equal to a constant p0 (x0 ) such that p i[1,|dom(x0 )|] p0 (x0 ) = 1p ,
and such that F act({x0 }) = {p0 (x0 )}; P contains first the constants defined above,
and second the factors expressing Pc | paG0 (c) in the chain graph for the components
c satisfying c  (D  O) = ; last, U contains the factors expressing Pc | paG0 (c) in the
chain graph for the components c such that c  (D  O) 6= , and a constant factor
p1 (x0 ) satisfying p1 (x0 ) p p0 (x0 ) = 1p for each component {x0 } created in the
splitting process described above, and hard constraints representing O ; with this
PFU network, the local normalization conditions are satisfied, and the combination
of the local functions equals PV p O ;
 PFU query: the query is simply Q = (N , (max, D).(u , V  D)).
An optimal decision rule for D can be recorded during the computation of Ans(Q).
(b) (Plausibility distribution computation task ) Given a plausibility distribution PV expressed as a combination of plausibility functions as in chain graphs, the goal is to
compute the plausibility distribution PS over a set S  V . The basic formula PS =
p V S PV proves that the query defined below actually computes PS . This query shows
the usefulness of free variables.
 the plausibility structure is (Ep , p , p ), the utility structure is (Eu , u ) = (Ep , p ),
and the expected utility structure is (Ep , Eu , u , pu ) = (Ep , Ep , p , p );
 PFU network: N = (V, G, P, , U ), with VE = V  S, VD = S, and with the DAG
G and the sets P , U obtained similarly as for the MAP case;
 PFU query: Q = (N , (u , V  S))
11. (Hybrid networks, Dechter & Larkin, 2001 )
A hybrid network is a triple (G, P, F ), where G is a DAG on a set of variables V partitioned
into R and D, P is a set of probability distributions expressing Pr | paG (r) for all r  R,
and F is a set of functions fpaG (d) for all d  D (variables in D are deterministic, in the
sense that their value is completely determined by the assignment of their parents). The
most general task on hybrid networks is the task of belief assessment conditioned on a formula  in conjunctive normal form. It consists of computing the probability distribution
of a variable x given a complex evidence  (complex because it may involve several variables).
P Ignoring a normalizing constant, it requires to compute, for all assignments (x, a) of
x, Adom(V {x}) | (A.(x,a))=t PV (A.(x, a)). If C = {C1 , . . . , Cm } denotes the set of clauses
P
Q
Q
Q
of , it also equals ( V {x} ( rR Pr | paG (r) )  ( dD fpaG (d) )  ( Ci C Ci ))((x, a)).
The query corresponding to this computation uses the probabilistic expected satisfaction structure (row 2 in Table 1), and the PFU network N = (V, G, P, , U ), with VE = V , VD = {x},
P = {Pr | paG (r) | r  R  {x}}  {fpaG (d) | d  D  {x}}, and either U = C  {Px | paG (x) } if
x  R or U = C  {fpaG (x) } if x  D. The query is Q = (N , (+, V  {x})).

484

fiThe PFU Framework

References
Bacchus, F., & Grove, A. (1995). Graphical Models for Preference and Utility. In Proc. of
the 11th International Conference on Uncertainty in Artificial Intelligence (UAI-95),
pp. 310, Montreal, Canada.
Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993).
Algebraic Decision Diagrams and Their Applications. In IEEE /ACM International
Conference on CAD, pp. 188191, Santa Clara, California, USA. IEEE Computer
Society Press.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
Bistarelli, S., Montanari, U., Rossi, F., Schiex, T., Verfaillie, G., & Fargier, H. (1999).
Semiring-Based CSPs and Valued CSPs: Frameworks, Properties and Comparison.
Constraints, 4 (3), 199240.
Bodlaender, H. (1997). Treewidth: Algorithmic techniques and results. In Proc. of the
22nd International Symposium on Mathematical Foundations of Computer Science
(MFCS-97).
Bordeaux, L., & Monfroy, E. (2002). Beyond NP: Arc-consistency for Quantified Constraints. In Proc. of the 8th International Conference on Principles and Practice of
Constraint Programming (CP-02), Ithaca, New York, USA.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: A Tool for
Representing and Reasoning with Conditional Ceteris Paribus Preference Statements.
Journal of Artificial Intelligence Research, 21, 135191.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-Theoretic Planning: Structural Assumptions and Computational Leverage. Journal of Artificial Intelligence Research,
11, 194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic Dynamic Programming
with Factored Representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-Specific Independence in Bayesian Networks. In Proc. of the 12th International Conference on
Uncertainty in Artificial Intelligence (UAI-96), pp. 115123, Portland, Oregon, USA.
Chellappa, R., & Jain, A. (1993). Markov Random Fields: Theory and Applications. Academic Press.
Chu, F., & Halpern, J. (2003a). Great Expectations. Part I: On the Customizability of
Generalized Expected Utility. In Proc. of the 18th International Joint Conference on
Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Chu, F., & Halpern, J. (2003b). Great Expectations. Part II: Generalized Expected Utility
as a Universal Decision Rule. In Proc. of the 18th International Joint Conference on
Artificial Intelligence (IJCAI-03), pp. 291296, Acapulco, Mexico.
Cooper, M., & Schiex, T. (2004). Arc Consistency for Soft Constraints. Artificial Intelligence, 154 (1-2), 199227.
Darwiche, A. (2001). Recursive Conditioning. Artificial Intelligence, 126 (1-2), 541.
485

fiPralet, Verfaillie, & Schiex

Darwiche, A., & Ginsberg, M. (1992). A Symbolic Generalization of Probability Theory.
In Proc. of the 10th National Conference on Artificial Intelligence (AAAI-92), pp.
622627, San Jose, CA, USA.
Dechter, R. (1999). Bucket Elimination: a Unifying Framework for Reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Dechter, R., & Fattah, Y. E. (2001). Topological Parameters for Time-Space Tradeoff.
Artificial Intelligence, 125 (1-2), 93118.
Dechter, R., & Larkin, D. (2001). Hybrid Processing of Beliefs and Constraints. In Proc. of
the 17th International Conference on Uncertainty in Artificial Intelligence (UAI-01),
pp. 112119, Seattle, WA, USA.
Dechter, R., & Mateescu, R. (2004). Mixtures of Deterministic-Probabilistic Networks
and their AND/OR Search Space. In Proc. of the 20th International Conference on
Uncertainty in Artificial Intelligence (UAI-04), Banff, Canada.
Demirer, R., & Shenoy, P. (2001). Sequential Valuation Networks: A New Graphical Technique for Asymmetric Decision Problems. In Proc. of the 6th European Conference on
Symbolic and Quantitavive Approaches to Reasoning with Uncertainty (ECSQARU01), pp. 252265, London, UK.
Dubois, D., & Prade, H. (1995). Possibility Theory as a Basis for Qualitative Decision
Theory. In Proc. of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95), pp. 19251930, Montreal, Canada.
Fargier, H., Lang, J., & Schiex, T. (1996). Mixed Constraint Satisfaction: a Framework
for Decision Problems under Incomplete Knowledge. In Proc. of the 13th National
Conference on Artificial Intelligence (AAAI-96), pp. 175180, Portland, OR, USA.
Fargier, H., & Perny, P. (1999). Qualitative Models for Decision Under Uncertainty without
the Commensurability Assumption. In Proc. of the 15th International Conference on
Uncertainty in Artificial Intelligence (UAI-99), pp. 188195, Stockholm, Sweden.
Fikes, R., & Nilsson, N. (1971). STRIPS: a New Approach to the Application of Theorem
Proving. Artificial Intelligence, 2 (3-4), 189208.
Fishburn, P. (1982). The Foundations of Expected Utility. D. Reidel Publishing Company,
Dordrecht.
Friedman, N., & Halpern, J. (1995). Plausibility Measures: A Users Guide. In Proc. of the
11th International Conference on Uncertainty in Artificial Intelligence (UAI-95), pp.
175184, Montreal, Canada.
Frydenberg, M. (1990). The Chain Graph Markov Property. Scandinavian Journal of
Statistics, 17, 333353.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory and Practice.
Morgan Kaufmann.
Giang, P., & Shenoy, P. (2000). A Qualitative Linear Utility Theory for Spohns Theory
of Epistemic Beliefs. In Proc. of the 16th International Conference on Uncertainty in
Artificial Intelligence (UAI-00), pp. 220229, Stanford, California, USA.

486

fiThe PFU Framework

Giang, P., & Shenoy, P. (2005). Two Axiomatic Approaches to Decision Making Using
Possibility Theory. European Journal of Operational Research, 162 (2), 450467.
Goldman, R., & Boddy, M. (1996). Expressive Planning and Explicit Knowledge. In Proc. of
the 3rd International Conference on Artificial Intelligence Planning Systems (AIPS96), pp. 110117, Edinburgh, Scotland.
Halpern, J. (2001). Conditional Plausibility Measures and Bayesian Networks. Journal of
Artificial Intelligence Research, 14, 359389.
Halpern, J. (2003). Reasoning about Uncertainty. The MIT Press.
Howard, R., & Matheson, J. (1984). Influence Diagrams. In Readings on the Principles and
Applications of Decision Analysis, pp. 721762. Strategic Decisions Group, Menlo
Park, CA, USA.
Jegou, P., & Terrioux, C. (2003). Hybrid Backtracking bounded by Tree-decomposition of
Constraint Networks. Artificial Intelligence, 146 (1), 4375.
Jensen, F., Nielsen, T., & Shenoy, P. (2004). Sequential Influence Diagrams: A Unified
Asymmetry Framework. In Proceedings of the Second European Workshop on Probabilistic Graphical Models (PGM-04), pp. 121128, Leiden, Netherlands.
Jensen, F., & Vomlelova, M. (2002). Unconstrained Influence Diagrams. In Proc. of the
18th International Conference on Uncertainty in Artificial Intelligence (UAI-02), pp.
234241, Seattle, WA, USA.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and Acting in Partially
Observable Stochastic Domains. Artificial Intelligence, 101 (1-2), 99134.
Kolhas, J. (2003). Information Algebras: Generic Structures for Inference. Springer.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An Algorithm for Probabilistic Planning.
Artificial Intelligence, 76 (1-2), 239286.
Larrosa, J., & Schiex., T. (2003). In the quest of the best form of local consistency for
weighted csp. In Proc. of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Lauritzen, S., & Nilsson, D. (2001). Representing and Solving Decision Problems with
Limited Information. Management Science, 47 (9), 12351251.
Littman, M., Majercik, S., & Pitassi, T. (2001). Stochastic Boolean Satisfiability. Journal
of Automated Reasoning, 27 (3), 251296.
Mackworth, A. (1977). Consistency in Networks of Relations. Artificial Intelligence, 8 (1),
99118.
Monahan, G. (1982). A Survey of Partially Observable Markov Decision Processes: Theory,
Models, and Algorithms. Management Science, 28 (1), 116.
Ndilikilikesha, P. (1994). Potential Influence Diagrams. International Journal of Approximated Reasoning, 10, 251285.
Nielsen, T., & Jensen, F. (2003). Representing and solving asymmetric decision problems.
International Journal of Information Technology and Decision Making, 2, 217263.

487

fiPralet, Verfaillie, & Schiex

Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
Perny, P., Spanjaard, O., & Weng, P. (2005). Algebraic Markov Decision Processes. In
Proc. of the 19th International Joint Conference on Artificial Intelligence (IJCAI05), Edinburgh, Scotland.
Pralet, C. (2006). A Generic Algebraic Framework for Representing and Solving Sequential Decision Making Problems with Uncertainties, Feasibilities, and Utilities. Ph.D.
thesis, Ecole Nationale Superieure de lAeronautique et de lEspace, Toulouse, France.
Pralet, C., Schiex, T., & Verfaillie, G. (2006a). Decomposition of Multi-Operator Queries
on Semiring-based Graphical Models. In Proc. of the 12th International Conference
on Principles and Practice of Constraint Programming (CP-06), pp. 437452, Nantes,
France.
Pralet, C., Schiex, T., & Verfaillie, G. (2006b). From Influence Diagrams to Multioperator
Cluster DAGs. In Proc. of the 22nd International Conference on Uncertainty in
Artificial Intelligence (UAI-06), Cambridge, MA, USA.
Pralet, C., Verfaillie, G., & Schiex, T. (2006c). Decision with Uncertainties, Feasibilities,
and Utilities: Towards a Unified Algebraic Framework. In Proc. of the 17th European
Conference on Artificial Intelligence (ECAI-06), pp. 427431, Riva del Garda, Italy.
Puterman, M. (1994). Markov Decision Processes, Discrete Stochastic Dynamic Programming. John Wiley & Sons.
Sabbadin, R. (1999). A Possibilistic Model for Qualitative Sequential Decision Problems
under Uncertainty in Partially Observable Environments. In Proc. of the 15th International Conference on Uncertainty in Artificial Intelligence (UAI-99), pp. 567574,
Stockholm, Sweden.
Sang, T., Beame, P., & Kautz, H. (2005). Solving Bayesian Networks by Weighted Model
Counting. In Proc. of the 20th National Conference on Artificial Intelligence (AAAI05), pp. 475482, Pittsburgh, PA, USA.
Schmeidler, D. (1989). Subjective Probability and Expected Utility without Additivity.
Econometrica, 57 (3), 571587.
Schrijver, A. (1998). Theory of Linear and Integer Programming. John Wiley and Sons.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton University Press.
Shenoy, P. (1991). Valuation-based Systems for Discrete Optimization. Uncertainty in
Artificial Intelligence, 6, 385400.
Shenoy, P. (1992). Valuation-based Systems for Bayesian Decision Analysis. Operations
Research, 40 (3), 463484.
Shenoy, P. (1994). Conditional Independence in Valuation-Based Systems. International
Journal of Approximated Reasoning, 10 (3), 203234.
Shenoy, P. (2000). Valuation Network Representation and Solution of Asymmetric Decision
Problems. European Journal of Operational Research, 121, 579608.

488

fiThe PFU Framework

Smith, J., Holtzman, S., & Matheson, J. (1993). Structuring Conditional Relationships in
Influence Diagrams. Operations Research, 41, 280297.
Spohn, W. (1990). A General Non-Probabilistic Theory of Inductive Reasoning. In Proc. of
the 6th International Conference on Uncertainty in Artificial Intelligence (UAI-90),
pp. 149158, Cambridge, MA, USA.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behaviour.
Princeton University Press.
Walsh, T. (2002). Stochastic Constraint Programming. In Proc. of the 15th European
Conference on Artificial Intelligence (ECAI-02), pp. 111115, Lyon, France.
Weydert, E. (1994). General Belief Measures. In Proc. of the 10th International Conference
on Uncertainty in Artificial Intelligence (UAI-94), pp. 575582.
Wilson, N. (1995). An Order of Magnitude Calculus. In Proc. of the 11th International
Conference on Uncertainty in Artificial Intelligence (UAI-95), pp. 548555, Montreal,
Canada.

489

fiJournal of Artificial Intelligence Research 29 (2007) 1-18

Submitted 07/06; published 05/07

Phase Transition for Random Quantified XOR-Formulas
Nadia Creignou

creignou@lif.univ-mrs.fr

LIF, UMR CNRS 6166
Universite de la Mediterranee
163, avenue de Luminy
13 288 Marseille, France

Herve Daude

daude@gyptis.univ-mrs.fr

LATP, UMR CNRS 6632
Universite de Provence
39, rue Joliot-Curie
13 453 Marseille, France

Uwe Egly

uwe@kr.tuwien.ac.at

Institut fur Informationssysteme 184/3
Technische Universitat Wien
Favoritenstrae 911
A1040 Wien, Austria

Abstract
The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT
in which the connective exclusive-or is used instead of the usual or. We study the phase
transition associated with random QXOR-SAT instances. We give a description of this
phase transition in the case of one alternation of quantifiers, thus performing an advanced
practical and theoretical study on the phase transition of a quantified problem.

1. Introduction
The last decade has seen a growth of interest in the phase transition for Boolean satisfiability
(SAT). The clausal version of this problem shows a sharp transition in the sense that,
as the density of clauses is increased, formulas abruptly change from being satisfiable to
being unsatisfiable at a critical threshold point. Numerous experimental studies have been
performed in the investigation of phase transition for different variants of SAT problems,
thus giving strong evidence that the location of the transition coincides with such instances
that are hard to solve. In the meantime, theoretical studies have been conducted in order to
better understand such transitions. Determining the nature of the phase transition (sharp
or coarse)1 , locating it, determining a precise scaling window and better understanding
the structure of the space of solutions turn out to be very challenging tasks, which have
aroused a lot of interest in different communities, namely mathematics, computer science
and statistical physics (see e.g., Dubois, Monasson, Selman, & Zecchina, 2001). From a
computer science point of view, the success of the SAT problem is due to two features. The
problem SAT provides a framework in which problems within the complexity class NP can
1. Definitions of sharp and coarse phase transitions can be found on p. 21 in (Janson, Luczak, & Rucinski,
2000).

c
2007
AI Access Foundation. All rights reserved.

fiCreignou, Daude, & Egly

be adequately expressed, and moreover practically efficient and highly optimized solvers are
available.
In order to obtain even stronger systems, many researchers have turned to a powerful
generalization of Boolean satisfiability, QSAT, where both universal and existential quantifiers over Boolean variables are permitted. The QSAT problem permits the adequate
representation and modeling of problems having higher complexitywithin the complexity
class PSPACEand coming from various fields of computer science such as knowledge representation, verification and logic. Recently, random instances of these quantified problems
have started to attract some attention (see Gent & Walsh, 1999; Chen & Interian, 2005).
Models for generating random instances have been developed, and experimental studies
have shown that in these models the QSAT property undergoes a phase transition that is
qualitatively similar to the one that appears for the ordinary SAT property. As stated by
Chen and Interian (2005), the hope is that research on developing competitive solvers for
quantified Boolean formulas could benefit from a better understanding of the typical behavior of random instances. Our study follows the pioneering work from Chen and Interian
(2005) who have made precise a promising model for generating random instances of the
QSAT problem. We use their model and apply it to the satisfiability problem QXOR-SAT
that we present below.
The difficulty of identifying transition factors and of performing theoretical explorations
of the SAT transition has incited many researchers to turn to a variant of the SAT problem:
the e-XOR-SAT problem. This satisfiability problem deals with Boolean formulas in conjunctive normal form with e variables per clause, in which the usual or is replaced by the
exclusive or (we call clauses with exclusive or as the only connective XOR-clauses). This
problem has contributed to develop or validate techniques, thus revealing typical behaviors
of both random instances and their space of solutions for SAT-type problems (see, e.g.,
Cocco, Dubois, Mandler, & Monasson, 2003; Creignou & Daude, 2003; Dubois, Boufkhad,
& Mandler, 2000; Dubois & Mandler, 2002; Franz, Leone, Ricci-Tersenghi, & Zecchina,
2001). Therefore, in order to understand how phase transitions evolve for satisfiability
when introducing quantified variables, it is quite natural to consider this problem.
Although the phase transition of random k-SAT is not yet well understood, generalization to the quantified version has started for some years. The hope is that a generalization
of the problem can help in understanding the original one. Another way of gaining insight
into a hard problem is to look at some tractable variants. For these reasons in this paper
we embark on a theoretical study of the phase transition for the QXOR-SAT problem, which
is the quantified version of the XOR-SAT problem. Let us emphasize that this quantified
problem is, as the usual XOR-SAT problem, in P, and hence will permit us to provide experiments at a large scale, thus giving useful intuition on the asymptotical behavior of random
instances. In order to efficiently solve an instance of the XOR-SAT problem, the clause set
is rewritten to a set of equations with coefficients from the finite field GF (2) and Gaussian
elimination is performed on the resulting set of equations. Gaussian elimination followed by
an examination of the quantifier structure provides an algorithm for the quantified version
of the XOR-SAT problem (for details, see Creignou, Khanna, & Sudan, 2001, chap. 6.4).
Following the previous studies conducted by Gent and Walsh (1999) as well as Chen
and Interian (2005), we focus on formulas in conjunctive normal form having two quantifier
2

fiPhase Transition for Random Quantified XOR-Formulas

blocks, namely on formulas of the type XY (X, Y ), where X and Y denote distinct sets of
variables, and (X, Y ) is a conjunction of XOR-clauses. Moreover, any variable occurring in
(X, Y ) occurs in X or Y , i.e., the formula XY (X, Y ) is closed. The model has several
parameters. First we consider (a,e)-QXOR-formulas, which are such that each clause in 
has exactly (a + e) variables, a from X and e from Y . The (a,e)-QXOR-SAT property is the
property for such a formula to be true. The second parameter is a pair (m, n) specifying the
number of variables in each quantifier block, i.e., in X and Y . The third parameter is L,
the number of clauses. To sum up, the generated formulas are of the form XY (X, Y ),
where X has m variables, Y has n variables, each clause in  has a variables from X and e
from Y and there is a total number of L clauses in . We are interested in the probability
that a formula drawn at random uniformly out of this set of formulas is true as n tends to
infinity (Section 2). We prove that the nature of the phase transition (coarse or sharp) for
(a,e)-QXOR-SAT is governed by the number of existential variables occurring in each clause.
For e = 2 and any a  1, we prove in Section 3 that the (a,2)-QXOR-SAT has a coarse phase
transition. Moreover we give an expression of the distribution function of the threshold for
(a,2)-QXOR-SAT and we show how it is influenced by the different parameters of the model.
For e  3, we prove in Section 4 that (a,e)-QXOR-SAT has a sharp phase transitionthus
getting the first proof of a sharp threshold for a natural quantified satisfiability problem.

2. QXOR, XOR and the Maximal rank Property
In this section, we relate QXOR and XOR to the Maximal rank property. We start with some
definitions and notations.
2.1 Notation
An e-XOR-clause, C, is a linear equation over the finite field GF (2) using exactly e distinct
variables, C = ((x1  . . .  xe ) = ) where  = 0 or 1.
An e-XOR-formula, , is a conjunction of not necessarily distinct e-XOR-clauses. A
truth assignment I is a mapping that assigns 0 or 1 to each variable in its domain, it satisfies
e
X
I(xi ) mod 2 =  and it
an XOR-clause C = ((x1  . . .  xe ) = ) if and only if I(C) :=
i=1

satisfies a formula  if and only if it satisfies every clause in .
We will denote by e-XOR-SAT the property for an e-XOR-formula of being satisfiable.
An (a,e)-QXOR-formula is a closed quantified formula of the following type
XY (X, Y ),

where X and Y denote distinct set of variables, (X, Y ) is an (a + e)-XOR-formula such
that each clause contains exactly a variables from X and exactly e variables from Y . Such
a formula is true if, for every assignment to the variables X, there exists an assignment to
the variables Y , such that  is true. Observe that, for closed formulas, the notions of truth
and satisfiability coincide. For this reason, we will use the two notions synonymously in the
following. We denote by (a,e)-QXOR-SAT the property for an (a,e)-QXOR-formula of being
true.
Throughout the paper, we reserve m for the number of universal variables (resp. n for
the number of existential variables), and {x1 , . . . , xm } (resp. {y1 , . . . , yn }) denotes the set
3

fiCreignou, Daude, & Egly

of such variables. Note that there are
   
m
n
N=

2
a
e

(1)

(a,e)-XOR-clauses. We consider random formulas XY (X, Y ) obtained by choosing uniformly independently and with replacement L clauses from all the possible N (a,e)-XORclauses. Using the terminology of Chen and Interian (2005), these formulas correspond
to (a,e)-QXOR((m,n),L)-formulas. We are interested in estimating the probability that
a randomly chosen (a,e)-QXOR((m,n),L)-formula is true. We denote this probability by
Pr(m,n,L) ((a,e)-QXOR-SAT), or shortly Pr((a,e)-QXOR-SAT) when no confusion can arise.
When restricted to the non-quantified case, e-XOR-SAT, i.e., when a=0, we omit the first
component in the subscript, thus discussing Prn,L (e-XOR-SAT), or shortly Pr(e-XOR-SAT).
We will show that the behavior of the (a,e)-QXOR-SAT property is bounded from
above and below by two monotone properties, namely the e-XOR-SAT property and the
Maximal rank property. Experiments will suggest that the right parameter in order to study
these properties is c, the ratio of the number of clauses over the number of existential variables. Moreover, according to the results obtained for e-XOR-SAT by Creignou, Daude, and
Dubois (2003), we know that the transition occurs when c < 1. Therefore, in the sequel we
will always suppose without loss of generality that L  n.
2.2 Upper and Lower Bounds for the QXOR-SAT Property
Note that a random (a,e)-QXOR((m,n),L)-formula can also be considered as the quantified
linear system
XY (AX + EY = C)
(2)
with coefficient arithmetic in GF (2), where A (respectively E) is a matrix chosen uniformly
from the set of Boolean L  m (resp. L  n) matrices with exactly a (respectively e) units
in each row, and C is a Boolean column vector of dimension L chosen uniformly from the
set of all such vectors. Moreover, A, E and C are chosen independently.
Observe that the quantified linear system
XY (AX + EY = C)
is consistent if and only if
X (C  AX)  Im(E),
where Im(E) represents the image of the linear application whose matrix representation is
E, that is Im(E) = {EY / Y  {0, 1}n }. Hence the quantified linear system is consistent if
and only if C  Im(E) and Im(A)  Im(E). Therefore, we get:
Pr((a,e)-QXOR-SAT) = Pr(XY (AX + EY = C) is consistent)
= Pr(C  Im(E) and Im(A)  Im(E)).
Thus, on the one hand
Pr((a,e)-QXOR-SAT)  Pr(C  Im(E)) = Pr(e-XOR-SAT)
4

fiPhase Transition for Random Quantified XOR-Formulas

holds, and on the other hand, the inequality
Pr((a,e)-QXOR-SAT)  Pr(Im(E) = {0, 1}L )
holds. Therefore, if Prn,L (e-Max-rank) denotes the probability that a random matrix from
the set of L  n Boolean matrices with e units per row is of maximal rank, then for every
m  a, all n and all L  n, we get the following inequalities:
Prn,L (e-Max-rank)  Pr(m,n,L) ((a,e)-QXOR-SAT)  Prn,L (e-XOR-SAT).

(3)

A natural question at this stage is to estimate the probability that a random matrix is
of maximal rank. In the following we will provide some experiments and theoretical results
comparing the behavior of the three properties, Maximal rank, QXOR-SAT and XOR-SAT,
thus making precise the behavior of the (a,e)-QXOR-SAT property according to the value
of e.

3. The Case e = 2
In this section, we restrict our attention to the case where all problems have two existential
variables in each clause (and the number of all variables is allowed to vary).
3.1 Experimental Results
In order to illustrate the inequalities (3) and to compare empirically the three properties,
we discuss experiments that we have performed. In the experiments, all formulas are closed.
1
2-XOR-SAT for n=10k
(1,2)-QXOR-SAT for m=n
(2,2)-QXOR-SAT for m=n
(3,2)-QXOR-SAT for m=n
(4,2)-QXOR-SAT for m=n
2-Max-rank for n=10k

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 1: The curves for (a,2)-QXOR-SAT, m = n = 10 000 and a varying.
In all cases, the experiments have been conducted according to the same scheme. Let
us describe it in detail for Figure 1. One experiment consisted in generating at random (in
5

fiCreignou, Daude, & Egly

drawing uniformly and independently) (1,2)-QXOR-formulas over 10 000 existential variables and 10 000 universal variables with a ratio number of clauses/number of existential
variables varying from 0.1 to 1 in steps of 0.05 or 0.1. For each of the chosen values of
ratio, a sample of 1000 formulas were studied using the algorithm described in the work of
Creignou et al. (2001, chap. 6.4), thus deciding their truth (or satisfiability) as quantified
formulas. The proportion of true instances for each considered value of ratio has been plotted in Figure 1. The same has been done for the other (a,2)-QXOR-SAT properties. Hence,
the different curves are independent from each other. For the 2-XOR-SAT experiment, we
used the same selection procedure over 10 000 existential variables. Again, Gaussian elimination together with an examination of the quantifier structure were used to determine
the logical status (true or false) of every formula. Additionally, it has been computed
whether the matrix E has full rank or not. The curve 2-Max-rank shows the proportion
of systems with full rank and it corresponds to the 2-XOR-SAT curve in the same figure.
A close look at Figure 1 reveals that some points from the (a,2)-QXOR-SAT curves are
slightly below the (theoretical) lower bound given by the curve for 2-Max-rank. The reason
for this phenomenon is the independence of all the satisfiability curves from each other and
the noise induced by the finite sampling of problems. If we had chosen corresponding
problems with exactly the same existential part (and only the universal part varies), then
we would have got all satisfiability curves above the curve for 2-Max-rank.
The experimental results shown in Figure 1 suggest first that the two bounding properties, namely 2-Max-rank and 2-XOR-SAT are distinguishable, second that, when m = n, the
(a,2)-QXOR-SAT property coincides asymptotically with the 2-Max-rank property independently of a  1, the number of universal variables per clause.
1
(1,2)-QXOR-SAT for m=1, n=10k
2-XOR-SAT for n=10k
2-Max-rank for n=10k

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 2: (1,2)-QXOR-SAT when m = 1 and n = 10 000 compared to 2-XOR-SAT and
2-Max-rank.

6

fiPhase Transition for Random Quantified XOR-Formulas

At another scale for m, for instance when m is constant, the experimental results reported in Figure 2 suggest that the (a,2)-QXOR-SAT property is in between the two properties 2-Max-rank and 2-XOR-SAT.
In the following, we will validate the conjectures suggested by these experiments and
prove that the property (a,2)-QXOR-SAT coincides asymptotically with the 2-Max-rank
property as soon as m is tending to infinity with n, and that it is in between the 2-Max-rank
property and the 2-XOR property when m is a fixed constant. In particular, we will make
clear the connection between random (a,2)-QXOR-formulas and random labelled graphs.
3.2 Bad Cycles and the (a,2)-QXOR-SAT Property
We are interested in the satisfiability of quantified systems of the form
s = (XY AX + EY = C),
where E (respectively A) is a matrix chosen uniformly in the set of Boolean L  n (respectively L  m) matrices with exactly 2 (respectively a) units in each row, and C is a random
column vector of dimension L, in which 0 and 1 occur with the same probability. The satisfiability of such a system is strongly related to the existence of cycles in graphs. Indeed, we
construct a graph Ga (s) with n vertices and L weighted edges. For each existential variable
yi we have a vertex in Ga (s). For each equation yi  yj = xi1  . . .  xia  , we add the
edge {yi , yj } to Ga (s) with the label xi1  . . .  xia  . A cycle is given by a sequence of
vertices (yi1 , . . . , yis ) such that for 1  j  s  1, {yij , yij+1 } is an edge in the graph, and
so is {yis , yi1 }. The cycle is said to be elementary if all the vertices in the sequence are
distinct. The weight of a cycle is the sum modulo 2 of the labels of its edges.
Example 3.1 Let X = {x1 , x2 , x3 } and let Y = {y1 , . . . , y7 }. The formula XY (X, Y )
with (X, Y ) being a conjunction of the following equations
y1  y2
y2  y3
y3  y4
y4  y5

=
=
=
=

x1
x3
x2  1
x3  1

y1  y7
y2  y6
y3  y5
y6  y7

=
=
=
=

x2
x2  1
x3
x1  1

can be represented by the graph in Figure 3.
In the following, we call a cycle bad when it has a nonzero weight, and good otherwise.
Example 3.2 In the graph associated with the formula described in the previous example,
there is a good cycle, (y1 , y2 , y6 , y7 ), whose weight is 0, and a bad one, (y3 , y4 , y5 ), whose
weight is x2 . For the latter cycle, the corresponding equations are y3  y4 = x2  1, y3  y5 =
x3 , and y4  y5 = x3  1. Adding these three equations yields the equation 0 = x2 which
cannot be satisfied because x2  X is universally quantified.
For systems containing only existential variables, i.e., a = 0, it has been observed by
Creignou and Daude (2003) that a 2-XOR-formula is satisfiable if and only if the graph
G0 (s) has no bad cycle, that is :
Pr(2-XOR-SAT) = Pr(G0 (s) has no bad cycle).
Using similar arguments, we get the following proposition.
7

(4)

fiCreignou, Daude, & Egly

y1
x1
y2
x3 x2+ 1
y3

y6

x2 + 1
y4

x2

x1 + 1
x3

y7

x3 + 1
y5

Figure 3: The graph Ga (s) from Example 3.1 (addition is performed mod2).
Proposition 3.3 The system s is satisfiable if and only if Ga (s) does not contain any
elementary bad cycle, i.e.,
Pr((a,2)-QXOR-SAT) = Pr(Ga (s) has no bad cycle).
Proof: Suppose we have an elementary cycle with nonzero weight in Ga (s). Clearly, to
such a cycle corresponds a subsystem in s, for which there exists an assignment to X such
that no assignment to Y will satisfy it (see Example 3.2 for an illustration). Hence, s is
unsatisfiable.
Conversely, suppose there is no (elementary) cycle with nonzero weight in Ga (s). Take
an arbitrary truth assignment I for the (universal) variables in X and apply it to Ga (s).
Since I(x)  {0, 1} for all x  X, the weight at each edge can be reduced to a constant
from {0, 1} by addition modulo 2. Moreover, since each cycle in Ga (s) has zero weight, the
corresponding cycle in the reduced version, Ga (s), of Ga (s) has also zero weight. The
graph Ga (s) corresponds to a system with existential quantifiers only.
In order to obtain a satisfying truth assignment for the existential variables, it suffices to
apply the following procedure to each connected component of Ga (s). Consider an arbitrary
root vertex y and assign an arbitrary truth value v to it. We obtain a truth value for each
vertex in Ga (s) by performing a depth-first search starting from y. During the search, if
there is an edge (y  , y  ) labelled with  and y  has no truth value yet, then we set the value
for y  to the value of y   . The assignment obtained in this way satisfies all the equations
since Ga (s) does not contain any cycle with nonzero weight.
Remark: Observe that
Pr(2-Max-rank) = Pr(Ga (s) has no cycle)
holds.
8

(5)

fiPhase Transition for Random Quantified XOR-Formulas

3.3 The Distribution Functions for 2-XOR-SAT and 2-Max-rank
In this section, we will give the exact asymptotical value of the bounds obtained in (3) in
terms of the order parameter c, where c  n is the number of clauses. For this we will use
well-known results from random graph theory.
Let us recall that we consider the classical probabilistic model where each clause/edge
is chosen uniformly and independently among the
   
m
n
N=

2
a
e
possible ones. According to Proposition 1.13 in (Janson et al., 2000), if we choose L = c  n
clauses, then this model is asymptotically equivalent to the one where each clause is drawn
independently with probability p, where


cn
p = m n
1 + O(n1/2 )
a  2 2
holds. So, in the following, we will work with the random labelled graphs Ga (s) associated
with quantified systems s, with labelled edge probability:
c
.
p=
n m
a

The corresponding probability is usually denoted by p . However, for simplicity we will
keep the notation Pr.
In the light of Proposition 3.3 and of (5), it appears that we have to study
Pr(Ga (s) has no (bad) cycle).
The asymptotic behavior of the number of cycles in random graphs has been first investigated by Erdos and Renyi (1960), and made precise by Janson (1987) and Takacs (1988).
This number converges in distribution to a Poisson law of parameter , where  is the limit
of the average number of cycles as n, the number of vertices, tends to infinity.
This result can be easily extended to our model of labelled graphs. In particular,
Pr(Ga (s) has no (bad) cycle)  exp(),
where  is the limit of the the average number of (bad) cycles. A challenging task is now
to get a simple expression of lambda.
Let Y be the random variable that counts the number of cycles. Let C be the set
of all possible cycles. For any cycle c, we introduce the random variable Xc such that
Xc (Ga (s)) = 1 holds, if and only if Ga (s) contains the cycle c. The average number of
cycles is
X
X
E(Y ) = E(
Xc ) =
E(Xc ).
cC

cC

Since every cycle c of length l has expectation E(Xc ) = pl and since the number of cycles
 l
m
n(n  1) . . . (n  l + 1)

 2l , we get that
of length l is
a
2l
X n(n  1) . . . (n  l + 1) ml

 2l  pl
E(Y ) =
a
2l
l2

9

fiCreignou, Daude, & Egly

holds. Thus,
lim E(Y ) =

n+

X (2c)l
l2

2l

1
=  ln(1  2c)  c
2

also holds. From (5), we obtain for every 0 < c <

1
2

that

1
lim Pr(n,cn) (2-Max-rank) = exp( ln(1  2c) + c)
n+
2
holds, and finally
lim Pr(n,cn) (2-Max-rank) = H (c)

(6)

n+

is established, where
(
exp(c)  (1  2c)1/2
H (c) =
0

for 0  c  21 ,
otherwise.

The experimental results shown in Figure 4 illustrate this asymptotical behavior.
1


H (x) = ex 1  2x
2-Max-rank for n=10k
2-Max-rank for n=20k
2-Max-rank for n=40k

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 4: The curves for the 2-Max-rank property.
According to (4)
lim Pr(n,cn) (2-XOR-SAT) = exp(0 )

n+

holds, where 0 denotes the limit of the average number of bad cycles. In this particular
case, the weight of a clause is either 0 or 1, which means that half of the cycles are bad.
Thus,
X n(n  1) . . . (n  l + 1) ml
1
c
0 = lim

 2l1  pl =  ln(1  2c)  .
n+
a
2l
4
2
l2

10

fiPhase Transition for Random Quantified XOR-Formulas

Therefore, for every c  0, we get
1
c
lim Pr(n,cn) (2-XOR-SAT) = exp( ln(1  2c) + ),
n+
4
2
and finally
lim Pr(n,cn) (2-XOR-SAT) = H0 (c)

(7)

n+

is established, where
(
exp(c/2)  (1  2c)1/4
H0 (c) =
0

for 0  c  12 ,
otherwise.

This is illustrated by Figure 5.
1

H0 (x) = ex/2 (1  2x)0.25
2-XOR-SAT for n=10k
2-XOR-SAT for n=20k
2-XOR-SAT for n=40k

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 5: The curves for the 2-XOR-SAT property.

3.4 The Distribution Function for (a,2)-QXOR-SAT
The results obtained in the previous section, (6) and (7), together with the inequalities (3)
are sufficient to conclude that the (a,2)-QXOR-SAT property has a coarse phase transition
at the scale L = c  n and that its distribution function is in between the functions H0 and
H described above. More precisely we get the following theorem.
Theorem 3.4 For any integer a  1 and every c  0, let us consider (a,2)-QXOR((m,n),cn)formulas consisting in the conjunction of c  n XOR-clauses, where each clause contains a
variables from a set of m universal variables, and e variables from a set of n existential
variables. Then, when n tends to infinity, the (a,2)-QXOR-SAT property has a coarse phase
11

fiCreignou, Daude, & Egly

transition whose asymptotical distribution function can be expressed as a function depending
on m. More precisely, for all c  0, every a  1 and m = a
Pr(a,n,cn)((a,2)-QXOR-SAT) n+ H(c)
holds, where
(
exp(c)(1  2c)1/2 (1  4c2 )1/8
H(c) =
0

for 0  c  21 ,
otherwise.

If m is a function of n tending to infinity with n, then, for every a  1,
Pr(m,n,cn) ((a,2)-QXOR-SAT) n+ H (c)
holds, where
(
exp(c)  (1  2c)1/2
H (c) =
0

for 0  c  12 ,
otherwise.

Moreover, for every fixed m  a  1, there exists a distribution function Hm such that
Pr(m,n,cn) ((a,2)-QXOR-SAT) n+ Hm (c),
with Hm satisfying
H < Hm < H0 ,
where
H0 (c) =

(

exp(c/2)(1  2c)1/4
0

for 0  c  21 ,
otherwise.

Proof: The proof is based on Proposition 3.3 and, as discussed in the previous section, on
an estimation of the number of bad cycles in the labelled graphs associated with random
formulas.
Let m,a be the limit of the average number of bad cycles. We will give a closed
expression of m,a . Observe that each label on the edges of the graph associated with a
(a,2)-QXOR((m,n),cn)-formula is formed with a constant,
0 or 1, and a variable-label made
m
of a universal variables. There are exactly a such variable-labels, which are numbered
from 1 to m
a . One can decide whether a cycle is good or bad according to the number
of 1 (even or odd) and the number of occurrences of each variable-label. Therefore it is
quite natural to associate to every cycle its length l, the sequence (N1 , . . . , N(m) ) of the
a
numbers of occurrences of each variable-label, and the parity  = 0 or 1 of the number of
occurrences of the constant 1. The limit of the average number of cycles whose parameter
(l, (N1 , . . . , N(m) ), ) is fixed is
a

cl 2l1
2l

l
N1 ,...,N



m
a

( )

m l
a



.

Moreover, from such a parameter (l, (N1 , . . . , N(m) ), ), one can decide whether a cycle is
a
bad or not.
12

fiPhase Transition for Random Quantified XOR-Formulas

For a better readability, let us focus on the case a = 1. In this particular case, the label
of an edge is of the form xi  , where  = 0 or 1 and 1  i  m. On the one hand, all cycles
of odd length are bad (for in the weight of such a cycle at least one of the coefficients of the
xi s will be nonzero). On the other hand, there are two kinds of cycles of even length that
are bad. The ones in which the constant 1 appears an odd number of times, and the ones
in which one of the universal variables appears an odd number of times. Since, for m  1,
we have


X 
X 
l
l
ml =
+
N1 , N2 , . . . , Nm
N1 , N2 , . . . , Nm
i Ni 0(2)

i Ni 1(2)

we get
m,1

X (2c)2u+1
1 X (2c)2v
1 X (2c)2v
=
+
+

2(2u + 1) 2
(2(2v)) 2
(2(2v))
u1

v1

v1


l
i Ni 1(2) N1 ,N2 ,...,Nm
.
ml

P

(8)

Standard combinatorial computations show that, for even l, the equation

P
l
m1
X m  1 (m  2k)l
i Ni 1(2) N1 ,N2 ,...,Nm
=1
 m1
ml
2
 ml
k
k=0

holds. Therefore, we rewrite (8) and obtain
m,1

X (2c)2u+1
X (2c)2v
X (2c)2v
=
+
+

2(2u + 1)
2(2(2v))
2(2(2v))
u1

v1

1

v1

m1
X
k=0

!

m1
(m  2k)l
.
 m1
2
 ml
k

First, observe that m,1 is a function of c, thus we deduce the last part of the theorem:
lim Pr(m,n,cn) ((1,2)-QXOR-SAT) = exp(m,1 ) = Hm (c).

n+

Second, from the above expression of m,1 and using the following inequality
1

m1
X
k=0


m1
(m  2k)l
4
 m1
1 ,
k
2
 ml
m

we get that
lim (m,1 ) =

m+

X (2c)l
l2

2l

1
=  ln(1  2c)  c
2

holds, which proves the second statement of the theorem.
In addition, when m = a = 1, we get the equation
1,1 =

X (2c)2v
X (2c)2u+1
1
1
+
=  ln(1  2c) + ln(1  4c2 )  c.
2(2u + 1)
2(2(2v))
2
8
u1

v1

Thus, we have established

13

fiCreignou, Daude, & Egly

lim Pr(1,n,cn) ((1,2)-QXOR-SAT) = exp(1,1 ) = H(c),

n+

where

(
exp(c)(1  2c)1/2 (1  4c2 )1/8
H(c) =
0

for 0  c  12 ,
otherwise.

This result is illustrated by Figure 6, while Figure 7 shows the comparative behavior of
the three distribution functions H0 , H and H .
1


H(x) = ex 1  2x (1  4x2 )1/8
(1,2)-QXOR-SAT for m=1, n=20k
(1,2)-QXOR-SAT for m=1, n=40k

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 6: The curves for the (1,2)-QXOR-SAT property with m = 1.

4. The Case e  3
It can be observed from the experimental results shown in Figure 8 that, contrary to what
has been observed in the previous section, the three smooth lines connecting the consecutive points and corresponding to the transition of the three properties 3-Max-rank,
(a,3)-QXOR-SAT and 3-XOR-SAT are difficult to distinguish. Moreover, when n increases
(see Figure 9), the curves straighten and come closer one to each other, showing thus strong
empirical evidence that the transitions of the three properties coincide asymptotically, with
a sharp phase transition at the critical value c3  0.918 (which is the critical ratio for
3-XOR-SAT, see Dubois & Mandler, 2002). We will show that, for e  3, the introduction
of universal variables in XOR-formulas does not influence the sharp transition.
Theorem 4.1 For every e  3 and any integer a, the (a,e)-QXOR-SAT property has a
sharp threshold which coincides with the one of the e-XOR-SAT property.
14

fiPhase Transition for Random Quantified XOR-Formulas

1


H(x) = ex 1  2x (1  4x2 )1/8
H0 (x) = ex/2 (1 2x)0.25
H (x) = ex 1  2x

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.1

0.2

0.3

0.4
0.5
0.6
#clauses/#exvars

0.7

0.8

0.9

1

Figure 7: The distribution functions H0 , H and H .
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3

#clauses/#exvars = 0.918
3-XOR-SAT for n=1k
3-Max-rank for n=1k
(1,3)-QXOR-SAT for m=n

0.2
0.1
0
0.8

0.85

0.9
#clauses/#exvars

0.95

1

Figure 8: The curves for 3-XOR-SAT, 3-Max-rank and (a,3)-QXOR-SAT for n=1000.
Proof: Let us recall that
Prn,L (e-XOR-SAT) = Pr(Y (EY = C) is consistent)
= Pr(C  Im(E))
X
=
Pr(C  V and Im(E) = V ),
V {0,1}L

15

fiCreignou, Daude, & Egly

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.8

#clauses/#exvars = 0.918
3-XOR-SAT for n=2k
3-Max-rank for n=2k
(1,3)-QXOR-SAT for m=n

0.85

0.9
#clauses/#exvars

0.95

1

Figure 9: The curves for 3-XOR-SAT, 3-Max-rank and (a,3)-QXOR-SAT for n=2000.
since E and C are chosen independently. Therefore, if P(r) denotes the probability that a
random matrix from the set of L  n Boolean matrices with e units per row is of rank r,
then
L
X
1
2rL P(r)  P(L) + (P(L1) + . . . P(0) ).
Prn,L (e-XOR-SAT) =
2
r=0

Now observe that P(0) + . . . + P(L) = 1, thus (P(L1) + . . . P(0) ) = 1  P(L) , and hence
we get
1 + P(L)
.
Prn,L (e-XOR-SAT) 
2
Therefore, according to (3), we have
2  Prn,L (e-XOR-SAT)  1  Prn,L ((a,e)-QXOR-SAT)  Prn,L (e-XOR-SAT).
Since we know that the property e-XOR-SAT exhibits a sharp threshold when L is (n)
(Creignou & Daude, 2003), this shows that (a,e)-QXOR-SAT also does. The same holds for
the property e-Max-rank (since Prn,L (e-Max-rank) = P(L) ). In particular, for e = 3, we have
shown that (a,e)-QXOR-SAT as well as 3-Max-rank have a sharp threshold with a critical
value c3  0.918, which is the critical ratio for 3-XOR-SAT (Dubois & Mandler, 2002).

5. Conclusion
We have (experimentally and theoretically) analyzed the phase transition for the quantified
problems (a,e)-QXOR-SAT. Our analysis has been conducted at the same level of sophistication as the one made for the e-XOR-SAT problem, thus showing that the model proposed

16

fiPhase Transition for Random Quantified XOR-Formulas

by Chen and Interian (2005) is mathematically tangible and provides the good parameters
in order to perform a mathematical analysis of the phase transition for quantified problems.
On the one hand, as observed for QSAT (Gent & Walsh, 1999; Chen & Interian, 2005),
we have proved that the nature of the transition is not influenced by the introduction
of universal variables. On the other hand, in contrast with QSAT, we have proved that
the location of the phase transitionthe critical ratiois the same for the two properties
XOR-SAT and QXOR-SAT, and that the difference of behavior between these two properties
occurs at the level of the distribution function.

Acknowledgments
This work has been supported by EGIDE 10632SE and OAD Amadee 2/2006.

References
Chen, H., & Interian, Y. (2005). A model for generating random quantified boolean formulas.
In Proceedings of the 19th International Joint Conference on Artificial Intelligence
(IJCAI2005), pp. 6671.
Cocco, S., Dubois, O., Mandler, J., & Monasson, R. (2003). Rigorous decimation-based
construction of ground pure states for spin glass models on random lattices. Physical
Review Letters, 90, 472051472054.
Creignou, N., & Daude, H. (2003). Coarse and sharp thresholds for random k-XOR-CNF. Informatique theorique et applications/Theoretical Informatics and Applications, 37 (2),
127147.
Creignou, N., Daude, H., & Dubois, O. (2003). Approximating the satisfiability threshold
for random k-XOR-CNF formulas. Combinatorics, Probability and Computing, 12 (2),
113126.
Creignou, N., Khanna, S., & Sudan, M. (2001). Complexity classifications of Boolean constraint satisfaction problems. SIAM Monographs On Discrete Mathematics And Applications. SIAM, Philadelphia, PA, USA.
Dubois, O., Boufkhad, Y., & Mandler, J. (2000). Typical random 3-SAT formulae and the
satisfiability threshold. In Proceedings of the 11th ACM-SIAM Symposium on Discrete
Algorithms (SODA2000), pp. 124126.
Dubois, O., & Mandler, J. (2002). The 3-XOR-SAT threshold. In Proceedings of the 43th
Annual IEEE Symposium on Foundations of Computer Science (FOCS 2002), pp.
769778.
Dubois, O., Monasson, R., Selman, B., & Zecchina, R. (2001). Editorial. Theoretical Computer Science, 265 (12).
Erdos, P., & Renyi, A. (1960). On the evolution of random graphs. In Publ. Math. Inst.
Hungar. Acad. Sci., Vol. 7, pp. 1761.
Franz, S., Leone, M., Ricci-Tersenghi, F., & Zecchina, R. (2001). Exact solutions for diluted
spin glasses and optimization problems. Physical Review Letters, 87, 127209127212.

17

fiCreignou, Daude, & Egly

Gent, I., & Walsh, T. (1999). Beyond NP: the QSAT phase transition. In Proceedings of
the 16th National Conference on Artificial Intelligence (AAAI99), pp. 648653.
Janson, S. (1987). Poisson convergence and Poisson processes with applications to random
graphs. Stochastic Processes and Applications, 26 (1), 130.
Janson, S., Luczak, T., & Rucinski, A. (2000). Random graphs. John Wiley and sons.
Takacs, L. (1988). On the limit distribution of the number of cycles in a random graph.
Journal of Applied Probability, 25, 359376.

18

fiJournal of Artificial Intelligence Research 29 (2007) 269-307

Submitted 8/06; published 7/07

Semantic Matchmaking as Non-Monotonic Reasoning: A
Description Logic Approach
Tommaso Di Noia
Eugenio Di Sciascio

t.dinoia@poliba.it
disciascio@poliba.it

SisInfLab - Politecnico di Bari, Bari, Italy

Francesco M. Donini

donini@unitus.it

Universita della Tuscia, Viterbo, Italy

Abstract
Matchmaking arises when supply and demand meet in an electronic marketplace, or
when agents search for a web service to perform some task, or even when recruiting agencies
match curricula and job proles. In such open environments, the objective of a matchmaking process is to discover best available oers to a given request.
We address the problem of matchmaking from a knowledge representation perspective,
with a formalization based on Description Logics. We devise Concept Abduction and Concept Contraction as non-monotonic inferences in Description Logics suitable for modeling
matchmaking in a logical framework, and prove some related complexity results. We also
present reasonable algorithms for semantic matchmaking based on the devised inferences,
and prove that they obey to some commonsense properties.
Finally, we report on the implementation of the proposed matchmaking framework,
which has been used both as a mediator in e-marketplaces and for semantic web services
discovery.

1. Introduction
The promise of the Semantic Web initiative is to revolutionize the way information is coded,
stored, and searched on the Internet (Berners-Lee, Hendler, & Lassila, 2001). The basic
idea is to structure information with the aid of markup languages, based on the XML
language, such as RDF and RDFS1 , and OWL2 . These languages have been conceived
for the representation of machine-understandable, and unambiguous, description of web
content through the creation of domain ontologies, and aim at increasing openness and
interoperability in the web environment.
Widespread availability of resources and services enablesamong other advantages
the interaction with a number of potential counterparts. The bottleneck is that it is dicult
nding matches, possibly the best ones, between parties.
The need for a matchmaking process arises when supply and demand have to meet in a
marketplace, or when web services able to perform some task have to be discovered, but also
when recruiting agencies match curricula and job proles or a dating agency has to propose
partners to a customer of the agency. Requests and oers may hence be generic demands
and supplies, web services, information, tangible or intangible goods, and a matchmaking
process should nd for any request an appropriate response. In this paper we concentrate
1. http://www.w3.org/RDF/
2. http://www.w3.org/TR/owl-features/
c
2007
AI Access Foundation. All rights reserved.

fiDi Noia, Di Sciascio & Donini

on automated matchmaking, basically oriented to electronic marketplaces and service discovery, although principles and algorithms are denitely general enough to cover also other
scenarios. We assume, as it is reasonable, that both requests and oers are endowed of
some kind of description. Based on these descriptions the target of the matching process
is nding, for a given request, best matches available in the oers set, and also, given an
oer, determine best matching requests in a peer-to-peer fashion. We may hence think of an
electronic mediator as the actor who actively tries to carry out the matchmaking process.
Obviously descriptions might be provided using unstructured text, and in this case such an
automated mediator should revert to adopting either basic string matching techniques or
more sophisticated Information Retrieval techniques.
The Semantic Web paradigm calls for descriptions that should be provided in a structured form based on ontologies, and we will assume in what follows that requests and oers
are given with reference to a common ontology. It should be noticed that even when requests
and oers are described in heterogeneous languages, or using dierent ontologies modelling
the same domain, schema/data integration techniques may be employed to make them
comparable, as proposed e.g., by Madhavan, Bernstein, and Rahm (2001), and Shvaiko and
Euzenat (2005); but once they are reformulated in a comparable way, one is still left with
the basic matchmaking problems: given a request, are there compatible oers? If there are
several compatible oers, which, and why, are the most promising ones?
Matchmaking has been widely studied and several proposals have been made in the past;
we report on them in Section 2. Recently, there has been a growing eort aimed at the
formalization with Description Logics (DLs) (Baader, Calvanese, Mc Guinness, Nardi, &
Patel-Schneider, 2003) of the matchmaking process (e.g., Di Sciascio, Donini, Mongiello, &
Piscitelli, 2001; Trastour, Bartolini, & Priest, 2002; Sycara, Wido, Klusch, & Lu, 2002; Di
Noia, Di Sciascio, Donini, & Mongiello, 2003b; Li & Horrocks, 2003; Di Noia, Di Sciascio,
Donini, & Mongiello, 2003c, 2003a, among others). DLs, in fact, allow to model structured
descriptions of requests and oers as concepts, usually sharing a common ontology. Furthermore DLs allow for an open-world assumption. Incomplete information is admitted,
and absence of information can be distinguished from negative information. We provide a
little insight on DLs in Section 3.
Usually, DL-based approaches exploit standard reasoning services of a DL system
subsumption and (un)satisabilityto match potential partners in an electronic transaction. In brief, if a supply is described by a concept Sup and a demand by a concept Dem,
unsatisability of the conjunction of Sup and Dem (noted as Sup  Dem) identies the incompatible proposals, satisability identies potential partnersthat still have to agree on
underspecied constraintsand subsumption between Sup and Dem (noted as Sup  Dem)
means that requirements on Dem are completely fullled by Sup.
Classication into compatible and incompatible matches can be useless in the presence of
several compatible supplies; some way to rank most promising ones has to be identied; also
some explanation on motivation of such a rank could be appreciated. On the other hand,
when there is lack of compatible matches one may accept to turn to incompatible matches
that could still be interesting, by revising some of the original requirements presented in
the request, as far as one could easily identify them.
In other words some method is needed to provide a logic-based score for both compatible
and incompatible matches and eventually provide a partial/full ordering, allowing a user
270

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

or an automated agent to choose most promising counteroers. Furthermore it should be
possible, given a score, to provide logical explanations of the resulting score, thus allowing
to understand the rank result and ease further interaction to rene/revise the request.
Although this process is quite simple for a human being it is not so in a logic-based fully
automated framework. We believe there is a need to dene non-monotonic reasoning services
in a DLs setting, to deal with approximation and ranking, and in this paper we propose
the use of Concept Abduction (Di Noia et al., 2003a) and Concept Contraction (Colucci,
Di Noia, Di Sciascio, Donini, & Mongiello, 2003), as services amenable to answer the above
highlighted issues in a satisfactory way. Contributions of this paper include:
 a logical framework to express requests and oers in terms of concept descriptions,
and properties that should hold in a matchmaking facilitator;
 Concept Abduction as a logical basis for ranking compatible counteroers to a given
oer and provide logical explanations of the ranking result;
 Concept Contraction as a logical basis for ranking incompatible matches, aimed at
discovering most promising near misses, and provide logical explanations of the
ranking result;
 algorithms implementing the formalized inferences for matchmaking purposes and
complexity results for a class of matchmaking problems;
 description of our system implementing semantic matchmaking services, and experimental evaluation.
The remaining of the paper is structured as follows: next Section reports on background
work on the subject. Then (Section 3) we briey revise Description Logics basics. To make
the paper self-contained we recall (Section 4) our logic-based framework for matchmaking,
pointing out properties that matchmaking algorithms and systems should guarantee. In
Sections 5 and 6 we present Concept Abduction and Concept Contraction, the two inference
services we devised to compute semantic matchmaking, and present suitable denitions
of the problem along with some complexity results. Then in Section 7 we describe our
matchmaker, and present (Section 7.1) an evaluation of results computed by the system
compared with human users behavior, and with a standard full text retrieval approach.
Conclusions close the paper.

2. Related Work on Matchmaking
Matchmaking has been investigated in recent years under a number of perspectives and for
dierent purposes, with a renovated interest as the information overload kept growing with
the Web widespreading use. We try here to summarize some of the relevant related work.
Vague query answering, proposed by Motro (1988), was an initial eort to overcome limitations of relational databases, using weights attributed to several search variables. More
recent approaches along these lines aim at extending SQL with preference clauses, in
order to softly matchmake data in structured databases (Kieling, 2002). Finin, Fritzson,
McKay, and McEntire (1994) proposed KQML as an agent communication language oriented to matchmaking purposes. Kuokka and Harada (1996) investigated matchmaking
271

fiDi Noia, Di Sciascio & Donini

as a process that allowed potential producers/consumers to provide descriptions of their
products/needs, either directly or through agents mediation, to be later unied by an engine identifying promising matches. Two engines were developed, the SHADE system,
which again used KQML, and as description language KIF, with matchmaking anyway not
relying on any logical reasoning, and COINS, which adopted classical unstructured-text information retrieval techniques, namely the SMART IR system. Similar methods were later
re-considered in the GRAPPA system (Veit, Muller, Schneider, & Fiehn, 2001). Classiedads matchmaking, at a syntactic level, was proposed by Raman, Livny, and Solomon (1998)
to matchmake semi-structured descriptions advertising computational resources in a fashion
anticipating Grid resources brokering. Matchmaking was used in SIMS (Arens, Knoblock,
& Shen, 1996) to dynamically integrate queries; the approach used KQML, and LOOM
as description language. LOOM is also used in the subsumption matching addressed by
Gil and Ramachandran (2001). InfoSleuth (Jacobs & Shea, 1995), a system for discovery
and integration of information, included an agent matchmaker, which adopted KIF and
the deductive database language LDL++. Constraint-based approaches to matchmaking
have been proposed and implemented in several systems, e.g., PersonaLogic3 , Kasbah4 and
systems by Maes, Guttman, and Moukas (1999), Karacapilidis and Moraitis (2001), Wang,
Liao, and Liao (2002), Strobel and Stolze (2002).
Matchmaking as satisability of concept conjunction in DLs was rst proposed in the
same venue by Gonzales-Castillo, Trastour, and Bartolini (2001) and by Di Sciascio et al.
(2001), and precisely dened by Trastour et al. (2002). Sycara, Paolucci, Van Velsen, and
Giampapa (2003) introduced a specic language for agent advertisement in the framework
of the Retsina Multiagent infrastructure. A matchmaking engine was developed (Sycara
et al., 2002; Paolucci, Kawamura, Payne, & Sycara, 2002), which carries out the process on
ve possible levels. Such levels exploit both classical text-retrieval techniques and semantic
match using -subsumption. Nevertheless, standard features of a semantic-based system,
as satisability check are unavailable. It is noteworthy that in this approach, the notion
of plug-in match is introduced, to overcome in some way the limitations of a matching approach based on exact matches. The approach of Paolucci et al. (2002) was later extended
by Li and Horrocks (2003), where two new levels for matching classication were introduced.
A similar classication was proposedin the same venueby Di Noia et al. (2003c), along
with properties that a matchmaker should have in a DL-based framework, and algorithms to
classify and semantically rank matches within classes. Benatallah, Hacid, Rey, and Toumani
(2003) proposed the Dierence Operator in DLs for semantic matchmaking. The approach
uses Concept Dierence, followed by a covering operation optimized using hypergraph techniques, in the framework of web services discovery. We briey comment on the relationship
between Concept Dierence and Concept Abduction at the end of Section 5. An initial DLbased approach, adopting penalty functions ranking, has been proposed by Cal, Calvanese,
Colucci, Di Noia, and Donini (2004), in the framework of dating systems. An extended
matchmaking approach, with negotiable and strict constraints in a DL framework has been
proposed by Colucci, Di Noia, Di Sciascio, Donini, and Mongiello (2005), using both Concept Contraction and Concept Abduction. Matchmaking in DLs with locally-closed world
3. http://www.PersonaLogic.com
4. http://www.kasbah.com

272

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

assumption applying autoepistemic DLs has been proposed by Grimm, Motik, and Preist
(2006).
The need to work in someway with approximation and ranking in DL-based approaches
to matchmaking has also recently led to adopting fuzzy-DLs, as in Smart (Agarwal &
Lamparter, 2005) or hybrid approaches, as in the OWLS-MX matchmaker (Klusch, Fries,
Khalid, & Sycara, 2005). Such approaches, anyway, relaxing the logical constraints, do not
allow any explanation or automated revision service.
Finally, it should be pointed out that matching in DLs, widely treated by Baader,
Kusters, Borgida, and Mc Guinness (1999) has no relation to matchmaking. In fact, in that
work expressions denoting concepts are considered, with variables in expressions. Then
a match is a substitution of variables with expressions that makes a concept expression
equivalent to another. Also the more general setting of concept rewriting in DLs has no
direct relation with matchmakingsee the discussion in Remark 1.

3. Description Logics Basics
In this Section we summarize the basic notions and denitions about Description Logics
(DLs), and about Classic, the knowledge representation system our application is inspired
by. We provide hereafter a brief guided-tour of DLs main characteristics, while the interested
reader can refer to the comprehensive handbook by Baader et al. (2003).
3.1 Description Logics
Description Logicsa.k.a. Terminological Logicsare a family of logic formalisms for Knowledge Representation. All DLs are endowed of a syntax, and a semantics, which is usually
model-theoretic. The basic syntax elements of DLs are:
 concept names, e.g., Computer, CPU, Device, Software,
 role names, like hasSoftware, hasDevice
 individuals, that are used for special named elements belonging to concepts.
Intuitively, concepts stand for sets of objects, and roles link objects in dierent concepts,
as the role hasSoftware that links computers to software. We are not using individuals in
our formalization, hence from now on we skip the parts regarding individuals.
Formally, a semantic interpretation is a pair I = (, I ), which consists of the domain
 and the interpretation function I , which maps every concept to a subset of , and every
role to a subset of   .
Basic elements can be combined using constructors to form concept and role expressions,
and each DL has its distinguished set of constructors. Every DL allows one to form a
conjunction of concepts, usually denoted as ; some DL include also disjunction  and
complement  to close concept expressions under boolean operations.
Roles can be combined with concepts using
 existential role quantication:
e.g., Computer  hasSoftware.WordProcessor
which describes the set of computers whose software include a word processor, and
273

fiDi Noia, Di Sciascio & Donini

 universal role quantication
e.g., Server  hasCPU.Intel
which describes servers with only Intel processors on board.
Other constructs may involve counting, as
 number restrictions:
e.g., Computer  ( 1 hasCPU)
expresses computers with at most one CPU, and
e.g., Computer  ( 4 hasCPU)
describes computers equipped with at least four CPUs.
Many other constructs can be dened, increasing the expressive power of the DL, up to
n-ary relations (Calvanese, De Giacomo, & Lenzerini, 1998).
In what follows, we call atomic concepts the union of concept names, negated concept
names, and unqualied number restrictions. We dene length of a concept C as the number
of atomic concepts appearing in C. We denote the length of C as |C|. Observe that we
consider 	 and  to have zero length. We dene the Quantication Nesting (QN) of a
concept as the following positive integer: the QN of an atomic concept is 0, the QN of a
universal role quantication R.F is 1 plus the QN of F , and the QN of a conjunction
C1  C2 is the maximum between the QNs of conjoined concepts C1 and C2 .
Expressions are given a semantics by dening the interpretation function over each
construct. For example, concept conjunction is interpreted as set intersection: (C  D)I =
C I  DI , and also the other boolean connectives  and , when present, are given the usual
set-theoretic interpretation of union and complement. The interpretation of constructs
involving quantication on roles needs to make domain elements explicit: for example,
(R.C)I = {d1   | d2   : (d1 , d2 )  RI  d2  C I }
3.2 TBoxes
Concept expressions can be used in axiomsthat can be either inclusions (symbol: ), or
denitions (symbol: )which impose restrictions on possible interpretations according
to the knowledge elicited for a given domain. For example, we could impose that monitors
can be divided into CRT and LCD using the two inclusions: Monitor  LCDMonitor 
CRTMonitor and CRTMonitor  LCDMonitor. Or, that computers for a domestic use have
only one operating system as HomePC  ( 1 hasOS). Denitions are useful to give a
meaningful name to particular combinations, as in Server  Computer  ( 2 hasCPU).
Historically, sets of such axioms are called a TBox (Terminological Box). There are
several possible types of TBoxes. General TBoxes are made by General Concept Inclusions
(GCI) of the form C  D, where both C and Dem can be any concept of the DL. For
general TBoxes, the distinction between inclusions and denitions disappears, since any
denition C  D can be expressed by two GCIs C  D, D  C. On the contrary, in
simple TBoxesalso called schemas by Calvanese (1996), and by Buchheit, Donini, Nutt,
and Schaerf (1998)only a concept name can appear on the left-hand side (l.h.s.) of an
axiom, and a concept name can appear on the l.h.s. of at most one axiom. Schemas can be
274

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

cyclic or acyclic, where cyclicity refers to the dependency graph GT between concept names,
dened as follows: every concept name is a node in GT , and there is an arc from concept
name A to concept name B if A appears on the l.h.s. of an axiom, and B appears (at any
level) in the concept on the right-hand side. T is acyclic if GT is, and it is cyclic otherwise.
We call an acyclic schema a simple TBox (Baader et al., 2003, Ch.2). The depth of a simple
TBox T is the length of the longest path in GT . Only for simple TBoxes, unfolding has
been dened as the following process (see Appendix A for a denition): for every denition
A  C, replace A with C in every concept; for every inclusion A  C, replace A with A  C
in every concept. Clearly, such a process trasforms every concept into an equivalent one,
where the TBox can be forgotten. However, for some TBoxes, unfolding can yield concepts
of exponential size w.r.t. the initial concepts. When such an exponential blow-up does not
happen, we call the TBox bushy but not deep (Nebel, 1990).
The semantics of axioms is based on set containment and equality: an interpretation I
satises an inclusion C  D if C I  DI , and it satises a denition C  D when C I = DI .
A model of a TBox T is an interpretation satisfying all axioms of T .
Observe that we make a distinction between equivalence  (used in axioms) and equality
= symbols. We use equality to instantiate generic concept symbols with the concepts they
stand for, e.g., when we write ... where C = A  R.B... we mean that the concept
symbol C stands for the concept expression A  R.B in the text.
3.3 Reasoning Services
DL-based systems usually provide two basic reasoning services:
1. Concept Satisability: given a TBox T and a concept C, does there exist at least one
model of T assigning a non-empty extension to C? We abbreviate satisability of a
concept C w.r.t. a TBox T as C T .
2. Subsumption: given a TBox T and two concepts C and D, is C I always contained in
D I for every model Iof T ? We abbreviate subsumption between C and D w.r.t. T
as C T D.
Since C is satisable i C is not subsumed by , complexity lower bounds for satisability
carry over (for the complement class) to subsumption, and upper bounds for subsumption
carry over to satisability. On the other hand, since C is subsumed by D i C  D is
unsatisable, subsumption is reducible to satisability in DLs admitting general concept
negation, but not in those DLs in which D is outside the languageas in the DLs of the
next Section.
3.4 The System Classic
The system Classic (Borgida, Brachman, McGuinness, & A. Resnick, 1989; Borgida &
Patel-Schneider, 1994) has been originally developed as a general Knowledge Representation
system, and has been successfully applied to conguration (Wright, Weixelbaum, Vesonder,
Brown, Palmer, Berman, & Moore, 1993) and program repositories management (Devambu,
Brachman, Selfridge, & Ballard, 1991).
Its language has been designed to be as expressive as possible while still admitting
polynomial-time inferences for bushy but not deep TBoxes. So it provides intersection of
275

fiDi Noia, Di Sciascio & Donini

name
top
bottom
intersection
universal
quantication
number
restrictions

concrete syntax
TOP
(and C D)

syntax
	

C D

semantics
I

I
C  DI

(all R C)

R.C

{d1 | d2 : (d1 , d2 )  RI  d2  C I }

(at-least n R)
(at-most n R)

( n R)
( n R)

{d1 | {d2 | (d1 , d2 )  RI }  n}
{d1 | {d2 | (d1 , d2 )  RI }  n}

Table 1: Syntax and semantics of some constructs of Classic
name
denition
inclusion
disjoint
group

system notation
(createConcept A C false)
(createConcept A C true)
(createConcept A1 C symbol)
...
(createConcept Ak C symbol)

syntax
AC
AC
disj(A1 , . . . ,Ak )

semantics
AI = C I
AI  C I
for i = 1, . . . , k AIi  C I
and for j = i + 1, . . . , k
AIi  AIj = 

Table 2: Syntax and semantics of the TBox Classic assertions (symbol is a name denoting
the group of disjoint concepts)

concepts but no union, universal but not existential quantication over roles, and number
restrictions over roles but no intersection of roles, since each of these combinations is known
to make reasoning np-hard (Donini, Lenzerini, Nardi, & Nutt, 1991; Donini, 2003).
For simplicity, we only consider a subset of the constructs, namely, conjunction, number
restrictions, and universal role quantications, summarized in Table 1. We abbreviate the
conjunction ( n R)  ( n R) as (= n R). We omit constructs ONE-OF(), FILLS(,)
that refer to individuals, and construct SAME-AS(,) equating llers in functional roles.
The subset of Classic we refer to is known as ALN (Attributive Language with unqualied
Number restrictions) (Donini, Lenzerini, Nardi, & Nutt, 1997b). When number restrictions
are not present, the resulting DL is known as AL (Schmidt-Schau & Smolka, 1991). ALN
provides a minimal set of constructs that allow one to represent a concept taxonomy, disjoint
groups, role restrictions (AL), and number restrictions (N ) to represent restriction son the
number of llers of a role.
Regarding axioms in a TBox, Classic allows one to state a simple TBox of assertions
of the form summarized in Table 2, where A, A1 , . . . ,Ak are all concept names. Axioms
in the TBox are subject to the constraints that every concept name can appear at most
once as the l.h.s. in a TBox, and every concept name cannot appear both on the l.h.s. of a
denition and in a disjointness assertion.
Every Classic concept can be given a normal form. Here we consider the normal form
only for the constructs of ALN that we used in the ontologies and applications. Intuitively,
the normal form pre-computes all implications of a concept, includingpossiblyits unsatisability. The normal form can be reached, up to commutativity of the operator ,
using well-known normalization rules, that we report in Appendix A to make the paper
276

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

self-contained. The normal form of an unsatisable concept is simply . Every satisable
concept C can be divided into three components: Cnames C Call . The component Cnames
is the conjunction of all concept names A1 , . . . ,Ah . The component C is the conjunction
of all number restrictions, no more than two for every role (the maximum at-least and the
minimum at-most for each role), including for every conjunct of C of the form R., the
number restriction ( 0 R) in C . The component Call conjoins all concepts of the form
R.D, one for each role R, where D is again in normal form. We call such form Conjunctive Normal FormCNF, in analogy with Propositional Logicand we observe that CNF
is unique (also said canonical), up to commutativity of conjunction.
Moreover, the TBox in Classic can be embedded into the concepts, by expanding
denitions, and adding the right-hand-side concepts of inclusions, and adding the negation
of disjoint concept namessee Appendix A for more details. For instance, suppose that a
TBox contains:
1. the denition Server  Computer  ( 2 hasCPU),
2. the inclusion Computer  ( 1 hasStorageDevice),
3. and the disjointness assertion disj(AMD, Intel).
Then, the concept ServerhasCPU.Intel can be rewritten into Computer( 2 hasCPU)
( 1 hasStorageDevice)hasCPU.(IntelAMD), which is equivalent to the former w.r.t.
models of the TBox. Observe that the concept name Computer is kept in the rewriting,
since the inclusion gives only a necessary condition ( 1 hasStorageDevice). The latter
concept can be safely conjoined to Computermaking the inclusion unnecessarybut cannot replace it since ( 1 hasStorageDevice) is not a sucient condition for Computer.
Instead, Computer  ( 2 hasCPU) replaces Server since it is a necessary and sucient
condition for it. The disjoint assertion generates Intel  AMD as the range for hasCPU..
Once this rewriting has been carried over all concepts, the TBox can be safely ignored when
computing subsumption (and satisability). In general, this unfolding may lead to an exponential blow-up of the TBox, making the entire computation (unfolding+subsumption) take
exponential time (and space) in the size of the initial concepts and TBox. Yet exponentialtime computation for subsumption is likely to be unavoidable, since even without rewriting,
taking the TBox into account makes subsumption np-hard (Nebel, 1990).
The normal form of concepts can take the TBox embedding into account (see Appendix A.2). In this case, the component Cnames of a Classic concept C contains concept
names Cnames+ and negations of concept names Cnames . In the following, we denote the
CNF of a concept C w.r.t. a simple TBox T as CNF (C, T ). Again, in general, the size
of CNF (C, T ) may be exponential w.r.t. the size of C and T . However, when T is xed,
CNF (C, T ) has polynomial-size w.r.t. the size of C i.e., the exponential increase comes only
from the TBox unfolding. In fact, if k is the maximum size of an unfolded concept name
(a constant if T is xed), the size of CNF (C, T ) can be at most k times the size of C. We
use this argument later in the paper, to decouple the complexity analysis of our reasoning
methods for matchmaking from the complexity raised by the TBox.
To ease presentation of what follows in the next Sections, we adopt a simple reference
ontology, pictured in Figure 1, which is used throughout the paper. To keep the representation within ALN , we modeled memory quantities with number restriction, e.g., 20GB as
277

fiDi Noia, Di Sciascio & Donini



CRTmonitor
LCDmonitor




Monitor 








=




DVDRecorder 





FloppyDisk
 StorageDevice 





HardDisk

Linux
Solaris
Windows2000
WindowsXp




















OperatingSystem 





Browser 




 Device

 Software

WordProcessor


PDA
PC

 Computer
=

Computer  ( 1 hasStorageDevice)  hasStorageDevice.StorageDevice 
hasSoftware.Software  ( 1 ram)
HomePC  PC  ( 1 hasSoftware) 
(= 1 hasOS)  ( 1 hasMonitor)  hasMonitor.Monitor
Server  Computer  ( 2 hasCPU) 
ram.( 512 mb)  hasStorageDevice.( 20000 mb)
Figure 1: Reference Ontology used for examples
( 20000 mb). For reasoners specialized for ALN , this is not a problem, since a number n
is never expanded as n llers (Borgida & Patel-Schneider, 1994; Donini et al., 1997b). For
more expressive DLs, Concrete Domains (Lutz, 1999) should be employed to represent such
quantities.

4. Semantic Matchmaking Using Description Logics
Matchmaking is a widely used term in a variety of frameworks, comprising severalquite
dierentapproaches. We begin this Section trying to provide a generic and sound denition of matchmaking.
Matchmaking is an information retrieval task whereby queries (a.k.a. demands) and resources (a.k.a. supplies) are expressed using semi-structured data
in the form of advertisements, and task results are ordered (ranked) lists of those
resources best fullling the query.
This simple denition implies thatdierently from classical unstructured-text Information
Retrieval systemssome structure in the advertisements is expected in a matchmaking
278

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

system, and matchmaking does not consider a xed database-oriented relational structure.
Furthermore, usually database systems provide answers to queries that do not include a
relevance ranking, which should be instead considered in a matchmaking process.
Semantic matchmaking is a matchmaking task whereby queries and resources
advertisements are expressed with reference to a shared specication of a conceptualization for the knowledge domain at hand, i.e., an ontology.
From now on, we concentrate on semantic matchmaking in marketplaces, adopting specic
terminology, to ease presentation of the approach. Nevertheless our approach applies to
generic matchmaking of semantically annotated resources.
We note that all denitions in this Section apply to every DL that can be used to
describe a marketplace (supplies, demands, background knowledge). We denote by L such
a generic DL. We suppose that a common ontology for supplies and demands is established,
as a TBox T in L. Now a match between a supply and a demand could be evaluated
according to T .
First of all, we remark that a logic-based representation of supplies and demands calls
for generally Open-world descriptions, that is, the absence of a characteristic in the description of a supply or demand should not be interpreted as a constraint of absence. Instead,
it should be considered as a characteristic that could be either rened later, or left open
if it is irrelevant for a user. Note that by generally open we mean that some specic
characteristic might be declared to be closed. However, such a closure should be made
piecewise, using some known declarative tool devised in Knowledge Representation for nonmonotonic reasoning, such as Defaults in DLs (Baader & Hollunder, 1992), Autoepistemic
DLs (Donini, Nardi, & Rosati, 1997a), Circumscription in DLs (Bonatti, Lutz, & Wolter,
2006) etc.
An analysis of recent literature allows to categorize the semantic matchmaking process
between a supply Sup and a demand Dem w.r.t. a TBox T in ve distinct classes:
 exact match: Sup T Dem, i.e., Sup T Dem and Dem T Sup, which amounts
to a perfect match, regardlessin a semantic based environmentof syntactic dierences, i.e., Sup and Dem are equivalent concepts (Di Sciascio et al., 2001; GonzalesCastillo et al., 2001).
 full match: Sup T Dem, which amounts to the demand being completely fullled
by the available supply, i.e., Sup has at least all features required by Dem, but not
necessarily vice versa, being the matchmaking process not symmetric (Di Noia et al.,
2003c); this kind of match is also named subsume match by Li and Horrocks (2003).
 plug-in match: Dem T Sup; it corresponds to demand Dem being sub-concept of
supply Sup,i.e., Dem is more specic than Sup (Sycara et al., 2002; Li & Horrocks,
2003).
 potential match: DemSup T , which corresponds to supply and demand having
something in common and no conicting characteristics (Di Noia et al., 2003c). This
relation is also named intersection-satisable by Li and Horrocks (2003).
279

fiDi Noia, Di Sciascio & Donini

 partial match: Dem  Sup T , which amounts to the presence of conict between
the demand and the available supply (Di Noia et al., 2003c). This relation is also
named disjoint by Li and Horrocks (2003)5 .
We stress that demands could be classied in the same way w.r.t. a given supply, when
its the suppliers turn to look into the marketplace to nd potential buyers. Hence, in the
rest of the paper we use the term oer denoted by the symbol Dto mean either a supply
Sup or a demand Dem, and the term counteroer denoted by Cto mean, respectively,
the demand Dem or the supply Sup that could match D.
Such a classication is still a coarse one, relying directly on known logical relations
between formulae. In fact, the result of matchmaking should be a rank of counteroers,
according to some criteriapossibly explicitso that a user trusting the system would
know whom to contact rst, and in case of failure, whom next, and so on. Such a ranking
process should satisfy some criteria that a Knowledge Representation approach suggests.
We formulate ranking requirements by referring to properties of penalty functions.
Definition 1 Given a DL L, two concepts C, D  L, and a TBox T in L, a penalty
function is a three-arguments function p(C, D, T ), that returns a null or positive integer.
We use penalty functions to rank counteroers C for a given demand (or supply) D w.r.t. a
TBox T . Intuitively, for two given counteroers C1 , C2 in the marketplace, if p(C1 , D, T ) <
p(C2 , D, T ) then the issuer of oer D should rank C1 better than C2 when deciding whom to
contact rst. Clearly, a 0-penalty should be ranked best, and counteroers with the same
penalties should be ranked breaking ties. The rst property we recall is Non-symmetric
evaluation of proposals.
Definition 2 A penalty function p(, , ) is non-symmetric if there exist concepts C, D and
a TBox T such that p(C, D, T ) = p(D, C, T ).
This property is evident when all constraints of D are fullled by C but not vice versa.
Hence, C should be among the top-ranked counteroers in the list of potential partners of
D, while D should not necessarily appear at the top in the list of potential partners of C.
So, a penalty function p(, , ) should not be expected to be a metric distance function.
Secondly, if logic is used to give some meaning to descriptions of supplies and demands,
then proposals with the same meaning should be equally penalized, independently of their
syntactic descriptions.
Definition 3 A penalty function p(, , ) is syntax independent if for every triple of concepts C1 , C2 , D, and TBox T , when T |= C1  C2 then p(C1 , D, T ) = p(C2 , D, T ), and the
same holds also for the second argument , i.e., p(D, C1 , T ) = p(D, C2 , T )
5. We note that preferring the term partial match instead of disjoint, we stress that the match may
still be recoverable, while disjoint is usually meant as a hopeless situation. Moreover, disjoint and
intersection satisable refer to the set-theoretic semantics of concepts in Description Logics, which
is quite hidden and far from the original problems of matchmaking. In a word, they are technologyoriented and not problem-oriented. For instance, if one used Propositional Logic, or Three-valued Logic
for modeling matchmaking, those terms would make no sense.

280

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Clearly, when the logic admits a normal form of expressionsas CNF or DNF for propositional logic, or the normal form of concepts for DLs dened in the previous Sectionusing
such a normal form in the computation of p(, , ) ensures by itself syntax independence.
Penalties should enjoy some desirable properties w.r.t. subsumption. For reasons explained below, we divide penalty functions for ranking potential matches from those for
ranking partial (conicting) matches.
Definition 4 A penalty function for potential matches is monotonic over subsumption
whenever for every issued oer D, for every pair of counteroers C1 and C2 , and TBox T ,
if C1 and C2 are both potential matches for D w.r.t. T , and (C1 T C2 ), then p(C1 , D, T ) 
p(C2 , D, T )
Intuitively, the above denition could be read of as: if C1 T C2 then C1 should be penalized
(and then ranked) either the same, or better than C2 . In a phrase, A ranking of potential
matches is monotonic over subsumption if more specic means better. A dual property
could be stated for the second argument: if D1 T D2 then a counteroer C is less likely to
fulll all characteristics required by D1 than D2 . However, since our scenario is: given an
issuer of a proposal D looking for a match in the marketplace, rank all possible counteroers
C1 , C2 , . . . , from the best one to the worst, we do not deal here with this duality between
rst and second argument of p(, , ).
When turning to partial matches, in which some properties are already in conict between supply and demand, the picture reverses. Now, adding another characteristic to an
unsatisfactory proposal may only worsen this ranking (when another characteristic is violated) or keep it the same (when the new characteristic is not in conict). Note that this
ranking should be kept dierent from the ranking for potential matches. After all, accepting
to discard one or more characteristics that we required is much worse than deciding which
proposal try rst among some potential ones.
Definition 5 A penalty function for partial matches is antimonotonic over subsumption
whenever for every issued oer D, for every pair of counteroers C1 and C2 , and TBox T ,
if C1 and C2 are both partial matches for D w.r.t. T , and (C1 T C2 ), then p(C1 , D, T ) 
p(C2 , D, T )
Intuitively, if C1 T C2 then C1 should be penalized (and then ranked) either the same,
or worse than C2 . In other words, A ranking of partial matches is antimonotonic over
subsumption if more specic means worse. The same property should hold also for the
second argument, since concept conjunction is commutative.
When we need to distinguish between a penalty function for potential matches and one
for partial matches, we put a subscript  in the former (as p ) and a subscript  for the
latter (as in q ).
Clearly, the above requirements are very general, and leave ample room for the denition
of penalty functions. A more subtle requirement would be that penalties should not change
when irrelevant details are added, e.g., if a second-hand computer is requested in a demand
Dem, with no specication for the brand of the CPU, then a supply Sup should be penalized
the same as another oer Sup hasCPU.Intel. However, instead of delving into irrelevance
and other logic-related issues directly from penalties, we now borrow well-known logical
281

fiDi Noia, Di Sciascio & Donini

reasoning frameworks in propositional knowledge representation. Such a detour will give us
a sound and declarative way of dening penalties, dealing with irrelevance as a byproduct,
and more generally bring well-studied non-standard reasoning techniques into matchmaking.

5. Concept Abduction
Abduction (Peirce, 1955) is a well known form of commonsense reasoning, usually aimed at
nding an explanation for some given symptoms or manifestations. Here we introduce Concept Abduction in DLs, showing how it can model potential matchmaking in a DL setting.
Following the notation proposed by Eiter and Gottlob (1995), we recall that a Propositional
Abduction Problem is a triple H, M, T  where H (Hypotheses) and M (Manifestations)
are sets of literals, and T (Theory) is a set of formulae. A solution for H, M, T  is an Explanation E  H such that T  E is consistent, and T  E |= M . We adapt this framework
to DLs as follows.
Definition 6 Let L be a DL, C, D, be two concepts in L, and T be a set of axioms in
L, where both C and D are satisable in T . The Concept Abduction Problem (CAP) for
a given L, C, D, T , is nding, if possible, a concept H  L such that C  H T , and
C  H T D.
We use P as a symbol for a generic CAP, and we denote with SOL(P) the set of all
solutions to a CAP P. Observe that in the denition, we limit the inputs of a CAP to
satisable concepts C and D, since C unsatisable implies that the CAP has no solution
at all, while D unsatisable leads to counterintuitive results (e.g., C would be a solution
in that case). As Propositional Abduction extends implication, Concept Abduction extends concept subsumption. But dierently from propositional abduction, we do not make
any distinction between manifestations and hypotheses, which is usual when abduction is
used for diagnosis. In fact, when making hypotheses about e.g., properties of goods in
e-marketplaces, there is no point in making such a distinction. This uniformity implies that
there is always the trivial solution D to a non-trivial CAP L, C, D, T , as stated more
formally as follows.
Proposition 1 Let L be a DL, let C, D be concepts in L, and T an L-TBox. Then CD T
 if and only if D  SOL(L, C, D, T ).
Proof. If C  D is satisable in T , then D fullls both requirements of Def. 6, the rst
one by hypothesis and the second one because C  D T D is a tautology. On the other
hand, if D  SOL(L, C, D, T ) then C  D T  by denition.
A simple interpretation of this property in our application domain, i.e., matchmaking,
is that if we hypothesize for the counteroer C exactly all specications in D, then the
counteroer trivially meets given specicationsif it was compatible anyway. However, not
all solutions to a CAP are equivalent when using Concept Abduction for matchmaking. To
make a simple example, suppose that already C T D. Then, both H1 = D and H2 = 	
(among others) are solutions of L, C, D, T . Yet, the solution H2 = 	 tells the issuer of
D that C already meets all of Ds specications, while the solution H1 = D is the least
282

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

informative solution from this point of view. Hence, if we want to use abduction to highlight
most promising counteroers, minimal hypotheses must be dened.
Definition 7 Let P =L, C, D, T  be a CAP. The set SOL (P) is the subset of SOL(P)
whose concepts are maximal under T . The set SOL (P) is the subset of SOL(P) whose
concepts have minimum length.
Clearly, being maximal w.r.t. T is still a minimality criterion, since it means that no
unnecessary hypothesis is assumed. It can be proved that the two measures are incomparable.
Proposition 2 There exists a CAP P such that the two sets SOL (P) and SOL (P) are
incomparable.
Proof. It is sucient to consider D = A1  A2  A3 , C = A1 , and T = {B  A2  A3 }. The
logic is even propositional. Then A2  A3  SOL (L, C, D, T ), B  SOL (L, C, D, T ),
and neither solution is in the other set.
The proof highlights that, although -minimality could be preferable for conciseness, it
is heavily dependent on T . In fact, for every concept H  SOL(P), it is sucient to add the
axiom A  H to get a -minimal solution A. On the other hand, also T -maximality has
some drawbacks: if concept disjunction  is present in L, then there is a single T -maximal
solution of P, that is equivalent to the disjunction of all solutions in SOL(P)not a very
useful solution. Making an analogy with Abduction-based Diagnosis (Console, Dupre, &
Torasso, 1991), we could say that the disjunction of all possible explanations is not a very
informative explanation itselfalthough it is maximal w.r.t. implication. We note that
nding a -minimal solution is np-hard for a TBox of depth 1, by a simple reduction from
Set Covering (Colucci, Di Noia, Di Sciascio, Donini, & Mongiello, 2004).
Remark 1 It is interesting to analyze whether concept minimal-rewriting techniquesas
dened by Baader, Kusters, and Molitor (2000)could be employed for computing some
minimal concept abduction, trying to rewrite C  D. The answer is denitely negative for
minimal length abduction: the length-minimal solution B in the proof of Proposition 2
could not be obtained by rewriting C  D = A1  A1  A2  A3 . In fact, A1  B is not
an equivalent rewriting of the former concept. Regarding T -maximality the answer is
more indirect. In fact, present rewriting techniques do not keep a subconcept xed in the
rewriting process. So consider a CAP in which D = A1 , C = A2 , and T = {B  A1  A2 }.
The only equivalent minimal rewriting of C  D is then B, in which a solution cannot be
identied since B cannot be separated into a concept Cthe original oneand a concept
H that is a solution of the CAP. It is open whether future extensions of rewriting might
keep a concept xed, and cope with this problem.
A third minimality criterion is possible for DLs which admit CNF, as for L = ALN .
Definition 8 Let P =L, C, D, T  be a CAP in which L admits CNF, and assume that
concepts in SOL(P) are in CNF. The set SOL (P) is the subset of SOL(P) whose concepts
are minimal conjunctions, i.e., if C  SOL (P) then no sub-conjunction of C (at any level
of nesting) is in SOL(P). We call such solutions irreducible.
283

fiDi Noia, Di Sciascio & Donini

It turns out that -minimality includes both T -maximality and -minimality.
Proposition 3 For every CAP P in which L admits a CNF, both SOL (P) and SOL (P)
are included in SOL (P).
Proof. By contraposition, if a concept H is not -minimal then there is another concept
H  a sub-conjunction of Hwhich is an -minimal solution. But |H  | < |H|, hence H is
not length-minimal. The same for T -maximality: since every sub-conjunction of a concept
H in CNF subsumes H, if H is not -minimal it is not T -maximal either.
The proof of Proposition 2 can be modied to show that minimum-length abduced
concepts are not unique: it is sucient to add another axiom B   A2  A3 to obtain
another minimum-length solution B  . A less obvious result is that also subsumptionmaximal solutions are not unique, at least in non-simple TBoxes: Let P = L, C, D, T 
with T = {A2  A3  A1 }, C = A3 , D = A1 . Then both A1 and A2 are T -maximal
solutions.
5.1 Irreducible Solutions in ALN -simple TBoxes
We assume here that the TBox T of a CAP P = L, C, D, T  is always a simple one. Finding
an irreducible solution is easier than nding a -minimal or a T -maximal solution, since a
greedy approach can be used to minimize the set of conjuncts in the solution. For example,
starting from C  D, we could delete one redundant conjunct at a time (at any level of
role quantication nesting) from D, using |D| calls to a subsumption-check procedure.
However, such an algorithm would be interesting only for theoretical purposes. Instead, we
adapt a structural subsumption algorithm (Borgida & Patel-Schneider, 1994) that collects
all concepts H that should be conjoined to C in order for C  H to be subsumed by D.
The algorithm operates on concepts in CNF. In the following algorithm, we abbreviate the
fact that a concept A appears as a conjunct of a concept C with A  C (thus extending the
meaning of  to conjunctions of concepts).
Algorithm ndIrred (P);
input: a CAP P = L, C, D, T , with L =ALN , simple T , C and D in CNF w.r.t. T
output: concept H  SOL (P) (where H = 	 means that C  D)
variables: concept H
begin
H := 	;
0. if D  C T 
return ;
1. for every concept name A in D
1.1
if A  C
then H := H  A;
2. for every concept ( n R)  D
2.1
such that there is no concept ( m R)  C with m  n
H := H  ( n R);
3. for every concept ( n R)  D
284

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

such that there is no concept ( m R)  C with m  n
H := H  ( n R);
4. for every concept R.E  D
4.1
if there exists R.F  C
4.1.1
then H := H  R.ndIrred (ALN , F, E, T );
4.1.2
else H := H  R.E;
/* now H  SOL(P), but it might be reducible */
5. for every concept Hi  H
if H without Hi is in SOL(P)
then delete Hi from H;
6. return H;
end.
3.1

Theorem 1 Given a CAP P, if ndIrred (P) returns the concept H, with H  , then H
is an irreducible solution of P.
Proof. We rst prove that before Step 5, the computed concept H is in SOL(P), that is,
both C  H T  and C  H T D hold. In fact, observe that CNF (D, T )  H, since all
conjuncts of H come from some conjunct of CNF (D, T ). Hence, D T H since CNF (D, T )
is equivalent to D in the models of T . Adding C to both sides of the subsumption yields
C  D T C  H, and since we assume that C  D T , also C  H T . This proves the
rst condition for H  SOL(P). Regarding the condition C  H T D, suppose it does not
hold: then, at least one conjunct of CNF (D, T ) should not appear in CNF (C  H, T ). But
this is not possible by construction, since H contains every conjunct which is in CNF (D, T )
and not in CNF (C, T ). Therefore, we conclude that H  SOL(P). Once we proved that
the H computed before Step 5 is a solution of P, we just note that Step 5 deletes enough
conjuncts to make H an irreducible solution.
The rst part of algorithm (before Step 5) easily follows well-known structural subsumption algorithms (Borgida & Patel-Schneider, 1994). Step 5 applies a greedy approach, hence
the computed solution, although irreducible, might not be minimal.
We explain the need for the reducibility check in Step 5 with the help of the following
example.
Example 1 Let T = {A1  A2 , A3  A4 }, and let C = A3 , D = A1  A4 . Then L is the
propositional part of AL. The normal form for C is C  = A3  A4 , while D = A1  A2  A4 .
Then before Step 5 the algorithm computes H = A1  A2 , which must still be reduced to
A1 . It is worth noticing that H is already subsumption-maximal since H T A1 . However,
-minimality is a syntactic property, which requires removal of redundant conjuncts.
As for complexity, we aim at proving that nding an irreducible solution is not more
complex than subsumption in ALN . A polynomial algorithm (w.r.t. the sizes of C, D
and T ) cannot be expected anyway, since subsumption in AL (the sublanguage of ALN
without Number Restrictions) with a simple T is conp-hard (Nebel, 1990; Calvanese, 1996).
However, Nebel (1990) argues that the unfolding of the TBox is exponential in the depth of
285

fiDi Noia, Di Sciascio & Donini

the hierarchy T ; if the depth of T grows as O(log |T |) as the size of T increasesa bushy
but not deep TBoxthen its unfolding is polynomial, and so is the above algorithm.
More generally, suppose that T is xed: this is not an unrealistic hypothesis for our
marketplace application, since T represents the ontology of the domain, that we do not
expect to vary while supplies and demands enter and exit the marketplace. In that case, we
can analyze the complexity of ndIrred considering only C and D for the size of the input
of the problem.
Theorem 2 Let P = L, C, D, T  be a CAP, with L =ALN , and T a simple TBox. Then
nding an irreducible solution to P is a problem solvable in time polynomial in the size of
C and D.
We note that the problem of the exponential-size unfolding might be mitigated by Lazy
Unfolding (Horrocks & Tobies, 2000). Using this technique, concept names in the TBox are
unfolded only when needed.
5.2 Abduction-Based Ranking of Potential Matches
We dene a penalty function p for potential matches based on the following intuition: the
ranking of potential matches should depend on how many hypotheses have to be made on
counteroers in order to transform them into full matches.
Definition 9 Given a simple TBox T in ALN , we dene a penalty function for the potential match of a counteroer C given an oer D, where both C and D are concepts in
ALN , as follows:
.
p (C, D, T ) = |ndIrred (ALN , CNF (C, T ), CNF (D, T ), )|

(1)

Note that, when computing p , a concept H is actually computed by ndIrred as an
intermediate step. This makes it easy to devise an explanation facility, so that the actual
obtained ranking can be immediately enriched with its logical explanation; thus improving
users trust and interaction with the matchmaking system.
We now prove that p is in accordance with properties higlighted in the previous Section.
Since the computation of Formula (1) starts by putting concepts C, D in normal form, we
recall that the normal form of C can be summarized as Cnames  C  Call , and similarly for
D. Without ambiguity, we use the three components also as sets of the conjoined concepts.
Theorem 3 The penalty function p is (i) non-symmetric, (ii) syntax independent, and
(iii) monotonic over subsumption.
Proof.
(i) Non-symmetricity is easily proved by providing an example: p (A, 	, ) =
p (	, A, ). In fact, ndIrred (ALN , A, 	, ) nds H1 = 	 as a solution (A  	 without
further hypothesis) while ndIrred (ALN , 	, A, ) nds H2 = A. Recalling that |	| = 0,
while |A| = 1, we get the rst claim.
(ii) Syntax independence follows from the fact that normal forms are used in Formula (1),
and as already said normal forms are unique up to commutativity of conjunction.
286

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

(iii) Monotonicity over subsumption is proved by analyzing the conditions for subsumption in ALN . A concept C  is subsumed by a concept C whenever all conditions below
hold. For each condition, we analyze the changes in the behavior of ndIrred , proving that
the provided solution H just adds other conjuncts. Recall that monotonicity over subsumption is applied only to potential matches, hence we assume that both C and C  are
consistent with D. Since ndIrred is recursive, the proof is also by induction on the quantication nesting (QN) of C  . For C  having QN equal to 0, C  can only be a conjunction
of atomic conceptsnames, negated names, number restrictions. Then the conditions for
subsumption are the following:
 The rst condition is that Cnames+  C  names+ . Hence, in Step 1.1 of ndIrred , the
number of concept names that are added to H  with respect to names added to H
can only decrease, and so |H  |  |H| considering names. Regarding negated names,
observe that they do not contribute to the solution of ndIrred , since they come from
a disjointness axiom and a positive name (that contributes).
 The second condition is that for every number restriction in C , either the same
number restriction appears in C   , or it is strengthened (an at-least increases, an atmost decreases) in C   . Hence, number restrictions added by Steps 2.1 and 3.1 to H 
can be either as many as those added to H, or less. Again, also considering number
restrictions |H  |  |H|.
The above two cases prove the basis of the induction (C  with QN equal to 0). Suppose now
the claim holds for concepts C  with QN n or less, and let C  have a QN of n + 1. Clearly,
in this case C  has at least one universal role quanticationcall it R.F  . The condition
for subsumption between C  and C is the following:
 Either for every universal role quantication R.F in C over the same role R, it must
hold F  T F , or there is no universal role quantication on R in C. In the former
case, observe that ndIrred is recursively called6 in Step 4.1.1 with arguments F , E,
and F  , E; we call I and I  , respectively, the solutions returned by ndIrred . Observe
that the QN of F  is n or less, hence by inductive hypothesis |I  |  |I|. Since Step 4.1.1
adds R.I  and R.I to H  and H, again |H  |  |H|. If instead there is no universal
role quantication on R in C, Step 4.1.2 adds R.E to H. If also C  does not contain
any role quantication on R, then Step 4.1.2 adds R.E also to H  , then H  cannot
be longer than H in this case. If a role quantication R.F  is in C  , then Step 4.1.1
makes a recursive call with arguments F  , E. In this case, the solution returned I 
has length less than or equal to |E|, hence the length of H  cannot be longer than the
length of H also in this case.
In summary, if C  T C then in no case the length of H  increases with respect to the
length of H. This proves the monotonicity over subsumption of p .
Intuitively, we could say that monotonicity over subsumption for potential matches means
the more specic C is, the lower its penalty, the better its ranking w.r.t. D. More
6. findIrred is called only once, because concepts in CNF have at most one universal role quantication
over any role R.

287

fiDi Noia, Di Sciascio & Donini

preciselybut less intuitivelywe should say that the rank of C w.r.t. D cannot worsen
when C is made more specic. Hence, given an oer D, a TBox T , a sequence of increasingly specic counteroers C1 T C2 T C3 T    are assigned to a sequence of
non-increasing penalties p (C1 , D, T )  p (C2 , D, T )  p (C3 , D, T )  . . . We now prove
that such sequences are well-founded, with bottom element zero, reached in case of subsumption.
Proposition 4 p (C, D, T ) = 0 if and only if C T D.
Proof.
Recall from Section 3.1 that 	 and  are the only concepts of length zero, and
ndIrred returns  if and only if C and D are not in a potential match (Step 0 in ndIrred ).
Hence, p (C, D, T ) = 0 if and only if the concept whose length is computed in Formula (1)
is 	. By construction of ndIrred , 	 is returned by the call
ndIrred (ALN , CNF (C, T ), CNF (D, T ), ) if and only if CNF (C, T )  CNF (D, T ), which
holds (see Borgida & Patel-Schneider, 1994) if and only if C T D.
Moreover, we could also prove that adding to C details that are irrelevant for D leaves the
penalty unaected, while adding to C details that are relevant for D lowers Cs penalty.
Note also that in Formula (1) we take T into account in the normal form of C, D, but
then we forget itwe use an empty TBoxwhen calling ndIrred . We explain such a choice
with the aid of an example.
Example 2 Given T = {A  A1  A2 }, let D = A be a Demand with the two following
supplies: C1 = A2 , C2 = 	. Observe that CNF (D, T ) = A  A1  A2 , CNF (C1 , T ) =
A2 , CNF (C2 , T ) = 	. If we used the following formula to compute the penalty
.
p (C, D, T ) = |ndIrred (ALN , C, D, )|

(2)

and ran the algorithm ndIrred (ALN , C1 , D, T ) and ndIrred (ALN , C2 , D, T ), before
Step 5 we would get, respectively,
H1 = A1  A
H2 = A1  A2  A
and after Step 5 ndIrred would return H1 = H2 = A, hence C1 and C2 would receive
the same penalty. However, we argue that C1 is closer to D than C2 is, because it contains a characteristic (A2 ) implicitly required by D, while C2 does not. If instead we call
ndIrred (ALN , CNF (C1 , T ), CNF (D, T ), ) and
ndIrred (ALN , CNF (C2 , T ), CNF (D, T ), ), we get the solutions H1 and H2 aboveand
Step 5 does not delete any conjunct, since T = . Therefore, C1 gets penalty 2, while C2
gets penalty 3, highlighting what is more specied in C1 w.r.t. C2 .
More generally, we can say that the reducibility step (Step 5 in ndIrred ) attens a solution
to its most specic conjuncts, leaving to the TBox the implicit representation of other
characteristics, both the ones already present in the supply and those not present. Therefore,
making an empirical decision, we consider the TBox in the normal form of C and D, but
we exclude it from further reductions in Step 5 of ndIrred .
288

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Remark 2 Although the denition of Concept Abduction could appear similar to Concept
Dierence, it is not so. We note that generically speaking, the name Concept Abduction
appeals to logic, while Concept Dierence appeals to algebra (although Dierence has
multiple solutions when L includes universal role quantication). More precisely, we recall
(Teege, 1994) that dierence is dened as: C  D = max {E  L : (E  D)  C} provided
that C  D. A more specialized denition of dierence (Brandt, Kusters, & Turhan, 2002)
refers only to DLs ALC and ALE. It is dened as: C  D = min {E  L : (E  D) 
(C  D)}where C, E  ALC, D  ALE, and minimality is w.r.t. a preorder  on a specic
normal form which extends CNF to ALC. No TBox is taken into account.
Instead, the solution of a CAP L, C, D, T  does not require that C T D, but only that
C  D T . In general, when D T C if we let H = D  C in a CAP P = L, C, D, T 
we get those solutions for which C  H  Dwhich obviously are not all solutions to P.
Hence D  C  SOL(P), but not vice versa (see the proof of Proposition 2 for an example).
When C T D this comparison is not even possible, since D  C is undened. However, in
a generic setting, e.g., in an e-commerce scenario, subsumption between demand and supply
is quite uncommon; most of oers are such that neither subsumes the other. Because of this
greater generality, for our specic application to matchmaking, Concept Abduction seems
more suited than Concept Dierence to make a basis for a penalty function.

6. Concept Contraction
If D  C is unsatisable in T , but the demander accepts to retract some of Ds constraints,
partially matching supplies may be reconsidered. However, other logic-based approaches
to matchmaking by Trastour et al. (2002), Sycara et al. (2002), Li and Horrocks (2003)
usually exclude the case in which the concept expressing a demand is inconsistent with the
concept expressing a supply, assuming that all requirements are strict ones. In contrast,
we believe that inconsistent matches can still be useful, especially in e-marketplaces. In
fact, partial (a.k.a. disjoint) matches can be the basis for a negotiation process, allowing
a user to specify negotiable requirementssome of which could be bargained in favor of
other. Such a negotiation process can be carried out in various ways adopting approaches
to matchmaking not based on logic (e.g., Strobel & Stolze, 2002), but also, as shown in
practice by Colucci et al. (2005), using Belief Revision. In fact, the logical formalization
of conicting matches, aimed at nding still interesting inconsistent matches without
having to revert to text-based or hybrid approaches, can be obtained exploiting denitions
typical of Belief Revision. In accordance with Gardenfors (1988) formalization, revision of
a knowledge base K with a new piece of knowledge A is a contraction operation, which


such that KA
|= A, followed by the addition of A
results in a new knowledge base KA

to KA usually modeled by conjunction. We call Concept Contraction our adaptation of
Belief Revision to DLs.
Starting with C  D unsatisable in a TBox T , we model with Concept Contraction
how, retracting requirements in C, we may still obtain a concept K (for Keep) such that
K  D is satisable in T . Clearly, a user is interested in what he/she must negotiate on to
start the transactiona concept G (for Give up) such that C  G  K.
289

fiDi Noia, Di Sciascio & Donini

For instance, with reference to the ontology in Figure 1, if a user demands Dem and a
supplier oers Sup, where Dem and Sup are described as follows:
Dem = HomePC  hasMonitor.LCDmonitor
Sup = HomePC  hasMonitor.CRTmonitor
it is possible to check that Sup  Dem is unsatisable. This is a partial match. Yet, in this
case, if the demander gives up the concept G = hasMonitor.LCDmonitor and keeps the
concept K = HomePC, K  Sup is satisable, hence K now potentially matches Sup.
More formally we model a Concept Contraction problem as follows.
Definition 10 (Concept Contraction) Let L be a DL, C, D, be two concepts in L, and
T be a set of axioms in L, where both C and D are satisable in T . A Concept Contraction
Problem (CCP), denoted as L, C, D, T , is nding a pair of concepts G, K  L  L such
that T |= C  GK, and K D is satisable in T . We call K a contraction of C according
to D and T .
We use Q as a symbol for a CCP, and we denote with SOLCCP (Q) the set of all
solutions to a CCP Q. Observe that as for concept abduction, we rule out cases where
either C or D are unsatisable, as they correspond to counterintuitive situations. We note
that there is always the trivial solution G, K = C, 	 to a CCP. This solution corresponds
to the most drastic contraction, that gives up everything of C. On the other hand, when
C  D is satisable in T , the best possible solution is 	, C, that is, give up nothing.
As Concept Abduction extends Subsumption, Concept Contraction extends satisability
in particular, satisability of a conjunction C  D. Hence, results about the complexity of
deciding Satisability of a given concept carry over to Contraction.
Proposition 5 Let L be a DL containing AL, and let Concept Satisability w.r.t. a TBox
in L be a problem C-hard for a complexity class C. Then deciding whether a given pair of
concepts G, K is a solution of a CCP Q =L, C, D, T  is C-hard.
Proof. A concept E  L is satisable w.r.t. a TBox T if and only if the CCP L, C, D, T 
has the solution 	, C, where C = R.E and D = R.	. Then, L should contain at least
universal role quantication (to express R.E), unqualied existential role quantication
(to express R.	), conjunction (to express that C  G  K) and at least the unsatisable
concept  (otherwise every concept is satisable, and the problem trivializes). The minimal, known DL containing all such constructs is the DL AL.
This gives a lower bound on the complexity of Concept Contraction, for all DLs that
include AL. For DLs not including AL, note that if the proof showing C-hardness of
satisability involves a concept with a topmost  symbol, the same proof could be adapted
for Concept Contraction.
Obviously, a user in a marketplace is likely to be willing to give up as few things as
possible, so some minimality in the contraction G must be dened. We skip for conciseness
the denitions of a minimal-length contraction and subsumption-maximal contraction, and
dene straightforwardly conjunction-minimal contraction for DLs that admit a normal form
made up of conjunctions.
290

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Definition 11 Let Q =L, C, D, T  be a CCP in which L admits a CNF. The set SOLCCP (Q)
is the subset of SOLCCP (Q) with the following property: if G, K  SOLCCP (Q) then
for no sub-conjunction G of G it holds G , K  SOLCCP (Q). We call such solutions
irreducible.
6.1 Number-Restriction Minimal Contractions
In what follows we focus on a specic class of irreducible solutions for a CCP ALN , C, D, T 
exposing interesting characteristics from a user-oriented point of view in a matchmaking
scenario. Before dening such a class we explain the rationale behind its investigation using
the following example.
Example 3 Suppose we have the following situation:
demand Dem = HomePC  hasMonitor.LCDmonitor
supply Sup = Server  hasMonitor.CRTmonitor
As T |= Dem  Sup   the demander can contract Dem in order to regain the satisability
with Sup. Two solutions for the CCP Q = ALN , Dem, Sup, T  are:


 G

= HomePC
K = PC  ( 1 hasSoftware)  (= 1 hasOS)


 hasMonitor.LCDmonitor



G = hasMonitor.LCDmonitor
K = HomePC

In G , K  the demander should give up the specication on HomePC; in G , K  the
demander should give up only some specications on the monitor type while keeping the
rest.
Observe that both solutions are in the previously dened class SOLCCP (Q), but from
a user-oriented point of view, G , K  seems the most reasonable solution to Q. Giving
up the HomePC concept in Demand then ( 1 hasMonitor) because of the axiom on
HomePCthe demander keeps all the specications on requested components, but they are
vacuously true, since K  Sup implies hasMonitor. i.e., no component is admitted.
In order to make our intuition more precise, we introduce the number-restriction-minimal
solutions for Q, whose set we denote SOLCCPN (Q). Intuitively, a solution G, K for Q is
in SOLCCPN (Q) when an at-least restriction ( n R) is in G only if it directly conicts
with an at-most restriction ( m R) (with m < n) in D. Solutions in which the atleast restriction is given up because of conicting universal role quanticationse.g., R.A
and R.Aare not in SOLCCPN (Q). Since this characteristic of number-restrictionminimal solutions should be enforced at any level of nesting, we rst introduce the role
path of a concept in ALN . Here we need to distinguish between a concept A and its
(dierent) occurrences in another concept, e.g., B = A  R.A. In theory, we should mark
each occurrence with a number, e.g., A1  R.A2 ; however, since we need to focus on one
occurrence at a time, we just mark it as A.
291

fiDi Noia, Di Sciascio & Donini

Definition 12 Given a concept B in ALN , and an occurrence A of an atomic (sub)concept
A in B, a role path for A in B, A (B) is a string such that:
 A (A) = , where  denotes the empty string
 A (B1  B2 ) = A (Bi ), where Bi , i  {1, 2}, is the concept in which the occurrence
of A appears
 A (R.B) = R  A (B), where  denotes string concatenation
The role path A (B) represents the role nesting of a concept A occurrence into a concept
B. Note that A (B) is the same for any commutation of conjunctions in B, and for any
rearrangement of universal role quanticationsif A was not atomic, this would not be
true7 . Using the previous denition we can now dene SOLCCPN (Q).
Definition 13 Let Q = ALN , C, D, T  be a CCP. The set SOLCCPN (Q) is the subset
of solutions G, K in SOLCCP (Q) such that if ( n R) occurs in G then there exists
( m R), with m < n, occurring in CNF (D, T ) and ( n R) (G) = ( m R) (CNF (D, T )).
We now illustrate an algorithm ndContract that returns a solution G, K  SOLCCPN (Q)
for Q = ALN , CNF (C, T ), CNF (D, T ), , that is, it compares two ALN -concepts C, and
D, both already in CNF w.r.t. a TBox T , and computes a number-restriction minimal contraction G, K of C w.r.t. D without considering the TBox.
Algorithm ndContract (C, D);
input ALN concepts C, D, both already in CNF
output number-restriction minimal contraction G, K,
where G, K = 	, C means that C  D is satisable
variables concepts G, K, G , K 
begin
1. if C = 
then return , 	; /* see comment 1 */
2. G := 	; K := 	  C; /* see comment 2 */
3. for each concept name A  Knames+
if there exists a concept A  Dnames
then G := G  A; delete A from K;
4. for each concept ( x R)  K
such that there is a concept ( y R)  D with y < x
G := G  ( x R); delete ( x R) from K;
5. for each concept ( x R)  K
such that there is a concept ( y R)  D with y > x
G := G  ( x R); delete ( x R) from K;
6. for each concept R.F  Kall
if there exist R.E  Dall and (
either ( x R)  K with x  1
7. For readers that are familiar with the concept-centered normal form of concepts (Baader et al., 2003),
we note that A (B) is a word for UA in the concept-centered normal form of B.

292

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

or ( x R)  D with x  1 )
then let G , K   be the result of ndContract (F, E) in
G := G  R.G ;
replace R.F in K with R.K  ;
7. return G, K;
end.
Let us comment on the algorithm:
1. the case in Step 1 cannot occur at the top level, since we assumed C and D be satisable in the denition of CCP. However,  may occur inside a universal quantication
e.g., C = R.hence, the case of Step 1 may apply in a recursive call of ndContract ,
issued from Step 6 of an outer call.
2. in Step 2, the conjunction 	  C is assigned to K in order to leave 	 in K if every
other concept is removed by the subsequent steps.
We denote by G , K  solutions for the CCP Q = ALN , CNF (C, T ), CNF (D, T ), . In
this simplied CCP Q , we completely unfold T in both C and D and then forget it.
Theorem 4 The pair G, K computed by ndContract (C, D) is a number-restrictionminimal contraction for Q = ALN , CNF (C, T ), CNF (D, T ), .
Proof.
We rst prove that G, K is a solution for Q , namely, that (i) G  K  C,
and that (ii) K  D is satisable. We prove (i) by induction. For the base cases, observe
that the claim is true in Step 2 by construction, and that in Steps 35 when a conjunct
is deleted from K, it is also added to G. Hence the claim holds when no recursive call is
made. For the inductive case, assume the claim holds for each recursive call in Step 6, that
is, G  K   F for every concept R.F  Kall . Let Gn , Kn be the values of variables G, K
before the execution of Step 6, and let Kn be the concept Kn without R.F . Then, after
Step 6 it is:
G  K = (by assigment)


Gn  R.G  Kn  R.K   (by denition of )
Gn  Kn  R.(G  K  )  (by inductive hypothesis)
Gn  Kn  R.F  (by denition of Kn )
Gn  Kn  (since the base case holds before Step 6)
C
Regarding (ii), the proof is again by induction, where the inductive hypothesis is that
K   E is satisable. Basically, we construct an interpretation (, I ) with an element x
such that x  (K  D)I , and show that we can keep constructing I without contradictions,
since contradicting concepts have been deleted from K. In the inductive case, we assume
the existence of an interpretation ( , J ) for K   E such that y    (K   E)J , and then
ff


build a joint interpretation ( , I ) by letting  =   , I  = I  J  {x, y  RI }.
We now prove that G, K is a number-restriction-minimal solution for Q . The proof
is by induction on the Quantication Nesting (QN) of C, dened in Section 3.1. Observe
that an at-least restriction is deleted from K only in Step 4 of ndContract . For the base
caseQN (C) = 0, no recursive callobserve that the role path of a retracted concept
293

fiDi Noia, Di Sciascio & Donini

( n R) in G is , same as the role path of the concept ( m R) in D causing Step 4 to
be executed. Hence, the claim holds in the base case. For the inductive case, assume that
the claim holds for all concepts with QNs smaller than QN (C). Observe that the concept
F in Step 6 is such a concept, since its QN is smaller by at least 1. Hence, if an (occurrence
of an) at-least restriction ( x R), with role path ( x R) (F ) is deleted in F , there exists
a conicting at-most restriction in E with the same role path. Since both F and E occur
inside the scope of a concept R.F , R.E respectively, the claim still holds with role path
( x R) (C) = R  ( x R) (F ).

6.2 Contraction-Based Ranking of Partial Matches
We now dene a penalty function p for partial matches based on the following intuition:
the partial matches should be ranked based on how many characteristics should be retracted
from each C to make them potential matches.
Algorithm penaltyPartial (C, D);
input ALN concepts C, D, both already in CNF
output a penalty for the partial match between C and D
where zero means that C  D is satisable
variables integer n
begin
1. if C = 
then return |D|; /* see Comment 1 */
2. n = 0;
3. for each concept name A  Cnames+
if there exists a concept A  Dnames
then n := n + 1;
4. for each concept ( x R)  C
such that there is a concept ( y R)  D with y < x
n := n + 1;
5. for each concept ( x R)  C
such that there is a concept ( y R)  D with y > x
n := n + 1;
6. for each concept R.F  Call
if there exist R.E  Dall and (
either (( x R)  C and ( y R)  D with x  y) /* see Comment 2 */
or ( x R)  D with x  1 )
then n := n + penaltyPartial (F, E);
7. return n;
end.
The above algorithm has a structure very similar to ndContract : whenever ndContract
removes concepts from K, penaltyPartial adds penalties to n. The two dierences are
explained in the following comments:
294

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

1. Step 1 adds the whole length of D when C = . This addition ensures antimonotonicity in the presence of , as explained in Example 4 below.
2. Step 6 has in penaltyPartial the additional condition and ( y R)  D with x  y.
This condition is necessary because penaltyPartial does not actually remove concepts,
but just counts them. If an at-least restriction in C is in contrast with an at-most
restriction in D , then ndContract removes it from K, while penaltyPartial just adds
1 to n. Yet, when the condition in Step 6 is evaluated, ndContract nds it false just
because the at-least restriction has been removed, while penaltyPartial would nd it
true, were it not for the additional condition.
We now use the outcome of penaltyPartial to dene a penalty function for partial matches.
Definition 14 Given a simple TBox T in ALN , let the penalty function p for the partial
match of a counteroer C given an oer D, where both C and D are concepts in ALN , be
as follows.
.
(3)
p (C, D, T ) = penaltyPartial (CNF (C, T ), CNF (D, T ))
Note that since penaltyPartial closely follows ndContract and ndIrred , in fact Formula (3)
is more similar to Formula (1) in Denition 9 than it might appear. Implicitly, we solve
Q = ALN , CNF (C, T ), CNF (D, T ), , and then use the result in the computation of the
penalty function, with a main dierence in Step 1, though. We explain such a dierence
with the help of an example.
Example 4 Let Dem 1 and Dem 2 be two demands, where Dem 2 T Dem 1 , and let Sup be
a supply, all modeled using the ontology T in Figure 1 as in the following:
Dem 1 = PC  hasMonitor.CRTmonitor
Dem 2 = PC  hasMonitor.
Sup = HomePC  hasMonitor.LCDmonitor
Computing ndContract and penaltyPartial for both CNF (Dem 1 , T ) and CNF (Dem 2 , T )
w.r.t. CNF (Sup, T ) we obtain:
ndContract (CNF (Dem 1 , T ), CNF (Sup, T )) = hasMonitor.CRTmonitor,
PC  hasMonitor.Monitor
penaltyPartial (CNF (Dem 1 , T ), CNF (Sup, T )) = 1
ndContract (CNF (Dem 2 , T ), CNF (Sup, T )) = hasMonitor., PC
penaltyPartial (CNF (Dem 2 , T ), CNF (Sup, T )) = 3
In summary, the concept  conicts with every other concept, yet when a concept
R. is given up, its length is zero (or any other constant), hence the length of G cannot
be directly used as an antimonotonic penalty function. This explains the importance of
Step 1 in the above algorithm.
We can show the following formal correspondence between p and the Concept Contraction
dened in the previous Section.
295

fiDi Noia, Di Sciascio & Donini

Theorem 5 Let Q = ALN , C, D, T  be a CCP, and let G , K  the solution to Q returned by ndContract (CNF (C, T ), CNF (D, T )). If G does not contain any occurrence of
the concept , then
p (C, D, T ) = |G |
Proof. The function p is based on penaltyPartial , and by inspection, whenever penaltyPartial
increments n, ndContract adds an atomic concept to G . The only exception is in Step 1
of penaltyPartial , which adds |D| while ndContract adds  to G . However, this case is
explicitly outside the claim.
We now prove that p is in accordance with properties highlighted in the previous Section.
Theorem 6 The penalty function p is (i) non-symmetric, (ii) syntax independent, and
(iii) antimonotonic over subsumption.
Proof.
(i) Non-symmetry is proven by example: let C = ( 1 R)  R.A, D =
( 2 R)  R.A. For simplicity, T = , and observe that both C and D are already in
CNF. We now show that p (C, D, ) = p (D, C, ). In fact, in the former case, observe that
C must give up everything: the at-most restriction because it is in contrast with the at-least
restriction, and A inside universal quantication because it is in contrast with R.A in
D. Hence, penaltyPartial returns 2 = (1 from Step 5) + (1 from Step 1 of the recursive
call). Hence, p (C, D, ) = 2. In the latter case, instead, once the at-least restriction is
given up (and penaltyPartial adds 1 to n in Step 4), since role llers are no more imposed,
the universal quantication is now compatible (the condition of the if in Step 6 is false).
Hence p (D, C, ) = 1.
(ii) syntax independency is an immediate consequence of the fact that Formula (3)
uses normal forms for concepts. Since normal forms are unique up to commutativity of
conjunctionthat can be xed by imposing some order to conjunctions, e.g., lexicographic
the claim holds.
(iii) antimonotonicity can be proved by induction on the QN of a generic concept C 
subsumed by C; we go through all conditions for subsumption, analyzing the changes in
the behavior of the algorithm from C to C  . Recall that our goal is now to prove that
p (C  , D, T )  p (C, D, T ). In order to make a clear distinction between the two computations, we let n be the (instance of the) variable used in the call to penaltyPartial (C  , D),
while n is used in the call to penaltyPartial (C, D). To ease notation, we assume that C, C 
are already in CNF.
 First of all, it could be the case that C  = . In this case, n = |D| from Step 1 of
penaltyPartial . On the other hand, observe that penaltyPartial (C, D)  |D| because
either C =  too, or every increase in n corresponds to an atomic concept in Dby
inspection of Steps 35, and this recursively in Step 6. Therefore, the claim holds for
this base case.
 Cnames  C  names . For this case, it is obvious that Step 3 in penaltyPartial can only
make more increments to n w.r.t. n, since for C  the number of iterations of the for
each increases.
296

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

 for every number restriction in C , either the same number restriction appears in C   ,
or it is strengthened (an at-least increases, an at-most decreases) in C   . Note that
strengthening a number restriction in C  can only turn from false to true the condition
for the increment of n in Steps 45. For instance, passing from ( x R)  C to
( x R)  C   with x  x, if there is ( y R)  D then y < x implies y < x . A
similar argument holds for the at-most. Moreover, number restrictions that appear
only in C   can only increase the number of iterations of Steps 45, hence n can only
increase w.r.t. n and the claim holds.
The above three cases prove the basis of the induction (C  with QN equal to 0). We now
prove the case for universal role quantication, assuming that the claim holds for QNs less
than QN (C  ).
 for every R.F   C  all , either R is not universally quantied in Call , or there is
R.F  Call such that F  is subsumed by F (with F  = F as a special case of subsumption). Roles which are not universally quantied in Call but are quantied in C  all ,
can only increase the number of iterations of Step 6, hence n can only increase due to
their presence. For roles that have a more specic restriction F  , the inductive hypothesis is assumed to hold, since QN (F  ) < QN (C  ). Hence p (F  , E, T )  p (F, E, T ).
This is equivalent to penaltyPartial (F  , E)  penaltyPartial (F, E). Moreover, if the
condition in Step 6 is true in the call penaltyPartial (C, D), then it is also true in
 , and ( x R)  C  , hence if the recursive
penaltyPartial (C  , D), since R.F   Call

call penaltyPartial (F, E) is issued, then also penaltyPartial (F  , E) is issued, increasing
n at least as much as n is increased, by inductive hypothesis. Hence the claim holds
also in the inductive case.

7. The Matchmaking System
The DLs-based approach to semantic matchmaking illustrated in previous Sections has been
implemented in the ALN reasoning engine MaMaS (MatchMaking Service). It features all
classical inference services of a DL reasoner, but also implements algorithms for the nonstandard services for matchmaking presented in previous Sections.
MaMaS is a multi-user, multi-ontology Java servlet based system; it is available as an
HTTP service at: http://dee227.poliba.it:8080/MAMAS-tng/DIG, and exposes a DIG
1.18 compliant interface. The basic DIG 1.1 has been extended to cope with non standard
services, and we briey describe here such additions.
New elements:
 Match type detection: <matchType>E1 E2</matchType>- computes the match type
according to the following classication: Exact (equivalence), Full, Plug-in, Potential,
Partial.
8. DIG 1.1 is the new standardized DL systems interface developed by the Description Logic Implementation
Group (DIG) (Haarslev & Moller, 2003).

297

fiDi Noia, Di Sciascio & Donini

 Concept Abduction: <abduce>E1 E2</abduce> - implements ndIrred .
 Concept Contraction: <contract>E1 E2</contract>- implements ndContract .
 Ranking Score: <rank type="potential">E1 E2</rank>
<rank type="partial">E1 E2</rank>- computes p (C, D, T ) and p (C, D, T ) as
presented in previous Sections.
New attributes for <newKB/>
 shared: the only values to be used are true and false. In MaMaS, when a new
knowledge base is created, each KB uri is associated with the IP address of the client
host (owner) instantiating the KB. If the shared attribute is set to false, only the
owner is authorized to submit tells statements and change the KB as well as to submit
asks. In this case, requests from IP addresses dierent from the owners one can be
only asks. If the shared attribute is set to true, then no restriction is set on both
tells and asks statements. True is the default value.
 permanent: the only values to be used are true and false. In MaMaS, if a KB is
not used for more than 300 seconds, the KB is automatically released. If a user wants
to maintain the KB indenitely, the permanent attribute must be set to true; false
is the default value.
It should also be pointed out that MaMaS only supports simple-TBox, that is, concept
axioms have a concept name on the left side9 .
We have been using MaMaS as matching engine in various applications, including emarketplaces, (see e.g., Colucci, Di Noia, Di Sciascio, Donini, Ragone, & Rizzi, 2006;
Colucci et al., 2005) and semantic web services discovery (Ragone, Di Noia, Di Sciascio,
Donini, Colucci, & Colasuonno, 2007). We do not delve in details of such applications here,
and refer the interested reader to the cited references.
7.1 Experimental Evaluation
The hypothesis we seek to conrm in this Section is that our approach performs eectively
in a wide range of matchmaking scenarios, i.e., it is able to model commonsense human
behavior in analyzing and ranking, given a request, available oers. Hence the experimental
framework relies on comparison of system behavior versus the judgement of human users.
Furthermore, although our system may allow the use of weights to increase the relevance of
concepts, in the following results refer to the basic unweighted version of the system, to
avoid biasing of results due to weights introduction.
The scenarios we tested our approach on were three: apartments rental, date/partner
nding, skill management for recruiting agencies. Several ontology design methodologies
have been proposed (Jones, Bench-Capon, & Visser, 1998); we adopted the one proposed
by N.F. Noy and D.L. McGuinness (2001).
9. Notice that since MaMaS supports ALN , only atomic negation can be expressed and then <disjoint/>
groups must contain only concepts specialized by an <impliesc> axiom (sub-concept axiom). Dened
concepts <equalc/> (same-class) are not admitted in a disjoint group.

298

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

For all three scenarios we carried out a thorough domain analysis, starting with a large
set of advertisements taken from newspapers or from descriptions of on-line agencies, and
designed ontologies describing the domain. In particular:
 Apartments rental ontology is made up of 146 concepts (primitive + dened) and 33
roles.
 Date/partner matching ontology is made up of 131 concepts (primitive + dened)
and 29 roles.
 Skill matching ontology is made up of 308 concepts (primitive + dened) and 38 roles.
For each scenario we selected several announcements. The total number used in the experiments with human users is 180 (120 oers, 60 requests) for the apartments rental, 215
(140 oers, 75 requests) for the skill matching. 100 advertisements for the Date matching
scenario were also selected, yet for these we did not actually distinguish among requests
and oers as announcements were in the form of proles, although they included preferences
for dating partner. All announcements were in natural language and they were manually
translated in DL syntax. We then created, for each domain, 50 sets of questionnaires.
Questionnaires were in the form of one request (a demand or a supply) and 10 oering advertisements. Three groups of ten randomly selected volunteers, were then asked to order,
according to their judgement advertisements, with respect to the given requests. Having
obtained average users rankings, we run the same sets of advertisements with our system,
which gave us a set of system provided rankings. System rankings that included partial
matching advertisements were simply ordered below worst potential matching advertisement. We adopted, as reference, a standard Vector Space Model (VSM) (Salton & Gill,
1983) system. We used terms in our ontologies attening the ontology descriptions, as dimensions of three separate vector spaces, and determined weights using classical T F  IDF
measure. Similarity results were computed using the well-known Cosine similarity measure
(Salton & Gill, 1983).
To summarize results we adopted the Rnorm (Bollmann, Jochum, Reiner, Weissmann,
& Zuse, 1985) as quality measure of our system eectiveness. Rnorm is dened as follows.
Given Sup, a nite set of descriptions with a user-dened preference relation  that is
complete and transitive, let usr be the rank ordering of Sup induced by users preference
relation, and let sys be the system-provided ranking. Rnorm is then dened as:
Rnorm (sys ) =

S+  S
1
 (1 +
)
+
2
Smax

where S + is the number of descriptions pairs where a better description is ranked by the
system ahead of a worse one; S  is the number of pairs where a worse description is ranked
+
is the maximum possible number of S + . It should be noticed
ahead of a better one and Smax
+

that the calculation of S , S , and Smax is based on the ranking of descriptions pairs in
sys relative to the ranking of corresponding descriptions pairs in usr . Rnorm values are
in the range [0,1]; a value of 1 corresponds to a system-provided ordering of the available
descriptions that is either identical to the one provided by the human users or has a higher
degree of resolution, lower values correspond to a proportional disagreement between the
two. For the three scenarios considered, results are presented in table 3.
299

fiDi Noia, Di Sciascio & Donini

Domain
Apartments rental
Date/partner matching
Skill matching

MaMaS
0.87
0.79
0.91

VSM
0.48
0.41
0.46

Table 3: Rnorm values: MaMaS: Semantic matchmaking results, VSM: Vector Space Model
results

Although they present a variability, which we believe is partly due to the ability to
capture the domain in the ontologies design, they show that our approach provides rankings
that are close to human commonsense behavior and are far better than those obtained with
unstructured text retrieval tools.

8. Conclusion
We have addressed the matchmaking problem between descriptions from a DL perspective.
We have analyzed semantic-based matchmaking process and devised general commonsense
properties a matchmaker should have. We have also pointed out that classical inference
services of DLs, such as satisability and subsumption, are needed and useful, but may be
not sucient to cope with challenges posed by matchmaking in an open environment.
Motivated by this we have studied Concept Abduction and Contraction as novel nonmonotonic inferences in DLs suitable for modeling semantic-based matchmaking scenarios.
We analyzed minimality criteria, and proved simple complexity results. We also presented
reasonable algorithms for classifying and ranking matches based on the devised inferences
in terms of penalty functions, and proved that they obey to properties individuated.
Although several other measures may be determined to compute a score for most
promising matches our proposal has logical foundations and we have empyrically shown it
is able to well simulate commonsense human reasoning. Obviously, as any other semanticbased approach, also our own has to rely on well-designed ontologies able to model the
application domain being considered.
Based on the theoretical work we have implemented a fully functional matchmaking
facilitator, oriented to both generic e-marketplace advertisements and to semantic-based
web-service discovery, which exploits state of art technologies and protocols, and it is, to
the best of our knowledge, the only running system able to cope with Concept Abduction
and Concept Contraction problems.
With specic reference to earlier work of the authors on the subject, Di Sciascio et al.
(2001) dened matchmaking as satisability of concept conjunction. Denitions of potential
match and near-miss i.e., partial match, in terms of abduction and belief-revision were outlined, and the need for ranking of matches motivated, in the work of Di Sciascio, Donini, and
Mongiello (2002). Di Noia et al. (2003b, 2003c) proposed a semantic-based categorization of
matches, logic-based ranking of matches within categories, and properties ranking functions
should have, in the framework of E-marketplaces. An extended and revised version of such
works is in (Di Noia, Di Sciascio, Donini, & Mongiello, 2004). Di Noia et al. (2003a) intro300

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

duced Concept Abduction in DLs and presented algorithms to solve a Concept Abduction
Problem in ALN . Colucci et al. (2003) proposed both Concept Abduction and Concept
Contraction as inferences suitable for semantic-matchmaking and explanation services. Cal
et al. (2004) proposed a basic approach adopting penalty functions ranking, in the framework of dating systems. Colucci et al. (2004) proposed initial results and algorithms based
on truth-prexed tableau to solve Concept Abduction and Contraction problems in ALN .
Colucci et al. (2005) showed that such services can be usefully adopted both for semanticmatchmaking and for nding negotiation spaces in an E-Commerce setting. The use of the
proposed inference services for renement purposes in the semantic-matchmaking process
has been outlined in the work of Colucci et al. (2006).
Our current research is oriented to the investigation of algorithms for more expressive
DLs and the development of a tableaux-based system for the proposed inference services.

Acknowledgments
We are grateful to the anonymous reviewers for comments and suggestions that improved the
quality of this paper. We thank Andrea Cal and Diego Calvanese for useful discussions, and
in particular for suggesting the term penalty function. Simona Colucci, Azzurra Ragone,
Marina Mongiello and all the people at SisInfLab gave us invaluable help and suggestions.
This research has been supported by EU FP-6 IST STREP TOWL co. 026896.

Appendix A. Rules for Normal Form
The normal form of a concept can be obtained by repeatedly applying the rules of the two
following Sections, until no rule is applicable at any level of nesting of concepts inside R.C.
A.1 Rules Involving Subconcepts
In the following rules, the  symbol on the l.h.s. should be considered as an associative and
commutative operator; hence, for instance, when writing ( n R)  ( m R) in the second
rule, this should be read as the concepts ( n R) and ( m R) appear in any order inside
a conjunction of two or more concepts.

C   
( n R)  ( m R)   if n > m
A  A  
( n R)  ( m R)  ( n R) if n > m
( n R)  ( m R)  ( n R) if n < m
R.D1  R.D2  R.(D1  D2 )
R.  R.  ( 0 R)
301

fiDi Noia, Di Sciascio & Donini

A.2 Rules Involving the Concept and the TBox

A  A  C if A  C  T
A  C if A  C  T
A  A  B1      Bk if disj (A, B1 , . . . , Bk )  T
Usually the concept resulting from the application of the above rules is referred to as an
expansion, or unfolding of a TBox.
A.3 Properties of the Normal Form
Let C be a concept in Classic, and let C  be any concept obtained from C by repeatedly
appying the above rules. Let |C|, |C  | denote the size of C, C  respectively. It can be proved
(Borgida & Patel-Schneider, 1994) that:
1. if |C  | is polynomially bounded in |C|, then C  can be computed in time O(|C|2 );
2. every concept resulting from the application of the rules is equivalent to C, w.r.t.
models of the TBox.
As a consequence of the latter property, C is unsatisable i its normal form is . Then,
as a consequence of the former property, unsatisability can be decided in polynomial time
(Borgida & Patel-Schneider, 1994). The fact that |C  | is polynomially bounded in |C| has
been intuitively related by Nebel (1990) to the form of TBoxes, that should be bushy but
not deep. A more precise denition has been given by Colucci et al. (2004).

References
Agarwal, S., & Lamparter, S. (2005). smart - a semantic matchmaking portal for electronic
markets. In Proceedings of the 7th International IEEE Conference on E-Commerce
Technology 2005.
Arens, Y., Knoblock, C. A., & Shen, W. (1996). Query Reformulation for Dynamic Information Integration. Journal of Intelligent Information Systems, 6, 99130.
Baader, F., Calvanese, D., Mc Guinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
The Description Logic Handbook. Cambridge University Press.
Baader, F., & Hollunder, B. (1992). Computing extensions of terminological default theories.
In Proceedings of ECAI Workshop on Knowledge Representation and Reasoning, pp.
3052.
Baader, F., Kusters, R., Borgida, A., & Mc Guinness, D. (1999). Matching in Description
Logics. Journal of Logic and Computation, 9 (3), 411447.
Baader, F., Kusters, R., & Molitor, R. (2000). Rewriting concepts using terminologies.
In Proceedings of the Seventh International Conference on Principles of Knowledge
Representation and Reasoning (KR2000), pp. 297308.
302

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Benatallah, B., Hacid, M.-S., Rey, C., & Toumani, F. (2003). Request Rewriting-Based Web
Service Discovery. In International Semantic Web Conference, Vol. 2870 of Lecture
Notes in Computer Science, pp. 242257. Springer.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientic American,
248 (4), 3443.
Bollmann, P., Jochum, F., Reiner, U., Weissmann, V., & Zuse, H. (1985). The LIVEProject-Retrieval experiments based on evaluation viewpoints. In Proceedings of the
8th Annual International ACM/SIGIR Conference on Research and Development in
Information Retrieval, pp. 213214. ACM, New York.
Bonatti, P., Lutz, C., & Wolter, F. (2006). Description logics with circumscription. In
Proceedings of the Tenth International Conference on Principles of Knowledge Representation and Reasoning (KR2006), pp. 400410.
Borgida, A., Brachman, R. J., McGuinness, D. L., & A. Resnick, L. (1989). CLASSIC: A
Structural Data Model for Objects. In Proceedings of the ACM SIGMOD International
Conference on Management of Data, pp. 5967.
Borgida, A., & Patel-Schneider, P. F. (1994). A Semantics and Complete Algorithm for
Subsumption in the CLASSIC Description Logic. Journal of Articial Intelligence
Research, 1, 277308.
Brandt, S., Kusters, R., & Turhan, A. (2002). Approximation and dierence in description logics. In Proceedings of the Eight International Conference on Principles of
Knowledge Representation and Reasoning (KR2002), pp. 203214. MK.
Buchheit, M., Donini, F., Nutt, W., & Schaerf, A. (1998). A rened architecture for terminological systems: Terminology = schema + views. Articial Intelligence, 99 (2),
209260.
Cal, A., Calvanese, D., Colucci, S., Di Noia, T., & Donini, F. M. (2004). A description logic
based approach for matching user proles. In Proceedings of the 17th International
Workshop on Description Logics (DL04), Vol. 104 of CEUR Workshop Proceedings.
Calvanese, D. (1996). Reasoning with Inclusion Axioms in Description Logics. In Proceedings
of the Twelfth European Conference on Articial Intelligence (ECAI96), pp. 303307.
John Wiley & Sons.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). On the Decidability of Query
Containment under Constraints. In Proceedings of the Seventeenth ACM SIGACT
SIGMOD SIGART Symposium on Principles of Database Systems (PODS98), pp.
149158.
Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2003). Concept Abduction and Contraction in Description Logics. In Proceedings of the 16th International
Workshop on Description Logics (DL03), Vol. 81 of CEUR Workshop Proceedings.
Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2004). A Uniform
Tableaux-Based Approach to Concept Abduction and Contraction in ALN. In Proceedings of the 17th International Workshop on Description Logics (DL04), Vol. 104
of CEUR Workshop Proceedings.
303

fiDi Noia, Di Sciascio & Donini

Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2005). Concept
Abduction and Contraction for Semantic-based Discovery of Matches and Negotiation
Spaces in an E-Marketplace. Electronic Commerce Research and Applications, 4 (4),
345361.
Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F., Ragone, A., & Rizzi, R. (2006). A
semantic-based fully visual application for matchmaking and query renement in B2C
e-marketplaces. In 8th International conference on Electronic Commerce, ICEC 06,
pp. 174184. ACM Press.
Console, L., Dupre, D., & Torasso, P. (1991). On the Relationship between Abduction and
Deduction. Journal of Logic and Computation, 1 (5), 661690.
Devambu, P., Brachman, R. J., Selfridge, P. J., & Ballard, B. W. (1991). LASSIE: A
Knowledge-Based Software Information System. Communications of the ACM, 34 (5),
3649.
Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2003a). Abductive matchmaking
using description logics. In Proceedings of the Eighteenth International Joint Conference on Articial Intelligence (IJCAI 2003), pp. 337342.
Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2003b). Semantic matchmaking
in a P-2-P electronic marketplace. In Proc. Symposium on Applied Computing (SAC
03), pp. 582586. ACM.
Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2003c). A system for principled
Matchmaking in an electronic marketplace. In Proc. International World Wide Web
Conference (WWW 03), pp. 321330. ACM, New York.
Di Noia, T., Di Sciascio, E., Donini, F., & Mongiello, M. (2004). A system for principled Matchmaking in an electronic marketplace. International Journal of Electronic
Commerce, 8 (4), 937.
Di Sciascio, E., Donini, F., & Mongiello, M. (2002). Knowledge representation for matchmaking in P2P e-commerce. In Atti dellVIII Convegno dellAssociazione Italiana di
Intelligenza Articiale, Siena.
Di Sciascio, E., Donini, F., Mongiello, M., & Piscitelli, G. (2001). A Knowledge-Based System for Person-to-Person E-Commerce. In Proceedings of the KI-2001 Workshop on
Applications of Description Logics (ADL-2001), Vol. 44 of CEUR Workshop Proceedings.
Donini, F. M. (2003). Complexity of reasoning. In Description Logics Handbook, chap. 3.
Cambridge University Press.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991). The Complexity of Concept Languages. In Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proceedings of the
Second International Conference on the Principles of Knowledge Representation and
Reasoning (KR91), pp. 151162. Morgan Kaufmann, Los Altos.
Donini, F. M., Nardi, D., & Rosati, R. (1997a). Autoepistemic description logics. In Proc.
of IJCAI 97, pp. 136141.
304

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1997b). The complexity of concept
languages. Information and Computation, 134, 158.
Eiter, T., & Gottlob, G. (1995). The Complexity of Logic-Based Abduction. Journal of the
ACM, 42 (1), 342.
Finin, T., Fritzson, R., McKay, D., & McEntire, R. (1994). KQML as an Agent Communication Language. In Proceedings of the Third International Conference on Information
and Knowledge Management (CIKM94), pp. 456463. ACM.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
Bradford Books, MIT Press, Cambridge, MA.
Gil, Y., & Ramachandran, S. (2001). PHOSPHORUS: a Task based Agent Matchmaker.
In Proc. International Conference on Autonomous Agents 01, pp. 110111. ACM.
Gonzales-Castillo, J., Trastour, D., & Bartolini, C. (2001). Description Logics for Matchmaking of Services. In Proceedings of the KI-2001 Workshop on Applications of Description Logics (ADL-2001), Vol. 44. CEUR Workshop Proceedings.
Grimm, S., Motik, B., & Preist, C. (2006). Matching Semantic Service Descriptions with
Local Closed-World Reasoning. In European Semantic Web Conference, pp. 575589.
Haarslev, V., & Moller, R. (2003). The dig description logic interface. In Proceedings of the
International Workshop on Description Logics (DL-2003), Vol. 81 of CEUR Workshop
Proceedings.
Horrocks, I., & Tobies, S. (2000). Reasoning with axioms: Theory and practice.. In Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning (KR2000), pp. 285296.
Jacobs, N., & Shea, R. (1995). Carnot and Infosleuth  Database Technology and the Web.
In Proceedings of the ACM SIGMOD International Conference on Management of
Data, pp. 443444. ACM.
Jones, D., Bench-Capon, T., & Visser, P. (1998). Methodologies for ontology development.
In J. Cuena, editor, Proc. 15th IFIP World Computer Congress, pp. 6275, London,
UK. Chapman and Hall.
Karacapilidis, N., & Moraitis, P. (2001). Building an Agent-Mediated Electronic Commerce
System with Decision Analysis Features. Decision Support Systems, 32, 5369.
Kieling, W. (2002). Foundations of preferences in database systems. In Proceedings of the
Twentyeight International Conference on Very Large Data Bases (VLDB 2002).
Klusch, M., Fries, B., Khalid, M., & Sycara, K. (2005). Owls-mx: Hybrid owl-s service
matchmaking. In Proceedings of 1st Intl. AAAI Fall Symposium on Agents and the
Semantic Web.
Kuokka, D., & Harada, L. (1996). Integrating Information Via Matchmaking. Journal of
Intelligent Information Systems, 6, 261279.
Li, L., & Horrocks, I. (2003). A Software Framework for Matchmaking Based on Semantic
Web Technology. In Proc. International World Wide Web Conference (WWW 03),
pp. 331339. ACM, New York.
305

fiDi Noia, Di Sciascio & Donini

Lutz, C. (1999). Reasoning with concrete domains. In Dean, T. (Ed.), Proceedings of the
Sixteenth International Joint Conference on Articial Intelligence (IJCAI99), pp.
9095, Stockholm, Sweden. Morgan Kaufmann, Los Altos.
Madhavan, J., Bernstein, P., & Rahm, E. (2001). Generic schema matching with cupid. In
Proceedings of the Twentyseventh International Conference on Very Large Data Bases
(VLDB 2001), pp. 4958.
Maes, P., Guttman, R., & Moukas, A. (1999). Agents that Buy and Sell. Communications
of the ACM, 42 (3), 8191.
Motro, A. (1988). VAGUE: A User Interface to Relational Databases that Permits Vague
Queries. ACM Transactions on Oce Information Systems, 6 (3), 187214.
Nebel, B. (1990). Terminological Reasoning is Inherently Intractable. Articial Intelligence,
43, 235249.
N.F. Noy and D.L. McGuinness (2001). Ontology Development 101: A Guide to Creating
Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report KSL01-05.
Paolucci, M., Kawamura, T., Payne, T., & Sycara, K. (2002). Semantic Matching of Web
Services Capabilities. In The Semantic Web - ISWC 2002, No. 2342 in Lecture Notes
in Computer Science, pp. 333347. Springer-Verlag.
Peirce, C. . (1955). Abduction and induction. In Philosophical Writings of Peirce, chap. 11.
J. Buchler.
Ragone, A., Di Noia, T., Di Sciascio, E., Donini, F., Colucci, S., & Colasuonno, F. (2007).
Fully Automated Web Services Discovery and Composition through Concept Covering
and Concept Abduction. International Journal of Web Services Research (JWSR),
4 (3).
Raman, R., Livny, M., & Solomon, M. (1998). Matchmaking: distributed resource management for high throughput computing. In Proceedings of IEEE High Performance
Distributed Computing Conf., pp. 140146.
Salton, G., & Gill, M. M. (1983). Introduction to Modern Information Retrieval. McGrawHill, New York.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive Concept Descriptions with Complements. Articial Intelligence, 48 (1), 126.
Shvaiko, P., & Euzenat, J. (2005). A survey of schema-based matching approaches. Journal
on Data Semantics, 4, 146171.
Strobel, M., & Stolze, M. (2002). A Matchmaking Component for the Discovery of Agreement and Negotiation Spaces in Electronic Markets. Group Decision and Negotiation,
11, 165181.
Sycara, K., Paolucci, M., Van Velsen, M., & Giampapa, J. (2003). The RETSINA MAS
infrastructure. Autonomous agents and multi-agent systems, 7, 2948.
Sycara, K., Wido, S., Klusch, M., & Lu, J. (2002). LARKS: Dynamic Matchmaking Among
Heterogeneus Software Agents in Cyberspace. Autonomous agents and multi-agent
systems, 5, 173203.
306

fiSemantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach

Teege, G. (1994). Making the dierence: A subtraction operation for description logics. In
Proceedings of the Fourth International Conference on the Principles of Knowledge
Representation and Reasoning (KR94), pp. 540550. MK.
Trastour, D., Bartolini, C., & Priest, C. (2002). Semantic Web Support for the Business-toBusiness E-Commerce Lifecycle. In Proc. International World Wide Web Conference
(WWW) 02, pp. 8998. ACM.
Veit, D., Muller, J., Schneider, M., & Fiehn, B. (2001). Matchmaking for Autonomous
Agents in Electronic Marketplaces. In Proc. International Conference on Autonomous
Agents 01, pp. 6566. ACM.
Wang, H., Liao, S., & Liao, L. (2002). Modeling Constraint-Based Negotiating Agents.
Decision Support Systems, 33, 201217.
Wright, J. R., Weixelbaum, E. S., Vesonder, G. T., Brown, K. E., Palmer, S. R., Berman,
J. I., & Moore, H. H. (1993). A Knowledge-Based Congurator that Supports Sales,
Engineering, and Manufacturing at AT&T Network Systems. AI Magazine, 14 (3),
6980.

307

fiJournal of Artificial Intelligence Research 29 (2007) 4977

Submitted 09/06; published 05/07

Solution-Guided Multi-Point Constructive Search for Job
Shop Scheduling
J. Christopher Beck

jcb@mie.utoronto.ca

Department of Mechanical & Industrial Engineering
University of Toronto, Canada

Abstract
Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel constructive
search technique that performs a series of resource-limited tree searches where each search
begins either from an empty solution (as in randomized restart) or from a solution that has
been encountered during the search. A small number of these elite solutions is maintained
during the search. We introduce the technique and perform three sets of experiments on
the job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS is
carried out to evaluate the performance impact of various parameter settings. Second, we
inquire into the diversity of the elite solution set, showing, contrary to expectations, that
a less diverse set leads to stronger performance. Finally, we compare the best parameter
setting of SGMPCS from the first two experiments to chronological backtracking, limited
discrepancy search, randomized restart, and a sophisticated tabu search algorithm on a set
of well-known benchmark problems. Results demonstrate that SGMPCS is significantly
better than the other constructive techniques tested, though lags behind the tabu search.

1. Introduction
A number of metaheuristic and evolutionary approaches to optimization can be described
as being solution-guided, multi-point searches. For example, in genetic and mimetic algorithms, a population of solutions is maintained and used as a basis for search. Each new
generation is created by combining aspects of the current generation: search is therefore
guided by existing solutions. As the population contains a number of individual solutions,
the search makes use of multiple points in the search space. Traditional single-point metaheuristics, such as tabu search, have been augmented in a similar way. The TSAB tabu
search (Nowicki & Smutnicki, 1996) maintains an elite pool consisting of a small number
of the best solutions found so far during the search. Whenever the basic search reaches a
threshold number of moves without finding a new best solution, search is restarted from one
of the elite solutions. Again, the higher-level search is guided by multiple existing solutions,
though the guidance is somewhat different than in genetic algorithms.
Solution-Guided Multi-Point Constructive Search (SGMPCS) 1 is a framework designed
to allow constructive search to be guided by multiple existing (suboptimal) solutions to
a problem instance. As with randomized restart techniques (Gomes, Selman, & Kautz,
1998), the framework consists of a series of tree searches restricted by some resource limit,
1. In previous conference and workshop publications, SGMPCS is referred to simply as Multi-Point Constructive Search (Beck, 2006; Heckman & Beck, 2006; Beck, 2005a, 2005b). Empirical evidence of
the importance of solution guidance motivated this change to a name more reflective of the important
differences between this work and existing tree search techniques.
c
2007
AI Access Foundation. All rights reserved.

fiBeck

typically a maximum number of fails. When the resource limit is reached, search restarts.
The difference with randomized restart is that SGMPCS keeps track of a small set of elite
solutions: the best solutions it has found. When search is restarted, it starts from an
empty solution, as in randomized restart, or from one of the elite solutions.
In this paper, we undertake the first fully crossed systematic empirical study of SGMPCS. In particular, in Section 3 we investigate the different parameter settings and their impact on search performance for the makespan-minimization variant of the job shop scheduling problem. Results indicate that guidance with elite solutions contributes significantly
to algorithm performance but, somewhat unexpectedly, that smaller elite set size results
in better performance. Indeed, an elite set size of one showed the best performance. This
result motivates subsequent experimentation on the diversity of the elite set in Section 4.
We show, again contrary to expectation but consistent with an elite set size of one, that the
less diverse the elite set, the stronger the performance. As discussed in-depth in Section
6, these two sets of experiments call into question the extent to which the exploitation
of multiple points in the search space is important for the performance of SGMPCS. A
final experiment (Section 5) compares the best parameter settings found in the first two
experiments with chronological backtracking, limited discrepancy search (Harvey, 1995),
randomized restart, and a state-of-the-art tabu search (Watson, Howe, & Whitley, 2006)
on a set of well-known benchmarks. These results show that SGMPCS significantly outperforms the other constructive search methods but does not perform as well as the tabu
search.
The contributions of this paper are as follows:
1. The introduction and systematic experimental evaluation of Solution-Guided MultiPoint Constructive Search (SGMPCS).
2. The investigation of the importance of the diversity of the elite set to the performance
of SGMPCS.
3. The demonstration that SGMPCS significantly out-performs chronological backtracking, limited discrepancy search, and randomized restart on a benchmark set of job
shop scheduling problems.

2. Solution-Guided Multi-Point Constructive Search
Pseudocode for the basic Solution-Guided Multi-Point Constructive Search algorithm is
shown in Algorithm 1. The algorithm initializes a set, e, of elite solutions and then enters
a while-loop. In each iteration, with probability p, search is started from an empty solution
(line 6) or from a randomly selected elite solution (line 12). In the former case, if the
best solution found during the search, s, is better than the worst elite solution, s replaces
the worst elite solution. In the latter case, s replaces the starting elite solution, r, if s is
better than r. Each individual search is limited by a maximum number of fails that can
be incurred. When an optimal solution is found and proved or when some overall bound
on the computational resources (e.g., CPU time, number of fails) is reached, the best elite
solution is returned.
The elite solutions can be initialized by any search technique. In this paper, we use 50
independent runs of the same randomized texture-based heuristic that is employed in the
50

fiSolution-Guided Multi-Point Constructive Search

SGMPCS():
1
2
3
4
5
6
7
8

9
10
11
12
13
14

15

initialize elite solution set e
while termination criteria unmet do
if rand[0, 1) < p then
set upper bound on cost function
set fail limit, l
s := search(, l)
if s 6=  and s is better than worst(e) then
replace worst(e) with s
end
else
r := randomly chosen element of e
set upper bound on cost function
set fail limit, l
s := search(r, l)
if s 6=  and s is better than r then
replace r with s
end
end
end
return best(e)
Algorithm 1: SGMPCS: Solution-Guided Multi-Point Constructive Search

main search (see Section 3.2). No backtracking is done and no upper bound is placed on the
cost function. Without an upper bound, each run will find a solution, though probably one
of quite low quality. From this initial set of 50 solutions, the |e| best solutions are inserted
into the elite set. The primary goal for the initialization is to quickly populate the elite set.
Previous work (Beck, 2006) has shown that while spending more effort in each run to find
good starting solutions (e.g., via backtracking search) does not significantly improve overall
performance, the number of runs does have an impact. When the variance in quality among
the initial solutions is high, the best starting solution of a large elite set will be much better
than that of a small elite set. This difference alone was sufficient to skew experiments that
measured the impact of different elite set sizes on overall performance. To mitigate this
effect we generate a fixed number of elite solution candidates (i.e., 50) and then choose the
|e| best. An interesting direction for future work is to adaptively determine the best time
to transition from the elite pool generation to the main search.
2.1 Search
In lines 6 and 12 the search(r, l) function is a standard tree search with some randomization,
limited by the number of fails, l, and, when r 6= , guided by solution r. The search function
returns the best solution found, if any, and an indication as to whether the search space has
been exhausted. Given a large enough fail limit, an individual search can completely search
the space. Therefore, the completeness of this approach depends on the policy for setting
and increasing the fail limit. As we will see in Experiment 3 (Section 5), SGMPCS is able
51

fiBeck

to find optimal solutions and prove their optimality. We place no other restrictions on the
search, allowing any tree traversal technique to be used. In particular, we experiment with
both chronological backtracking and limited discrepancy search (Harvey, 1995).
When r 6= , the search is guided by the reference solution, r. The guiding solution
is simply used as the value ordering heuristic: we search using any (randomized) variable
ordering heuristic and by specifying that the value assigned to a variable is the one in the
reference solution, provided it is still in the domain of the variable.
A search tree is created by asserting a series of choice points of the form: hV i = xihVi 6=
xi, where Vi is a variable and x is the value assigned to V i . Given the importance of variable
ordering heuristics in constructive search, we expect that the order of these choice points will
have an impact on search performance. SGMPCS can, therefore, use any variable ordering
heuristic to choose the next variable to assign. The choice point is formed using the value
assigned in the reference solution or, if the value in the reference solution is inconsistent,
a heuristically chosen value. More formally, let a reference solution, r, be a set of variable
assignments, {hV1 = x1 i, hV2 = x2 i, . . . , hVm = xm i}, m  n, where n is the number of
variables. The variable ordering heuristic has complete freedom to choose a variable, V i , to
be assigned. If xi  dom(Vi ), where hVi = xi i  r, the choice point is made with x = x i .
Otherwise, if xi 
/ dom(Vi ), any value ordering heuristic can be used to choose x  dom(V i ).
We need to account for the possibility that x i 
/ dom(Vi ) because the reference solution
is not necessarily a valid solution later in the SGMPCS search process. To take a simple
example, if the reference solution has a cost of 100 and we constrain the search to find a
better solution, we will not reach the reference solution. Rather, via constraint propagation,
we will reach a dead-end or different solution.
This technique for starting constructive search from a reference solution is quite general.
Existing high-performance variable ordering heuristics can be exploited and, by addressing
the case of xi 
/ dom(Vi ), we make no assumptions about changes to the constraint model
that may have been made after the reference solution was originally found. In particular,
this means that an elite solution could be the solution to a relaxation of the full problem.
2.2 Setting the Bounds on the Cost Function
Before each individual search (lines 6 and 12), we place an upper bound on the cost function.
The bound has an impact on the set of solutions and, therefore, on the solutions that may
enter the elite set. Intuitions from constructive search and metaheuristics differ on the
appropriate choice of an upper bound. In standard tree search for optimization with a
discrete cost function, the usual approach is to use c   1 as the upper bound, where c is
the best solution found so far. Using a higher bound would only expand the search space
without providing any heuristic benefit. In contrast, in a standard metaheuristic approach,
search is not usually restricted by enforcing an upper bound on the cost of acceptable states:
the search is allowed to travel through worse states in order to (hopefully) find better ones.
As a consequence, it is common to replace an elite solution when a better, but not necessarily
best-known, solution is found. Since the elite solutions are used to heuristically guide search,
even solutions which are not the best-known can provide heuristic guidance.
These two perspectives result in two policies:
1. Global Bound: Always set the upper bound on the search cost to c   1.
52

fiSolution-Guided Multi-Point Constructive Search

2. Local Bound: When starting from an empty solution, set the upper bound to be
equal to one less than the cost of the worst elite solution. When starting from an elite
solution, set the upper bound to be one less than the cost of the starting solution.

In constraint programming, back-propagation is the extent to which placing a bound on
the cost function results in domain reductions for decision variables. Previous experiments
with SGMPCS on optimization problems with strong back-propagation (such as job shop
scheduling with the objective of minimizing makespan) show that the global bound policy
is superior (Beck, 2006). For problems with weaker back-propagation and for satisfaction
problems (where there is no back-propagation), the local bound approach performs better
(Beck, 2006; Heckman & Beck, 2006). Based on these results, we use the global bound
policy here.
2.3 Related Work
SGMPCS is most directly inspired by the TSAB tabu search algorithm (Nowicki & Smutnicki, 1996) noted above. In TSAB, an elite pool consisting of a small number of the best
solutions found is maintained during the search. Whenever the basic tabu search stagnates,
that is, when it reaches a threshold number of moves without finding a new best solution,
search is restarted from one of the elite solutions. The tabu list is modified so that when
search is restarted, it will follow a different search path. This is the basic mechanism,
adapted for constructive search, that is used in SGMPCS. For a number of years, TSAB
was the state-of-the-art algorithm for job shop scheduling problems. It has recently been
over-taken by i-TSAB, an algorithm based on TSAB that makes a more sophisticated use
of the elite pool (Nowicki & Smutnicki, 2005). For an in-depth analysis of i-TSAB see the
work of Watson, Howe, and Whitley (2006).
SGMPCS performs a series of resource-limited tree searches. It is clear that such behaviour is related to the extensive work on randomized restart (Gomes et al., 1998; Horvitz,
Ruan, Gomes, Kautz, Selman, & Chickering, 2001; Kautz, Horvitz, Ruan, Gomes, & Selman, 2002; Gomes, Fernandez, Selman, & Bessiere, 2005; Hulubei & OSullivan, 2006).
Indeed, setting p, the probability of searching from an empty solution, to 1 results in a
randomized restart technique. It has been observed that search effort for chronological
backtracking and a given variable ordering forms a heavy-tailed distribution. Intuitively,
this means that a randomly chosen variable ordering has a non-trivial chance of resulting
in either a small or a large cost to find a solution to a problem instance. If no solution is
found after some threshold amount of effort, it is beneficial to restart search with a different
variable ordering as the new ordering has a non-trivial probability of quickly leading to a
solution.
There are a number of other techniques that make use of randomized or heuristic backtracking (Prestwich, 2002; Jussien & Lhomme, 2002; Dilkina, Duan, & Havens, 2005) to
form a hybrid of local search and tree search and allow an exploration of the search space
that is not as constrained as standard tree search. These approaches differ from SGMPCS
at the fundamental level: they do not use (multiple) existing solutions to guide search.
53

fiBeck

3. Experiment 1: Parameter Settings
The primary purpose of this experiment is to understand the impact of the different parameter settings on the performance of SGMPCS algorithms. We present a fully crossed
experiment to evaluate the impact of varying the parameters of SGMPCS.
3.1 SGMPCS Parameters
Elite Set Size The number of elite solutions that are maintained during the search is
a key parameter controlling the extent to which multiple points in the search space are
exploited by SGMPCS. While there does not seem to have been significant experimentation
with the elite set size in the metaheuristic community, anecdotally, a hybrid tabu search
with an elite set smaller than six performs much worse than larger elite sets on job shop
scheduling problems.2 In this paper, we experiment with elite set sizes of {1, 4, 8, 12, 16, 20}.
The Proportion of Searches from an Empty Solution The p parameter controls
the probability of searching from an empty solution versus searching from one of the elite
solutions. A high p value will result in algorithm behaviour similar to randomized restart
and indeed, p = 1 is a randomized restart algorithm. One reason that the p parameter was
included in SGMPCS was the intuition that it also has an impact on the diversity of the
elite pool: the higher the p value the more diverse the elite pool will be because solutions
unrelated to the current elite solutions are more likely to enter the pool. As we will see
in Experiment 2, this intuition is contradicted by our empirical results. Here, we study
p = {0, 0.25, 0.5, 0.75, 1}.
The Fail Limit Sequence The resource limit sets the number of fails allowed for each
tree search. Rather than have a constant limit and be faced with the problem of tuning the
limit (Gomes et al., 1998), following the work of Kautz, Horvitz, Ruan, Gomes, and Selman
(2002), we adopt a dynamic restart policy where the limit on the number of fails changes
during the problem solving. We look at two simple fail limit sequences (seq):
 Luby - the fail limit sequence is the optimal sequence for satisfaction problems under the condition of no knowledge about the solution distribution (Luby, Sinclair, &
Zuckerman, 1993). The sequence is as follows: 1, 1, 2, 1, 1, 2, 4, 1, 1, 2, 1, 1, 2, 4, 8,
.... That is, the fail limit for the first and second searches is 1 fail, for the third search
is 2 fails, and so on. The sequence is independent of the outcome of the searches and
of whether the search is from an empty solution or guided by an elite solution.
 Polynomial (Poly) - the fail limit is initialized to 32 and reset to 32 whenever a new
best solution is found. Whenever a search fails to find a new best solution, the bound
grows polynomially by adding 32 to the fail limit. The value 32 was chosen to give a
reasonable increase in the fail limit on each iteration. No tuning was done to determine
the value of 32. As with the Luby limit, the Poly fail limit is independent of the choice
to search from an empty solution or from an elite solution.
2. Jean-Paul Watson  personal communication.

54

fiSolution-Guided Multi-Point Constructive Search

Backtrack Method Finally, as noted above, the style of an individual tree search is not
limited to chronological backtracking. Whether search begins from an empty solution or an
elite solution, we have a choice as to how the search should be performed. In particular, our
backtracking (bt) factor is either standard chronological backtracking or limited discrepancy
search (LDS) (Harvey, 1995). In either case, the search is limited by the fail limit as
described above.
3.2 Experimental Details
Our experimental problems are job shop scheduling problem (JSP) instances. An n  m job
shop scheduling problem consists of a set of n jobs, each consisting of a complete ordering
of m activities. Each activity has a duration and a specified resource on which it must
execute. The ordering of activities in a job represents a chain of precedence constraints: an
activity cannot start until the preceding activity in the job has completed. Once an activity
begins execution, it must execute for its complete duration (i.e., no pre-emption is allowed).
There are m unary capacity resources, meaning that each resource can be used by only one
activity at a time. An optimal solution to a JSP is a sequence of the activities on each
resource such that the union of the job sequences and resource sequences is acyclic, and the
makespan (the time between the start of the earliest job and the end of the latest job) is
minimized. The JSP is NP-hard (Garey & Johnson, 1979) and has received extensive study
in both the operations research and the artificial intelligence literature (Jain & Meeran,
1999).
The experimental instances are twenty 20  20 problem instances generated using an
existing generator (Watson, Barbulescu, Whitley, & Howe, 2002). The durations of the
activities are independently drawn with uniform probability from [1, 99]. The machine
routings are generated to create work-flow problems where each job visits the first 10 machines before any of the second 10 machines. Within the two machine sets, the routings are
generated randomly with uniform probability. Work-flow JSPs are used as they have been
shown to be more difficult than JSPs with random machine routings (Watson, 2003).
Each algorithm run has a 20 CPU minute time-out, and each problem instance is solved
10 times independently for a given parameter configuration. All algorithms are implemented
in ILOG Scheduler 6.2 and run on a 2GHz Dual Core AMD Opteron 270 with 2Gb RAM
running Red Hat Enterprise Linux 4.
For this experiment, the dependent variable is mean relative error (MRE) relative to
the best solution known for the problem instance. The MRE is the arithmetic mean of the
relative error over each run of each problem instance:
MRE (a, K, R) =

1 X X c(a, k, r)  c (k)
|R||K|
c (k)

(1)

rR kK

where K is a set of problem instances, R is a set of independent runs with different random
seeds, c(a, k, r) is the lowest cost found by algorithm a on instance k in run r, and c  (k) is
the lowest cost known for k. As these problem instances were generated for this experiment,
the best-known solution was found either by the algorithms tested here or by variations used
in preliminary experiments.3
3. Problem instances and best-known solutions are available from the author.

55

fiBeck

The variable ordering heuristic chooses a pair of activities on the same resource to
sequence. Texture-based heuristics (Beck & Fox, 2000) are used to identify a resource and
time point with maximum contention among the activities and to then choose a pair of
unordered activities, branching on the two possible orders. The heuristic is randomized by
specifying that the hresource, time pointi pair is chosen with uniform probability from the
top 10% most critical pairs. When starting search from an elite solution, the same heuristic
is used to choose a pair of activities to be sequenced, and the ordering found in this solution
is asserted. The standard constraint propagation techniques for scheduling (Nuijten, 1994;
Laborie, 2003; Le Pape, 1994) are used for all algorithms.
3.3 Results
A fully crossed experimental design was implemented, consisting of four factors (|e|, p, seq,
bt) with a total of 120 cells (6  5  2  2). Each cell is the result of 10 runs of each of
20 problem instances, with a time limit on each run of 20 minutes. These results were
generated in about 333 CPU days.
Analysis of variance (ANOVA) on the MRE at 1200 seconds shows that all factors and
all interactions are significant at p  0.005. The ANOVA is shown in Table 1.
Factor(s)
e
p
bt
seq
e:p
e:bt
p:bt
e:seq
p:seq
bt:seq
e:p:bt
e:p:seq
e:bt:seq
p:bt:seq
e:p:bt:seq
Residuals

Df
5
4
1
1
20
5
4
5
4
1
20
20
5
4
20
23880

Sum Sq
0.9995
21.9376
0.8626
0.4924
0.3735
0.1144
0.3023
0.1359
0.2265
0.0036
0.0372
0.0503
0.0041
0.0078
0.0105
3.8821

Mean Sq
0.1999
5.4844
0.8626
0.4924
0.0187
0.0229
0.0756
0.0272
0.0566
0.0036
0.0019
0.0025
0.0008
0.0020
0.0005
0.0002

F value
1229.6015
33736.2277
5306.1350
3028.6761
114.8711
140.7780
464.9442
167.1942
348.3872
22.1468
11.4361
15.4859
5.0191
12.0281
3.2147

Pr(>F)
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
2.540e-06
< 2.2e-16
< 2.2e-16
0.0001342
9.144e-10
1.547e-06

Table 1: Summary of the analysis of variance found using the R statistical package (R
Development Core Team, 2006). All factors and all interactions are significant at
p  0.005.

To attain a more detailed view of the results, a Tukey HSD test (R Development Core
Team, 2006) was performed for each of the factors. The Tukey HSD allows for the comparison of multiple means while controlling for the problems of multiple testing. Table 2 shows
that, at significance level p  0.005:
 Smaller |e| is significantly better than larger |e|.
56

fiSolution-Guided Multi-Point Constructive Search

 p = 0 and p = 0.25 are not significantly different. However, they both result in
significantly lower MRE than p = 0.50. For p > 0.25, a smaller value of p is better.
 The Luby fail limit sequence is significantly better than the Poly sequence.
 Chronological backtracking is significantly better than LDS.
|e|
p
seq
bt

1 < 4 < 8 < 12 < 16 < 20
{0, 0.25} < 0.50 < 0.75 < 1.00
Luby < Poly
chron < lds

Table 2: The results of independent Tukey HSD tests on each factor. Significance level of
the test on each parameter is p  0.005. a < b means that a incurs a lower MRE
than b, and the difference in MRE values is statistically significant. Parenthesis
(i.e., {}) indicate no statistically significant difference in MRE.
Finally, Table 3 presents the five best and five worst parameter settings as determined
by the MRE at 1200 CPU seconds. It is interesting to note that the five worst settings all
have p = 1.00, which corresponds to a pure randomized restart algorithm.
|e|

p
BT
Seq.
MRE
Five Best Parameter Settings
1 0.25 chron Luby 0.03158449
4 0.25 chron Luby 0.03308468
1 0.25 chron Poly 0.03328429
4 0.50 chron Luby 0.03390888
1 0.50 chron Poly 0.03421443
Five Worst Parameter Setting
4 1.00 chron Poly 0.12637893
20 1.00 chron Poly 0.12645527
1 1.00 chron Poly 0.12651117
12 1.00 chron Poly 0.12653876
8 1.00 chron Poly 0.12711269

Table 3: The best and worst parameter combinations in Experiment 1 based on MRE.
A graphical representation of all results from this experiment is impractical. However,
the statistical analysis is based on the performance of each set of parameter values at 1200
seconds, and so the evolution of performance over time is not reflected in these results.
Given the arbitrariness of the 1200 second time limit, it is a valid question to wonder if
the results would change given a different limit. To address this concern and to provide
a graphical sense of the results, we present graphs of the experimental results where one
parameter is varied and the others are held at their best values. For the parameters with
only two values (i.e., seq and bt) we display the results for two different values of |e| as well.
Elite Set Size: |e| Figure 1 shows the results of varying the elite set size with the other
parameter settings as follows: p = 0.25, seq = Luby, bt = chron. The differences between
57

fiBeck

the various levels of |e| and the conclusion that lower |e| results in better performance can
be seen to hold for all time limits less than 1200 seconds. In fact, the superiority of the
algorithms with small |e| is most visible early in the search; after about 200 seconds, the
gaps among the algorithms begin to narrow.
0.2

SGMPCS |e|=20 (p=0.25, seq=luby, bt=chron)
SGMPCS |e|=16 (p=0.25, seq=luby, bt=chron)
SGMPCS |e|=12 (p=0.25, seq=luby, bt=chron)
SGMPCS |e|=8 (p=0.25, seq=luby, bt=chron)
SGMPCS |e|=4 (p=0.25, seq=luby, bt=chron)
SGMPCS |e|=1 (p=0.25, seq=luby, bt=chron)

Mean Relative Error

0.15

0.1

0.05

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 1: The mean relative error for SGMPCS on a set of makespan JSPs as the size of
the elite set is varied.
Given the importance of diversity for elite solution sets within the metaheuristic literature, the performance of the algorithms with an elite size of 1 is somewhat surprising
and seems to contradict some of our original intuitions and motivations for SGMPCS. We
return to this point in Experiment 2.
The Probability of Search from an Empty Solution: p Figure 2 displays the results
of varying p while holding the other parameter values constant at |e| = 1, seq = Luby, and
bt = chron. The most dramatic result is the performance of p = 1.00, which is a pure
randomized restart technique. All the other settings of p result in performance that is more
than an order of magnitude4 better than p = 1.00.
Unlike in the experiments with the |e| values, we do observe a change in the relative
strengths of the different parameter settings with different time limits. While p = 0.25
results in the best performance for all time limits, for low limits p = 0 appears to out-perform
p = 0.50 and p = 0.75. Later, the latter two parameter values result in better performance
than p = 0. Note that this apparent contradiction of the statistical significance findings in
4. The MRE value achieved by p = 1.00 at 1200 seconds is achieved by all other p values at less than 100
seconds.

58

fiSolution-Guided Multi-Point Constructive Search

0.2

SGMPCS p=1.00 (|e|=1, seq=luby, bt=chron)
SGMPCS p=0.00 (|e|=1, seq=luby, bt=chron)
SGMPCS p=0.75 (|e|=1, seq=luby, bt=chron)
SGMPCS p=0.50 (|e|=1, seq=luby, bt=chron)
SGMPCS p=0.25 (|e|=1, seq=luby, bt=chron)

Mean Relative Error

0.15

0.1

0.05

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 2: The mean relative error for varying p-values for SGMPCS on makespan JSPs.
Table 2 can be explained by the fact that there is interaction among the parameters and
p = 0 performs better with other values of the rest of the parameters.
Fail Sequence: seq Plots comparing the two different fail sequences are shown in Figure
3 with p = 0.25, bt = chron, and with two different |e| values, |e| = 1 and |e| = 4. For
run-times less than about 100 CPU seconds, the Poly fail sequence performs better than
the Luby sequence in both conditions. After that threshold, Luby performs better.
Backtracking Method: bt Finally, Figure 4 displays the result of varying the backtracking method under the parameters of p = 0.25, seq = luby, and |e| = 1 or |e| = 4. Using
chronological backtracking for these problems clearly results in superior performance at all
time limits when compared to LDS.
3.4 Summary
This experiment demonstrates that for job shop scheduling with makespan minimization,
the best-performing parameter settings for SGMPCS are: a small elite set, a relatively low
probability of starting search from an empty solution, the Luby fail limit sequence, and
chronological backtracking. In general, these results are robust to changes in the time limit
placed on the runs.
One should be careful in interpreting these results for a number of reasons.
1. As shown by the ANOVA, all the parameters have statistically significant interactions,
and this was directly seen in the performance of p = 0, |e| = 1 in Figure 2.
59

fiBeck

0.2

SGMPCS poly, |e|=4 (p=0.25, bt=chron)
SGMPCS luby, |e|=4 (p=0.25, bt=chron)
SGMPCS poly, |e|=1 (p=0.25, bt=chron)
SGMPCS luby, |e|=1 (p=0.25, bt=chron)

Mean Relative Error

0.15

0.1

0.05

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 3: The mean relative error on makespan JSPs for the two different fail sequences for
|e| = 1 and |e| = 4.

2. While there is a statistically significant effect for all factors, with the exception of the
very poor performance of p = 1.00, the performance of different parameter settings
displayed in the graphs are not wildly varying. While the differences among the
levels of different factors may be statistically significant, they may not be practically
significant. This is an advantage for SGMPCS as it suggests that fine tuning of
parameters is not really necessary: SGMPCS is somewhat robust in the sense that
small changes in parameters result in small changes in performance (again, with the
exception of p = 1.00).
3. The results presented here are based on a single problem, job shop scheduling with
makespan minimization. We comment on the applicability of these results to other
problems in Section 6.2.

4. Experiment 2: The Impact of Elite Set Diversity
SGMPCS was designed with a number of intuitions about the impact of diversity on performance and on the likely effect of different parameter settings on performance. In particular,
we test the following intuitions:
 A higher |e| will tend to result in a higher diversity. This is not a strict relationship
as it is possible that all solutions in e are identical.
60

fiSolution-Guided Multi-Point Constructive Search

0.2

SGMPCS lds, |e|=4 (p=0.25, seq=luby)
SGMPCS lds, |e|=1 (p=0.25, seq=luby)
SGMPCS chron, |e|=4 (p=0.25, seq=luby)
SGMPCS chron, |e|=1 (p=0.25, seq=luby)

Mean Relative Error

0.15

0.1

0.05

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 4: The mean relative error using the Luby fail limit and either chronological backtracking or LDS on the makespan JSPs.

 A higher p value will tend to increase diversity. Since a higher p increases the proportion of searches from an empty solution, it will lead to a wider exploration of the
search space and therefore a more diverse elite set.
 The extent to which exploitation of multiple points in the search space is important
for SGMPCS should be reflected in the performance of sets with different levels of
diversity. That is, if it is important to simultaneously share search effort among a
number of regions in the search space, we would expect that higher levels of diversity
would out-perform lower levels up to some threshold of diminishing returns.
4.1 Measuring Diversity
The disjunctive graph (Pinedo, 2005) is a standard representation of a job shop scheduling
problem where each activity is a node and the precedence constraints relating the activities
in the same job are directed, conjunctive arcs. For each pair of activities in different jobs
but on the same resource, there is a disjunctive arc: an arc that can be directed either
way. In a solution, each disjunctive arc must be oriented in one direction so that the graph
(which now contains only conjunctive arcs) is acyclic.
Following the work of Watson, Beck, Howe, and Whitley (2003), we measure the diversity
of the elite pool by the mean pair-wise disjunctive graph distance. A binary variable is
introduced for each disjunctive constraint where one value represents one orientation of the
arc and the other value, the opposite orientation. A solution to the problem can therefore
61

fiBeck

be represented by an assignment to these disjunctive graph variables. The distance between
a pair of solutions is then simply the Hamming distance between the disjunctive graph
variable assignments. For a given elite set, we take the mean pair-wise distance as a measure
of diversity.
Clearly, this measure is not well-formed for |e| = 1. We assume that the diversity of an
elite set of size 1 is 0.
4.2 Initial Evaluation of Diversity
Our initial evaluation of diversity is simply to measure the diversity for the problem instances and a subset of the parameter values used in Experiment 1. The SGMPCS solver
was instrumented to calculated the pair-wise Hamming distance whenever a new solution
was inserted into the elite set.
Figure 5 displays the diversity of the elite set over time for different elite set sizes. As
expected, a higher elite set size results in a higher diversity. However, it is interesting to
note the stability of the diversity: after the first 100 seconds, the diversity of the set changes
very little, while the quality of solutions (see Figure 1) continues to improve.
1200

Mean Pair-wise Hamming Distance

1000

800

SGMPCS |e| = 20 (p=0.25, seq=luby, bt=chron)
SGMPCS |e| = 16 (p=0.25, seq=luby, bt=chron)
SGMPCS |e| = 12 (p=0.25, seq=luby, bt=chron)
SGMPCS |e| = 8 (p=0.25, seq=luby, bt=chron)
SGMPCS |e| = 4 (p=0.25, seq=luby, bt=chron)

600

400

200

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 5: The diversity measured by mean pair-wise Hamming distance among the solutions
in elite set for different elite set sizes.
Figure 6 shows the diversity with changing p values. Contrary to our expectations,
higher p values exhibit lower diversity. Further analysis shows that the primary cause of
this pattern is the way in which elite solutions are replaced. When search starts from
an elite solution, an improved solution replaces the starting solution. Because the fail
limit is relatively low, the starting solution is, with very high probability, also the closest
62

fiSolution-Guided Multi-Point Constructive Search

elite solution to the improved solution. Therefore, replacing the starting elite solution has
a relatively small impact on the overall diversity. In contrast, when search starts from
an empty solution, the worst elite solution is replaced by an improved solution. As we
demonstrate below, this difference in replacement policy results in a significantly lower elite
pool diversity when more searches start from an empty solution: diversity decreases with
increasing p.
1200

Mean Pair-wise Hamming Distance

1000

800

600
SGMPCS p = 0.00 (|e|=4, seq=luby, bt=chron)
SGMPCS p = 0.25 (|e|=4, seq=luby, bt=chron)
SGMPCS p = 0.50 (|e|=4, seq=luby, bt=chron)
SGMPCS p = 0.75 (|e|=4, seq=luby, bt=chron)
SGMPCS p = 1.00 (|e|=4, seq=luby, bt=chron)

400

200

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 6: The diversity measured by mean pair-wise Hamming distance among the solutions
in elite set for values of p.

4.3 Manipulating Diversity
Motivated by our interpretation of the results in Figure 6, in this section we experiment with
the manipulation of the diversity by changing the elite solution replacement rule. Three
levels of diversity are defined as follows:
 Low Diversity: Regardless of whether search starts from an elite solution or an empty
solution, an improved solution replaces the worst elite one. No distance-based criteria
is used. In the initialization phase, we follow the same approach as used above: 50
elite solutions are independently generated without constraining the makespan, and
the |e| best solutions inserted into the elite set.
 Medium Diversity: The standard elite set replacement rules used in Experiment 1 and
defined in Section 2 are used.
63

fiBeck

 High Diversity: When search starts from an empty solution, the closest elite solution
is replaced if an improving solution is found. When search starts from an elite solution, the starting solution replaced. As noted above, this latter rule results in the
replacement of the closest solution with high probability. Therefore, these two rules
are almost always equivalent to replacing the closest solution. During the initialization
phase, |e| solutions are generated and inserted into the elite pool. Then, an additional
50  |e| solutions are generated and, if one of these solutions is better than the worst
elite solution, the new solution is inserted into the elite set, replacing the closest elite
solution.
To verify that our manipulations do indeed affect the diversity of the elite set as expected,
we conduct an initial experiment over a subset of the parameter space. Using the problem
instances from Experiment 1 and the same hardware and software configurations, we solved
each problem instance 10 times under each diversity condition while varying |e| and p.
Rather than doing a fully crossed experiment, we set |e| = 4 and varied p from 0 to 1, and
set p = 0.25 and varied |e| from 4 to 20.
Figures 7 and 8 demonstrate that the above manipulations affect the diversity of the
elite set as expected. They show the different diversity levels with only two |e| values and
two p values as displaying all the data was impractical. It is interesting to note that for the
high and low diversity conditions, the effect on diversity of the other parameters disappears:
there is little variation in the diversity when |e| and p are varied under those two diversity
conditions.
1200

Mean Pair-wise Hamming Distance

1000

800

High: |e| = 8 (p=0.25, seq=luby, bt=chron)
High: |e| = 4 (p=0.25, seq=luby, bt=chron)
Medium: |e| = 8 (p=0.25, seq=luby, bt=chron)
Medium: |e| = 4 (p=0.25, seq=luby, bt=chron)
Low: |e| = 8 (p=0.25, seq=luby, bt=chron)
Low: |e| = 4 (p=0.25, seq=luby, bt=chron)

600

400

200

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 7: The diversity measured by mean pair-wise Hamming distance among the solutions
in the elite set for different diversity levels for |e| = 4 and |e| = 8.

64

fiSolution-Guided Multi-Point Constructive Search

1200

Mean Pair-wise Hamming Distance

1000

800
High: p = 0.00 (|e|=4, seq=luby, bt=chron)
High: p = 0.25 (|e|=4, seq=luby, bt=chron)
Medium: p = 0.00 (|e|=4, seq=luby, bt=chron)
Medium: p = 0.25 (|e|=4, seq=luby, bt=chron)
Low: p = 0.00 (|e|=4, seq=luby, bt=chron)
Low: p = 0.25 (|e|=4, seq=luby, bt=chron)

600

400

200

0

0

200

400

600
Time (secs)

800

1000

1200

Figure 8: The diversity measured by mean pair-wise Hamming distance among the solutions
in the elite set for different diversity levels for p = 0 and p = 0.25.

4.4 Experimental Details
Having verified that we do indeed have three different diversity settings, we can now test
the impact of different diversity levels on the performance of SGMPCS. We perform a fully
crossed experiment with three independent variables: |e| which, as above, takes the values
{1, 4, 8, 12, 16, 20}; p which, as above, takes the values {0, 0.25, 0.5, 0.75, 1}; and diversity
(div) taking the values low, medium, and high corresponding to the manipulations described
above. In all conditions, we use chronological backtracking and the Luby fail limit sequence.
The other experimental details including the problem instances, hardware and software,
the 1200 CPU second time limit, heuristics and propagators, and our evaluation criteria
(MRE) are the same as in Experiment 1 (see Section 3.2).
4.5 Results
The fully crossed experimental design results in 90 cells (6  5  3). Each cell is the result
of 10 runs of each of the 20 problem instances with a 20 minute time limit. These results
were generated in about 250 CPU days.
The summary of the analysis of variance is shown in Table 4. These results demonstrate
that all factors and all interactions are significant at p  0.005. A Tukey HSD test (R
Development Core Team, 2006) with significance level p  0.005 was done on each of the
factors, and the results are summarized in Table 5. The Tukey HSD results indicate that:
65

fiBeck

 As with Experiment 1, lower |e| is better, though in this case there is no significant
difference between |e| = 1 and |e| = 4.
 p = 0 is significantly worse than p = 0.50 which in turn is significantly worse than
p = 0.25. Recall that in Experiment 1, p = 0 was not significantly different from
p = 0.25.
 Lower diversity is better than medium which in turn is better than high diversity.
Factor(s)
e
p
div
e:p
e:div
p:div
e:p:div
Residuals

Df
5
4
2
20
10
8
40
17910

Sum Sq
0.0709
21.4690
0.0706
0.0584
0.0234
0.0563
0.0186
3.1008

Mean Sq
0.0142
5.3673
0.0353
0.0029
0.0023
0.0070
0.0005
0.0002

F value
81.9130
31000.5636
204.0232
16.8679
13.4938
40.6166
2.6925

Pr(>F)
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
< 2.2e-16
4.298e-08

Table 4: Summary of the analysis of variance found using the R statistical package (R
Development Core Team, 2006). All factors and all interactions are significant at
p  0.005.

|e|
p
div

{1, 4} < 8 < {12, 16} < 20
0.25 < 0.50 < 0 < 0.75 < 1.00
low < medium < high

Table 5: The results of independent Tukey tests on each factor in the diversity experiment.
Significance level of the test on each parameter is p  0.005.
Finally, Table 6 presents the parameter values that result in the five lowest and five
highest MRE results. Note that the previous best set of parameter values (|e| = 1, p = 0.25,
div = med) now incurs a slightly worse MRE than |e| = 4, p = 0.25, div = low.
4.6 Summary
Our experiment with diversity has addressed a number of our intuitions:
 As expected, a larger elite set results in a higher elite set diversity.
 Contrary to our expectations, a higher probability of searching from an empty solution
decreases diversity. We were able to show that this impact was not directly due to
the p value but rather to the different elite set replacement rules.
 Finally, and most importantly, it appears that the diversity of the elite set is negatively
correlated with performance: the lower the diversity, the higher the performance.
66

fiSolution-Guided Multi-Point Constructive Search

|e|
p
Div
MRE
Five Best Parameter Settings
4 0.25 low
0.03085739
1 0.25 med 0.03158449
8 0.25 low
0.03224803
16 0.25 low
0.03231168
12 0.25 low
0.03233298
Five Worst Parameter Setting
20 1.00 med 0.12482888
8 1.00 low
0.12484571
1 1.00 low
0.12487085
12 1.00 med 0.12488335
16 1.00 low
0.12489075

Table 6: Best and worst parameters for the diversity experiments.
This result calls into question the extent to which SGMPCS performance is based on
exploiting multiple points in the search space. If such exploitation were important
for performance, we would expect higher diversity to out-perform lower diversity. We
return to this question in Section 6.

5. Experiment 3: Benchmark Comparison with Other Techniques
Our first two experiments concentrated on providing basic data on the performance of the
different parameter settings of SGMPCS and an initial inquiry into the reasons underlying
SGMPCS performance. In this experiment, we turn to comparisons of SGMPCS with
existing heuristic search techniques.
5.1 Experimental Details
We use three sets of well-known JSP benchmark problem instances (Taillard, 1993). Each
set contains 10 instances, and the different sets have problems of different size: 20  15,
20  20, and 30  15. The problems are numbered from 11 though 40. 5 These instances
were not used during the development of SGMPCS.
Five algorithms are tested:
 Standard chronological backtracking (Chron): A non-randomized version of the same
texture-based heuristic employed above is used together with the same global constraint propagators. As the heuristic is not randomized, one run is done for each
problem instance.
 Limited Discrepancy Search (LDS): This is an identical algorithm to Chron except
the backtracking is LDS.
5. See http://ina2.eivd.ch/collaborateurs/etd/problemes.dir/ordonnancement.dir/ordonnancement.html
for the benchmark instances. The best-known upper and lower bounds are from the latest summary
file on the same website, dated 23/11/05.

67

fiBeck

 Randomized Restart (Restart): This is a randomized restart algorithm using the
same randomized texture-based heuristic and global constraint propagators used in
Experiment 1 and 2. The backtracking between restarts is chronological and the fail
limit used is the Luby limit. Each problem instance is solved 10 times.
 Solution-Guided Multi-Point Constructive Search (SGMPCS): We take the best parameters from Experiments 1 and 2: |e| = 4, p = 0.25, seq = Luby, bt = chron, and
div = low. With these parameter settings, the sole difference between SGMPCS and
Restart is the use of the elite set and the fact that some searches are guided by an elite
solution. In particular, they use the same heuristics, propagators, fail limit sequence,
and type of backtracking. Each problem instance is solved 10 times.
 Iterated Simple Tabu Search (i-STS): The i-STS algorithm is a sophisticated multiphase tabu search built to model the state-of-the-art i-TSAB (Nowicki & Smutnicki,
2005) but with the goal of simplifying it in order to study how its various components
contribute to the overall performance (Watson et al., 2006). On the Taillard benchmarks, i-STS only slightly under-performs i-TSAB in terms of solution quality given
an equal number of iterations.6 We use the parameters recommended7 for the Taillard
instances: |E| = 8, Xa = 40000, Xb = 7000, pi = pd = 0.5. For a full definition of
these parameters, see the work of Watson et al. (2006).
The time limit for each run is 3600 CPU seconds. The other experimental details,
including hardware and software for the first four algorithms, and the evaluation criteria
are the same as in Experiment 1 (see Section 3.2). The i-STS algorithm is Watson et al.s
C++ implementation run on the same hardware as the other algorithms, meaning that
direct run-time comparison is meaningful.
For all the constructive search-based approaches (i.e., all algorithms tested here except
i-STS), the Global Bound policy is followed (see Section 2.2): whenever a new best solution
is found, the global upper bound on the cost function is modified to be one less than this
new best cost. In particular, this means that Restart benefits from the back-propagation of
the cost constraint in exactly same way that SGMPCS does.
5.2 Results
The mean and best makespan found for each problem set are shown in Tables 7 through 9.
Table 10 shows the performance in terms of finding and proving the optimal makespan for
those problems for which the optimal solution is known.
5.2.1 Comparing Constructive Search Algorithms
On the 20  15 problems (Table 7), SGMPCS dominates the other constructive algorithms,
finding the lowest makespan (as judged by the mean makespan), for all but one instance
(instance 14). In particular, on all problem instances the mean SGMPCS solution is better
than the best solution found by Restart. In terms of mean relative error, SGMPCS outperforms each of the other constructive algorithms by a factor of about 3 to 8.
6. As with the previous experiments, here we use a CPU time limit. It is estimated that i-STS is about 5
to 7 times slower than i-TSAB.
7. Jean-Paul Watson, personal communication.

68

fiSolution-Guided Multi-Point Constructive Search

Prob.
11
12
13
14
15
16
17
18
19
20
MRE

LB/UB
1323/1359
1351/1367
1282/1342
1345
1304/1339
1302/1360
1462
1369/1396
1297/1335
1318/1348
(vs. UB)

Chron
1444
1587
1401
1496
1436
1496
1597
1663
1457
1387
0.0956

LDS
1410
1411
1401
1345
1403
1424
1485
1464
1388
1390
0.0343

Restart
mean
best
1412.4
1408
1404.7
1402
1388.6
1385
1378.5
1370
1432.2
1427
1416.2
1408
1509.0
1507
1459.9
1456
1393.5
1386
1388.1
1378
0.0389 0.0348

SGMPCS
mean
best
1387.8
1365
1377.2
1367
1352.9
1343
1345.2 1345
1375.9
1364
1373.3
1365
1472.7
1462
1423.2
1400
1349.9
1335
1361.5
1356
0.0122 0.0036

i-STS
mean
best
1366.6
1365
1376.3
1375
1349.7
1347
1345
1345
1350.2
1342
1362.3
1362
1467.8
1464
1407.1
1404
1339.2
1335
1355.3
1350
0.0049 0.0026

Table 7: Results for Taillards 20  15 instances. Bold entries indicate the best performance
across the five algorithms on each instance. For Restart, SGMPCS, and i-STS,
we use the mean makespan as the performance measure. We also include the best
makespan found by the algorithms that solve an instance multiple times. The
 indicates that the optimal makespan was found and proved for the problem
instance. The final row shows the mean relative error (relative to the best-known
upper bound) for each algorithm.

It is interesting to note the similar performance of LDS and Restart. We observe that
when using a dynamic variable ordering, LDS performs partial restarts when jumping to
the top of the tree to introduce a discrepancy. This suggests that some of the performance
of LDS with dynamic variable orderings may be due to an exploitation of the heavy-tails
phenomenon. The similar results here and on the other JSP instances in this section support
this idea. To our knowledge this relationship has not been commented on before.

Prob.
21
22
23
24
25
26
27
28
29
30
MRE

LB/UB
1539/1644
1511/1600
1472/1557
1602/1646
1504/1595
1539/1645
1616/1680
1591/1603
1514/1625
1473/1584
(vs. UB)

Chron
1809
1689
1657
1810
1685
1827
1827
1778
1718
1666
0.0793

LDS
1699
1659
1620
1676
1669
1723
1755
1645
1678
1659
0.0373

Restart
mean
best
1694.5
1686
1654.0
1649
1614.2
1602
1697.5
1694
1673.1
1664
1706.9
1701
1754.6
1750
1663.7
1656
1665.5
1660
1646.5
1641
0.0366 0.0324

SGMPCS
mean
best
1665.7
1649
1632.1
1621
1571.4
1561
1663.9
1652
1619.6
1608
1669.4
1656
1715.6
1706
1628.1
1619
1642.2
1626
1606.9
1598
0.0146 0.0072

i-STS
mean
best
1648.0
1647
1614.1
1600
1560.2
1557
1653.2
1647
1599.3
1595
1653.3
1651
1690.0
1687
1617.4
1614
1628.0
1627
1587.2
1584
0.0044 0.0019

Table 8: Results for Taillards 20  20 instances. See the caption of Table 7.
Table 8 displays the results for the 20  20 problems. Again, SGMPCS dominates
the other constructive algorithms, finding a mean makespan that is better than the best
makespan found by any of the other constructive techniques. SGMPCS was unable to find
69

fiBeck

solutions as good as the best-known upper bound for any of these instances. In terms of
MRE, SGMPCS out-performs the other algorithms by a factor of 3 to 5.

Prob.
31
32
33
34
35
36
37
38
39
40
MRE

LB/UB
1764
1774/1795
1778/1791
1828/1829
2007
1819
1771
1673
1795
1631/1674
(vs. UB)

Chron
2118
2163
2138
2096
2110
2411
2018
2005
2118
2106
0.190

LDS
1912
1975
1987
1989
2007
1964
1947
1853
1904
1870
0.0832

Restart
mean
best
1896.8
1888
1983.1
1978
2021.6
2015
1968.4
1962
2007.0 2007
1957.1
1949
1940.3
1935
1822.0
1817
1896.1
1881
1859.4
1855
0.0813 0.0776

SGMPCS
mean
best
1774.0
1766
1828.3
1804
1840.9
1814
1863.9
1833
2007.0 2007
1832.7 1819
1810.6
1787
1701.7
1691
1803.5 1795
1714.7
1690
0.0147 0.0051

i-STS
mean
best
1764.0
1764
1813.4
1804
1804.2
1799
1831.9
1831
2007.0
2007
1819.7
1819
1791.1
1778
1675.7
1673
1799.3
1797
1689.4
1686
0.0044 0.0022

Table 9: Results for Taillards 30  15 instances. See the caption of Table 7.
Table 9 displays the results on the largest problem instances (30  15). On all instances
but one, the mean solution found by SGMPCS is better than the best solution from each
of the other constructive algorithms. For instance 35, SGMPCS equals the performance of
LDS and Restart in finding (and, in some cases, proving) the optimal solution. Overall,
SGMPCS is a factor of 5 to 13 better in terms of MRE.
Prob.
14
17
31
35
36
37
38
39

Opt.
1345
1462
1764
2007
1819
1771
1673
1795

Chron
0(0)
0(0)
0(0)
0(0)
0(0)
0(0)
0(0)
0(0)

LDS
10(10)
0(0)
0(0)
10(0)
0(0)
0(0)
0(0)
0(0)

Restart
0(0)
0(0)
0(0)
10(2)
0(0)
0(0)
0(0)
0(0)

SGMPCS
9(9)
1(0)
0(0)
10(4)
1(1)
0(0)
0(0)
3(3)

i-STS
10(0)
0(0)
10(10)
10(0)
8(0)
0(0)
1(1)
0(0)

Table 10: Results for the Taillard instances for which the optimal solution is known. The
first two columns are the problem index and optimal makespan respectively. The
rest of the columns are the number of runs for which each algorithm found an
optimal solution and, in parenthesis, the number of times that it proved optimality. Recall that both Chron and LDS are run once per instance because they are
not stochastic. However, to provide a fair basis of comparison, we present their
results assuming they produced identical results in each of ten runs per instance.
While i-STS is not a complete algorithm, there are some structural characteristics of a solution that imply optimality (Nowicki & Smutnicki, 1996). When a
solution with such a characteristic is found, i-STS is able to prove optimality as
shown in two instances: tai31 and tai38.

70

fiSolution-Guided Multi-Point Constructive Search

Finally, Table 10 presents the number of runs for which each algorithm was able to find
and prove the optimal solutions for those problem instances with known optimal. SGMPCS
finds the optimal solution at least once for five instances and proves the optimality at least
once for four of those instances. Chron is unable to find or prove optimality for any instances,
while Restart only does so for one instance, and LDS is able to find an optimal solution for
two instances and prove it for one.
5.2.2 SGMPCS vs. i-STS
On almost all instances in Tables 8 and 9 i-STS performs substantially better than SGMPCS. In many cases, the mean solution found by i-STS is better than the best found by
SGMPCS. However, on seven of the ten smallest instances (Table 7), the best solution
found by SGMPCS is as good or better than that found by i-STS, and SGMPCS is strictly
better on five instances. As with the larger problems, however, the mean makespan found
by i-STS is better than that found by SGMPCS on all instances.
Recall that each algorithm was run for 3600 CPU seconds. While we do not include
graphs of the run-time distributions, we have observed that the performance gap in terms
of MRE between SGMPCS and i-STS at 3600 seconds is present at all time points from 60
seconds. In other words, i-STS substantially out-performs SGMPCS in the first 60 seconds
and thereafter both algorithms find better solutions at about the same rate.
Table 10 shows that the one area that SGMPCS is clearly superior to i-STS is in
proving the optimality of solutions. While i-STS is not a complete algorithm, it can identify
solutions with a particular structure as optimal (Nowicki & Smutnicki, 1996). SGMPCS is
able to find and prove optimality within the time limit on four instances in at least one run
while i-STS can only do so for two instances.
5.3 Summary
Of the 30 problem instances used in this experiment, the mean solution found by SGMPCS
was better than the best solution found by any of the other constructive techniques in 28
instances. Of the remaining instances, SGMPCS performs as well as LDS and Restart
for instance 35 and slightly worse than LDS on instance 14. Overall, in terms of mean
relative error, SGMPCS is between 3 and 13 times better than the other constructive search
algorithms on the different problem sets.
SGMPCS does not perform as well as i-STS in terms of mean makespan; however, on
the smaller problems the best solution it is able to find is better than that of i-STS on five
instances.

6. Discussion and Future Work
This paper demonstrates that Solution-Guided Multi-Point Constructive Search can significantly out-perform existing constructive search techniques in solving hard combinatorial
search problems but trails behind the state-of-the-art in metaheuristic search. In this section, we present some preliminary ideas regarding the reasons for the observed performance,
a discussion of the generality of SGMPCS, and some directions for extensions of SGMPCS.
71

fiBeck

6.1 Why Does SGMPCS Work?
To the extent that SGMPCS out-performs existing constructive search approaches for solving hard combinatorial search problems, the most interesting question arising from the
above experiments is understanding the reasons for this strong performance. We speculate
that there are three, non-mutually exclusive, candidates: the exploitation of heavy-tails, the
impact of revisiting previous high-quality solutions, and the use of multiple elite solutions.
6.1.1 Exploiting Heavy-Tails
SGMPCS is a restart-based algorithm. Even with p = 0, search periodically restarts, albeit
with a value ordering based on an elite solution. We believe that it is likely, therefore, that
SGMPCS exploits heavy-tailed distributions in much the same way as randomized restart
(Gomes et al., 2005; Gomes & Shmoys, 2002).
One way to test this idea is to reproduce Gomes et al.s original experiment for SGMPCS
as follows: for a random variable ordering, solve a problem instance to optimality starting
from a given sub-optimal solution, s, and record the search effort involved; repeat for k different random variable orderings for a large k; and finally observe the frequency distribution
of search effort. The whole experiment can then be repeated for different starting solutions.
If the resulting distributions exhibit heavy-tailed behaviour, the reasons that randomized
restart is able to take advantage of heavy-tailed distributions may be shared by SGMPCS.
We are currently pursuing such an experiment.
6.1.2 Revisiting Solutions
While we believe it likely that the experiment suggested in Section 6.1.1 will demonstrate
that SGMPCS takes advantage of heavy-tailed distributions, the significant performance
advantage of SGMPCS over Restart in Experiment 3 as well as the very poor performance
of the p = 1 parameter setting in Experiments 1 and 2, lead us to expect that there are
additional factors needed to account for the performance of SGMPCS.
We believe that a leading candidate for one of these additional factors is the impact
of revisiting high-quality solutions using a different variable ordering. Each time an elite
solution is revisited with a different variable ordering, a different search tree is created. A
resource-limited chronological search will only visit nodes deep in the tree before the resource
limit is reached. However, a different variable ordering results in a different set of nodes that
are deep in the tree and that are, therefore, within reach of the search. 8 The strong results of
SGMPCS with |e| = 1 may be an indication that the mechanism responsible for the strong
performance is the sampling of solutions close to an elite solution in different search trees.
Our primary direction for future research is to formalize the meaning of close within a
search tree to provide a firm empirical foundation on which to investigate the impact of
revisiting solutions. We hope to adapt the significant work on fitness-distance correlation
(Hoos & Stuzle, 2005) in the local search literature to constructive search.

8. Similar reasoning applies to the use of LDS.

72

fiSolution-Guided Multi-Point Constructive Search

6.1.3 Exploiting Multiple Points in the Search Space
The use of multiple solutions and, more specifically, the balance between intensification
and diversification is viewed as very important in the metaheuristic literature (Rochat
& Taillard, 1995). Intensification suggests searching in the region of good solutions while
diversification suggests searching in areas that have not been searched before. Furthermore,
one of the important aspects of the metaheuristics based on elite solutions is how the
diversity of the elite set is maintained (Watson, 2005).
However, the experiments presented here suggest that increased diversity is not an
important factor for performance of SGMPCS. The best performance was achieved with
very small elite set sizes and even, in Experiment 1, an elite set size of 1. Based on such
results, the original motivations for SGMPCS are, to say the least, suspect.
Our results may be due to idiosyncrasies of the makespan JSP problem. While experiments on some other problems (see below) have not directly manipulated diversity, the
results have indicated better relative performance for larger elite set sizes than was observed
here. This may be an indication that on other problems we will see a positive contribution
of maintaining multiple viewpoints.
On a speculative note, a closer look at Figure 1 may show that diversity does play a
role in search performance. That figure shows that the greatest differences in performance
from different elite set sizes comes early in the search, where it is relatively easy to find an
improving solution. Later in search, the performance difference narrows, though does not
close completely within the time limit. One interpretation of this pattern is that, early in
the search, when it is relatively easy to improve upon existing elite solutions, a large elite
pool distracts the search by guiding it with an elite solution that is significantly worse
than the best elite solution. The narrowing of the performance gap may be simply due to
the fact that, with better solutions, it is harder to improve upon them and so regardless
of the size of the elite set, the rate of improvement will decrease. Since the algorithms
with lower |e| have better solutions, their rate slows earlier. An alternative explanation
is that maintaining multiple elite solutions has a positive influence only after the initial
easy phase of search. When better solutions are harder to find, having a diverse elite set
may help the search as the probability that at least one of the elite solutions has a better
solution in its vicinity rises with the elite set size. 9 Further experimentation is required to
investigate these intuitions.
6.2 Generality
SGMPCS is a general technique for conducting constructive search: nothing in the SGMPCS
framework is specific to scheduling or constraint programming. However, in this paper only
one type of problem was used to evaluate SGMPCS and therefore the question of its practical
utility and generality should be addressed.
Existing work shows that SGMPCS can be effectively applied to other optimization and
satisfaction problems such as quasigroup-with-holes completion (Beck, 2005b; Heckman &
Beck, 2006), job shop scheduling with the objective to minimize weighted tardiness (Beck,
2006), and multi-dimensional knapsack optimization (Heckman & Beck, 2007). In addi9. If this explanation is accurate, an adaptive strategy with |e| growing during the search might be worth
investigating.

73

fiBeck

tion, recent work by Sellmann and Ansotegui (2006) demonstrates good performance of
a closely related technique on diagonally ordered magic squares and some SAT instances.
However, SGMPCS performs worse than randomized restart (though better than chronological backtracking) on magic square instances, and both randomized restart and SGMPCS
perform much worse than chronological backtracking on a satisfaction version of the multidimensional knapsack problem (Heckman & Beck, 2006).
The application of SGMPCS to such a variety of problems demonstrates that it is indeed
a general technique whose impact can be applied beyond job shop scheduling. At the same
time, the negative results on some problems point to our lack of understanding as to the
mechanisms behind SGMPCS performance and motivates our future work.
6.3 Extending SGMPCS
While the immediate focus of our future work is on understanding the reasons for its performance, there are a number of ways in which the framework can be extended.
First, as implied by our speculations regarding the impact of diversity in Section 6.1.3,
dynamic parameter learning (Horvitz et al., 2001) would appear very useful to the SGMPCS
framework. For example, one could imagine adapting p during the search depending on the
relative success of searching from an empty solution versus searching from an elite solution.
Second, given that the metaheuristics community has been working with elite solutions
for a number of years, there are a number of techniques which may fruitfully extend SGMPCS. For example, in path relinking (Glover, Laguna, & Marti, 2004) a pair of elite solutions
is taken as end-points of a local search trajectory. Path relinking has an elegant counterpart
in SGMPCS: two elite solutions are chosen, the variable assignments they have in common
are fixed, defining a sub-space of the variable assignments in which the two solutions differ.
Unlike in path relinking for local search, in constructive search one can perform a complete search of this sub-space and then post a no-good removing that sub-space from future
consideration. Some preliminary experiments with such an approach appear promising.
Third, clause learning techniques, which originated as conflict learning in constraint
programming (Prosser, 1993), are widely used with restart in state-of-the-art satisfiability
solvers (Huang, 2007). It seems natural to investigate combining conflict learning and
solution-guidance. These techniques may have an interesting relationship as the former tries
to learn the mistakes that led to a dead-end while the latter attempts to heuristically
identify the correct decisions that were made.
Finally, work on loosely coupled hybrid search techniques that share single solutions
(Carchrae & Beck, 2005) is easily generalizable to share a set of solutions. To date, rather
than being able to exploit a full solution shared by some other technique, constructive search
is only able to use the bound on the cost function. Therefore, the revisiting of solutions
provides a way to exploit the much richer information (i.e., full solutions) that is available
in a hybrid search technique.

7. Conclusion
This paper presents the first fully crossed study of Solution-Guided Multi-Point Constructive Search. Using a set of job shop scheduling problems, we varied the SGMPCS parameter
settings to control the size of the elite set, the probability of searching from an empty so74

fiSolution-Guided Multi-Point Constructive Search

lution, the fail sequence, the form of backtracking, and the diversity level of the elite set.
Experiments indicated that low elite set sizes, low probability of searching from an empty
solution, the Luby fail sequence, chronological backtracking, and low diversity lead to the
best performance. We then compared the best SGMPCS parameters found to existing constructive search techniques and to a state-of-the-art tabu search algorithm on a well-known
set of benchmark problems. The results demonstrated that SGMPCS significantly outperforms chronological backtracking, limited discrepancy search, and randomized restart while
being out-performed by the tabu search algorithm.
The primary contribution of this paper is the introduction of a new search framework
and the demonstration that it can significantly out-perform existing constructive search
techniques. Secondary contributions include the demonstration that the impact of elite set
diversity on performance is the opposite of what was expected (i.e., low diversity leads to
higher performance) and the identification of research directions into the reasons underlying
the performance of SGMPCS by focusing on the quantification of the effects of heavy-tails,
of the impact of revisiting solutions with different variable orderings, and of the exploitation
of multiple points in the search space.

Acknowledgments
This research was supported in part by the Natural Sciences and Engineering Research
Council and ILOG, S.A. Thanks to Jean-Paul Watson, Daria Terekhov, Tom Carchrae,
Ivan Heckman, and Lei Duan for comments on early versions of the paper. A preliminary
version of parts of this work has been previously published (Beck, 2006).

References
Beck, J. C. (2005a). Multi-point constructive search. In Proceedings of the Eleventh International Conference on Principles and Practice of Constraint Programming (CP05),
pp. 737741.
Beck, J. C. (2005b). Multi-point constructive search: Extended remix. In Proceedings of the
CP2005 Workshop on Local Search Techniques for Constraint Satisfaction, pp. 1731.
Beck, J. C. (2006). An empirical study of multi-point constructive search for constraintbased scheduling. In Proceedings of the Sixteenth International on Automated Planning
and Scheduling (ICAPS06), pp. 274283.
Beck, J. C., & Fox, M. S. (2000). Dynamic problem structure analysis as a basis for
constraint-directed scheduling heuristics. Artificial Intelligence, 117 (1), 3181.
Carchrae, T., & Beck, J. C. (2005). Applying machine learning to low knowledge control of
optimization algorithms. Computational Intelligence, 21 (4), 372387.
Dilkina, B., Duan, L., & Havens, W. (2005). Extending systematic local search for job
shop scheduling problems. In Proceedings of Eleventh International Conference on
Principles and Practice of Constraint Programming (CP05), pp. 762766.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman and Company, New York.
75

fiBeck

Glover, F., Laguna, M., & Marti, R. (2004). Scatter search and path relinking: advances
and applications. In Onwubolu, G., & Babu, B. (Eds.), New Optimization Techniques
in Engineering. Springer.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through
randomization. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), pp. 431437.
Gomes, C. P., Fernandez, C., Selman, B., & Bessiere, C. (2005). Statistical regimes across
constrainedness regions. Constraints, 10 (4), 317337.
Gomes, C., & Shmoys, D. (2002). Completing quasigroups or latin squares: A structured
graph coloring problem. In Proceedings of the Computational Symposium on Graph
Coloring and Generalizations.
Harvey, W. D. (1995). Nonsystematic backtracking search. Ph.D. thesis, Department of
Computer Science, Stanford University.
Heckman, I., & Beck, J. C. (2006). An empirical study of multi-point constructive search
for constraint satisfaction. In Proceedings of the Third International Workshop on
Local Search Techniques in Constraint Satisfaction.
Heckman, I., & Beck, J. C. (2007). An empirical study of multi-point constructive search
for constraint satisfaction. Submitted to Constraints.
Hoos, H., & Stuzle, T. (2005). Stochastic Local Search: Foundations and Applications.
Morgan Kaufmann.
Horvitz, E., Ruan, Y., Gomes, C., Kautz, H., Selman, B., & Chickering, M. (2001). A
bayesian approach to tacking hard computational problems. In Proceedings of the
Seventeenth Conference on Uncertainty and Artificial Intelligence (UAI-2001), pp.
235244.
Huang, J. (2007). The effect of restarts on the efficiency of clause learning. In Proceedings
of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI07),
pp. 23182323.
Hulubei, T., & OSullivan, B. (2006). The impact of search heuristics on heavy-tailed
behaviour. Constraints, 11 (23), 159178.
Jain, A. S., & Meeran, S. (1999). Deterministic job-shop scheduling: Past, present and
future. European Journal of Operational Research, 113 (2), 390434.
Jussien, N., & Lhomme, O. (2002). Local search with constraint propagation and conflictbased heuristics. Artificial Intelligence, 139, 2145.
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C., & Selman, B. (2002). Dynamic restart policies.
In Proceedings of the Eighteenth National Conference on Artifiical Intelligence (AAAI02), pp. 674681.
Laborie, P. (2003). Algorithms for propagating resource constraints in AI planning and
scheduling: Existing approaches and new results. Artificial Intelligence, 143, 151188.
Le Pape, C. (1994). Implementation of resource constraints in ILOG Schedule: A library
for the development of constraint-based scheduling systems. Intelligent Systems Engineering, 3 (2), 5566.
76

fiSolution-Guided Multi-Point Constructive Search

Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Nowicki, E., & Smutnicki, C. (1996). A fast taboo search algorithm for the job shop problem.
Management Science, 42 (6), 797813.
Nowicki, E., & Smutnicki, C. (2005). An advanced tabu algorithm for the job shop problem.
Journal of Scheduling, 8, 145159.
Nuijten, W. P. M. (1994). Time and resource constrained scheduling: a constraint satisfaction approach. Ph.D. thesis, Department of Mathematics and Computing Science,
Eindhoven University of Technology.
Pinedo, M. (2005). Planning and Scheduling in Manufacturing and Services. Springer.
Prestwich, S. (2002). Combining the scalability of local search with the pruning techniques
of systematic search. Annals of Operations Research, 115, 5172.
Prosser, P. (1993). Hybrid algorithms for the constraint satisfaction problem. Computational
Intelligence, 9 (3), 268299.
R Development Core Team (2006). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.
Rochat, Y., & Taillard, E. D. (1995). Probabilistic diversification and intensification in local
search for vehicle routing. Journal of Heuristics, 1, 147167.
Sellmann, M., & Ansotegui, C. (2006). Disco-novo-gogo: Integrating local search and complete saerch with restarts. In Proceedings of the Twenty-First National Conference on
Artificial Intelligence (AAAI06), pp. 10511056.
Taillard, E. D. (1993). Benchmarks for basic scheduling problems. European Journal of
Operational Research, 64, 278285.
Watson, J.-P. (2003). Empirical Modeling and Analysis of Local Search Algorithms for the
Job-Shop Scheduling Problem. Ph.D. thesis, Dept. of Computer Science, Colorado
State University.
Watson, J.-P. (2005). On metaheuristics failure modes: A case study in tabu search for jobshop scheduling. In Proceedings of the Fifth Metaheuristics International Conference.
Watson, J.-P., Barbulescu, L., Whitley, L. D., & Howe, A. E. (2002). Contrasting structured
and random permutation flow-shop scheduling problems: search-space topology and
algorithm performance. INFORMS Journal on Computing, 14 (2), 98123.
Watson, J.-P., Beck, J. C., Howe, A. E., & Whitley, L. D. (2003). Problem difficulty for
tabu search in job-shop scheduling. Artificial Intelligence, 143 (2), 189217.
Watson, J.-P., Howe, A. E., & Whitley, L. D. (2006). Deconstructing Nowicki and Smutnickis i-TSAB tabu search algorithm for the job-shop scheduling problem. Computers
and Operations Research, 33 (9), 26232644.

77

fiJournal of Artificial Intelligence Research 29 (2007) 153-190

Submitted 10/06; published 06/07

The Generalized A* Architecture
Pedro F. Felzenszwalb

pff@cs.uchicago.edu

Department of Computer Science
University of Chicago
Chicago, IL 60637

David McAllester

mcallester@tti-c.org

Toyota Technological Institute at Chicago
Chicago, IL 60637

Abstract
We consider the problem of computing a lightest derivation of a global structure using
a set of weighted rules. A large variety of inference problems in AI can be formulated in
this framework. We generalize A* search and heuristics derived from abstractions to a
broad class of lightest derivation problems. We also describe a new algorithm that searches
for lightest derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion.
We discuss how the algorithms described here provide a general architecture for addressing the pipeline problem  the problem of passing information back and forth between
various stages of processing in a perceptual system. We consider examples in computer vision and natural language processing. We apply the hierarchical search algorithm to the
problem of estimating the boundaries of convex objects in grayscale images and compare
it to other search methods. A second set of experiments demonstrate the use of a new
compositional model for finding salient curves in images.

1. Introduction
We consider a class of problems defined by a set of weighted rules for composing structures
into larger structures. The goal in such problems is to find a lightest (least cost) derivation
of a global structure derivable with the given rules. A large variety of classical inference
problems in AI can be expressed within this framework. For example the global structure
might be a parse tree, a match of a deformable object model to an image, or an assignment
of values to variables in a Markov random field.
We define a lightest derivation problem in terms of a set of statements, a set of weighted
rules for deriving statements using other statements and a special goal statement. In each
case we are looking for the lightest derivation of the goal statement. We usually express a
lightest derivation problem using rule schemas that implicitly represent a very large set
of rules in terms of a small number of rules with variables. Lightest derivation problems
are formally equivalent to search in AND/OR graphs (Nilsson, 1980), but we find that our
formulation is more natural for the applications we are interested in.
One of the goals of this research is the construction of algorithms for global optimization
across many levels of processing in a perceptual system. As described below our algorithms
can be used to integrate multiple stages of a processing pipeline into a single global optimization problem that can be solved efficiently.
c
2007
AI Access Foundation. All rights reserved.

fiFelzenszwalb & McAllester

Dynamic programming is a fundamental technique for designing efficient inference algorithms. Good examples are the Viterbi algorithm for hidden Markov models (Rabiner,
1989) and chart parsing methods for stochastic context free grammars (Charniak, 1996).
The algorithms described here can be used to speed up the solution of problems normally
solved using dynamic programming. We demonstrate this for a specific problem, where the
goal is to estimate the boundary of a convex object in a cluttered image. In a second set
of experiments we show how our algorithms can be used to find salient curves in images.
We describe a new model for salient curves based on a compositional rule that enforces
long range shape constraints. This leads to a problem that is too large to be solved using
classical dynamic programming methods.
The algorithms we consider are all related to Dijkstras shortest paths algorithm (DSP)
(Dijkstra, 1959) and A* search (Hart, Nilsson, & Raphael, 1968). Both DSP and A* can be
used to find a shortest path in a cyclic graph. They use a priority queue to define an order
in which nodes are expanded and have a worst case running time of O(M log N ) where N
is the number of nodes in the graph and M is the number of edges. In DSP and A* the
expansion of a node v involves generating all nodes u such that there is an edge from v to u.
The only difference between the two methods is that A* uses a heuristic function to avoid
expanding non-promising nodes.
Knuth gave a generalization of DSP that can be used to solve a lightest derivation
problem with cyclic rules (Knuth, 1977). We call this Knuths lightest derivation algorithm
(KLD). In analogy to Dijkstras algorithm, KLD uses a priority queue to define an order in
which statements are expanded. Here the expansion of a statement v involves generating
all conclusions that can be derived in a single step using v and other statements already
expanded. As long as each rule has a bounded number of antecedents KLD also has a worst
case running time of O(M log N ) where N is the number of statements in the problem
and M is the number of rules. Nilssons AO* algorithm (1980) can also be used to solve
lightest derivation problems. Although AO* can use a heuristic function, it is not a true
generalization of A*  it does not use a priority queue, only handles acyclic rules, and can
require O(M N ) time even when applied to a shortest path problem.1 In particular, AO*
and its variants use a backward chaining technique that starts at the goal and repeatedly
refines subgoals, while A* is a forward chaining algorithm.2
Klein and Manning (2003) described an A* parsing algorithm that is similar to KLD but
can use a heuristic function. One of our contributions is a generalization of this algorithm
to arbitrary lightest derivation problems. We call this algorithm A* lightest derivation
(A*LD). The method is forward chaining, uses a priority queue to control the order in
which statements are expanded, handles cyclic rules and has a worst case running time of
O(M log N ) for problems where each rule has a small number of antecedents. A*LD can be
seen as a true generalization of A* to lightest derivation problems. For a lightest derivation
problem that comes from a shortest path problem A*LD is identical to A*.
Of course the running times seen in practice are often not well predicted by worst case
analysis. This is specially true for problems that are very large and defined implicitly. For
example, we can use dynamic programming to solve a shortest path problem in an acyclic
graph in O(M ) time. This is better than the O(M log N ) bound for DSP, but for implicit
1. There are extensions that handle cyclic rules (Jimenez & Torras, 2000).
2. AO* is backward chaining in terms of the inference rules defining a lightest derivation problem.

154

fiThe Generalized A* Architecture

graphs DSP can be much more efficient since it expands nodes in a best-first order. When
searching for a shortest path from a source to a goal, DSP will only expand nodes v with
d(v)  w . Here d(v) is the length of a shortest path from the source to v, and w is the
length of a shortest path from the source to the goal. In the case of A* with a monotone and
admissible heuristic function, h(v), it is possible to obtain a similar bound when searching
implicit graphs. A* will only expand nodes v with d(v) + h(v)  w .
The running time of KLD and A*LD can be expressed in a similar way. When solving a
lightest derivation problem, KLD will only expand statements v with d(v)  w . Here d(v)
is the weight of a lightest derivation for v, and w is the weight of a lightest derivation of the
goal statement. Furthermore, A*LD will only expand statements v with d(v) + h(v)  w .
Here the heuristic function, h(v), gives an estimate of the additional weight necessary for
deriving the goal statement using a derivation of v. The heuristic values used by A*LD are
analogous to the distance from a node to the goal in a graph search problem (the notion
used by A*). We note that these heuristic values are significantly different from the ones
used by AO*. In the case of AO* the heuristic function, h(v), would estimate the weight
of a lightest derivation for v.
An important difference between A*LD and AO* is that A*LD computes derivations
in a bottom-up fashion, while AO* uses a top-down approach. Each method has advantages, depending on the type of problem being solved. For example, a classical problem in
computer vision involves grouping pixels into long and smooth curves. We can formulate
the problem in terms of finding smooth curves between pairs of pixels that are far apart.
For an image with n pixels there are (n2 ) such pairs. A straight forward implementation
of a top-down algorithm would start by considering these (n2 ) possibilities. A bottomup algorithm would start with O(n) pairs of nearby pixels. In this case we expect that a
bottom-up grouping method would be more efficient than a top-down method.
The classical AO* algorithm requires the set of rules to be acyclic. Jimenez and Torras
(2000) extended the method to handle cyclic rules. Another top-down algorithm that can
handle cyclic rules is described by Bonet and Geffner (2005). Hansen and Zilberstein (2001)
described a search algorithm for problems where the optimal solutions themselves can be
cyclic. The algorithms described in this paper can handle problems with cyclic rules but
require that the optimal solutions be acyclic. We also note that AO* can handle rules with
non-superior weight functions (as defined in Section 3) while KLD requires superior weight
functions. A*LD replaces this requirement by a requirement on the heuristic function.
A well known method for defining heuristics for A* is to consider an abstract or relaxed
search problem. For example, consider the problem of solving a Rubiks cube in a small
number of moves. Suppose we ignore the edge and center pieces and solve only the corners.
This is an example of a problem abstraction. The number of moves necessary to put the
corners in a good configuration is a lower bound on the number of moves necessary to solve
the original problem. There are fewer corner configurations than there are full configurations
and that makes it easier to solve the abstract problem. In general, shortest paths to the
goal in an abstract problem can be used to define an admissible and monotone heuristic
function for solving the original problem with A*.
Here we show that abstractions can also be used to define heuristic functions for A*LD.
In a lightest derivation problem the notion of a shortest path to the goal is replaced by
the notion of a lightest context, where a context for a statement v is a derivation of the
155

fiFelzenszwalb & McAllester

goal with a hole that can be filled in by a derivation of v. The computation of lightest
abstract contexts is itself a lightest derivation problem.
Abstractions are related to problem relaxations defined by Pearl (1984). While abstractions often lead to small problems that are solved through search, relaxations can lead to
problems that still have a large state space but may be simple enough to be solved in closed
form. The definition of abstractions that we use for lightest derivation problems includes
relaxations as a special case.
Another contribution of our work is a hierarchical search method that we call HA*LD.
This algorithm can effectively use a hierarchy of abstractions to solve a lightest derivation
problem. The algorithm is novel even in the case of classical search (shortest paths) problem. HA*LD searches for lightest derivations and contexts at every level of abstraction
simultaneously. More specifically, each level of abstraction has its own set of statements
and rules. The search for lightest derivations and contexts at each level is controlled by a
single priority queue. To understand the running time of HA*LD, let w be the weight of a
lightest derivation of the goal in the original (not abstracted) problem. For a statement v
in the abstraction hierarchy let d(v) be the weight of a lightest derivation for v at its level
of abstraction. Let h(v) be the weight of a lightest context for the abstraction of v (defined
at one level above v in the hierarchy). Let K be the total number of statements in the
hierarchy with d(v) + h(v)  w . HAL*D expands at most 2K statements before solving
the original problem. The factor of two comes from the fact that the algorithm computes
both derivations and contexts at each level of abstraction.
Previous algorithms that use abstractions for solving search problems include methods based on pattern databases (Culberson & Schaeffer, 1998; Korf, 1997; Korf & Felner,
2002), Hierarchical A* (HA*, HIDA*) (Holte, Perez, Zimmer, & MacDonald, 1996; Holte,
Grajkowski, & Tanner, 2005) and coarse-to-fine dynamic programming (CFDP) (Raphael,
2001). Pattern databases have made it possible to compute solutions to impressively large
search problems. These methods construct a lookup table of shortest paths from a node
to the goal at all abstract states. In practice the approach is limited to tables that remain
fixed over different problem instances, or relatively small tables if the heuristic must be
recomputed for each instance. For example, for the Rubiks cube we can precompute the
number of moves necessary to solve every corner configuration. This table can be used to
define a heuristic function when solving any full configuration of the Rubiks cube. Both
HA* and HIDA* use a hierarchy of abstractions and can avoid searching over all nodes at
any level of the hierarchy. On the other hand, in directed graphs these methods may still
expand abstract nodes with arbitrarily large heuristic values. It is also not clear how to
generalize HA* and HIDA* to lightest derivation problems that have rules with more than
one antecedent. Finally, CFDP is related to AO* in that it repeatedly solves ever more
refined problems using dynamic programming. This leads to a worst case running time of
O(N M ). We will discuss the relationships between HA*LD and these other hierarchical
methods in more detail in Section 8.
We note that both A* search and related algorithms have been previously used to solve
a number of problems that are not classical state space search problems. This includes the
traveling salesman problem (Zhang & Korf, 1996), planning (Edelkamp, 2002), multiple
sequence alignment (Korf, Zhang, Thayer, & Hohwald, 2005), combinatorial problems on
graphs (Felner, 2005) and parsing using context-free-grammars (Klein & Manning, 2003).
156

fiThe Generalized A* Architecture

The work by Bulitko, Sturtevant, Lu, and Yau (2006) uses a hierarchy of state-space abstractions for real-time search.
1.1 The Pipeline Problem
A major problem in artificial intelligence is the integration of multiple processing stages to
form a complete perceptual system. We call this the pipeline problem. In general we have
a concatenation of systems where each stage feeds information to the next. In vision, for
example, we might have an edge detector feeding information to a boundary finding system,
which in turn feeds information to an object recognition system.
Because of computational constraints and the need to build modules with clean interfaces
pipelines often make hard decisions at module boundaries. For example, an edge detector
typically constructs a Boolean array that indicates weather or not an edge was detected
at each image location. But there is general recognition that the presence of an edge at a
certain location can depend on the context around it. People often see edges at places where
the image gradient is small if, at higher cognitive level, it is clear that there is actually an
object boundary at that location. Speech recognition systems try to address this problem
by returning n-best lists, but these may or may not contain the actual utterance. We would
like the speech recognition system to be able to take high-level information into account
and avoid the hard decision of exactly what strings to output in its n-best list.
A processing pipeline can be specified by describing each of its stages in terms of rules for
constructing structures using structures produced from a previous stage. In a vision system
one stage could have rules for grouping edges into smooth curves while the next stage could
have rules for grouping smooth curves into objects. In this case we can construct a single
lightest derivation problem representing the entire system. Moreover, a hierarchical set of
abstractions can be applied to the entire pipeline. By using HA*LD to compute lightest
derivations a complete scene interpretation derived at one level of abstraction guides all
processing stages at a more concrete level. This provides a mechanism that enables coarse
high-level processing to guide low-level computation. We believe that this is an important
property for implementing efficient perceptual pipelines that avoid making hard decisions
between processing stages.
We note that the formulation of a complete computer vision system as a lightest derivation problem is related to the work by Geman, Potter, and Chi (2002), Tu, Chen, Yuille,
and Zhu (2005) and Jin and Geman (2006). In these papers image understanding is posed
as a parsing problem, where the goal is to explain the image in terms of a set of objects that
are formed by the (possibly recursive) composition of generic parts. Tu et al. (2005) use
data driven MCMC to compute optimal parses while Geman et al. (2002) and Jin and
Geman (2006) use a bottom-up algorithm for building compositions in a greedy fashion.
Neither of these methods are guaranteed to compute an optimal scene interpretation. We
hope that HA*LD will provide a more principled computational technique for solving large
parsing problems defined by compositional models.
1.2 Overview
We begin by formally defining lightest derivation problems in Section 2. That section also
discusses dynamic programming and the relationship between lightest derivation problems
157

fiFelzenszwalb & McAllester

and AND/OR graphs. In Section 3 we describe Knuths lightest derivation algorithm. In
Section 4 we describe A*LD and prove its correctness. Section 5 shows how abstractions
can be used to define mechanically constructed heuristic functions for A*LD. We describe
HA*LD in Section 6 and discuss its use in solving the pipeline problem in Section 7. Section 8 discusses the relationship between HA*LD and other hierarchical search methods. In
Sections 9 and 10 we present some experimental results. We conclude in Section 11.

2. Lightest Derivation Problems
Let  be a set of statements and R be a set of inference rules of the following form,
A1 = w1
..
.
An = wn
C = g(w1 , . . . , wn )
Here the antecedents Ai and the conclusion C are statements in , the weights wi are
non-negative real valued variables and g is a non-negative real valued weight function. For
a rule with no antecedents the function g is simply a non-negative real value. Throughout
the paper we also use A1 , . . . , An g C to denote an inference rule of this type.
A derivation of C is a finite tree rooted at a rule A1 , . . . , An g C with n children, where
the i-th child is a derivation of Ai . The leaves of this tree are rules with no antecedents.
Every derivation has a weight that is the value obtained by recursive application of the
functions g along the derivation tree. Figure 1 illustrates a derivation tree.
Intuitively a rule A1 , . . . , An g C says that if we can derive the antecedents Ai with
weights wi then we can derive the conclusion C with weight g(w1 , . . . , wn ). The problem
we are interested in is to compute a lightest derivation of a special goal statement.
All of the algorithms discussed in this paper assume that the weight functions g associated with a lightest derivation problem are non-decreasing in each variable. This is a
fundamental property ensuring that lightest derivations have an optimal substructure property. In this case lightest derivations can be constructed from other lightest derivations.
To facilitate the runtime analysis of algorithms we assume that every rule has a small
number of antecedents. We use N to denote the number of statements in a lightest derivation
problem, while M denotes the number of rules. For most of the problems we are interested
in N and M are very large but the problem can be implicitly defined in a compact way,
by using a small number of rules with variables as in the examples below. We also assume
that N  M since statements that are not in the conclusion of some rule are clearly not
derivable and can be ignored.
2.1 Dynamic Programming
We say that a set of rules is acyclic if there is an ordering O of the statements in  such
that for any rule with conclusion C the antecedents are statements that come before C in
the ordering. Dynamic programming can be used to solve a lightest derivation problem if
158

fiThe Generalized A* Architecture

A1
A2
A3
C

Derivation
of A1

Derivation
of A2

Derivation
of A3

Figure 1: A derivation of C is a tree of rules rooted at a rule r with conclusion C. The
children of the root are derivations of the antecedents in r. The leafs of the tree
are rules with no antecedents.

the functions g in each rule are non-decreasing and the set of rules is acyclic. In this case
lightest derivations can be computed sequentially in terms of an acyclic ordering O. At the
i-th step a lightest derivation of the i-th statement is obtained by minimizing over all rules
that can be used to derive that statement. This method takes O(M ) time to compute a
lightest derivation for each statement in .
We note that for cyclic rules it is sometimes possible to compute lightest derivations by
taking multiple passes over the statements. We also note that some authors would refer
to Dijkstras algorithm (and KLD) as a dynamic programming method. In this paper we
only use the term when referring to algorithms that compute lightest derivations in a fixed
order that is independent of the solutions computed along the way (this includes recursive
implementations that use memoization).
2.2 Examples
Rules for computing shortest paths from a single source in a weighted graph are shown
in Figure 2. We assume that we are given a weighted graph G = (V, E), where wxy is a
non-negative weight for each edge (x, y)  E and s is a distinguished start node. The first
rule states that there is a path of weight zero to the start node s. The second set of rules
state that if there is a path to a node x we can extend that path with an edge from x to
y to obtain an appropriately weighted path to a node y. There is a rule of this type for
each edge in the graph. A lightest derivation of path(x) corresponds to shortest path from
s to x. Note that for general graphs these rules can be cyclic. Figure 3 illustrates a graph
159

fiFelzenszwalb & McAllester

(1)

path(s) = 0
(2) for each (x, y)  E,
path(x) = w
path(y) = w + wxy
Figure 2: Rules for computing shortest paths in a graph.

b

c

path(d) = w

path(c) = w

path(b) = w + wdb

path(b) = w + wcb

path(s) = w

path(e) = w

path(d) = w + wsd

path(c) = w + wec

d
a
e
s
path(s) = w
path(s) = 0
path(e) = w + wse

path(s) = 0

Figure 3: A graph with two highlighted paths from s to b and the corresponding derivations
using rules from Figure 2.

and two different derivations of path(b) using the rules just described. These corresponds
to two different paths from s to b.
Rules for chart parsing are shown in Figure 4. We assume that we are given a weighted
context free grammar in Chomsky normal form (Charniak, 1996), i.e., a weighted set of
productions of the form X  s and X  Y Z where X, Y and Z are nonterminal symbols
and s is a terminal symbol. The input string is given by a sequence of terminals (s1 , . . . , sn ).
160

fiThe Generalized A* Architecture

(1) for each production X  si ,

phrase(X, i, i + 1) = w(X  si )
(2) for each production X  Y Z and 1  i < j < k  n + 1,
phrase(Y, i, j) = w1
phrase(Z, j, k) = w2
phrase(X, i, k) = w1 + w2 + w(X  Y Z)
Figure 4: Rules for parsing with a context free grammar.
The first set of rules state that if the grammar contains a production X  si then there is a
phrase of type X generating the i-th entry of the input with weight w(X  si ). The second
set of rules state that if the grammar contains a production X  Y Z and there is a phrase
of type Y from i to j and a phrase of type Z from j to k then there is an, appropriately
weighted, phrase of type X from i to k. Let S be the start symbol of the grammar. The
goal of parsing is to find a lightest derivation of phrase(S, 1, n + 1). These rules are acyclic
because when phrases are composed together they form longer phrases.
2.3 AND/OR Graphs
Lightest derivation problems are closely related to AND/OR graphs. Let  and R be a set
of statements and rules defining a lightest derivation problem. To convert the problem to
an AND/OR graph representation we can build a graph with a disjunction node for each
statement in  and a conjunction node for each rule in R. There is an edge from each statement to each rule deriving that statement, and an edge from each rule to its antecedents.
The leaves of the AND/OR graph are rules with no antecedents. Now derivations of a
statement using rules in R can be represented by solutions rooted at that statement in the
corresponding AND/OR graph. Conversely, it is also possible to represent any AND/OR
graph search problem as a lightest derivation problem. In this case we can view each node
in the graph as a statement in  and build an appropriate set of rules R.

3. Knuths Lightest Derivation
Knuth (1977) described a generalization of Dijkstras shortest paths algorithm that we call
Knuths lightest derivation (KLD). Knuths algorithm can be used to solve a large class of
lightest derivation problems. The algorithm allows the rules to be cyclic but requires that
the weight functions associated with each rule be non-decreasing and superior. Specifically
we require the following two properties on the weight function g in each rule,
non-decreasing:
superior:

if wi0  wi then g(w1 , . . . , wi0 , . . . , wn )  g(w1 , . . . , wi , . . . , wn )
g(w1 , . . . , wn )  wi
161

fiFelzenszwalb & McAllester

For example,
g(x1 , . . . , xn ) = x1 +    + xn
g(x1 , . . . , xn ) = max(x1 , . . . , xn )
are both non-decreasing and superior functions.
Knuths algorithm computes lightest derivations in non-decreasing weight order. Since
we are only interested in a lightest derivation of a special goal statement we can often stop
the algorithm before computing the lightest derivation of every statement.
A weight assignment is an expression of the form (B = w) where B is a statement in 
and w is a non-negative real value. We say that the weight assignment (B = w) is derivable
if there is a derivation of B with weight w. For any set of rules R, statement B, and weight
w we write R ` (B = w) if the rules in R can be used to derive (B = w). Let `(B, R) be
the infimum of the set of weights derivable for B,
`(B, R) = inf{w : R ` (B = w)}.
Given a set of rules R and a statement goal   we are interested in computing a derivation
of goal with weight `(goal , R).
We define a bottom-up logic programming language in which we can easily express the
algorithms we wish to discuss throughout the rest of the paper. Each algorithm is defined
by a set of rules with priorities. We encode the priority of a rule by writing it along the
line separating the antecedents and the conclusion as follows,
A1 = w1
..
.
An = wn
p(w1 , . . . , wn )
C = g(w1 , . . . , wn )
We call a rule of this form a prioritized rule. The execution of a set of prioritized rules
P is defined by the procedure in Figure 5. The procedure keeps track of a set S and a
priority queue Q of weight assignments of the form (B = w). Initially S is empty and Q
contains weight assignments defined by rules with no antecedents at the priorities given by
those rules. We iteratively remove the lowest priority assignment (B = w) from Q. If B
already has an assigned weight in S then the new assignment is ignored. Otherwise we add
the new assignment to S and expand it  every assignment derivable from (B = w) and
other assignments already in S using some rule in P is added to Q at the priority specified
by the rule. The procedure stops when the queue is empty.
The result of executing a set of prioritized rules is a set of weight assignments. Moreover,
the procedure can implicitly keep track of derivations by remembering which assignments
were used to derive an item that is inserted in the queue.
Lemma 1. The execution of a finite set of prioritized rules P derives every statement that
is derivable with rules in P .
Proof. Each rule causes at most one item to be inserted in the queue. Thus eventually Q
is empty and the algorithm terminates. When Q is empty every statement derivable by a
162

fiThe Generalized A* Architecture

Procedure Run(P )
1. S  
2. Initialize Q with assignments defined by rules with no antecedents at their priorities
3. while Q is not empty
4.
Remove the lowest priority element (B = w) from Q
5.
if B has no assigned weight in S
6.
S  S  {(B = w)}
7.
Insert assignments derivable from (B = w) and other assignments in S using
some rule in P into Q at the priority specified by the rule
8. return S
Figure 5: Running a set of prioritized rules.
single rule using antecedents with weight in S already has a weight in S. This implies that
every derivable statement has a weight in S.
Now we are ready to define Knuths lightest derivation algorithm. The algorithm is
easily described in terms of prioritized rules.
Definition 1 (Knuths lightest derivation). Let R be a finite set of non-decreasing and
superior rules. Define a set of prioritized rules K(R) by setting the priority of each rule in
R to be the weight of the conclusion. KLD is given by the execution of K(R).
We can show that while running K(R), if (B = w) is added to S then w = `(B, R).
This means that all assignments in S represent lightest derivations. We can also show that
assignments are inserted into S in non-decreasing weight order. If we stop the algorithm as
soon as we insert a weight assignment for goal into S we will expand all statements B such
that `(B, R) < `(goal , R) and some statements B such that `(B, R) = `(goal , R). These
properties follow from a more general result described in the next section.
3.1 Implementation
The algorithm in Figure 5 can be implemented to run in O(M log N ) time, where N and
M refer to the size of the problem defined by the prioritized rules P .
In practice the set of prioritized rules P is often specified implicitly, in terms of a small
number of rules with variables. In this case the problem of executing P is closely related to
the work on logical algorithms described by McAllester (2002).
The main difficulty in devising an efficient implementation of the procedure in Figure 5
is in step 7. In that step we need to find weight assignments in S that can be combined
with (B = w) to derive new weight assignments. The logical algorithms work shows how a
set of inference rules with variables can be transformed into a new set of rules, such that
every rule has at most two antecedents and is in a particularly simple form. Moreover,
this transformation does not increase the number of rules too much. Once the rules are
transformed their execution can be implemented efficiently using a hashtable to represent
S, a heap to represent Q and indexing tables that allow us to perform step 7 quickly.
163

fiFelzenszwalb & McAllester

Consider the second set of rules for parsing in Figure 4. These can be represented by
a single rule with variables. Moreover the rule has two antecedents. When executing the
parsing rules we keep track of a table mapping a value for j to statements phrase(Y, i, j)
that have a weight in S. Using this table we can quickly find statements that have a weight
in S and can be combined with a statement of the form phrase(Z, j, k). Similarly we keep
track of a table mapping a value for j to statements phrase(Z, j, k) that have a weight in
S. The second table lets us quickly find statements that can be combined with a statement
of the form phrase(Y, i, j). We refer the reader to (McAllester, 2002) for more details.

4. A* Lightest Derivation
Our A* lightest derivation algorithm (A*LD) is a generalization of A* search to lightest
derivation problems that subsumes A* parsing. The algorithm is similar to KLD but it can
use a heuristic function to speed up computation. Consider a lightest derivation problem
with rules R and goal statement goal . Knuths algorithm will expand any statement B
such that `(B, R) < `(goal , R). By using a heuristic function A*LD can avoid expanding
statements that have light derivations but are not part of a light derivation of goal .
Let R be a set of rules with statements in , and h be a heuristic function assigning
a weight to each statement. Here h(B) is an estimate of the additional weight required to
derive goal using a derivation of B. We note that in the case of a shortest path problem this
weight is exactly the distance from a node to the goal. The value `(B, R) + h(B) provides
a figure of merit for each statement B. The A* lightest derivation algorithm expands
statements in order of their figure of merit.
We say that a heuristic function is monotone if for every rule A1 , . . . , An g C in R
and derivable weight assignments (Ai = wi ) we have,
wi + h(Ai )  g(w1 , . . . , wn ) + h(C).

(1)

This definition agrees with the standard notion of a monotone heuristic function for rules
that come from a shortest path problem. We can show that if h is monotone and h(goal ) = 0
then h is admissible under an appropriate notion of admissibility. For the correctness of
A*LD, however, it is only required that h be monotone and that h(goal ) be finite. In this
case monotonicity implies that the heuristic value of every statement C that appears in a
derivation of goal is finite. Below we assume that h(C) is finite for every statement. If h(C)
is not finite we can ignore C and every rule that derives C.
Definition 2 (A* lightest derivation). Let R be a finite set of non-decreasing rules and h
be a monotone heuristic function for R. Define a set of prioritized rules A(R) by setting
the priority of each rule in R to be the weight of the conclusion plus the heuristic value,
g(w1 , . . . , wn ) + h(C). A*LD is given by the execution of A(R).
Now we show that the execution of A(R) correctly computes lightest derivations and
that it expands statements in order of their figure of merit values.
Theorem 2. During the execution of A(R), if (B = w)  S then w = `(B, R).
Proof. The proof is by induction on the size of S. The statement is trivial when S = .
Suppose the statement was true right before the algorithm removed (B = wb ) from Q and
164

fiThe Generalized A* Architecture

added it to S. The fact that (B = wb )  Q implies that the weight assignment is derivable
and thus wb  `(B, R).
Suppose T is a derivation of B with weight wb0 < wb . Consider the moment right before
the algorithm removed (B = wb ) from Q and added it to S. Let A1 , . . . , An g C be a
rule in T such that the antecedents Ai have a weight in S while the conclusion C does not.
Let wc = g(`(A1 , R), . . . , `(An , R)). By the induction hypothesis the weight of Ai in S is
`(Ai , R). Thus (C = wc )  Q at priority wc + h(C). Let wc0 be the weight that T assigns to
C. Since g is non-decreasing we know wc  wc0 . Since h is monotone wc0 +h(C)  wb0 +h(B).
This follows by using the monotonicity condition along the path from C to B in T . Now
note that wc + h(C) < wb + h(B) which in turn implies that (B = wb ) is not the weight
assignment in Q with minimum priority.
Theorem 3. During the execution of A(R) statements are expanded in order of the figure
of merit value `(B, R) + h(B).
Proof. First we show that the minimum priority of Q does not decrease throughout the
execution of the algorithm. Suppose (B = w) is an element in Q with minimum priority.
Removing (B = w) from Q does not decrease the minimum priority. Now suppose we add
(B = w) to S and insert assignments derivable from (B = w) into Q. Since h is monotone
the priority of every assignment derivable from (B = w) is at least the priority of (B = w).
A weight assignment (B = w) is expanded when it is removed from Q and added to S.
By the last theorem w = `(B, R) and by the definition of A(R) this weight assignment was
queued at priority `(B, R) + h(B). Since we removed (B = w) from Q this must be the
minimum priority in the queue. The minimum priority does not decrease over time so we
must expand statements in order of their figure of merit value.
If we have accurate heuristic functions A*LD can be much more efficient than KLD.
Consider a situation where we have a perfect heuristic function. That is, suppose h(B)
is exactly the additional weight required to derive goal using a derivation of B. Now the
figure of merit `(B, R) + h(B) equals the weight of a lightest derivation of goal that uses
B. In this case A*LD will derive goal before expanding any statements that are not part
of a lightest derivation of goal .
The correctness KLD follows from the correctness of A*LD. For a set of non-decreasing
and superior rules we can consider the trivial heuristic function h(B) = 0. The fact that
the rules are superior imply that this heuristic is monotone. The theorems above imply
that Knuths algorithm correctly computes lightest derivations and expands statements in
order of their lightest derivable weights.

5. Heuristics Derived from Abstractions
Here we consider the case of additive rules  rules where the weight of the conclusion is
the sum of the weights of the antecedents plus a non-negative value v called the weight of
the rule. We denote such a rule by A1 , . . . , An v C. The weight of a derivation using
additive rules is the sum of the weights of the rules that appear in the derivation tree.
A context for a statement B is a finite tree of rules such that if we add a derivation of
B to the tree we get a derivation of goal . Intuitively a context for B is a derivation of goal
with a hole that can be filled in by a derivation of B (see Figure 6).
165

fiFelzenszwalb & McAllester

...
...
goal

A1
A2
A3
context for C

C

context for A3

Derivation
of A1

Derivation
of A2

Derivation
of A3

Figure 6: A derivation of goal defines contexts for the statements that appear in the derivation tree. Note how a context for C together with a rule A1 , A2 , A3  C and
derivations of A1 and A2 define a context for A3 .

For additive rules, each context has a weight that is the sum of weights of the rules in it.
Let R be a set of additive rules with statements in . For B   we define `(context(B), R)
to be the weight of a lightest context for B. The value `(B, R) + `(context(B), R) is the
weight of a lightest derivation of goal that uses B.
Contexts can be derived using rules in R together with context rules c(R) defined as
follows. First, goal has an empty context with weight zero. This is captured by a rule with
no antecedents 0 context(goal ). For each rule A1 , . . . , An v C in R we put n rules in
c(R). These rules capture the notion that a context for C and derivations of Aj for j 6= i
define a context for Ai ,
context(C), A1 , . . . , Ai1 , Ai+1 , . . . , An v context(Ai ).
Figure 6 illustrates how a context for C together with derivations of A1 and A2 and a rule
A1 , A2 , A3  C define a context for A3 .
166

fiThe Generalized A* Architecture

We say that a heuristic function h is admissible if h(B)  `(context(B), R). Admissible
heuristic functions never over-estimate the weight of deriving goal using a derivation of a
particular statement. The heuristic function is perfect if h(B) = `(context(B), R). Now we
show how to obtain admissible and monotone heuristic functions from abstractions.
5.1 Abstractions
Let (, R) be a lightest derivation problem with statements  and rules R. An abstraction
of (, R) is given by a problem (0 , R0 ) and a map abs :   0 , such that for every rule
A1 , . . . , An v C in R there is a rule abs(A1 ), . . . , abs(An ) v0 abs(C) in R0 with v 0  v.
Below we show how an abstraction can be used to define a monotone and admissible heuristic
function for the original problem.
We usually think of abs as defining a coarsening of  by mapping several statements
into the same abstract statement. For example, for a parser abs might map a lexicalized
nonterminal N Phouse to the nonlexicalized nonterminal N P . In this case the abstraction
defines a smaller problem on the abstract statements. Abstractions can often be defined in
a mechanical way by starting with a map abs from  into some set of abstract statements
0 . We can then project the rules in R from  into 0 using abs to get a set of abstract
rules. Typically several rules in R will map to the same abstract rule. We only need to
keep one copy of each abstract rule, with a weight that is a lower bound on the weight of
the concrete rules mapping into it.
Every derivation in (, R) maps to an abstract derivation so we have `(abs(C), R0 ) 
`(C, R). If we let the goal of the abstract problem be abs(goal ) then every context in (, R)
maps to an abstract context and we see that `(context(abs(C)), R0 )  `(context(C), R).
This means that lightest abstract context weights form an admissible heuristic function,
h(C) = `(context(abs(C)), R0 ).
Now we show that this heuristic function is also monotone.
Consider a rule A1 , . . . , An v C in R and let (Ai = wi ) be weight assignments derivable
using R. In this case there is a rule abs(A1 ), . . . , abs(An ) v0 abs(C) in R0 where v 0  v
and (abs(Ai ) = wi0 ) is derivable using R0 where wi0  wi . By definition of contexts (in the
abstract problem) we have,
X
`(context(abs(Ai )), R0 )  v 0 +
wj0 + `(context(abs(C)), R0 ).
j6=i

Since v 0  v and wj0  wj we have,
`(context(abs(Ai )), R0 )  v +

X

wj + `(context(abs(C)), R0 ).

j6=i

Plugging in the heuristic function h from above and adding wi to both sides,
X
wi + h(Ai )  v +
wj + h(C),
j

which is exactly the monotonicity condition in equation (1) for an additive rule.
167

fiFelzenszwalb & McAllester

If the abstract problem defined by (0 , R0 ) is relatively small we can efficiently compute
lightest context weights for every statement in 0 using dynamic programming or KLD.
We can store these weights in a pattern database (a lookup table) to serve as a heuristic
function for solving the concrete problem using A*LD. This heuristic may be able to stop
A*LD from exploring a lot of non-promising structures. This is exactly the approach that
was used by Culberson and Schaeffer (1998) and Korf (1997) for solving very large search
problems. The results in this section show that pattern databases can be used in the more
general setting of lightest derivations problems. The experiments in Section 10 demonstrate
the technique in a specific application.

6. Hierarchical A* Lightest Derivation
The main disadvantage of using pattern databases is that we have to precompute context
weights for every abstract statement. This can often take a lot of time and space. Here we
define a hierarchical algorithm, HA*LD, that searches for lightest derivations and contexts
in an entire abstraction hierarchy simultaneously. This algorithm can often solve the most
concrete problem without fully computing context weights at any level of abstraction.
At each level of abstraction the behavior HA*LD is similar to the behavior of A*LD
when using an abstraction-derived heuristic function. The hierarchical algorithm queues
derivations of a statement C at a priority that depends on a lightest abstract context for
C. But now abstract contexts are not computed in advance. Instead, abstract contexts are
computed at the same time we are computing derivations. Until we have an abstract context
for C, derivations of C are stalled. This is captured by the addition of context(abs(C))
as an antecedent to each rule that derives C.
We define an abstraction hierarchy with m levels to be a sequence of lightest derivation problems with additive rules (k , Rk ) for 0  k  m  1 with a single abstraction
function abs. For 0  k < m  1 the abstraction function maps k onto k+1 . We require that (k+1 , Rk+1 ) be an abstraction of (k , Rk ) as defined in the previous section:
if A1 , . . . , An v C is in Rk then there exists a rule abs(A1 ), . . . , abs(An ) v0 abs(C) in
Rk+1 with v 0  v. The hierarchical algorithm computes lightest derivations of statements
in k using contexts from k+1 to define heuristic values. We extend abs so that it maps
m1 to a most abstract set of statements m containing a single element . Since abs is
onto we have |k |  |k+1 |. That is, the number of statements decrease as we go up the
abstraction hierarchy. We denote by abs k the abstraction function from 0 to k obtained
by composing abs with itself k times.
We are interested in computing a lightest derivation of a goal statement goal  0 . Let
goal k = abs k (goal ) be the goal at each level of abstraction. The hierarchical algorithm is
defined by the set of prioritized rules H in Figure 7. Rules labeled UP compute derivations
of statements at one level of abstraction using context weights from the level above to define
priorities. Rules labeled BASE and DOWN compute contexts in one level of abstraction
using derivation weights at the same level to define priorities. The rules labeled START1
and START2 start the inference by handling the most abstract level.
The execution of H starts by computing a derivation and context for  with START1
and START2. It continues by deriving statements in m1 using UP rules. Once the
lightest derivation of goal m1 is found the algorithm derives a context for goal m1 with a
168

fiThe Generalized A* Architecture

0

START1:
=0

START2:

0
context() = 0

BASE:

goal k = w
w
context(goal k ) = 0

UP:

context(abs(C)) = wc
A1 = w1
..
.
An = wn
v + w1 +    + wn + wc
C = v + w1 +    + wn

DOWN:

context(C) = wc
A1 = w1
..
.
An = wn
v + wc + w1 +    + wn
context(Ai ) = v + wc + w1 +    + wn  wi

Figure 7: Prioritized rules H defining HA*LD. BASE rules are defined for 0  k  m  1.
UP and DOWN rules are defined for each rule A1 , . . . , An v C  Rk with
0  k  m  1.

BASE rule and starts computing contexts for other statements in m1 using DOWN rules.
In general HA*LD interleaves the computation of derivations and contexts at each level of
abstraction since the execution of H uses a single priority queue.
Note that no computation happens at a given level of abstraction until a lightest derivation of the goal has been found at the level above. This means that the structure of the
abstraction hierarchy can be defined dynamically. For example, as in the CFDP algorithm,
we could define the set of statements at each level of abstraction by refining the statements
that appear in a lightest derivation of the goal at the level above. Here we assume a static
abstraction hierarchy.
For each statement C  k with 0  k  m  1 we use `(C) to denote the weight of
a lightest derivation for C using Rk while `(context(C)) denotes the weight of a lightest
context for C using Rk . For the most abstract level we define `() = `(context()) = 0.
169

fiFelzenszwalb & McAllester

Below we show that HA*LD correctly computes lightest derivations and lightest contexts
at every level of abstraction. Moreover, the order in which derivations and contexts are
expanded is controlled by a heuristic function defined as follows. For C  k with 0  k 
m  1 define a heuristic value for C using contexts at the level above and a heuristic value
for context(C) using derivations at the same level,
h(C) = `(context(abs(C))),
h(context(C)) = `(C).
For the most abstract level we define h() = h(context()) = 0. Let a generalized statement
 be either an element of k for 0  k  m or an expression of the form context(C) for
C  k . We define an intrinsic priority for  as follows,
p() = `() + h().
For C  k , we have that p(context(C)) is the weight of a lightest derivation of goal k that
uses C, while p(C) is a lower bound on this weight.
The results from Sections 4 and 5 cannot be used directly to show the correctness of
HA*LD. This is because the rules in Figure 7 generate heuristic values at the same time
they generate derivations that depend on those heuristic values. Intuitively we must show
that during the execution of the prioritized rules H, each heuristic value is available at an
appropriate point in time. The next lemma shows that the rules in H satisfy a monotonicity
property with respect to the intrinsic priority of generalized statements. Theorem 5 proves
the correctness of the hierarchical algorithm.
Lemma 4 (Monotonicity). For each rule 1 , . . . , m   in the hierarchical algorithm, if
the weight of each antecedent i is `(i ) and the weight of the conclusion is  then
(a) the priority of the rule is  + h().
(b)  + h()  p(i ).
Proof. For the rules START1 and START2 the result follows from the fact that the rules
have no antecedents and h() = h(context()) = 0.
Consider a rule labeled BASE with w = `(goal k ). To see (a) note that  is always zero
and the priority of the rule is w = h(context(goal k )). For (b) we note that p(goal k ) =
`(goal k ) which equals the priority of the rule.
Now consider a rule labeled UP with wc = `(context(abs(C))) and wi = `(Ai ) for all i.
For part (a) note how the priority of the rule is  + wc and h(C) = wc . For part (b) consider
the first antecedent of the rule. We have h(context(abs(C))) = `(abs(C))  `(C)  , and
p(context(abs(C))) = wc + h(context(abs(C)))   + wc . Now consider an antecedent
Ai . If abs(Ai ) =  then p(Ai ) = wi   + wc . If abs(Ai ) 6=  then we can show that
h(Ai ) = `(context(abs(Ai )))  wc +   wi . This implies that p(Ai ) = wi + h(Ai )   + wc .
Finally consider a rule labeled DOWN with wc = `(context(C)) and wj = `(Aj ) for all
j. For part (a) note that the priority of the rule is  + wi and h(context(Ai )) = wi . For
Ppart
(b) consider the first antecedent of the rule. We have h(context(C)) = `(C)  v + j wj
and we see that p(context(C)) = wc + h(C)   + wi . Now consider an antecedent Aj . If
abs(Aj ) =  then h(Aj ) = 0 and p(Aj ) = wj   + wi . If abs(Aj ) 6=  we can show that
h(Aj )   + wi  wj . Hence p(Aj ) = wj + h(Aj )   + wi .
170

fiThe Generalized A* Architecture

Theorem 5. The execution of H maintains the following invariants.
1. If ( = w)  S then w = `().
2. If ( = w)  Q then it has priority w + h().
3. If p() < p(Q) then ( = `())  S
Here p(Q) denotes the smallest priority in Q.
Proof. In the initial state of the algorithm S is empty and Q contains only ( = 0) and
(context() = 0) at priority 0. For the initial state invariant 1 is true since S is empty;
invariant 2 follows from the definition of h() and h(context()); and invariant 3 follows
from the fact that p(Q) = 0 and p()  0 for all . Let S and Q denote the state of the
algorithm immediately prior to an iteration of the while loop in Figure 5 and suppose the
invariants are true. Let S 0 and Q0 denote the state of the algorithm after the iteration.
We will first prove invariant 1 for S 0 . Let ( = w) be the element removed from Q in
this iteration. By the soundness of the rules we have w  `(). If w = `() then clearly
invariant 1 holds for S 0 . If w > `() invariant 2 implies that p(Q) = w +h() > `()+h()
and by invariant 3 we know that S contains ( = `()). In this case S 0 = S.
Invariant 2 for Q0 follows from invariant 2 for Q, invariant 1 for S 0 , and part (a) of the
monotonicity lemma.
Finally, we consider invariant 3 for S 0 and Q0 . The proof is by reverse induction on the
abstraction level of . We say that  has level k if   k or  is of the form context(C)
with C  k . In the reverse induction, the base case considers  at level m. Initially the
algorithm inserts ( = 0) and (context() = 0) in the queue with priority 0. If p(Q0 ) > 0
then S 0 must contain ( = 0) and (context() = 0). Hence invariant 3 holds for S 0 and Q0
with  at level m.
Now we assume that invariant 3 holds for S 0 and Q0 with  at levels greater than k
and consider level k. We first consider statements C  k . Since the rules Rk are additive,
every statement C derivable with Rk has a lightest derivation (a derivation with weight
`(C)). This follows from the correctness of Knuths algorithm. Moreover, for additive
rules, subtrees of lightest derivations are also lightest derivations. We show by structural
induction that for any lightest derivation with conclusion C such that p(C) < p(Q0 ) we
have (C = `(C))  S 0 . Consider a lightest derivation in Rk with conclusion C such that
p(C) < p(Q0 ). The final rule in this derivation A1 , . . . , An v C corresponds to an UP rule
where we add an antecedent for context(abs(C)). By part (b) of the monotonicity lemma
all the antecedents of this UP rule have intrinsic priority less than p(Q0 ). By the induction
hypothesis on lightest derivations we have (Ai = `(Ai ))  S 0 . Since invariant 3 holds for
statements at levels greater than k we have (context(abs(C)) = `(context(abs(C))))  S 0 .
This implies that at some point the UP rule was used to derive (C = `(C)) at priority p(C).
But p(C) < p(Q0 ) and hence this item must have been removed from the queue. Therefore
S 0 must contain (C = w) for some w and, by invariant 1, w = `(C).
Now we consider  of the form context(C) with C  k . As before we see that c(Rk ) is
additive and thus every statement derivable with c(Rk ) has a lightest derivation and subtrees
of lightest derivations are lightest derivations themselves. We prove by structural induction
171

fiFelzenszwalb & McAllester

that for any lightest derivation T with conclusion context(C) such that p(context(C)) <
p(Q0 ) we have (context(C) = `(context(C)))  S 0 . Suppose the last rule of T is of the form,
context(C), A1 , . . . , Ai1 , Ai+1 , . . . , An v context(Ai ).
This rule corresponds to a DOWN rule where we add an antecedent for Ai . By part (b) of
the monotonicity lemma all the antecedents of this DOWN rule have intrinsic priority less
than p(Q0 ). By invariant 3 for statements in k and by the induction hypothesis on lightest
derivations using c(Rk ), all antecedents of the DOWN rule have their lightest weight in
S 0 . So at some point (context(Ai ) = `(context(Ai ))) was derived at priority p(Ai ). Now
p(Ai ) < p(Q0 ) implies the item was removed from the queue and, by invariant 1, we have
(context(Ai ) = `(context(Ai )))  S 0 .
Now suppose the last (and only) rule in T is 0 context(goal k ). This rule corresponds to a BASE rule where we add goal k as an antecedent. Note that p(goal k ) =
`(goal k ) = p(context(goal k )) and hence p(goal k ) < p(Q0 ). By invariant 3 for statements
in k we have (goal k = `(goal k )) in S 0 and at some point the BASE rule was used to queue
(context(goal k ) = `(context(goal k ))) at priority p(context(goal k )). As in the previous cases
p(context(goal k )) < p(Q0 ) implies (context(goal k ) = `(context(goal k )))  S 0 .
The last theorem implies that generalized statements  are expanded in order of their
intrinsic priority. Let K be the number of statements C in the entire abstraction hierarchy
with p(C)  p(goal ) = `(goal ). For every statement C we have that p(C)  p(context(C)).
We conclude that HA*LD expands at most 2K generalized statements before computing a
lightest derivation of goal .
6.1 Example
Now we consider the execution of HA*LD in a specific example. The example illustrates
how HA*LD interleaves the computation of structures at different levels of abstraction.
Consider the following abstraction hierarchy with 2 levels.
0 = {X1 , . . . , Xn , Y1 , . . . , Yn , Z1 , . . . , Zn , goal 0 }, 1 = {X, Y, Z, goal 1 },




1 X,
i Xi ,

















Y,

Y
,


 1
 i i
Xi , Yj ij goal 0 ,
X, Y 1 goal 1 ,
, R1 =
,
R0 =








Z,
,
Y

Z
,
X,
Y

X




5







 i i 5 i
Z 1 goal 1 ,
Zi i goal 0 ,
with abs(Xi ) = X, abs(Yi ) = Y , abs(Zi ) = Z and abs(goal0 ) = goal1 .
1. Initially S =  and Q = {( = 0) and (context() = 0) at priority 0}.
2. When ( = 0) comes off the queue it gets put in S but nothing else happens.
3. When (context() = 0) comes off the queue it gets put in S. Now statements in 1
have an abstract context in S. This causes UP rules that come from rules in R1 with
no antecedents to fire, putting (X = 1) and (Y = 1) in Q at priority 1.
172

fiThe Generalized A* Architecture

4. When (X = 1) and (Y = 1) come off the queue they get put in S, causing two UP
rules to fire, putting (goal 1 = 3) at priority 3 and (Z = 7) at priority 7 in the queue.
5. We have,
S = {( = 0), (context() = 0), (X = 1), (Y = 1)}
Q = {(goal 1 = 3) at priority 3, (Z = 7) at priority 7}
6. At this point (goal 1 = 3) comes off the queue and goes into in S. A BASE rule fires
putting (context(goal 1 ) = 0) in the queue at priority 3.
7. (context(goal 1 ) = 0) comes off the queue. This is the base case for contexts in 1 . Two
DOWN rules use (context(goal 1 ) = 0), (X = 1) and (Y = 1) to put (context(X) = 2)
and (context(Y ) = 2) in Q at priority 3.
8. (context(X) = 2) comes off the queue and gets put in S. Now we have an abstract
context for each Xi  0 , so UP rules to put (Xi = i) in Q at priority i + 2.
9. Now (context(Y ) = 2) comes off the queue and goes into S. As in the previous step
UP rules put (Yi = i) in Q at priority i + 2.
10. We have,
S = {( = 0), (context() = 0), (X = 1), (Y = 1), (goal 1 = 3),
(context(goal 1 ) = 0), (context(X) = 2), (context(Y ) = 2)}
Q = {(Xi = i) and (Yi = i) at priority i + 2 for 1  i  n, (Z = 7) at priority 7}
11. Next both (X1 = 1) and (Y1 = 1) will come off the queue and go into S. This causes
an UP rule to put (goal 0 = 3) in the queue at priority 3.
12. (goal 0 = 3) comes off the queue and goes into S. The algorithm can stop now since
we have a derivation of the most concrete goal.
Note how HA*LD terminates before fully computing abstract derivations and contexts.
In particular (Z = 7) is in Q but Z was never expanded. Moreover context(Z) is not even
in the queue. If we keep running the algorithm it would eventually derive context(Z), and
that would allow the Zi to be derived.

7. The Perception Pipeline
Figure 8 shows a hypothetical run of the hierarchical algorithm for a processing pipeline
of a vision system. In this system weighted statements about edges are used to derive
weighted statements about contours which provide input to later stages ultimately resulting
in statements about recognized objects.
It is well known that the subjective presence of edges at a particular image location can
depend on the context in which a given image patch appears. This can be interpreted in
the perception pipeline by stating that higher level processes  those later in the pipeline
 influence low-level interpretations. This kind of influence happens naturally in a lightest
173

fiFelzenszwalb & McAllester

m1

Edges

Contours

Recognition

1

Edges

Contours

Recognition

0

Edges

Contours

Recognition

Figure 8: A vision system with several levels of processing. Forward arrows represent the
normal flow of information from one stage of processing to the next. Backward
arrows represent the computation of contexts. Downward arrows represent the
influence of contexts.

derivation problem. For example, the lightest derivation of a complete scene analysis might
require the presence of an edge that is not locally apparent. By implementing the whole
system as a single lightest derivation problem we avoid the need to make hard decisions
between stages of the pipeline.
The influence of late pipeline stages in guiding earlier stages is pronounced if we use
HA*LD to compute lightest derivations. In this case the influence is apparent not only
in the structure of the optimal solution but also in the flow of information across different
stages of processing. In HA*LD a complete interpretation derived at one level of abstraction
guides all processing stages at a more concrete level. Structures derived at late stages of
the pipeline guide earlier stages through abstract context weights. This allows the early
processing stages to concentrate computational efforts in constructing structures that will
likely be part of the globally optimal solution.
While we have emphasized the use of admissible heuristics, we note that the A* architecture, including HA*LD, can also be used with inadmissible heuristic functions (of course
this would break our optimality guarantees). Inadmissible heuristics are important because
admissible heuristics tend to force the first few stages of a processing pipeline to generate
too many derivations. As derivations are composed their weights increase and this causes a
large number of derivations to be generated at the first few stages of processing before the
first derivation reaches the end of the pipeline. Inadmissible heuristics can produce behavior
similar to beam search  derivations generated in the first stage of the pipeline can flow
through the whole pipeline quickly. A natural way to construct inadmissible heuristics is to
simply scale-up an admissible heuristic such as the ones obtained from abstractions. It is
then possible to construct a hierarchical algorithm where inadmissible heuristics obtained
from one level of abstraction are used to guide search at the level below.

8. Other Hierarchical Methods
In this section we compare HA*LD to other hierarchical search methods.
174

fiThe Generalized A* Architecture

8.1 Coarse-to-Fine Dynamic Programming
HA*LD is related to the coarse-to-fine dynamic programming (CFDP) method described by
Raphael (2001). To understand the relationship consider the problem of finding the shortest
path from s to t in a trellis graph like the one shown in Figure 9(a). Here we have k columns
of n nodes and every node in one column is connected to a constant number of nodes in
the next column. Standard dynamic programming can be used to find the shortest path
in O(kn) time. Both CFDP and HA*LD can often find the shortest path much faster. On
the other hand the worst case behavior of these algorithms is very different as we describe
below, with CFDP taking significantly more time than HA*LD.
The CFDP algorithm works by coarsening the graph, grouping nodes in each column
into a small number of supernodes as illustrated in Figure 9(b). The weight of an edge
between two supernodes A and B is the minimum weight between nodes a  A and b  B.
The algorithm starts by using dynamic programming to find the shortest path P from s
to t in the coarse graph, this is shown in bold in Figure 9(b). The supernodes along P
are partitioned to define a finer graph as shown in Figure 9(c) and the procedure repeated.
Eventually the shortest path P will only go through supernodes of size one, corresponding
to a path in the original graph. At this point we know that P must be a shortest path from
s to t in the original graph. In the best case the optimal path in each iteration will be a
refinement of the optimal path from the previous iteration. This would result in O(log n)
shortest paths computations, each in fairly coarse graphs. On the other hand, in the worst
case CFDP will take (n) iterations to refine the whole graph, and many of the iterations
will involve finding shortest paths in large graphs. In this case CFDP takes (kn2 ) time
which is much worst than the standard dynamic programming approach.
Now suppose we use HA*LD to find the shortest path from s to t in a graph like the
one in Figure 9(a). We can build an abstraction hierarchy with O(log n) levels where each
supernode at level i contains 2i nodes from one column of the original graph. The coarse
graph in Figure 9(b) represents the highest level of this abstraction hierarchy. Note that
HA*LD will consider a small number, O(log n), of predefined graphs while CFDP can end
up considering a much larger number, (n), of graphs. In the best case scenario HA*LD
will expand only the nodes that are in the shortest path from s to t at each level of the
hierarchy. In the worst case HA*LD will compute a lightest path and context for every
node in the hierarchy (here a context for a node v is a path from v to t). At the i-th
abstraction level we have a graph with O(kn/2i ) nodes and edges. HA*LD will spend at
most O(kn log(kn)/2i ) time computing paths and contexts at level i. Summing over levels
we get at most O(kn log(kn)) time total, which is not much worst than the O(kn) time
taken by the standard dynamic programming approach.
8.2 Hierarchical Heuristic Search
Our hierarchical method is also related to the HA* and HIDA* algorithms described by
Holte et al. (1996) and Holte et al. (2005). These methods are restricted to shortest paths
problems but they also use a hierarchy of abstractions. A heuristic function is defined for
each level of abstraction using shortest paths to the goal at the level above. The main
idea is to run A* or IDA* to compute a shortest path while computing heuristic values ondemand. Let abs map a node to its abstraction and let g be the goal node in the concrete
175

fiFelzenszwalb & McAllester

s

t

(a)

s

t

s

(b)

t

(c)

Figure 9: (a) Original dynamic programming graph. (b) Coarse graph with shortest path
shown in bold. (c) Refinement of the coarse graph along the shortest path.

graph. Whenever the heuristic value for a concrete node v is needed we call the algorithm
recursively to find the shortest path from abs(v) to abs(g). This recursive call uses heuristic
values defined from a further abstraction, computed through deeper recursive calls.
It is not clear how to generalize HA* and HIDA* to lightest derivation problems that
have rules with multiple antecedents. Another disadvantage is that these methods can
potentially stall in the case of directed graphs. For example, suppose that when using
HA* or HIDA* we expand a node with two successors x and y, where x is close to the goal
but y is very far. At this point we need a heuristic value for x and y, and we might have
to spend a long time computing a shortest path from abs(y) to abs(g). On the other hand,
HA*LD would not wait for this shortest path to be fully computed. Intuitively HA*LD
would compute shortest paths from abs(x) and abs(y) to abs(g) simultaneously. As soon
as the shortest path from abs(x) to abs(g) is found we can start exploring the path from x
to g, independent of how long it would take to compute a path from abs(y) to abs(g).
176

fiThe Generalized A* Architecture

r2
r1

r3

r0
r4
r7
r5

r6

Figure 10: A convex set specified by a hypothesis (r0 , . . . , r7 ).

9. Convex Object Detection
Now we consider an application of HA*LD to the problem of detecting convex objects in
images. We pose the problem using a formulation similar to the one described by Raphael
(2001), where the optimal convex object around a point can be found by solving a shortest
path problem. We compare HA*LD to other search methods, including CFDP and A*
with pattern databases. The results indicate that HA*LD performs better than the other
methods over a wide range of inputs.
Let x be a reference point inside a convex object. We can represent the object boundary
using polar coordinates with respect to a coordinate system centered at x. In this case the
object is described by a periodic function r() specifying the distance from x to the object
boundary as a function of the angle . Here we only specify r() at a finite number of angles
(0 , . . . , N 1 ) and assume the boundary is a straight line segment between sample points.
We also assume the object is contained in a ball of radius R around x and that r() is an
integer. Thus an object is parametrized by (r0 , . . . , rN 1 ) where ri  [0, R  1]. An example
with N = 8 angles is shown in Figure 10.
Not every hypothesis (r0 , . . . , rN 1 ) specifies a convex object. The hypothesis describes
a convex set exactly when the object boundary turns left at each sample point (i , ri ) as
i increases. Let C(ri1 , ri , ri+1 ) be a Boolean function indicating when three sequential
values for r() define a boundary that is locally convex at i. The hypothesis (r0 , . . . , rN 1 )
is convex when it is locally convex at each i.3
Throughout this section we assume that the reference point x is fixed in advance. Our
goal is to find an optimal convex object around a given reference point. In practice
reference locations can be found using a variety of methods such as a Hough transform.
3. This parametrization of convex objects is similar but not identical to the one used by Raphael (2001).

177

fiFelzenszwalb & McAllester

Let D(i, ri , ri+1 ) be an image data cost measuring the evidence for a boundary segment
from (i , ri ) to (i+1 , ri+1 ). We consider the problem of finding a convex object for which
the sum of the data costs along the whole boundary is minimal. That is, we look for a
convex hypothesis minimizing the following energy function,
E(r0 , . . . , rN 1 ) =

N
1
X

D(i, ri , ri+1 ).

i=0

The data costs can be precomputed and specified by a lookup table with O(N R2 ) entries.
In our experiments we use a data cost based on the integral of the image gradient along
each boundary segment. Another approach would be to use the data term described by
Raphael (2001) where the cost depends on the contrast between the inside and the outside
of the object measured within the pie-slice defined by i and i+1 .
An optimal convex object can be found using standard dynamic programming techniques. Let B(i, r0 , r1 , ri1 , ri ) be the cost of an optimal partial convex object starting at
r0 and r1 and ending at ri1 and ri . Here we keep track of the last two boundary points to
enforce the convexity constraint as we extend partial objects. We also have to keep track
of the first two boundary points to enforce that rN = r0 and the convexity constraint at r0 .
We can compute B using the recursive formula,
B(1, r0 , r1 , r0 , r1 ) = D(0, r0 , r1 ),
B(i + 1, r0 , r1 , ri , ri+1 ) = min B(i, r0 , r1 , ri1 , ri ) + D(i, ri , ri+1 ),
ri1

where the minimization is over choices for ri1 such that C(ri1 , ri , ri+1 ) = true. The
cost of an optimal object is given by the minimum value of B(N, r0 , r1 , rN 1 , r0 ) such that
C(rN 1 , r0 , r1 ) = true. An optimal object can be found by tracing-back as in typical dynamic programming algorithms. The main problem with this approach is that the dynamic
programming table has O(N R4 ) entries and it takes O(R) time to compute each entry. The
overall algorithm runs in O(N R5 ) time which is quite slow.
Now we show how optimal convex objects can be defined in terms of a lightest derivation
problem. Let convex (i, r0 , r1 , ri1 , ri ) denote a partial convex object starting at r0 and r1
and ending at ri1 and ri . This corresponds to an entry in the dynamic programming table
described above. Define the set of statements,
 = {convex (i, a, b, c, d) | i  [1, N ], a, b, c, d  [0, R  1]}  {goal }.
An optimal convex object corresponds to a lightest derivations of goal using the rules in
Figure 11. The first set of rules specify the cost of a partial object from r0 to r1 . The
second set of rules specify that an object ending at ri1 and ri can be extended with a
choice for ri+1 such that the boundary is locally convex at ri . The last set of rules specify
that a complete convex object is a partial object from r0 to rN such that rN = r0 and the
boundary is locally convex at r0 .
To construct an abstraction hierarchy we define L nested partitions of the radius space
[0, R  1] into ranges of integers. In an abstract statement instead of specifying an integer
value for r() we will specify the range in which r() is contained. To simplify notation we
178

fiThe Generalized A* Architecture

(1) for r0 , r1  [0, R  1],

convex (1, r0 , r1 , r0 , r1 ) = D(0, r0 , r1 )
(2) for r0 , r1 , ri1 , ri , ri+1  [0, R  1] such that C(ri1 , ri , ri+1 ) = true,
convex (i, r0 , r1 , ri1 , ri ) = w
convex (i + 1, r0 , r1 , ri , ri+1 ) = w + D(i, ri , ri+1 )
(3) for r0 , r1 , rN 1  [0, R  1] such that C(rN 1 , r0 , r1 ) = true,
convex (N, r0 , r1 , rN 1 , r0 ) = w
goal = w
Figure 11: Rules for finding an optimal convex object.
assume that R is a power of two. The k-th partition P k contains R/2k ranges, each with
2k consecutive integers. The j-th range in P k is given by [j  2k , (j + 1)  2k  1].
The statements in the abstraction hierarchy are,
k = {convex (i, a, b, c, d) | i  [1, N ], a, b, c, d  P k }  {goal k },
for k  [0, L  1]. A range in P 0 contains a single integer so 0 = . Let f map a range
in P k to the range in P k+1 containing it. For statements in level k < L  1 we define the
abstraction function,
abs(convex (i, a, b, c, d)) = convex (i, f (a), f (b), f (c), f (d)),
abs(goal k ) = goal k+1 .
The abstract rules use bounds on the data costs for boundary segments between (i , si )
and (i+1 , si+1 ) where si and si+1 are ranges in P k ,
Dk (i, si , si+1 ) =

min
D(i, ri , ri+1 ).
ri  si
ri+1  si+1

Since each range in P k is the union of two ranges in P k1 one entry in Dk can be computed
quickly (in constant time) once Dk1 is computed. The bounds for all levels can be computed in O(N R2 ) time total. We also need abstract versions of the convexity constraints.
For si1 , si , si+1  P k , let C k (si1 , si , si+1 ) = true if there exist integers ri1 , ri and ri+1
in si1 , si and si+1 respectively such that C(ri1 , ri , ri+1 ) = true. The value of C k can be
defined in closed form and evaluated quickly using simple geometry.
The rules in the abstraction hierarchy are almost identical to the rules in Figure 11.
The rules in level k are obtained from the original rules by simply replacing each instance
of [0, R  1] by P k , C by C k and D by Dk .
179

fiFelzenszwalb & McAllester

Standard DP
CFDP
HA*LD
A* with pattern database in 2
A* with pattern database in 3

6718.6 seconds
13.5 seconds
8.6 seconds
14.3 seconds
29.7 seconds

Table 1: Running time comparison for the example in Figure 12.
9.1 Experimental Results
Figure 12 shows an example image with a set of reference locations that we selected manually
and the optimal convex object found around each reference point. There are 14 reference
locations and we used N = 30 and R = 60 to parametrize each object. Table 1 compares the
running time of different optimization algorithms we implemented for this problem. Each
line shows the time it took to solve all 14 problems contained in the example image using
a particular search algorithm. The standard DP algorithm uses the dynamic programming
solution outlined above. The CFDP method is based on the algorithm by Raphael (2001)
but modified for our representation of convex objects. Our hierarchical A* algorithm uses
the abstraction hierarchy described here. For A* with pattern databases we used dynamic
programming to compute a pattern database at a particular level of abstraction, and then
used this database to provide heuristic values for A*. Note that for the problem described
here the pattern database depends on the input. The running times listed include the time
it took to compute the pattern database in each case.
We see that CFDP, HA*LD and A* with pattern databases are much more efficient than
the standard dynamic programming algorithm that does not use abstractions. HA*LD is
slightly faster then the other methods in this example. Note that while the running time
varies from algorithm to algorithm the output of every method is the same as they all find
globally optimum objects.
For a quantitative evaluation of the different search algorithms we created a large set of
problems of varying difficulty and size as follows. For a given value of R we generated square
images of width and height 2  R + 1. Each image has a circle with radius less than R near
the center and the pixels in an image are corrupted by independent Gaussian noise. The
difficulty of a problem is controlled by the standard deviation, , of the noise. Figure 13
shows some example images and optimal convex object found around their centers.
The graph in Figure 14 shows the running time (in seconds) of the different search
algorithms as a function of the noise level when the problem size is fixed at R = 100.
Each sample point indicates the average running time over 200 random inputs. The graph
shows running times up to a point after which the circles can not be reliably detected. We
compared HA*LD with CFDP and A* using pattern databases (PD2 and PD3). Here PD2
and PD3 refer to A* with a pattern database defined in 2 and 3 respectively. Since the
pattern database needs to be recomputed for each input there is a trade-off in the amount
of time spent computing the database and the accuracy of the heuristic it provides. We
see that for easy problems it is better to use a smaller database (defined at a higher level
of abstraction) while for harder problems it is worth spending time computing a bigger
database. HA*LD outperforms the other methods in every situation captured here.
180

fiThe Generalized A* Architecture

(a)

(b)
Figure 12: (a) Reference locations. (b) Optimal convex objects.

181

fiFelzenszwalb & McAllester

Figure 13: Random images with circles and the optimal convex object around the center of
each one (with N = 20 and R = 100). The noise level in the images is  = 50.

Figure 15 shows the running time of the different methods as a function of the problem
size R, on problems with a fixed noise level of  = 100. As before each sample point
indicates the average running time taken over 200 random inputs. We see that the running
time of the pattern database approach grows quickly as the problem size increases. This is
because computing the database at any fixed level of abstraction takes O(N R5 ) time. On
the other hand the running time of both CFDP and HA*LD grows much slower. While
CFDP performed essentially as well as HA*LD in this experiment, the graph in Figure 14
shows that HA*LD performs better as the difficulty of the problem increases.

10. Finding Salient Curves in Images
A classical problem in computer vision involves finding salient curves in images. Intuitively
the goal is to find long and smooth curves that go along paths with high image gradient.
The standard way to pose the problem is to define a saliency score and search for curves
optimizing that score. Most methods use a score defined by a simple combination of local
terms. For example, the score usually depends on the curvature and the image gradient at
each point of a curve. This type of score can often be optimized efficiently using dynamic
programming or shortest paths algorithms (Montanari, 1971; Shashua & Ullman, 1988;
Basri & Alter, 1996; Williams & Jacobs, 1996).
Here we consider a new compositional model for finding salient curves. An important
aspect of this model is that it can capture global shape constraints. In particular, it looks
for curves that are almost straight, something that can not be done using local constraints
alone. Local constraints can enforce small curvature at each point of a curve, but this is
182

fiThe Generalized A* Architecture

Figure 14: Running time of different search algorithms as a function of the noise level  in
the input. Each sample point indicates the average running time taken over 200
random inputs. In each case N = 20 and R = 100. See text for discussion.

not enough to prevent curves from turning and twisting around over long distances. The
problem of finding the most salient curve in an image with the compositional model defined
here can be solved using dynamic programming, but the approach is too slow for practical
use. Shortest paths algorithms are not applicable because of the compositional nature of
the model. Instead we can use A*LD with a heuristic function derived from an abstraction
(a pattern database).
Let C1 be a curve with endpoints a and b and C2 be a curve with endpoints b and c.
The two curves can be composed to form a curve C from a to c. We define the weight of the
composition to be the sum of the weights of C1 and C2 plus a shape cost that depends on
the geometric arrangement of points (a, b, c). Figure 16 illustrates the idea and the shape
costs we use. Note that when C1 and C2 are long, the arrangement of their endpoints reflect
non-local geometric properties. In general we consider composing C1 and C2 if the angle
formed by ab and bc is at least /2 and the lengths of C1 and C2 are approximately equal.
These constraints reduce the total number of compositions and play an important role in
the abstract problem defined below.
Besides the compositional rule we say that if a and b are nearby locations, then there is
a short curve with endpoints a and b. This forms a base case for creating longer curves. We
183

fiFelzenszwalb & McAllester

Figure 15: Running time of different search algorithms as a function of the problem size R.
Each sample point indicates the average running time taken over 200 random
inputs. In each case N = 20 and  = 100. See text for discussion.

b
a

t

c

Figure 16: A curve with endpoints (a, c) is formed by composing curves with endpoints
(a, b) and (b, c). We assume that t  /2. The cost of the composition is
proportional to sin2 (t). This cost is scale invariant and encourages curves to be
relatively straight.

assume that these short curves are straight, and their weight depends only on the image
data along the line segment from a to b. We use a data term, seg(a, b), that is zero if the
image gradient along pixels in ab is perpendicular to ab, and higher otherwise.
Figure 17 gives a formal definition of the two rules in our model. The constants k1 and
k2 specify the minimum and maximum length of the base case curves, while L is a constant
184

fiThe Generalized A* Architecture

(1) for pixels a, b, c where the angle between ab and bc is at least /2 and for 0  i  L,
curve(a, b, i) = w1
curve(b, c, i) = w2
curve(a, c, i + 1) = w1 + w2 + shape(a, b, c)
(2) for pixels a, b with k1  ||a  b||  k2 ,

curve(a, b, 0) = seg(a, b)
Figure 17: Rules for finding almost straight curves between a pair of endpoints. Here L,
k1 and k2 are constants, while shape(a, b, c) is a function measuring the cost of
a composition.

controlling the maximum depth of derivations. A derivation of curve(a, b, i) encodes a curve
from a to b. The value i can be seen as an approximate measure of arclength. A derivation
of curve(a, b, i) is a full binary tree of depth i that encodes a curve with length between
2i k1 and 2i k2 . We let k2 = 2k1 to allow for curves of any length.
The rules in Figure 17 do not define a good measure of saliency by themselves because
they always prefer short curves over long ones. We can define the saliency of a curve in
terms of its weight minus its arclength, so that salient curves will be light and long. Let 
be a positive constant. We consider finding the lightest derivation of goal using,
curve(a, b, i) = w
goal = w  2i
For an n  n image there are (n4 ) statements of the form curve(a, c, i). Moreover, if a
and c are far apart there are (n) choices for a midpoint b defining the two curves that
are composed in a lightest derivation of curve(a, c, i). This makes a dynamic programming
solution to the lightest derivation problem impractical. We have tried using KLD but even
for small images the algorithm runs out of memory after a few minutes. Below we describe
an abstraction we have used to define a heuristic function for A*LD.
Consider a hierarchical set of partitions of an image into boxes. The i-th partition is
defined by tiling the image into boxes of 2i  2i pixels. The partitions form a pyramid with
boxes of different sizes at each level. Each box at level i is the union of 4 boxes at the level
below it, and the boxes at level 0 are the pixels themselves. Let fi (a) be the box containing
a in the i-th level of the pyramid. Now define
abs(curve(a, b, i)) = curve(fi (a), fi (b), i).
185

fiFelzenszwalb & McAllester

A
B

a
b
d
c

D
C

Figure 18: The abstraction maps each curve statement to a statement about curves between
boxes. If i > j then curve(a, b, i) gets coarsened more than curve(c, d, j). Since
light curves are almost straight, i > j usually implies that ||a  b|| > ||c  d||.

Figure 18 illustrates how this map selects a pyramid level for an abstract statement. Intuitively abs defines an adaptive coarsening criteria. If a and b are far from each other, a
curve from a to b must be long, which in turn implies that we map a and b to boxes in a
coarse partition of the image. This creates an abstract problem that has a small number of
statements without losing too much information.
To define the abstract problem we also need to define a set of abstract rules. Recall
that for every concrete rule r we need a corresponding abstract rule r0 where the weight
of r0 is at most the weight of r. There are a small number of rules with no antecedents in
Figure 17. For each concrete rule seg(a,b) curve(a, b, 0) we define a corresponding abstract
rule, seg(a,b) abs(curve(a, b, 0)). The compositional rules from Figure 17 lead to abstract
rules for composing curves between boxes,
curve(A, B, i), curve(B, C, i) v curve(A0 , C 0 , i + 1),
where A, B and C are boxes at the i-th pyramid level while A0 and C 0 are the boxes at
level i + 1 containing A and C respectively. The weight v should be at most shape(a, b, c)
where a, b and c are arbitrary pixels in A, B and C respectively. We compute a value for
v by bounding the orientations of the line segments ab and bc between boxes.
186

fiThe Generalized A* Architecture

146  137 pixels. Running time: 50 seconds (38 + 12).

122  179 pixels. Running time: 65 seconds (43 + 22).

226  150 pixels. Running time: 73 seconds (61 + 12).
Figure 19: The most salient curve in different images. The running time is the sum of
the time spent computing the pattern database and the time spent solving the
concrete problem.

187

fiFelzenszwalb & McAllester

Figure 20: An example where the most salient curve goes over locations with essentially no
local evidence for a the curve at those locations.

The abstract problem defined above is relatively small even in large images, so we can
use the pattern database approach outlined in Section 5.1. For each input image we use
KLD to compute lightest context weights for every abstract statement. We then use these
weights as heuristic values for solving the concrete problem with A*LD. Figure 19 illustrates
some of the results we obtained using this method. It seems like the abstract problem is
able to capture that most short curves can not be extended to a salient curve. It took
about one minute to find the most salient curve in each of these images. Figure 19 lists the
dimensions of each image and the running time in each case.
Note that our algorithm does not rely on an initial binary edge detection stage. Instead
the base case rules allow for salient curves to go over any pixel, even if there is no local
evidence for a boundary at a particular location. Figure 20 shows an example where this
happens. In this case there is a small part of the horse back that blends with the background
if we consider local properties alone.
The curve finding algorithm described in this section would be very difficult to formulate
without A*LD and the general notion of heuristics derived from abstractions for lightest
derivation problems. However, using the framework introduced in this paper it becomes
relatively easy to specify the algorithm.
In the future we plan to compose the rules for computing salient curves with rules for
computing more complex structures. The basic idea of using a pyramid of boxes for defining
an abstract problem should be applicable to a variety of problems in computer vision.
188

fiThe Generalized A* Architecture

11. Conclusion
Although we have presented some preliminary results in the last two sections, we view the
main contribution of this paper as providing a general architecture for perceptual inference.
Dijkstras shortest paths algorithm and A* search are both fundamental algorithms with
many applications. Knuth noted the generalization of Dijkstras algorithm to more general
problems defined by a set of recursive rules. In this paper we have given similar generalizations for A* search and heuristics derived from abstractions. We have also described
a new method for solving lightest derivation problems using a hierarchy of abstractions.
Finally, we have outlined an approach for using these generalizations in the construction of
processing pipelines for perceptual inference.

Acknowledgments
This material is based upon work supported by the National Science Foundation under
Grant No. 0535174 and 0534820.

References
Basri, R., & Alter, T. (1996). Extracting salient curves from images: An analysis of the
saliency network. In IEEE Conference on Computer Vision and Pattern Recognition.
Bonet, B., & Geffner, H. (2005). An algorithm better than AO*. In Proceedings of the
National Conference on Artificial Intelligence.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2006). State abstraction in real-time heuristic
search. Technical Report, University of Alberta, Department of Computer Science.
Charniak, E. (1996). Statistical Language Learning. MIT Press.
Culberson, J., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14 (3),
318334.
Dijkstra, E. (1959). A note on two problems in connection with graphs. Numerical Mathematics, 1, 269271.
Edelkamp, S. (2002). Symbolic pattern databases in heuristic search panning. In International Conference on AI Planning and Scheduling.
Felner, A. (2005). Finding optimal solutions to the graph partitioning problem with heuristic
search. Annals of Mathematics and Artificial Intelligence, 45 (3-4), 293322.
Geman, S., Potter, D., & Chi, Z. (2002). Composition systems. Quarterly of Applied
Mathematics, 707736.
Hansen, E., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129, 3562.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimal cost paths. IEEE Transactions on Systems Science and Cybernetics, 4 (2),
100107.
Holte, R., Grajkowski, J., & Tanner, B. (2005). Hierarchical heuristic search revisited. In
Symposium on Abstraction, Reformulation and Approximation.
189

fiFelzenszwalb & McAllester

Holte, R., Perez, M., Zimmer, R., & MacDonald, A. (1996). Hierarchical A*: Searching abstraction hierarchies efficiently. In Proceedings of the National Conference on Artificial
Intelligence.
Jimenez, P., & Torras, C. (2000). An efficient algorithm for searching implicit AND/OR
graphs with cycles. Artificial Intelligence, 124, 130.
Jin, Y., & Geman, S. (2006). Context and hierarchy in a probabilistic image model. In
IEEE Conference on Computer Vision and Pattern Recognition.
Klein, D., & Manning, C. (2003). A* parsing: Fast exact viterbi parse selection. In Proceedings of the HLT-NAACL.
Knuth, D. (1977). A generalization of Dijkstras algorithm. Information Processing Letters,
6 (1), 15.
Korf, R. (1997). Finding optimal solutions to Rubiks cube using pattern databases. In
Proceedings of the National Conference on Artificial Intelligence.
Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 922.
Korf, R., Zhang, W., Thayer, I., & Hohwald, H. (2005). Frontier search. Journal of the
ACM, 52 (5), 715748.
McAllester, D. (2002). On the complexity analysis of static analyses. Journal of the ACM,
49 (4), 512537.
Montanari, U. (1971). On the optimal detection of curves in noisy pictures. Communications
of the ACM, 14 (5).
Nilsson, N. (1980). Principles of Artificial Intelligence. Morgan Kaufmann.
Pearl, J. (1984). Heuristics: intelligent search strategies for computer problem solving.
Addison-Wesley.
Rabiner, L. (1989). A tutorial on hidden Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77 (2), 257286.
Raphael, C. (2001). Coarse-to-fine dynamic programming. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 23 (12), 13791390.
Shashua, A., & Ullman, S. (1988). Structural saliency: The detection of globally salient
structures using a locally connected network. In IEEE International Conference on
Computer Vision.
Tu, Z., Chen, X., Yuille, A., & Zhu, S. (2005). Image parsing: Unifying segmentation,
detection, and recognition. International Journal of Computer Vision, 63 (2), 113
140.
Williams, L., & Jacobs, D. (1996). Local parallel computation of stochastic completion
fields. In IEEE Conference on Computer Vision and Pattern Recognition.
Zhang, W., & Korf, R. (1996). A study of complexity transitions on the asymmetric traveling
salesman problem. Artificial Intelligence, 81, 12.

190

fiJournal of Artificial Intelligence Research 29 (2007) 391-419

Submitted 1/2007; published 8/2007

Obtaining Reliable Feedback for Sanctioning Reputation
Mechanisms
Radu Jurca
Boi Faltings

radu.jurca@epfl.ch
boi.faltings@epfl.ch

Ecole Polytechnique Federale de Lausanne (EPFL)
Artificial Intelligence Laboratory (LIA)
CH-1015 Lausanne, Switzerland
http: // liawww. epfl. ch

Abstract
Reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard. Future clients guide their business decisions by considering the feedback from past transactions; if truthfully exposed, cheating
behavior is sanctioned and thus becomes irrational.
It therefore becomes important to ensure that rational clients have the right incentives
to report honestly. As an alternative to side-payment schemes that explicitly reward truthful reports, we show that honesty can emerge as a rational behavior when clients have a
repeated presence in the market. To this end we describe a mechanism that supports an
equilibrium where truthful feedback is obtained. Then we characterize the set of paretooptimal equilibria of the mechanism, and derive an upper bound on the percentage of false
reports that can be recorded by the mechanism. An important role in the existence of this
bound is played by the fact that rational clients can establish a reputation for reporting
honestly.

1. Introduction
The availability of ubiquitous communication through the Internet is driving the migration of business transactions from direct contact between people to electronically mediated
interactions. People interact electronically either through human-computer interfaces or
through programs representing humans, so-called agents. In either case, no physical interactions among entities occur, and the systems are much more susceptible to fraud and
deception.
Traditional methods to avoid cheating involve cryptographic schemes and trusted third
parties (TTPs) that overlook every transaction. Such systems are very costly, introduce
potential bottlenecks, and may be difficult to deploy due to the complexity and heterogeneity of the environment: e.g., agents in different geographical locations may be subject to
different legislation, or different interaction protocols.
Reputation mechanisms offer a novel and effective way of ensuring the necessary level
of trust which is essential to the functioning of any market. They are based on the observation that agent strategies change when we consider that interactions are repeated: the other
party remembers past cheating, and changes its terms of business accordingly in the future.
Therefore, the expected gains due to future transactions in which the agent has a higher
reputation can offset the loss incurred by not cheating in the present. This effect can be amc
2007
AI Access Foundation. All rights reserved.

fiJurca & Faltings

plified considerably when such reputation information is shared among a large population,
and thus multiplies the expected future gains made accessible by honest behavior.
Existing reputation mechanisms enjoy huge success. Systems such as eBay1 or Amazon2
implement reputation mechanisms which are partly credited for the businesses success.
Studies show that human users seriously take into account the reputation of the seller when
placing bids in online auctions (Houser & Wooders, 2006), and that despite the incentive
to free ride, feedback is provided in more than half of the transactions on eBay (Resnick &
Zeckhauser, 2002).
One important challenge associated with designing reputation mechanisms is to ensure
that truthful feedback is obtained about the actual interactions, a property called incentivecompatibility. Rational users can regard the private information they have observed as a
valuable asset, not to be freely shared. Worse even, agents can have external incentives
to misreport and thus manipulate the reputation information available to other agents
(Harmon, 2004). Without proper measures, the reputation mechanism will obtain unreliable
information, biased by the strategic interests of the reporters.
Honest reporting incentives should be addressed differently depending on the predominant role of the reputation mechanisms. The signaling role is useful in environments where
the service offered by different providers may have different quality, but all clients interacting with the same provider are treated equally (markets with adverse selection). This
is the case, for example, in a market of web-services. Different providers possess different
hardware resources and employ different algorithms; this makes certain web-services better
than others. Nevertheless, all requests issued to the same web-service are treated by the
same program. Some clients might experience worse service than others, but these differences are random, and not determined by the provider. The feedback from previous clients
statistically estimates the quality delivered by a provider in the future, and hence signals
to future clients which provider should be selected.
The sanctioning role, on the other hand, is present in settings where service requests
issued by clients must be individually addressed by the provider. Think of a barber, who
must skillfully shave every client that walks in his shop. The problem here is that providers
must exert care (and costly effort) for satisfying every service request. Good quality can
result only when enough effort was exerted, but the provider is better off by exerting less
effort: e.g., clients will anyway pay for the shave, so the barber is better off by doing a sloppy
job as fast as possible in order to have time for more customers. This moral hazard situation
can be eliminated by a reputation mechanism that punishes providers for not exerting effort.
Low effort results in negative feedback that decreases the reputation, and hence the future
business opportunities of the provider. The future loss due to a bad reputation offsets the
momentary gain obtained by cheating, and makes cooperative behavior profitable.
There are well known solutions for providing honest reporting incentives for signaling
reputation mechanisms. Since all clients interacting with a service receive the same quality
(in a statistical sense), a clients private observation influences her belief regarding the
experience of other clients. In the web-services market mentioned before, the fact that one
client had a bad experience with a certain web-service makes her more likely to believe
that other clients will also encounter problems with that same web-service. This correlation
1. www.ebay.com
2. www.amazon.com

392

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

between the clients private belief and the feedback reported by other clients can be used
to design feedback payments that make honesty a Nash equilibrium. When submitting
feedback, clients get paid an amount that depends both on the the value they reported
and on the reports submitted by other clients. As long as others report truthfully, the
expected payment of every client is maximized by the honest report  thus the equilibrium.
Miller, Resnick, and Zeckhauser (2005) and Jurca and Faltings (2006) show that incentivecompatible payments can be designed to offset both reporting costs and lying incentives.
For sanctioning reputation mechanisms the same payment schemes are not guaranteed to
be incentive-compatible. Different clients may experience different service quality because
the provider decided to exert different effort levels. The private beliefs of the reporter
may no longer be correlated to the feedback of other clients, and therefore, the statistical
properties exploited by Miller et al. (2005) are no longer present.
As an alternative, we propose different incentives to motivate honest reporting based
on the repeated presence of the client in the market. Game theoretic results (i.e., the folk
theorems) show that repeated interactions support new equilibria where present deviations
are made unattractive by future penalties. Even without a reputation mechanism, a client
can guide her future play depending on the experience of previous interactions. As a first
result of this paper, we describe a mechanism that indeed supports a cooperative equilibrium
where providers exert effort all the time. The reputation mechanism correctly records when
the client received low quality.
There are certainly some applications where clients repeatedly interact with the same
seller with a potential moral hazard problem. The barber shop mentioned above is one
example, as most people prefer going to the same barber (or hairdresser). Another example
is a market of delivery services. Every package must be scheduled for timely delivery,
and this involves a cost for the provider. Some of this cost may be saved by occasionally
dropping a package, hence the moral hazard. Moreover, business clients typically rely
on the same carrier to dispatch their documents or merchandise. As their own business
depends on the quality and timeliness of the delivery, they do have the incentive to form a
lasting relationship and get good service. Yet another example is that of a business person
who repeatedly travels to an offshore client. The business person has a direct interest to
repeatedly obtain good service from the hotel which is closest to the clients offices.
We assume that the quality observed by the clients is also influenced by environmental
factors outside the control of, however observable by, the provider. Despite the barbers
best effort, a sudden movement of the client can always generate an accidental cut that will
make the client unhappy. Likewise, the delivery company may occasionally lose or damage
some packages due to transportation accidents. Nevertheless, the delivery company (like
the barber) eventually learns with certainty about any delays, damages or losses that entitle
clients to complain about unsatisfactory service.
The mechanism we propose is quite simple. Before asking feedback from the client, the
mechanism gives the provider the opportunity to acknowledge failure, and reimburse the
client. Only when the provider claims good service does the reputation mechanism record
the feedback of the client. Contradictory reports (the provider claims good service, but the
client submits negative feedback) may only appear when one of the parties is lying, and
therefore, both the client and the provider are sanctioned: the provider suffers a loss as a
consequence of the negative report, while the client is given a small fine.
393

fiJurca & Faltings

One equilibrium of the mechanism is when providers always do their best to deliver
the promised quality, and truthfully acknowledge the failures caused by the environmental
factors. Their honest behavior is motivated by the threat that any mistake will drive
the unsatisfied client away from the market. When future transactions generate sufficient
revenue, the provider does not afford to risk losing a client, hence the equilibrium.
Unfortunately, this socially desired equilibrium is not unique. Clients can occasionally
accept bad service and keep returning to the same provider because they dont have better
alternatives. Moreover, since complaining for bad service is sanctioned by the reputation
mechanism, clients might be reluctant to report negative feedback. Penalties for negative
reports and the clients lack of choice drives the provider to occasionally cheat in order to
increase his revenue.
As a second result, we characterize the set of pareto-optimal equilibria of our mechanism
and prove that the amount of unreported cheating that can occur is limited by two factors.
The first factor limits the amount of cheating in general, and is given by the quality of
the alternatives available to the clients. Better alternatives increase the expectations of the
clients, therefore the provider must cheat less in order to keep his customers.
The second factor limits the amount of unreported cheating, and represents the cost
incurred by clients to establish a reputation for reporting the truth. By stubbornly exposing
bad service when it happens, despite the fine imposed by the reputation mechanism, the
client signals to the provider that she is committed to always report the truth. Such signals
will eventually change the strategy of the provider to full cooperation, who will avoid the
punishment for negative feedback. Having a reputation for reporting truthfully is of course,
valuable to the client; therefore, a rational client accepts to lie (and give up the reputation)
only when the cost of building a reputation for reporting honestly is greater than the
occasional loss created by tolerated cheating. This cost is given by the ease with which
the provider switches to cooperative play, and by the magnitude of the fine imposed for
negative feedback.
Concretely, this paper proceeds as follows. In Section 2 we describe related work, followed by a more detailed description of our setting in Section 3. Section 4 presents a game
theoretic model of our mechanism and an analysis of reporting incentives and equilibria.
Here we establish the existence of the cooperative equilibrium, and derive un upper bound
on the amount of cheating that can occur in any pareto-optimal equilibrium.
In Section 5 we establish the cost of building a reputation for reporting honestly, and
hence compute an upper bound on the percentage of false reports recorded by the reputation
mechanism in any equilibrium.
We continue in Section 6 by analyzing the impact of malicious buyers that explicitly
try to destroy the reputation of the provider. We give some initial approximations on the
worst case damage such buyers can cause to providers. Further discussions, open issues and
directions for future work are discussed in Section 7. Finally, Section 8 concludes our work.

2. Related Work
The notion of reputation is often used in Game Theory to signal the commitment of a player
towards a fixed strategy. This is what we mean by saying that clients establish a reputation
for reporting the truth: they commit to always report the truth. Building a reputation
394

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

usually requires some incomplete information repeated game, and can significantly impact
the set of equilibrium points of the game. This is commonly referred to as the reputation
effect, first characterized by the seminal papers of Kreps, Milgrom, Roberts, and Wilson
(1982), Kreps and Wilson (1982) and Milgrom and Roberts (1982).
The reputation effect can be extended to all games where a player (A) could benefit
from committing to a certain strategy  that is not credible in a complete information
game: e.g., a monopolist seller would like to commit to fight all potential entrants in a
chain-store game (Selten, 1978), however, this commitment is not credible due to the cost
of fighting. In an incomplete information game where the commitment type has positive
probability, As opponent (B) can at some point become convinced that A is playing as if
she were the commitment type. At that point, B will play a best response against , which
gives A the desired payoff. Establishing a reputation for the commitment strategy requires
time and cost. When the higher future payoffs offset the cost of building reputation, the
reputation effect prescribes minimum payoffs any equilibrium strategy should give to player
A (otherwise, A can profitably deviate by playing as if she were a commitment type).
Fudenberg and Levine (1989) study the class of all repeated games in which a long-run
player faces a sequence of single-shot opponents who can observe all previous games. If the
long-run player is sufficiently patient and the single-shot players have a positive prior belief
that the long-run player might be a commitment type, the authors derive a lower bound on
the payoff received by the long-run player in any Nash equilibrium of the repeated game.
This result holds for both finitely and infinitely repeated games, and is robust against further
perturbations of the information structure (i.e., it is independent of what other types have
positive probability).
Schmidt (1993) provides a generalization of the above result for the two long-run player
case in a special class of games called of conflicting interests, when one of the players is
sufficiently more patient than the opponent. A game is of conflicting interests when the
commitment strategy of one player (A) holds the opponent (B) to his minimax payoff. The
author derives an upper limit on the number of rounds B will not play a best response to
As commitment type, which in turn generates a lower bound on As equilibrium payoff. For
a detailed treatment of the reputation effect, the reader is directed to the work of Mailath
and Samuelson (2006).
In computer science and information systems research, reputation information defines
some aggregate of feedback reports about past transactions. This is the semantics we are
using when referring to the reputation of the provider. Reputation information encompasses
a unitary appreciation of the personal attributes of the provider, and influences the trusting
decisions of clients. Depending on the environment, reputation has two main roles: to signal
the capabilities of the provider, and to sanction cheating behavior (Kuwabara, 2003).
Signaling reputation mechanisms allow clients to learn which providers are the most
capable of providing good service. Such systems have been widely used in computational
trust mechanisms. Birk (2001) and Biswas, Sen, and Debnath (2000) describe systems
where agents use their direct past experience to recognize trustworthy partners. The global
efficiency of the market is clearly increased, however, the time needed to build the reputation
information prohibits the use of this kind of mechanisms in a large scale online market.
A number of signaling reputation mechanisms also take into consideration indirect reputation information, i.e., information reported by peers. Schillo, Funk, and Rovatsos (2000)
395

fiJurca & Faltings

and Yu and Singh (2002, 2003) use social networks in order to obtain the reputation of an
unknown agent. Agents ask acquaintances several hops away about the trustworthiness of
an unknown agent. Recommendations are afterwards aggregated into a single measure of
the agents reputation. This class of mechanisms, however intuitive, does not provide any
rational participation incentives for the agents. Moreover, there is little protection against
untruthful reporting, and no guarantee that the mechanism cannot be manipulated by a
malicious provider in order to obtain higher payoffs.
Truthful reporting incentives for signaling reputation mechanisms are described by
Miller et al. (2005). Honest reports are explicitly rewarded by payments that take into
account the value of the submitted report, and the value of a report submitted by another
client (called the reference reporter ). The payment schemes are designed based on proper
scoring rules, mathematical functions that make possible the revelation of private beliefs
(Cooke, 1991). The essence behind honest reporting incentives is the observation that the
private information a client obtains from interacting with a provider changes her belief regarding the reports of other clients. This change in beliefs can be exploited to make honesty
an ex-ante Nash equilibrium strategy.
Jurca and Faltings (2006) extend the above result by taking a computational approach
to designing incentive compatible payment schemes. Instead of using closed form scoring
rules, they compute the payments using an optimization problem that minimizes the total
budget required to reward the reporters. By also using several reference reports and filtering
mechanisms, they render the payment mechanisms cheaper and more practical.
Dellarocas (2005) presents a comprehensive investigation of binary sanctioning reputation mechanisms. As in our setting, providers are equally capable of providing high quality,
however, doing so requires costly effort. The role of the reputation mechanism is to encourage cooperative behavior by punishing cheating: negative feedback reduces future revenues
either by excluding the provider from the market, or by decreasing the price the provider
can charge in future transactions. Dellarocas shows that simple information structures and
decision rules can lead to efficient equilibria, given that clients report honestly.
Our paper builds upon such mechanisms by addressing reporting incentives. We will abstract away the details of the underlying reputation mechanism through an explicit penalty
associated with a negative feedback. Given that such high enough penalties exist, any reputation mechanism (i.e., feedback aggregation and trusting decision rules) can be plugged
in our scheme.
In the same group of work that addresses reporting incentives, we mention the work of
Braynov and Sandholm (2002), Dellarocas (2002) and Papaioannou and Stamoulis (2005).
Braynov and Sandholm consider exchanges of goods for money and prove that a market
in which agents are trusted to the degree they deserve to be trusted is equally efficient as
a market with complete trustworthiness. By scaling the amount of the traded product,
the authors prove that it is possible to make it rational for sellers to truthfully declare
their trustworthiness. Truthful declaration of ones trustworthiness eliminates the need of
reputation mechanisms and significantly reduces the cost of trust management. However,
the assumptions made about the trading environment (i.e. the form of the cost function and
the selling price which is supposed to be smaller than the marginal cost) are not common
in most electronic markets.
396

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

For e-Bay-like auctions, the Goodwill Hunting mechanism (Dellarocas, 2002) provides a
way to make sellers indifferent between lying or truthfully declaring the quality of the good
offered for sale. Momentary gains or losses obtained from misrepresenting the goods quality
are later compensated by the mechanism which has the power to modify the announcement
of the seller.
Papaioannou and Stamoulis (2005) describe an incentive-compatible reputation mechanism that is particularly suited for peer-to-peer applications. Their mechanism is similar to
ours, in the sense that both the provider and the client are punished for submitting conflicting reports. The authors experimentally show that a class of common lying strategies are
successfully deterred by their scheme. Unlike their results, our paper considers all possible
equilibrium strategies and sets bounds on the amount of untruthful information recorded
by the reputation mechanism.

3. The Setting
We assume an online market, where rational clients (she) repeatedly request the same service from one provider (he). Every client repeatedly interacts with the service provider,
however, successive requests from the same client are always interleaved with enough requests generated by other clients. Transactions are assumed sequential, the provider does
not have capacity constraints, and accepts all requests.
The price of service is p monetary units, and the service can have either high (q1 ) or
low (q0 ) quality. Only high quality is valuable to the clients, and has utility u(q1 ) = u.
Low quality has utility 0, and can be precisely distinguished from high quality. Before each
round, the client can decide to request the service from the provider, or quit the market and
resort to an outside provider that is completely trustworthy. The outside provider always
delivers high quality service, but for a higher price p(1 + ).
If the client decides to interact with the online provider, she issues a request to the
provider, and pays for the service. The provider can now decide to exert low (e0 ) or high
(e1 ) effort when treating the request. Low effort has a normalized cost of 0, but generates
only low quality. High effort is expensive (normalized cost equals c(e1 ) = c) and generates
high quality with probability  < 1.  is fixed, and depends on the environmental factors
outside the control of the provider. p > c, so that it is individually rational for providers
to exert effort.
After exerting effort, the provider can observe the quality of the resulting service. He
can then decide to deliver the service as it is, or to acknowledge failure and roll back the
transaction by fully reimbursing3 the client. We assume perfect delivery channels, such
that the client perceives exactly the same quality as the provider. After delivery, the client
inspects the quality of service, and can accuse low quality by submitting a negative report
to the reputation mechanism.
The reputation mechanism (RM) is unique in the market, and trusted by all participants.
It can oversee monetary transactions (i.e., payments made between clients and the provider)
and can impose fines on all parties. However, the RM does not observe the effort level
exerted by the provider, nor does it know the quality of the delivered service.
3. In reality, the provider might also pay a penalty for rolling back the transaction. As long as this penalty
is small, the qualitative results we present in this paper remain valid.

397

fiJurca & Faltings

The RM asks feedback from the client only if she chose to transact with the provider in
the current round (i.e., paid the price of service to the provider) and the provider delivered
the service (i.e., provider did not reimburse the client). When the client submits negative
feedback, the RM punishes both the client and the provider: the client must pay a fine ,
and the provider accumulates a negative reputation report.
3.1 Examples
Although simplistic, this model retains the main characteristics of several interesting applications. A delivery service for perishable goods (goods that lose value past a certain
deadline) is one of them. Pizza, for example, must be delivered within 30 minutes, otherwise it gets cold and loses its taste. Hungry clients can order at home, or drive to a
more expensive local restaurant, where theyre sure to get a hot pizza. The price of a home
delivered pizza is p = 1, while at the restaurant, the same pizza would cost p(1 + ) = 1.2.
In both cases, the utility of a warm meal is u = 2.
The pizza delivery provider must exert costly effort to deliver orders within the deadline.
A courier must be dispatched immediately (high effort), for an estimated cost of c = 0.8.
While such action usually results in good service (the probability of a timely delivery is
 = 99%), traffic conditions and unexpected accidents (e.g., the address is not easily found)
may still delay some deliveries past the deadline.
Once at the destination, the delivery person, as well as the client, know if the delivery
was late or not. As it is common practice, the provider can acknowledge being late, and
reimburse the client. Clients may provide feedback to a reputation mechanism, but their
feedback counts only if they were not reimbursed. The clients fine for submitting a negative
report can be set for example at  = 0.01. The future loss to the provider caused by the
negative report (and quantified through ) depends on the reputation mechanism.
A simplified market of car garagists or plumbers could fit the same model. The provider
is commissioned to repair a car (respectively the plumbing) and the quality of the work
depends on the exerted effort. High effort is more costly but ensures a lasting result with
high probability. Low effort is cheap, but the resulting fix is only temporary. In both
cases, however, the warranty convention may specify the right of the client to ask for a
reimbursement if problems reoccur within the warranty period. Reputation feedback may
be submitted at the end of the warranty period, and is accepted only if reimbursements
didnt occur.
An interesting emerging application comes with a new generation of web services that
can optimally decide how to treat every request. For some service types, a high quality
response requires the exclusive use of costly resources. For example, computation jobs
require CPU time, storage requests need disk space, information requests need queries to
databases. Sufficient resources, is a prerequisite, but not a guarantee for good service.
Software and hardware failures may occur, however, these failures are properly signaled
to the provider. Once monetary incentives become sufficiently important in such markets,
intelligent providers will identify the moral hazard problem, and may act strategically as
identified in our model.
398

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

4. Behavior and Reporting Incentives
From game theoretic point of view, one interaction between the client and the provider can
be modeled by the extensive-form game (G) with imperfect public information, shown in
Figure 1. The client moves first and decides (at node 1) whether to play in and interact
with the provider, or to play out and resort to the trusted outside option.
Once the client plays in, the provider can chose at node 2 whether to exert high or
low effort (i.e., plays e1 or e0 respectively). When the provider plays e0 the generated
quality is low. When the provider plays e1 , nature chooses between high quality (q1 ) with
probability , and low quality (q0 ) with probability 1  . The constant  is assumed
common knowledge in the market. Having seen the resulting quality, the provider delivers
(i.e., plays d) the service, or acknowledges low quality and rolls back the transaction (i.e.,
plays l) by fully reimbursing the client. If the service is delivered, the client can report
positive (1) or negative (0) feedback.
A pure strategy is a deterministic mapping describing an action for each of the players
information sets. The client has three information sets in the game G. The first information
set is singleton and contains the node 1 at the beginning of game when the client must
decide between playing in or out. The second information set contains the nodes 7 and 8
(the dotted oval in Figure 1) where the client must decide between reporting 0 or 1, given
that she has received low quality, q0 . The third information set is singleton and contains the
node 9 where the client must decide between reporting 0 or 1, given that she received high
quality, q1 . The strategy in0q0 1q1 , for example, is the honest reporting strategy, specifying
that the client enters the game, reports 0 when she receives low quality, and reports 1 when
she receives high quality. The set of pure strategies of the client is:
AC = {out1q0 1q1 , out1q0 0q1 , out0q0 1q1 , out0q0 0q1 , in1q0 1q1 , in1q0 0q1 , in0q0 1q1 , in1q0 1q1 };
Similarly, the set of pure strategies of the provider is:
AP = {e0 l, e0 d, e1 lq0 lq1 , e1 lq0 dq1 , e1 dq0 lq1 , e1 dq0 dq1 };
where e1 lq0 dq1 , for example, is the socially desired strategy: the provider exerts effort at
node 2, acknowledges low quality at node 5, and delivers high quality at node 6. A pure
strategy profile s is a pair (sC , sP ) where sC  AC and sP  AP . If (A) denotes the set of
probability distributions over the elements of A, C  (AC ) and P  (AP ) are mixed
strategies for the client, respectively the provider, and  = (C , P ) is a mixed strategy
profile.
The payoffs to the players
depend on the chosen strategy profile, and on the move

of nature. Let g() = gC (), gP () denote the pair of expected payoffs received by
the client, respectively by the provider when playing strategy profile . The function g :
(AC )  (AP )  R2 is characterized in Table 1 and also describs the normal form
transformation of G. Besides the corresponding payments made between the client and
the provider, Table 1 also reflects the influence of the reputation mechanism, as further
explained in Section 4.1. The four strategies of the client that involve playing out at node 1
generate the same outcomes, and therefore, have been collapsed for simplicity into a single
row of Table 1.
399

fiJurca & Faltings

Client
1

in

out

Provider
2

e0

u-p(1+r)
0

e1
Nature
3

q0

4

Provider

Provider

d

l

d

q1

5

6

Provider

d

l

l

Client
7

0
0

0

-p-e
p-e

Client
1

8

9

0

-p
p

0
-c

1

-p-e
p-c-e

-p
p-c

0

u-p-e
p-c-e

0
-c

1

u-p
p-c

Figure 1: The game representing one interaction. Empty circles represent decision nodes,
edge labels represent actions, full circles represent terminal nodes and the dotted
oval represents an information set. Payoffs are represented in rectangles, the top
row describes the payoff of the client, the second row describes the payoff of the
provider.

4.1 The Reputation Mechanism
For every interaction, the reputation mechanism records one of the three different signals it
may receive: positive feedback when the client reports 1, negative feedback when the client
reports 0, and neutral feedback when the provider rolls back the transaction and reimburses
the client. In Figure 1 (and Table 1) positive and neutral feedback do not influence the
payoff of the provider, while negative feedback imposes a punishment equivalent to .
Two considerations made us choose this representation. First, we associate neutral and
positive feedback with the same reward (0 in this case) because intuitively, the acknowledgement of failure may also be regarded as honest behavior on behalf of the provider.
Failures occur despite best effort, and by acknowledging them, the provider shouldnt suffer.
However, neutral feedback may also result because the provider did not exert effort. The
lack of punishment for these instances contradicts the goal of the reputation mechanism to
400

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

in1q0 1q1

Provider

e0 l
e0 d
e1 lq0 lq1
e1 lq0 dq1
e1 dq0 lq1
e1 dq0 dq1

Client
in0q0 1q1

in1q0 0q1
0

0

0
0

0

p
p

p
p

0
p  
p  

0
c

out
0

p  
p  

0
c

in0q0 0q1
0

0
c

u  p(1 + )
0
u  p(1 + )
0

0
c

u  p(1 + )
0

(u  p)

(u  p  )

(u  p)

(u  p  )

p  c
(1  )p

(p  )  c
(1  )p

p  c
(1  )(p + )

(p  )  c
(1  )(p + )

0

(1  )p  c
u  p

(1  )p  c
(u  )  p

(1)(p )c
u(1)p

(1)(p )c
u    p

0

pc

p    c

p  (1  )  c

p    c

0

u  p(1 + )
u  p(1 + )
u  p(1 + )

Table 1: Normal transformation of the extensive form game, G
encourage exertion of effort. Fortunately, the action e0 l can be the result of rational behavior
only in two circumstances, both excusable: one, when the provider defends himself against
a malicious client that is expected to falsely report negative feedback (details in Section
6), and two, when the environmental noise is too big ( is too small) to justify exertion of
effort. Neutral feedback can be used to estimate the parameter , or to detect coalitions
of malicious clients, and indirectly, may influence the revenue of the provider. However,
for the simplified model presented above, positive and neutral feedback are considered the
same in terms of generated payoffs.
The second argument relates to the role of the RM to constrain the revenue of the
provider depending on the feedback of the client. There are several ways of doing that.
Dellarocas (2005) describes two principles, and two mechanisms that punish the provider
when the clients submit negative reports. The first, works by exclusion. After each negative
report the reputation mechanism bans the provider from the market with probability .
This probability can be tuned such that the provider has the incentive to cooperate almost
all the time, and the market stays efficient. The second works by changing the conditions
of future trade. Every negative report triggers the decrease of the price the next N clients
will pay for the service. For lower values of N the price decrease is higher, nonetheless, N
can take any value in an efficient market.
Both mechanisms work because the future losses offset the momentary gain the provider
would have had by intentionally cheating on the client. Note that these penalties are given
endogenously by lost future opportunities, and require some minimum premiums for trusted
providers. When margins are not high enough, providers do not care enough about future
transactions, and will use the present opportunity of cheating.
Another option is to use exogenous penalties for cheating. For example, the provider
may be required to buy a licence for operating in the market4 . The licence is partially
destroyed by every negative feedback. Totaly destroyed licences must be restored through
a new payment, and remaining parts can be sold if the provider quits the market. The
price of the licence and the amount that is destroyed by a negative feedback can be scaled
4. The reputation mechanism can buy and sell market licences

401

fiJurca & Faltings

such that rational providers have the incentive to cooperate. Unlike the previous solutions,
this mechanism does not require minimum transaction margins as punishments for negative
feedback are directly subtracted from the upfront deposit.
One way or another, all reputation mechanisms foster cooperation because the provider
associates value to client feedback. Let V (R+ ) and V (R ) be the value of a positive, respectively a negative report. In the game in Figure 1, V (R+ ) is normalized to 0, and V (R )
is . By using this notation, we abstract away the details of the reputation mechanism,
and retain only the essential punishment associated with negative feedback. Any reputation mechanism can be plugged in our scheme, as long as the particular constraints (e.g.,
minimum margins for transactions) are satisfied.
One last aspect to be considered is the influence of the reputation mechanism on the
future transactions of the client. If negative reports attract lower prices, rational long-run
clients might be tempted to falsely report in order to purchase cheaper services in the future.
Fortunately, some of the mechanisms designed for single-run clients, do not influence the
reporting strategy of long-run clients. The reputation mechanism that only keeps the last
N reports (Dellarocas, 2005) is one of them. A false negative report only influences the
next N transactions of the provider; given that more than N other requests are interleaved
between any two successive requests of the same client, a dishonest reporter cannot decrease
the price for her future transactions.
The licence-based mechanism we have described above is another example. The price
of service remains unchanged, therefore reporting incentives are unaffected. On the other
hand, when negative feedback is punished by exclusion, clients may be more reluctant to
report negatively, since they also lose a trading partner.
4.2 Analysis of Equilibria
The one-time game presented in Figure 1 has only one subgame equilibrium where the client
opts out. When asked to report feedback, the client always prefers to report 1 (reporting
0 attracts the penalty ). Knowing this, the best strategy for the provider is to exert low
effort and deliver the service. Knowing the provider will play e0 d, it is strictly better for
the client to play out.
The repeated game between the same client and provider may, however, have other
equilibria. Before analyzing the repeated game, let us note that every interaction between a
provider and a particular client can be strategically isolated and considered independently.
As the provider accepts all clients and views them identically, he will maximize his expected
revenue in each of the isolated repeated games.
From now on, we will only consider the repeated interaction between the provider and
one client. This can be modeled by a T -fold repetition of the stage game G, denoted GT ,
where T is finite or infinite. In this paper we will deal with the infinite horizon case, however,
the results obtained can also be applied with minor modifications to finitely repeated games
where T is large enough.
If  is the per period discount factor reflecting the probability that the market ceases
to exist after each round, (or the present value of future revenues), let us denote by  the
expected discount factor in the game GT . If our client interacts with the provider on the
average every N rounds,  =  N .
402

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

The life-time expected payoff of the players is computed as:
T
X

  gi ;

 =0

where i  {C, P } is the client, respectively the provider, gi is the expected payoff obtained
by player i in the  th interaction, and   is the discount applied to compute the present day
value of gi .
We will consider normalized life-time expected payoffs, so that payoffs in G and GT can
be expressed using the same measure:
T
X

Vi = (1  )

  gi ;

(1)

 =0

We define the average continuation payoff for player i from period t onward (and including period t) as:
Vit = (1  )

T
X

  t gi ;

(2)

 =t

The set of outcomes publicly perceived by both players after each round is:
Y = {out, l, q0 1, q0 0, q1 1, q1 0}

where:
 out is observed when the client opts out,
 l is observed when the provider acknowledges low quality and rolls back the transaction,
 qi j is observed when the provider delivers quality qi  {q0 , q1 } and the client reports
j  {0, 1}.
We denote by ht a specific public history of the repeated game out of the set H t = (Y )t of
all possible histories up to and including period t. In the repeated game, a public strategy
i of player i is a sequence of maps (it ), where it : H t1  (Ai ) prescribes the (mixed)
strategy to be played in round t, after the public history ht1  H t1 . A perfect public
equilibrium (PPE) is a profile of public strategies  = (C , P ) that, beginning at any time
t and given any public history ht1 , form a Nash equilibrium from that point on (Fudenberg,
Levine, & Maskin, 1994). Vit () is the continuation payoff to player i given by the strategy
profile .
G is a game with product structure since any public outcome can be expressed as a vector
of two components (yC , yP ) such that the distribution of yi depends only on the actions of
player i  {C, P }, the client, respectively the provider. For such games, Fudenberg et al.
(1994) establish a Folk Theorem proving that any feasible, individually rational payoff
profile is achievable as a PPE of G when the discount factor is close enough to 1. The set
of feasible, individually rational payoff profiles is characterized by:
 the minimax payoff to the client, obtained by the option out: VC = u  p(1 + );
403

fiJurca & Faltings

VC
(in1q01q1 ; e 1lq0d q1 )

(u -p)

(in1q01q1; e 1dq0d q1 )

u-p
pareto optimal
frontier

u -p(1+)

0

p-c


p-c


VP
p

VP

-p
(in1q01q1; e 0d)

Figure 2: The pareto-optimal frontier of the set of feasible, individually rational payoff
profiles of G.

 the minimax payoff to the provider, obtained when the provider plays e0 l: VP = 0;
 the pareto optimal frontier (graphically presented in Figure 2) delimited by the
payoffs given by (linear combination of) the strategy profiles (in1q0 1q1 , e1 lq0 dq1 ),
(in1q0 1q1 , e1 dq0 dq1 ) and (in1q0 1q1 , e0 d).
and contains more than one point (i.e., the payoff when the client plays out) when (up) >
u  p(1 + ) and p  c > 0. Both conditions impose restrictions on the minimum margin
generated by a transaction such that the interaction is profitable. The PPE payoff profile
that gives the provider the maximum payoff is (VC , VP ) where:
(
VP =

  u  c  u + p(1 + ) if  
p + c(pu)
if  >
u

u(1)
p
u(1)
p

and VC is defined above.
While completely characterizing the set of PPE payoffs for discount factors strictly
smaller than 1 is outside the scope of this paper, let us note the following results:
First, if the discount factor is high enough (but strictly less than 1) with respect to the
profit margin obtained by the provider from one interaction, there is at least one PPE such
that the reputation mechanism records only honest reports. Moreover, this equilibrium is
pareto-optimal.
Proposition 1 When  >

p
p(1+)c ,

the strategy profile:

 the provider always exerts high effort, and delivers only high quality; if the client
deviates from the equilibrium , the provider switches to e0 d for the rest of the rounds;
404

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

 the client always reports 1 when asked to submit feedback; if the provider deviates,
(i.e., she receives low quality), the client switches to out for the rest of the rounds.
is a pareto-optimal PPE.
Proof. It is not profitable for the client to deviate from the equilibrium path. Reporting
0 attracts the penalty  in the present round, and the termination of the interaction with
the provider (the provider stops exerting effort from that round onwards).
The provider, on the other hand, can momentarily gain by deviating to e1 dq0 dq1 or e0 d.
A deviation to e1 dq0 dq1 gives an expected momentary gain of p(1  ) and an expected
continuation loss of (1  )(p  c). A deviation to e0 d brings an expected momentary
gain equal to (1  )p + c and an expected continuation loss of p  c. For the discount
factor satisfying our hypothesis, both deviations are not profitable. The discount factor
is low enough with respect to profit margins, such that the future revenues given by the
equilibrium strategy offset the momentary gains obtained by deviating.
The equilibrium payoff profile is (VC , VP ) = ((u  p), p  c), which is pareto-optimal
and socially efficient.

Second, we can prove that the client never reports negative feedback in any paretooptimal PPE, regardless the value of the discount factor. The restriction to pareto-optimal
is justifiable by practical reasons: assuming that the client and the provider can somehow
negotiate the equilibrium they are going to play, it makes most sense to choose one of the
pareto-optimal equilibria.
Proposition 2 The probability that the client reports negative feedback on the equilibrium
path of any pareto-optimal PPE strategy is zero.
Sketch of Proof. The full proof presented in Appendix A follows the following steps.
Step 1, all equilibrium payoffs can be expressed by adding the present round payoff to the
discounted continuation payoff from the next round onward. Step 2, take the PPE payoff
profile V = (VC , VP ), such that there is no other PPE payoff profile V 0 = (VC0 , VP ) with
VC < VC0 . The client never reports negative feedback in the first round of the equilibrium
that gives V . Step 3, the equilibrium continuation payoff after the first round also satisfies
the conditions set for V . Hence, the probability that the client reports negative feedback on
the equilibrium path that gives V is 0. Pareto-optimal PPE payoff profiles clearly satisfy
the definition of V , hence the result of the proposition.

The third result we want to mention here, is that there is an upper bound on the
percentage of false reports recorded by the reputation mechanism in any of the paretooptimal equilibria.
Proposition 3 The upper bound on the percentage of false reports recorded by the reputation mechanism in any PPE equilibrium is:
(


(1)(pu)+p
p

p
u

405

if p  u(1  );
if p > u(1  )

(3)

fiJurca & Faltings

Sketch of Proof. The full proof presented in Appendix B builds directly on the result of
Proposition 2. Since clients never report negative feedback along pareto-optimal equilibria,
the only false reports recorded by the reputation mechanism appear when the provider
delivers low quality, and the client reports positive feedback. However, any PPE profile
must give the client at least VC = u  p(1 + ), otherwise the client is better off by resorting
to the outside option. Every round in which the provider deliberatively delivers low quality
gives the client a payoff strictly smaller than u  p(1 + ). An equilibrium payoff greater
than VC is therefore possible only when the percentage of rounds where the provider delivers
low quality is bounded. The same bound limits the percentage of false reports recorded by
the reputation mechanism.

For a more intuitive understanding of the results presented in this section, let us refer
to the pizza delivery example detailed in Section 3.1. The price of a home delivered pizza is
p = 1, while at the local restaurant the same pizza would cost p(1 + ) = 1.2. The utility of
a warm pizza to the client is u = 2, the cost of delivery is c = 0.8 and the probability that
unexpected traffic conditions delay the delivery beyond the 30 minutes deadline (despite
the best effort of the provider) is 1   = 0.01.
The client can secure a minimax payoff of VC = u  p(1 + ) = 0.8 by always going
out to the restaurant. However, the socially desired equilibrium happens when the client
orders pizza at home, and the pizza service exerts effort to deliver pizza in time: in this
case the payoff of the client is VC = (u  p) = 0.99, while the payoff of the provider is
VP = p  c = 0.19.
Proposition 1 gives a lower bound on the discount factor of the pizza delivery service
such that repeated clients can expect the socially desired equilibrium. This bound is  =
p
p(1+)c = 0.84; assuming that the daily discount factor of the pizza service is  = 0.996,
the same client must order pizza at home at least once every 6 weeks. The values of the
discount factors can also be interpreted in terms of the minimum number of rounds the
client (and the provider) will likely play the game. For example, the discount factor can be
viewed as the probability that the client (respectively the provider) will live for another
interaction in the market. It follows that the average lifetime of the provider is at least
1/(1  ) = 250 interactions (with all clients), while the average lifetime of the client is at
least 1/(1  ) = 7 interactions (with the same pizza delivery service). These are clearly
realistic numbers.
Proposition 3 gives an upper bound on the percentage of false reports that our mechanism may record in equilibrium from the clients. As u(1  ) = 0.02 < 0.2 = p, this limit
is:
=

p
= 0.1;
u

It follows that at least 90% of the reports recorded by our mechanism (in any equilibrium)
are correct. The false reports (false positive reports) result from rare cases where the pizza
delivery is intentionally delayed to save some cost but clients do not complain. The false
report can be justified, for example, by the providers threat to refuse future orders from
clients that complain. Given that late deliveries are still rare enough, clients are better off
with the home delivery than with the restaurant, hence they accept the threat. As other
options become available to the clients (e.g., competing delivery services) the bound  will
decrease.
406

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

Please note that the upper bound defined by Proposition 3 only depends on the outside
alternative available to the provider, and is not influenced by the punishment  introduced
by the reputation mechanism. This happens because the revenue of a client is independent
of the interactions of other clients, and therefore, on the reputation information as reported
by other clients. Equilibrium strategies are exclusively based on the direct experience of
the client. In the following section, however, we will refine this bound by considering that
clients can build a reputation for reporting honestly. There, the punishment  plays an
important role.

5. Building a Reputation for Truthful Reporting
An immediate consequence of Propositions 2 and 3 is that the provider can extract all of the
surplus created by the transactions by occasionally delivering low quality, and convincing
the clients not to report negative feedback (providers can do so by promising sufficiently high
continuation payoffs that prevent the client to resort to the outside provider). Assuming
that the provider has more power in the market, he could influence the choice of the
equilibrium strategy to one that gives him the most revenue, and holds the clients close to
the minimax payoff VC = u  p(1 + ) given by the outside option.5
However, a client who could commit to report honestly, (i.e., commit to play the strategy
sC = in0q0 1q1 ) would benefit from cooperative trade. The providers best response against
sC is to play e1 lq0 dq1 repeatedly, which leads the game to the socially efficient outcome.
Unfortunately the commitment to sC is not credible in the complete information game, for
the reasons explained in Section 4.2.
Following the results of Kreps et al. (1982), Fudenberg and Levine (1989) and Schmidt
(1993) we know that such honest reporting commitments may become credible in a game
with incomplete information. Suppose that the provider has incomplete information in G ,
and believes with non-negative probability that he is facing a committed client that always
reports the truth. A rational client can then fake the committed client, and build a
reputation for reporting honestly. When the reputation becomes credible, the provider
will play e1 lq0 dq1 (the best response against sC ), which is better for the client than the
payoff she would obtain if the provider knew she was the rational type.
As an effect of reputation building, the set of equilibrium points is reduced to a set
where the payoff to the client is higher than the payoff obtained by a client committed to
report honestly. As anticipated from Proposition 3, a smaller set of equilibrium points also
reduces the bound of false reports recorded by the reputation mechanism. In certain cases,
this bound can be reduced to almost zero.
Formally, incomplete information can be modeled by a perturbation of the complete
information repeated game G such that in period 0 (before the first round of the game is
played) the type of the client is drawn by nature out of a countable set  according to
the probability measure . The clients payoff now additionally depends on her type. We
5. All pareto-optimal PPE payoff profiles are also renegotiation-proof (Bernheim & Ray, 1989; Farrell &
Maskin, 1989). This follows from the proof of Proposition 3: the continuation payoffs enforcing a paretooptimal PPE payoff profile are also pareto-optimal. Therefore, clients falsely report positive feedback
even under the more restrictive notion of negotiation-proof equilibrium.

407

fiJurca & Faltings

say that in the perturbed game G () the provider has incomplete information because he
is not sure about the true type of the client.
Two types from  have particular importance:
 The normal type of the client, denoted by 0 , is the rational client who has the
payoffs presented in Figure 1.
 The commitment type of the client, denoted by  , always prefers to play the commitment strategy sC . From a rational perspective, the commitment type client obtains
an arbitrarily high supplementary reward for reporting the truth. This external reward makes the strategy sC the dominant strategy, and therefore, no commitment
type client will play anything else than sC .
In Theorem 1 we give an upper bound kP on the number of times the provider delivers
low quality in G (), given that he always observes the client reporting honestly.
The intuition behind this result is the following. The providers best response to a
honest reporter is e1 lq0 dq1 : always exert high effort, and deliver only when the quality is
high. This gives the commitment type client her maximum attainable payoff in G (),
corresponding to the socially efficient outcome. The provider, however, would be better off
by playing against the normal type client, against whom he can obtain an expected payoff
greater than p  c.
The normal type client may be distinguished from a commitment type client only in
the rounds when the provider delivers low quality: the commitment type always reports
negative feedback, while the normal type might decide to report positive feedback in order
to avoid the penalty . The provider can therefore decide to deliver low quality to the client
in order to test her real type. The question is, how many times should the provider test
the true type of the client.
Every failed test (i.e., the provider delivers low quality and the client reports negative
feedback) generates a loss of  to the provider, and slightly enforces the belief that the
client reports honestly. Since the provider cannot wait infinitely for future payoffs, there
must be a time when the provider will stop testing the type of the provider, and accepts to
play the socially efficient strategy, e1 lq0 dq1 .
The switch to the socially efficient strategy is not triggered by a revelation of the clients
type. The provider believes that the client behaves as if she were a commitment type, not
that the client is a commitment type. The client may very well be a normal type who
chooses to mimic the commitment type, in the hope that she will obtain better service from
the provider. However, further trying to determine the true type of the client is too costly
for the provider. Therefore, the provider chooses to play e1 lq0 dq1 , which is the best response
to the commitment strategy sC .
Theorem 1 If the provider has incomplete information in G , and assigns positive probability to the normal and commitment type of the client ((0 ) > 0, 0 = ( ) > 0), there
is a finite upper bound, kP , on the number of times the provider delivers low quality in any
equilibrium of G (). This upper bound is:



kP = 





ln(0 )


P p+c)+(1)p
ln (V
(VP p+c)+(1)

408

(4)

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

Proof. First, we use an important result obtained by Fudenberg and Levine (1989) about
statistical inference (Lemma 1): If every previously delivered low quality service was sanctioned by a negative report, the provider must expect with increasing probability that his
next low quality delivery will also be sanctioned by negative feedback. Technically, for any
 < 1, the provider can deliver at most n() low quality services (sanctioned by negative
feedback) before expecting that the n() + 1 low quality delivery will also be sanctioned by
negative feedback with probability greater then . This number equals to:



ln 
n() =
;
ln 

As stated earlier, this lemma does not prove that the provider will become convinced
that he is facing a commitment type client. It simply proves that after a finite number of
rounds the provider becomes convinced that the client is playing as if she were a commitment
type.
VP
Second, if  > V +(1)
but is strictly smaller than 1, the rational provider does
P
not deliver low quality (it is easy to verify that the maximum discounted future gain does
not compensate for the risk of getting a negative feedback in the present round). By the
previously mentioned lemma, it must be that in any equilibrium, the provider delivers low
quality a finite number of times.
Third, let us analyze the round, t, when the provider is about to deliver a low quality
service (play dq0 ) for the last time. If  is the belief of the provider that the client reports
honestly in round t, his expected payoff (just before deciding to deliver the low quality
service) can be computed as follows:
 with probability  the client reports 0. Her reputation for reporting honestly becomes
credible, so the provider plays e1 lq0 dq1 in all subsequent rounds. The provider gains
p   in the current round, and expects p  c for the subsequent rounds;
 with probability 1, the client reports 1 and deviates from the commitment strategy,
the provider knows he is facing a rational client, and can choose a continuation PPE
strategy from the complete information game. He gains p in the current round, and
expects at most VP in the subsequent rounds;
VP  (1  )(p   ) + ((p  c) + (1  )VP )

On the other hand, had the provider acknowledged the low quality and rolled back the
transaction (i.e., play lq0 ), his expected payoff would have been at least:
VP0  (1  )0 + (p  c)

Since the provider chooses nonetheless to play dq0 it must be that VP  VP0 which is
equivalent to:
=

(VP  p + c) + (1  )p
(VP  p + c) + (1  )

409

(5)

fiJurca & Faltings

Finally, by replacing Equation (5) in the definition of n() we obtain the upper bound
on the number of times the provider delivers low quality service to a client committed to
report honestly.

The existence of kP further reduces the possible equilibrium payoffs a client can get in
G (). Consider a rational client who receives for the first time low quality. She has the
following options:
 report negative feedback and attempt to build a reputation for reporting honestly.
Her payoff for the current round is p  . Moreover, her worst case expectation for
the future is that the next kP  1 rounds will also give her p  , followed by the
commitment payoff equal to (u  p):
VC |0 = (1  )(p  ) + (1   kP 1 )(p  ) +  kP (u  p);

(6)

 on the other hand, by reporting positive feedback she reveals to be a normal type,
loses only p in the current round, and expects a continuation payoff equal to VC given
by a PPE strategy profile of the complete information game G :
VC |1 = (1  )(p) +  VC ;

(7)

The reputation mechanism records false reports only when clients do not have the incentive to build a reputation for reporting honestly, and VC |1 > VC |0; this is true for:
VC >  kP 1 (u  p)  (1   kP 1 )(p + ) 

1
;


Following the argument of Proposition 3 we can obtain a bound on the percentage of
false reports recorded by the reputation mechanism in a pareto-optimal PPE that gives the
client at least VC :
(
 =

(up)VC
p
upVC
u

if VC  u  p;
if VC < u  p

(8)

Of particular importance is the case when kP = 1. VC and  become:
VC = (u  p) 

1
;


 =

(1  )
;
p

(9)

so the probability of recording a false report (after the first one) can be arbitrarily close to
0 as   0.
For the pizza delivery example introduced in Section 3.1, Figure 3 plots the bound, kP ,
defined in Theorem 1, as a function of the prior belief (0 ) of the provider that the client
is an honest reporter. We have used a value of the discount factor equal to  = 0.95, such
that on average, every client interacts 1/(1  ) = 20 times with the same provider. The
penalty for negative feedback was taken  = 2.5. When the provider believes that 20% of
410

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

10

9

8

7

kP

6

5

4

3

2

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0

Figure 3: The upper bound kP as a function of the prior belief 0 .
the clients always report honestly, he will deliver at most 3 times low quality. When the
belief goes up to 0 = 40% no rational provider will deliver low quality more than once.
In Figure 4 we plot the values of the bounds  (Equation (3)) and  (Equation (8)) as
a function of the prior belief 0 . The bounds simultaneously hold, therefore the maximum
percentage of false reports recorded by the reputation mechanism is the minimum of the
two. When 0 is less 0.25, kP  2,   , and the reputation effect does not significantly
reduce the worst case percentage of false reports recorded by the mechanism. However, when
0  (0.25, 0.4) the reputation mechanism records (in the worst case) only half as many false
reports, and as 0 > 0.4, the percentage of false reports drops to 0.005. This probability
can be further decreased by decreasing the penalty . In the limit, as  approaches 0, the
reputation mechanism will register a false report with vanishing probability.
The result of Theorem 1 has to be interpreted as a worst case scenario. In real markets,
providers that already have a small predisposition to cooperate will defect fewer times.
Moreover, the mechanism is self enforcing, in the sense that the more clients act as commitment types, the higher will be the prior beliefs of the providers that new, unknown clients
will report truthfully, and therefore the easier it will be for the new clients to act as truthful
reporters.
As mentioned at the end of Section 4.2, the bound  strongly depends on the punishment
 imposed by the reputation mechanism for a negative feedback. The higher , the easier it
is for clients to build a reputation, and therefore, the lower the amount of false information
recorded by the reputation mechanism.

6. The Threat of Malicious Clients
The mechanism described so far encourages service providers to do their best and deliver
good service. The clients were assumed rational, or committed to report honestly, and
411

fiJurca & Faltings

0.4



min(, )

0.35

0.3

, 

0.25

0.2

0.15

0.1

0.05

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0

Figure 4: The maximum probability of recording a false report as a function of the prior
belief 0 .

in either case, they never report negative feedback unfairly. In this section, we investigate
what happens when clients explicitly try to hurt the providers by submitting fake negative
ratings to the reputation mechanism.
An immediate consequence of fake negative reports is that clients lose money. However,
the costs  of a negative report would probably be too small to deter clients with separate agendas from hurting the provider. Fortunately, the mechanism we propose naturally
protects service providers from consistent attacks initiated by malicious clients.
Formally, a malicious type client,   , obtains a supplementary (external) payoff 
for reporting negative feedback. Obviously,  has to be greater than the penalty , otherwise
the results of Proposition 2 would apply. In the incomplete information game G (), the
provider now assigns non-zero initial probability to the belief that the client is malicious.
When only the normal type, 0 , the honest reporter type  and the malicious type
 have non-zero initial probability, the mechanism we describe is robust against unfair
negative reports. The first false negative report exposes the client as being malicious, since
neither the normal, nor the commitment type report 0 after receiving high quality. By
Bayes Law, the providers updated belief following a false negative report must assign
probability 1 to the malicious type. Although providers are not allowed to refuse service
requests, they can protect themselves against malicious clients by playing e0 l: i.e., exert
low effort and reimburse the client afterwards. The RM records neutral feedback in this
case, and does not sanction the provider. Against e0 l, malicious clients are better off by
quitting the market (opt out), thus stopping the attack. The RM records at most one false
negative report for every malicious client, and assuming that identity changes are difficult,
providers are not vulnerable to unfair punishments.
412

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

When other types (besides 0 ,  and  ) have non-zero initial probability, malicious
clients are harder to detect. They could masquerade client types that are normal, but
accidentally misreport. It is not rational for the provider to immediately exclude (by playing
e0 l) normal clients that rarely misreport: the majority of the cooperative transactions
rewarded by positive feedback still generate positive payoffs. Let us now consider the
client type 0 ()   that behaves exactly like the normal type, but misreports 0 instead
of 1 independently with probability . When interacting with the client type 0 (), the
provider receives the maximum number of unfair negative reports when playing the efficient
equilibrium: i.e., e1 lq0 dq1 . In this case, the providers expected payoff is:
VP = p  c   ;

Since VP has to be positive (the minimax payoff of the provider is 0, given by e0 l), it must
be that   pc
 .
The maximum value of  is also a good approximation for the maximum percentage
of false negative reports the malicious type can submit to the reputation mechanism. Any
significantly higher number of harmful reports exposes the malicious type and allows the
provider to defend himself.
Note, however, that the malicious type can submit a fraction  of false reports only
when the type 0 () has positive prior probability. When the provider does not believe that
a normal client can make so many mistakes (even if the percentage of false reports is still
low enough to generate positive revenues) he attributes the false reports to a malicious type,
and disengages from cooperative behavior. Therefore, one method to reduce the impact of
malicious clients is to make sure that normal clients make few or no mistakes. Technical
means (for example by providing automated tools for formatting and submitting feedback),
or improved user interfaces (that make it easier for human users to spot reporting mistakes)
will greatly limit the percentage of mistakes made by normal clients, and therefore, also
reduce the amount of harm done by malicious clients.
One concrete method for reducing mistakes is to solicit only negative feedback from the
clients (the principle that no news is good news, also applied by Dellarocas (2005)). As
reporting involves some conscious decision, mistakes will be less frequent. On the other
hand, the reporting effort will add to the penalty for a negative report, and makes it harder
for normal clients to establish a reputation for honest reporters. Alternative methods for
reducing the harm done by malicious clients (like filtering mechanisms, etc., ) as well as
tighter bounds on the percentage of false reports introduced by such clients will be further
addressed in future work.

7. Discussion and Future Work
Further benefits can be obtained if the clients reputation for reporting honestly is shared
within the market. The reports submitted by a client while interacting with other providers
will change the initial beliefs of a new provider. As we have seen in Section 5, providers
cheat less if they a priory expect with higher probability to encounter honest reporting
clients. A client that has once built a reputation for truthfully reporting the providers
behavior will benefit from cooperative trade during her entire lifetime, without having to
convince each provider separately. Therefore the upper bound on the loss a client has to
withstand in order to convince a provider that she is a commitment type, becomes an upper
413

fiJurca & Faltings

bound on the total loss a client has to withstand during her entire lifetime in the market.
How to effectively share the reputation of clients within the market remains an open issue.
Correlated with this idea is the observation that clients that use our mechanism are
motivated to keep their identity. In generalized markets where agents are encouraged to
play both roles (e.g. a peer-2-peer file sharing market where the fact that an agent acts
only as provider can be interpreted as a strong indication of double identity with the
intention of cheating) our mechanism also solves the problem signaled by Friedman and
Resnick (2001) related to cheap online pseudonyms. The price to pay for the new identity
is the loss due to building a reputation as truthful reporter when acting as a client.
Unlike incentive-compatible mechanism that pay reporters depending on the feedback
provided by peers, the mechanism described here is less vulnerable to collusion. The only
reason individual clients would collude is to badmouth (i.e., artificially decrease the reputation of) a provider. However, as long as the punishment for negative feedback is not
super-linear in the number of reports (this is usually the case), coordinating within a coalition brings no benefits for the colluders: individual actions are just as effective as the actions
when part of a coalition. The collusion between the provider and client can only accelerate the synchronization of strategies on one of the PPE profiles (collusion on a non-PPE
strategy profile is not stable), which is rather desirable. The only profitable collusion can
happen when competitor providers incentivize normal clients to unfairly downrate their
current provider. Colluding clients become malicious in this case, and the limits on the
harm they can do are presented in Section 6.
The mechanism we describe here is not a general solution for all online markets. In
general retail e-commerce, clients dont usually interact with the same service provider more
than once. As we have showed along this paper, the assumption of a repeated interaction
is crucial for our results. Nevertheless, we believe there are several scenarios of practical
importance that do meet our requirements (e.g., interactions that are part of a supply chain).
For these, our mechanism can be used in conjunction with other reputation mechanisms to
guarantee reliable feedback and improve the overall efficiency of the market.
Our mechanism can be further criticized for being centralized. The reputation mechanism acts as a central authority by supervising monetary transactions, collecting feedback
and imposing penalties on the participants. However, we see no problem in implementing
the reputation mechanism as a distributed system. Different providers can use different
reputation mechanisms, or, can even switch mechanisms given that some safeguarding measures are in place. Concrete implementations remain to be addressed by future work.
Although we present a setting where the service always costs the same amount, our
results can be easily extended to scenarios where the provider may deliver different kinds
of services, having different prices. As long as the provider believes that requests are
randomly drawn from some distribution, the bounds presented above can be computed
using the average values of u, p and c. The constraint on the providers belief is necessary
in order to exclude some unlikely situations where the provider cheats on a one time high
value transaction, knowing that the following interactions carry little revenue, and therefore,
cannot impose effective punishments.
In this paper, we systematically overestimate the bounds on the worst case percentage
of false reports recorded by the mechanism. The computation of tight bounds requires a
precise quantitative description of the actual set of PPE payoffs the client and the provider
414

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

can have in G . Fudenberg et al. (1994) and Abreu, Pearce, and Stacchetti (1990) pose
the theoretical grounds for computing the set of PPE payoffs in an infinitely repeated game
with discount factors strictly smaller than 1. However, efficient algorithms that allow us to
find this set are still an open question. As research in this domain progresses, we expect to
be able to significantly lower the upper bounds described in Sections 4 and 5.
One direction of future research is to study the behavior of the above mechanism when
there is two-sided incomplete information: i.e. the client is also uncertain about the type
of the provider. A provider type of particular importance is the greedy type who always
likes to keep the client to a continuation payoff arbitrarily close to the minimal one. In this
situation we expect to be able to find an upper bound kC on the number of rounds in which a
rational client would be willing to test the true type of the provider. The condition kP < kC
describes the constraints on the parameters of the system for which the reputation effect
will work in the favor of the client: i.e. the provider will give up first the psychological
war and revert to a cooperative equilibrium.
The problem of involuntary reporting mistakes briefly mentioned in Section 6 needs
further addressing. Besides false negative mistakes (reporting 0 instead of 1), normal clients
can also make false positive mistakes (report 1 instead of the intended 0). In our present
framework, one such mistake is enough ro ruin the reputation of a normal type client to
report honestly. This is one of the reasons why we chose a sequential model where the
feedback of the client is not required if the provider acknowledges low quality. Once the
reputation of the client becomes credible, the provider always rolls back the transactions that
generate (accidentally or not) low quality, so the client is not required to continuously defend
her reputation. Nevertheless, the consequences of reporting mistakes in the reputation
building phase must be considered in more detail. Similarly, mistakes made by the provider,
monitoring and communication errors will also influence the results presented here.
Last, but not the least, practical implementations of the mechanism we propose must
address the problem of persistent online identities. One possible attack created by easy
identity changes has been mentioned in Section 6: malicious buyers can continuously change
identity in order to discredit the provider. In another attack, the provider can use fake
identities to increase his revenue. When punishments for negative feedback are generated
endogenously by decreased prices in a fixed number of future transactions (e.g., Dellarocas,
2005), the provider can adopt the following strategy: he cheats on all real customers, but
generates a sufficient number of fake transactions in between two real transactions, such
that the effect created by the real negative report disappears. An easy fix to this latter
attack is to charge transaction or entrance fees. However, these measures also affect the
overall efficiency of the market, and therefore, different applications will most likely need
individual solutions.

8. Conclusions
Effective reputation mechanisms must provide appropriate incentives in order to obtain
honest feedback from self-interested clients. For environments characterized by adverse
selection, direct payments can explicitly reward honest information by conditioning the
amount to be paid on the information reported by other peers. The same technique unfortunately does not work when service providers have moral hazard, and can individually
415

fiJurca & Faltings

decide which requests to satisfy. Sanctioning reputation mechanisms must therefore use
other mechanisms to obtain reliable feedback.
In this paper we describe an incentive-compatible reputation mechanism when the clients
also have a repeated presence in the market. Before asking feedback from the clients, we
allow the provider to acknowledge failures and reimburse the price paid for service. When
future transactions generate sufficient profit, we prove that there is an equilibrium where
the provider behaves as socially desired: he always exerts effort, and reimburses clients that
occasionally receive bad service due to uncontrollable factors. Moreover, we analyze the
set of pareto-optimal equilibria of the mechanism, and establish a limit on the maximum
amount of false information recorded by the mechanism. The bound depends both on the
external alternatives available to clients and on the ease with which they can commit to
reporting the truth.

Appendix A. Proof of Proposition 2
The probability that the client reports negative feedback on the equilibrium path of any
pareto-optimal PPE strategy is zero.
Proof.
Step 1. Following the principle of dynamic programming (Abreu et al., 1990), the payoff
profile V = (VC , VP ) is a PPE of G , if and only if there is a strategy profile  in G, and
the continuation PPE payoffs profiles {W (y)|y  Y } of G , such that:
 V is obtained by playing  in the current round, and a PPE strategy that gives W (y)
as a continuation payoff, where y is the public outcome of the current round, and
P r[y|] is the probability of observing y after playing :
VC = (1  )gC () + 

X
yY

VP = (1  )gP () + 

X


P r[y|]  WC (y) ;

P r[y|]  WP (y) ;

yY

 no player finds it profitable to deviate from :
X

 0



0
VC  (1  )gC (C
, P ) + 
P r y|(C
, P )  WC (y) ;

0
C
6= C

yY

X





VP  (1  )gP (C , P0 ) + 
P r y|(C , P0 )  WP (y) ;

P0 6= P

yY

The strategy  and the payoff profiles {W (y)|y  Y } are said to enforce V .
Step 2. Take the PPE payoff profile V = (VC , VP ), such that there is no other PPE
payoff profile V 0 = (VC0 , VP ) with VC < VC0 . Let  and {W (y)|y  Y } enforce V , and assume
that  assigns positive probability 0 = P r[q0 0|] > 0 to the outcome q0 0. If 1 = P r[q0 1|]
(possibly equal to 0), let us consider:
0 ,  ) where  0 is obtained from  by asking the client
 the strategy profile  0 = (C
P
C
C
to report 1 instead of 0 when she receives low quality (i.e., q0 );

416

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

 the continuation payoffs {W 0 (y)|y  Y } such that Wi0 (q0 1) = 0 Wi (q0 0) + 1 Wi (q0 1)
and Wi0 (y 6= q0 1) = Wi (y) for i  {C, P }. Since, the set of correlated PPE payoff
profiles of G is convex, if W (y) are PPE payoff profiles, so are W 0 (y).
The payoff profile (VC0 , VP ), VC0 = VC + (1  )0  is a PPE equilibrium profile because
it can be enforced by  0 and {W 0 (y)|y  Y }. However, this contradicts our assumption that
VC0 < VC , so P r[q0 0|] must be 0. Following exactly the same argument, we can prove that
P r[q1 0|] = 0.
Step 3. Taking V ,  and {W (y)|y  Y } from step 2, we have:
VC = (1  )gC () + 

X


P r[y|]  WC (y) ;

(10)

yY

If no other PPE payoff profile V 0 = (VC0 , VP ) can have VC0 > VC , it must be that the
continuation payoffs W (y) satisfy the same property. (Assume otherwise that there is a
PPE (WC0 (y), WP (y)) with WC0 (y) > WC (y). Replacing WC0 (y) in (10) we obtain V 0 that
contradicts the hypothesis).
By continuing the recursion, we obtain that the client never reports 0 on the equilibrium
path that enforces a payoff profile as defined in Step 2. Pareto-optimal payoff profiles clearly
enter this category, hence the result of the proposition.


Appendix B. Proof of Proposition 3
The upper bound on the percentage of false reports recorded by the reputation mechanism
in any PPE equilibrium is:
(


(1)(pu)+p
p
p
u

if p  u(1  );
if p > u(1  )

Proof. Since clients never report negative feedback along pareto-optimal equilibria,
the only false reports recorded by the reputation mechanism appear when the provider
delivers low quality, and the client reports positive feedback. Let  = (C , P ) be a paretooptimal PPE strategy profile.  induces a probability distribution over public histories
and, therefore, over expected outcomes in each of the following transactions. Let t be the
probability distribution induced by  over the outcomes in round t. t (q0 0) = t (q1 0) = 0
as proven by Proposition 2. The payoff received by the client when playing  is therefore:
VC ()  (1  )


X



 t t (q0 1)(p) + t (q1 1)(u  p) + t (l)0 + t (out)(u  p  p) ;

t=0

where t (q0 1)+t (q1 1)+t (l)+t (out) = 1 and t (q0 1)+t (l)  (1)t (q1 1)/, because
the probability of q0 is at least (1  )/ times the probability of q1 .
When the discount factor, , is the probability that the repeated interaction will stop
after each transaction, the expected probability of the outcome q0 1 is:
 = (1  )


X
t=0

417

 t t (q0 1);

fiJurca & Faltings

Since any PPE profile must give the client at least VC = up(1+), (otherwise the client
is better off by resorting to the outside option), VC ()  VC . By replacing the expression
of VC (), and taking into account the constraints on the probability of q1 we obtain:


(p) + (u  p)  min 1  ,   VC ;
(


(1)(pu)+p
p

p
u

if p  u(1  );
if p > u(1  )


References
Abreu, P., Pearce, D., & Stacchetti, E. (1990). Toward a Theory of Discounted Repeated
Games with Imperfect Monitoring. Econometrica, 58 (5), 1041  1063.
Bernheim, B. D., & Ray, D. (1989). Collective Dynamic Consistency in Repeated Games.
Games and Economic Behavior, 1, 295326.
Birk, A. (2001). Learning to Trust. In Falcone, R., Singh, M., & Tan, Y.-H. (Eds.), Trust
in Cyber-societies, Vol. LNAI 2246, pp. 133144. Springer-Verlag, Berlin Heidelberg.
Biswas, A., Sen, S., & Debnath, S. (2000). Limiting Deception in a Group of Social Agents.
Applied Artificial Intelligence, 14, 785797.
Braynov, S., & Sandholm, T. (2002). Incentive Compatible Mechanism for Trust Revelation.
In Proceedings of the AAMAS, Bologna, Italy.
Cooke, R. (1991). Experts in Uncertainity: Opinion and Subjective Probability in Science.
Oxford University Press: New York.
Dellarocas, C. (2002). Goodwill Hunting: An Economically Efficient Online Feedback. In
Padget, J., & et al. (Eds.), Agent-Mediated Electronic Commerce IV. Designing Mechanisms and Systems, Vol. LNCS 2531, pp. 238252. Springer Verlag.
Dellarocas, C. (2005). Reputation Mechanism Design in Online Trading Environments with
Pure Moral Hazard. Information Systems Research, 16 (2), 209230.
Farrell, J., & Maskin, E. (1989). Renegotiation in Repeated Games. Games and Economic
Behavior, 1, 327360.
Friedman, E., & Resnick, P. (2001). The Social Cost of Cheap Pseudonyms. Journal of
Economics and Management Strategy, 10(2), 173199.
Fudenberg, D., & Levine, D. (1989). Reputation and Equilibrium Selection in Games with
a Patient Player. Econometrica, 57, 759778.
Fudenberg, D., Levine, D., & Maskin, E. (1994). The Folk Theorem with Imperfect Public
Information. Econometica, 62 (5), 9971039.
Harmon, A. (2004). Amazon Glitch Unmasks War of Reviewers. The New York Times.
Houser, D., & Wooders, J. (2006). Reputation in Auctions: Theory and Evidence from
eBay. Journal of Economics and Management Strategy, 15, 353369.
418

fiObtaining Reliable Feedback for Sanctioning Reputation Mechanisms

Jurca, R., & Faltings, B. (2006). Minimum Payments that Reward Honest Reputation
Feedback. In Proceedings of the ACM Conference on Electronic Commerce (EC06),
pp. 190199, Ann Arbor, Michigan, USA.
Kreps, D. M., Milgrom, P., Roberts, J., & Wilson, R. (1982). Rational Cooperation in the
Finitely Repeated Pisoners Dilemma. Journal of Economic Theory, 27, 245252.
Kreps, D. M., & Wilson, R. (1982). Reputation and Imperfect Information. Journal of
Economic Theory, 27, 253279.
Kuwabara, K. (2003). Decomposing Reputation Effects: Sanctioning or Signaling?. Working
paper.
Mailath, G., & Samuelson, L. (2006). Repeated Games and Reputations: Long-Run Relationships. Oxford University Press.
Milgrom, P., & Roberts, J. (1982). Predation, Reputation and Entry Deterrence. Journal
of Economic Theory, 27, 280312.
Miller, N., Resnick, P., & Zeckhauser, R. (2005). Eliciting Informative Feedback: The PeerPrediction Method. Management Science, 51, 1359 1373.
Papaioannou, T. G., & Stamoulis, G. D. (2005). An Incentives Mechanism Promoting
Truthful Feedback in Peer-to-Peer Systems. In Proceedings of IEEE/ACM CCGRID
2005.
Resnick, P., & Zeckhauser, R. (2002). Trust Among Strangers in Electronic Transactions:
Empirical Analysis of eBays Reputation System. In Baye, M. (Ed.), The Economics
of the Internet and E-Commerce, Vol. 11 of Advances in Applied Microeconomics.
Elsevier Science, Amsterdam.
Schillo, M., Funk, P., & Rovatsos, M. (2000). Using Trust for Detecting Deceitful Agents
in Artificial Societies. Applied Artificial Intelligence, 14, 825848.
Schmidt, K. M. (1993). Reputation and Equilibrium Characterization in Repeated Games
with Conflicting Interests. Econometrica, 61, 325351.
Selten, R. (1978). The Chain-Store Paradox. Theory and Decision, 9, 127159.
Yu, B., & Singh, M. (2002). An Evidential Model of Distributed Reputation Management.
In Proceedings of the AAMAS, Bologna, Italy.
Yu, B., & Singh, M. (2003). Detecting Deception in Reputation Management. In Proceedings
of the AAMAS, Melbourne, Australia.

419

fiJournal of Artificial Intelligence Research 29 (2007) 221267

Submitted 12/06; published 06/07

On the Formal Semantics of
Speech-Act Based Communication in an
Agent-Oriented Programming Language
Renata Vieira

renatav@unisinos.br

Universidade do Vale do Rio dos Sinos
Sao Leopoldo, RS, 93022-000, Brazil

Alvaro Moreira

Alvaro.Moreira@inf.ufrgs.br

Universidade Federal do Rio Grande do Sul
Porto Alegre, RS, 91501-970, Brazil

Michael Wooldridge

mjw@csc.liv.ac.uk

University of Liverpool
Liverpool L69 3BX, United Kingdom

Rafael H. Bordini

R.Bordini@durham.ac.uk

University of Durham
Durham DH1 3LE, United Kingdom

Abstract
Research on agent communication languages has typically taken the speech acts paradigm
as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial
agent systems. In particular, it has proved to be extremely difficult to give a satisfactory
semantics to speech-act based agent communication languages. In part, the problem is
that speech-act semantics typically make reference to the mental states of agents (their
beliefs, desires, and intentions), and there is in general no way to attribute such attitudes
to arbitrary computational agents. In addition, agent programming languages have only
had their semantics formalised for abstract, stand-alone versions, neglecting aspects such
as communication primitives. With respect to communication, implemented agent programming languages have tended to be rather ad hoc. This paper addresses both of these
problems, by giving semantics to speech-act based messages received by an AgentSpeak
agent. AgentSpeak is a logic-based agent programming language which incorporates the
main features of the PRS model of reactive planning systems. The paper builds upon a
structural operational semantics to AgentSpeak that we developed in previous work. The
main contributions of this paper are as follows: an extension of our earlier work on the
theoretical foundations of AgentSpeak interpreters; a computationally grounded semantics
for (the core) performatives used in speech-act based agent communication languages; and
a well-defined extension of AgentSpeak that supports agent communication.

1. Introduction
First introduced in 1987, the reactive planning model of Georgeff and Lanskys PRS system has subsequently proved to be one of the most influential and long-lived approaches to
programming multi-agent systems (Georgeff & Lansky, 1987). The AgentSpeak programming language, introduced by Rao (1996), represents an attempt to distill the key features
c
2007
AI Access Foundation. All rights reserved.

fiVieira, Moreira, Wooldridge, & Bordini

of the PRS approach into a simple, abstract, logic-based language. AgentSpeak is particularly interesting, in comparison to other agent-oriented languages, in that it retains the
most important aspects of the BDI-based reactive planning systems on which it was based,
and at the same time it has robust working interpreters (Bordini, Hubner, & Vieira, 2005;
Bordini & Hubner, 2007; Bordini, Bazzan, Jannone, Basso, Vicari, & Lesser, 2002), its formal semantics and relation to BDI logics (Rao & Georgeff, 1998; Wooldridge, 2000b) have
been thoroughly studied (Bordini & Moreira, 2004; Moreira, Vieira, & Bordini, 2004; Moreira & Bordini, 2002), and there is ongoing work on the use of model-checking techniques
for verification of AgentSpeak multi-agent systems (Bordini, Fisher, Visser, & Wooldridge,
2004; Bordini, Visser, Fisher, Pardavila, & Wooldridge, 2003; Bordini, Fisher, Pardavila, &
Wooldridge, 2003).
In the original formulation of AgentSpeak (Rao, 1996), the main emphasis was on the internal control structures and decision-making cycle of an agent: the issue of communication
between agents was not addressed. Accordingly, most attempts to give a formal semantics
to the language have focused on these internal aspects (Moreira & Bordini, 2002). Although
several extensions to AgentSpeak have been proposed in an attempt to make it a practically more useful language (Bordini et al., 2005, 2002), comparatively little research has
addressed the issue of a principled mechanism to support communication in AgentSpeak,
which is clearly essential for engineering multi -agent systems.
Most agent communication languages have taken speech-act theory (Austin, 1962; Searle,
1969) as their starting point. As is suggested by its name, speech-act theory is predicated
on the view that utterances are actions, performed by rational agents in the furtherance of
their personal desires and intentions. Thus, according to speech-act theory, utterances may
be considered as actions performed by an agent, typically with the intention of changing
the mental state of the hearer(s) of the utterance. Speech-act theory thus seems particularly appropriate as a foundation for communication among intentional agents. Through
communication, an agent can share its internal state (beliefs, desires, intentions) with other
agents, and can attempt to influence the mental states of other agents.
Although an initial speech-act based communication model for AgentSpeak agents was
previously introduced (Bordini et al., 2003), no formal semantics of that model was given
in that paper. A preliminary formal account for communication of AgentSpeak agents
was first given by Moreira et al. (2004). The main contribution of the present paper is
to thoroughly extend the operational semantics of AgentSpeak accounting for speech-act
style communication. Our semantics precisely defines how to implement the processing of
messages received by an AgentSpeak agent; that is, how the computational representations
of mental states are changed when a message is received. Note that in implementations of
the BDI architecture, the concepts of plan and plan library is used to simplify aspects of
deliberation and means-ends reasoning. Therefore, an AgentSpeak agent sends a message
whenever there is a communicative action in the body of an intended plan that is being
executed; such plans are typically written by an agent programmer.
As pointed out by Singh (1998), well-known approaches to agent communication focus
largely on the senders perspective, ignoring how a message should be processed and understood. This is the main aspect of agent communication that we consider in this paper. In
extending the operational semantics of AgentSpeak to account for inter-agent communication, we also touch upon another long-standing problem in the area of multi-agent systems:
222

fiSpeech-Act Based Communication in Agent Programming

the semantics of communication languages based on speech acts. The difficulty here is that,
taking their inspiration from attempts to develop a semantics of human speech acts, most
semantics for agent communication languages have defined the meaning of messages between agents with respect to the mental states of communication participants. While this
arguably has the advantage of remaining neutral on the actual internal structure of agents,
a number of authors have observed that this makes it impossible in general to determine
whether or not some program that claims to be implementing the semantics really is implementing it (Wooldridge, 1998; Singh, 1998). The problem is that if the semantics makes
reference to an agent believing (or intending a state satisfying) a certain proposition, there
is no way to ensure that any software using that communication language complies with
the underlying semantics of belief (or intention, or mental attitudes in general).
This is related to the fact that previous approaches attempt to give a programming
language independent semantics of agent communication. Our semantics, while developed
for one specific language, have the advantage of not relying on mechanisms  such as
abstractly defined mental states  that cannot be verified for real programs. We note that,
to the best of our knowledge, our work represents the first semantics given for a speech-act
style, knowledge level communication language that is used in a real system.
Since a precise notion of Belief-Desire-Intention has been given previously for
AgentSpeak agents (Bordini & Moreira, 2004), we can provide such a computationally
grounded (Wooldridge, 2000a) semantics of speech-act based communication for this language, making it possible to determine how an AgentSpeak agent interprets a particular
message when it is received. Note, however, that whether and how an agent acts upon
received communication depends on its plan library and its other circumstances at the time
the message is processed. Also, although our approach is tied to a particular language, it
can be usefully employed as a reference model for developing communication semantics and
implementing communication in other agent programming languages.
The remainder of this paper is organised as follows. Section 2 provides the general
background on PRS-style BDI architectures and speech-act based agent communication.
Section 3 presents AgentSpeak syntax and semantics  a much revised version of the syntax
and semantics of AgentSpeak presented by Moreira and Bordini (2002, 2004). Section 4
presents the speech-act based communication model for AgentSpeak agents, an extension
of the preliminary formal account given by Moreira et al. (2004). Section 5 illustrates
the semantics with an example of the semantic rules applied in a typical reasoning cycle.
In Section 6, we show how programmers can use our basic communication constructs to
develop some of the more elaborate forms of communication required by some multi-agent
applications (for example, ensuring that a belief is shared between two agents and keeping
track of the progress in the achievement of a delegated goal), and in Section 7 we give a
simple example of the use of our framework for proving properties of communicating agents.
Section 8 presents a discussion on applications and further developments for the language
presented in this paper. Conclusions and planned future work are given in the final section.

2. Background
The ability to plan seems to be one of the key components of rational action in humans.
Planning is the ability to take a goal, and from this goal generate a recipe (i.e., plan) for
223

fiVieira, Moreira, Wooldridge, & Bordini

action such that, if this recipe is followed (under favourable conditions), the goal will be
achieved. Accordingly, a great deal of research in artificial intelligence has addressed the
issue of automatic planning: the synthesis of plans by agents from first principles (Allen,
Hendler, & Tate, 1990). Unfortunately, planning is, like so many other problems in artificial
intelligence, prohibitively expensive in computational terms. While great strides have been
made in developing efficient automatic planning systems (Ghallab, Nau, & Traverso, 2004),
the inherent complexity of the process inevitably casts some doubt on whether it will be
possible to use plan-synthesis algorithms to develop plans at run-time in systems that
must operate under tight real-time constraints. Many researchers have instead considered
approaches that make use of pre-compiled plans, i.e., plans developed off-line, at design time.
The Procedural Reasoning System (PRS) of Georgeff and Lansky is a common ancestor of
many such approaches (Georgeff & Lansky, 1987).
2.1 The PRS and AgentSpeak
On one level, the PRS can be understood simply as an architecture for executing precompiled plans. However, the control structures in the architecture incorporate a number
of features which together provide a sophisticated environment for run-time practical reasoning. First, plans may be invoked by their effect, rather than simply by name (as is the
case in conventional programming languages). Second, plans are associated with a context,
which must match the agents current situation in order for the plan to be considered a
viable option. These two features mean that an agent may have multiple potential plans for
the same end, and can dynamically select between these at run-time, depending on current
circumstances. In addition, plans are associated with triggering events, the idea being that
a plan is made active by the occurrence of such an event, which may be external or internal to the agent. External events are changes in the environment as perceived by the agent;
an example of an internal event might be the creation of a new sub-goal, or the failure of
a plan to achieve its desired effect. Thus, overall, plans may be invoked in a goal-driven
manner (to satisfy a sub-goal that has been created) or in an event-driven manner. The
PRS architecture is illustrated in Figure 1. The AgentSpeak language, introduced by Rao
(1996), represents an attempt to distill the essential features of the PRS into a simple,
unified programming language1 ; we provide a detailed introduction to AgentSpeak below,
after we discuss speech-act theory and agent communication.
2.2 Speech Acts
The PRS model, and the AgentSpeak language in turn, are primarily concerned with the
internal structure of decision making, and in particular the interplay between the creation
of (sub-)goals and the execution of plans to achieve these (sub-)goals. The twin issues of
communication and multi-agent interaction are not addressed within the basic architecture.
This raises the question of how such issues might be dealt with within the architecture.
While BDI theory is based on the philosophical literature on practical reasoning (Bratman,
1. The name of the language originally introduced by Rao (1996) was AgentSpeak(L). In this paper, we
adopt the simpler form AgentSpeak instead, and we use it to refer both to the original language and the
variants that appeared in the literature.

224

fiSpeech-Act Based Communication in Agent Programming

Plan
Library

Beliefs

Sensor Input

Action Output

Interpreter

Goals

Intentions

Figure 1: The PRS architecture.

1987), agent communication in multi-agent systems is typically based on the speech-act
theory, in particular the work of Austin (1962) and Searle (1969).
Speech-act theory starts from the principle that language is action: a rational agent
makes an utterance in an attempt to change the state of the world, in the same way that
an agent performs physical actions to change the state of the world. What distinguishes
speech acts from other (non-speech) actions is that the domain of a speech act  the
part of the world that the agent wishes to modify through the performance of the act  is
mostly the mental state(s) of the hearer(s) of the utterance.
Speech acts are generally classified according to their illocutionary force  the type
of the utterance. In natural language, illocutionary forces are associated to utterances (or
locutionary acts). The utterance the door is open, for example, is generally an inform or
tell type of action. The perlocutionary force represents what the speaker of the utterance
is attempting to achieve by performing the act. In making a statement such as open
the door, the perlocutionary force will generally be the state of affairs that the speaker
hopes to bring about by making the utterance; of course, the actual effect of an utterance
will be beyond the control of the speaker. Whether I choose to believe you when you
inform me that the door is open depends upon how I am disposed towards you. In natural
language, the illocutionary force and perlocutionary force will be implicit within the speech
act and its context. When the theory is adapted to agent communication, however, the
225

fiVieira, Moreira, Wooldridge, & Bordini

illocutionary forces are made explicit to facilitate processing the communication act. The
various types of speech acts are generally referred to as performatives in the context of
agent communication.
Other pragmatic factors related to communication such as social roles and conventions have been discussed in the literature (Levinson, 1981; Ballmer & Brennenstuhl, 1981;
Singh, 1994). Illocutionary forces may require the existence of certain relationships between speaker and hearer for them to be felicitous. A command, for instance, requires
a subordination relation between the individuals involved in the communication, whereas
such subordination is not required in a request.
Apart from illocutionary forces and social roles, other classifications of the relations
among speech acts have been proposed (Levinson, 1981); for example, a reply follows a
question, and threatening is stronger than warning. Such categories place messages in the
larger context of a multi-agent dialogue. In multi-agent systems, communicative interactions can be seen as communication protocols, which in turn are normally related to a
specific coordination/cooperation mechanism. The Contract Net (Smith, 1980), for example, is a protocol for task allocation, which is defined in terms of a number of constituent
performatives (such as announcing and bidding).
2.3 Agent Communication Languages: KQML & FIPA
The Knowledge Query and Manipulation Language (KQML), developed in the context of the
Knowledge Sharing Effort project (Genesereth & Ketchpel, 1994), was the first attempt to
define a practical agent communication language that included high level (speech-act based)
communication as considered in the distributed artificial intelligence literature. KQML is
essentially a knowledge-level messaging language (Labrou & Finin, 1994; Mayfield, Labrou,
& Finin, 1996). KQML defines a number of performatives, which make explicit an agents
intentions in sending a message. For example, the KQML performative tell is used with
the intention of changing the receivers beliefs, whereas achieve is used with the intention
of changing the receivers goals. Thus the performative label of a KQML message explicitly
identifies the intent of the message sender.
The FIPA standard for agent communication2 was released in 2002. This standard is
closely based on KQML, being almost identical conceptually and syntactically, while differing in the performative set and certain details of the semantic framework (Labrou, Finin, &
Peng, 1999). These differences are not important for the purposes of this paper; when we
refer to traditional approaches to semantics of speech-act based inter-agent communication,
the reference applies to both equally. However, for historical reasons, we refer mainly to
KQML and the richer literature that can be found on its semantics.
2.4 The Semantics of Agent Communication Languages
Perhaps the first serious attempt to define the semantics of KQML was made by Labrou and
Finin (1994). Their work built on the pioneering work of Cohen and Perrault on an actiontheoretic semantics of natural language speech acts (Cohen & Perrault, 1979). The key
insight in Cohen and Perraults work was that, if we take seriously the idea of utterances as
2. http://www.fipa.org/specs/fipa00037/SC00037J.html

226

fiSpeech-Act Based Communication in Agent Programming

action, then we should be able to apply a formalism for reasoning about action to reasoning
about utterances. They used a STRIPS-style pre- and post-condition formalism to define
the semantics of inform and request speech acts (perhaps the canonical examples of
speech acts), where these pre- and post-conditions were framed in terms of the beliefs,
desires, and abilities of conversation participants. When applied by Labrou and Finin to
the KQML language (1994), the pre- and post-conditions defined the mental states of the
sender and receiver of a KQML message before and after sending such message. For the
description of mental states, most of the work in the area is based on Cohen and Levesques
theory of intention (1990a, 1990b). Agent states are described through mental attitudes
such as belief (bel), knowledge (know), desire (want), and intention (intend). These mental
attitudes normally have propositions (i.e., symbolic representations of states of the world)
as arguments. Figures 2 and 3 give semantics for the KQML performatives tell(S, R, X) (S
tells R that S believes that X is true), and ask if (S, R, X) (S asks R if R believes that X
is true), in the style introduced by Labrou and Finin (1994).
 Pre-conditions on the states of S and R:
 P re(S):

bel(S, X)  know(S, want(R, know(R, bel(S, X))))

 P re(R):

intend(R, know(R, bel(S, X)))

 Post-conditions on S and R:
 P os(S):

know(S, know(R, bel(S, X)))

 P os(R):

know(R, bel(S, X))

 Action completion:
 know(R, bel(S, X))

Figure 2: Semantics for tell (Labrou & Finin, 1994).

 Pre-conditions on the states of S and R:
 P re(S): want(S, know(S, Y ))  know(S, intend(R, process(R, M ))) where Y is
either bel(R, X) or bel(R, X) and M is ask-if (S, R, X)
 P re(R):

intend(R, process(R, M ))

 Post-conditions about R and S:
 P os(S):

intend(S, know(S, Y ))

 P os(R) : know(R, want(S, know(S, Y )))
 Action completion:
 know(S, Y )

Figure 3: Semantics for ask-if (Labrou & Finin, 1994).
227

fiVieira, Moreira, Wooldridge, & Bordini

As noted above, one of the key problems with this (widely used) approach to giving
semantics to agent communication languages is that there is no way to determine whether
or not any software component that uses such a communication language complies with the
semantics. This is because the semantics makes reference to mental states, and we have in
general no principled way to attribute such mental states to arbitrary pieces of software.
This is true of the semantic approaches to both KQML and FIPA, as discussed by both
Wooldridge (1998) and Singh (1998). As an example, consider a legacy software component
wrapped in an agent that uses KQML or FIPA to interoperate with other agents. One
cannot prove communication properties of such system, as there is no precise definition of
when the legacy system believes that (or intends to achieve a state of the world where)
some proposition is true. Our approach builds on the work of Bordini and Moreira (2004),
which presented a precise definition of what it means for an AgentSpeak agent to believe,
desire, or intend a certain formula; that approach is also adopted in our work on modelchecking for AgentSpeak (Bordini et al., 2004). As a consequence, we are able to successfully
and meaningfully apply speech act-style semantics to communication in AgentSpeak. The
drawback, of course, is that the approach is, formally, limited to AgentSpeak agents, even
though the same ideas can be used in work on semantics for other agent languages.

3. Syntax and Semantics of AgentSpeak
The AgentSpeak programming language was introduced by Rao (1996). It can be understood as a natural extension of logic programming for the BDI agent architecture, and
provides an elegant abstract framework for programming BDI agents. The BDI architecture is, in turn, perhaps one of the major approaches to the implementation of rational
practical reasoning agents (Wooldridge, 2000b).
An AgentSpeak agent is created by the specification of a set of beliefs forming the initial
belief base and a set of plans forming the plan library. An agents belief base is a set of
ground first-order predicates, which will change over time to represent the current state of
the environment as perceived by the agent.
AgentSpeak distinguishes two types of goals: achievement goals and test goals. Achievement and test goals are predicates (as with beliefs), prefixed with one of the operators !
and ?, respectively. Achievement goals state that the agent wants to achieve a state of
the world where the associated predicate is true; in practice, as we will see, this is done
by the execution of a plan. A test goal returns a unification for the associated predicate
with one of the agents beliefs; it fails if no such unification is possible. A triggering event
defines which events may initiate the execution of a plan. An event can be internal (when a
subgoal needs to be achieved), or external (when generated from belief updates as a result
of perceiving the environment). Additionally, with respect to the model of communication
in this paper, external events can be related to messages received from other agents. There
are two types of triggering events: those related to the addition (+) and deletion (-) of
mental attitudes (beliefs or goals).
Plans refer to the basic actions that an agent is able to perform on its environment. A
plan is formed by a triggering event, denoting the events for which that plan is relevant.
The triggering event is followed by a conjunction of belief literals representing a context for
the plan. The context must be a logical consequence of the agents current beliefs for the
228

fiSpeech-Act Based Communication in Agent Programming

+concert(A,V) : likes(A)
 !book tickets(A,V).
+!book tickets(A,V) : busy(phone)
 ?phone number(V,N);
call(N);
. . .;
!choose seats(A,V).

Figure 4: Examples of AgentSpeak plans.

plan to be applicable  one of the plans that are both relevant and applicable is chosen for
execution so as to handle a particular event. The remainder of the plan is a sequence of
basic actions or (sub-)goals that the agent has to achieve (or test) when the plan is executed.
Figure 4 shows some examples of AgentSpeak plans. The first plan tells us that, when
a concert is announced for artist A at venue V (so that, from perceiving the environment, a
belief concert(A,V) is added to the belief base), provided that the agent happens to like
artist A, it will have the new achievement goal of booking tickets for that concert. The
second plan tells us that whenever this agent adopts the goal of booking tickets for As
performance at V, provided it is the case that the telephone is not busy, it can execute a
plan consisting of retrieving from its belief base the telephone number of venue V (with
the test goal ?phone number(V,N)), performing the basic action call(N) (assuming that
making a phone call is one of the actions that the agent is able to perform), followed by
a certain protocol for booking tickets (indicated by . . .), which in this case ends with the
execution of a plan for choosing the seats for such performance at that particular venue.
Next, we formally present the syntax and semantics of AgentSpeak. Note that we do
not yet consider communication; we extend the semantics to deal with communication in
Section 4.

3.1 Abstract Syntax
The syntax of an AgentSpeak agent program ag is defined by the grammar below. In
AgentSpeak, an agent program is simply given by a set bs of beliefs and a set ps of plans.
The beliefs bs define the initial state of the agents belief base (i.e., the state of the belief base
when the agent starts running), and the plans ps form the agents plan library. The atomic
formul at of the language are predicates, where P is a predicate symbol and t1 , . . . , tn are
standard terms of first order logic. A belief is an atomic formula at with no variables; we
use b as a meta-variable for beliefs.
229

fiVieira, Moreira, Wooldridge, & Bordini

ag
bs
ps
p
te
ct
ct 1
h
h1
at
s
a
g
u

::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
|
::=
::=
::=
::=

bs ps
b1 . . . b n
p1 . . . p n
te : ct  h
+at
| at | +g
ct 1
| T
at
| at
| ct 1  ct 1
h1 ;T
| T
a
| g
| u
P(t1 , . . . , tn )
P(t1 , . . . , tn )[s1 , . . . , sm ]
percept | self | id
A(t1 , . . . , tn )
!at
| ?at
+b
| at

(n  0)
(n  1)
|

g

|
| h1 ;h1
(n  0)
(n  0, m > 0)
(n  0)

The grammar above gives an alternative definition for at, extending the conventional
syntactic form of predicates. The extension allows annotations to be associated with a
predicate; this is an extension of AgentSpeaks original syntax motivated by our work on
communication, which is discussed below in Section 3.2. For the time being, suffice it to say
that the idea is to annotate each atomic formula with its source: either a term id identifying
which agent previously communicated that information, self to denote beliefs created by
the agent itself (through belief update operations within a plan, as described below), or
percept to indicate that the belief was acquired through perception of the environment.
So, for example, if agent i has a belief concert(a, v)[j] in its belief base, this would mean
that agent j had previously informed agent i of concert(a, v)  in other words, that j
wanted i to believe that there will be a concert by a at v. Similarly, concert(a, v)[percept, j]
would mean that as concert at v is believed not only because j has informed agent i of
this, but because i itself also perceived this fact (e.g., by seeing a poster when walking past
the theatre).
A plan in AgentSpeak is given by p above, where te is the triggering event, ct is the plans
context, and h is sequence of actions, goals, or belief updates (which should be thought of
as mental notes created by the agent itself). We refer to te : ct as the head of the plan,
and h is its body. The set of plans of an agent is given by ps. Each plan has as part of
its head a formula ct that specifies the conditions under which the plan can be chosen for
execution.
A triggering event te can then be the addition or the deletion of a belief from an agents
belief base (denoted +at and at, respectively), or the addition or the deletion of a goal
(+g and g, respectively3 ). For plan bodies, we assume the agent has at its disposal a set of
actions and we use a as a meta-variable ranging over them. We are largely unconcerned here
with respect to exactly what such actions are. Actions are written using the same notation
3. Triggering events of the form g, in our approach, are used in practice for handling plan failure. Although
we have left this construct in the grammar, we have omitted the discussion and formalisation of plan
failure for clarity, as the focus in this paper is on the semantics of communication.

230

fiSpeech-Act Based Communication in Agent Programming

as predicates, except that an action symbol A is used instead of a predicate symbol. Goals
g can be either achievement goals (!at) or test goals (?at). Finally, +b and at (in the body
of a plan) represent operations for updating (u) the belief base by, respectively, adding or
removing beliefs; recall that an atomic formula must be ground if it is to be added to the
belief base.
3.2 Semantics
We define the semantics of AgentSpeak using operational semantics, a widely used method
for giving semantics to programming languages (Plotkin, 1981). The operational semantics is given by a set of rules that define a transition relation between configurations
hag, C, M, T, si where:
 An agent program ag is, as defined above, a set of beliefs bs and a set of plans ps.
 An agents circumstance C is a tuple hI, E, Ai where:
 I is a set of intentions {i, i0 , . . .}. Each intention i is a stack of partially instantiated plans.
 E is a set of events {(te, i), (te 0 , i0 ), . . .}. Each event is a pair (te, i), where te is
a triggering event and i is an intention  a stack of plans in case of an internal
event, or the empty intention T in case of an external event. When the belief
revision function (which is not part of the AgentSpeak interpreter but rather of
the agents overall architecture), updates the belief base, the associated events
 i.e., additions and deletions of beliefs  are included in this set. These are
called external events; internal events are generated by additions or deletions of
goals from plans currently executing.
 A is a set of actions to be performed in the environment.
 M is a tuple hIn, Out, SIi whose components characterise the following aspects of
communicating agents (note that communication is asynchronous):
 In is the mail inbox: the system includes all messages addressed to this agent
in this set. Elements of this set have the form hmid , id , ilf , cnti, where mid is a
message identifier, id identifies the sender of the message, ilf is the illocutionary
force of the message, and cnt its content: a (possibly singleton) set of AgentSpeak
predicates or plans, depending on the illocutionary force of the message.
 Out is where the agent posts messages it wishes to send; it is assumed that some
underlying communication infrastructure handles the delivery of such messages.
(We are not concerned with this infrastructure here.) Messages in this set have
exactly the same format as above, except that here id refers to the agent to
which the message is to be sent.
 SI is used to keep track of intentions that were suspended due to the processing
of communication messages; this is explained in more detail in the next section,
but the intuition is as follows: intentions associated with illocutionary forces that
231

fiVieira, Moreira, Wooldridge, & Bordini

require a reply from the interlocutor are suspended, and they are only resumed
when such reply has been received.
 It is useful to have a structure which keeps track of temporary information that may
be subsequently required within a reasoning cycle. T is a tuple hR, Ap, , , i with
such temporary information; these components are as follows:
 R is the set of relevant plans (for the event being handled).
 Ap is the set of applicable plans (the relevant plans whose contexts are true).
 , , and  record a particular intention, event, and applicable plan (respectively)
being considered along the execution of one reasoning cycle.
 The current step within an agents reasoning cycle is symbolically annotated by
s  {ProcMsg, SelEv, RelPl, ApplPl, SelAppl, AddIM, SelInt, ExecInt, ClrInt}. These labels stand for, respectively: processing a message from the agents mail inbox, selecting an event from the set of events, retrieving all relevant plans, checking which of
those are applicable, selecting one particular applicable plan (the intended means),
adding the new intended means to the set of intentions, selecting an intention, executing the selected intention, and clearing an intention or intended means that may
have finished in the previous step.
In the interests of readability, we adopt the following notational conventions in our
semantic rules:
 If C is an AgentSpeak agent circumstance, we write CE to make reference to the E
component of C, and similarly for other components of a configuration.
 We write T = (the underscore symbol) to indicate that there is no intention presently
being considered in that reasoning cycle. Similarly for T and T .
 We write i[p] to denote the intention that has plan p on top of intention i.
The AgentSpeak interpreter makes use of three selection functions that are defined by
the agent programmer. The selection function SE selects an event from the set of events CE ;
the selection function SAp selects one applicable plan given a set of applicable plans; and
SI selects an intention from the set of intentions CI (the chosen intention is then executed).
Formally, all the selection functions an agent uses are also part of its configuration (as is the
social acceptance function that we mention later when we formalise agent communication).
However, as they are defined by the agent programmer at design time and do not (in
principle) change at run time, we avoid including them in the configuration for the sake of
readability.
We define some functions which help simplify the semantics. If p is a plan of the form
te : ct  h, we define TrEv(p) = te and Ctxt(p) = ct. That is, these projection functions
return the triggering event and the context of the plan, respectively. The TrEv function can
also be applied to the head of a plan rather than the whole plan, but works similarly in
that case.
232

fiSpeech-Act Based Communication in Agent Programming

Next, we need to define the specific (limited) notion of logical consequence used here.
We assume a procedure that computes the most general unifier of two literals (as usual in
logic programming), and with this, define the logical consequence relation |= that is used
in the definitions of the functions for checking for relevant and applicable plans, as well as
executing test goals. Given that we have extended the syntax of atomic formul so as to
include annotations of the sources for the information symbolically represented by it, we
also need to define |= in our particular context, as follows.
Definition 1 We say that an atomic formula at1 with annotations s11 , . . . , s1n is a logical
consequence of a set of ground atomic formul bs, written bs |= at1 [s11 , . . . , s1n ] if, and
only if, there exists at2 [s21 , . . . , s2m ]  bs such that (i) at1  = at2 , for some most general
unifier , and (ii) {s11 , . . . , s1n }  {s21 , . . . , s2m }.
The intuition is that, not only should predicate at unify with some belief in bs (i), but
also that all specified sources of information for at should be corroborated in bs (ii). Thus,
for example, p(X)[ag1 ] follows from {p(t)[ag1 ,ag2 ]}, but p(X)[ag1 ,ag2 ] does not follow
from {p(t)[ag1 ]}. More concretely, if, in order to be applicable, a plan requires that
a drowning person was explicitly perceived rather than communicated by another agent
(which can be represented by drowning(Person)[percept]), this follows from a belief
drowning(man)[percept,passerby] (i.e., that this was both perceived and communicated
by a passerby). On the other hand, if the required context was that two independent sources
provided the information, say cheating(Person)[witness1,witness2], this cannot be
inferred from a belief cheating(husband)[witness1].
In order to make some semantic rules more readable, we use two operations on a belief
base (i.e., a set of annotated ground atomic formul). We use bs 0 = bs + b to say that bs 0 is
as bs except that bs 0 |= b. Similarly bs 0 = bs  b means that bs 0 is as bs except that bs 0 6|= b.
A plan is considered relevant in relation to a triggering event if it has been written to
deal with that event. In practice, this is checked by trying to unify the triggering event part
of the plan with the triggering event within the event that has been selected for treatment
in that reasoning cycle. In the definition below, we use the logical consequence relation
defined above to check if a plans triggering event unifies with the event that has occurred.
To do this, we need to extend the |= relation so that it also applies to triggering events
instead of predicates. In fact, for the purposes here, we can consider that any operators in
a triggering event (such as + or !) are part of the predicate symbol or, more precisely, let
at1 be the predicate (with annotation) within triggering event te 1 and at2 the one within
te 2 , then {te 2 } |= te 1 if, and only if, {at2 } |= at1 and, of course, the operators prefixing te 1
and te 2 are exactly the same. Because of the requirement of inclusion of annotations, the
converse might not be true.
Definition 2 Given plans ps of an agent and a triggering event te, the set RelPlans(ps, te)
of relevant plans for te is defined as follows:
RelPlans(ps, te) = {(p, ) | p  ps and  is s.t. {te} |= TrEv(p)}.
The intuition regarding annotations is as follows. The programmer should include in
the annotations of a plans triggering event all the sources that must have generated the
233

fiVieira, Moreira, Wooldridge, & Bordini

event for that plan to be relevant (or include no annotation if the source of information
is not important for the plan to be considered relevant). For the plan to be relevant, it
therefore suffices for the annotations in the plans triggering event to be a subset of those
in the event that occurred. A plan with triggering event +!p(X)[s] is relevant for an event
h+!p(t)[s, t], Ti since RelPlans requires that {p(t)[s, t]} |= p(X)[s] (for some most general
unifier ), which in turn requires that {s}  {s, t}. As a consequence, for a plan with a
triggering event that has no annotations (e.g., +!p(X)) to be relevant for a particular event
(say, h+!p(t)[ag1 ], ii) it only requires that the predicates unify in the usual sense since
{}  S, for any set S.
A plan is applicable if it is relevant and its context is a logical consequence of the agents
beliefs. Again we need to extend slightly the definition of |= given above. A plans context
is a conjunction of literals (l is either at or at). We can say that bs |= l1  . . .  ln if, and
only if, bs |= li if li is of the form at, and bs 6|= li of li is of the form at, for 1  i  n.
The function for determining the applicable plans in a set of relevant plans is formalised as
follows.
Definition 3 Given a set of relevant plans R and the beliefs bs of an agent, the set of
applicable plans AppPlans(bs, R) is defined as follows:
AppPlans(bs, R) = {(p, 0  ) | (p, )  R and 0 is s.t. bs |= Ctxt(p)0 }.
We need another function to be used in the semantic rule for when the agent is to
execute a test goal. The evaluation of a test goal ?at consists in testing if the formula at
is a logical consequence of the agents beliefs. The function returns a set of most general
unifiers all of which make the formula at a logical consequence of a set of formul bs, as
follows.
Definition 4 Given a set of formul bs and a formula at, the set of substitutions
Test(bs, at) produced by testing at against bs is defined as follows:
Test(bs, at) = { | bs |= at}.
Next, we present the reasoning cycle of AgentSpeak agents and the rules which define
the operational semantics.
3.3 Reasoning Cycle
Figure 5 shows the possible transitions between the various steps in an agents reasoning
cycle as determined by an AgentSpeak interpreter. The labels in the nodes identify each step
of the cycle, which are: processing received messages (ProcMsg); selecting an event from
the set of events (SelEv); retrieving all relevant plans (RelPl); checking which of those are
applicable (ApplPl); selecting one particular applicable plan (the intended means) (SelAppl);
adding the new intended means to the set of intentions (AddIM); selecting an intention
(SelInt); executing the selected intention (ExecInt), and clearing an intention or intended
means that may have finished in the previous step (ClrInt).
In the general case, an agents initial configuration is hag, C, M, T, ProcMsgi, where ag
is as given by the agent program, and all components of C, M , and T are empty. Note that
234

fiSpeech-Act Based Communication in Agent Programming

ProcMsg

SelEv

ClrInt

RelPl

ExecInt

ApplPl

SelInt

SelAppl

AddIM

Figure 5: The AgentSpeak agent reasoning cycle.

a reasoning cycle starts with processing received messages (ProcMsg)  the semantics for
this part of the reasoning cycle are given in the main section of this paper. After that, the
original AgentSpeak reasoning cycle takes place. An event selection (SelEv) is made, which
is followed by determining relevant and applicable plans (RelPl and ApplPl, respectively).
One of the relevant plans is then selected (SelAppl); note that when there are no events to
be treated or when there are no applicable plans to deal with an event the agent turns its
attention to the selection of an intended means (SelInt) to be executed next. After one of
the relevant plans is selected (SelAppl) and an instance of that plan becomes an intended
means and is therefore included in the set of intentions (AddIM). When there are more
than one intention (which is normally the case except for extremely simple agents), one of
those intentions is selected (SelInt) and executed (ExecInt).
These are the most important transitions; the others will be made clearer when the
semantics is presented. The rules which define the transition systems giving operational
semantics to AgentSpeak (without communication) are presented next.

3.4 Semantic Rules
In this section, we present an operational semantics for AgentSpeak that formalises the
transitions between possible steps of the interpretation of AgentSpeak agents as shown in
Figure 5. In the general case, an agents initial configuration is hag, C, M, T, ProcMsgi,
where ag is as given by the agent program, and all components of C, M , and T are empty.
Note that a reasoning cycle starts with processing received messages (ProcMsg), according
to the most recent extension of the semantics to be presented in Section 4. An event
selection (SelEv) is then made, starting the reasoning cycle as originally defined for the
language, which is the part of the semantics presented below.
Event Selection: The rule below assumes the existence of a selection function SE that
selects events from a set of events E. The selected event is removed from E and it is
assigned to the  component of the temporary information. Rule SelEv2 skips to the
intention execution part of the cycle, in case there are no events to handle.
235

fiVieira, Moreira, Wooldridge, & Bordini

SE (CE ) = hte, ii
hag, C, M, T, SelEvi  hag, C 0 , M, T 0 , RelPli
where:

(SelEv1 )

CE0 = CE \ {hte, ii}
T0 = hte, ii

CE = {}
hag, C, M, T, SelEvi  hag, C, M, T, SelInti

(SelEv2 )

Relevant Plans: Rule Rel1 assigns the set of relevant plans to component TR . Rule Rel2
deals with the possibility that there are no relevant plans for an event, in which case the
event is simply discarded. In fact, an intention associated with the event might also be
discarded: if there are no relevant plans to handle an event generated by that intention,
it cannot be further executed. In practice, instead of simply discarding the event (and
possibly an intention with it), this leads to the activation of the plan failure mechanism,
which we do not discuss here for clarity of presentation, as discussed earlier.
T = hte, ii
RelPlans(agps , te) 6= {}
hag, C, M, T, RelPli  hag, C, M, T 0 , ApplPli
where:

(Rel1 )

TR0 = RelPlans(agps , te)

RelPlans(agps , te) = {}
hag, C, M, T, RelPli  hag, C, M, T, SelEvi

(Rel2 )

An alternative approach for situations where there are no relevant plans for an event
was introduced by Ancona, Mascardi, Hubner, and Bordini (2004). It assumes that in some
cases, explicitly specified by the programmer, the agent will want to ask other agents what
are the recipes they use for handling such events. The mechanism for plan exchange between
AgentSpeak agents they proposed allows the programmer to specify which triggering events
should generate attempts to retrieve external plans, which plans an agent agrees to share
with others, what to do once the plan has been used for handling that particular event
instance, and so forth.
Applicable Plans: The rule Appl1 assigns the set of applicable plans to the TAp component; rule Appl2 applies when there are no applicable plans for an event, in which case the
event is simply discarded. Again, in practice, this normally leads to the plan failure mechanism being activated, rather than simply discarding the event (and the whole intention
with it).
AppPlans(agbs , TR ) 6= {}
hag, C, M, T, ApplPli  hag, C, M, T 0 , SelAppli
where:

(Appl1 )

0
TAp
= AppPlans(agbs , TR )

AppPlans(agbs , TR ) = {}
hag, C, M, T, ApplPli  hag, C, M, T, SelInti
236

(Appl2 )

fiSpeech-Act Based Communication in Agent Programming

Selection of an Applicable Plan: This rule assumes the existence of a selection function
SAp that selects one plan from a set of applicable plans TAp . The selected plan is then
assigned to the T component of the configuration.
SAp (TAp ) = (p, )
hag, C, M, T, SelAppli  hag, C, M, T 0 , AddIMi
where:

(SelAppl)

T0 = (p, )

Adding an Intended Means to the Set of Intentions: Events can be classified as
external or internal (depending on whether they were generated from the agents perception,
or whether they were generated by the previous execution of other plans, respectively). Rule
ExtEv determines that, if the event  is external (which is indicated by T in the intention
associated to ), a new intention is created and the only intended means in that new
intention is the plan p assigned to the  component. If the event is internal, rule IntEv
determines that the plan in  should be put on top of the intention associated with the
event.
T = hte, Ti
T = (p, )
hag, C, M, T, AddIMi  hag, C 0 , M, T, SelInti
where:

CI0

= CI  { [p] }

T = hte, ii T = (p, )
hag, C, M, T, AddIMi  hag, C 0 , M, T, SelInti
where:

CI0

(ExtEv)

(IntEv)

= CI  { i[(p)] }

Note that, in rule IntEv, the whole intention i that generated the internal event needs
to be inserted back in CI , with p pushed onto the top of that intention. This is related
to resuming suspended intentions; the suspending of intentions appears in rule AchvGl
below.
Intention Selection: Rule SelInt1 assumes the existence of a function SI that selects an
intention for processing next, while rule SelInt2 takes care of the situation where the set
of intentions is empty (in which case the reasoning cycle simply starts again).
CI 6= {}
SI (CI ) = i
hag, C, M, T, SelInti  hag, C, M, T 0 , ExecInti
where:

(SelInt1 )

T0 = i

CI = {}
hag, C, M, T, SelInti  hag, C, M, T, ProcMsgi

(SelInt2 )

Executing an Intention: The group of rules below express the effects of executing a
formula in the body of the plan. Each rule deals with one type of formula that can appear
237

fiVieira, Moreira, Wooldridge, & Bordini

in a plan body. Recall from Section 3.2 that an intention is a stack of (partially instantiated)
plan instances; a plan instance is a copy of a plan from the agents plan library). The plan
instance to be executed is always the one at the top of the intention that was selected in the
previous step (rule SelInt1 ); the specific formula to be executed is the one at the beginning
of the body of that plan.
Actions: When the formula to be executed is an action, the action a in the body of the plan
is added to the set of actions A (which, recall, denotes that the action is to be executed using
the agents effectors). The action is removed from the body of the plan and the intention
is updated to reflect this removal.
T = i[head  a;h]
hag, C, M, T, ExecInti  hag, C 0 , M, T, ClrInti
where:

(Action)

0
CA
= CA  {a}
CI0 = (CI \ {T })  {i[head  h]}

Achievement Goals: This rule registers a new internal event in the set of events E. This
event can then be selected for handling in a future reasoning cycle (see rule SelEv1 ). When
the formula being executed is a goal, the formula is not removed from the body of the plan,
as in the other cases. This only happens when the plan used for achieving that goal finishes
successfully; see rule ClrInt2 . The reasons for this are related to further instantiation of
the plan variables as well as handling plan failure.
T = i[head  !at;h]
hag, C, M, T, ExecInti  hag, C 0 , M, T, ProcMsgi
where:

(AchvGl)

CE0 = CE  {h+!at, T i}
CI0 = CI \ {T }

Note how the intention that generated the internal event is removed from the set of
intentions CI , capturing the idea of suspended intentions. In a plan body, if we have !g; f 
(where f is any formula that can appear in plan bodies), this means that, before f can
be executed, the state of affairs represented by goal g needs to be achieved (through the
execution of some relevant, applicable plan). The goal included in the new event created by
rule AchvGl above is treated as any other event, which means it will go the set of events
until it is eventually selected in a later reasoning cycle, according to the agents specific
priorities for selecting events (rule SelEv1 ). Meanwhile, that plan (with formula f to be
executed next) can no longer be executed, hence the whole intention is suspended by being
placed, within the newly created event, in the set of events and removed from the set of
intentions. When the event created by the rule above is selected and an applicable plan
for achieving g has been chosen, that intended means is pushed on top of the suspended
intention, which can then be resumed (i.e., moved back to the set of intentions), according
to rule IntEv. The next time that intention is selected, its execution will then proceed with
a plan for achieving g at the top, and only when that plan is finished will f be executed (as
that plan, now without the achieved goal, will be at the top of the intention again); further
details on suspended intentions can be found in the AgentSpeak literature (e.g., see Bordini
& Moreira, 2004).
238

fiSpeech-Act Based Communication in Agent Programming

Test Goals: These rules are used when a test goal formula ?at is to be executed. Rule
TestGl1 is used when there is a set of substitutions that can make at a logical consequence
of the agents beliefs, which means that the test goal succeeded. If the test goal succeeds,
the substitution is applied to the whole intended means, and the reasoning cycle can be
continued. If that is not the case, it might turn out that the test goal is used as a triggering
event of a plan, which is used by programmers to formulate more sophisticated queries.
Rule TestGl2 is used in such case: it generates an internal event, which may trigger the
execution of a plan, as for achievement goals. If to carry out a plan an agent is required
to obtain information (at the time of actual execution of the plan) which is not directly
available in its belief base, a plan for a test goal can be written which, for example, sends
messages to other agents, or processes available data, so that the particular test goal can be
concluded (producing an appropriate instantiation of logical variables). If an internal event
is generated for the test goal being executed, the process is very similar to achievement goals,
where the intention is suspended until a plan is selected to achieve the goal, as explained
above.
T = i[head  ?at;h]
Test(agbs , at) 6= {}
hag, C, M, T, ExecInti  hag, C 0 , M, T, ClrInti
where:

CI0

= (CI \ {T })  {i[(head  h)]}
  Test(agbs , at)

T = i[head  ?at;h]
Test(agbs , at) = {}
hag, C, M, T, ExecInti  hag, C 0 , M, T, ClrInti
where:

(TestGl1 )

(TestGl2 )

CE0 = CE  {h+?at, T i}
CI0 = CI \ {T }

Updating Beliefs: In the rules below, the set of beliefs of the agent is modified in a way
that either an atomic formula (with annotation self) is included in the new set of beliefs
(rule AddBel) or it is removed from there (rule DelBel). Both rules add a new event to
the set of events E, and update the intention by removing from it the +b or at formula
just executed. Note that belief deletions can have variables (at), whilst only ground atoms
(b) can be added to the belief base.
T = i[head  +b;h]
hag, C, M, T, ExecInti  hag 0 , C 0 , M, T, ClrInti
where:

ag 0bs
CE0
CI0

= ag bs + b[self]
= CE  {h+b[self], Ti}
= (CI \ {T })  {i[head  h]}

T = i[head  at;h]
hag, C, M, T, ExecInti  hag 0 , C 0 , M, T, ClrInti
where:

ag 0bs
CE0
CI0

(AddBel)

= ag bs  at[self]
= CE  {hat[self], Ti}
= (CI \ {T })  {i[head  h]}
239

(DelBel)

fiVieira, Moreira, Wooldridge, & Bordini

Clearing Intentions: Finally, the following rules remove empty intended means or intentions from the set of intentions. Rule ClrInt1 simply removes a whole intention when there
is nothing else to be executed in that intention. Rule ClrInt2 clears the remainder of the
plan with an empty body currently at the top of a (non empty) intention. In this case, it
is necessary to further instantiate the plan below the finished plan (currently at the top of
that intention), and remove the goal that was left at the beginning of the body of the plan
below (see rules AchvGl and TestGl). Note that, in this case, further clearing might be
necessary, hence the next step is still ClrInt. Rule ClrInt3 takes care of the situation where
no (further) clearing is required, so a new reasoning cycle can start (at step ProcMsg).
j = [head  T], for some j  CI
hag, C, M, T, ClrInti  hag, C 0 , M, T, ProcMsgi
where:

CI0

(ClrInt1 )

= CI \ {j}

j = i[head  T], for some j  CI
hag, C, M, T, ClrInti  hag, C 0 , M, T, ClrInti

(ClrInt2 )

where: CI0 = (CI \ {j})  {k[(head0  h)]}
if i = k[head0  g;h] and  is s.t. g = TrEv(head)
j=
6 [head  T]  j 6= i[head  T], for any j  CI
hag, C, M, T, ClrInti  hag, C, M, T, ProcMsgi

(ClrInt3 )

4. Semantics of Communicating AgentSpeak Agents
The rules in the previous section give semantics to the key internal decision making and
control aspects of AgentSpeak. Furthermore, the overall agent architecture will have sensors
(with an associated belief revision function) and effectors, in addition to an AgentSpeak
interpreter. The relation of these components to the AgentSpeak interpreter is not essential
for giving a semantics to the language itself. It suffices to note that belief revision from
perception of the environment adds (external) events to the set CE (which is then used in
the AgentSpeak interpretation cycle), while the effectors simply execute every action that
is included by the reasoner in the set CA .
Similarly, the mechanism that allows messages to be exchanged is part of the overall
agent architecture  it is not part of its practical reasoning component, which is specifically what we program with AgentSpeak. The notion of internal actions in AgentSpeak
(Bordini et al., 2002) is appropriate here: sending a message corresponds to executing the
(predefined) internal action .send that appears in a plan body. The underlying agent architecture ensures that the necessary technical means is used for the message to reach the
agent to which the message is addressed. However, as we will be referring to a special type
of communication action that involves suspending intentions, we now need to include such
details in the semantics.4
4. Some aspects of the whole framework are still not included in the formalisation given in this paper. We
extend the semantics only to the point required for accounting for the semantics of speech-act based
messages received by an agent.

240

fiSpeech-Act Based Communication in Agent Programming

The format of messages is hmid , id , ilf , cnti, where mid uniquely identifies the message,
id identifies the agent to which the message is addressed (when the message is being sent)
or the agent that has sent the message (when the message is being received), ilf is the
illocutionary force (i.e., the performative) associated with the message, and cnt is the
message content. Depending on the illocutionary force of the message, its content can be:
an atomic formula (at); a set of formul (AT s); a ground atomic formula (b); a set of
ground atomic formul (Bs); or a set of plans (P Ls).
A mechanism for receiving and sending messages asynchronously is then defined. Messages are stored in a mail box and one of them is processed by the agent at the beginning of
a reasoning cycle. Recall that, in a configuration of the transition system, MIn is the set of
messages that the agent has received but has not processed yet, MOut is the set of messages
to be sent to other agents, and MSI is a set of suspended intentions awaiting replies for
(information request) messages previously sent. More specifically, MSI is a set of pairs of
the form (mid , i), where mid is a message identifier that uniquely identifies the previously
sent message that caused intention i to be suspended.
When sending messages with illocutionary forces related to information requests, we
have chosen a semantics in which the intention is suspended until a reply is received from
the interlocutor, very much in the way that intentions get suspended when they are waiting
for an internal event to be handled. With this particular semantics for ask messages, the
programmer knows with certainty that any subsequent action in the body of a plan is only
executed after the requested information has already been received. However, note that the
information received as a reply is stored directly in the agents belief base, so a test goal is
required if the information is to be used in the remainder of the plan.
We now give two rules for executing the (internal) action of sending a message to another
agent: the first is for the ask messages which require suspending intentions and the second
is for other types of messages. These rules have priority over Action; although Action
could also be applied on the same configurations, we assume the rules below are used if the
formula to be executed is specifically a .send action. (We did not include this as a proviso
in rule Action to improve readability.)
T = i[head  .send(id , ilf , cnt);h]
ilf  {AskIf , AskAll , AskHow }
hag, C, M, T, ExecInti  hag, C 0 , M 0 , T, ProcMsgi
where:

0
MOut
0
MSI

CI0

(ExecActSndAsk)

= MOut  {hmid , id , ilf , cnti}
= MSI  {(mid , i[head  h])},
with mid a new message identifier;
= (CI \ {T })

The semantics of sending other types of illocutionary forces is then simply to add a
well-formed message to the agents mail outbox (rule ExecActSnd). Note that in the rule
above, as the intention is suspended, the next step in the reasoning cycle is ProcMsg(i.e.,
a new cycle is started), whereas in the rule below it is ClrInt, as the updated intention 
with the sending action removed from the plan body  might require clearing, as with
any of the intention execution rules seen in the previous section.
241

fiVieira, Moreira, Wooldridge, & Bordini

T = i[head  .send(id , ilf , cnt);h]
ilf 6 {AskIf , AskAll , AskHow }
hag, C, M, T, ExecInti  hag, C 0 , M 0 , T, ClrInti
where:

0
MOut

CI0

(ExecActSnd)

= MOut  {hmid , id , ilf , cnti},
with mid a new message identifier;
= (CI \ {T })  {i[head  h]}

Whenever new messages are sent, we assume the system creates unique message identifiers (mid ). Later, we shall see that, when replying to a message, the same message
identifier is kept in the message, similar to the way that reply-with is used in KQML.
Thus, the receiving agent is aware that a particular message is a reply to a previous one by
checking the message identifiers in the set of intentions that were suspended waiting for a
reply. This feature will be used when we give semantics to receiving Tell messages, which
can be sent by an agent when it spontaneously wants the receiver to believe something (or
at least to believe something about the senders beliefs), but can also be used when the
agent receives an ask type of message and chooses to reply to it.
As mentioned earlier, it is not our aim to formalise every aspect of a system of multiple AgentSpeak agents. We extend the previous semantics only to the extent required
to formalise speech-act based communication for such agents. It is relevant, therefore, to
consider a rule that defines message exchange as accomplished by the underlying message
exchange mechanism available in an overall agent architecture. This is abstracted away in
the semantics by means of the following rule, where each AGid k , k = 1 . . . n, is an agent
configuration hag id k , Cid k , Mid k , Tid k , sid k i:
hmid , id j , ilf , cnti  Mid i Out
{AGid 1 , . . . AGid i , AGid j , . . . AGid n , env} 
{AGid 1 , . . . AG0id i , AG0id j , . . . AGid n , env}
where:

(MsgExchg)

0
Mid
= Mid i Out \ {hmid , id j , ilf , cnti}
i Out
0
Mid j
= Mid j In  {hmid , id i , ilf , cnti}
In

In the rule above, there are n agents, and env denotes the environment in which the
agents are situated; typically, this is not an AgentSpeak agent, it is simply represented as a
set of properties currently true in the environment and how they are changed by an agents
actions. Note how, in a message that is to be sent, the second component identifies the
addressee (the agent to which the message is being sent), whereas in a received message
that same component identifies the sender of the message.
4.1 Speech-act Based Communication for AgentSpeak
In this section we discuss the performatives that are most relevant for communication
in AgentSpeak. These are largely inspired by corresponding KQML performatives. We
also consider some new performatives, related to plan exchange rather than communication
about propositions as usual. The performatives that we consider are briefly described below,
where s denotes the agent that sends the message, and r denotes the agent that receives
242

fiSpeech-Act Based Communication in Agent Programming

the message. Note that tell and untell can be used either for an agent to pro-actively
send information to another agent, or as replies to previous ask messages.
tell: s intends r to believe (that s believes) the sentence in the messages content to be
true;
untell: s intends r not to believe (that s believes) the sentence in the messages content
to be true;
achieve: s requests that r to intend to achieve a state of the world where the message
content is true;
unachieve: s requests r to drop the intention of achieving a state of the world where the
message content is true;
tell-how: s informs r of a plan (i.e., some know-how of s);
untell-how: s requests r to disregard a certain plan (i.e., to delete that plan from its plan
library);
ask-if: s wants to know if the content of the message is true for r;
ask-all: s wants all of rs answers to a question (i.e., all the beliefs that unify with the
message content);
ask-how: s wants all of rs plans for a particular triggering event (in the message content).
For processing messages, a new selection function is necessary, which operates in much
the same way as the other selection functions described in the previous section. The new
selection function is called SM , and selects a message from MIn ; intuitively, it represents
the priority assigned to each type of message by the programmer. We also need another
given function, but its purpose is different from selection functions. The Boolean function
SocAcc(id , ilf , at), where ilf is the illocutionary force of the message from agent id , with
propositional content at, determines when a message is socially acceptable in a given context.
For example, for a message of the form hmid , id , Tell , ati, the receiving agent may want to
consider whether id is a relevant source of information, as even remembering that id believes
at might not be appropriate. For a message with illocutionary force Achieve, an agent would
normally check, for example, whether id has sufficient social power over itself, or whether
it wishes to act altruistically towards id , before actually committing to do whatever it is
being asked.
We should mention that the role of SocAcc() in our framework is analogous, on the
receivers side, to that of the cause to want and cause to believe predicates in Cohen
and Perraults plan-based theory of speech acts (1979). That is, it provides a bridge from
the illocutionary force of a message to its perlocutionary force. The idea of having userdefined functions determining relations such as trust and power has already been used
in practice by Bordini et al. (2003). Similar interpretations for the use of SocAcc when
applied to other types of messages (e.g., AskIf ) can easily be derived.
There is considerable work on elaborate conceptions of trust in the context of multi-agent
systems, for example in the work of Castelfranchi and Falcone (1998). In our framework,
243

fiVieira, Moreira, Wooldridge, & Bordini

more sophisticated notions of trust and power can be implemented by considering the
annotation of the sources of information during the agents practical reasoning rather than
the simple use of SocAcc. The annotation construct facilitates determining, in a plan
context, the source of a belief before that plan becomes an intended means.
Before we start the presentation of the semantic rules for communication, it is worth
noting that, in this paper in particular, we do not consider nested annotations. Nested
annotations allow the representation of beliefs about other agents beliefs, or more generally
situations in which an agent i was told  by j, which in turn was told  by k, and so forth.

4.2 Semantic Rules for Interpreting Received Messages
Receiving a Tell Message: A Tell message might be sent to an agent either as a reply or
as an inform action. When receiving a Tell message as an inform (as opposed to a reply to
a previous request), the AgentSpeak agent will include the content of the received message
in its knowledge base and will annotate the sender as a source for that belief. Note that this
corresponds, in a way, to what is specified as the action completion condition by Labrou
and Finin (1994): the receiver will know about the senders attitude regarding that belief.
To account for the social aspects of multi-agent systems, we consider that social relations
will regulate which messages the receiver will process or discard; this is referred to in the
semantics by the SocAcc function, which is assumed to be given by the agent designer. The
rule shows that the annotated belief is added to the belief base, and the appropriate event
is generated.

SM (MIn ) = hmid , id , Tell , Bsi
(mid , i) 6 MSI (for any intention i)
SocAcc(id , Tell , Bs)
hag, C, M, T, ProcMsgi  hag 0 , C 0 , M 0 , T, SelEvi
where:

0
MIn

(Tell)

= MIn \ {hmid , id , Tell , Bsi}

and for each b  Bs :
ag 0bs = ag bs + b[id ]
CE0
= CE  {h+b[id ], Ti}

Receiving a Tell Message as a Reply: This rule is similar to the one above, except
that now the suspended intention associated with that particular message  given that it
is a reply to a previous ask message sent by this agent  needs to be resumed. Recall
that to resume an intention we just need to place it back in the set of intentions (CI0 ).
244

fiSpeech-Act Based Communication in Agent Programming

SM (MIn ) = hmid , id , Tell , Bsi
(mid , i)  MSI (for some intention i)
SocAcc(id , Tell , Bs)
hag, C, M, T, ProcMsgi  hag 0 , C 0 , M 0 , T, SelEvi
where:

0
MIn
0
MSI
0
CI

(TellRepl)

= MIn \ {hmid , id , Tell , Bsi}
= MSI \ {(mid , i)}
= CI  {i}

and for each b  Bs :
ag 0bs = ag bs + b[id ]
CE0
= CE  {h+b[id ], Ti}

Receiving an Untell Message: When receiving an Untell message, the sender of the
message is removed from the set of sources giving accreditation to the atomic formula in
the content of the message. In case the sender was the only source for that information,
the belief itself is removed from the receivers belief base. Note that, as the atomic formula
in the content of an Untell message can have uninstantiated variables, each belief in the
agents belief base that can be unified with that formula needs to be considered in turn,
and the appropriate events generated.

SM (MIn ) = hmid , id , Untell , AT si
(mid , i) 6 MSI (for some intention i)
SocAcc(id , Untell , AT s)
hag, C, M, T, ProcMsgi  hag 0 , C 0 , M 0 , T, SelEvi
where:

0
MIn

(Untell)

= MIn \ {hmid , id , Untell , AT si}

and for each b  {at |
  Test(agbs , at)  at  AT s}
ag 0bs = ag bs  b[id ]
CE0
= CE  {hb[id ], Ti}

Receiving an Untell Message as a Reply: As above, the sender as source for the
belief, or the belief itself, is excluded from the belief base of the receiver, except that now
a suspended intention needs to be resumed (similarly to a Tell as a reply).
245

fiVieira, Moreira, Wooldridge, & Bordini

SM (MIn ) = hmid , id , Untell , AT si
(mid , i)  MSI (for some intention i)
SocAcc(id , Untell , AT s)
hag, C, M, T, ProcMsgi  hag 0 , C 0 , M 0 , T, SelEvi
where:

0
MIn
0
MSI
0
CI

(UntellRepl)

= MIn \ {hmid , id , Untell , AT si}
= MSI \ {(mid , i)}
= CI  {i}

and for each b  {at |
  Test(agbs , at)  at  AT s}
0
ag bs = ag bs  b[id ]
CE0
= CE  {hb[id ], Ti}
Receiving an Achieve Message: In an appropriate social context (e.g., if the sender has
power over the receiver), the receiver will try to execute a plan whose triggering event is
+!at; that is, it will try to achieve the goal associated with the propositional content of the
message. An external event is thus included in the set of events (recall that external events
have the triggering event associated with the empty intention T).
Note that it is now possible to have a new focus of attention (a stack of plans in the set
of intentions I) being initiated by the addition (or deletion, see below) of an achievement
goal. Originally, only a belief change arising from perception of the environment initiated
a new focus of attention; the plan chosen for that event could, in turn, have achievement
goals in its body, thus pushing new plans onto the stack.
SM (MIn ) = hmid , id , Achieve, ati
SocAcc(id , Achieve, at)
hag, C, M, T, ProcMsgi  hag, C 0 , M 0 , T, SelEvi
where:

0
MIn
0
CE

(Achieve)

= MIn \ {hmid , id , Achieve, ati}
= CE  {h+!at, Ti}

We shall later discuss in more detail the issue of autonomy. While this gives the impression that simply accepting orders removes the agents autonomy (and similarly with
regards to acquired beliefs), the way the agent will behave once aware that another agent
is attempting to delegate a goal completely depends on the particular plans that happen
to be in the agents plan library. If a suitable plan exists, the agent could simply drop the
goal, or could tell the interlocutor that the goal delegation was noted but the goal could
not be adopted as expected, etc.
Receiving an Unachieve Message: This rule is similar to the preceding one, except
that now the deletion (rather than addition) of an achievement goal is included in the set
of events. The assumption here is that, if the agent has a plan with such a triggering
event, then that plan should handle all aspects of dropping an intention. However, doing
so in practice may require the alteration of the set of intentions, thus requiring special
mechanisms which have not been included in any formalisation of AgentSpeak as yet, even
though it is already available in practice, for example in the Jason interpreter (Bordini &
Hubner, 2007).
246

fiSpeech-Act Based Communication in Agent Programming

SM (MIn ) = hmid , id , Unachieve, ati
SocAcc(id , Unachieve, at)
hag, C, M, T, ProcMsgi  hag, C 0 , M 0 , T, SelEvi
where:

0
MIn
0
CE

(Unachieve)

= MIn \ {hmid , id , Unachieve, ati}
= CE  {h!at, Ti}

Receiving a Tell-How Message: The AgentSpeak notion of plan is related to Singhs
concept of know-how (1994). Accordingly, we use the TellHow performative when agents
wish to exchange know-how rather than communicate beliefs or delegate goals. That is,
a TellHow message is used by the sender (an agent or external source more generally) to
inform an AgentSpeak agent of a plan that can be used for handling certain types of events
(as expressed in the plans triggering event). If the source is trusted, the plans in the
message content are simply added to the receivers plan library.
SM (MIn ) = hmid , id , TellHow , P Lsi
(mid , i) 6 MSI (for any intention i)
SocAcc(id , TellHow , P Ls)
hag, C, M, T, ProcMsgi  hag 0 , C, M 0 , T, SelEvi
where:

(TellHow)

0
MIn
= MIn \ {hmid , id , TellHow , P Lsi}
0
ag ps = ag ps  P Ls

Note that we do not include any annotation to identify the source of a plan, and so,
with this semantics, it is not possible to take into account the identity of the agent that
provided a plan when deciding whether to use it. In practice, this feature is implemented
in the Jason interpreter, as the language is extended with the use of annotated predicates
as plan labels. This also allows programmers to annotate plans with information that can
be used for meta-level reasoning (e.g., choosing which plan to use in case various applicable
plans are available, or which intention to execute next); examples of such information would
be the expected payoff of a specific plan and its expected chance of success, thus allowing
the use of decision-theoretic techniques in making those choices.
Receiving a Tell-How Message as a Reply: The TellHow performative as a reply
will also cause the suspended intention  the one associated with the respective AskHow
message previously sent  to be resumed.
SM (MIn ) = hmid , id , TellHow , P Lsi
(mid , i)  MSI (for some intention i)
SocAcc(id , TellHow , P Ls)
hag, C, M, T, ProcMsgi  hag 0 , C 0 , M 0 , T, SelEvi
where:

0
MIn
0
MSI
0
CI
ag 0ps

=
=
=
=

MIn \ {hmid , id , TellHow , P Lsi}
MSI \ {(mid , i)}
CI  {i}
ag ps  P Ls
247

(TellHowRepl)

fiVieira, Moreira, Wooldridge, & Bordini

Receiving an Untell-How Message: This is similar to the rule above, except that plans
are now removed from the receivers plan library. An external source may find that a plan is
no longer appropriate for handling the events it was supposed to handle; it may then want
to inform another agent about that. Thus, when receiving a socially acceptable UntellHow
message, the agent removes the associated plans (i.e., those in the message content) from
its plan library.
SM (MIn ) = hmid , id , UntellHow , P Lsi
SocAcc(id , UntellHow , P Ls)
hag, C, M, T, ProcMsgi  hag 0 , C, M 0 , T, SelEvi
where:

(UntellHow)

0
MIn
= MIn \ {hmid , id , UntellHow , P Lsi}
0
ag ps = ag ps \ P Ls

Receiving an Ask-If Message: The receiver will respond to this request for information
if certain conditions imposed by the social settings (the SocAcc function) hold between
sender and receiver.
Note that ask-if and ask-all differ in the kind of request made to the receiver. With
the former, the receiver should just confirm whether the predicate in the message content
is in its belief base or not; with the latter, the agent replies with all the predicates in its
belief base that unify with the formula in the message content. The receiver processing an
AskIf message responds with the action of sending either a Tell (to reply positively) or
Untell message (to reply negatively); the reply message has the same content as the AskIf
message. Note that a reply is only sent if the social context is such that the receiver wishes
to consider the senders request.
SM (MIn ) = hmid , id , AskIf , {b}i
SocAcc(id , AskIf , b)
hag, C, M, T, ProcMsgi  hag, C, M 0 , T, SelEvi
where:
0
MIn
0
MOut

= M
( In \ {hmid , id , AskIf , {b}i}
MOut  {hmid , id , Tell , {b}i}
=
MOut  {hmid , id , Untell , {b}i}

(AskIf )

if ag bs |= b
if ag bs 6|= b

The role that SM plays in the agents reasoning cycle is slightly more important here
than originally conceived (Moreira et al., 2004). An agent considers whether to accept a
message or not, but the reply message is automatically assembled when the agent selects
(and accepts) any of the ask messages. However, providing such a reply may require
considerable computational resources (e.g., the whole plan library may need to be scanned
and a considerable number of plans retrieved from it in order to produce a reply message).
Therefore, SM should normally be defined so that the agent only selects an AskIf , AskAll ,
or AskHow message if it determines the agent is not currently too busy to provide a reply.
Receiving an AskAll: As for AskIf , the receiver processing an AskAll has to respond
either with Tell or Untell , provided the social context is such that the receiver will choose
248

fiSpeech-Act Based Communication in Agent Programming

to respond. As noted above, here the agent replies with all the predicates in the belief base
that unify with the formula in the message content.
SM (MIn ) = hmid , id , AskAll , {at}i
SocAcc(id , AskAll , at)
hag, C, M, T, ProcMsgi  hag, C, M 0 , T, SelEvi
where:
0
MIn
0
MOut

= 
MIn \ {hmid , id , AskAll , {at}i}

 MOut  {hmid , id , Tell , AT si},
AT s = {at |   Test(agbs , at)}
=

 M
Out  {hmid , id , Untell , {at}i}

(AskAll)

if Test(agbs , at) 6= {}
otherwise

Receiving an AskHow: The receiver of an AskHow has to respond with the action of
sending a TellHow message, provided the social configuration is such that the receiver will
consider the senders request. In contrast to the use of Untell in AskAll , the response when
the receiver knows no relevant plan (for the triggering event in the message content) is a
reply with an empty set of plans.
SM (MIn ) = hmid , id , AskHow , tei
SocAcc(id , AskHow , te)
hag, C, M, T, ProcMsgi  hag, C, M 0 , T, SelEvi
where:
0
MIn
0
MOut

(AskHow)

= MIn \ {hmid , id , AskHow , tei}
= MOut  {hmid , id , TellHow , P Lsi}
and P Ls = {p | (p, )  RelPlans(agps , te)}

When SocAcc Fails: All the rules above consider that the social relations between sender
and receiver are favourable for the particular communicative act (i.e, they require SocAcc
to be true). If the required social relation does not hold, the message is simply discarded
 it is removed from the set of messages and ignored. The rule below is used for receiving
a message from an untrusted source, regardless of the performative.
SM (MIn ) = hmid , id , ilf , Bsi
SocAcc(id , ilf , Bs)
(with ilf  {Tell , Untell , TellHow , UntellHow ,
Achieve, Unachieve, AskIf , AskAll , AskHow })
hag, C, M, T, ProcMsgi  hag, C, M 0 , T, SelEvi
where:

0
MIn

(NotSocAcc)

= MIn \ {hmid , id , ilf , Bsi}

When MIn is empty: This last semantic rule states that, when the mail inbox is empty,
the agent simply goes to the next step of the reasoning cycle (SelEv).
MIn = {}
hag, C, M, T, ProcMsgi  hag, C, M, T, SelEvi

249

(NoMsg)

fiVieira, Moreira, Wooldridge, & Bordini

4.3 Comments on Fault Detection and Recovery
As in any other distributed system, multi-agent systems can (and do) fail in the real world.
Possibly even more so than typical distributed systems, given that multi-agent systems
are normally used in dynamic, unpredictable environments. In such contexts, failures are
expected to happen quite often, so agents need to recover from them in the best possible
way. In the specific case of systems composed of AgentSpeak agents, failures can occur
when, for instance, the agent for which a message has been sent has left the multi-agent
system, cannot be contacted, or has ceased to exist (e.g., because of a machine or network
crash). Equally, an intention that was suspended waiting for a reply may never be resumed
again due to a failure in the agent that was supposed to provide a reply. (However, note
that AgentSpeak agents typically have various concurrent foci of attention  i.e., multiple
intentions currently in the set of intentions  so even if one particular intention can never
progress because another agent never replies, the agent will simply carry on working on the
other foci of attention.)
In the context of AgentSpeak agents, both fault detection and recovery start at the
level of the infrastructure that supports the agent execution. This infrastructure can adopt
techniques available in traditional distributed systems but with a fundamental difference:
it is responsible for adding appropriate events signaling failures in the set CE of external
events, or possibly resuming suspended intentions and immediately making them fail if for
example a message reply has timed out. Such events, when treated by the agent in its
normal reasoning cycle, using the plan failure mechanism not formalised here but available
in practical interpreters, will trigger a plan specifically written by the agent programmer
which defines a strategy for failure recovery. Therefore, from the point of view of the formal
semantics for AgentSpeak, failure recovery reduces to event handling and plan execution,
and is partly the responsibility of the underlying execution infrastructure and partly the
responsibility of programmers. We should note that various approaches for failure detection and recovery within multi-agent systems in particular appear in the literature (e.g.,
Jennings, 1995; Kumar & Cohen, 2000). They typically involve the use of special agents or
plans defined to deal with failure.
A natural concern when we have a set of agents executing concurrently is that shared
resources should always be left in a consistent state. This is, of course, a classical problem in
concurrency, and is typically solved by atomically executing the parts of the code that access
the shared resources. Many programming language have constructs that enable a programmer to guarantee the atomic execution of critical sections. In a multi-agent system written
in AgentSpeak, atomicity is not immediately an issue since there are no critical sections,
given that different AgentSpeak agents do not directly share memory. However, AgentSpeak
agents do exchange information in the form of messages, but the responsibility for the management of such exchanges lies with the underlying message passing infrastructure. On the
other hand, agents in a multi-agent systems typically share an environment, and if a particular application requires environment resources to be shared by agents, clearly programmers
need to ensure that suitable agent interaction protocols are used to avoid dead-/live-locks
or starvation.
Another possible source of concern regarding atomicity is the concurrent execution of an
agents intentions. Agents can have several intentions ready to be executed and each one of
250

fiSpeech-Act Based Communication in Agent Programming

them can read/write data that is shared with other intentions (as they all access the same
belief base). It is not in the scope of this paper to formalise mechanisms to control such
concurrency, but it is worth mentioning that the Jason interpreter provides programmers
with the possibility of annotating plans as being atomic, so that when one of them is
selected for execution (see the SelInt semantic rule), it is guaranteed by the runtime agent
platform that the plan execution will not be suspended/interrupted (i.e., that no other
intention will be selected for execution in the following reasoning cycles) before the whole
plan finishes executing.
Next, we give an example intended to illustrate how the semantic rules are applied during
a reasoning cycle. The example includes agents exchanging messages using the semantic
framework for agent communication formalised earlier in this section.

5. Example of Reasoning Cycles of Communicating Agents
Consider the following scenario. Firefighting robots are in action trying to control a rapidly
spreading fire in a building, under the supervision of a commander robot. Another robot
is piloting a helicopter to observe in which direction the fire is spreading most rapidly. Let
the robot in the helicopter be r1, let r2 be the ground commander, and let r3 be one of
the firefighting robots.
One of the plans in r1s plan library, which we shall refer to as ps1, is as shown in
Figure 6. This plan says that as soon as r1 perceives fire spreading in direction D, it tells R
that fire is spreading towards D, where R is the agent it believes to be the ground commander.
Plan ps2 is one of the plans that robot r2 (the commander) has in its plan library, which is
also shown in Figure 6. Plan ps2 says that, when r2 gets to believe5 that fire is spreading
in direction D, it will request the robot believed to be closest to that part of the building to
achieve a state of the world in which D is the fighting post of that robot (in other words,
that the robot should relocate to the part of the building in direction D).
We now proceed to show how the rules of the operational semantics apply, by using
one reasoning cycle of the AgentSpeak agent that controls r1 as an example; the rules for
communication will be exemplified afterwards. For simplicity, we assume r1 is currently
defined by a configuration hag 1 , C1 , M1 , T1 , s1 i, with s1 = ProcMsg and the ag 1 component
having:
ag 1bs = {commander(r2)}, and
ag 1ps = {ps1}.
Suppose r1 has just perceived fire spreading towards the south. After the belief revision function (see Section 3.2) has operated, r1s beliefs will be updated to ag 1bs =
{commander(r2), spreading(south)} and the C1E component of r1s configuration (i.e.,
its set of events) will be as follows:
C1E = {h+spreading(south), Ti}.
5. Note that, because the plans triggering event does not require a particular source for that information (as in, e.g., +spreading(D)[percept]), this plan can be used when such belief is acquired from
communication as well as perception of the environment.

251

fiVieira, Moreira, Wooldridge, & Bordini

r1s plan ps1
+spreading(D)
: commander(R);
<- .send(R,tell,spreading(D)).
r2s plan ps2
+spreading(D)
: closest(D,A);
<-.send(A,achieve,fight post(A,D)).

Figure 6: Plans used in the firefighting robots example.
At this point, we can show the sequence of rules that will be applied to complete one
reasoning cycle: see Table 1, where the left column shows the rule being applied and
the right column shows only the components of the configuration which have changed as
a consequence of that rule having been applied. Note that the next step component s1
changes in an obvious way (given the rules being applied), so we only show its change for
the very first step, when there are no messages to be processed and the cycle goes straight
on to selecting an event to be handled in that reasoning cycle.
Table 1: Example sequence of rules applied in one reasoning cycle.
Rule
Changed Configuration Components
NoMsg
s1 = SelEv
SelEv1
C1E = {}
T1 = h+spreading(south), Ti
Rel1
T1R = {(ps1, R )}, where R = {D 7 south}
Appl1
T1Ap = {(ps1, A )}, where A = {D 7 south, R 7 r2}
SelApp
T1 = (ps1, A )
ExtEv
C1I = {[ps1A ]}
SelInt1
T1 = [ps1A ]
ExecActSnd M1Out = {hmid 1 , r2, Tell , {spreading(south)i}}
C1I = {[+spreading(south) : commander(r2) <- T]}
ClrInt1
C1I = {}

After r1s reasoning cycle shown in the table, rule MsgExchg applies and, assuming
hag 2 , C2 , M2 , T2 , s2 i is r2s configuration, which for simplicity we assume is the initial (i.e.,
empty) configuration hence s2 = ProcMsg, we shall have that:
252

fiSpeech-Act Based Communication in Agent Programming

M2In = {hmid 1 , r1, Tell , {spreading(south)}i},
which then leads to rule Tell being applied, thus starting a reasoning cycle (similar to the
one in Table 1) in r2 from a configuration that will have had the following components
changed (see Rule Tell):
M2In = {}
ag 2bs = {spreading(south)[r1]}
C2E = {h+spreading(south)[r1], Ti}.
After a reasoning cycle in r2, we would have for r3 that:
M3In = {hmid 2 , r2, Achieve, fight post(r3,south)i},
where M3In is r3s mail inbox (and assuming M3In was previously empty). Note that r3s
SocAcc function used by rule Achieve (leading to mid 2 being included in M3In as stated
above) would probably consider the hierarchy determined by the firefighters ranks. Robot
r3 would then consider the events generated by this received message in its subsequent
reasoning cycles, and would act in accordance to the plans in its plan library, which we do
not show here, for simplicity.

6. Developing More Elaborate Communication Structures
We sometimes require more elaborate communication structures than performatives such
as those discussed in Section 4. On the other hand, it is of course important to keep any
communication scheme and its semantic basis as simple as possible. We emphasise that, in
our approach, more sophisticated communication structures can be programmed, on top of
the basic communication features formalised here, through the use of plans that implement
interaction protocols. In practical AgentSpeak interpreters, such communication features
(built by composing of the atomic performatives) can be provided to programmers either as
extra pre-defined performatives or as plan templates in plan libraries made publicly available
 in Jason (Bordini & Hubner, 2007), the former approach has been used, but the latter
is also possible. In this section, we give, as examples of more advanced communication
features, plans that allow agents to reach shared beliefs and ensure that agents are kept
informed of the adoption of their goals by other agents. Note however that the examples
make use of a simple practical feature (available, e.g., in Jason) which does not appear in
the abstract syntax we used earlier in the formal presentation: a variable instantiated with
a first order term can be used, within certain constructs (such as belief or goal additions),
in place of an atomic formula, as usual also in Prolog implementations.
Example 1 (Shared Beliefs) If a network infrastructure is reliable, it is easy to ensure
that agents reach shared beliefs. By reaching a shared belief, we mean two agents believing
b as well as believing that the other agent also believes b. More explicitly, we can say agents
ag1 and ag2 share belief b if ag1 believes both b[self] and b[ag2], at the same time that ag2
253

fiVieira, Moreira, Wooldridge, & Bordini

believes both b[self] and b[ag1]. In order to allow agents ag1 and ag2 to reach such shared
beliefs, it suffices6 to provide both agents with copies of the following plans:
rsb1
+!reachSharedBel(P,A) : not P[self]
<- +P;
!reachSharedBel(P,A).
rsb2
+!reachSharedBel(P,A) : P[self] & not P[A]
<- .send(A,tell,P);
.send(A,achieve,reachSharedBel(P,me)).
rsb3
+!reachSharedBel(P,A) : P[self] & P[A]
<- true.
In the plans above, me stands for the agents own name. (Recall that, as in Prolog, an
uppercase initial denotes a logical variable.) Assume agent ag1 has the above plans and
some other plan, an instance of which is currently in its set of intentions, which requires
itself and ag2 to share belief p(X). Such a plan would have the following goal in its body:
!reachSharedBel(p(X),ag2). This would eventually lead to the execution of the plans in
the example above, which can now be explained. The plan labelled rsb1 says that if ag1
has a (new) goal of reaching a shared belief P with agent A, in case ag1 does not yet believe
P itself, it should first make sure itself believes P  note that +P; in the body of that
plan will add the ground predicate bound to P with source self as a new belief to agent
ag1  then it should again have the goal of reaching such shared belief (note that this is a
recursive plan). This time, plan rsb1 will no longer be applicable, so rsb2 will be chosen
for execution. Plan rsb2 says that, provided ag1 believes P but does not yet believe that
agent A believes P, it should tell agent A that itself (ag1) believes P, then finally ask A to
also achieve such shared belief with ag1.
Agent ag2, which also has copies of the plans in the example above, would then, given
the appropriate SocAcc function, have an instance of plan rsb1 in its own set of intentions,
and will eventually execute rsb2 as well, or directly rsb2 as the case may be. Note that the
last line of plan rsb2, when executed by the agent that was asked to reach a shared believe,
rather than the one who took the initiative, is redundant and will lead the other agent
to using rsb3, which only says that no further action is required, given that the shared
belief has already been obtained. Clearly, there are more efficient ways of implementing
a protocol for reaching shared belief, but we present this because the same plans can be
used regardless of whether the agent takes the initiative to reach a shared belief or not.
The version we give here is therefore arguably more elegant, and its symmetry facilitates
reasoning about the protocol.
6. Even if both agents do not have such plans in advance, but are willing to be told how to reach shared
beliefs (by accepting TellHow messages from agents who have such know-how), they can become capable
of reaching shared beliefs too.

254

fiSpeech-Act Based Communication in Agent Programming

We now give another example, which shows how agents can have further information
about requests for goal adoption (i.e., when they ask another agent to achieve some state
of affairs on their behalf).
Example 2 (Feedback on Goal Adoption) It is often the case that, if one agent asks
another agent to do something, it may want to have at least some reassurance from the
other agent that it has agreed to do whatever it has been asked. Furthermore, it may want
to know when the other agent believes it has accomplished the task. The following plans can
be used for ag1 to delegate tasks to ag2 in such a way.
ag1 plans:
nsd1
+needDoneBy(G,A) : not delegatedTo(G,A)
<- +delegatedTo(G,A);
.send(A,achieve,doAndFeedbackTo(G,me)).
nsd2
+needDoneBy(G,A)
: agreedToDo(G)[A] & not finishedDoing(G)[A]
<- .send(A,tell,shouldHaveFinished(G));
...
nsd3
+needDoneBy(G,A) : finishedDoing(G)[A]
<- ...
...
fd
+finishedDoing(G)[A] : true
<- -delegatedTo(G,A);
-agreedToDo(G)[A].

ag2 plans:
dft1
+!doAndFeedbackTo(G,A) : cntxt1
<- .send(A,tell,agreedToDo(G));
+!G;
.send(A,tell,finishedDoing(G)).
dft2
+!doAndFeedbackTo(G,A) : cntxt2
<- .send(A,tell,cannotDo(G)).
255

fiVieira, Moreira, Wooldridge, & Bordini

In the example above, we assume that something perceived in the environment leads
the agent to believe that it needs some goal G to be achieved by agent A, and that such
perception recurs at certain intervals, when the need that motivated the request still exists
and the result of As achieving G has not been observed. Plan nsd1 is used when such a need
occurs but no request has been as yet sent to A. The plan ensures that ag1 will remember
that it already asked A (say, ag2) to do G and then that agent to achieve a goal associated
with a special plan: see plan dft1 in ag2. Such plan makes sure that the requesting agent
is informed both that ag2 has adopted the goal as requested (before it attempts to achieve
it) as well as when the agent believes to have achieved G. The programmer should define the
SocAcc function so that ag2 accepts such requests from ag1, but the programmer can still
determine how autonomous ag2 will be by using appropriate plan contexts. In plan dft1,
context cntxt1 would determine the circumstances under which agent ag2 believes it will be
able to adopt the goal, and context cntxt2 , in plan dft2, can be used for the circumstances
in which ag2 should simply inform it will not adopt the goal as requested by ag1 (a more
elaborate plan could explain why the agent cannot adopt the goal, for example in case there
are more than one situation in which the goal cannot be adopted).
Going back to plans nsd2 and nsd3 in agent ag1, the former is used to put pressure
on the agent that has adopted ag1s goal G, as the need for that has been perceived again
and A has already agreed to do that, so presumably it is not doing it fast enough. Clearly,
the shouldHaveFinished belief should trigger some plan in ag2 for it to have the desired
effect. Plan nsd3 is just a template for one of various alternative courses of actions to be
taken by ag1 when the need that motivated a request for ag2 to adopt a goal still exists but
ag2 believes the goal has already been achieved: that might be an old belief which needs to
be revised and a new request made, or ag1 could try asking another agent, or inform ag2
that its belief about achieving G might be wrong, etc. Plan fd is used simply to remove
unnecessary beliefs used in previous stages of the interaction aimed at a goal adoption.
It is not difficult to see that plans for other important multi-agent issues, such as ensuring
agents are jointly committed to some course of action, can be developed by elaborating on
the combinations of communication performatives along the lines of the examples above.
On the other hand, many other complications related to agent interaction might need to be
accounted for which could not be addressed in the simple examples provided here, such as
shared beliefs becoming inaccurate with the passage of time. Further plans to go with the
ones shown here would be required for coping with such complications, when necessary in
particular applications.

7. Proving Communication Properties of AgentSpeak Agents
Bordini and Moreira (2004) introduced a framework for proving BDI properties of
AgentSpeak agents based on its operational semantics. The framework included precise
definitions of how the BDI modalities are interpreted in terms of configurations of the transition system that gives semantics to AgentSpeak. Those same definitions are used in the
work on model checking for AgentSpeak (Bordini et al., 2004), which allows the use of
automated techniques for verification of AgentSpeak programs. Below, we give an example
of a proof using the operational semantics for a simple property that involves only the belief
modality. As the belief modality is very clear with respect to an AgentSpeak agent, given
256

fiSpeech-Act Based Communication in Agent Programming

that its architecture includes a belief base explicitly, we avoid the need to discuss in this
paper our previous work on the interpretation of the modalities (Bordini & Moreira, 2004).
Proposition 1 (Reachability of Shared Beliefs) If any two AgentSpeak agents ag1
and ag2 have in their plan libraries the rsb1, rsb2, and rsb3 plans shown in Example 1, and they also have an appropriate SocAcc function as well as the usual implementation of selection functions (or others for which fairness is also guaranteed, in the sense
that all events and intentions are eventually selected), if at some moment in time ag1
has reachSharedBel(b,ag2 ) as a goal in its set of events (i.e., it has an event such as
h+!reachSharedBel(b,ag2 ), ii, with i an intention), then eventually both agents will believe b and believe that the other agent also believes b  note that this can be formulated
using a BDI-like logic on top of LTL as
3((Bel ag1 b[self ])  (Bel ag2 b[ag1 ])  (Bel ag2 b[self ])  (Bel ag1 b[ag2 ])).
Proof.
It is assumed that ag1 has h+!reachSharedBel(b, ag2 ), ii in its set of events.
Assume further that this is precisely the event selected when rule SelEv1 is applied. Then
rule Rel1 would select plans rsb1, rsb2, and rsb3 as relevant for the chosen event. Rule
Appl1 would narrow this down to rsb1 only as, presumably, ag1 does not yet believe b
itself. Rule SelAppl would necessarily select rsb1 as intended means, given that it is the
only applicable plan, and rule IntEv would include i[rsb1] in the set of intentions (i.e.,
the chosen intended means would be pushed on top of the intention that generated the
event above). Consider now that in this same reasoning cycle (for simplicity), rule SelInt1
would choose precisely that intention for execution within this reasoning cycle. Then rule
AddBel would add b[self ] to ag1 s belief base, hence (Bel ag1 b[self ]).
In subsequent reasoning cycles, when ag1 s intention selection function selects the
above intention for further execution, rule AchvGl would generate again an internal event
h+!reachSharedBel(b, ag2 ), ii. The process is then as above expect that plan rsb1 is no
longer applicable, but rsb2 is, and is therefore chosen as intended means. When that
plan is executed (similarly as described above), rule ExecActSnd would add message
hmid1 , ag2 , Tell , bi to ag1 s MOut component. Rule MsgExchg then ensures that message hmid, ag1 , Tell , bi is added to ag2 s MIn component, which in the beginning of the
next reasoning cycle would lead to rule Tell adding b[ag1 ] to ag2 s belief base, hence
(Bel ag2 b[ag1 ]). When the intention is selected for execution in a third reasoning cycle, the final formula in the body of plan rsb1 would be executed. By the use of similar rules for sending and receiving messages, we would have ag2 receiving a message
hmid2 , ag1 , Achieve, reachSharedBel(b, ag1 )i, so now rule Achieve is used for interpreting
the illocutionary force in that message, thus adding an event h+!reachSharedBel(b, ag1 ), ii
to ag2 s set of events. Note that this is precisely how the process started in ag1
so the same sequence of rules will apply to ag2 , which will, symmetrically, lead to
(Bel ag2 b[self ]) and (Bel ag1 b[ag2 ]) being true, eventually. At that point in time we
will have ((Bel ag1 b[self ])  (Bel ag2 b[ag1 ])  (Bel ag2 b[self ])  (Bel ag1 b[ag2 ])).
As discussed earlier, because ag2 is using exact copies of the plans used by ag1 , ag2 will
also ask ag1 to reach b as a shared belief, even though ag1 has already executed its part of
257

fiVieira, Moreira, Wooldridge, & Bordini

the joint plan. This is why plan rsb3 is important. It ensures that the agent will act no
further when its own part of the joint plan for reaching a shared belief has already been
achieved.
Note, however, that it is only possible to guarantee that a shared belief is reached in
all possible runs if neither agent has plans that can interfere negatively with the execution
of the plans given in Example 1, for example by forcing the deletion of any instance of
belief b before such shared belief is reached. This is a verification exercise different from
the proposition we wanted to prove, showing that shared beliefs can be reached (under the
given assumptions).

8. Applications of AgentSpeak and Ongoing Work
We mention here some of the applications written in AgentSpeak. The AgentSpeak programming language has also been used in academia for student projects in various courses.
It should be noted, however, that the language is clearly suited to a large range of applications for which it is known that BDI systems are appropriate; various applications of PRS
(Georgeff & Lansky, 1987) and dMARS (Kinny, 1993), for example, have appeared in the
literature (Wooldridge, 2002, Chapter 11).
One particular area of application in which we have great interest is Social Simulation
(Doran & Gilbert, 1994). In fact, AgentSpeak is being used as part of a project to produce a
platform tailored particularly for social simulation. The platform is called MAS-SOC is being developed by Bordini, da Rocha Costa, Hubner, Moreira, Okuyama, and Vieira (2005);
it includes a high-level language called ELMS (Okuyama, Bordini, & da Rocha Costa,
2005) for describing environments to be shared by multiple agents. This approach was
used to develop, for example, a social simulation on social aspects of urban growth (Krafta,
de Oliveira, & Bordini, 2003). Another area of application that has been initially explored
is the use of AgentSpeak for defining the behaviour of animated characters for computer
animation or virtual reality environments (Torres, Nedel, & Bordini, 2004).
More recently, AgentSpeak has been used in the implementation of a team of gold
miners as an entry to an agent programming competition (Bordini, Hubner, & Tralamazza,
2006). In this scenario7 , teams of agents must coordinate their actions in order to collect as
much gold as they can and to deliver the gold to a trading agent located in a depot where
the gold is safely stored. The AgentSpeak team, composed of four mining agents and one
leader that helped coordinate the team of miners, won the competition in 2006. It is worth
noting that the language support for high-level communication (formalised in this paper)
proved to be an important feature for designing and implementing the system.
The AgentSpeak interpreter and multi-agent platform Jason is being constantly improved, with the long term goal of supporting various multi-agent systems technologies.
An important aspect of Jason is precisely that of having formal semantics for most of its
essential features. Various projects are currently looking at extending Jason in various
ways, for example to combine it with an organisational model such as the one propose by
Hubner, Sichman, and Boissier (2004). This is particularly important given that social
structure is a fundamental notion for developing complex multi-agent systems. Another
area of development is to incorporate ontologies into an AgentSpeak belief base (Moreira,
7. See http://cig.in.tu-clausthal.de/CLIMAContest/ for details.

258

fiSpeech-Act Based Communication in Agent Programming

Vieira, Bordini, & Hubner, 2006; Vieira, Moreira, Bordini, & Hubner, 2006), facilitating the
use of Jason for Semantic Web applications. Recent work has also considered automated
belief revision (Alechina, Bordini, Hubner, Jago, & Logan, 2006) and plan exchange mechanisms (Ancona et al., 2004). A more detailed description of the language and comparison
with other agent-oriented programming languages was given by Bordini et al. (2005).

9. Conclusions
As pointed out by Singh (1998), there are various perspectives for the semantics of agent
communication. Whereas the senders perspective is the most common one in the literature,
our approach uses primarily that of the receiver. We have given a formal semantics to the
processing of speech-act based messages by an AgentSpeak agent. Previous attempts to
define the semantics of agent communication languages (e.g., Labrou & Finin, 1994) were
based on the pre-condition  action  post-condition approach, referring to agent mental states in modal languages typically based on Cohen and Levesques work on intention
(1990a). Our semantics for communication, besides being more closely linked to implementation (as it serves as the specification for an interpreter for an agent programming
language), can also be used in the proof of communication properties (Wooldridge, 2000c).
Our work is somewhat related to that of de Boer, van Eijk, Van Der Hoek, and Meyer
(2000) and van Eijk, de Boer, Van Der Hoek, and Meyer (2003), which also provide an
operational semantics for an agent communication language. However, their work does not
consider the effects of communication in terms of BDI-like agents (such as those written
in AgentSpeak). The idea of giving semantics to speech-act based communication within
a BDI programming language was first introduced by Moreira et al. (2004). Subsequently,
Dastani, van der Ham, and Dignum (2003) also published some initial work on the semantics of communication for 3APL agents, although with the emphasis being on formalising
the message exchange mechanisms for synchronous and asynchronous communication. In
contrast, we largely abstract away from the specific message exchange mechanism (this is
formalised at a very high level in our semantics), and we are interested only in asynchronous
communication (which is the usual communication model for cognitive agents). In order to
illustrate their message exchange mechanism, Dastani et al. gave semantics to the effects
of receiving and treating request and inform messages  that is, they only consider
information exchange. Our work uses a much more comprehensive selection of illocutionary
forces, and the main contribution is precisely in giving detailed semantics to the ways in
which the various illocutionary forces affect the mental states of agents implemented in a
programming language which actually has precise definitions for the notions of the BDI
architecture. A denotational semantics for agent communication languages was proposed
by Guerin and Pitt (2001), but the semantics is given for an abstract version of an ACL
and does not address the issues of interaction between an ACL and other components of an
agent architecture.
In this paper we provided new semantic rules for all the illocutionary forces used in
a communication language for AgentSpeak agents. In giving semantics to communicating
AgentSpeak agents, we have provided the means for the implementation of AgentSpeak
interpreters with such functionality, as well as given a more computationally grounded
semantics of speech-act based agent communication. In fact, the operational semantics
259

fiVieira, Moreira, Wooldridge, & Bordini

presented in this paper proved useful in the implementation of AgentSpeak interpreters
such as Jason (Bordini & Hubner, 2007). While Singhs proposal for a social-agency based
semantics (1998) may be appropriate for general purpose agent communication languages
such as FIPA or KQML, within the context of a BDI agent programming language, our
approach can be used without any of the drawbacks pointed out by Singh.
The fact that we have to deal with the intentional states of other agents when giving semantics of communication leads us to a number of related pragmatic questions. First, many
treatments of speech-act style communication make use of mutual mental states  mutual
belief, common knowledge, and similar. We do not make use of mutual mental states in our
formalisation. There are good reasons for this. First, although mutual mental states are a
useful and elegant tool for analysis, it is known that they represent theoretical idealisations
only, which cannot be achieved in systems which admit the possibility of message delivery
failure (Halpern, 1990). Thus, although mutual mental states are a useful abstraction for
understanding how communication works, they cannot, realistically, be implemented, as
there will always be a mismatch between the implementation (which excludes the possibility of mutual mental states being faithfully implemented) and the theory. This is primarily
why mutual mental states form no part of our language or semantics, but are built on top
of the fundamental communication primitives that we formalised in this paper, as shown in
Section 6. Note that it is also known that mutual mental states can be simulated, to any
desired degree of nesting, by an appropriate message acknowledgement scheme (Halpern &
Zuck, 1992), therefore in our approach this problem can be solved by mechanisms such as
processed messages triggering the action of sending a message that acknowledges receipt. It
is also worth adding that the belief annotation scheme used in our language permits agents
to have a simple mechanism for nested beliefs: the annotation of source in a belief is an
indication that the agent who sent the message believed in its propositional content at the
time the message was sent (but note that this is an indication only, unless agent veracity
is guaranteed). Annotation of information source at the time a message is received is done
automatically according to the semantics we have given. However, programmers can also
use the belief base to register sent messages, possibly using annotations in the same manner
as for received messages. These would function as an indication of other agents states of
mind, but from the point of view of the sender. We plan to deal with these questions which
lie in the gray area between semantics and pragmatics in more detail in future work.
While discussing models of mutual mental states, we should also mention in passing
that joint intentions do not form part of our semantics, although they are widely used in
the implementation of coordination schemes for multi-agent systems, following the seminal
work of Levesque, Cohen, and Nunes (1990). The fact that such constructs are not built
into the language (or the language semantics) as primitives does not preclude them being
implemented using the language constructs, provided the usual practical considerations and
assumptions, such as limiting the number of required acknowledgement messages for the
achievement of shared beliefs, are in place. Indeed, this is exactly the approach taken by
Tambe, in his STEAM system (1997), and Jennings, in his GRATE* system (1995). The
examples in Section 6 help indicate how this can be achieved by further elaboration of
those plans, making use of the communication primitives for which we gave semantics in
this paper.
260

fiSpeech-Act Based Communication in Agent Programming

We anticipate that readers will ponder whether our semantics limits the autonomy
of agents that use our approach to communication. We provide the SocAcc() function
which works as an initial filter, but this may give the impression that beliefs are just
acquired/trusted and goals adopted after such simple filter. It is very important to emphasise that the actual behaviour of the agent ensuing from communication received from
other agents completely depends on the particular plans the agent happens to have in its
plan library; in the current semantics, only the ask variants, TellHow , and UntellHow
performatives are dependent solely on the SocAcc filter. In Example 2, we mentioned that
some plan contexts should be used to determine whether the agent would actually act towards achieving a goal as requested by another agent, or choose not to commit to achieving
the goal. This is the general rule: the agent autonomy depends on the plans given by the
agent programmer or obtained by communication with other agents (the plans currently
in the agents plan library). It would be typically the programmers responsibility to write
plans that ensure that an agent will be sufficiently autonomous for its purpose in a given
application or, to use a more interesting notion, to program agents with adjustable autonomy. Similarly, how benevolent or self-interested an agent will be, and to what extent
beliefs acquired from other agents are to be trusted, are all issues that programmers have
to be careful about: the semantics of communication itself does not ensure one case or the
other. Needless to say, it will be a much more difficult task to program agents to take
part in open systems where other agents are self-interested and cannot be trusted. While
an agent programming language combined with a suitable agent communication language
gives much support for such task, it surely does not automatically solve all those problems;
it still remains a complex programming task.
It is also worth commenting on how our semantics can be used by other researchers,
particularly those using agent programming languages other than AgentSpeak. The main
point here is that our semantics provides a reference to the semantics of the communication
language used in the context of agent-oriented programming. That is, using our semantics, it is possible to predict exactly how a particular AgentSpeak agent would interpret
a particular message in a given situation. Using this as a reference model, it should in
principle be possible to implement communication for other agent programming languages.
Of course, our semantics is not language independent: it was developed specifically for
AgentSpeak, so language specifics ought to be considered. However, attempts at giving
semantics of agent communication that are language independent have their own problems,
most notably the computational grounding problem referred to above. Our semantics, while
developed specifically for a practical agent programming language, have the advantage of
not relying on mechanisms (such as abstractly defined mental states) that cannot be checked
for real programs. We note that, to the best of our knowledge, our work represents the first
semantics given for a speech-act style, knowledge level communication language that is
used in a real system.
Our current work does not consider commissive and declarative speech acts. These are
surely relevant topics for future work, since commissive acts and declarations are relevant
for various forms of agent interaction, such as negotiation. Nevertheless, in the proposed
framework it is possible for the programmer or multi-agent system designer to incorporate
such more elaborate forms of interactions by writing appropriate plans.
261

fiVieira, Moreira, Wooldridge, & Bordini

In this work, we assume that communication occurs among agents written in the same
programming language, and cannot be adopted directly in heterogeneous multi-agent systems. (Consider, for example, the issues arising in processing an AskHow performative,
which involves sending a plan to another agent.) However, for a variety of other agent
languages, it should not be difficult to write wrappers for translating message contents.
Other relevant areas for future investigation are those regarding role definitions and
social structures or agent organisations. We consider that these would be interesting developments of the proposed SocAcc() function and libraries of plans or plan patterns. Deontic
relationships and social norms are also closely related to such extensions. In the case of
e-business, for instance, a contract usually creates a number of obligations for the contractors.
Future work should also consider giving a better formal treatment of information sources,
in particular for the case of plans being exchanged between agents. Further communication aspects such as ontological agreement among AgentSpeak agents, and reasoning about
information sources (e.g., in executing test goals or choosing plans based on annotations)
will also be considered in future work. We further expect sophisticated multi-agent system
applications to be developed with AgentSpeak interpreters implemented according to our
semantics.

Acknowledgements
Many thanks to Jomi F. Hubner for his comments and suggestions on an earlier version of
this paper, and to Berndt Farwer and Louise Dennis who carefully proofread it. The first
and second authors acknowledge the support of CNPq.

References
Alechina, N., Bordini, R. H., Hubner, J. F., Jago, M., & Logan, B. (2006). Automating
belief revision for agentspeak. In Baldoni, M., & Endriss, U. (Eds.), Proceedings of
the Fourth International Workshop on Declarative Agent Languages and Technologies
(DALT 2006), held with AAMAS 2006, 8th May, Hakodate, Japan, pp. 116.
Allen, J. F., Hendler, J., & Tate, A. (Eds.). (1990). Readings in Planning. Morgan Kaufmann.
Ancona, D., Mascardi, V., Hubner, J. F., & Bordini, R. H. (2004). Coo-AgentSpeak:
Cooperation in AgentSpeak through plan exchange. In Jennings, N. R., Sierra, C.,
Sonenberg, L., & Tambe, M. (Eds.), Proceedings of the Third International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS-2004), New
York, NY, 1923 July, pp. 698705 New York, NY. ACM Press.
Austin, J. L. (1962). How to Do Things with Words. Oxford University Press, London.
Ballmer, T. T., & Brennenstuhl, W. (1981). Speech Act Classification: A Study in the
Lexical Analysis of English Speech Activity Verbs. Springer-Verlag, Berlin.
262

fiSpeech-Act Based Communication in Agent Programming

Bordini, R. H., Bazzan, A. L. C., Jannone, R. O., Basso, D. M., Vicari, R. M., & Lesser,
V. R. (2002). AgentSpeak(XL): Efficient intention selection in BDI agents via decisiontheoretic task scheduling. In Castelfranchi, C., & Johnson, W. L. (Eds.), Proceedings
of the First International Joint Conference on Autonomous Agents and Multi-Agent
Systems (AAMAS-2002), 1519 July, Bologna, Italy, pp. 12941302 New York, NY.
ACM Press.
Bordini, R. H., da Rocha Costa, A. C., Hubner, J. F., Moreira, A. F., Okuyama, F. Y., &
Vieira, R. (2005). MAS-SOC: a social simulation platform based on agent-oriented
programming. Journal of Artificial Societies and Social Simulation, 8 (3). JASSS
Forum, <http://jasss.soc.surrey.ac.uk/8/3/7.html>.
Bordini, R. H., Fisher, M., Pardavila, C., & Wooldridge, M. (2003). Model checking
AgentSpeak. In Rosenschein, J. S., Sandholm, T., Wooldridge, M., & Yokoo, M.
(Eds.), Proceedings of the Second International Joint Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS-2003), Melbourne, Australia, 1418 July,
pp. 409416 New York, NY. ACM Press.
Bordini, R. H., Fisher, M., Visser, W., & Wooldridge, M. (2004). Model checking rational
agents. IEEE Intelligent Systems, 19 (5), 4652.
Bordini, R. H., & Hubner, J. F. (2007).
Jason:
A Java-based Interpreter for an Extended version of AgentSpeak (Manual, version 0.9 edition).
http://jason.sourceforge.net/.
Bordini, R. H., Hubner, J. F., & Tralamazza, D. M. (2006). Using Jason to implement
a team of gold miners (a preliminary design). In Inoue, K., Satoh, K., & Toni, F.
(Eds.), Proceedings of the Seventh Workshop on Computational Logic in Multi-Agent
Systems (CLIMA VII), held with AAMAS 2006, 89th May, Hakodate, Japan, pp.
233237. (Clima Contest paper).
Bordini, R. H., Hubner, J. F., & Vieira, R. (2005). Jason and the Golden Fleece of
agent-oriented programming. In Bordini, R. H., Dastani, M., Dix, J., & El Fallah Seghrouchni, A. (Eds.), Multi-Agent Programming: Languages, Platforms, and
Applications, chap. 1. Springer-Verlag.
Bordini, R. H., & Moreira, A. F. (2004). Proving BDI properties of agent-oriented programming languages: The asymmetry thesis principles in AgentSpeak(L). Annals of
Mathematics and Artificial Intelligence, 42 (13), 197226. Special Issue on Computational Logic in Multi-Agent Systems.
Bordini, R. H., Visser, W., Fisher, M., Pardavila, C., & Wooldridge, M. (2003). Model
checking multi-agent programs with CASP. In Hunt Jr., W. A., & Somenzi, F. (Eds.),
Proceedgins of the Fifteenth Conference on Computer-Aided Verification (CAV-2003),
Boulder, CO, 812 July, No. 2725 in LNCS, pp. 110113 Berlin. Springer-Verlag. Tool
description.
Bratman, M. E. (1987). Intentions, Plans and Practical Reason. Harvard University Press,
Cambridge, MA.
263

fiVieira, Moreira, Wooldridge, & Bordini

Castelfranchi, C., & Falcone, R. (1998). Principles of trust for MAS: Cognitive anatomy,
social importance, and quantification. In Demazeau, Y. (Ed.), Proceedings of the Third
International Conference on Multi-Agent Systems (ICMAS98), 47 July, Paris, pp.
7279 Washington. IEEE Computer Society Press.
Cohen, P., & Perrault, R. (1979). Elements of a plan based theory of speech acts. Cognitive
Science, 3, 177212.
Cohen, P. R., & Levesque, H. J. (1990a). Intention is choice with commitment. Artificial
Intelligence, 42 (3), 213261.
Cohen, P. R., & Levesque, H. J. (1990b). Rational interaction as the basis for communication. In Cohen, P. R., Morgan, J., & Pollack, M. E. (Eds.), Intentions in Communication, chap. 12, pp. 221255. MIT Press, Cambridge, MA.
Dastani, M., van der Ham, J., & Dignum, F. (2003). Communication for goal directed
agents. In Huget, M.-P. (Ed.), Communication in Multiagent Systems, Vol. 2650 of
LNCS, pp. 239252. Springer-Verlag.
de Boer, F. S., van Eijk, R. M., Van Der Hoek, W., & Meyer, J.-J. C. (2000). Failure
semantics for the exchange of information in multi-agent systems. In Palamidessi, C.
(Ed.), Eleventh International Conference on Concurrency Theory (CONCUR 2000),
University Park, PA, 2225 August, No. 1877 in LNCS, pp. 214228. Springer-Verlag.
Doran, J., & Gilbert, N. (1994). Simulating societies: An introduction. In Gilbert, N., &
Doran, J. (Eds.), Simulating Society: The Computer Simulation ofSocial Phenomena,
chap. 1, pp. 118. UCL Press, London.
Genesereth, M. R., & Ketchpel, S. P. (1994). Software agents. Communications of the
ACM, 37 (7), 4853.
Georgeff, M. P., & Lansky, A. L. (1987). Reactive reasoning and planning. In Proceedings of
the Sixth National Conference on Artificial Intelligence (AAAI87), 1317 July,1987,
Seattle, WA, pp. 677682 Manlo Park, CA. AAAI Press / MIT Press.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory and Practice.
Morgan Kaufmann.
Guerin, F., & Pitt, J. (2001). Denotational semantics for agent communication language.
In Proceedings of the fifth international conference on Autonomous Agents (Agents
2001), 28th May  1st June, Montreal Canada, pp. 497504. ACM Press.
Halpern, J. Y. (1990). Knowledge and common knowledge in a distributed environment.
Journal of the ACM, 37 (3).
Halpern, J. Y., & Zuck, L. D. (1992). A little knowledge goes a long way: knowledge-based
derivations and correctness proofs for a family of protocols. Journal of the ACM,
39 (3), 449478.
264

fiSpeech-Act Based Communication in Agent Programming

Hubner, J. F., Sichman, J. S., & Boissier, O. (2004). Using the Moise+ for a cooperative
framework of MAS reorganisation.. In Bazzan, A. L. C., & Labidi, S. (Eds.), Advances
in Artificial Intelligence - SBIA 2004, 17th Brazilian Symposium on Artificial Intelligence, Sao Luis, Maranhao, Brazil, September 29 - October 1, 2004, Proceedings, Vol.
3171 of LNCS, pp. 506515. Springer-Verlag.
Jennings, N. R. (1995). Controlling cooperative problem solving in industrial multi-agent
systems using joint intentions. Artificial Intelligence, 75 (2), 195240.
Kinny, D. (1993). The distributed multi-agent reasoning system architecture and language
specification. Tech. rep., Australian Artificial Intelligence Institute, Melbourne, Australia.
Krafta, R., de Oliveira, D., & Bordini, R. H. (2003). The city as object of human agency.
In Fourth International Space Syntax Symposium (SSS4), London, 1719 June, pp.
33.133.18.
Kumar, S., & Cohen, P. R. (2000). Towards a fault-tolerant multi-agent system architecture.
In Proceedings of the Fourth International Conference on Autonomous Agents (Agents
2000), 37 June, Barcelona, Spain, pp. 459466. ACM Press.
Labrou, Y., & Finin, T. (1994). A semantics approach for KQMLa general purpose
communication language for software agents. In Proceedings of the Third International
Conference on Information and Knowledge Management (CIKM94), 29th November
 2nd December, Gaithersburg, MD. ACM Press.
Labrou, Y., Finin, T., & Peng, Y. (1999). The current landscape of agent communication
languages. Intelligent Systems, 14 (2), 4552.
Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). On acting together. In Proceedings
of the Eighth National Conference on Artificial Intelligence (AAAI-1990), 29th July
 3rd August, Boston, MA, pp. 9499. AAAI Press.
Levinson, S. C. (1981). The essential inadequacies of speech act models of dialogue. In Parret, H., Sbisa, M., & Verschuren, J. (Eds.), Possibilities and limitations of pragmatics:
Proceedings of the Conference on Pragmatics at Urbino, July, 1979, pp. 473492. Benjamins, Amsterdam.
Mayfield, J., Labrou, Y., & Finin, T. (1996). Evaluation of KQML as an agent communication language. In Wooldridge, M., Muller, J. P., & Tambe, M. (Eds.), Intelligent
Agents IIProceedings of the Second International Workshop on Agent Theories, Architectures, and Languages (ATAL95), held as part of IJCAI95, Montreal, Canada,
August1995, No. 1037 in LNAI, pp. 347360 Berlin. Springer-Verlag.
Moreira, A. F., & Bordini, R. H. (2002). An operational semantics for a BDI agent-oriented
programming language. In Meyer, J.-J. C., & Wooldridge, M. J. (Eds.), Proceedings
of the Workshop on Logics for Agent-Based Systems (LABS-02), held in conjunction
with the Eighth International Conference on Principles of Knowledge Representation
and Reasoning (KR2002), April 2225, Toulouse, France, pp. 4559.
265

fiVieira, Moreira, Wooldridge, & Bordini

Moreira, A. F., Vieira, R., & Bordini, R. H. (2004). Extending the operational semantics of
a BDI agent-oriented programming language for introducing speech-act based communication. In Leite, J., Omicini, A., Sterling, L., & Torroni, P. (Eds.), Declarative
Agent Languages and Technologies, Proceedings of the First International Workshop
(DALT-03), held with AAMAS-03, 15 July, 2003, Melbourne, Australia (Revised Selected and Invited Papers), No. 2990 in LNAI, pp. 135154 Berlin. Springer-Verlag.
Moreira, A. F., Vieira, R., Bordini, R. H., & Hubner, J. F. (2006). Agent-oriented programming with underlying ontological reasoning. In Baldoni, M., Endriss, U., Omicini, A.,
& Torroni, P. (Eds.), Proceedings of the Third International Workshop on Declarative
Agent Languages and Technologies (DALT-05), held with AAMAS-05, 25th of July,
Utrecht, Netherlands, No. 3904 in LNCS, pp. 155170. Springer-Verlag.
Okuyama, F. Y., Bordini, R. H., & da Rocha Costa, A. C. (2005). ELMS: an environment
description language for multi-agent simulations. In Weyns, D., van Dyke Parunak,
H., Michel, F., Holvoet, T., & Ferber, J. (Eds.), Environments for Multiagent Systems, State-of-the-art and Research Challenges. Proceedings of the First International
Workshop on Environments for Multiagent Systems (E4MAS), held with AAMAS-04,
19th of July, No. 3374 in LNAI, pp. 91108 Berlin. Springer-Verlag.
Plotkin, G. (1981). A structural approach to operational semantics.. Technical Report,
Department of Computer Science, Aarhus University.
Rao, A. S. (1996). AgentSpeak(L): BDI agents speak out in a logical computable language.
In van de Velde, W., & Perram, J. (Eds.), Proceedings of the 7th Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW96), 2225 January,
Eindhoven, The Netherlands, No. 1038 in LNAI, pp. 4255 London. Springer-Verlag.
Rao, A. S., & Georgeff, M. P. (1998). Decision procedures for BDI logics. Journal of Logic
and Computation, 8 (3), 293343.
Searle, J. R. (1969). Speech Acts: An Essay in the Philosophy of Language. Cambridge
University Press, Cambridge.
Singh, M. P. (1994). Multiagent SystemsA Theoretic Framework for Intentions, KnowHow, and Communications. No. 799 in LNAI. Springer-Verlag, Berlin.
Singh, M. P. (1998). Agent communication languages: Rethinking the principles. IEEE
Computer, 31 (12), 4047.
Smith, R. G. (1980). The contract net protocol: High-level communication and control in a
distributed problem solver. IEEE Transactions on Computers, c-29 (12), 11041113.
Tambe, M. (1997). Towards flexible teamwork. Journal of Artificial Intelligence Research,
7, 83124.
Torres, J. A., Nedel, L. P., & Bordini, R. H. (2004). Autonomous agents with multiple foci
of attention in virtual environments. In Proceedings of 17th International Conference
on Computer Animation and Social Agents (CASA 2004), Geneva, Switzerland, 79
July, pp. 189196.
266

fiSpeech-Act Based Communication in Agent Programming

van Eijk, R. M., de Boer, F. S., Van Der Hoek, W., & Meyer, J.-J. C. (2003). A verification
framework for agent communication. Autonomous Agents and Multi-Agent Systems,
6 (2), 185219.
Vieira, R., Moreira, A. F., Bordini, R. H., & Hubner, J. (2006). An agent-oriented programming language for computing in context. In Debenham, J. (Ed.), Proceedings of
Second IFIP Symposium on Professional Practice in Artificial Intelligence, held with
the 19th IFIP World Computer Congress, TC-12 Professional Practice Stream, 2124
August, Santiago, Chile, No. 218 in IFIP, pp. 6170 Berlin. Springer-Verlag.
Wooldridge, M. (1998). Verifiable semantics for agent communication languages. In Proceedings of the Third International Conference on Multi-Agent Systems (ICMAS98),
47 July, Paris, pp. 349365. IEEE Computer Society Press.
Wooldridge, M. (2000a). Computationally grounded theories of agency. In Durfee, E. (Ed.),
Proceedings of the Fourth International Conference on Multi-Agent Systems (ICMAS2000),1012 July, Boston, pp. 1320 Los Alamitos, CA. IEEE Computer Society.
Paper for an Invited Talk.
Wooldridge, M. (2000b). Reasoning about Rational Agents. The MIT Press, Cambridge,
MA.
Wooldridge, M. (2000c). Semantic issues in the verification of agent communication languages. Autonomous Agents and Multi-Agent Systems, 3 (1), 931.
Wooldridge, M. (2002). An Introduction to MultiAgent Systems. John Wiley & Sons.

267

fiJournal of Artificial Intelligence Research 29 (2007) 79-103

Submitted 09/06; published 06/07

NP Animacy Identification for Anaphora Resolution
Constantin Orasan
Richard Evans

C.Orasan@wlv.ac.uk
R.J.Evans@wlv.ac.uk

Research Group in Computational Linguistics
School of Humanities, Languages and Social Sciences
University of Wolverhampton
Stafford St., Wolverhampton, WV1 1SB
United Kingdom

Abstract
In anaphora resolution for English, animacy identification can play an integral role in
the application of agreement restrictions between pronouns and candidates, and as a result,
can improve the accuracy of anaphora resolution systems. In this paper, two methods for
animacy identification are proposed and evaluated using intrinsic and extrinsic measures.
The first method is a rule-based one which uses information about the unique beginners
in WordNet to classify NPs on the basis of their animacy. The second method relies on a
machine learning algorithm which exploits a WordNet enriched with animacy information
for each sense. The effect of word sense disambiguation on the two methods is also assessed.
The intrinsic evaluation reveals that the machine learning method reaches human levels
of performance. The extrinsic evaluation demonstrates that animacy identification can
be beneficial in anaphora resolution, especially in the cases where animate entities are
identified with high precision.

1. Introduction
Anaphora resolution is the process which attempts to determine the meaning of expressions
such as pronouns or definite descriptions whose interpretation depends on previously
mentioned entities or discourse segments. Anaphora resolution is very important in
many fields of computational linguistics such as machine translation, natural language
understanding, information extraction and text generation (Mitkov, 2002).
Previous work in anaphora resolution (AR) has shown that its levels of performance
are related to both the type of text being processed and to the average number of noun
phrases (NPs) under consideration as a pronouns antecedent (Evans & Orasan, 2000).
Acknowledging this, researchers have proposed and incorporated various methods intended
to reduce the number of candidate NPs considered by their anaphora resolution systems.
Most approaches to pronominal anaphora resolution rely on compatibility of the agreement
features between pronouns and antecedents, as a means of minimising the number of NP
candidates. Although, as noted by Barlow (1998) and Barbu, Evans, and Mitkov (2002),
this assumption does not always hold, it is reliable in enough cases to be of great practical
value in anaphora and coreference resolution systems. Such systems rely on knowledge
about the number and gender of NP candidates in order to check the compatibility between
pronouns and candidates (Hobbs, 1976; Lappin & Leass, 1994; Kennedy & Boguraev, 1996;
Mitkov, 1998; Cardie & Wagstaff, 1999; Ng & Cardie, 2002). In addition to number and

c
2007
AI Access Foundation. All rights reserved.

fiOrasan & Evans

gender compatibility, researchers reduced the number of competing candidates considered
by their systems by means of syntactic filters (Hobbs, 1976; Lappin & Leass, 1994), semantic
filters (Hobbs, 1978) or discourse structure (Brennan, Friedman, & Pollard, 1987; Cristea,
Ide, Marcu, & Tablan, 2000).
In English, the automatic identification of the specific gender of NPs is a difficult task
of arguably limited utility. Despite this, numerous researchers (Hale & Charniak, 1998;
Denber, 1998; Cardie & Wagstaff, 1999) have proposed automatic methods for identifying
the potential gender of NPs referents. In this paper, the problem of animacy identification
is tackled. The concern with animacy as opposed to gender arises from the observation that
animacy serves as a more reliable basis for agreement between pronouns and candidates (see
examples in Section 2). Animacy identification can be very useful in tasks like anaphora
resolution and coreference resolution where the level of ambiguity can be reduced by filtering
out candidates which do not have the same value for animacy as the anaphor, as well as in
question answering, where it can be used to improve system responses to who questions
by allowing them to ensure that the generated answers consist of animate references.
In this research, a NP is considered to be animate if its referent can also be referred to
using one of the pronouns he, she, him, her, his, hers, himself, herself, or a combination of
such pronouns (e.g. his/her ). Section 2 provides more clarity on this definition, considering
a range of exceptions and problematic cases, as well as examining some consequences of
this treatment of animacy. The corpus used in this research is described in Section 3. In
this paper several methods for animacy identification are proposed and evaluated. First,
a simple statistical method based on WordNet (Fellbaum, 1998) is described in Section
4.1. Following from the description of the simple statistical method, a machine learning
method that overcomes some of the problems of the simple method, offering improved
performance, is described in Section 4.2. In the latest stages of development, word sense
disambiguation (WSD) is added to further improve the accuracy of the classification. This
is presented in Section 4.3. In Section 5, the systems are evaluated using both intrinsic
and extrinsic evaluation methods, and it is noted that the machine learning methods reach
human performance levels. Finally, Section 6 is dedicated to related work and is followed
by conclusions.

2. What Constitutes an Animate Noun Phrase?
It has been argued that in English nouns are not classified grammatically, but semantically
according to their coreferential relations with personal, reflexive and wh-pronouns (Quirk,
Greenbaum, Leech, & Svartvik, 1985, p. 314). According to their classification, animate
noun phrases contain both personal (e.g. male, female, dual, common and collective nouns)
and non-personal noun phrases (e.g. common, collective and animal nouns). In this paper,
our goal is to design a method which improves the performance of anaphora resolution
methods by filtering out candidates which do not agree in terms of animacy with a given
referential pronoun. For this reason, the more specific definition of animacy given in the
introduction is used. This means that in this paper, those noun phrases which can normally
be referred to by the pronouns he and she and their possessive and reflexive forms, are
considered animate, but no distinction is made between those pronouns to determine their
gender. This view is adopted because, in the linguistic processing of English documents, it
80

fiNP Animacy Identification for Anaphora Resolution

Figure 1: Quirk el. al. (1985) vs our classification of animacy. (Adapted from Quirk el. al.
(1985, p. 314, Fig. 5.104))
is vital to distinguish between neuter and animate references but problematic, and often of
limited utility, to distinguish between masculine and feminine ones.
To illustrate, in the sentence The primary user of the machine should select his or
her own settings, considering the noun phrase the primary user of the machine to be either
masculine or feminine, and then applying strict agreement constraints between this reference
and the subsequent pronominal ones, will adversely affect the performance of reference
resolution systems because such constraints will eliminate the antecedent from the list of
candidates of one of the pronouns depending on the gender attached to the NP. Ideally, the
reference should be considered animate - compatible, in terms of agreement, with subsequent
animate pronouns, and incompatible with neuter pronouns.
Figure 1 presents the differences between Quirk et. al.s (1985) classification of animacy
and the one used in this paper. As can be seen in the figure, their definition of animate
nouns is much wider than that used in this paper. We consider of the classes presented by
Quirk et al.s (1985), the only animate entities to be male, female, dual 1 and some common
gender 2 nouns.
Common gender nouns are defined by Quirk et al. (1985) as intermediate between
personal and non-personal nouns. For us, their animacy can be either animate or inanimate,
depending on which pronoun is used to refer to them. However, the animacy of some of the
common nouns such as cat depends upon the perception of that entity by the speaker/writer.
If the noun is being used to refer to a pet, the speaker/writer is also likely to use animate
1. Dual nouns are nouns that refer to people but whose gender is underspecified such as artist, cook, etc.
2. In Figure 1 these nouns are labeled as common.

81

fiOrasan & Evans

pronouns rather than inanimate ones to refer to it. Such circumstances are not detected
by our method: they may be the focus of methods which try to identify the sentiments of
speakers/writers towards entities.
Collective nouns such as team, that refer to sets of animate entities, may intuitively be
considered animate. However, the only suitable pronominal references for the denotation of
such phrases are singular neuter pronouns or plural pronouns of unspecified gender. These
referents are never referred to using animate pronouns. Given that the raison detre for
our research into animacy identification is the facilitation of real-world anaphora resolution,
such NPs are considered inanimate in the current work, and not animate as they are by
Quirk et al. (1985).
Collective nouns such as people pose further problems to annotation and processing. In
some contexts the word people can be used as the plural form of person, in which case it
should be considered animate by the definition presented earlier. However in some cases it
is used more generically to refer to national populations (e.g. the peoples of Asia) in which
case it should be considered inanimate. In light of this, it seems that the class of this word
depends on its context. However, in practical terms, the morpho-syntactic parsing software
that we use (Tapanainen & Jarvinen, 1997) returns people and not person as the root the
noun, so for this reason, the noun people is considered inanimate for our purposes. The
same reasoning was applied to other similar nouns. The drawback of this approach is that
annotators did not find this very intuitive and as a result errors were introduced in the
annotation (as discussed in the next section).
The rest of the categories introduced by Quirk et al. (1985): non-personal higher, nonpersonal lower and inanimate correspond in our definition to inanimate nouns. As with
common gender nouns, it is possible to have non-personal higher and lower nouns such as
horse and rabbit which can be pets and therefore are referred to by speakers using he or
she. As we cannot detect such usages, they are all considered inanimate.3
In the present work, the animacy of a noun phrase (NP) is considered to derive from the
animacy of its head. To illustrate, both the man and the dead man can be referred to using
the same animate pronoun. Moreover, when considering the animacy of plural NPs such
as mileage claimants, the singular form mileage claimant is derived and used as the basis
of classification because the plural form shares the animacy of its singular form. In this
way, our treatment of NP animacy mirrors the treatment of grammatical number under the
Government and Binding Theory (Chomsky, 1981). Under this approach, the projection
principle implies that agreement information for a NP is derived from that of its head.
In this paper, the animacy of only common nouns is determined and not of proper
nouns such as named entities (NE). The reason for this is that the separate task of named
entity recognition is normally used to classify NEs into different categories such as person,
organization, and location. Given that they label entities of similar semantic types,
these categories can then be used to determine the animacy of all the entities that belong
to them. It is acknowledged that named entity recognition is an important component in
the identification of animate references, but one which lies beyond the scope of the present
work. Methods based on semantics, such as the ones described in Section 4 are especially
vulnerable to errors caused by a failure to recognise the difference between words such as
3. Actually on the basis of the explanation provided by Quirk et al. (1985) the distinction between common
nouns and higher and lower non-personal nouns when the latter are personified seems very fuzzy.

82

fiNP Animacy Identification for Anaphora Resolution

No of words
No of animate entities
No of inanimate entities
Percentage of animate entities
Total entities

SEMCOR
104,612
2,321
17,380
12%
19,701

AI
15,767
538
2,586
21%
3,124

Table 1: The characteristics of the two corpora used
Cat or Bob when used as common nouns which are inanimate references or as proper nouns
which are animate references.

3. Corpus-Based Investigation
The identification of NP animacy, as described in the previous section, was amenable to a
corpus-based solution. In this research two corpora are being used: The first is a collection of
texts from Amnesty International (AI) which were selected because they contain a relatively
large proportion of references to animate entities. The second is a selection of texts from
the SEMCOR corpus (Landes, Leacock, & Tengi, 1998), chosen because their nouns were
annotated with senses from WordNet. This annotation made them suitable for exploitation
in the development of the automatic method for animacy identification described in Section
4.2. The SEMCOR corpus was built on the basis of Brown Corpus (Frances & Kucera,
1982) and for our experiments we use texts from newswire, science, fiction and humor.
In order to make the data suitable for evaluation purposes, NPs from the two corpora
have been manually annotated with information about their animacy. The characteristics
of these corpora are summarized in Table 1. As can be seen in the table, even though texts
which contain many references to animate entities were selected, the number of inanimate
entities is still much larger than the number of animate ones.
To assess the difficulty of the annotation task, and implicitly, to estimate the upper
performance limit of automatic methods, a second annotator was asked to annotate a part
of the corpus and inter-annotator agreement was calculated. To this end, the whole AI
corpus, and nine texts with over 3,500 references from the SEMCOR corpus have been
randomly selected and annotated. Comparison between the two annotations revealed a
level of agreement of 97.5% between the two annotators and a value of 0.91 for the kappa
statistic which indicates very high agreement between annotators. The agreement on the
SEMCOR data was slightly higher than that for the AI corpus, but the difference was not
statistically significant.
Investigation of the annotation performed by the two annotators and discussion with
them revealed that the main source of disagreement was the monotony of the task. The
two annotators had to use a simple interface which displayed for each sentence one NP at a
time, and were required to indicate whether the NP was animate or inanimate by choosing
one of two key strokes. Due to the large number of inanimate entities in the corpus, the
annotators often marked animate entities as inanimate accidentally. In some cases they
noticed their mistake and corrected it, but it is very likely that many such mistakes went
unobserved.

83

fiOrasan & Evans

Another source of disagreement were collective nouns such as people, government, jury or
folk which according to the discussion in Section 2 should normally be marked as inanimate.
In some cases, the context of the NP or tiredness on the part of the annotator led to
them being erroneously marked as animate. Similarly, it was noticed that the annotators
wrongly considered some plural noun phrases such as observers, delegates, communists, and
assistants to be collective ones and marked them as inanimate. However, it is likely that
some of these errors were introduced due to the monotony of the task. Unfamiliar nouns
such as thuggee, and words used in some specialized domains such as baseball also caused
difficulties. Finally, another source of error arose from the use of Connexors FDG Parser
(Tapanainen & Jarvinen, 1997) to identify the noun phrases for annotation. As a result,
some of the noun phrases recognized by the system were ambiguous (e.g. specialists and
busy people was presented as one NP and according to the definition of animacy adopted
in the present work, specialists is animate, whereas busy people is inanimate4 .).

4. Methods for Animacy Identification
By contrast to the situation with proper name recognition and classification, which can
exploit surface textual clues such as capitalization and the explicit occurrence of words in a
small gazetteer of titles, knowledge as to the animacy of common NPs appears to be purely
implicit. Recognition of references to animate entities must, at some point, be grounded
in world-knowledge and computed from explicit features of the text. This section presents
two methods developed for animacy identification which rely on information extracted from
WordNet, an electronic lexical resource organized hierarchically by relations between sets
of synonyms or near-synonyms called synsets (Fellbaum, 1998). The first method is a rulebased one which employs a limited number of resources and is presented in Section 4.1. Its
shortcomings are addressed by the machine learning method presented in Section 4.2. Both
methods consider all the senses of a word before taking a decision about its animacy. For
this reason, the word sense disambiguation (WSD) module briefly discussed in Section 4.3
was integrated into them.
4.1 Rule-Based Method
In WordNet, each of the four primary classes of content-words (nouns, verbs, adjectives
and adverbs) are arranged under a small set of top-level hypernyms called unique beginners
(Fellbaum, 1998). Investigation of these unique beginners revealed that several of them were
of interest with respect to the aim of identifying the animate entities in a text. In the case of
nouns there are 25 unique beginners, three of which are expected to be hypernyms of senses
of nouns that usually refer to animate entities. These are animal, reference number (05),
person (18), and relation (24).5 There are also four verb sense hierarchies out of fourteen,
that allow the inference to be made that their subject NPs should be animate. The unique
beginners in these cases are cognition (31), communication (32), emotion (37) and social
4. It can be argued that the singular form of people is person, and that it should therefore be marked as
animate. However, as discussed in Section 2 due to the way it is processed by the preprocessing tools
employed here, annotators were asked to consider it inanimate
5. The unique beginner animal corresponds to both animate and inanimate entities while relation subsumes
mainly human relationships such as brother, sister, parent, etc.

84

fiNP Animacy Identification for Anaphora Resolution

(41).6 It has been noted that inanimate entities such as organizations and animals can also
be agents of these types of verb, but it is expected in the general case that these instances
will be rare enough to ignore. In light of the way in which WordNet is organized, it was clear
that it could be exploited in order to associate the heads of noun phrases with a measure
of confidence that the associated NP has either an animate or inanimate referent.
It is very common for a noun to have more than one meaning, in many cases
corresponding to sense hierarchies which start from different unique beginners. For this
reason, the decision about whether a noun phrase is animate or inanimate should be taken
only after all the possible senses of the head noun have been consulted. Given that some
of these senses are animate whilst others are inanimate, an algorithm which counts the
number of animate senses that are listed for a noun (hyponyms of unique beginners 05, 18,
or 24) and the number of inanimate senses (hyponyms of the remaining unique beginners)
was proposed. Two ratios are then computed for each noun:
N oun animacy (N A) =
N oun inanimacy (N I) =

N umber of animate senses
T otal number of senses
N umber of inanimate senses
T otal number of senses

and compared to pre-defined thresholds in order to classify them as animate or inanimate.
Similarly, in the case of nouns that are the heads of subject NPs, counts are made of the
animate and inanimate senses of the verbs they are subjects of and used to calculate Verb
animacy (VA) and Verb inanimacy (VI) in the same way as NA and NI. These ratios
are also used to determine the animacy of the subject NP. Finally, contextual rules (e.g.
the presence of NP-internal complementizers and reflexives such as who or herself ) are
applied in order to improve the classification. The algorithm is presented in Algorithm 1
and evaluated in Section 5. The three thresholds used in the algorithm were determined
through experimentation and the best values were found to be t1 = 0.71, t2 = 0.92 and
t3 = 0.90.
4.2 Machine Learning for Animacy Identification
The method presented in the previous section has two main weaknesses. The first one is
that the unique beginners used to determine the number of animate/inanimate senses are
too general, and in most cases they do not reliably indicate the animacy of each sense in the
class. The second weakness is due to the nave nature of the rules that decide whether a NP
is animate or not. Their application is simple and involves a comparison of values obtained
for a NP with threshold values that were determined on the basis of a relatively small
number of experiments. In light of these problems, a two step approach, each addressing
one of the aforementioned weaknesses, was proposed. In the first step, an annotated corpus
is used to determine the animacy of WordNet synsets. This process is presented in Section
4.2.1. Once this information is propagated through the whole of WordNet, it is used by a
machine learning algorithm to determine the animacy of NPs. This method is presented in
Section 4.2.2.
6. The social unique beginner subsumes relations such as abdicate, educate and socialize.

85

fiOrasan & Evans

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Data: NP is the noun phrase for which animacy has to be determined, t1 , t2 , t3
Result: The animacy of the NP
Compute NA, NI, VA, VI for NP;
if N A > t1 then
NP if animate;
Stop;
end
if N I > t2 then
NP is inanimate;
Stop;
end
if (N A > N I) and (V A > V I) then
NP is animate;
Stop;
end
if (NP contains the complementizer who) or (V A > t3 ) then
NP is animate;
Stop;
end
NP is inanimate;
1: The rule-based algorithm used to determine the animacy of a noun phrase

4.2.1 The Classification of the Senses
As previously mentioned, the unique beginners are too general to be satisfactorily classified
as wholly animate or inanimate. However, this does not mean that it is not possible to
uniquely classify more specific senses as animate or inanimate. In this section, a corpusbased method which classifies synsets from WordNet according to their animacy is presented.
The starting point for classifying the synsets was the information present in our
annotated version of the SEMCOR corpus. The reason for this is that by adding our
animacy annotation to nouns which were annotated with their corresponding sense from
WordNet, this information could be used to determine the animacy of the synset. However,
due to linguistic ambiguities and tagging errors not all the senses can be classified adequately
in this way. Moreover, many senses from WordNet do not appear in SEMCOR, which means
that no direct animacy information can be determined for them. In order to address this
problem, the decision was made to use a bottom up procedure which begins by classifying
unambiguous terminal nodes and then propagates this information to more general nodes.
A terminal node is unambiguously classified using the information from the annotated files
if all its occurrences in the corpus are annotated with the same class. In the same way, a
more general node can be unambiguously classified if all of its hyponyms have been assigned
to the same class.
Due to annotation errors or rare uses of a sense, this condition is rarely met and a
statistical measure must be employed in order to test the animacy of a more general node.
A simple approach which classifies a synset using a simple voting procedure on behalf of its

86

fiNP Animacy Identification for Anaphora Resolution

Figure 2: Example showing the propagation of animacy from the corpus to more general
senses
hyponyms will be unsatisfactory because it is necessary to know when a node is too general
to be able to assign it to one of the classes. For this reason a statistical measure was used
to determine the animacy of a node in ambiguous cases.
The statistical measure used in this process is chi-squared, a non-parametric test which
can be used to estimate whether or not there is any significant difference between two
different populations. In order to test whether or not a node is animate, the two populations
to be compared are:
1. an observed population which consists of the senses of the nodes hyponyms which
were annotated as animate, and
2. a hypothetical population in which all of the nodes hyponyms are animate.
If chi-square indicates that there is no difference between the two populations then the
node is classified as animate. The same process is repeated in order to classify an inanimate
node. If neither test is passed, it means that the node is too general, and it and all of its
hypernyms can equally refer to both animate and inanimate entities. In unambiguous cases
(i.e. when all the hyponyms observed in the corpus7 are annotated as either animate or
inanimate, but not both), the more general node is classified as its hyponyms are. The way
in which information is propagated from the corpus into WordNet is presented in Figure 2.
To illustrate, for a more general node which has n hyponyms the contingency table
(Table 2) can be built and used to determine its animacy. Each hyponym is considered
7. Either directly or indirectly via hyponymy relations.

87

fiOrasan & Evans

Observed
Expected

Sense1
ani1
ani1 + inani1

Sense2
ani2
ani2 + inani2

Sense3
ani3
ani3 + inani3

...
...
...

Sensen
anin
anin + inanin

Table 2: Contingency table for testing the animacy of a hypernym

to have two attributes: the number of times it has been annotated as animate (anii ) and
the number of times it has been annotated as inanimate (inanii ). The figures for anii and
inanii include both the number of times that the sense directly appears in the corpus and
the number of times it appears indirectly via its hyponyms. Given that the system is testing
to see whether the more general node is animate or not, for each of its hyponyms, the total
number of occurrences of a sense in the annotated corpus is the expected value (meaning
that all the instances should be animate and those which are not marked as animate are
marked that way because of annotation error or rare usage of the sense) and the number
of times the hyponym is annotated as referring to an animate entity is the observed value.
Chi-square is calculated, and the result is compared with the critical level obtained for n  1
degrees of freedom and a significance level of .05. If the test is passed, the more general
node is classified as animate.
In order to be a valid test of significance, chi-square usually requires expected frequencies
to be 5 or more. If the contingency table is larger than two-by-two, some few exceptions
are allowed as long as no expected frequency is less than one and no more than 20% of the
expected frequencies are less than 5 (Sirkin, 1995). In the present case it is not possible for
expected frequencies to be less than one because this would entail no presence in the corpus.
If, when the test is applied, more than 20% of the senses have an expected frequency less
than 5, the two similar senses with the lowest frequency are merged and the test is repeated.8
If no senses can be merged and still more than 20% of the expected frequencies are less
than 5, the test is rejected.
This approach is used to classify all the nodes from WordNet as animate, inanimate
or undecided. The same approach is also employed to classify the animacy of verbs on
the basis of the animacy of their subjects. An assessment of the coverage provided by the
method revealed that almost 94% of the nodes from WordNet can be classified as animate
or inanimate. This is mainly due to the fact that some very general nodes such as person,
plant or abstraction can be classified without ambiguity and as a result all their hyponyms
can be classified in the same way. This enriched version of WordNet is then used to classify
nouns as described in the next section.
4.2.2 The Classification of a Noun
The classification described in the previous section is useful for determining the animacy of
a sense, even for those which were not previously found in the annotated corpus, but which
are hyponyms of a node that has been classified. However, nouns whose sense is unknown
cannot be classified directly and therefore an additional level of processing is necessary. In
8. In this context, two senses are considered similar if they both have the same attribute (i.e. animacy or
inanimacy) equal to zero.

88

fiNP Animacy Identification for Anaphora Resolution

this section, the use of timbl (Daelemans, Zavrel, van der Sloot, & van den Bosch, 2000)
to determine the animacy of nouns is described.
Timbl is a program which implements several machine learning techniques.
Experimenting with the algorithms available in timbl with different configurations, the best
results were obtained using instance-based learning with gain ratio as the weighting measure
(Quinlan, 1993; Mitchell, 1997). In this type of learning, all the instances are stored without
trying to infer anything from them. At the classification stage, the algorithm compares a
previously unseen instance with all the data stored at the training stage. The most frequent
class in the k nearest neighbors is assigned as the class to which that instance belongs. After
experimentation, it was noticed that the best results were obtained when the three nearest
neighbors were used (k=3), the distance between two instances is calculated using overlap
metric and the importance of each feature is weighted using gain ratio (Daelemans et al.,
2000).
In the present case, the instances used in training and classification consist of the
following information:
1. The lemma of the noun which is to be classified.
2. The number of animate and inanimate senses of the word. As mentioned before, in
the cases where the animacy of a sense is not known, it is inferred from its hypernyms.
If this information cannot be found for any of a words hypernyms, information on
the unique beginners for the words sense is used, in a manner similar to that used by
the rule-based system described in Section 4.1.
3. For the heads of subject NPs, the number of animate/inanimate senses of its verb.
For those senses for which the classification is not known, an algorithm similar to the
one described for nouns is employed. These values are 0 for heads of non-subjects.
4. The ratio of the number of animate singular pronouns (e.g he or she) to inanimate
singular pronouns (e.g. it) in the whole text. The justification for this feature is that
a text containing a large number of gender marked pronouns will be more likely to
mention many animate entities
These features were encoded as vectors to be classified by timbl using the algorithm and
settings described earlier. The algorithm described in this section is evaluated in Section 5.
4.3 Word Sense Disambiguation
It is difficult to disambiguate the possible senses of words in unrestricted texts, but it is not
so difficult to identify those senses which are more likely to be used in a text than others.
Such information was not considered in the methods presented in Sections 4.1 and 4.2.
Instead, in those methods, all the senses were considered to have an equal weight. In order
to address this shortcoming, the word sense disambiguation (WSD) method described by
Resnik (1995) was implemented and used in the classification algorithm. The WSD method
computes the weight of each possible sense of each noun by considering the other nouns in
a text. These weights were used to compute the number of animate/inanimate senses. The
underlying hypothesis is that the animacy/inanimacy of senses which are more likely to be
89

fiOrasan & Evans

used in a particular text should count more than that of improbable senses. The impact of
this approach on the animacy identifiers presented in the previous section is also evaluated.

5. Evaluation of the Systems
In this section, the systems presented in Section 4 are evaluated using intrinsic and
extrinsic evaluation methods (Sparck Jones & Galliers, 1996). Both evaluation methods
are necessary because the aim is not only to find out which of the methods can classify
references to animate entities most accurately, but also to assess how appropriate they are
for inclusion into an anaphora resolution method. In addition, the complexity of the systems
is considered.
In order to increase the reliability of the evaluation, the systems are assessed on both
corpora described in Section 3. The thresholds used in the simple method presented in
Section 4.1 were determined through direct observation of the performance results when
the system was applied to the AI corpus. Evaluating the method on the SEMCOR corpus
allows its performance to be measured on completely unseen data. In addition, the texts
from SEMCOR are in a completely different genre from AI, allowing an assessment to be
made of the degree to which the system described in Section 4.1 is genre independent.
Evaluation raises more serious problems when the machine learning method is
considered. As is well known, whenever a machine learning method is evaluated, a clear
distinction has to be made between training data and testing data. In the case of the
system described in Section 4.2, the approach was evaluated using 10-fold cross-validation
over the SEMCOR corpus. Given that the AI corpus is available, the systems can also
be evaluated on data from a domain which was not used in setting the parameters of the
machine learning method. In addition, the evaluation of the machine learning methods on
the AI corpus is useful in proving that the classification of the synsets from WordNet on
the basis of the animacy annotation added to SEMCOR can be used to develop a system
whose performance is not text-dependent.
5.1 Intrinsic Evaluation
Intrinsic evaluation methods measure the accuracy of a system in performing the task which
it was designed to carry out. In the present case, it is the accuracy with which an entity can
be classified as animate or inanimate. In order to assess the performance of the systems,
four measures are considered:
Correctly classif ied items
T otal number of items

(1)

T rue positives
T rue positives + F alse positives

(2)

Accuracy =
P recision =
Recall =

T rue positives
T rue positives + F alse negatives

F measure =

2  P recision  Recall
P recision + Recall

90

(3)
(4)

fiNP Animacy Identification for Anaphora Resolution

Figure 3: Evaluation of methods on AI corpus
The accuracy (1) measures how well a system can correctly classify a reference to an
entity as animate or inanimate, but it can be misleading because of the large number of
inanimate entities mentioned in texts. As is clear from Table 1, even though the texts were
chosen so as to contain a large number of references to animate entities, the ratio between
the number of references to animate entities and inanimate entities is approximatively 1
to 7.5 for SEMCOR, and 1 to 4.8 for AI. This means that a method which classifies all
references to entities as inanimate would have an accuracy of 88.21% on SEMCOR and
82.77% on AI. As can be seen in Figures 3 and 4, as well as in Table 4, these results are
not very far from the accuracy obtained by the system described in Section 4.1. However,
as mentioned earlier, the intention is to use the filtering of references to animate entities
for anaphora resolution and therefore, the use of a filter which classifies all the references
as inanimate would be highly detrimental.
It is clearly important to know how well a system is able to identify references to animate
and inanimate entities. In order to measure this, precision (2) and recall (3) are used for
each class. The precision with which a system can identify animate references is defined
as the ratio between the number of references correctly classified by the system as animate
and the total number of references it classifies as animate (including the wrongly classified
ones). A methods recall in classifying references to animate entities is defined as the ratio
between the number of references correctly classified as animate and the total number of
animate references to be classified. The precision and recall of inanimate classification is
defined in a similar manner. The f-measure (4) combines precision and recall into one value.
Several formulae for f-measure were proposed, the one used here gives equal importance to
precision and recall.
Figures 3 and 4, as well as Table 4 at the end of the paper, present the accuracy of
the classification, and f-measures for classifying the animate and inanimate references. In
addition to the methods presented in Section 4, three baseline methods were introduced.
The first one classifies a reference to an entity as animate or inanimate on a random basis
and is referred to in the figures as baseline. A second random baseline was introduced

91

fiOrasan & Evans

Figure 4: Evaluation of methods on SEMCOR corpus
because it was assumed that the number of gender marked pronouns in a text can indicate
how likely it is that a particular noun appearing in that text will be animate or inanimate.
In this case, the probability of a reference to be animate is proportional to the number of
animate pronouns in the text and the classification is made on a weighted random basis.
A similar rule applies for inanimate references. This second baseline is referred to in the
figures as W-baseline. For purposes of comparison, a method was included which classifies
all references as inanimate. This method is referred to as the dummy method.
Figures 3 and 4 show that all the other methods significantly outperform the baselines
used. Close investigation of the figures, as well as of Table 4, shows that, for both corpora,
the best method is the one which uses machine learning (presented in Section 4.2). It obtains
high accuracy when classifying references to both animate and inanimate entities. In terms
of accuracy, the simple method performs unexpectedly well, but it fails to accurately classify
references to animate entities. Moreover, comparison with the dummy method on both files
shows that the results of the simple method are not much better, which suggests that
the simple method has a bias towards recognition of references to inanimate entities. The
integration of word sense disambiguation yields mixed results: it increases the accuracy
of the simple method, but it slightly decreases the performance of the machine learning
method.
The relatively poor accuracy of the Simple system was expected and can be explained
by the fact that the unique beginners, which are used as the basis for classification in that
method, cover a range of senses which is too wide to be classified as belonging to a single
animate or inanimate class. They are too general to be used as the basis for accurate
classification. Additionally, the rules used to assist classification only provide limited recall
in identifying animate references.
Comparison between the accuracy of the machine learning method and the level of interannotator agreement reveals that the automatic method agrees with the first annotator more
than the second annotator does. As a result of this, it can be concluded that the accuracy
of the automatic method matches human performance levels.

92

fiNP Animacy Identification for Anaphora Resolution

Figure 5: The accuracy of mars when the different animacy filters are applied
5.2 Extrinsic evaluation
In the previous section, the performance of the classification methods was evaluated and it
was demonstrated that even simple methods can achieve high accuracy at the expense of low
precision and recall in the classification of references to animate entities. In computational
linguistics, the output of one method is often used as the input for another one, and therefore
it is important to know how the results of the first method influence the results of the
second. This kind of evaluation is called extrinsic evaluation. Given that the identification
of references to animate entities is not very useful in its own right, but can be vital for tasks
like anaphora resolution, it is necessary to perform extrinsic evaluation too. In the case
of this evaluation, the assumption is that the performance of anaphora resolution can be
improved by filtering out candidates which do not agree in animacy with each referential
pronoun.
The influence of animacy identification on anaphora resolution is thus evaluated using
mars (Mitkov, Evans, & Orasan, 2002), a robust anaphora resolver which relies on a set of
boosting and impeding indicators to select an antecedent from a set of competing candidates.
Due to the fact that the evaluation of mars requires the manual annotation of pronouns
antecedents, which is a time consuming task, this evaluation was carried out only on a part of
the corpus. To this end, the entire Amnesty International corpus as well as 22 files from the
SEMCOR corpus have been used. Given that the animacy identifier can only influence the
accuracy of anaphora resolvers with respect to third person singular pronouns, the accuracy
of the resolver is reported only for these pronouns. Accuracy in anaphora resolution was
calculated as the ratio between the number of pronouns correctly resolved and the total
number of third person singular pronouns appearing in the test data. Figure 5 and Table
5 display this accuracy for alternate versions of mars that exploit different methods for
animacy identification.
Mars was designed to process texts from the technical domain, and therefore its
performance is rather poor on this test corpus. Moreover, its performance can vary greatly
from one domain to another. In light of the fact that the results of a different anaphora
resolver may be very different on the same set of data, in addition to the performance of
93

fiOrasan & Evans

Figure 6: The average number of candidates and the percentage of pronouns without correct
candidates when different animacy filters are applied
mars with respect to third person singular pronouns, Figure 6 and Table 5 also present
the reduction in the number of candidates that results from the animacy filtering, and the
increase in the number of pronouns whose sets of competing candidates contain no valid
antecedents as a result of this filtering. The former number is presented as the average
number of candidates per pronoun, and the latter as the percentage of pronouns without
valid antecedents in the list of candidates. The justification for reporting these two measures
is that a good animacy filter will eliminate as many candidates as possible, but will not
eliminate antecedents and leave pronouns without any correct candidates to be resolved to.9
As can be seen in Figure 5 and Table 5, regardless of which animacy identification
method is used, the accuracy of the anaphora resolver improves. The degree of improvement
varies from one corpus to another, but the pattern regarding the reduction in the number of
candidates and the increase in the number of pronouns whose sets of competing candidates
contain no valid antecedent is the same across both corpora. For the AI corpus, the best
performance is obtained when the simple method enhanced with word sense disambiguation
is used, followed by the simple method. Both improvements are statically significant10 , as
well as the difference between them. Both versions of the machine learning method improve
the success rate of mars by a small margin which is not statistically significant, but they
increase the number of pronouns with no valid antecedent to select by only one, an increase
which is not statistically significant. For the simple methods, the increase in the number
of this type of pronoun is much larger and is statistically significant. Therefore in the
case of the AI corpus, it can be concluded that, for mars, a more aggressive method for
filtering out candidates, such as the simple method with word sense disambiguation, is more
appropriate. However, it is possible that for other anaphora resolution methods this result
is not valid because they may be more strongly influenced by the increase in the number of
pronouns with no valid antecedent to select.
9. It should be noted that, even without filtering, there are pronouns which do not have any candidates
due to errors introduced by preprocessing tools such as the NP extractor which fails to identify some of
the NPs.
10. In all the cases where we checked whether the differences between two results are statistically significant
we used t-test with 0.05 confidence level.

94

fiNP Animacy Identification for Anaphora Resolution

Processing the SEMCOR corpus, the best results for mars are obtained by the machine
learning method without the WSD module followed by the one which performs WSD.
In both cases the increase over the performance of the unfiltered version is statistically
significant, but the differences between the two machine learning methods are too small to
be significant. In addition, these two methods ensure a large reduction in the number of
candidates with the smallest increase in the number of pronouns whose sets of competing
candidates contain no valid antecedent, an increase which is not significant.
As expected, the three baselines perform rather poorly. All three of them reduce the
number of candidates at the expense of a high increase in the number of pronouns with no
valid antecedent available for selection. Both the reduction in the number of candidates and
the increase in the number of pronouns with no valid antecedent are statistically significant
when compared to the system that does not use any filtering.
The results of marss performance are rather mixed when these baselines are used. For
the AI corpus, the random baseline leads to a better result for mars than the machine
learning methods, but the differences are not statistically significant. However, this is
achieved with a large increase in the number of pronouns which cannot be correctly resolved
because all their valid antecedents have also been filtered by the method. For the AI corpus,
application of the other two baselines led to results worse than or equal to those of mars
when no filtering is applied as a result of the large drop in the number of candidates.
For the SEMCOR corpus, all three baselines give rise to statistically significant
improvements in performance levels over those obtained when no filtering is applied, but
this is achieved by dramatically reducing the number of candidates considered. Integration
of the dummy method into mars leads to results which are better than the simple methods
but, as argued before, this method is not appropriate for anaphora resolvers because it
prevents them from correctly resolving any animate pronoun.
Investigation of the results revealed that for about 31% of the candidates it was not
possible to apply any animacy filtering. There are three reasons for this. First, in the
majority of cases, candidates are named entities which, as mentioned in Section 2, are not
tackled by our method, though they constitute a relatively high proportion of the noun
phrases occurring in the chosen texts. A second reason for these cases is the fact that some
of the words are not present in WordNet and as a result, they are ignored by our method.
Finally, in a limited number of cases the noun phrases identified by mars did not match
those identified by our animacy identifiers and for this reason it was not possible to classify
them.11
5.3 Extrinsic Evaluation on Simulated Data
The results presented in the previous section makes it difficult to have a clear idea about how
accurate the animacy identifier needs to be in order to have a significant positive influence
on the performance of mars. In light of this, we performed an experiment in which animacy
identifiers which perform with a predefined accuracy were simulated. These systems were
designed in such a way that the precision of animacy identification varies in 1% increments
11. The animacy identifiers proposed in this paper use both the NP and its context (i.e. the verb on which
it depends and the number of pronouns in the text) and therefore they have to be run independently
from any other module which uses their results.

95

fiOrasan & Evans

Figure 7: The evolution of success rate with changes in precision and recall
from 10% to 100%, whilst recall varies from 50% to 100%.12 In order to achieve this, we
introduced a controlled number of errors in the annotated data by randomly changing the
animacy of a predetermined number of noun phrases. In order to ensure fair results, the
experiment was run 50 times for each precision-recall pair, so that a different set of entities
were wrongly classified in each run. The list of classified entities (in this case derived
directly from the annotated data and not processed by any of the methods described in this
paper) was then used by MARS in the resolution process. Figure 7 presents the evolution
of success rate as recall and precision are changed. In order to see how the success rate
is influenced by the increase in recall, we calculated the success rates corresponding to the
chosen recall value and all the values for precision between 10% and 100% and averaged
them. In the same way we calculated the evolution of success rate with changes in precision.
As can be seen in the figures, precision has a greater influence on the success rate of
mars than recall because by increasing precision, we notice an almost continuous increase
in the success rate. Overall, increasing recall also leads to an increase in the success rate,
but this increase is not smooth. On the basis of this, we can conclude that high precision
of animacy identification is more important than recall. These results are also supported
by Table 5 where the Simple method leads to good performance for mars despite its low
recall (but higher precision) in the identification of animate entities.
Our experiments also reveal that for values higher than 80% for precision and recall, the
success rate can vary considerably. For this reason we decided to focus on this region. Figure
8 presents the success rate corresponding to different precision-recall pairs using a contour
chart. The darker areas correspond to higher values of success rate. As noticed before, the
areas which correspond to high precision and high recall also feature high success rates,
but it is difficult to identify clear thresholds for precision and recall which lead to improved
performance especially because most of the differences between the first four intervals are
not statistically significant.
12. We decided to control only the precision and recall of animacy identification because in this way,
indirectly, we also control the recall and precision of the identification of inanimate entities.

96

fiNP Animacy Identification for Anaphora Resolution

Figure 8: Contour chart showing the success rate for different values of precision and recall
5.4 The Complexity of the Systems
One aspect which needs to be considered whenever a system is developed is its complexity.
This becomes a very important issue whenever such a system is integrated with a larger
system, which needs to react promptly to its input (e.g. systems which are available over
the Web). In the present case, each method presented in Section 4 is more complex than
the previous one, and therefore requires more time to run. Table 3 shows the time necessary
to run each system on the two corpora. As can be seen, the fastest method is the simple
method which has a complexity proportional to n*m where n is the number of entities in
the entire corpus, and m is the average number of senses for each word in WordNet. The
method which uses machine learning is slower because it has to prepare the data for the
machine learning algorithm, a process which has a similar complexity to the simple method,
and in addition it has to run the memory-based learning algorithm, which compares each
new instance with all instances already seen. Even though timbl, the machine learning
algorithm used, employs some sophisticated indexing techniques to speed up the process,
for large training sets, the algorithm is slow. It has been noted that k-NN is an extremely
slow classifier and the use of alternate ML algorithms, such as maximum entropy, may lead
to quicker classification times with no loss in accuracy. When word sense disambiguation is
97

fiOrasan & Evans

Method
Simple method
ML
Simple+WSD
ML+WSD

AI
SEMCOR
3 sec.
25 sec.
51 sec.
286 sec.
Several hours
Several hours

Table 3: The run time necessary for different methods

used, the processing time increases dramatically, because the complexity of the algorithm
used is nm where n is the number of distinct nouns from a text to be disambiguated, and m
is the average number of senses from WordNet for each noun. When the performance and
run time of the methods is considered, the best performing method is ML, which ensures
high accuracy together with relatively low execution time. The use of an alternate WSD
method that exploits N-best lists, rather than considering all possible assignments of word
senses, would be likely to improve the speed of disambiguation. An approach of this type
has not yet been tested in our current work.

6. Related Work
With regard to work concerned with recognition of NP animacy, the sole concern in
this section is with those methods which tackle the problem in English texts, a problem
concerned with semantics that cannot be addressed using morphological information, as it
can be in other languages.
Identification of the specific gender of proper names has been attempted by Hale and
Charniak (1998). That method works by processing a 93931-word portion of the PennTreebank corpus with a pronoun resolution system and then noting the frequencies with
which particular proper nouns are identified as the antecedents of feminine or masculine
pronouns. Their paper reports an accuracy of 68.15% in assigning the correct gender to
proper names.
The approach taken by Cardie and Wagstaff (1999) is similar to the simple statistical one
described in Section 4.1, though the one described in this paper exploits a larger number of
unique beginners in the ontology, considers semantic information about the verbs for which
NPs are arguments, and also considers all possible senses for each noun. In the approach
presented by Cardie and Wagstaff (1999), nouns with a sense subsumed by particular nodes
in the WordNet ontology (namely the nodes human and animal) are considered animate.
In terms of gender agreement, gazetteers are also used to assign each NP with a value for
gender from the set of masculine, feminine, either (which can be assumed to correspond
to animate), or neuter. The method employed by Cardie and Wagstaff is fairly simple
and is incorporated as just one feature in a vector used to classify coreference between
NPs. The employed machine learning method blindly exploits the value assigned to the
animacy feature, without interpreting it semantically. WordNet has been used to identify
NP animacy in work by Denber (1998). Unfortunately, no evaluation of the task of animacy
identification was reported in those papers.

98

fiNP Animacy Identification for Anaphora Resolution

7. Conclusions
Animacy identification is a preprocessing step which can improve the performance of
anaphora resolution in English by filtering out candidates which are not compatible, in
terms of their agreement features, with the referential expression. In this paper, a more
specific definition for animacy is used than the one proposed by Quirk et al. (1985). The
adopted definition is more appropriate and conveys the usefulness of this feature in anaphora
resolution. In the present study, the animacy of a noun phrase is determined by the fact
that it can be referred to by means of a masculine or feminine pronoun as well as their
possessive, relative and reflexive forms.
In this paper, two different animacy identifiers were presented and evaluated. The
first one relies on the unique beginners from WordNet in combination with simple rules to
determine the animacy of a noun phrase. Given that the unique beginners are too general
to be used in this way and that the rules were designed through nave observations, a second
method was proposed. This second approach relies on a machine learning method and an
enhanced WordNet to determine the animacy of a noun phrase. In addition to the normal
semantic information, this enhanced WordNet contains information about the animacy of
a synset. This animacy information was automatically calculated on the basis of manual
annotation of the SEMCOR corpus with animacy information.
The two animacy identifiers were evaluated using intrinsic and extrinsic methods. The
intrinsic evaluation employed several measures to determine the most appropriate identifier.
Comparison between the results of these methods revealed that it is easy to obtain relatively
high overall accuracy at the expense of low accuracy for the classification of animate
references. For this reason, it was concluded that the extra resources required by the
machine learning method, the best performing method, are fully justified. Inter-annotator
agreement was measured in order to ascertain the difficulty of the task and as a result of this,
it was noted that the machine learning method reaches a level of performance comparable
to that of humans.
The extrinsic evaluation focused on how the performance of mars, a robust anaphora
resolver, is influenced by the animacy identifier. In light of the fact that mars was designed
to resolve anaphors in texts from a different genre, the results reported in the extrinsic
evaluation did not focus only on the accuracy of that system, but also on how many
candidates are removed by the animacy identifier and how many pronouns are left with
no valid antecedent to select from their sets of candidates as a result of this process.
Evaluation of mars revealed that both of the methods proposed in this paper improve its
accuracy, but the degree of improvement varies from one corpus to another. In terms of the
reduction of the number of candidates that the anaphora resolution system has to consider,
the machine learning method eliminates the fewest candidates, but as a result it only evokes
small increases in the number of pronouns whose sets of competing candidates contain no
valid antecedents. For this reason, we argue that extrinsic evaluation also shows that the
machine learning approach is the most appropriate method to determine the animacy of
noun phrases.
Experiments with WSD produced mixed results. Only on one of the corpora used in
this research did it lead to small improvements in performance. We thus conclude that the
extra computation required in order to disambiguate words is unnecessary.

99

fi28.24%
29.23%
67.73%
70.83%
94.20%
93.65%

82.11%
79.27%
82.77%
88.93%
91.60%
98.33%
98.34%

50.32%
20.60%
100%
99.24%
96.66%
99.26%
99.07%

62.39%
32.70%
90.57%
93.80%
94.06%
98.79%
98.70%

22.05%
15.09%
68.90%
76.50%
90.93%
90.05%

86.19%
88.41%
88.21%
91.81%
93.94%
98.75%
98.59%

50.14%
31.64%
100%
98.51%
98.38%
98.57%
98.56%

63.39%
46.60%
93.73%
95.04%
96.11%
98.65%
98.57%

Table 4: The results of the classification

Orasan & Evans

Prec

Acknowledgments

100

Random baseline
Weighted baseline
Dummy method
Simple system
Simple system + WSD
Machine learning system
Machine learning with WSD

Inanimacy
Recall F-meas

F-meas

We would like to thank Laura Hasler for helping us with the annotation process and the
three referees for their useful comments which enabled us to improve the paper.

Random baseline
Weighted baseline
Dummy method
Simple system
Simple system + WSD
Machine learning system
Machine learning with WSD

Animacy
Prec
Recall
On AI corpus
50.60% 19.37% 52.13%
31.01% 18.07% 76.48%
82.77%
0%
89.61% 94.79% 52.69%
90.14% 81.60% 62.57%
98.04% 96.31% 92.19%
97.85% 95.37% 92.00%
On SEMCOR corpus
50.19% 14.11% 50.49%
37.62% 8.40% 74.44%
88.21%
0%
91.42% 88.48% 56.42%
93.33% 88.88% 67.14%
97.72% 91.91% 89.99%
97.51% 89.97% 90.14%
Acc

Appendix A. Tables

Experiment

fiNo filtering
Simple
Simple + WSD
Machine learning
Machine learning + WSD
Random baseline
Weighted baseline
Dummy method

101

No filtering
Simple
Simple + WSD
Machine learning
Machine learning + WSD
Random baseline
Weighted baseline
Dummy method

Average candidates per pronouns Percentage of pronouns without antecedent
Results on the AI Corpus: 215 animate pronouns
17.20
20.46
12.37
26.04
12.47
24.18
13.71
20.93
13.70
20.93
9.95
33.02
10.57
40.46
9.17
42.32
Results on part of SEMCOR: 1250 animate pronouns
10.20
24.80
8.44
26.96
8.66
26.88
8.33
26.32
8.33
26.32
7.55
33.12
7.28
36.16
7.83
38.16

Table 5: The results of extrinsic evaluation

MARS accuracy
40.00%
43.26%
45.58%
40.93%
40.93%
41.40%
38.60%
40.00%
29.60%
37.60%
37.50%
39.60%
39.52%
36.96%
34.08%
38.16%

NP Animacy Identification for Anaphora Resolution

System

fiOrasan & Evans

References
Barbu, C., Evans, R., & Mitkov, R. (2002). A corpus based analysis of morphological
disagreement in anaphora resolution. In Proceedings of Third International Conference
on Language Resources and Evaluation (LREC2002), pp. 1995  1999 Las Palmas de
Gran Canaria, Spain.
Barlow, M. (1998). Feature mismatches and anaphora resolution.
DAARC2, pp. 34  41 Lancaster, UK.

In Proceedings of

Brennan, S. E., Friedman, M. W., & Pollard, C. J. (1987). A centering approach to
pronouns. In Proceedings of the 25th Annual Metting of the ACL, pp. 155  162
Stanford, California.
Cardie, C., & Wagstaff, K. (1999). Noun phrase coreference as clustering. In Proceedings of
the 1999 Joint SIGDAT conference on Emphirical Methods in NLP and Very Large
Corpora (ACL99), pp. 82  89 University of Maryland, USA.
Chomsky, N. (1981). Lectures on Government and Binding. Dordrecht: Foris.
Cristea, D., Ide, N., Marcu, D., & Tablan, V. (2000). An empirical investigation of the
relation between discourse structure and co-reference. In Proceedings of the 18th
International Conference on Computational Linguistics (COLING2000), pp. 208 
214 Saarbrucken, Germany.
Daelemans, W., Zavrel, J., van der Sloot, K., & van den Bosch, A. (2000). TiMBL: Tilburg
memory based learner, version 3.0, reference guide, ilk technical report 00-01. Ilk
00-01, Tilburg University.
Denber, M. (1998). Automatic resolution of anaphora in English. Tech. rep., Eastman
Kodak Co, Imaging Science Division.
Evans, R., & Orasan, C. (2000). Improving anaphora resolution by identifying animate
entities in texts. In Proceedings of the Discourse Anaphora and Reference Resolution
Conference (DAARC2000), pp. 154  162 Lancaster, UK.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. The MIT Press.
Frances, W., & Kucera, H. (1982). Frequency Analysis of English Usage. Houghton Mifflin,
Boston.
Hale, J., & Charniak, E. (1998). Getting useful gender statistics from English text. Tech.
rep. CS-98-06, Brown University.
Hobbs, J. (1976). Pronoun resolution. Research report 76-1, City College, City University
of New York.
Hobbs, J. (1978). Pronoun resolution. Lingua, 44, 339352.

102

fiNP Animacy Identification for Anaphora Resolution

Kennedy, C., & Boguraev, B. (1996). Anaphora for everyone: pronominal anaphora
resolution without a parser. In Proceedings of the 16th International Conference on
Computational Linguistics (COLING96), pp. 113  118 Copenhagen, Denmark.
Landes, S., Leacock, C., & Tengi, R. I. (1998). Building semantic concordances. In Fellbaum
(Fellbaum, 1998), pp. 199  216.
Lappin, S., & Leass, H. J. (1994). An algorithm for pronominal anaphora resolution.
Computational Linguistics, 20 (4), 535  562.
Mitchell, T. M. (1997).
McGraw-Hill.

Machine learning.

McGraw-Hill Series in Computer Science.

Mitkov, R. (1998). Robust pronoun resolution with limited knowledge. In Proceedings of the
18th International Conference on Computational Linguistics (COLING98/ACL98),
pp. 867  875 Montreal, Quebec, Canada.
Mitkov, R. (2002). Anaphora resolution. Longman.
Mitkov, R., Evans, R., & Orasan, C. (2002). A new, fully automatic version of Mitkovs
knowledge-poor pronoun resolution method. In Proceedings of CICLing-2002, pp. 168
 186 Mexico City, Mexico.
Ng, V., & Cardie, C. (2002). Improving machine learning approaches to coreference
resolution. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL2002), pp. 104  111 Philadelphia, Pennsylvania.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.
Quirk, R., Greenbaum, S., Leech, G., & Svartvik, J. (1985). A Comprehensive Grammar
of the English Language. Longman.
Resnik, P. (1995). Disambiguating noun groupings with respect to Wordnet senses. In
Yarovsky, D., & Church, K. (Eds.), Proceedings of the Third Workshop on Very Large
Corpora, pp. 5468 Somerset, New Jersey. Association for Computational Linguistics.
Sirkin, R. M. (1995). Statistics for the social sciences. SAGE Publications.
Sparck Jones, K., & Galliers, J. R. (1996). Evaluating natural language processing systems:
an analysis and review. No. 1083 in Lecture Notes in Artificial Intelligence. Springer.
Tapanainen, P., & Jarvinen, T. (1997). A non-projective dependency parser. In Proceedings
of the 5th Conference of Applied Natural Language Processing, pp. 64  71 Washington
D.C., USA.

103

fiJournal of Artificial Intelligence Research 29 (2007) 1947

Submitted 03/06; published 05/07

Computationally Feasible VCG Mechanisms
Noam Nisan

noam@cs.huji.ac.il

School of Computer Science and Engineering,
The Hebrew University of Jerusalem, Israel

Amir Ronen

amirr@ie.technion.ac.il

Faculty of Industrial Engineering & Management,
Technion - Israel Institute of Technology,
Haifa 32000, Israel

Abstract
A major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When applying
this method to complex problems such as combinatorial auctions, a diculty arises: VCG
mechanisms are required to compute optimal outcomes and are, therefore, computationally infeasible. However, if the optimal outcome is replaced by the results of a sub-optimal
algorithm, the resulting mechanism (termed VCG-based) is no longer necessarily truthful.
The rst part of this paper studies this phenomenon in depth and shows that it is near
universal. Specically, we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield
non-truthful VCG-based mechanisms. We generalize these results for ane maximizers.
The second part of this paper proposes a general method for circumventing the above
problem. We introduce a modication of VCG-based mechanisms in which the agents are
given a chance to improve the output of the underlying algorithm. When the agents behave
truthfully, the welfare obtained by the mechanism is at least as good as the one obtained
by the algorithms output. We provide a strong rationale for truth-telling behavior. Our
method satises individual rationality as well.

1. Introduction
Mechanism design is a sub-eld of game theory and microeconomics that studies the design of protocols for non-cooperative environments. In such environments the participating
agents follow their own goals and do not necessarily act as instructed by the mechanism.
This theory has traditionally been applied to economic applications such as auctions of various kinds. An introduction to mechanism design can be found in several books (Osborne
& Rubinstein, 1994; Mas-Collel, Whinston, & Green, 1995). In recent years, problems
on the border of mechanism design and computer science have attracted the attention of
many researchers, both within and outside the AI community. In particular, mechanism
design models were applied to multi-agent systems (Rosenschein & Zlotkin, 1994; Wellman,
Wurman, Walsh, & MacKie-Mason, 2001; Shoham & Tanaka, 1997; Shoham & Tennenholtz, 2001), decentralized resource and task allocations (Nisan & Ronen, 2001; Wellman
et al., 2001; Elkind, Sahai, & Steiglitz, 2004; Porter, Ronen, Shoham, & Tennenholtz, 2002),
economic and electronic commerce applications (Parkes, 1999; Cramton, 1997), and communication networks (Feigenbaum, Papadimitriou, & Shenker, 2000; Anderson, Kelly, &
Steinberg, 2002).
c
2007
AI Access Foundation. All rights reserved.

fiNisan & Ronen

The canonical mechanism design problem can be described as follows: A set of rational
agents needs to collaboratively choose an outcome o from a nite set O of possibilities.
Each agent i has a privately known valuation function v i : O  R quantifying the agents
benet from each possible outcome. The agents are supposed to report their valuation
functions v i () to some centralized mechanism. The goal of the mechanism is to choose an

outcome o that maximizes the total welfare i v i (o). The main diculty is that agents may
choose to misreport their valuations in an attempt to aect the outcome to their liking.
Such manipulations are likely to severely damage the resulting welfare (simulations that
demonstrate this welfare loss can be found in Carroll & Grosu, 2005). The tool that the
mechanism uses to motivate the agents to reveal the truth is monetary payments. These
payments need to be designed in a way that ensures that rational agents always reveal their
true valuations. Mechanisms with this property are called incentive compatible or truthful
(in dominant strategies). To date, only one general method, called VCG (Vickrey, 1961;
Clarke, 1971; Groves, 1973) (or slightly more generally, ane maximization), is known for
designing such a payment structure1 . In some settings, it is known that this method is the
sole available one (Roberts, 1979; Lavi, Nisan, & Mualem, 2003).
Many novel applications of mechanism design are complex and require implementation
on computer systems. Cases in point include combinatorial auctions where multiple items
are concurrently sold in an auction (Cramton, Shoham, & Steinberg, 2006), decentralized
task and resource allocation problems (Nisan & Ronen, 2001; Wellman et al., 2001), and
networking applications (Feigenbaum et al., 2000; Anderson et al., 2002). For many of
these applications, the range of possible outcomes is huge and even nding an outcome that
maximizes the total welfare is known to be NP-complete. Since for such cases computing
the optimal outcome is intractable, the VCG method cannot be applied.
A natural general approach for the development of mechanisms for such cases would be
to use a sub-optimal polynomial time algorithm for computing the outcome, and calculate
the payments by applying the VCG payment rule to the underlying algorithm. We term
such mechanisms VCG-based.
The starting point of this paper is the observation, noted already by some researchers
(Lehmann, OCallaghan, & Shoham, 2002; Nisan & Ronen, 2001), that VCG-based mechanisms are not necessarily truthful. Thus, rational agents may lie, taking advantage of quirks
in the outcome determination algorithm.
1.1 VCG-based Mechanisms are Generally not Truthful
The rst part of this paper examines this last phenomenon in depth and shows that it is
near universal: essentially all reasonable VCG-based mechanisms are not truthful.
We rst turn our attention to combinatorial auctions and characterize the class of truthful VCG-based mechanisms for this problem2 . We say that an allocation algorithm for
1. Recently, a few truthful mechanisms, which are not ane maximizers, were obtained for combinatorial
auctions (Bartal, Gonen, & Nisan, 2003).
2. The importance of combinatorial auctions is twofold. First, they have direct applications such as FCC
auctions. Second, they abstract many problems of resource allocation among self-interested agents. A
comprehensive survey of research on combinatorial auctions can be found in a recent book (Cramton
et al., 2006).

20

fiComputationally Feasible VCG Mechanisms

combinatorial auctions is reasonable if, whenever an item is desired by a single agent only,
this agent receives the item. The above characterization leads to the following corollary:
Theorem: Any truthful VCG-based mechanism for combinatorial auctions is not reasonable (unless it uses the exponential optimal allocation algorithm).
In particular, unless P = N P , every polynomial time, truthful VCG-based mechanism is
not reasonable.
Loosely speaking, we show that essentially the only degree of freedom available to truthful VCG-based mechanisms is the choice of range over which to optimize. Within this range
perfect optimization is needed. This theorem seems intuitive as VCG payments identify
each agents utility with that of society, and thus if the social welfare is not optimized by
the mechanism, then agents are motivated to to lie in order to do so. Yet, such an argument
only shows that the outcome must be locally optimal  with locality dened as a deviation
by a single agent. The heart of our argument is a delicate hybrid argument showing that
in a general context this local optimization essentially implies global optimization.
Next we study a family of problems termed cost minimization allocation problems. This
family contains many natural decentralized task allocation problems such as mechanism
design versions of the shortest path problem (Elkind et al., 2004; Nisan & Ronen, 2001;
Rosenschein & Zlotkin, 1994). We call a mechanism for such a problem degenerate if there
exist inputs that cause it to produce results that are arbitrarily far from the optimal.
Theorem: For any cost minimization allocation problem, any sub-optimal truthful VCGbased mechanism is degenerate.
A word is in order here about the signicance of these results. VCG-based mechanisms are
not just some special case of truthful mechanisms  they are essentially the only general
method known for truthful mechanisms in non-single dimensional settings. Moreover, in
certain settings they are known to indeed be the only truthful mechanisms (Roberts, 1979;
Lavi et al., 2003). More precisely, weighted versions of VCG-based mechanisms  called
ane maximizers  are truthful, but our results extend (as we show) to these cases as well.
Consequently, our results imply that designing truthful mechanisms for computationally
intractable problems requires either restricting the range of the outcomes (getting unreasonable or degenerate mechanisms) or developing entirely new techniques for truthful
mechanisms  which may not even exist. A similar implication results if the intractability
stems not from computational considerations, but rather from communication considerations (Cramton et al., 2006, Chapter 11).
1.2 The Second Chance Mechanism
The second part of this paper proposes a general method for circumventing the diculty
of constructing truthful mechanisms. While VCG-based mechanisms lose their incentive
compatibility, they still pose a very special property. Loosely speaking, in such a mechanism,
the only reason for an agent to misreport its valuation is to help the algorithm compute
a better outcome. We would like to exploit this property to obtain mechanisms that are
almost truthful.
21

fiNisan & Ronen

Given any algorithm for the corresponding optimization problem we dene the second chance mechanism based on it. This mechanism is a modication of the VCG-based
mechanism where, in addition to their valuations, the agents are allowed to submit appeal
functions. An appeal function allows the agent to give the algorithm an input (a vector of
declared valuations), which is dierent from the original input, but without misreporting
its type. When the agents behave truthfully, the welfare obtained by the mechanism is at
least as good as the one obtained by the algorithms output.
We then formulate the rationale for truthful behavior in our mechanism. Informally, our
argument is as follows: Under reasonable assumptions, in any situation in which the agent
believes it is benecial for it to lie to the mechanism, it is better for the agent to report its
actual type to the mechanism and ask its appeal to check whether this lie really helps it.
Thus, the agent can construct a truthful strategy premised on the fact that it is not aware
of any situation in which another strategy would be better. We believe that this is a strong
argument for truth-telling.
We construct a version of our mechanism that satises individual rationality as well.
A generalization of our results to ane maximization and to compensation and bonus
mechanisms (Nisan & Ronen, 2001) is straightforward.
Several alternative approaches aimed at handling the diculty of developing truthful
mechanisms were suggested in the past. One approach is the construction of mechanisms
that are computationally hard to manipulate (e.g., Bartholdi et al., 1992). To the best of
our knowledge such manipulations are only hard in the worst case (e.g., it may be NPhard to always compute such a manipulation). Nevertheless, such hardness does not rule
out the possibility that manipulations may be easy to compute in typical cases. Another
possible approach is to consider other equilibria of VCG (Holzman, Kr-Dahav, Monderer, &
Tennenholtz, 2004; Holzman & Monderer, 2004). However, there is no apparent way for the
agents to coordinate such equilibria. Several recent works construct ascending mechanisms
for combinatorial auctions (e.g. Parkes, 1999). Such mechanisms rely on assumptions about
the agents which are very dierent from ours (e.g., myopic behavior). It may be interesting
to compare the virtues of such mechanisms to those of ours.
A few multi-round mechanisms for combinatorial auctions that let the agents improve
the provisional allocation were proposed and tested in the past (Banks, Ledyard, & Porter,
1989). Our argument for truthfulness in second chance mechanisms may provide a partial
explanation for the relative success reported in these experiments.

2. Preliminaries
In this section we formally present our model. We attempt as much as possible to use the
standard notions of both mechanism design and computational complexity theories.
2.1 Mechanism Design Problems
This section formulates the class of mechanism design problems that we study.
Denition 1 (utilitarian mechanism design problem) A (utilitarian) mechanism design problem is described by:
1. A nite set O of allowed outputs.
22

fiComputationally Feasible VCG Mechanisms

2. Each agent i = (1, . . . , n) has a real function v i (o  O) called its valuation or type.
This is a quantication of its benet from each possible output o in terms of some
common currency. v i (.) is privately known to agent i.
3. If the mechanisms output is o and in addition the mechanism hands the agent pi units
of currency, then its utility ui equals3 v i (o) + pi . This utility is what the agent aims
to optimize.
4. The goal of the mechanism is to select an output o  O that maximizes the total

welfare g(v, o) = i v i (o).
An example of such a problem can be found in Section 2.4.
Note that the goal in these problems is to maximize the total welfare but not necessarily
the revenue. This goal, also known as economic eciency, is justied in many settings and
is extensively studied in economics.
In a direct revelation mechanism, the participants are simply asked to reveal their types
to the mechanism. Based on these declarations the mechanism computes the output o and
the payment pi for each of the agents.
Denition 2 (mechanism) A (direct revelation) mechanism is a pair m = (k, p) such
that:
 The output function k accepts as input a vector w = (w1 , . . . , wn ) of declared valuation
functions4 and returns an output k(w)  O.
 The payment function p(w) = (p1 (w), . . . , pn (w)) returns a real vector quantifying the
payment handed by the mechanism to each of the agents (e.g. if pi = 2, the mechanism
pays two units of currency to agent i).
The agents try to maximize their own utility and thus may lie to the mechanism.
As these lies might severely reduce the total welfare, the mechanism should be carefully
designed such that it will be for the benet of the agents to report their types truthfully.
Notation: We denote the tuple (a1 , ...ai1 , ai+1 , ..., an ) by ai . We let (ai , ai ) denote the
tuple (a1 , . . . , an ).
Denition 3 (truthful mechanism) A mechanism is called truthful if truth-telling is a
dominant strategy, i.e., for every agent i of type v i and for every type declaration wi for
the other agents, the agents utility is maximized when it declares its real valuation function
vi.
As an example consider the famous Vickrey auction (Vickrey, 1961): A seller wishes to
sell one item in an auction. There are n buyers, each privately knowing its valuation v i for
this item. (The value for not winning is assumed to be zero.) In a Vickrey auction each
of the buyers is simply asked for its valuation; the item is allocated to the buyer with the
3. This assumption is called quasi-linearity and is very common in mechanism design.
4. We do not consider the issue of how to represent the valuations.

23

fiNisan & Ronen

highest bid for the price of the second highest. The reader may verify that this mechanism
is truthful. Another example of a truthful mechanism can be found in Section 2.4.
In general, the communication protocol of a mechanism can be complicated. A simple
observation known as the revelation principle for dominant strategies (e.g., Mas-Collel et al.,
1995, pp. 871) states that for every mechanism where the agents have dominant strategies,
there exists an equivalent truthful mechanism. Thus, w.l.o.g. it is possible to focus on
truthful mechanisms.
2.2 VCG-based Mechanisms
This subsection presents the celebrated VCG mechanisms. Intuitively, these mechanisms
solve utilitarian problems by identifying the utility of truthful agents with the declared
total welfare. We then generalize these mechanisms.
Denition 4 (VCG mechanism, (via Groves, 1973)) A mechanism m = (k, p) belongs
to the VCG family if:
 k(w) maximizes the total welfare according to w. That is, for all w, k(w)  arg maxo g(w, o).
 The payment is calculated according to the VCG formula: pi (w) =
hi (wi ) (hi (.) is an arbitrary function of wi ).



j=i w

j (k(w))

+

The reader may verify that the Vickrey auction is a VCG mechanism. It is well known
that VCG mechanisms are truthful (Groves, 1973).
Unfortunately, for many applications, the task of nding an output k(w) that maximizes the total welfare is computationally infeasible (e.g., NP-hard). In this paper we
consider VCG mechanisms where the optimal algorithm is replaced by a sub-optimal but
computationally feasible one.
Denition 5 (VCG-based mechanism) Let k(w) be an algorithm that maps type declarations into allowable outputs. We call m = (k(w), p(w)) a VCG mechanism based on

k(.) if p(.) is calculated according to the VCG formula: pi (w) = j=i wj (k(w)) + hi (wi )
(where hi (.) is an arbitrary function of wi ).
Obviously, a VCG-based mechanism that is based on an optimal algorithm is a VCG mechanism. Note that the payment function of a VCG-based mechanism is not identical to the
VCG payment because the algorithm k(.) is plugged into the payment formula. We will now
characterize the utility of an agent in VCG-based mechanisms. This utility is equivalent to
the total welfare according to the declared types of the other agents and the actual type of
the agent under consideration.
Lemma 2.1 (VCG-based utility) Consider a VCG-based mechanism dened by the allocation algorithm k(.), and the functions (h1 (.), . . . , hn (.)). Suppose that the actual valuation
of agent i is v i , and the declarations are w = (w1 (.), . . . , wn (.)). Then the utility of agent i
equals g((v i , wi ), k(w)) + hi (wi ).
24

fiComputationally Feasible VCG Mechanisms

Proof: The proof is immediate from the denitions. The agents utility equals v i (k(w)) +

pi (w) = v i (k(w)) + j=i v j (k(w)) + hi (wi ) = g((v i , wi ), k(w)) + hi (wi ).
In other words, a VCG-based mechanism identies the utility of truthful agents with the
total welfare. In particular, when k(.) is optimal, g((v i , wi ), k(w)) is maximized when the
agent is truthful. This implies that VCG mechanisms are truthful but this truthfulness is
not necessarily preserved by VCG-based mechanisms.
2.2.1 Example: Non Optimal Vickrey Auction
This section demonstrates the problems that might occur when the optimal algorithm in a
VCG mechanism is replaced by a sub-optimal one. Consider the sale of a single item. As
we already commented, a Vickrey auction is a VCG mechanism. Its algorithm allocates the

item to the agent with the highest declared value. The function hi (wi ) =  j=i wj (o)
equals the negation of the second highest value in case i is winning.
Consider the same mechanism where the optimal algorithm is replaced by an algorithm
that only chooses the second highest agent. The mechanism will now give the object to the
agent with the second highest declaration for the price of the third highest agent.
Suppose that there are three agents. Alice who has a value of $2 million, Bob with a
value of $1.7 million, and Carol who has a value of $1 million. When the agents are truthful,
Bob wins and pays $1 million. In this case it is for Alices benet to reduce her declaration
below Bobs. Similarly, if Alice wins, Bob would like to lower his declaration further, and
so on. Note that there are natural situations where Carol will win as well.
It is not dicult to see that there are no dominant strategies in this game. The outcome
of the mechanism is highly unpredictable, depending heavily on the agents beliefs about
the others, their risk attitude, and their level of sophistication. Such a mechanism can
yield inecient outcomes. The eciency loss may get much worse when the underlying
optimization problem has a complex combinatorial structure (simulations that demonstrate
this in the context of scheduling were done by Carroll & Grosu, 2005).
2.2.2 Affine-based mechanisms
It is possible to slightly generalize the class of VCG mechanisms and obtain mechanisms
called ane maximizers. Such mechanisms maximize ane transformations of the valuations. When the domain of valuations is unrestricted, ane maximizers are the sole
truthful mechanisms (Roberts, 1979; Lavi et al., 2003). Similarly to VCG, we generalize
these mechanisms to incorporate sub-optimal algorithms.
Notation: Let a = (a0 , . . . , an ) be an n + 1-tuple such that a0 (.) is a valuation function,
and a1 , . . . , an are strictly positive. We dene the weighted welfare ga (w, o) of an output o

as a0 (o) + i>0 ai  wi (o) where w is a vector of types and o an output.
Denition 6 (ane-based mechanism) Let k(w) be an algorithm that maps type declarations into allowable outputs, a = (a0 , . . . , an ) be an n + 1-tuple such that a0 (.) is a
valuation function, and a1 , . . . , an are strictly positive. We call m = (k(w), p(w)) an ane

mechanism based on k if p is calculated according to the formula: pi (w) = a1i ( j=i,0 aj 
wj (k(w)) + hi (wi )) (where hi () is an arbitrary function of wi ).
25

fiNisan & Ronen

The function a0 (.) can be interpreted as the preferences of the mechanism over the set of
the alternatives and a1 , . . . , an as weights over the agents. As in VCG mechanisms, the
agents utility have a convenient characterization.
Lemma 2.2 (ane-based utility) Consider an ane-based mechanism dened by the
allocation algorithm k(.), a tuple a and the functions h1 (.), . . . , hn (.). Suppose that the
actual valuation of agent i is v i , and the declarations are w = (w1 (.), . . . , wn (.)). Then the
utility of agent i equals a1i (ga ((v i , wi ), k(w)) + hi (wi )).
Proof: The proof is immediate from the denitions. The agents utility equals v i (k(w) +
pi (w)) = a1i (ai v i (k(w)) + pi (w) = a1i (ga ((v i , wi ), k(w)) + hi (wi )).
In other words, an ane-based mechanism identies the agents utility with the ane
transformation of the valuations it aims to optimize. In particular, when k(.) maximizes
ga (w, .), the mechanism is truthful.
2.3 Computational Considerations in Mechanism Design
This section adopts standard notions of computational complexity to revelation mechanisms.
Denition 7 (polynomial mechanism) A mechanism (k, p) is called polynomial time
computable if both k(w) and p(w) run in polynomial time (in the size of w).
Note that a VCG-based mechanism is polynomial i its output algorithm and the functions hi (.) are polynomial. We sometimes call polynomial algorithms and mechanisms
computationally feasible.
Denition 8 (NP-complete problem) A mechanism design problem is called NP-Complete
if the problem of nding an output that maximizes the total welfare is NP-Complete.
We use the term feasible to denote acceptable computational time and infeasible for
unacceptable computational time. In particular, NP-hard problems and exponential algorithms are considers infeasible, while polynomial algorithms are considered feasible. We use
these non-standard terms because most of our results are not limited to specic complexity
classes.
2.4 Example: Combinatorial Auctions
The problem of combinatorial auctions has been extensively studied in recent years (a recent
book can be found at Cramton et al., 2006). The importance of this problem is twofold.
Firstly, several important applications rely on it (e.g., the FCC auction Cramton, 1997).
Secondly, it is a generalization of many other problems of interest, in particular in the eld
of electronic commerce.
The problem: A seller wishes to sell a set S of items (radio spectra licenses, electronic
devices, etc.) to a group of agents who desire them. Each agent i has, for every subset
s  S of the items, a non-negative number v i (s) that represents how much s is worth for
it. v i (.) is privately known to each agent i. We make two standard additional assumptions
on the agents type space:
26

fiComputationally Feasible VCG Mechanisms

No externalities The valuation of each agent i depends only on the items allocated to
it. In other words, for every two allocations x = (x1 , . . . , xn ) and y = (y1 , . . . , yn ),
if xi = yi , then v i (x) = v i (y). Thus, we denote the valuation of each agent i by
v i : 2S  R.
Free disposal Items have non-negative values, i.e., if s  t, then v i (s)  v i (t). Also,
v i () = 0.


Items can either be complementary, i.e., v i (S T )  v i (S) + v i (T ), or substitutes, i.e.,

v i (S T )  v i (S) + v i (T ) (for disjoint S and T ). For example, a buyer may be willing to
pay $200 for a T.V. set, $150 for a VCR, $450 for both and only $200 for two VCRs.
If agent i gets the set si of items, and its payment is pi , its utility is v i (si ) + pi . (The
payments in combinatorial auctions are non-positive.) This utility is what each agent tries
to optimize. For example, an agent prefers to buy a $1000 valued VCR for $600, gaining
$400, rather buy a $1500 valued VCR for $1250.
In a VCG mechanism for a combinatorial auction, the participants are rst required
to reveal their valuation functions to the mechanism. The mechanism then computes,
according to the agents declarations, an allocation s that maximizes the total welfare. The
payment for each of the agents is calculated according to the VCG formula. By Lemma
2.1, the utility ui = v i (si ) + pi of each of the agents is maximized when it reveals its true
valuation to the mechanism. When all agents are truthful, the mechanism maximizes the
total welfare.
Consider, however, the computational task faced by such a mechanism. After the types
are declared, the mechanism needs to select, among all possible allocations, one that maximizes the total welfare. This problem is known to be NP-Complete. Therefore, unless
the number of agents and items is small, such a mechanism is computationally infeasible.
Even the problem of nding
an allocation that approximates the optimal allocation within

a reasonable factor of |S|   is N P -Complete (Zuckerman, 2006). Nevertheless, various
heuristics and tractable sub-cases have been analyzed in the literature (Cramton et al.,
2006, Chapter 13). We would like to nd a way to turn these sub-optimal algorithms into
mechanisms.
We note that, in general, revealing a valuation function requires exponential communication. While we ignore communication issues in this paper, a subsequent work (Ronen,
2001) extends the second chance method to address communication limitations as well.

3. Limitations of Truthful VCG-based Mechanisms
This section studies the limitations of truthful VCG-based mechanisms. Section 3.1 characterizes these mechanisms for the important problem of combinatorial auctions (see Section
2.4). This characterization precludes the possibility of obtaining truthfulness by applying
VCG rules to many of the proposed heuristics for combinatorial auctions (e.g., the greedy
algorithms of Lehmann et al., 2002 and Nisan, 2000). Moreover, we show that any truthful
non-optimal VCG-based mechanism for combinatorial auctions suers from abnormal behavior. Section 3.2 shows that for many natural cost minimization problems, any truthful
VCG-based mechanism is either optimal or produces results that are arbitrarily far from
27

fiNisan & Ronen

optimal. As a result, when such a problem is computationally intractable, any truthful computationally feasible VCG-based mechanism has inputs that cause it to produce degenerate
results. Furthermore, since standard algorithmic techniques do not yield such anomalies, it
might be dicult to develop algorithms that can be plugged into truthful mechanisms. We
generalize these results to ane-based mechanisms as well.
3.1 Truthful VCG-based Mechanisms for Combinatorial Auctions
This section characterizes the class of truthful VCG-based mechanisms for combinatorial
auctions.
Denition 9 (maximal in its range) Let k(w) be an algorithm that maps type decladf



rations into allowable outputs. Let V = ni=1 V i be the space of all possible types and let
V   V be a subspace of V . Let O denote the range of k at V  , i.e. O = {k(w)|w  V  }.
We say that k is maximal in its range at V  if for every type w  V  , k(w) maximizes g
over O. We say that k is maximal in its range if it is maximal in its range at V .
Consider, for example, an algorithm for combinatorial auctions that allocates all the
items (the set S) to the agent with the highest valuation v i (S). Clearly, this polynomial
time algorithm is maximal in its range . The welfare obtained by the allocation of this
algorithm achieves at least a factor of max(1/n, 1/|S|) of the optimal welfare (where n
denotes the number of agents).
Proposition 3.1 A VCG-based mechanism with an output algorithm that is maximal in
its range is truthful.
Proof: Such a mechanism is a VCG mechanism where the set of allowable outputs is the
range of its output algorithm. By Lemma 2.1 such a mechanism is truthful.
We will now show that the above proposition almost characterizes the class of truthful
VCG-based mechanisms for the combinatorial auction problem.
Notation: We let V denote the space of all types v = (v 1 , . . . , v n ) such that for any two
dierent allocations x and y, g(v, x) = g(v, y). (Recall that g(.) denotes the total welfare.)
It is not dicult to see that V contains almost all the types, i.e. V  V has a measure zero
in V .
Theorem 3.2 If a VCG-based mechanism for the combinatorial auction problem is truthful,
then its output algorithm is maximal in its range at V .
Proof: Assume by contradiction that m = (k, p) is truthful but k(.) is not maximal in its
range at V . Since the functions hi (.) do not aect the truthfulness of the mechanism, we

can assume that they are all zero, i.e., we assume that for all i, pi (w) = j=i wj (k(w)).

According to Lemma 2.1, the utility of each agent i equals v i (k(w)) + j=i wj (k(w)) =
g((v i , wi ), k(w)).
Let O denote the range of k(.) at V and let v  V be a type such that k(v) is not
optimal over O. Let y = arg maxoO g(v, o) be the optimal allocation among O. Note that
28

fiComputationally Feasible VCG Mechanisms

from the denition of V , y is unique. Finally, let w  V be a type such that y = k(w). Such
a type exists since y is in the range of the algorithm.
Dene a type vector z by

i

z (s) =

v i (s) if s  y i

if s  y i .

where  stands for a suciently large number. In other words, each agent i strongly desires
the set y i . Apart from this, v i and z i are identical. We assume that z  V . Otherwise we
could add suciently small noise i (s) to z such that all the claims below remain true.
We will show that z forces the algorithm to output y. We will then show that if
the algorithm outputs y when the type is z, it must also output y when the type is v  a
contradiction.
Lemma 3.3 y = k(z).
Proof: Dene a sequence of type vectors by:
w0 = (w1 , . . . , wn )
w1 = (z 1 , w2 , . . . , wn )
w2 = (z 1 , z 2 , w3 , . . . , wn )
..
.
wn = (z 1 , . . . , z n ).
In other words, every agent in turn moves from wi to z i . We assume that wj  V for all
j. It is not dicult to see that z can be modied by adding small noise to it, in a way that
guarantees the above.
Claim 3.4 k(w1 ) = y.
Proof: Assume by contradiction that this is false. From the denition of V we obtain that
g(w1 , k(w1 )) = g(w1 , y).
Consider the case where agent 1s type is z 1 and the types of the others are w2 , . . . , wn .
By declaring w1 , agent 1 can force the algorithm to decide on y. Since the mechanism is
truthful, it must be that g(w1 , k(w1 )) > g(w1 , y).
Since  is large, it must be that k 1 (w1 )  y 1 (i.e., agent 1 gets all the items it gets

when its type is w1 ). Thus, from the denition of z 1 , we obtain  + nj=2 wj (k(w1 )) >

 + nj=2 wj (y). Because, due to the free disposal assumption, w1 (k(w1 ))  w1 (y), we


obtain that w1 (k(w1 )) + nj=2 wj (k(w1 )) > w1 (y) + nj=2 wj (y) (even when z is perturbed).
Thus, g(w0 , k(w1 )) > g(w0 , y).
Therefore, when the type of agent 1 is w1 , it is better o declaring z 1 , forcing the
mechanism to output k(w1 ). This contradicts the truthfulness of the mechanism.
Similarly, by induction on j, we obtain that k(wj ) = y for all j, and in particular for wn = z.
This completes the proof of Lemma 3.3.
We will now show that k(z) = y implies that k(v) = y  a contradiction. Consider the
following sequence of type vectors:
29

fiNisan & Ronen

v0 = (v 1 , . . . , v n )
v1 = (z 1 , v 2 , . . . , v n )
..
.
vn = (z 1 , . . . , z n ).
In other words, every agent in turn, moves from v i to z i . Again we can choose z such that
all vj s are in V .
Claim 3.5 For all vj , y maximizes g on O.
Proof: We will show this for v1 . The proof for j > 1 follows from a similar argument.
Assume by contradiction that x = y maximizes the welfare for v1 . Since  is arbitrarily
large it must be that x1  y i . Consequently, in both cases agent 1s valuation equals .
Recall that y uniquely maximizes g on O for v0 . Thus, for every allocation x = y, we have




v 1 (y) + nj=2 v j (y) > v 1 (x) + nj=2 v j (x). Therefore,  + nj=2 v j (y) >  + nj=2 v j (x). But
the left hand side equals g(v1 , y) and the right hand side equals g(v1 , x). Thus, g(v1 , y) >
g(v1 , x)  contradiction.
Claim 3.6 k(vn1 ) = y.
Proof: We showed that k(vn ) = y. (Recall that vn = z.) We also showed that y uniquely
maximizes g(vn1 , .). Let xn1 = k(vn1 ). Assume by contradiction that xn1 = y. According to Lemma 2.1, the utility of agent n when it is truthful is g(vn1 , xn1 ). Thus,
when agent ns type is v n , it is better o declaring z n obtaining a utility of g(vn1 , y). This
contradicts the truthfulness of the mechanism.
Similarly, by downward induction on j, we obtain that k(v0 ) = y. But v0 = v and we
assumed that k(v) = y  a contradiction. This completes the proof of Theorem 3.2.
Remarks The above theorem characterizes the output algorithms that could be incorporated into truthful VCG-based mechanisms on all but a zero-measured subset of the types.
This characterization holds even when the set of possible types is discrete (under the mild
condition that the type vector z can be dened such that the agents are not indierent
between allocations). The theorem gives rise to several interesting algorithmic and combinatorial questions. For example, given an approximation factor c  1, what is the minimal
size of a sub-family O  O such that for every v, maxyO g(v, y)  c  gopt (v)? A limited
version of this question was analyzed by Holzman et al., 2004 and Holzman & Monderer,
2004.
Corollary 3.7 Consider a VCG-based mechanism for a combinatorial auction with an output algorithm k. If the mechanism is truthful, there exists an output algorithm k, maximal
in its range, such that for every v, g(v, k(v)) = g(v, k(v)).
Proof: Let O denote the range of k on V , and dene another algorithm that is optimal
in its range by v, k(v)  arg maxoO . According to Proposition 3.1, a VCG mechanism
30

fiComputationally Feasible VCG Mechanisms

based on k is truthful. Consider the case where the agents are truthful. Recall that the
utility of all agents is determined by the resulting total welfare. Thus, it is not dicult to
see that the welfares g(v, k(v)) and g(v, k(v)) must be continuous in v. Two continuous real
functions, which are identical on a dense subspace, are identical on the whole space and
thus the corollary follows.
We now show that non-optimal truthful VCG-based mechanisms suer from the following
disturbing abnormal behavior:
Denition 10 (reasonable mechanism) A mechanism for combinatorial auctions is
called reasonable if whenever there exists an item j and an agent i such that:
1. For all S, if j 
/ S, then v i (S  {j}) > v i (S), and,
2. For every agent l = i, S, v i (S  {j}) = v i (S),
then j is allocated to agent i.
Simply put, in situations where only one agent desires an item, that agent gets it.
Theorem 3.8 Any non-optimal truthful VCG-based mechanism for combinatorial auctions
is not reasonable.
Proof: Consider such a mechanism m. According to Corollary 3.7 there exists an equivalent
mechanism m = (k, p), which is optimal in its range. Since m must also be sub-optimal,
there exists at least one partition s = (s1 , . . . , sn ) that is not in the range of the mechanism.
Dene a vector of types by:

1 if x  si
i
v (x) =
0 otherwise.
In other words, each agent i wants a single set si , and no two agents want the same item
(as the sets are disjoined). Since s is not in the range of m it must be that k(v) = s. Since
s is strictly optimal, k(v) must also be suboptimal. Hence, there exists at least one agent
i that does not get si . In particular, there exists at least one item j  si that agent i does
not get. Since i is the only agent that desires j, the theorem follows.
Corollary 3.9 Unless P = N P , any polynomial time truthful VCG-based mechanism for
combinatorial auctions is not reasonable.
We believe that most of the natural allocation algorithms (e.g., linear programming relaxations, algorithms that greedily allocate items to agents, local search algorithms) will
not yield the above anomaly. In particular, we presume that when each agent wants a single
subset of items and these subsets are disjoined, any such algorithm will nd the optimal
allocation. Thus, the above corollary suggests that it might be dicult to develop allocation
algorithms that yield truthful VCG-based mechanisms.
We now show how to generalize our results to any ane-based mechanism. Given a tuple
a = (a0 , . . . , an ), we dene V to be the space of all types v such that for any two dierent
allocations x and y, ga (v, x) = ga (v, y). Similarly to the unweighted case, we say that an
algorithm is optimal in its range with respect to ga (.) if it always produces allocations that
maximize ga (.).
31

fiNisan & Ronen

Theorem 3.10 Consider an ane-based mechanism for the combinatorial auction problem
dened by an allocation algorithm k(.), and a tuple a = (a0 , . . . , an ). If the mechanism is
truthful, then k(.) is maximal in its range with respect to ga (.) at V .
Proof: (sketch) The proof is similar to the proof of Theorem 3.2 and we thus only sketch it.
Dene V and O similarly but w.r.t. the ane transformation ga (.). Assume by contradiction
that there exists a type vector v such that k(v) is not optimal in O. Let y be the optimal
allocation in the range O, and w  V such that k(w) = y. According to Lemma 2.2, the
utility of each agent is maximized with the weighted welfare ga ((v i , wi ), .). Thus, it is
possible to proceed along the lines of the proof of Theorem 3.2: Dene a type vector z
similarly; then, start from w and gradually transform all agents to z and conclude that
k(z) = y; then gradually transform all agents from z to v and show that k(v) = y, i.e.,  a
contradiction.
Open Questions We currently do not know whether theorems similar to Theorem 3.2
hold when the valuations are bounded. Moreover, we do not know how to get rid of the
usage of V . Thus, we do not preclude the possibility that Corollary 3.7 will not hold when
the space of possible types is discrete. We also do not know whether they hold when the
allocation algorithm is randomized or whether Bayesian versions of our theorems apply to
the expected externality mechanism (dAspremont & Gerard-Varet, 1979) (an analog of
VCG in the Bayesian model). We leave this to future research. We conjecture that similar
theorems apply to many other mechanism design problems.
3.2 Truthful VCG-based Mechanisms for Cost Minimization Problems
We now show that for many natural cost minimization problems, any truthful VCG-based
mechanism is either optimal or produces results that are arbitrarily far from the optimal.
We start with a sample problem.
Multicast transmissions: A communication network is modeled by a directed graph
G = (V, E). Each edge e is a privately owned link. The cost te of sending a message along
that edge is privately known to its owner. Given a source s  V and a set T  V of
terminals, the mechanism must select a subtree rooted in s that covers all the terminals.
The message is then broadcasted along this tree. We assume that no agent owns a cut in
the network.
Naturally, the goal of the mechanism is to select, among all possible trees, a tree R that

minimizes the total cost:
eR te . The goal of each agent is to maximize its own prot:

pi  (eR owned by i) te . It is not dicult to see that this is a utilitarian mechanism design
problem.
This example was introduced by Feigenbaum et al., 2000 (using a dierent model). It is
motivated by the need to broadcast long messages (e.g., movies) over the Internet. We now
generalize this example.
Denition 11 (cost minimization allocation problem)
A cost minimization allocation problem (CMAP) is a mechanism design problem described by:
32

fiComputationally Feasible VCG Mechanisms

i ). We let m =
Type space The type of each agent i is described by a vector (v1i , . . . , vm
i

i
i mi . (In our multicast example ve corresponds to the negation of the cost te .)

Allowable outputs Each output is denoted by a bit vector x = (x11 , . . . , x1m1 , . . . , xn1 , . . . , xnmn ) 
{0, 1}m . We denote (xi1 , . . . , ximi ) by xi . There may be additional constraints on the
set O of allowable outputs. (In our example x corresponds to a tree in the networks
graph where xij equals 1 i the corresponding edge is in the chosen tree.)
such that the following conditions are satised:
i ) describes a type for agent i and w i  v i (as
Unbounded costs If v i = (v1i , . . . , vm
i
i
vectors), then w also describes a type.

Independence and monotonicity Each valuation v i depends only on is bits xi . (In our
example, the agent valuation of a given tree depends only on its own edges in it.) If
for all j, wji  vji , then for every output x, wi (xi )  v i (xi ).
Forcing condition For every type v, an allowable output x and a real number ,
dene a type v[] by


v[]ij

=

vji


if xij = 1
otherwise.

The forcing condition is satised if for every allowable output y = x, lim g(t(), y) =
.
Many natural decentralized task allocation problems in which the goal is to minimize
the total cost under given constraints belong to this class. In particular the reader may
verify that our multicast example falls into this category. Another example is the shortest
path problem studied extensively in recent years (e.g., Rosenschein & Zlotkin, 1994; Archer
& Tardos, 2002; Elkind et al., 2004).
Notation: For a type v we let gopt (v) denote the optimal value of g. We denote g(v, k(v))
by gk (v).
Denition 12 (degenerate algorithm) An output algorithm k is called degenerate if the
g (v)gopt (v)
is unbounded, i.e., there exist vs such that rk (v) is arbitrarily
ratio rk (v) = k|gopt (v)|+1
large.
A degenerate algorithm is arbitrarily far from optimal, both additively and multiplicatively. Note that this should not be confused with the standard notion of an approximation
ratio, as our denition corresponds to a single problem. In particular, the number of agents
is xed. We note that we do not rule out the possibility that such an algorithm will be
good by some non worst case metric.
Theorem 3.11 If a VCG-based mechanism for a CMAP is truthful, then its output algorithm is either optimal or degenerate.
33

fiNisan & Ronen

Before stating the proof let us illustrate it using the multicast transmission example.
Suppose that we start with a type vector that leads to a sub-optimal solution. If we raise
the cost of an edge, the utility of the owner cannot increase (due to the truthfulness and
Lemma 2.1). We then gradually raise the cost of all edges except the ones in the optimal
tree. Still, the algorithm will have to choose a sub-optimal tree. However, the cost of any
suboptimal tree is now arbitrarily high while the optimal cost remains the same.
Proof: Let m = (k, p) be a non-optimal truthful VCG-based mechanism for a CMAP. As

in Theorem 3.2, assume that pi (w) = j=i wj (k(w)). Let v be a type vector such that k(v)
is not optimal and let y = opt(v) be an optimal output.
We dene a type z by:


zji =

vji
if yji = 1
 otherwise.

where  is arbitrarily large.
Consider the type sequence:
v0 = (v 1 , . . . , v n )
v1 = (z 1 , v 2 , . . . , v n )
..
.
vn = (z 1 , . . . , z n ).
Claim 3.12 For all j, y = opt(vj ).
Proof: By denition y is optimal for v0 . Let x = y be an allocation. From the independence
condition, for all j, g(vj , y) = g(v0 , y). From the monotonicity, g(vj , x)  g(v0 , x). Together,
g(vj , x)  g(v0 , x)  g(v0 , y) = g(vj , y).
Claim 3.13 g(v1 , k(v1 )) < g(v1 , y)
Proof: Assume by contradiction that the claim is false. Since y is optimal for v1 , this
means that g(v1 , k(v1 )) = g(v1 , y). From independence, g(v1 , y) = g(v0 , y). Recall that
k(v0 ) is suboptimal so g(v0 , y) > g(v0 , k(v0 )). From monotonicity (we only worsen the
type of agent 1), g(v0 , k(v1 ))  g(v1 , k(v1 )). Thus, together g(v0 , k(v1 ))  g(v1 , k(v1 )) =
g(v1 , y) = g(v0 , y) > g(v0 , k(v0 )). In particular, g(v0 , k(v1 )) > g(v0 , k(v0 )).
Consider the case where agent 1s type is v 1 and the declarations of the other agents are
(v 2 , . . . , v n ). According to Lemma 2.1, its utility when it is truthful, equals g(v0 , k(v0 )). On
the other hand, when it falsely declares z 1 , its utility equals g(v0 , k(v1 )). Since we showed
that g(v1 , k(v1 )) > g(v0 , k(v0 )), this contradicts the truthfulness of the mechanism.
Similarly, we obtain that g(vn , k(vn )) < g(vn , y) = g(v0 , y). By the forcing condition,
g(vn , k(vn ))   when   . Thus, the algorithm is degenerate.
Corollary 3.14 Unless P = N P , any polynomial time truthful VCG-based mechanism for
an NP-hard CAMP is degenerate.
34

fiComputationally Feasible VCG Mechanisms

Note that due to the revelation principle, the theorems in this section hold for any mechanism where the agents have dominant strategies. Similarly to Theorem 3.11, any mechanism
that uses VCG payments and has a non-optimal ex-post Nash equilibrium also has equilibria
which are arbitrarily far from optimal.
We now show how to generalize the theorems in this section to ane-based mechanisms.
Theorem 3.15 If an ane-based mechanism (k, p) for a CMAP is truthful, then its output
algorithm is either optimal or degenerate.
Proof:(sketch) The proof is almost identical to the proof of Theorem 3.11. Let v be a
type such that k(w) is not optimal w.r.t. to the corresponding ane transformation ga . We
dene a type vector z similarly to Theorem 3.11 and consider a sequence of type vectors
where each agent in turn changes its type from wi to z i . Due to the incentive compatibility
and Lemma 2.2 , the utility of each agent cannot increase, meaning that the weighted welfare
ga remains sub-optimal. Due to the forcing condition, all outputs except the optimal, have
arbitrarily high cost. This means that the algorithm is degenerate.
The compensation and bonus mechanism (Nisan & Ronen, 2001) identies the utility of
agents with the total welfare similarly to VCG, i.e., the utility of an agent can be described similarly to Lemma 2.1. Thus, all the theorems in this section can be applied to
compensation and bonus mechanisms as well.

4. Second Chance Mechanisms
To date, ane maximization is the only known general method for the development of
truthful mechanisms. Therefore, the results in the previous section do not leave much hope
for the development of truthful mechanisms for many complex problems.
This section proposes a method for circumventing this problem. Consider a VCG-based
mechanism. An immediate consequence of Lemma 2.1 is that the only reason for an agent
to misreport its type is to help the algorithm to improve the overall result. This leads to
the intuition that if the agents cannot improve upon the underlying algorithm, they can
do no better than be truthful. We would like to exploit this special property of VCG-based
mechanisms and construct mechanisms that are almost truthful.
Given any algorithm for the corresponding optimization problem we dene the second chance mechanism based on it. This mechanism is a modication of the VCG-based
mechanism where in addition to their valuations, the agents are allowed to submit appeal
functions. An appeal function allows the agent to give the algorithm an input (vector of
declared valuations) that is dierent from the original input but without misreporting its
type. When the agents behave truthfully, the welfare obtained by the mechanism is at least
as good as the one obtained by the algorithms output.
We then formulate the rationale for truthfulness in second chance mechanisms. Informally, our argument is as follows: Under reasonable assumptions, in any situation in which
the agent believes it is benecial for it to lie to the mechanism, it is better for it to report its
actual type to the mechanism and ask its appeal to check whether this lie is indeed helpful.
Thus, the agent can construct a truthful strategy premised on the fact that it is not aware
35

fiNisan & Ronen

of any situation in which another strategy is better for it. We believe that this is a strong
argument for truth-telling.
A generalization of our results to ane maximization and to compensation and bonus
mechanisms is straightforward.
4.1 The Mechanism
In this section we formulate the second chance mechanism and its basic properties.
Denition 13 (appeal function) Let V =
appeal is a partial function5 l : V  V .



i Vi

denote the type space of the agents. An

The semantics of an appeal l(.) is: when the agents type vector is v = (v1 , . . . , vn ), I
believe that the output algorithm k(.) produces a better result (w.r.t. v) if it is given the
input l(v) instead of the actual input v. An appeal function gives the agent an opportunity
to improve the algorithms output. If v is not in the domain of l(.), the semantics is that
the agent does not know how to cause the algorithm to compute a better result than k(v).
The second chance mechanism is dened in Figure 4.1. It is a modication of VCG that
allows the agents to submit appeal functions as well.

Before the execution The manager of the mechanism publishes the outcome determination algorithm and a time limit T on the computation time of each appeal.
Declaration Each agent submits a type declaration wi and an appeal function li (.) to the
mechanism. The appeals must adhere to the specied time limit.
Allocation Let w = (w1 , . . . , wn ). The mechanism computes k(w), k(l1 (w)), . . . , k(ln (w))
and chooses among these outputs the one that maximizes the total welfare (according
to w).
Payment Let o denote the chosen output. The mechanism calculates the payments ac
cording to the VCG formula: pi = j=i wj (o) + hi (wi , li ) (where hi (.) is any real
function).
Figure 1: The Second Chance Mechanism
Remarks The agents send programs that represent their appeal functions to the mechanism. These programs are then executed by the mechanism. The mechanism can terminate
the computation of each appeal after T units of computation time (and refer to the vector
of declarations w as if it is not in the appeals domain). Thus, we can assume w.l.o.g. that
all appeals adhere to the given time limit. A discussion on the choice of the time limit and
alternative representations of the appeal functions appears in Section 4.3. We believe that
it is possible to construct software tools and APIs that will make the formulation of the
appeals an easy task.
5. A function f : D  R is called partial if its domain is a subset of D, i.e. if Dom(f )  D.

36

fiComputationally Feasible VCG Mechanisms

The functions hi (.) do not play any role in the agents considerations as every hi (.) is
independent of is actions. Until Section 4.4 it is possible to simply assume that hi (.)  0
for all i. In Section 4.4 we will use these functions in order to satisfy individual rationality.
Denition 14 (truthful action) An action in the second chance mechanism is a pair
(wi , li ) where wi is a type declaration and li (.) is an appeal function. An action is called
truthful if wi = v i .
The following observation is a key property of the mechanism.
Proposition 4.1 Consider a second chance mechanism with an output algorithm k. For
every type vector v = (v 1 , . . . , v n ), if all agents are truth-telling, g(v, o)  g(w, k(v)).

In other words, when the agents are truth-telling, the result of the mechanism is at least
as good as k(v). The proof is immediate from the denition of the mechanism. We now
formulate an analog of Lemma 2.1. The proof is similar to the lemmas proof and is
henceforth omitted.
Lemma 4.2 (second chance utility) Consider a second chance mechanism. Let o be the
chosen output. The utility of agent i equals g((v i , wi ), o) + hi (wi , li ).

Therefore, informally, it is benecial for an agent to declare wi = v i only if it either helps
the output algorithm k(.) to compute a better result (w.r.t. (v i , wi )) or it helps one of the
appeals of the other agents.
Note that lying to a second chance mechanism may harm an agent in two ways. First,
it can damage the output algorithm k(.). Second, it can cause the mechanism to measure
the welfare according to a wrong type vector and thus cause it to choose an inferior output.
Notation: We say that a second chance mechanism is T-limited if the time limit it species
is T . Similarly, an algorithm is called T-limited if its computational time never exceeds T
units of computation.
The following proposition is obvious.
Proposition 4.3 Consider a T -limited second chance mechanism. If the output algorithm
of the mechanism is also T -limited, the overall computational time of the mechanism is
O(nT ).

4.1.1 A toy example
Consider a combinatorial auction of two items. A type of an agent is a 3-tuple representing
its value for every non empty subset of items. Suppose that agent i values the pair of
items at $3 million but values every single item at $1 million. Its type is, therefore, v i =
37

fiNisan & Ronen

{3, 1, 1}. Suppose that the agent notices that the allocation algorithm often produces better
allocations if it declares wi = {3, 0, 0} (i.e., it hides its willingness to accept only one item).
In a VCG-based mechanism the agent may prefer to declare wi instead of its actual type.
This might cause two problems:
1. Even when the others are truthful, there may be many type vectors v i belonging to the other agents, for which declaring wi damages the chosen allocation, i.e.,
g((v i , wi ), k((wi , wi ))) < g((v i , wi ), k((v i , wi ))).
2. Even when this is not the case and every agent i chooses a declaration wi such that
g((v i , wi ), k((wi , v i )))  g((v i , wi ), k(w)), it may be that according to the actual
type vector v the output k(w) may be inferior to k(v) (i.e., g(v, k(w)) < g(v, k(v))).
The second chance mechanism enables the agent to check whether declaring a falsied type
would yield a better result. Instead of declaring wi = {3, 0, 0}, the agent can declare its
actual type and dene its appeal as li (w ) = (wi , wi ). In this way the agent enjoys both
worlds. In cases where the falsied type is better, the mechanism will prefer k((wi , wi ))
over k((v i , wi )). In cases where the truthful declaration is better, the mechanism will prefer
k((v i , wi )). Note that the mechanism allows an appeal to modify not only the declaration
of the agent that submitted it but also the whole vector of declarations. This will allow us
to provide a strong argument for truth-telling.
Possible Variants of the Second Chance Mechanism One alternative denition
of the mechanism is to let the agents submit outcome determination algorithms instead
of appeals. It is possible to apply reasoning similar to ours to this variant. However,
formulating such output algorithms might be a demanding task for many applications.
There are also other more delicate dierences.
Another possibility is to dene a multi-round variant of the mechanism. In the rst
round the agents submit type declarations w. Then, at each round, each agent gets the
chance to improve the allocation found by the algorithm k(w). The mechanism terminates
when no agent improves the current allocation. The strategy space of multi-round mechanisms is very complex. Yet, under myopic behavior (Parkes, 1999), arguments similar to
ours can be used to justify truthful behavior. Such arguments may explain the relative
success of ad hoc mechanisms such as iterative VCG (IVG) and AUSM6 reported by Banks
et al., 1989.
Standard Equilibria in Second Chance Mechanisms The second chance mechanism
uses VCG payments and, therefore, the theorems of the rst part of the paper apply to it.
From Lemma 4.2, a vector of truthful actions is an ex post equilibrium if and only if the
resulting allocation o is optimal in the range of the algorithm. Moreover, consider agent
i and let (wi , li ) be a set of actions of the other agents. (wi , li ) is a best response for
the agent if and only if the resulting allocation o is optimal in the range of the underlying
algorithm with respect to (v i , wi ). At least intuitively, nding such a response is at least as
6. Both mechanisms are in the spirit of our second chance mechanism, as they let the agents improve the
allocation. The actual rules of these mechanisms are complicated and are described by Banks et al.,
1989.

38

fiComputationally Feasible VCG Mechanisms

hard as nding an allocation that is optimal in the range of the algorithm. Thus, one should
not expect the agents to follow equilibrium strategies in the traditional sense. We argue
that similar arguments can be made for every game in which computing the best response
is computationally dicult. Hence, an argument that takes into account the agents own
limitations is required. We note that we did not succeed in nding natural complexity
limitations under which truth-telling is an equilibria for the agents. We leave this as an
intriguing open problem.
4.2 The Rationale for Truth-telling
As we noted, standard equilibria should not be expected in second chance mechanisms. This
section formulates the rationale for truth-telling under these mechanisms. We rst introduce
the notion of feasibly dominant actions7 which takes into account the fact that the agents
capabilities are limited. We then demonstrate that under reasonable assumptions about
the agents, truthful, polynomial time, feasibly dominant actions exist.
4.2.1 Feasible Truthfulness
The basic models of equilibria in game theory are justied by the implicit assumption
that the agents are capable of computing their best response functions. In many games,
however, the action space is huge and this function is too complex to be computed, even
approximately within a reasonable amount of time. In such situations the above assumption
seems no longer valid.
In this section we re-formulate the concept of dominant actions under the assumption
that agents have a limited capability of computing their best response. Our concept is meant
to be used in the context of one stage games, i.e. games in which the agents choose their
actions without knowing anything about the others choices. The second chance mechanism
is a one stage-game. In a nutshell, an action is feasibly dominant if the agent is not aware
of any situation (a vector of the other agents actions) where another action is better for it.
Notation: We denote the action space of each agent i by Ai . Given a tuple a = (a1 , . . . , an )
of actions chosen by the agents, we denote the utility of agent i by ui (a).
Denition 15 (revision function) A revision function of agent i is any partial function
of the form bi : Ai  Ai .
The semantics of bi (ai ) is If I knew that the actions of the others are ai , I would choose
bi (ai ) (instead of ai ). A revision function captures all the cases where the agent knows
how it would like to act if it knew the others actions. Note that optimal revision functions
are standard best-response functions. When a vector of actions ai does not belong to the
domain of bi (.), the semantics is that the agent prefers to stick to its action.
Denition 16 (feasible non-regret) Let i be an agent, bi (.) its revision function, and
ai a vector of actions for the other agents. An action ai satises the feasible non-regret
7. We make a standard distinction between an action and a strategy  a mapping from the agents type to
its action.

39

fiNisan & Ronen

condition (w.r.t. ai and bi ), if either ai is not in the domain of bi or ui ((bi (ai ), ai )) 
ui (a).
In other words, other actions may be better against ai , but the agent is unaware of them
or cannot compute them when choosing its action.
When the revision function of the agent is optimal, a feasible non-regret is equivalent
to the standard non-regret (best response) condition.
Denition 17 (feasibly dominant action) Let i be an agent, bi (.) its revision function.
An action ai is called feasibly dominant (w.r.t. bi (.)) if for every vector ai of the actions
of the other agents, ai satises the feasible non-regret condition (w.r.t. ai and bi ).
Put dierently, an action ai is feasibly dominant if (when choosing its action) the agent is
not aware of any action ai and any vector ai of the actions of the other agents, such that
it is better o choosing ai when the others choose ai . A dominant action is always feasibly
dominant. When the revision function is optimal, a feasibly dominant action is dominant.
Example In order to demonstrate the concept of feasibly dominant actions consider a
chess match in which Alice and Bob submit computer programs that play on their behalf.
Currently, of course it is not known how to compute an equilibrium in chess and therefore
standard equilibria are not relevant for the analysis of such a game. A program aA is feasibly
dominant for Alice if she is not aware of any possible program of Bob against which she is
better o submitting another program.
Denition 18 (feasibly truthful action) An action ai in the second chance mechanism
is called feasibly truthful if it is both, truthful and feasibly dominant.
4.2.2 Natural revision functions that give rise to feasibly truthful actions
Beforehand we showed that when the agents are truthful, the total welfare is at least
g(v, k(v)). We also argued that if a feasibly truthful action is available, the agent has a strong
incentive to choose it. This subsection demonstrates that under reasonable assumptions
about the agents, polynomial time feasibly truthful actions do exist.
Notation: We let  denote the empty appeal. By (w, ) we denote an action vector where
the declaration of each agent i is wi and all the appeals are empty.
Denition 19 (appeal-independent revision function) A revision function bi (.) is
called appeal independent if every vector in its domain includes only empty appeals, i.e. for
all ai  dom(bi ), there exists a vector wi such that ai = (wi , ).
We say that an appeal independent function is T -limited if its own computational time
is bounded by T and so is every appeal function in its range.
The class of appeal-independent revision functions represents agents that only explore
the output algorithm (or alternatively, base their choice of action solely on the output
algorithm). This approach seems reasonable as the space of appeals of the other agents is
40

fiComputationally Feasible VCG Mechanisms

huge, with no apparent structure. At least intuitively, it seems unreasonable that an agent
will be able to lie in a way that will improve the result of the appeals of the other agents
with signicant probability. Moreover, as we commented, an agent has an obvious potential
loss from misreporting its type.
Theorem 4.4 Consider a second chance mechanism with a T -limited output algorithm.
Suppose that an agent has a T -limited appeal-independent revision function. For every
T  = (T ), if the mechanism is T  -limited, the agent has a feasibly truthful action.
Proof: Let bi (.) be the agents revision function. Dene an appeal li (.) as follows. For
every vector wi , let (wi ,  i ) = bi ((wi , )). Let w = (wi , wi ). Consider the outputs
o1 = k(w) and o2 = k( i (w)). We dene li (w) to be the better of the two outputs, i.e.,
li (w) = arg maxj=1,2 g((v i , wi ), oj ). Intuitively, li (.) checks whether declaring wi is helpful
for the agent.
Claim 4.5 ai = (v i , li ) is feasibly truthful.
Proof: If it is not, there exists a vector ai = (wi , ) in the domain of bi (.) such that
u(ai , ai ) < u(bi (ai ), ai ). Let bi (ai ) = (wi ,  i ). Recall that according to Lemma 4.2,
the agents utility is equivalent to the total welfare g((v i , wi ), o) of the chosen output o
(up to adding hi (.), which is independent of the agents actions).
Consider the case when the agents action is bi (ai ). Let o denote the chosen output
in this case. According to the denition of the mechanism, o is taken from the set {o1 , o2 }
and the welfare is measured according to the declaration w.
When the agent chooses the truthful action ai , the output (denoted o) is chosen from
the outputs o0 = k((v i , wi )) (from the denition of the mechanism), and both, o1 , o2 (from
the denition of li ). This is a superset of the set outputs of the rst case. Moreover,
the output is chosen according to the right type vector (v i , wi ). Thus, g((v i , wi ), o) 
g((v i , wi ), o), implying that the agent has a higher utility in the second case  a contradiction.
It remains to show that li (.) is (T )-limited. This is obvious as both, k(.) and  i (.) are
T -limited. This completes the proof of the theorem.
Given the agents revision function, it is easy to construct the appeal li (.) dened above (i.e.,
construct the program that computes it). Thus, if the agent has such an appeal independent
function, it can guarantee itself a feasibly dominant action.
A more general class of revision functions can be found in the Appendix. Interestingly,
there is a tradeo between the generality of the class and the time limit, which suces for
feasible truthfulness.
4.3 Remarks on the Choice of the Time Limit
Sections 4.2.2 and A.1 demonstrate two natural classes of revision functions under which
the agents have polynomial time feasibly truthful actions. We do not claim that every
revision function in practice will fall into these categories. Yet, it is plausible that this will
be the case in many applications. In general, there exists a tradeo between the generality
41

fiNisan & Ronen

of the class of the revision functions and the time limit required for feasible truthfulness.
In particular, without any time limit, submitting an optimal appeal is dominant. On the
other hand, it is plausible that small time limits will suce in practice. We leave a more
comprehensive study of this tradeo to future research.
An interesting future direction is to develop representations of the appeal functions that
relate the time limit imposed on each agent to its actual revision function. One possibility
is to represent the appeals by decision trees where the agents are required to supply for each
leaf , a type vector v , such that the algorithms result is strictly improved when it is given
l(t ) instead of the actual input v . v proves to the mechanism that the computational time
required to compute the leaf  is indeed needed in order to represent the agents revision
function. A related possibility is to allow the agent to purchase additional computational
time.
Currently, we do not know whether every polynomial class of revision functions guarantees the existence of polynomial feasibly truthful actions. If an agent has substantial
knowledge of the appeal space of the other agents, it may be able to nd a falsied declaration that causes typical appeals to produce better results. In such a case, it may be
benecial for an agent to lie. We do not know whether such knowledge will exist in practice.
If yes, it may be possible to overcome this by allowing the agents to submit meta-appeals,
i.e., functions that let the agents modify the input of the appeals of the other agents. We
leave this to future research.
4.4 Obtaining Individual Rationality
A basic desirable property of mechanisms is that the utility of a truthful agent is guaranteed
to be non-negative (individual rationality). In this section we construct a variant of second
chance mechanisms that satises this property.
Let gopt (v) denote the optimal welfare obtained when the type vector is v . We shall
assume that for each agent i, there exists a type v i such that for every v = (v 1 , . . . , v n ),
gopt ((v i , v i ))  gopt (v). We call such a type the lowest. In a combinatorial auction for
example, the lowest type is dened by the zero valuation v i (s) = 0 for every combination s
of items.
The Clarke mechanism (Clarke, 1971) is a VCG mechanism in which hi (wi ) = gopt (v i , wi ),

i.e., pi (w) = j=i wj (opt(w))  gopt (v i , (wi )). In other words, each agent pays the welfare
loss it causes to the society. Thus, it is natural to dene the payment of a VCG-based

mechanism as j=i wj (opt(w))  g((v i , wi ), k((v i , wi ))).
Like truthfulness, individual rationality may not be preserved when the optimal algorithm in the Clarke mechanism is replaced by a sub-optimal one. In order to x this we
need to ensure that the result of the algorithm will not improve when the declaration wi is
replaced by the lowest type v i .

Denition 20 (lowest type closure) Given an allocation algorithm k(w) we dene its
lowest type closure k as the best allocation (according to w) among the outputs (k(w), k((v 1 , w1 )), . . . , k((v n , w
Since k(.) calls k(.) n times, if k is T -limited, then k is O(nT )-limited.
Claim 4.6 For every w, g(w, k(w))  g((v i , wi ), k((v i , wi ))).
42

fiComputationally Feasible VCG Mechanisms

Proof: Since k((v i , wi )) is a candidate output that k tests, g(w, k(w))  g(w, k((v i , wi ))).
Given the denition of v i , g(w, k((v i , wi )))  g((v i , wi ), k(v i , wi )), the claim follows.
Denition 21 (second chance-IR) Given an allocation algorithm k(w) and a time limit
T we dene the corresponding second chance-IR mechanism as the second chance mechanism
with output algorithm k(.), time limit T , and for every agent i, hi (wi ) = g((v i , wi ), k((v i , wi ))).
The utility of a truthful agent in the above mechanism equals ui = g(w, o)g((v i , wi ), k((v i , wi ))) 
g(w, k(w))  g((v i , wi ), k((v i , wi )))  0. Therefore, the mechanism satises individual rationality.

5. Conclusion and Future Research
This paper studies VCG mechanisms in which the optimal outcome determination algorithm
is replaced by some sub-optimal but computationally tractable algorithm. The rst part of
the paper shows that for a wide range of problems, such mechanisms lose the game theoretic
virtues of their optimal counterparts. Similar results hold for ane maximization. These
results do not leave much hope for the development of polynomial time truthful mechanisms
for many problems of high complexity.
The second part of the paper proposes a general method for overcoming the diculty
of constructing truthful mechanisms. Given any algorithm for the underlying optimization
problem we dene the second chance mechanism based on it. We demonstrate that under
reasonable assumptions about the agents, truth-telling is still the rational strategy for the
agents. When the agents are truthful, the welfare obtained by the mechanism is at least as
good as the one obtained by the underlying algorithm.
Successful implementation of second chance mechanisms relies on several tools to be
developed  in particular, tools for the description of valuations and appeal functions.
These engineering issues require further exploration.
It is important to stress that the second chance method has not yet been tested. In
particular, the truthfulness of the agents should be validated experimentally. On the other
hand, we believe that in practice, small time limits on the agents appeals are likely to
guarantee the truthfulness of the agents. Several questions regarding the payment properties
of second chance mechanisms are open. We leave them for future research.
Several open questions, which directly stem from this work, are raised within the body
of the paper.

Acknowledgments
We thank Abraham Newman and Motty Perry for helpful discussions at various stages of this
work. We thank Ron Lavi, Ahuva Mualem, Elan Pavlov, Inbal Ronen, and the anonymous
reviewers for comments on earlier drafts of this paper. Noam Nisan was supported by
grants from the Israel Science Foundation and from the USA-Israel Binational Science
Foundation. Amir Ronen was supported in part by grant number 969/06 from the Israel
Science Foundation. A preliminary version of this paper appeared in the proceedings of the
3rd ACM Conference on Electronic Commerce (EC 01).
43

fiNisan & Ronen

Appendix A. d-bounded Revision Functions
The class of d-bounded revision functions represents agents that, in addition to the output
algorithm, explore a polynomial family of potential appeals of the other agents. This class
is a generalization of d-limited appeal-independent functions.
Denition 22 (d-bounded revision function) We say that a revision function bi (.) is
d-bounded if the following hold:
1. The revision function bi (.) is O(nd )-limited.
2. Let
L = {lj | li,j , wi s.t. (wi , (li , li,j ))  Dom(bi )}



{li | (wi , li ), wi s.t. (wi , li ) = bi ((wi , li ))}

be the family of all appeals that appear in either the domain or range of bi (.). Then
|L| = O(nd ).
3. There exists a constant c such that every appeal l  L is cnd -limited.
Theorem A.1 Consider a second chance mechanism with an O(nd )-limited output algorithm. Suppose that an agent has a d-bounded revision function. For every T  = (n2d ), if
the mechanism is T  -limited, the agent has a feasibly truthful action.
Proof: Let i be the agent and let bi be its revision function. We again use a simulation
argument in order to dene the appeal li (.). For every vector wi we compute the following
outputs:
1. o0 = k(w).
2. Similarly to the the proof of Theorem 4.4, let L = {1 . . . |L| } be the family of all
appeal functions that are in the domain or the range of bi . For all j = 1, . . . |L| dene
oj = k(j (w))).
3. Dene l(w) = arg max0j|L| g((v i , wi ), oj ) as the output with the maximum welfare
according to (v i , wi ) among all the outputs dened above.
Claim A.2 li (.) is n2d -limited.
Proof: W.l.o.g. the running time of k(.) is bounded by cnd . Otherwise, we will raise the
constant. According to the denitions, the appeal li performs nd + 1 computations, each
requiring at most cnd time units. Thus, the overall computation takes at most O(n2d ).
Claim A.3 ai = (v i , li ) is feasibly truthful.
Proof: Assume by contradiction that there exists an action vector ai in dom(bi ) such that
u((ai , ai ) < u((bi (ai ), ai ).
Consider the case when the agent chooses bi (ai ) = (wi ,  i ). The mechanism takes the
output o that maximizes the welfare (according to w) from the following set S of outputs:
44

fiComputationally Feasible VCG Mechanisms

1. o0 = k(w).
2. oj = k(lj (w)) for every j = i, i.e. the result of the appeals of the other agents.
3. oi = k( i (w)).
When the agent chooses ai , the outputs are measured according to the right type
vector (vi , wi ). Moreover, it is taken from the following superset of the outputs in S:
1. o0 = k((vi , wi )) (from the denition of the mechanism).
2. oj = k(lj ((vi , wi ))) for every j = i, i.e., the result of the appeals of the other agents
(also, from the denition of the mechanism).
3. oj = k( (w)) for every   L. Since ai is in the domain of bi , this set includes all the
outputs of the form k(lj (w)) from the case where i chooses bi (ai ). It also contains
the result of its own appeal  i (w).
4. k(w) (from the denition of li (.)).
Let o be the chosen output in this case. Since the set of outputs in the second case is a
superset of the rst, g((vi , wi ), o)  g((vi , wi ), o). According to Lemma 4.2 the utility of
the agent when choosing ai is thus higher than when choosing bi (ai )  a contradiction.
This completes the proof of Theorem A.1.
As in the case of appeal-independent functions, the theorem gives a prescription for constructing an appeal that guarantees the agent a feasibly dominant action.

References
Anderson, E., Kelly, F., & Steinberg, R. (2002). A contract and balancing mechanism for
sharing capacity in a communication network.. To appear.
Archer, A., & Tardos, E. (2002). Frugal path mechanisms. In Proceedings of the 13th
Annual ACM-SIAM Symposium on Discrete Algorithms, 991999.
Banks, J., Ledyard, J., & Porter, D. (1989). Allocating uncertain and unresponsive resources: An experimental approach. RAND Journal of Economics, 20, 125.
Bartal, Y., Gonen, R., & Nisan, N. (2003). Incentive compatible multi unit combinatorial
auctions. In Proceedings of Ninth Conference of Theoretical Aspects of Rationality
and Knowledge, pp. 7287.
Bartholdi, J. J., Tovey, C. A., & Trick., M. A. (1992). How hard is it to control an election?.
Mathematical and Computer Modelling (Special Issue on Formal Theories of Politics),
16, 2740.
Carroll, T. E., & Grosu, D. (2005). Distributed algorithmic mechanism design for scheduling
on unrelated machines. In Proceedings of the 8th International Symposium on Parallel
Architectures, Algorithms, and Networks, pp. 194199.
45

fiNisan & Ronen

Clarke, E. H. (1971). Multipart pricing of public goods. Public Choice, 1733.
Cramton, P. (1997). The fcc spectrum auction: an early assessment. Journal of Economics
and Management Strategy, 431495.
Cramton, P., Shoham, Y., & Steinberg, R. (2006). Combinatorial Auctions. MIT Press.
dAspremont, C., & Gerard-Varet, L. (1979). Incentives and incomplete information. Journal of Public Economics, 11 (1), 2545.
Elkind, E., Sahai, A., & Steiglitz, K. (2004). Frugality in path auctions. In Proceedings of
the 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 701709.
Feigenbaum, J., Papadimitriou, C., & Shenker, S. (2000). Sharing the cost of multicast
transmissions. In Proceeding of the Thirty-Second Annual ACM Symposium on Theory
of Computing.
Groves, T. (1973). Incentives in teams. Econometrica, 41, 617631.
Holzman, R., Kr-Dahav, N., Monderer, D., & Tennenholtz, M. (2004). Bundling equilibrium in combinatorial auctions. Games and Economic Behavior, 47, 104123.
Holzman, R., & Monderer, D. (2004). Characterization of ex post equilibrium in the vcg
combinatorial auctions. Games and Economic Behavior, 47, 87103.
Lavi, R., Nisan, N., & Mualem, A. (2003). Towards a characterization of truthful combinatorial auctions. In Proceedings of the 44th Annual IEEE Symposium on Foundations
of Computer Science.
Lehmann, D., OCallaghan, L., & Shoham, Y. (2002). Truth revelation in rapid, approximately ecient combinatorial auctions. Journal of the ACM, 49 (5), 577602. A
preliminay version appeared at Proc. of the rst ACM Conference on Electronic Commerce.
Mas-Collel, A., Whinston, W., & Green, J. (1995). Microeconomic Theory. Oxford university
press.
Nisan, N. (2000). Bidding and allocation in combinatorial auctions. In Proceedings of the
Second ACM Conference on Electronic Commerce, pp. 112.
Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games and Economic
Behaviour, 35, 166196. Extended abstract appeared in Proceedings of the Thirty
First Annual ACM symposium on Theory of Computing.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. MIT press.
Parkes, D. (1999). ibundle: An ecient ascending price bundle auction.. In Proceedings of
the ACM Conference on Electronic Commerce (EC-99), pp. 148157.
Porter, R., Ronen, A., Shoham, Y., & Tennenholtz, M. (2002). Mechanism design with
execution uncertainty. In In Proceedings of the 18th Conference on Uncertainty in
Articial Intelligence, pp. 414421.
Roberts, K. (1979). The characterization of implementable choise rules. In Laont, J.-J.
(Ed.), Aggregation and Revelation of Preferences, pp. 321349. North-Holland. Papers
presented at the rst European Summer Workshop of the Econometric Society.
46

fiComputationally Feasible VCG Mechanisms

Ronen, A. (2001). Mechanism design with incomplete languages. In Proceedings of the
Third ACM Conference on Electronic Commerce, 105114.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter: Designing Conventions for
Automated Negotiation Among Computers. MIT Press.
Shoham, Y., & Tanaka, K. (1997). A dynamic theory of incentives in multi-agent systems
(preliminary report). In Proceedings of the Fifteenth International Joint Conferences
on Articial Intelligence, pp. 626631.
Shoham, Y., & Tennenholtz, M. (2001). The fair imposition of tasks in multi-agent systems.
In Proceedings of the International Conference on Articial Intelligence, pp. 1083
1088.
Vickrey, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal
of Finance, 837.
Wellman, M., Wurman, P., Walsh, W., & MacKie-Mason, J. (2001). Auction protocols for
decentralized scheduling. Games and Economic Behavior, 35, 271303.
Zuckerman, D. (2006). Linear degree extractors and the inapproximability of max clique
and chromatic number. In Proceedings of the 38th ACM Symposium on Theory of
Computing, Seattle, Washington, USA.

47

fi