Journal of Artificial Intelligence Research 39 (2010) 633-662

Submitted 05/10; published 11/10

A Utility-Theoretic Approach to Privacy in Online Services
Andreas Krause

KRAUSEA @ CALTECH . EDU

California Institute of Technology,
1200 E California Blvd.,
Pasadena, CA 91125, USA

Eric Horvitz

HORVITZ @ MICROSOFT. COM

Microsoft Research,
One Microsoft Way,
Redmond, WA 98052-6399, USA

Abstract
Online offerings such as web search, news portals, and e-commerce applications face the challenge of providing high-quality service to a large, heterogeneous user base. Recent efforts have
highlighted the potential to improve performance by introducing methods to personalize services
based on special knowledge about users and their context. For example, a users demographics,
location, and past search and browsing may be useful in enhancing the results offered in response
to web search queries. However, reasonable concerns about privacy by both users, providers, and
government agencies acting on behalf of citizens, may limit access by services to such information. We introduce and explore an economics of privacy in personalization, where people can opt
to share personal information, in a standing or on-demand manner, in return for expected enhancements in the quality of an online service. We focus on the example of web search and formulate
realistic objective functions for search efficacy and privacy. We demonstrate how we can find a
provably near-optimal optimization of the utility-privacy tradeoff in an efficient manner. We evaluate our methodology on data drawn from a log of the search activity of volunteer participants.
We separately assess users preferences about privacy and utility via a large-scale survey, aimed at
eliciting preferences about peoples willingness to trade the sharing of personal data in returns for
gains in search efficiency. We show that a significant level of personalization can be achieved using
a relatively small amount of information about users.

1. Introduction
Information about the preferences, activities, and demographic attributes of people using online
applications can be leveraged to personalize the services for individuals and groups of users. For
example, knowledge about the current locations of users performing web searches can help identify
their informational goals. Researchers and organizations have pursued explicit and implicit methods
for personalizing online services. For web search, explicit personalization methods rely on users
indicating sets of topics of interest that are stored on a server or client. Implicit methods make use
of information collected in the absence of user effort and awareness. Data collected implicitly in
web search can include users locations and search activities, capturing such information as how
people specify and reformulate queries and click, dwell, and navigate on results. Beyond web
search, data collected about users in an implicit manner can be used to custom-tailor the behaviors
of a broad spectrum of online applications from informational services like news summarizers to

c
2010
AI Access Foundation. All rights reserved.

fiK RAUSE & H ORVITZ

e-commerce services that provide access to online shopping, and that seek to maximize sales with
targeted advertising.
The potential value of harnessing data about people to enhance online services coupled with
the growing ubiquity of online services raises reasonable concerns about privacy. Both users and
the hosts of online applications may benefit from the custom-tailoring of services. However, both
may be uncomfortable with the access and use of personal information. There has been increasing
discussion about incursions into the privacy of users implied by the general logging and storing of
online data (Adar, 2007). Beyond general anxieties with sharing personal information, people may
more specifically have concerns about becoming increasingly identifiable; as increasing amounts
of personal data are acquired, users become members of increasingly smaller groups of people
associated with the same attributes.
Most work to date on personalizing online services has either ignored the challenges of privacy
and focused efforts solely on maximizing utility (c.f., Sugiyama, Hatano, & Ikoma, 2004) or has
completely bypassed the use of personal data. One vein of research has explored the feasibility
of personalizing services with methods that restrict the collection and analysis of personal data to
users own computing devices (Horvitz, 2006). Research in this realm includes efforts to personalize
web search by making use of the content stored on local machines, as captured within the index of
a desktop search service (Teevan, Dumais, & Horvitz, 2005; Xu, Zhang, & Wang, 2007). Rather
than cut off opportunities to make personal data available for enhancing online services or limit personalization to client-side analyses, we introduce and study utility-theoretic methods that balance
the costs of sharing of personal data with online services in return for the benefits of personalization. Such a decision-theoretic perspective on privacy can allow systems to weigh the benefits of
enhancements that come with adaptation with the costs of sensing and storage according to users
preferences.
We characterize the utility of sharing attributes of private data via value-of-information analyses
that take into consideration the preferences to users about the sharing of personal information. We
explicitly quantify preferences about utility and privacy and then solve an optimization problem to
find the best trade. Our approach is based on two fundamental observations. The first is that, for
practical applications, the utility gained with sharing of personal data may often have a diminishing
returns property; acquiring more information about a user adds decreasing amounts to the utility of
personalization given what is already known about the users needs or intentions. On the contrary, the
more information that is acquired about a user, the more concerning the breach of privacy becomes.
For example, a set of individually non-identifying pieces of information may, when combined,
hone down the user to membership in a small group, or even identify an individual. We map the
properties of diminishing returns on utility and the concomitant accelerating costs of revelation to
the combinatorial concepts of submodularity and supermodularity, respectively.
Although the economic perspective on privacy is relevant to a wide spectrum of applications,
and to studies of the foundations of privacy more broadly, we shall illustrate the concepts in application to personalizing web search. We employ a probabilistic model to predict the web page
that a searcher is going to visit given the search query and attributes describing the user. We define
the utility of a set of personal attributes by the focusing power of the information gained with respect to the prediction task. Similarly, we use the same probabilistic model to quantify the risk of
identifying users given a set of personal attributes. We then combine the utility and cost functions
into a single objective function, which we use to find a small set of attributes which maximally
increases the likelihood of predicting the target website, while making identification of the user as
634

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

difficult as possible. The challenges of this optimization are in identifying the benefits and costs
of sharing information and grappling with the computational hardness of the analysis. Solving for
the best set of attributes for users to reveal (and hence for the optimal setting of the utility-privacy
tradeoff) is an NP-hard search problem, and thus intractable in general for large sets of attributes.
We shall demonstrate how we can use the submodularity of the utility and supermodularity of privacy in order to find a near-optimal tradeoff efficiently. To our knowledge, no existing approach
(such as LeFevre, DeWitt, & Ramakrishnan, 2006; Chen, LeFevre, & Ramakrishnan, 2007; Hore &
R. Jammalamadaka, 2007) provides such theoretical guarantees. We evaluate our approach on realworld search log data, as well as from data collected from a user study with over 1,400 participants
focused on the elicitation of preferences about sharing sensitive information. Our results indicate
the existence of prominent sweet spots in the utility-privacy tradeoff curve, at which most of the
utility can be achieved with the sharing of a minimal amount of private information.
The manuscript is organized as follows. In Section 2, we formalize the utility-privacy tradeoff as
an optimization problem and introduce objective functions. Section 3 identifies the submodular and
supermodular structure of the utility and cost functions. In Section 4, we introduce an algorithm
for finding a near-optimal solution, which exploits this combinatorial structure. In Section 6, we
describe the experimental design of our user study. Section 5 describes the experimental setup,
and Section 7 presents empirical evaluation of our approach on real-world search data. Section 8
presents related work. Section 9 reviews approaches to deploying the methodology described in this
paper, and Section 10 presents a summary and conclusions.

2. Privacy-Aware Personalization
We consider the challenge of personalization as diagnosis under uncertainty: We seek to predict a
searchers information goals, given such noisy clues as query terms and potentially additional attributes that describe users and their interests and activities. We frame the challenge probabilistically
(as done, e.g., in Dou, Song, & Wen, 2007; Downey, Dumais, & Horvitz, 2007 in the search context),
by modeling a joint distribution P over random variables, which comprise the target intention X,
some request-specific attributes (e.g., the query term) Q, the identity of the user Y , and several attributes V = {V1 , V2 , . . . , Vm } containing private information. Such attributes include user-specific
variables (such as demographic information, search history, word frequencies on the local machine,
etc.) and request-specific variables (such as the period of time since an identical query was submitted). We describe the concrete attributes used in this work for the web search context in Section 5.
Additional examples are described by Downey et al. (2007) and Teevan et al. (2005). We shall describe the use of statistical techniques to learn a predictive model P from training data for frequent
queries. Then, we present methods for trading off utility and privacy in the context of the model.
2.1 Utility of Accessing Private Data
Upon receiving a new request Q, and given a subset A  V of the attributes, we can use the probabilistic model to predict the target intention by performing inference, computing the conditional
distribution P (X | Q, A). Then, we use this distribution to inform the decision of, e.g., which
search results to present to the user. We use the notation #intents to refer to the domain size of X
(e.g., the maximum number of different webpages clicked on by the users). The hope in personalization is that additional knowledge about the user (i.e., the observed set of attributes A) will help to
simplify the prediction task, via reducing the uncertainty in P (X | Q, A). Based on this intuition,
635

fiK RAUSE & H ORVITZ

we quantify the uncertainty in our prediction using the conditional Shannon entropy (c.f., Cover &
Thomas, 1991) associated with the variance in target web sites following queries,
X
P (x, q, a) log2 P (x | q, a).
H(X | Q, A) = 
x,q,a

Hence, for any subset A  V, we define its utility U (A) to be the information gain, i.e., expected
entropy reduction achieved by observing A:
U (A) = H(X | Q)  H(X | Q, A)
X
P (x, q, a) [log2 P (x | q)  log2 P (x | q, a)] .
=
x,q,a

Such click entropy has been previously been found effective by Dou et al. (2007).
2.2 Cost of Sharing Private Data
Several different models of privacy have been proposed in prior work (c.f., Sweeney, 2002; Machanavajjhala, Kifer, Gehrke, & Venkitasubramaniam, 2006; Dwork, 2006). Our cost function is motivated
by the consideration that sets of attributes A  V should be preferred that make identification of individuals as difficult as possible. We can consider the observed attributes A as noisy observations of
the (unobserved) identity Y = y of the user. Intuitively, we want to associate high cost C(A) with
sets A which allow accurate prediction of Y given A, and low cost for sets A for which the conditional distributions P (Y | A) are highly uncertain. For a distribution P (Y ) over users, we hence
define an identifiability loss function L(P (Y )) which maps probability distributions over users Y to
the real numbers. L is chosen in a way, such that if there exists a user y such that P (Y = y) is close
to 1, then the loss L(P (Y )) is very large. If P (Y ) is the uniform distribution, then L(P (Y )) is close
to 0. We will explore different loss functions L below. Based on such loss functions, we define the
identifiability cost I(A) as the expected loss of the conditional distributions P (Y | A = a), where
the expectation is taken over the observations A = a1 :
X
I(A) =
P (a)L(P (Y | A = a)).
a

P In addition to identifiability, we introduce an additional additive cost component S(A) =
aA s(a), where s(a)  0 is a nonnegative quantity modeling the subjective sensitivity of attribute a, and other additive costs, such as data acquisition cost, etc. The final cost function C(A)
is a combination of the identifiability cost I(A) and sensitivity S(A), i.e., C(A) = I(A) + S(A).
2.2.1 I DENTIFIABILITY L OSS F UNCTIONS
There are several ways to quantify identifiability. One approach to representing the loss L is with
the negative entropy of the distribution P (Y ) of the users identity Y 2 , as used to quantify utility.
However, in the context of privacy, this choice is rather poor: Consider a case where we seek to
quantify the cost associated with the change in identifiability of the user that comes with learning
1. Similarly, we can additionally take the expectation over the request Q
2. Note that instead of the users identity (composed of all attribute values) in principle Y could refer to a particular
sensitive attribute (such as, e.g., sexual orientation).

636

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

the searchers gender. Assuming an equal distribution of males and females, learning the gender
of the searcher would halve the space of possible searchers, hence increasing the entropy loss by
one. However, this increase is independent of whether we start with (a) one billion or (b) only two
searchers. In contrast to the influence on utility, where halving the search space of pages to consider
is a very large gain, independent of the number of pages we start with (Dou et al., 2007), such a diminishment of anonymity is enormous: In case (a), an adversary trying to identify the searcher based
on knowing their gender has almost no chance of success, whereas in case (b) they would always
identify the person. Motivated by this consideration, we represent the privacy cost in our experiments as the maxprob loss (Chen et al., 2007), Lm (P (Y )) = maxy P (y). This loss function can be
interpreted as follows: An adversary seeks to identify the user Y , and predicts the most likely user,
and receives one unit reward if the user is guessed correctly, and 0 otherwise. The identifiability cost
X
P (a) max(P (y | A = a))
Im (A) =
a

y

then is the expected win obtained by the adversary. This objective function makes most sense if we
believe the adversary only has access to the same data sources as we do (and thus the probability
distribution P captures the assumptions about the adversarys inferences). We also consider the cost
function


X
I` (A) = 
P (a) log 1  max(P (y | A = a)) .
y

a

I` is a rescaled variant of the maxprob loss, with the property that certainty (i.e., maxy (P (y |
A = a)  1) is more severely penalized.
Another criterion for identifiability is k-anonymity (Sweeney, 2002). With this measure, a data
set is called k-anonymous, if any combination of attributes is matched by at least k people. We can
define a probabilistic notion of k-anonymity, Ik , by using the loss function Lk (P (Y )) which is 1 if
P is nonzero for less than k values of Y , 0 otherwise. The identifiability cost is then
X
Ik (A) =
P (a)Lk (P (Y | A = a)).
a

Ik (A) can be interpreted as the expected number of violations of k-anonymity; a database (empirical distribution over users) is k-anonymous if and only if Ik (A) = 0. See Lebanon, Scannapieco,
Fouad, and Bertino (2009) for a justification of using a decision-theoretic analysis to quantify privacy and how inferential attacks through side information can be handled.
We experimentally compare the cost metrics in Section 7.1.
2.3 Optimizing the Utility-Privacy Tradeoff
Previously, we described how we can quantify the utility U (A) for any given set of attributes A,
and its associated privacy cost C(A). Our goal is to find a set A, for which U (A) is as large as
possible, while keeping C(A) as small as possible. To optimize utility for users under this tradeoff,
we use scalarization (Boyd & Vandenberghe, 2004), and define a new, scalar objective
F (A) = U (A)  C(A).

637

fiK RAUSE & H ORVITZ

Hereby,  plays the role of a privacy-to-utility conversion factor. The goal is to solve the following
optimization problem:
A = argmax F (A)
(2.1)
A

By varying , we can find different solutions A .

If we choose a very small , we find solutions with
higher utility and higher cost; large values of  will lead to lower utility, but also lower privacy cost.
If the set of attributes V is large, then (2.1) is a difficult search problem, as the number of subsets
A grows exponentially in the size of V. It can be shown that the solution to this problem is hard
even to approximate:
Theorem 2.1. If there is a constant  > (11/e) and there exists an algorithm which is guaranteed
to find a set A0 such that F1 (A0 )   maxA F1 (A), then P = N P .
The proofs of all theorems are presented in the Appendix. Given the complexity, we cannot
expect to find a solution A efficiently which achieves even slightly more than (1  1/e)  63% of
the optimal score. However, we can find a solution which is guaranteed to achieve at least 1/3 of
the optimal value.
2.4 Hard Constraints on Privacy Cost
Instead of optimizing the tradeoff F (A) = U (A)  C(A), one may be interested in maximizing
the utility U (A) subject to a hard constraint on the cost C(A), i.e., solve
A = argmax U (A) s.t. C(A)  B,

(2.2)

A

for some value of B  0. For example, users may be interested in maximizing utility while enforcing k-anonymity (in which case we would constrain Ik (A)  0). This solution would then provide
per-user guarantees, i.e., k-anonymity is never violated. In principle, one could solve the tradeoff
F (i.e., problem (2.1)) for different values of  and then, e.g., using binary search, choose  that
maximizes U (A) among all feasible solutions (i.e., C(A)  B). In a sense, (2.1) can be seen as a
Lagrangian relaxation of (2.2).
In the following, we will focus on the tradeoff problem (2.1). Using the procedure described
above, our approach may be useful to solve the constrained problem (2.2) in practice (however our
approximation guarantees will only hold for problem (2.1)).

3. Properties of the Utility-Privacy Tradeoff
As mentioned above, we would expect intuitively that the more information we already have about
a user (i.e., the larger |A|), the less the observation of a new, previously unobserved, attribute would
help with enhancing a service. The combinatorial notion of submodularity formally captures this intuition. A set function G : 2V  R mapping subsets A  V into the real numbers is called submodular (Nemhauser, Wolsey, & Fisher, 1978), if for all A  B  V, and V 0  V\B, it holds that G(A
{V 0 })  G(A)  G(B  {V 0 }  G(B), i.e., adding V 0 to a set A increases G more than adding V 0
to a superset B of A. G is called nondecreasing, if for all A  B  V it holds that G(A)  G(B).
A result of Krause and Guestrin (2005) shows that, under certain common conditional independence conditions, the reduction in click entropy is submodular and nondecreasing:

638

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Theorem 3.1 (Krause & Guestrin,
Q 2005). If the attributes V are conditionally independent given
X, i.e., P (V1 , . . . , Vm | X) = i P (Vi | X) then U (A) is submodular in A.
We discussed earlier how we expect the privacy cost to behave differently: Adding a new attribute would likely make a stronger incursion into personal privacy when we know a great deal
about a user, and less if we know little. This increasing costs property corresponds with the combinatorial notion of supermodularity: A set function G : 2V  R is called supermodular (Nemhauser
et al., 1978), if for all A  B  V, and V 0  V \ V , it holds that G(A  {V 0 })  G(A) 
G(B  {V 0 }  G(B), i.e., adding V 0 to a large set B increases G more than adding V 0 to a subset
A of B. In fact, we can prove that the maxprob identifiability cost function introduced in Section 2
is supermodular.
Theorem 3.2. Assume, the attributes V are marginally independent, and the user Y is completely
characterized by the attributes, i.e., Y = (V). Then the maxprob loss Im (A) is supermodular in A.
Note that the attribute sensitivity S(A) is per definition additive and hence supermodular as well.
Thus, as a positive linear combination of supermodular functions, C(A) = I(A) + S(A) is supermodular in A for I(A) = Im (A). In our empirical evaluation, we verify the submodularity of U (A)
and supermodularity C(A) even without the assumptions made by Theorem 3.1 and Theorem 3.2.
Motivated by the above insights about the combinatorial properties of utility and privacy, we
formulate a general approach to trading off utility and privacy. We only assume that the utility
U (A) is a submodular set function, and that C(A) is a supermodular set function. We define the
general utility-privacy tradeoff problem as follows:
Problem 3.3. Given a set V of possible attributes to select, a nondecreasing submodular utility
function U (A), a nondecreasing supermodular cost function C(A), and a constant   0, our goal
is to find a set A such that
A = argmax F (A) = argmax U (A)  C(A)
A

(3.1)

A

Since C(A) is supermodular if and only if C(A) is submodular, and since nonnegative linear combinations of submodular set functions are submodular as well, the scalarized objective
F (A) = U (A)  C(A) is submodular as well. Hence, problem (3.1) requires the maximization of a submodular set function.

4. Optimization Algorithms
As the number of subsets A  V grows exponentially with the size n of V, and the NP-hardness of
Problem (2.1), we cannot expect to find the optimal solution A efficiently. Theorem 2.1 shows that
it is NP-hard to even approximate the optimal solution to better than a constant factor of (11/e). A
fundamental result by Nemhauser et al. (1978) characterized the performance of the simple greedy
algorithm, which starts with the empty set A =  and myopically adds the attribute which increases
the score the most, i.e., A  A  {argmaxV 0 F (A  {V 0 })}, until k elements have been selected
(where k is a specified constant). It was shown that, if F is nondecreasing, submodular and F () =
0, then the greedy solution AG satisfies F (AG )  (1  1/e) max|A|=k F (A), i.e., the greedy solution achieves a value of at least a factor of 11/e of the optimal solution. Although this result would
allow us to perform such tasks as selecting a near-optimal set of k private attributes maximizing the
639

fiK RAUSE & H ORVITZ

utility U (A) (which satisfies the conditions of the result from Nemhauser et al., 1978), it unfortunately does not apply in the more general case, where the objective F (A) is not nondecreasing.
The problem of maximizing such non-monotone submodular functions has been resolved by
Feige, Mirrokni, and Vondrak (2007). A local search algorithm, named LS, was proved to guarantee
a near-optimal solution ALS , if F is a nonnegative3 (but not necessarily nondecreasing) submodular
function:
1. Let V   argmaxV 0 V F ({V 0 }) and init. A  {V  }
2. If there exists an element V 0  V \ A such that F (A  {V 0 }) > (1 +
A  A  {V 0 }, and repeat step 2.
3. If there exists an element V 0  A such that F (A \ {V 0 }) > (1 +
A \ {V 0 }, and go back to step 2.


)F (A),
n2


)F (A),
n2

then let

then let A 

4. Return ALS  argmax{F (A), F (V \ A)}.
This algorithm works in an iterative manner to add or remove an element V 0 in order to increase
the score, until no further improvement can be achieved. Feige et al. (2007) prove the following
Theorem:
Theorem 4.1 (Feige et al., 2007). If F is a nonnegative submodular function, then, for the solution
ALS returned by algorithm LS, it holds that


1

F (ALS ) 

max F (A).
A
3 n
LS uses at most O( 1 n3 log n) function evaluations.
Hence, LS returns a solution ALS achieving at least 1/3 of the optimal score.
4.1 An Efficient Implementation
The description of Algorithm LS allows some freedom for implementing the search in steps 2 and
3. In our implementation, we select in a greedy manner the element V 0 which most increases the
objective function. Furthermore, to speed up computation, we use lazy evaluation to find the greedy
elements (Robertazzi & Schwartz, 1989).
The algorithm LLS (c.f., Figure 1) performs a sequence of upwards and downwards passes,
adding and removing elements which improve F by at least a factor of (1 + n2 ). The upwards pass
is done in a lazy mannerthe increments V are lazily updated only when necessary. We found
that the lazy computation can reduce the running time by an order of magnitude. The correctness
of this lazy procedure follows directly from submodularity: Submodularity implies that the V are
monotonically nonincreasing as more and more elements are added to A. The lazy updates can also
be applied for the greedy downward pass. We discovered that Algorithm 1 usually terminates after
one single downwards pass, without removing a single element.
3. If F takes negative values, then it can be normalized by considering F 0 (A) = F (A)  F (V), which however can
impact the approximation guarantees.

640

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Input: Submodular function F
Output: Near-optimal selection A of personal attributes
begin
A  ;
repeat
change = f alse;
/* Lazy greedy upward pass:
*/
foreach V  V do V  F (A  {V })  F (A); currentV  true;
repeat
V 0  argmaxV V\A V ;
if currentV 0 = true then
if V 0 > (1 + n2 )F (A) then
A  A  {V 0 }; foreach V  V do currentV  f alse
else
break;
end
else
V 0  F (A  {V 0 })  F (A); currentV 0  true;
end
until V 0  (1 + n2 )F (A) ;
/* Lazy greedy downward pass:
*/
foreach V  A do V  F (A \ {V })  F (A); currentV  true;
repeat
V 0  argmaxV A V ;
if currentV 0 = true then
if V 0 > (1 + n2 )F (A) then
A  A \ {V 0 }; change = true; foreach V  A do currentV  f alse
else
break;
end
else
V 0  F (A \ {V 0 })  F (A); currentV 0  true;
end
until V 0  (1 + n2 )F (A) ;
until change = f alse ;
return argmax{F (A), F (V \ A)}
end
Algorithm 1: The lazy local search (LLS) algorithm.

641

fiK RAUSE & H ORVITZ

4.2 Evaluating Utility and Cost
To run LLS, we need to be able to efficiently evaluate the utility U (A) and cost C(A). In principle, we can compute the objective functions from the empirical distribution of the training data, by
explicitly evaluating the sums defining U (A) and C(A) (c.f., Section 2). However, this approach is
very inefficient  (N 2 ) where N is the number of training examples. Instead, we can estimate
U (A) and C(A) by sampling. Krause and Guestrin (2005) show how the Hoeffding inequality (Hoeffding, 1963) can be used in order to approximately compute conditional entropies. The Hoeffding
inequality allows us to acquire bounds on the number of samples needed in order to determine the
expectation E[H] of a random variable H, if H is bounded. In the case of click entropy reduction,
we use the random variable
H | [Q = q, A = a] = H(X)  H(X | q, a).
H is a deterministic function modeling the click entropy reduction if request Q = q and attributes
A = a are observed. Since Q and A are random variables, H is random as well. Since H is bounded
between 0 and log2 (#intents), the Hoeffding inequality can be applied, and the following holds:
Lemma 4.2 (Krause & Guestrin, 2005). For any  > 0 and  > 0, we need
& 
'

1 log2 (#intents) 2
1
log
2


samples in order to estimate U (A) to absolute error  with confidence at least 1  .
For the identifiability loss I(A), we can proceed in a similar manner. Both the maximum probability and the k-anonymity loss are bounded between 0 and 1. Using a similar argument as in the
proof of Lemma 4.2, we have the following result:


Lemma 4.3. For any  > 0 and  > 0, we need 212 log 1 samples in order to estimate C(A) to
absolute error  with confidence at least 1  .
We can generalize Theorem 4.1 to also hold in the case where utility and cost are estimated up
to small constant error. The following theorem summarizes the analysis:
Theorem 4.4. If  such that F (V)  0, then LLS, using sampling to estimate C(A) and U (A),
computes a solution ALLS such that


1

F (ALLS ) 

max F (A)  nS ,
A
3 n
with probability at least 1  . The algorithm uses at most
O

1 3
n log n




log2 (#intents)
S

2

1
log 3
n

!

samples.
Hence, the algorithm LLS will efficiently find a solution ALLS which achieves at least a constant fraction of 1/3 times the value of the optimal solution.
642

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

4.3 Computing Online Bounds
The bounds provided by Theorem 4.4 are offline, in the sense that they can be stated before running
Algorithm 1. We can use the submodularity of U (A) and supermodularity of C(A) additionally for
computing online bounds on the performance of any algorithm.
Theorem 4.5. Let A0  V be an arbitrary set of attributes. For each V  V \ A define V =
U (A0  {V })  U (A0 )  C({V }). Let B = {V : V > 0}. Then
X
max F (A)  U (A0 ) +
V .
A

V B

It is again possible to extend these bounds to the case where the objective function F is evaluated with small absolute error via sampling.
4.4 Finding the Optimal Solution
Although LLS allows us to find a near-optimal solution in polynomial time, submodularity of
F can also be exploited to find an optimal solution in a more informed way, allowing us to bypass an exhaustive search through all exponentially many subsets A. Existing algorithms for optimizing submodular functions include branch and bound search, e.g., in the data-correcting algorithm by Goldengorin, Sierksma, Tijssen, and Tso (1999), as well as mixed-integer programming
(Nemhauser & Wolsey, 1981). These approaches also do not require nonnegativity of F . The
mixed-integer programming approach by Nemhauser et.al. effectively uses bounds similar to those
presented in Theorem 4.5.

5. Search Log Data and Attributes
We estimate the utility U (A) and cost C(A) of sets of private attributes from data. We use search
log data, based on a total of 247,684 queries performed by 9,523 users from 14 months between
December 2005 and January 2007. The search data was obtained from users who had volunteered
to participate in a public Microsoft data sharing program centering on the use of information about
their search activities to enhance search. The data was filtered to include only those queries which
had been performed by at least 30 different users, resulting in a total of 914 different queries.
For the utility U (A), we compute the average reduction in click entropy (in bits) with respect to
the per-query distribution of web pages chosen, as defined in Section 2. From the demographic
information and the search logs, we compute 31 different user / query specific attributes. In selecting
our attributes, we chose a very coarse discretization. No attribute is represented by more than three
bits, and most attributes are binary. We consider the following attributes, which are summarized in
Table 1.
5.1 Demographic Attributes
The first set of attributes contains demographic information, which were voluntarily provided separately as part of signing up for a set of online services. The attributes contain gender, age group,
occupation and region, each very coarsely discretized into at most three bits. Gender was specified
by 86% of users, age by 94%, occupation by 57%. 44% of the users specified their gender as male,
42% as female. 8% of users asserted that they are less than 18 years, 53% between 18 and 55, and
643

fiK RAUSE & H ORVITZ

33% 55 and older. Most users were from the US and Canada (53%), followed by 7% from the European Union and 3% from Asia. Other locations were unspecified. 10% of the searchers specified
that they are students.
5.2 Search Activity Attributes
The next set of attributes contains features extracted from search history data. For each query,
we determine whether the same query has been performed before (AQRY; 70% of the queries are
repeated), as well as whether the searcher has visited the same webpage (ACLK) before (53% of
the clicks). We consider the sequence of websites visited following a query, and associate the
hostname of the first website on which the surfer dwells for at least 30 seconds as the intended
target. The attribute AFRQ describes whether the user performed at least one query each day. 47%
of the searchers performed at least one search per day. We also log the top-level domain (ATLV)
determined by reverse DNS lookup of the query IP address, and used only the domains .net, .com,
.org and .edu. 83% of the queries were associated with one of these domains. We determine if a
user ever performed queries from at least 2 different zip codes (AZIP; true 31%), cities (ACTY; true
31%) and countries (ACRY; true 2%), by performing reverse DNS lookup of the query IP addresses.
For each query, we store whether the query was performed during working hours (AWHR; between
7 am and 6 pm) and during workdays (AWDY; Mon-Fri) or weekend (Sat, Sun), without accounting
for holidays. Workdays account for 73% of the queries, but only 40% of the queries were done
during working hours.
5.3 Topic Interests
We looked up all websites visited by the user during 2006 in the 16 element top-level category
of the Open Directory Project directory (www.dmoz.org). For each category, we use a binary
attribute indicating whether the user has ever visited a website in that category (acronyms for topics
are indicated with prefix T). Topic classification was available for 96% of the queries.

6. Survey on Privacy Preferences
Although identifiability can be an important part of privacy, people may have different preferences
about sharing individual attributes (Olson, Grudin, & Horvitz, 2005). We set out to assess preferences about cost and benefits of the sharing of different kinds of personal data. Related work has
explored elicitation of private information (c.f., Huberman, Adar, & Fine, 2005; Wattal, Telang,
Mukhopadhyay, & Boatwright, 2005; Hann, Hui, Lee, & Png, 2002). We are not familiar with a
similar study for the context of web search. Our survey was designed specifically to probe preferences about revealing different attributes of private data in return for increases in the utility of a
service (in this case, in terms of enhanced search efficiency). As previous studies by Olson et al.
(2005) show, willingness to share information greatly depends on the type of information being
shared, with whom the information is shared, and how the information is going to be used. In designing the survey, we tried to be as specific as possible, by specifying a low-risk situation, in which
the personal information would be shared and used only with respect to a single specified query,
and discarded immediately thereafter. Our survey contained questions both on the sensitivity of
individual attributes and on concerns about identifiability. The survey was distributed within Microsoft Corporation via an online survey tool. We motivated people to take the survey by providing

644

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Figure 1: Willingness to share attributes
5

Sensitivity

4

3

2

1

AFRQ TNWS TSCI TART TGMS TBUS THEA TSOC DGDR AQRY ACLK ACRY DOCC DMTL DCHD AWHR

Figure 2: Sensitivity of individual attributes (with 95% confidence intervals)

participants with a lottery where they could win a media player via a random drawing. The survey
was open to worldwide entries, and we received a total of 1,451 responses. Again, we use acronyms
to refer to the personal attributes, as defined in Table 1.

645

fiK RAUSE & H ORVITZ

6.1 Questions about Individual Attributes
First, we assessed attributes that participants would be willing to share with the search engine if
revealing these attributes would double search performance. Interestingly, for all attributes probed,
more than 50% of study participants asserted that they would agree to share the information given
the promised efficiency gain. Also, the sharing rates are very similar to those estimated in the survey
(78% for gender, 82% for age, and 64% for occupation). The least willingness to share (54.4%
and 52.6% respectively) was exhibited for marital status (DMTL) and whether the participant has
children (DCHD), closely followed by occupation (63.9%). Most participants (92.7%) would rather
share their region than share that their interests include news-related webpages (TNWS). Figure 1
presents the results for this question. We also asked the participants to classify the sensitivity of
the attributes on a Likert scale from 1 (not very sensitive) to 5 (highly sensitive). The order of the
questions was randomized. Figure 2 presents the results. The frequency of search engine usage
(AFRQ) as well as very general topic interests, e.g., in news pages (TNWS), are considered to
be of low sensitivity. Interestingly, we found that there are significant differences in preferences
among participants even for sharing with a service interests in different topics; participants showed
significantly greater sensitivity to sharing their interest in health or society related websites (THEA,
TSOC) than in news or science-related pages (TNWS, TSCI). The biggest jump in sensitivity
occurs between attributes ACLK, referring to sharing a repeated visit to same website, and ACRY,
referring to having recently traveled internationally. We found that participants were most sensitive
to sharing whether they are at work while performing a query (AWHR).
6.2 Questions about Identifiability
We also elicited preferences about sharing personal data at different degrees of precision and with
different levels of identifiability. First, we sought to identify changes in the sensitivity associated
with sharing personal data at increasingly higher resolution. More specifically, we inquired about
the sensitivities of participants to sharing their ages at the level of groups of 20, 10, 5, and 1 years,
and their exact birth dates. Similarly, we asked how sensitive participants would be to sharing
their location at the region, country, state, city, zip code, or address levels of detail. Figures 3(a)
and 3(b) present the mean sensitivity with 95% confidence intervals for this experiment. We also
assessed participants sensitivities to having their search activity stored in different ways. More
specifically, we asked users to assess their sensitivity about storing only a topic classification of
the visited websites (i.e., whether a news- , business-, health-related etc. site was visited), storing
all searches for 1 or 3 years, or storing all searches indefinitely. Lastly, we asked the participants
how sensitive they would be, if, in spite of sharing the information, they would be guaranteed to
remain indistinguishable from at least k other people (thereby eliciting preferences about k of kanonymity). Here, we varied k among 1, 10, 100, 1,000, 10,000, 100,000 and 1 million. Figure 3(c)
presents the results of these experiments.
We draw a number of conclusions about the preferences of the population studied. First, storing
search activity is generally considered more sensitive than sharing certain demographic information.
For example, even storing only the topic classification of visited web pages is considered significantly more sensitive than sharing the city or birth year of a user. This result indicates that logging
search activity is considered at least as threatening to privacy as sharing certain demographic information. The survey also shows that study participants have strong preferences about the granularity
of the shared information. As explained below in Section 7.2, we can use the information obtained
646

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

5

Sensitivity

4

3

2

1

region country state

(a) Age

city

zip

address

(b) Location

5

Sensitivity

4

3

2

1
(c) Storage

1M 100K 10K

1K

100

10

1

(d) Discriminability

Figure 3: Sensitivity of sharing age (a) and location (b) under different levels of discretization.
(c) Sensitivity of storing searches for varying amounts of time. (d) Sensitivity of kdiscriminability levels (right). Plots show 95% confidence intervals.

from this experiment to explicitly take into account peoples preferences when trading off privacy
and utility.
6.3 Questions about Utility
In addition to assessing the sensitivity of sharing different kinds of personal information, we asked
the participants to assess the degree of improvement they would require in order to share attributes
of a given sensitivity level. More specifically, we asked: How much would a search engine have to
improve its performance, such that you would be willing to share information you consider 1/2/....
As response options, we offered average improvements by 25%, 50%, 100%, as well as the outcome
of immediately presenting the desired page 95% of the time (which we associated with a speedup
by a factor of 4). We also allowed participants to opt to never share information at a specified
sensitivity level. These responses, in conjunction with the earlier sensitivity assessments, allowed

647

fiK RAUSE & H ORVITZ

Figure 4: Which attributes are currently stored?

Figure 5: Using sensitivity as a common currency

us to establish sensitivity as a common currency of utility and cost. Figure 5 presents the median, 25
and 75-percentiles of the responses for the k-discriminability question and the above utility question.
6.4 Questions about the Current State
Additionally, we assessed the current understanding of the state of privacy in search. More specifically, we asked, whether the participants believe that most current search engines store the following
attributes: DGDR, DAGE, DOCC, DZIP, ASEA, DCRY, TNWS, DEML. We found that most participants (96%) assume that search engines know the country (DCRY) a searcher is from. Also,
most participants (86%) assume that all searches are stored by the search engines. Fewer participants believe that demographic information such as occupation (16%), age (28%) or gender (34%)

648

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Figure 6: Utility (average click entropy reduction in bits) according to greedy ordering

Figure 7: Cost according to greedy ordering

are known to the search engine. This result is an important baseline in order to understand the
sensitivity classification of the individual attributes.

7. Results
We now describe our empirical results on calibrating and optimizing the utility-privacy tradeoff.
7.1 Computing Utility and Cost
We use the empirical distribution of the data described in Section 5, and evaluate utility and cost by
sampling. Each sample is a row picked uniformly at random from the search logs. We then find all
queries matching the selected attributes (A = a), and compute the conditional entropy of the click
distribution, as well as the identifiability loss function. In order to avoid overfitting with sparse data,

649

fiK RAUSE & H ORVITZ

median #bits required

4

3
Identifiability cost
(maxprob)

2

1

Median #bits
(from survey)

0
region

(a) Tradeoff-curve

country

state

city

zip

(b) Calibration

Figure 8: (a) Tradeoff-curve for varying . (b) Calibrating the tradeoff.

we applied Dirichlet smoothing. In our experiments, we use 1000 independent samples in order to
estimate U (A) and I(A).
We first apply the greedy algorithm to select an increasing number of attributes, maximizing
the utility and ignoring the cost. Figure 6 presents the greedy ordering and the achieved entropy reductions. The greedy algorithm selects the attributes DOCC, ATLV, DAGE, ACTY, AQRY, ACLK,
AWHR, TADT, AWDY, THOM, TCIN, DGDR, TGMS, TREG, in this order. After selecting these
attributes, the utility does not increase significantly anymore. The entropy reduction levels off at
roughly 1.92 bits. Figure 6 underscores the diminishing-returns property of click entropy reduction.
Similarly, we generate a greedy ordering of the attributes, in order of minimum incremental
cost. Figure 7 presents the results of this experiment, using the maxprob cost metric. As expected,
the curve looks convex (apart from small variations due to the sampling process). The cost initially increases very slowly, and the growth increases as more attributes are selected. This behavior
empirically corroborates the supermodularity assumption for the cost metric.
Figure 9 compares the three cost metrics, as more and more attributes are selected. All three
metrics initially behave qualitatively similarly. However, the k-anonymity metric flattens out after
25 out of 31 attributes have been selected. This is expected, as eventually enough personal information is available in order to (almost) always reduce the candidate set of people to less than k = 100.
At this point, adding more attributes will not dramatically increase the cost anymore. However,
when trading off utility and privacy, one is interested in solutions with small cost, and in this critical
region, the cost function behaves supermodularly as well.
7.2 Calibrating the Tradeoff with Assessed Preferences
We now use the scalarization (3.1) to trade off utility and cost. For this optimization, we need to
choose the tradeoff parameter . Instead of committing to a single value of , we generate solutions
for increasing values of . For each such value, we use LLS to find an approximate solution, and
plot its utility and cost. Figure 8(a) shows the tradeoff curve obtained from this experiment. We can
see that this curve exhibits a prominent knee: For values 1    10, small increases of the utility

650

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Figure 9: Cost comparison according to greedy ordering

lead of big increases in cost, and vice versa. Hence, at the knee, we can achieve near-maximal utility
at near-minimum cost.
To integrate peoples preference in the analysis of the tradeoff, we perform the following calibration procedure. From the search log data, we determined how increasing the resolution of a
persons location increases the privacy cost. We vary the location granularity from region (coarsest)
to zip code (finest). For example, we compute the values Im ({zip code}), Im ({city}), etc. from
data. As explained in Section 6.2, we had asked the subjects to assess the sensitivity of sharing their
locations at different levels of precision. This approach also allows us put the identifiability cost
I(A) and sensitivity S(A) into the same units.
Similarly, we assessed the amount of improvement in search performance that would be required
in order to share attributes of a given sensitivity. We associate a number of bits with each level
of improvement: A speedup by a factor of x would require log2 x bits (i.e., doubling the search
performance would require 1 bit, etc.). We then concatenat the mappings from location granularity
to sensitivity, and from sensitivity to utility (bits), and compute the median number of bits required
for sharing each location granularity. Using this approach, we can put sensitivity S(A) and utility
U (A) into the same units. Thereby, we effectively use sensitivity as a common currency between
utility and cost. This procedure is (up to discretization) invariant of the particular scale (such as 1
to 5) used to assess sensitivity.
We perform a linear regression analysis to align the cost curve estimated from data with the
curve obtained from the survey. The least-squares alignment is presented in Figure 8(b), and obtained for a value of   5.12. Note that this value of  maps exactly into the sweet spot 1    10
of the tradeoff curve of Figure 8(b).
7.3 Optimizing the Utility-Privacy Tradeoff
Based on the calibration described above, our goal is to find a set of attributes A maximizing the
calibrated objective F (A) according to (3.1).
First, we use the greedy algorithm to obtain an ordering of the attributes, similarly to the cases
where we optimize utility and cost separately. Figure 10 presents the results of this experiment.

651

fiK RAUSE & H ORVITZ

Figure 10: Greedy solutions for calibrated objective

Figure 11: Comparison with heuristics

Instead of using the greedy algorithm, we can use LLS to approximately solve this optimization
problem. The algorithm terminates after two upward and downward passes, with solution AFRQ,
ATLV, AWDY, AQRY, ACLK, DAGE and TSPT. Note that the first element selected during the
initial greedy upward pass is DOCC (the occupation), but it is discarded in the first downward pass
again, since its individual sensitivity is quite high (c.f., Figure 2), and the additional information
provided over the remaining 6 attributes is not high enough to warrant its presence in the optimal
solution.
We also compared the optimized solution Aopt to the results of various heuristic procedures. For
example, we compared it to the candidate solution Adem where we select all demographic attributes
(all attributes starting with D); Atopic where we select all topic interest attributes (starting with T);
Asearch including all search statistics (ATLV, AWDY, AWHR, AFRQ); AIP , the entire IP address
or AIP 2 , the first two bytes of the IP address. Figure 11 presents the results of this comparison. The
optimized solution Aopt obtains the best score of 0.83, achieving a click entropy reduction of  1.4.

652

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Figure 12: Running times

The search statistics Asearch performs second best, with a score of 0.57, but achieving a drastically
lower utility of only 0.8. Demographic information Adem achieves higher utility of 0.95, but much
higher cost, and hence an even lower total score of 0.3. Perhaps surprisingly, the collection of topic
interests, Atopic results in negative total score of -1.73, achieving less utility than the optimized
solution. We believe that the reason for this is that knowledge of the exact topic interest profile
frequently suffices to uniquely identify a searcher. As expected, the IP address (even the first 2
bytes) are quite identifying in this data set, and hence has very high cost. This experiment shows that
the optimization problem is non-trivial, and the optimized solution outperforms heuristic choices.
We also measured the running time of the algorithms, on a standard desktop PC (3 GHz) using
a C#-implementation. Figure 12 presents the running times for the greedy and local search algorithms, both with and without the lazy evaluation trick. We note that the local search algorithm is
not much slower than the greedy algorithm; frequently, only a single upward and downward pass
are necessary, and only a small number of attributes have to be considered for elimination in the
downward pass. We also note that using lazy evaluations drastically speeds up the running time
from 522.1 minutes without to 225.7 with lazy evaluations. To obtain a tradeoff curve like the one
in Figure 8(b), the algorithm has to be run for each value of ; hence any improvement is important.
We also estimated the running time required for exhaustive search. Here, we considered only sets
of size at most 8 (assuming that the optimal solution is of similar size as the approximate solution).
Even this very optimistic estimate would require 50.1 years of computation time.

8. Related Work
This paper is an extended version of a paper that appeared at the 23rd Conference on Artificial Intelligence (AAAI) (Krause & Horvitz, 2008). The present version is significantly extended, including
several additional experimental results, a detailed discussion of our LLS algorithm as well as new

653

fiK RAUSE & H ORVITZ

Label
DGDR
DAGE
DOCC
DREG
DMTL
DCHD
AQRY
ACLK
AFRQ
AZIP
ACTY
ACRY
AWHR
AWDY
ATLV
TART
TADT
TBUS
TCMP
TGMS
THEA
THOM
TKID
TNWS
TREC
TREF
TREG
TSCI
TSHP
TCIN
TSOC
TSPT
TWLD

Type
Demographic
Demographic
Demographic
Demographic
Demographic
Demographic
Activity
Activity
Activity
Activity
Activity
Activity
Activity
Activity
Activity
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic
Topic

bits
1
2
3
2
1
1
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Description
Gender
Age group (<18, 18-50, >50)
Occupation (6 groups of related jobs)
Region (4 geographic regions)
Marital status (*)
Whether the searcher has children or not (*)
Performed same query before
Visited same website before
User performs at least 1 query per day on average
User performed queries from at least 2 different zip codes
User performed queries from at least 2 different cities
User performed queries from at least 2 different countries
Current query performed during working hours
Current query performed during workday / weekend
Top-level domain of query IP address (.com, .net, .org, .edu)
User previously visited arts related webpage
User previously visited webpage with adult content
User previously visited business related webpage
User previously visited compute related webpage
User previously visited games related webpage
User previously visited health related webpage
User previously visited home related webpage
User previously visited kids / teens related webpage
User previously visited news related webpage
User previously visited recreation related webpage
User previously visited reference related webpage
User previously visited webpage with regional content
User previously visited science related webpage
User previously visited shopping related webpage
User previously visited consumer information webpage
User previously visited society related webpage
User previously visited sports related webpage
User previously visited world related webpage

Table 1: 33 Attributes used in our experiments. Attributes marked (*) were not available in search
log data. Total number of bits = 38.

654

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

theoretical bounds (Section 4.3). We now review work related to the key concepts and methods
presented in this paper.
8.1 Personalized Search, Probabilistic Models for Search
The problem of personalized search has received a great deal of attention (c.f., Xu et al., 2007 for
an overview of recent work). Teevan et al. considered the use of local term frequencies for reranking web search results (Teevan et al., 2005). The personal data is kept private as the analysis
of personal information for re-ranking occurs entirely on the client. Recently, Xu et al. considered
the problem of privacy preserving web search. Based on local information, a user profile is built,
which is used for personalization. Two parameters called minDetail and expRatio are used to specify
preferences about privacy. While similar in spirit, their approach does not tradeoff cost and benefit
for sets of attributes as our approach does. Moreover, their approach does not consider the aspect of
discriminability across users.
There have been efforts to employ probabilistic models in web search, e.g., for predicting relevance. Examples include the techniques proposed by Downey et al. (2007). The methods proposed
in this paper can use any such probabilistic model, which allows to answer questions like how
much would my uncertainty decrease if I knew the searchers gender.
8.2 Value of Information in Probabilistic Models
Optimizing value of information has been a cornerstone of principled approaches to information
gathering (Howard, 1966; Lindley, 1956; Heckerman, Horvitz, & Middleton, 1993), and was popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984).
Several researchers (van der Gaag & Wessels, 1993; Cohn, Gharamani, & Jordan, 1996; Dittmer
& Jensen, 1997; Kapoor, Horvitz, & Basu, 2007) suggested myopic, i.e., greedy approaches for
selectively gathering observations. These algorithms typically do not have theoretical guarantees.
Heckerman et al. (1993) propose a method to compute the maximum expected utility for specific
sets of observations. They provide only large sample guarantees for the evaluation of a given sequence of observations, and use a heuristic without guarantees to select such sequences. Krause
and Guestrin (2009) develop dynamic programming based algorithms for finding optimal sets of
observations. However, their algorithms only apply to chain structured graphical models. Krause
and Guestrin (2005) show that under certain conditional independence assumptions, the information gain is a submodular function, an observation that we build on in this paper. To our knowledge,
this paper provides the first principled approach for efficiently, nonmyopically trading off value of
information and privacy cost.
8.3 Valuation of Private Information
The problem of estimating sensitivity or monetary value of private information has been studied extensively, e.g., in the economics literature. Huberman et al. (2005) propose a second-price auction
for estimating the sensitivity of demographic attributes. In a more application specific approach,
Wattal et al. study, whether the inclusion of names or personal product preferences can enhance
the effectiveness of email marketing (Wattal et al., 2005). Hann et al. (2002) study how economic
incentives affect individuals preferences with different privacy policies, quantifying the value of
disallowing the use of personal information. Kleinberg, Papadimitriou, and Raghavan (2001) quan-

655

fiK RAUSE & H ORVITZ

tify the value of private information using the Shapley value of a coalitional game. This approach
provides an alternative, theoretically well-motivated way of eliciting the subjective cost of sharing
a set of personal information, and could potentially be combined with our approach to optimize a
utility-privacy tradeoff.
8.4 Mathematical Notions of Privacy
The field of mathematical (or cryptographic) privacy has been studied extensively (c.f., Adam
& Wortmann, 1989 for a survey on early work). A pioneering result was the definition of kanonymity, and the development of algorithms for maintaining this indiscriminability guarantee
(Sweeney, 2002). Follow up work has led to other notions of indiscriminability, such as l-diversity
(Machanavajjhala et al., 2006) etc. These notions describe properties of databases in isolation,
and are sometimes called non-interactive (Dwork, 2006). Often, inference attacks using auxiliary
knowledge are problematic in this context. Lebanon et al. (2009) consider decision theoretic variants of k-anonymity and related notions. Their approach allows to quantify the risk in (partially)
releasing sanitized records. While they consider approaches to quantifying privacy cost, they do not
present efficient algorithms with guarantees for trading off utility and privacy as we do in this paper.
Another important class of cryptographic approaches to privacy consider protection of privacy
in the context of interactive analyses. In these approaches, a database contains an arbitrary amount
of private information, which is securely guarded. Access to this database is enabled via a limited
form of queries, which guarantee privacy. One prominent example of such techniques is the notion of differential privacy (Dwork, 2006). This notion quantifies the risk for each user incurred
by participating in a database. Intuitively, queries are only allowed, if the corresponding results
do not significantly depend on the presence or absence of individuals in the database. Blum et al.
(2008) show how differential privacy (and a stronger variant called distributional privacy) can be
achieved even in a non-interactive setting. However their approach is efficient only for a limited
class of queries that, e.g., does not allow to estimate click probabilities as considered in this paper.
Although these approaches provide crisp mathematical definitions of privacy, they are not designed
with a specific notion of utility in mind. Rather, a specific privacy requirement is formulated, and
analyses show the limits of data usage that are consistent with the requirement. We believe that
the utility-theoretic approach to privacy is complementary to the cryptographic definitions of privacy. While our approach can be used to develop a privacy-aware design, existing techniques such
as differential privacy can, e.g., be used to guard access to the private information. The utilitytheoretic analyses also allow for new scenarios, such as identifying the value of specific private data
for a specific context, and requesting this data in real time, for short-term usage. Such applications
bypass the assumption of long-term, large-scale storage of private data that are explored within
cryptographical analyses of privacy.
8.5 Utility-Privacy Tradeoff
Algorithmic approaches for optimizing the tradeoff have been considered in the area of privacypreserving data publishing, which focuses on anonymization techniques. In these settings, a constraint on privacy is typically specified (in terms of k-anonymity, Sweeney, 2002, or l-diversity,
Machanavajjhala et al., 2006) and then a recoding scheme is applied which maximizes a specified quality metric. Recent work involves the development of a greedy algorithm (LeFevre et al.,

656

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

2006) and branch and bound search (Hore & R. Jammalamadaka, 2007). To our knowledge, these
algorithms have no performance and approximation guarantees.
Our approach is different, in that we explicitly quantify both utility and cost in the same units,
and optimize a single scalar value of information problem. Moreover, we use an application specific
utility function, rather than a distortion metric. Furthermore, our algorithm is guaranteed to provide
near-optimal solutions to the optimization problem.

9. Discussion
The proposed methodology could be implemented in a variety of ways. The most critical design
choices are where and when the utility-privacy tradeoff is optimized. In the following, we shall
explore these parameters in the context of personalized search.
9.1 Location of private information
Private information can be either located at the client (i.e., the recipient of the service, e.g., the web
searcher), the server (i.e., the service provider, e.g., the search engine), or in the channel.
9.2 Client-side Private Information
In a client-side setting, private information is stored locally, and never shared with the network; this
configuration has been considered by Teevan et al. (2005) and Xu et al. (2007). With such an approach, private information is used to re-rank search results locally, and never transmitted across the
network, avoiding the risk of misuse or unintended release. However, the private information does
not (easily) migrate with the user across multiple machines, and privacy concerns are effectively
replaced by security concerns, requiring that the local machine not be compromised. The proposed
methodology could be useful in mitigating security concerns; client services might be optimized to
acquire and store only the most relevant information.
9.3 Server-side Private Information
On the server side, designs for privacy, especially with regard to the storage and usage of behavioral
data, is of critical importance to services and their users.
The methods described can be used to balance user sensitivity and service utility, and to address
such questions as which data should be logged, and which anonymization techniques should be used
(c.f., Adar, 2007 for a discussion of the difficulties and possible options in anonymizing web search
logs). Furthermore, specific personal (e.g., demographic) information could be voluntarily made
available by the web searcher to the web service in form of, e.g., a user profile. In both settings, the
proposed methodology could potentially be used to trade off privacy and utility.
9.4 Channel-side Private Information
We also foresee applications where private data is transmitted on a per-request basis. In this setting,
the proposed methodology can be used to design which private bits to transmit to maximize utility
while minimizing the privacy risk.

657

fiK RAUSE & H ORVITZ

9.5 A Priori vs. Dynamic Tradeoff Optimization
Another important design parameter is when the tradeoff between privacy and utility is made.
9.5.1 A PRIORI T RADEOFF O PTIMIZATION / P RIVACY-AWARE S ERVICE D ESIGN
One option is to apply the proposed methodology a priori, i.e., before the system is being used.
Here, the utility-privacy tradeoff would determine which information is being logged, collected in
local profiles, or which private bits are made part of the protocol used to transmit service requests.
This approach has the advantage that always the same private bits are used, and inference attacks
combining private information from different requests are not possible.
9.5.2 DYNAMIC O PTIMIZATION
As we mentioned earlier, systems can be endowed with the ability to make dynamic recommendations to users or take actions in accordance with user preferences in the course of online activities.
Dynamic variants of the methods we have presented can be used to identify the best subset of private information for sharing so as to optimize utility for a user. With web search, depending on
the query, different kinds of personal information may be most helpful for disambiguation. The
utility-theoretic approach to privacy could be used to determine needs in real time. Challenges with
implementing such a real-time, interactive approach, include grappling with the possibility that inference attacks can combine the bits from different requests. Anonymization procedures can be
designed to resist such challenges. Another challenge is computational efficiency, as solving an
optimization problem may be required for each service request. The main advantage of employing
a dynamic approach is that the average number of private bits (and hence the identifiability cost) per
request could be lower than in the a priori privacy-aware design. The a priori design requires a collection of private bits which help disambiguating all queries versus context-specific situations. This
interactive approach would additionally allow users to share private information on a per-request
basis. Such interactive decisions also require considerations of how preferences about tradeoffs
and private information are acquired from usersand will rely on the design of user interaction
models and interfaces to provide users with awareness about opportunities for enhancing the value
of services and controls for guiding the revelation of private information. Recent developments
in adaptive submodularity may provide the methods necessary to achieve such dynamic, adaptive
optimization of tradeoffs (Golovin & Krause, 2010).
9.6 Other Applications
The increasing cost property which characterizes the identifiability cost potentially arises in other
contexts as well. For example, for many algorithms, the computational cost scales superlinearly in
the number of dimensions / attributes considered. In such cases, the problem of trading off information (such as the click entropy reduction) and computation cost have the same combinatorial structure as the utility-privacy tradeoff addressed in this paper. Some of the algorithmic/computational
considerations presented in this paper, such as the lazy local search algorithm, or sample complexity
analyses, apply to such contexts as well.

658

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

10. Conclusions
We presented an approach for explicitly optimizing the utility-privacy tradeoff in personalized services such as web search. We showed that utility functions like click entropy reduction satisfy
submodularity, an intuitive diminishing-returns property. In contrast, privacy concerns show supermodularity; the more private information we accrue, the faster sensitivity and the risk of identifiability grow. Based on the submodular utility and supermodular cost functions, we demonstrated how
we can efficiently find a provably near-optimal utility-privacy tradeoff. We evaluated our methodology on real-world web search data. We demonstrated how the quantitative tradeoff can be calibrated
according to personal preferences, obtained from user study with over 1,400 participants. Overall,
we found that significant personalization can be achieved using only a small amount of information
about users. We believe that the principles and methods employed in the utility-theoretic analysis
of tradeoffs for web search have applicability to the personalization of a broad variety of online
services. The results underscore the value of taking a decision-theoretic approach to privacy, where
we seek to jointly understand the utility of personalization that can be achieved via access to information about users, and the preferences of users about the costs and benefits of selectively sharing
their personal data with online services.
Acknowledgments
Andreas Krause was an intern at Microsoft Research while this work was performed. We would like
to thank the searchers who provided usage data for use in this research, the participants in our survey
about privacy preferences, and the anonymous referees for their helpful comments and suggestions.

Appendix A. Proofs
Proof of Theorem 2.1. Let n = |cV |. We prove the result for a choice of I(A), where I(A) = 0 if
|A| < n/2 and I(A) = 2M (|A|n/2) otherwise, where M = U (V) is the maximum achievable
utility. In this case, F1 (A) = U (A)  0 if |A|  n/2, and F1 (A) < 0 if |A| > n/2. Hence,
argmax F1 (A) = argmax U (A).
A

A:|A|n/2

However, Krause and Guestrin (2005) show that if there were a polynomial time algorithm which is
guaranteed to find a set A0 such that U (A0 )  (1  1/e) maxA:|A|n/2 U (A), then P = N P .
Proof of Theorem 3.2. Let Y = (A, B) (i.e., V is partitioned into A and B). Then,
X
X
Im (A) =
P (a) max P (b | a) =
P (a) max P (b)
a

b

= max P (b) = max
b

b

a
b

Y

P (bi ) =

i

Y
i

max P (bi ) =
bi

Y

wi

i

where wi = maxbi P (bi ). Now, Im (A) is nondecreasing in A. Furthermore, for subsets A, A0  V
and B = A  A0 , it holds that Im (B) = Im (A) for   1. Furthermore, if V  V \ B and
w = maxv P (V = v), then Im (B  {V }) = w1 Im (B) and Im (A  {V }) = w1 Im (A). Hence,
Im (B  {V })  Im (B)
(w1  1)
=
=   1,
Im (A  {V })  Im (A)
w1  1
659

fiK RAUSE & H ORVITZ

which proves the supermodularity of Im (A).
Proof of Theorem 4.4. Additive error carries through Lemma 3.3. and subsequently through the
proof of Theorem 3.4. of Feige et al. (2007). In order to guarantee the confidence 1  , we apply
the union bound. The sample complexity then follows from Lemma 4.2 and Lemma 4.3.
Proof of Theorem 4.5. Let C be an optimal solution. Since U is nondecreasing,
F (C) = U (C)  C(C)  U (A0  C)  C(C)
X
 U (A) +
[U (A  {V })  U (A)]  C(C)
V C

X

 U (A) +

[U (A  {V })  U (A)  C({V })]

V C

= U (A0 ) +

X

V  U (A0 ) +

V C

X

V .

V B

References
Adam, N. R., & Wortmann, J. C. (1989). Security-control methods for statistical databases: A
comparative study. ACM Computing Surveys, 21(4), 515556.
Adar, E. (2007). User 4xxxxx9: Anonymizing query logs. In Query Logs Workshop, World Wide
Web Conference.
Blum, A., Ligett, K., & Roth, A. (2008). A learning theory approach to non-interactive database
privacy. In Symposium on the Theory of Computing.
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
Chen, B., LeFevre, K., & Ramakrishnan, R. (2007). Privacy skyline: Privacy with multidimensional
adversarial knowledge. In International Conference on Very Large Data Bases (VLDB).
Cohn, D. A., Gharamani, Z., & Jordan, M. I. (1996). Active learning with statistical models. Journal
of Artificial Intelligence Research, 4, 129145.
Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory. Wiley Interscience.
Dittmer, S., & Jensen, F. (1997). Myopic value of information in influence diagrams. In Conference
on Uncertainty in Artificial Intelligence (UAI), pp. 142149, San Francisco.
Dou, Z., Song, R., & Wen, J.-R. (2007). A large-scale evaluation and analysis of personalized search
strategies. In World Wide Web Conference (WWW).
Downey, D., Dumais, S., & Horvitz, E. (2007). Models of searching and browsing: Languages,
studies, and applications. In International Joint Conference on Artificial Intelligence (IJCAI).
Dwork, C. (2006). Differential privacy. In International Colloquium on Automata, Languages and
Programming (ICALP).
Feige, U., Mirrokni, V., & Vondrak, J. (2007). Maximizing non-monotone submodular functions.
In IEEE Symposium on Foundations of Computer Science (FOCS).
660

fiA U TILITY-T HEORETIC A PPROACH TO P RIVACY IN O NLINE S ERVICES

Goldengorin, B., Sierksma, G., Tijssen, G. A., & Tso, M. (1999). The data-correcting algorithm for
the minimization of supermodular functions. Management Science, 45(11), 15391551.
Golovin, D., & Krause, A. (2010). Adaptive submodularity: A new approach to active learning and
stochastic optimization. In International Conference on Learning Theory (COLT).
Hann, I., Hui, K., Lee, T., & Png, I. (2002). Online-information privacy: Measuring the cost-benefit
tradeoff. In International Conference on Information Systems.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). An approximate nonmyopic computation for
value of information. IEEE Trans. Pattern Analysis and Machine Intelligence, 15, 292298.
Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of
the American Statistical Association, 58(301), 1330.
Hore, B., & R. Jammalamadaka, S. M. (2007). Flexible anonymization for privacy preserving data
publishing: A systematic search based approach. In SIAM Conference on Data Mining (SDM).
Horvitz, E. (2006). Machine learning, reasoning, and intelligence in daily life: Directions and
challenges. Tech. rep. TR-2006-185, Microsoft Research.
Howard, R. A. (1966). Information value theory. In IEEE Transactions on Systems Science and
Cybernetics (SSC-2).
Howard, R. A., & Matheson, J. (1984). Readings on the Principles and Applications of Decision
Analysis II, chap. Influence Diagrams, pp. 719762. Strategic Decision Group, Menlo Park.
Reprinted 2005 in Decision Analysis 2(3) 127-143.
Huberman, B. A., Adar, E., & Fine, L. R. (2005). Valuating privacy. IEEE Security & Privacy, 3(5),
2225.
Kapoor, A., Horvitz, E., & Basu, S. (2007). Selective supervision: Guiding supervised learning with
decision-theoretic active learning. In International Joint Conference on Artificial Intelligence
(IJCAI).
Kleinberg, J., Papadimitriou, C. H., & Raghavan, P. (2001). On the value of private information.
TARK: Theoretical Aspects of Reasoning about Knowledge, 8.
Krause, A., & Guestrin, C. (2009). Optimal value of information in graphical models. Journal of
Artificial Intelligence Research, 35, 557591.
Krause, A., & Horvitz, E. (2008). A utility-theoretic approach to privacy and personalization. In
Proc. 23rd Conference on Artificial Intelligence (AAAI), Special Track on AI & the Web.
Krause, A., & Guestrin, C. (2005). Near-optimal nonmyopic value of information in graphical
models. In Conference on Uncertainty in Artificial Intelligence (UAI).
Lebanon, G., Scannapieco, M., Fouad, M. R., & Bertino, E. (2009). Beyond k-anonymity: A decision theoretic framework for assessing privacy risk. Transactions on Data Privacy, 2(3),
153183.
LeFevre, K., DeWitt, D., & Ramakrishnan, R. (2006). Mondrian multidimensional k-anonymity. In
IEEE International Conference on Data Engineering (ICDE).
Lindley, D. V. (1956). On a measure of the information provided by an experiment. Annals of
Mathematical Statistics, 27, 9861005.

661

fiK RAUSE & H ORVITZ

Machanavajjhala, A., Kifer, D., Gehrke, J., & Venkitasubramaniam, M. (2006). L-diversity: Privacy
beyond k-anonymity. In IEEE International Conference on Data Engineering (ICDE).
Nemhauser, G., Wolsey, L., & Fisher, M. (1978). An analysis of the approximations for maximizing
submodular set functions. Mathematical Programming, 14, 265294.
Nemhauser, G. L., & Wolsey, L. A. (1981). Studies on Graphs and Discrete Programming, chap.
Maximizing Submodular Set Functions: Formulations and Analysis of Algorithms, pp. 279
301. North-Holland.
Olson, J. S., Grudin, J., & Horvitz, E. (2005). A study of preferences for sharing and privacy. In
ACM Conference on Human Factors in Computing Systems (CHI).
Robertazzi, T. G., & Schwartz, S. C. (1989). An accelerated sequential algorithm for producing
D-optimal designs. SIAM Journal of Scientific and Statistical Computing, 10(2), 341358.
Sugiyama, K., Hatano, K., & Ikoma, T. (2004). Adaptive web search based on user profile constructed without any effort from users. In World Wide Web Conference (WWW).
Sweeney, L. (2002). k-anonymity: a model for protecting privacy. International Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5), 557570.
Teevan, J., Dumais, S. T., & Horvitz, E. (2005). Personalizing search via automated analysis of
interests and activities. In International ACM SIGIR Conference.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering for diagnostic belief networks.
AISB Quart., 86, 2334.
Wattal, S., Telang, R., Mukhopadhyay, T., & Boatwright, P. (2005). Examining the personalizationprivacy tradeoff an empirical investigation with email advertisements. Management Science.
Xu, Y., Zhang, B., & Wang, K. (2007). Privacy-enhancing personalized web search. In World Wide
Web Conference (WWW).

662

fiJournal of Artificial Intelligence Research 39 (2010) 745774

Submitted 03/10; published 12/10

Intrusion Detection using Continuous Time Bayesian Networks
Jing Xu
Christian R. Shelton

JINGXU @ CS . UCR . EDU
CSHELTON @ CS . UCR . EDU

Department of Computer Science and Engineering
University of California, Riverside
Riverside, CA 92521, USA

Abstract
Intrusion detection systems (IDSs) fall into two high-level categories: network-based systems
(NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls.
In this work, we present a general technique for both systems. We use anomaly detection, which
identifies patterns not conforming to a historic norm. In both types of systems, the rates of change
vary dramatically over time (due to burstiness) and over components (due to service difference).
To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid
specifying a fixed update interval common to discrete-time models. We build generative models
from the normal training data, and abnormal behaviors are flagged based on their likelihood under
this norm. For NIDS, we construct a hierarchical CTBN model for the network packet traces
and use Rao-Blackwellized particle filtering to learn the parameters. We illustrate the power of
our method through experiments on detecting real worms and identifying hosts on two publicly
available network traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel
learning method to deal with the finite resolution of system log file time stamps, without losing the
benefits of our continuous time model. We demonstrate the method by detecting intrusions in the
DARPA 1998 BSM dataset.

1. Introduction
Misuse or abuse of computer systems is a critical issue for system administrators. Our goal is to
detect these attacks that attempt to compromise the performance quality of a particular host machine.
It is time-consuming and error-prone to acquire labeled data that contains both good and bad
behaviors from which to build a classifier. Additionally, the frequency with which attacks are developed can make maintaining a database of all previously seen attacks inefficient or even infeasible.
Anomaly detection can identify new attacks even if the attack type was unknown beforehand. Unsupervised learning allows the anomaly detector to adapt to changing environments, thereby extending
its domain of usefulness. By modeling normal behavior from historic clean data, we can identify
abnormal activity without a direct prior model of the attack by simply comparing its deviation from
the learned norm.
In a network-based intrusion detection system (NIDS), the network packet traces are monitored.
Network traffic traces collect information from a networks data stream and provide an external
view of the network behavior. In a host-based intrusion detection system (HIDS), the internal state
of a computing system is analyzed. System call logs are a convenient way of monitoring executing
programs behavior through their operating system calls.
Both systems are composed of activities that happen at dramatically different time granularity.
Users alternate between busily using their computer and resting. During the busy period, a burst of
action may cause a peak of network traffic flow or operating system usage. However, during the
c
2010
AI Access Foundation. All rights reserved.

fiX U & S HELTON

resting period, the computer just maintains its regular running pattern, and network or system activities are much less intense, e.g. automatically checking email every few minutes. Even within each
of these global modes there are variations. Therefore, a dynamic model that requires discretizing
the time is not efficient. We develop intrusion detection techniques using continuous time Bayesian
networks (CTBNs) (Nodelman, Shelton, & Koller, 2002) for both data types. Although the two
data are of completely different formats and semantic meaning, we demonstrate the flexibility of a
continuous time generative model (such as a CTBN) to describe either.
Our first effort is to detect anomalies from network traffic traces (NIDS). Abnormal traffic must
differ in some way from the normal traffic patterns. While this difference may be very subtle and
difficult to detect, the more subtle the attack, the longer the attack will take and the more it will
stress the patience of the attacker. Looking at summarized information like flow statistics is not
helpful, especially for stealthy worms which can mingle well with normal traffic by sacrificing their
spreading speed and scale. We, therefore, feel that looking for abnormalities in the detailed network
traffic flow level is a utile method for finding attacks. A network flow for a given host machine
is a sequence of continuous-time asynchronous events. Furthermore, these events form a complex
structured system, where statistical dependencies relate network activities like packet emissions and
connections. We employ CTBNs to reason about these structured stochastic network processes.
Our CTBN model contains a number of observed network events (packet emissions and concurrent port connections changes). To allow our model to be more descriptive, we also add latent
variables that tie the activity variables together. Exact inference in this method is no longer feasible.
Therefore, we use Rao-Blackwellized particle filtering (RBPF) to estimate the parameters.
Our second effort is to detect intrusions using system call logs (HIDS). A system log file contains an ordered list of calls made to a computers operating system by an executing program. We
focus on analyzing the ordering and the context of the sequence, rather than simply counting the
overall statistics. A CTBN is a natural way of modeling such sequential data. Because of the finite
resolution of computer clock, all the system calls issued within a clock tick are assigned a same
time stamp. Therefore the data stream consists of long periods of time with no activity, followed by
sequences of calls in which the order is correctly recorded, but the exact timing information is lost.
This poses a new challenge for CTBN reasoning. We present here a learning method for such type
of data without resorting to time discretization.
We validate our NIDS technique on the MAWI dataset and the LBNL dataset, and our HIDS
technique on the DARPA 1998 BSM dataset. Both applications give good results when compared
with other method.
In Section 2 we discuss the related work in intrusion detection. In Section 3 we review continuoustime Markov processes and continuous time Bayesian networks. In Section 4 we describe the CTBN
model and the RBPF inference algorithms for the NIDS problem. In Section 5 we describe the
CTBN model and the parameter estimation algorithm for HIDS, including how to deal with imprecise timing measurements. In Section 6 we show our experimental results for both of the applications.

2. Related Work
Much of the previous work in intrusion detection focuses on one area only  either detecting the
network traffic or mining the system call logs. The work of Eskin, Arnold, Prerau, Portnoy, and
Stolfo (2002) is similar to our approach in that they apply their method to both of these kinds of
746

fiI NTRUSION D ETECTION USING CTBN S

data. They map data elements to a feature space and detect anomalies by determining which points
lie in sparse regions using cluster-based estimation, K-nearest neighbors and one-class SVM. They
use a data-dependent normalization feature map for network traffic data and a spectrum kernel for
system call traces.
2.1 NIDS
For network traffic data, we build upon our previous work (Xu & Shelton, 2008). There we made
the assumption that network activities are independent across different ports. This allowed us to
factorize the model into port-level submodels and standard exact inference techniques could be used
for parameter learning. In this paper, we remove this restriction. There is no application-specific
reason that traffic should be independent by ports. By tying the traffic together, our model describes
more complicated structural dependencies among variables. We derive a Rao-Blackwellized particle
filtering algorithm to estimate the parameters for our model. Our work also differs in that we are
not only interested in the intrusion detection problem, but host identity recognition as well.
As a signature-based detection algorithm, we share many of the assumptions of Karagiannis,
Papagiannaki, and Faloutsos (2005). In particular, we also assume that we do not have access to
the internals of the machines on the networks, which rules out methods like those of Malan and
Smith (2005), Cha (2005), Qin and Lee (2004), and Eskin et al. (2002). However, we differ in
that our approach does not rely on preset values, require human intervention and interpretation, nor
assume that we have access to network-wide traffic information. Network-wide data and human
intervention have advantages, but they can also lead to difficulties (data collation in the face of an
attack and increased human effort), so we chose to leave them out of our solution.
Many learning, or adaptive, methods have been proposed for network data. Some of these 
for example, those of Zuev and Moore (2005) and Soule, Salamatian, Taft, Emilion, and Papagiannali (2004)  approach the problem as a classification task which requires labeled data. Dewaele,
Fukuda, and Borgnat (2007) profile the statistical characteristics of anomalies by using random projection techniques (sketches) to reduce the data dimensionality and a multi-resolution non-Gaussian
marginal distribution to extract anomalies at different aggregation levels. The goal of such papers
is usually not to detect attacks but rather to classify non-attacks by traffic type; if applied to attack
detection, they would risk missing new types of attacks. Furthermore, they frequently treat each
network activity separately, instead of considering their temporal context.
Lakhina, Crovella, and Diot (2005) has a nice summary of adaptive (or statistical) methods that
look at anomaly detection (instead of classification). They use an entropy-based method for the
entire network traffic. Many of the other methods, such as that of Ye, Emran, Chen, and Vilbert
(2002), use either statistical tests or subspace methods that assume the features of the connections
or packets are distributed normally. Rieck and Laskov (2007) model the language features like
n-grams and words from connection payloads. Xu, Zhang, and Bhattacharyya (2005) also use
unsupervised methods, but they concentrate on clustering traffic across a whole network. Similarly,
Soule, Salamatian, and Taft (2005) build an anomaly detector based on Markov models, but it is for
the network traffic patterns as a whole and does not function at the host level.
The work of Soule et al. (2004) is very similar in statistical flavor to our work. They also
fit a distribution (in their case, a histogram modeled as a Dirichlet distribution) to network data.
However, they model flow-level statistics, whereas we work at the level of individual connections.
Additionally, they are attempting network-wide clustering of flows instead of anomaly detection.
747

fiX U & S HELTON

The work of Moore and Zuev (2005), like our approach, models traffic with graphical models, in
particular, Naive Bayes networks. But their goal is to categorize network traffic instead of detecting
attacks. Kruegel, Mutz, Robertson, and Valeur (2003) present a Bayesian approach to the detecting
problem as an event classification task while we only care about whether the host is under attack
during an interval.
The work of Lazarevic, Ertoz, Kumar, Ozgur, and Srivastava (2003) is also similar to our work.
It is one of the few papers to attempt to find attacks at the host level. They employ nearest neighbor,
a Mahalanobis distance approach, and a density-based local outliers method, each using 23 features
of the connections. Although their methods make the standard i.i.d. assumption about the data
(and therefore miss the temporal context of the connection) and use 23 features (compared to our
few features), we compare our results to theirs in Section 6, as the closest prior work. Agosta,
Duik-Wasser, Chandrashekar, and Livadas (2007) present an adaptive detector whose threshold is
time-varying. It is similar to our work in that they also rely on model-based algorithms. But they
employ the host internal states like CPU loads which are not available to us.
While there has been a great variety of previous work, our work is novel in that it detects
anomalies at the host level using only the timing features of network activities. We do not consider
each connection (or packet) in isolation, but rather in a complex context. We capture the statistical
dynamic dependencies between packets and connections to find sequences of network traffic that
are anomalous as a group.
2.2 HIDS
Previous work on detecting intrusions in system call logs can be roughly grouped into two categories: sequence-based and feature-based. Sequence-based methods focus on the sequential order
of the events while feature-based methods treat system calls as independent data elements. Our
method belongs to the former category since we use a CTBN to model the dynamics of the sequences.
Time-delay embedding (tide) and sequence time-delay embedding (stide) are two examples of
sequence based methods (Forrest, A.Hofmeyr, Somayaji, & A.Longstaff, 1996; A.Hofmeyr, Forrest, & Somayaji, 1998). They generalize the data by building a database storing previously seen
system call sub-sequences, and test by looking up subsequences in the database. These methods are
straightforward and often achieve good results. We compare with them in our experiments. Tandon
and Chan (2005) look at a richer set of attributes like return value and arguments associated with a
system call while we only make use of the system call names.
Feature based methods like those of Hu, Liao, and Vemuri (2003) use the same dataset we use,
the DARPA 1998 BSM dataset, but their training data is noisy and they try to find a classification
hyperplane using robust support vector machines (RSVMs) to separate normal system call profiles
from intrusive ones. Eskin (2000) also works on noisy data. They make the assumption that their
training data contains a large portion of normal elements and few anomalies. They present a mixture
of distribution over normal and abnormal data and calculate the likelihood change if a data point is
moved from normal part to abnormal part to get the optimum data partition.
Yeung and Ding (2002) try to use both techniques. They provide both dynamic and static behavioral models for system call data. For the dynamic method, a hidden Markov model (HMM)
is used to model the normal system events and a likelihood is calculated for each testing sequence
and compared against a certain threshold. Our work for the system call traces problem is very close
748

fiI NTRUSION D ETECTION USING CTBN S

to their framework since we also build a dynamic model for the sequential data and compute the
likelihood of a testing example as a score. But we are different in that our CTBN models the continuous time dynamics rather than time-sliced behaviors. For the static method, they represent the
normal behavior by a command occurrence frequency distribution and measure the distance from
the testing example to this norm by cross entropy. The dataset they use is KDD archive dataset.
2.3 Other Work
Simma et al. (2008) also use a continuous-time model to reason about network traffic. They apply
their method to find dependences in exterprise-level services. Their model is non-Markovian, but
also deals with network events as the basic observational unit.
To estimate the parameters of the large network we build for the network traffic data, we use
Rao-Blackwellized particle filters (RBPFs). Doucet, de Freitas, Murphy, and Russel (2000) propose
a RBPF algorithm for dynamic Bayesian networks that works in discrete time fashion by exploiting
the structure of the DBN. Ng, Pfeffer, and Dearden (2005) extend the RBPF to continuous time dynamic systems and apply the method to the K-9 experimental Mars rover at NASA Ames Research
Center. Their model is a hybrid system containing both discrete and continuous variables. They
use particle filters for the discrete variables and unscented filters for the continuous variables. Our
work are similar to theirs in that we apply a RBPF to a CTBN. But our model only contains discrete
variables and our evidence is over continuous time (as opposed to only snapshots of the system
state).

3. Continuous Time Bayesian Networks
We begin by briefly reviewing the definition of Markov processes and continuous time Bayesian
networks (CTBNs).
3.1 Homogeneous Markov Process
A finite-state, continuous-time, homogeneous Markov process Xt is described by an initial distribution PX0 and, given a state space V al(X) = {x1 , ..., xn }, an n  n matrix of transition intensities:



QX = 


qx1
q x2 x1
..
.

q x1 x2
qx2
..
.

q xn x1

q xn x2

. . . q x1 xn
. . . q x2 xn
..
..
.
.
. . . qxn




.


P
qxi xj is the intensity (or rate) of transition from state xi to state xj and qxi = j6=i qxi xj .
The transient behavior of Xt can be described as follows. Variable X stays in state x for time
exponentially distributed with parameter qx . The probability density function f for Xt remaining
at x for duration t is fx (q, t) = qx exp(qx t) for t  0. The expected time to the next transition
given the state is currently x is 1/qx . Upon transitioning, X shifts to state x0 with probability
xx0 = qxx0 /qx . Note that given qx , xx0 and qxx0 are iosmorphic. We will sometime gives formulae
in terms of xx0 where it simplifies the expression.
The distribution over the state of the process X at some future time t, Px (t), can be computed
directly from QX . If PX0 is the distribution over X at time 0 (represented as a vector), then, letting
749

fiX U & S HELTON

exp be the matrix exponential,
PX (t) = PX0 exp(QX  t) .
3.2 Complete Data
Complete data for an HMP are represented by a set of trajectories D = {1 , ...n }. Each trajectory
i is a complete set of state transitions: d = {(xd , td , x0d )}, meaning that X stayed in state xd for a
duration of td , and then transitioned to state x0d . Therefore we know the exact state of the variable
X at any time 0  t  T .
3.3 Sufficient Statistics and Likelihood
Given an HMP and its full data D, the likelihood of a single state transition d = {(xd , td , x0d )}  D
is
LX (q,  : d) = (qxd exp(qxd td ))(xd x0d ) .
The likelihood function for D can be decomposed by transition:
Y
Y
LX (q,  : D) = (
LX (q : d))(
LX ( : d))
dD

dD

Y
Y Y M [x,x0 ]
= ( qxM [x] exp(qx T [x]))(
xx0
).
x x0 6=x

x

If we take the log of the above function, we get the log likelihood:
lX (q,  : D) = lX (q : D) + lX ( : D)
X
X
=
(M [x] ln(qx )  qx T [x] +
M [x, x0 ] ln(xx0 )) .
x0 6=x

x

M [x, x0 ]

Here
and T [x] are the sufficient statistics of the HMP
model. M [x, x0 ] is the number
P
of times X transitions from the state x to x0 . We denote M [x] = x0 M [x, x0 ], the total number of
times the system leaves state x. T [x] is the total duration that X stays in the state x.
3.4 Learning from Complete Data
To estimate the parameters of the transition intensity matrix Q, we maximize the above log likelihood function. This yields the maximum likelihood estimates:
qx =

M [x]
,
T [x]

xx0 =

M [x, x0 ]
.
M [x]

3.5 Incomplete Data
Incomplete data from an HMP are composed partially observed trajectories D = {1 , ...n }. Each
trajectory i consists of a set of d = {(Sd , td , dt)} observations, where Sd is a subsystem (a
nonempty subset of the states of X) of the process. Each of the triplets specifies an interval
evidence. It states that the variable X is in the subsystem Sd from time td to time td + dt. Some of
the observations may be duration-free. i.e., we only observe X  Sd at time t, but do not know how
long it stayed there. This is called a point evidence and can be generalized using the same triplet
notation described above by setting the duration to be 0. For a partially observed trajectory, we only
observe sequences of subsystems, and do not observe the state transitions within the subsystems.
750

fiI NTRUSION D ETECTION USING CTBN S

3.6 Expected Sufficient Statistics and Expected Likelihood
We can consider possible completions of a partially observed trajectory that specify the transitions
that are consistent with the partial trajectory. By combining the partial trajectory and its completion,
we get a full trajectory. We define D+ = {1+ , ..., n+ } to be completions of all the partial trajectories
in D. Given a model, we have a distriubtion over D+ , given D.
For data D+ , the expected sufficient statistics with respect to the probability density over possible completions of the data are T [x], M [x, x0 ] and M [x]. The expected log likelihood is
E[lX (q,  : D+ )] = E[lX (q : D+ )] + E[lX ( : D+ )]
X
X
(M [x] ln(qx )  qx T [x] +
=
M [x, x0 ] ln(xx0 )) .
x0 6=x

x

3.7 Learning from Incomplete Data
The expectation maximization (EM) algorithm can be used to find a local maximum of the likelihood
from partial trajectory. The EM algorithm iterates over the following E step and M step until the
convergence on the derived likelihood function.
E step: Given the current HMP parameters, compute the expected sufficient statistics: T [x],
M [x, x0 ] and M [x] for the data set D. This is the most complex part of the algorithm. We give
further details below.
M step: From the computed expected sufficient statistics, update the new model parameters for
the next EM iteration:
M [x, x0 ]
M [x]
, xx0 =
.
qx =
T [x]
M [x]
Now we show how to calculate the expected sufficient statistics using the forward-backward
message passing method.
A trajectory   D can be devided into N intervals where each of the interval is separated
by adjacent event changes. Assume the trajectory spans the time interval [0, T ), and let  [v, w] be
the observed evidence between time v and w, including events on the time stamp v and w, and let
 (v, w) be the same set of evidence but excluding v and w. Let S be the subsystem the states are
restricted on this interval.
We define
t = P (Xt ,  [0, t]), t = P ( [t, T ] | Xt )
to be vectors (indexed by possible assignments to Xt ). Similarly, we define the corresponding
distribution that excludes certain point evidence as follows.
t = P (Xt ,  [0, t)),

t+ = P ( (t, T ] | Xt ) .

Denote j to be a vector of all 0s except for its j-th position being 1, and denote ij be a matrix
of all 0s except that the element on i-th row and j-th column is 1.
We are now able to show the derived expected sufficient statistics. For time,
Z T
E[T [x]] =
P (Xt |  [0, T ])x dt
0
N
1 Z ti+1
X
1
=
P (Xt ,  [0, T ])x dt .
P ( [0, T ])
ti
i=0

751

fiX U & S HELTON

The constant fraction at the beginning of the last line serves to make the total expected time over all
j sum to  . The integral on each interval can be further expressed as
Z

w

Z

w

v exp(QS (t  v))xx exp(QS (w  t))w dt ,

P (Xt ,  [0, T ])x dt =
v

v

where QS is the same as QX except all elements that correspond to transitions to or from S are set
to 0.
The equation for expected transition counts can similarly be defined:
N 1

E[M [x, x0 ]] =

X
qx,x0
+
[
ti x,x0 t+i
P ( [0, T ])
i=1
N
1 Z ti+1
X
+
ti exp(QS (t  ti ))x,x0 exp(QS (ti+1  t))ti+1 dt] .
i=0

ti

The integrals appearing in E[T ] and E[M ] can be computed via a standard ODE solver, like
the Runge-Kutta method (Press, Teukolsky, Vetterling, & Flannery, 1992). Such a method uses an
adaptive step size to move quickly through times of few expected changes and more slowly through
times of rapid transitions.
Now the only remaining problem is to calculate  and . Let QSS0 be the transitioning intensity
matrix of the HMP from one subsystem S to another S 0 . This matrix is the same as QX , but only
elements corresponding to transitions from S to S 0 are non-zero.
ti = ti1 exp(QSi1 (ti  ti1 )) ,
ti = ti QSi1 Si ,
ti = exp(QSi (ti+1  ti ))ti+1 ,
ti = QSi1 Si ti .
During this forward-backward calculation, it is also trivial to answer queries such as
P (Xt = x |  [0, T ]) =

1
 xx t .
P ( ) t

3.8 Continuous Time Bayesian Networks
While HMPs are good for modeling many dynamic systems, they have their limitations when the
systems have multiple components because the state space grows exponentially in the number of
variables. An HMP does not model the variable independencies and therefore it has to use a unified
state X to represent the joint behavior of all the involving components in the system. In the this
section, we show how a continuous time Bayesian network can be used to address this issue.
Nodelman et al. (2002) extend the theory of HMPs and present continuous time Bayesian networks (CTBNs), which model the joint dynamics of several local variables by allowing the transition
model of each local variable X to be a Markov process whose parametrization depends on some
subset of other variables U .
752

fiI NTRUSION D ETECTION USING CTBN S

3.9 Definition
We first give an definition of an inhomogeneous Markov process called a conditional Markov process. It is a critical concept for us to formally introduce the CTBN framework.
Definition 1 (Nodelman, Shelton, & Koller, 2003) A conditional Markov process X is an inhomogeneous Markov process whose intensity matrix varies as a function of the current values of a set
of discrete conditioning variables U . It is parametrized using a conditional intensity matrix (CIM)
QX|U  a set of homogeneous intensity matrices QX|u , one for each instantiation of values u to U .
We call U the parents of X. When the set of U is empty, the CIM is simply a standard intensity
matrix.
CIMs provide a way to model the temporal behavior of one variable conditioned on some other
variables. By putting these local models together, we have a joint structured model  a continuous
time Bayesian network.
Definition 2 (Nodelman et al., 2003) A continuous time Bayesian network N over a set of stochastic processes X consists of two components: an initial distribution PX0 , specified as a Bayesian
network B over a set of random variables X, and a continuous transition model, specified using a
directed (possibly cyclic) graph G whose nodes are X  X; UX denotes the parents of X in G. Each
variable X  X is associated with a conditional intensity matrix, QX|UX .
The dynamics of a CTBN are quantitatively defined by a graph. The instantaneous evolution of
a variable depends only on the current value of its parents in the graph. The quantitative description
of a variables dynamics is given by a set of intensity matrices, one for each value of its parents.
That means the transition behavior of the variable is controlled by the current values of its parents.
The standard notion of d-separation from Bayesian networks carries over to CTBNs. Because
graphs are cyclic and variables represent processes (not single random variables), the implications
are a little different. A variable (process) is still independent of its non-descendants given its parents,
and it is still independent of everything given its Markov blanket (any variable that is either a parent,
a child, or a parent of a child). Cycles can cause parents to also be children, but provided they are
considered as both, the above definitions still hold. More importantly, the notion of given works
only if the full trajectory for the variable in question is known. Therefore, X and its grandchildren
are not independent given Xs childrens values at a single instant. Rather, they are only independent
given Xs childrens full trajectories from time 0 until the last time of interest.
If we amalgamate all the variables in the CTBN together, we get a single homogeneous Markov
process over the joint state space. In the joint state intensity matrix, a rate of 0 is assigned to any
transition that involves changing more than one variables value at the exact same time. All other
intensities can be found by looking up the value in the corresponding conditional intensity matrix
for the variable that changes. The diagonal elements are the negative row sums.
Forward sampling can be done quickly in a CTBN without generating the full joint intensity
matrix. We keep track of the next event time for each variable (sampled from the relevant exponential distribution given the current values of itself and its parent). We then select the earliest
event time and change that variable (sampling from the multinomial distribution implied by the row
of that variables relevant intensity matrix). The next event time for the variable that just changed
and all of its children must be resampled, but no other variables time must be resampled due to the
memoriless property of the exponential distribution. In this way a sequence of events (a trajectory)
can be sampled.
753

fiX U & S HELTON

3.10 Learning
In the context of CTBNs, the model parameters consist of the CTBN structure G, the initial distribution P0 parameterized by a regular Bayesian network, and the conditional intensity matrices (CIMs)
of each variable in the network. In this section, we assume the CTBN structure is known to us, so
we only focus on the parameter learning. We also assume the model is irreducible. So the initial
distribution P0 becomes less important in the context of CTBN inference and learning, especially
when the time range becomes significantly large. Therefore, parameter learning in our context is
to estimate the conditional intensity matrices QXi |Ui for each variable Xi , where Ui is the set of
parent variables of Xi .
3.10.1 L EARNING FROM C OMPLETE DATA
Nodelman et al. (2003) presented an efficient way to learn a CTBN model from fully observed
trajectories. With complete data, we know full instantiations to all the variables for the whole
trajectory. So we know which CIM is governing the transition dynamics of each variable at any
time. The sufficient statistics are M [x, x0 |u]  the number of times X transitions from the state x
to x0 given its parent instantiation u  and T [x|u] P
the total duration that X stays in the state x
given its parent instantiation u. We denote M [x|u] = x0 M [x, x0 |u].
The likelihood function for D can be decomposed as
Y
LN (q,  : D) =
LXi (qXi |Ui : D)LXi (Xi |Ui : D))
(1)
Xi X

where
LX (qX|U : D) =

YY
u

M [x|u]

qx|u

exp(qx|u T [x|u])

(2)

x

and
LX ( : D) =

YY Y
u

M
0
xx
0 |u [x, x |u] .

(3)

x x0 6=x

If we put the above functions together and take the log, we get the log likelihood component for
a single variable X:
lX (q,  : D) = lX (q : D) + lX ( : D)
XX
=
M [x|u] ln(qx |u)  q[x|u] T [x|u]
u

+

x

XX X
u

M [x, x0 |u] ln(xx0 |u )).

(4)

x x0 6=x

By maximizing the above log likelihood function, the model parameters can be estimated as
qx|u =

M [x|u]
,
T [x|u]

xx0 |u =

754

M [x, x0 |u]
.
M [x|u]

(5)

fiI NTRUSION D ETECTION USING CTBN S

3.10.2 L EARNING FROM I NCOMPLETE DATA
Nodelman, Shelton, and Koller (2005) present the expectation maximization (EM) algorithm to
learn a CTBN model from partially observed trajectories D. The expected sufficient statistics are
M [x, x0 |u], the expected number of times that X transitions from state x to x0 when its parent set
U takes the values u, and T [x|u], the expected P
amount of time that X stays in the state x under the
parent instantiation u. We denote M [x|u] to be x0 M [x, x0 |u]. The expected log likelihood can be
decomposed in the same way as in Equation 4, except that the sufficient statistics M [x, x0 |u], T [x|u]
and M [x|u] are now replaced with expected sufficient statistics M [x, x0 |u], T [x|u] and M [x|u].
The EM algorithm for a CTBN works essentially in the same way as for an HMP. The expectation step is to calculate the expected sufficient statistics using inference method (will be described
in Section 3.11). The maximization step is to update the model parameters:

qx|u =

M [x|u]
,
T [x|u]

xx0 |u =

M [x, x0 |u]
.
M [x|u]

3.11 Inference
Now given a CTBN model and some (partially) observed data, we would like to query the model.
For example, we may wish to calculate the expected sufficient statistics for the above EM algorithm.
3.11.1 E XACT I NFERENCE
Nodelman et al. (2005) provide an exact inference algorithm using expectation maximization to
reason and learn the parameters from partially observed data. This exact inference algorithm requires flattening all the variables into a single Markov process and performing inference as in an
HMP. It has the problem that it makes the state space grow exponentially large. Therefore, the exact
inference method is only feasible for problems with very small state spaces.
3.11.2 A PPROXIMATE I NFERENCE
Because of the issue addressed below, much work has been done on CTBN approximate inference. Nodelman, Koller, and Shelton (2005) present an expectation propagation algorithm. Saria,
Nodelman, and Koller (2007) give another message passing algorithm that adapts the time granularity. Cohn, El-Hay, Friedman, and Kupferman (2009) provide a mean field variational approach.
El-Hay, Friedman, and Kupferman (2008) show a Gibbs sampling method approach using Monte
Carlo expectation maximization. Fan and Shelton (2008) give another sampling based approach
that uses importance sampling. El-Hay, Cohn, Friedman, and Kupferman (2010) describe a different expectation propagation approach.
To estimate the parameters of the models we build for the two applications (NIDS and HIDS),
we employ inference algorithms including exact inference and a Rao-Blackwellized particle filtering
(RBPF) algorithm, depending on the model size. Ng et al. (2005) extended RBPF to CTBNs. Their
model was a hybrid system containing both discrete and continuous variable. They used particle
filters for the discrete variables and unscented filters for the continuous variable. Our work are
similar to this work in the method of applying RBPF to CTBNs, but our model contains only discrete
variables and our evidence is over continuous intervals.
755

fiX U & S HELTON

PORT
80
8080
443
113
5101
995
51730
59822

DESCRIPTION
World Wide Web HTTP
HTTP Alternate
HTTP protocol over TLS/SSL
Authentication Service
Talarian TCP
pop3 protocol over TLS/SSL
unknown
unknown

PORT
80
139
443
445
1863
2678
1170
110

DESCRIPTION
World Wide Wed HTTP
NETBIOS Session Service
HTTP protocol over TLS/SSL
Microsoft-DS
MSNP
Gadget Gate 2 Way
AT+C License Manager
Post Office Protocol - Version 3

Figure 1: Ranking of the most frequent ports on MAWI dataset (left) and LBNL dataset (right).

3.12 CTBN Applications
Although inference and learning algorithms have been well developed for CTBNs, there have been
only a few applications to real world problems. Nodelman and Horvitz (2003) used CTBNs to
reason about users presence and availability over time. Ng et al. (2005) used CTBNs to monitor
a mobile robot. Nodelman et al. (2005) used CTBNs to model life event history. Fan and Shelton
(2009) modeled social networks via CTBNs. Our previous work (Xu & Shelton, 2008) presented
an NIDS for host machine using CTBNs, but did not include HIDS.

4. Anomaly Detection Using Network Traffic
In this section, we present an algorithm to detect anomalies in network traffic data using CTBNs.
We only focus on a single host on the network. The sequence and timing of events (e.g. packet
transimission and connection establishment) are very important in network traffic flow. It matters
not just how many connections were initiated in the past minute, but also their timing: if they were
evenly spaced the trace is probably normal, but if they all came in a quick burst it is more suspicious.
Similarly, the sequence is important. If the connections were made to sequentially increasing ports
it is more likely to be a scanning virus, whereas the same set of ports in random order is more likely
to be normal traffic. These are merely simple examples. We would like to detect more complex
patterns.
A typical machine in the network may have diverse activities with various service types (e.g.
HTTP, SMTP). The destination port number roughly describes the type of service to which a particular network activity belongs. Some worms propagate malicious traffic toward certain well known
ports to affect the quality of the associated services. By looking at traffic associated with different
ports we are more sensitive to subtle variations that do not appear if we aggregate trace information
across ports. Figure 1 shows the most popular ports ranked by their frequencies in the network
traffic on the datasets we use (described in more depth later). These services are, to some extent,
independent of each other. We therefore model each ports traffic with its own CTBN submodel.
We denote  as the whole observed traffic sequences on the particular host, and j as the traffic
associated with port j.
756

fiI NTRUSION D ETECTION USING CTBN S

G

N
H

Pin

Pout

Cinc

Cdec

Figure 2: CTBN model for network traffic as a plate model. N is the number of port .

4.1 A CTBN Model for Network Traffic
We use the same port-level submodel as our previous work (Xu & Shelton, 2008). We have a latent
variable H and four fully observed toggle variables: Pin , Pout , Cinc , Cdec .
The nodes packet-in, Pin , and packet-out, Pout , represent the transmission of a packet to or from
the host. They have no intrinsic state: the transmission of a packet is an essentially instantaneous
event. Therefore they have events (or transitions) without having state. This is modeled using a
toggle variable in which an event is evidence of a change in the state of the variable and the rate of
transition associated with each state is required to be the same.
The nodes connection-increase Cin and connection-decrease Cdec together describe the status
of the number of concurrent connections C active on the host. Notice that C can only increase or
decrease by one at any given event (the beginning or ending time of a connection). We assume that
the arrival of a new connection and the termination of an existing connection are both independent
of the number of other connections. Thus the intensity with which some connection starts (or stops)
is same as any other connections. Therefore, these are also modeled as toggle variables.
Node H has 8 states that represent different abstract attributes about the machines internal state.
The toggle variables (Pin , Pout , Cinc and Cdec ) are each allowed to change only for 2 of the states
of H and they are required to have the same rate for both of these states. 2 hidden states per toggle
variable was chosen as a balance between expressive power and model efficiency.
In previous work, we assumed that the traffic associated with different ports are independent of
each other, so the port-level submodels are isolated. Here we remove this restriction by introducing
another latent variable G that ties the port submodels together. The full model is shown in Figure 2.
4.2 Parameter Learning Using RBPF
To calculate the expected sufficient statistics in the E-step of EM for parameter learning, the exact
inference algorithm of Nodelman et al. (2002) flattens all the variables into a joint intensity matrix
and reasons about the resulting homogeneous Markov process. The time complexity is exponential
in the number of variables. For example, if there are 9 port models, the network contains 46 variables
in total. Approximate inference techniques like the clique tree algorithm (Nodelman et al., 2002),
message passing algorithms (Nodelman et al., 2005; Saria et al., 2007), importance sampling (Fan
757

fiX U & S HELTON

& Shelton, 2008) and Gibbs sampling (El-Hay et al., 2008) overcome this problem by sacrificing
accuracy.
We notice that our model has a nice tree structure which makes Rao-Blackwellized particle
filtering (RBPF) a perfect fit. RBPF uses a particle filter to sample a portion of the variables and
analytically integrates out the rest. It decomposes the model structure efficiently and thus reduces
the sampling space.
If we denote the N port-level hidden variables as H1 , ..., HN , the posterior
distribution of the
QN
whole model can be factorized as P (G, H1 , ..., HN |  ) = P (G |  ) i=1 P (Hi | G,  ). Note
that G and Hi are processes, so this probability is a density over complete trajectories. We use a
particle filter to estimate Gs conditional distribution P (G |  ) as a set of sampled trajectories of
G. It is difficult to sample directly from the posterior distribution, so we use an importance sampler
to sample a particle from a proposal distribution and the particles are weighted by the ratio of its
likelihood under the posterior distribution to the likelihood under the proposal distribution (Doucet
et al., 2000). Since the variable G is latent and has no parents, we can use forward sampling
to sample the particles from P (G) and the weight of each particle is simply the likelihood of 
conditioned on this trajectory for G (Fan & Shelton, 2008). Each port-level submodel is then dseparated from the rest of the network, given full trajectory of G (see Section 3.9 for d-separation
in CTBNs). Since each is small (only 8 hidden states), they can be marginalized out exactly. That
is, we can calculate P (i | G) (where i is the portion of the trajectory for submodel i) exactly,
marginalizing out Hi with the - recursions from Section 3.7.
The expected sufficient statistics (ESS) for any variable X in a CTBN are TX|U [x|u], the expected amount of time X stays at state x given its parent instantiation u, and MX|U [x, x0 |u], the
expected number of transitions from state x to x0 given Xs parent instantiation u. Let g i  P (G),
i )
i = 1, . . . , M be the particles. We define their likelihood weights to be wi = PP(g(g|
i ) and let
P
W = i wi be the sum of the weights. Then general importance sampling allows that an expected
sufficient statistic can be estimated in the following way, where SS is any sufficient statistic:

E(g,h1 ,...,hN )P (G,H1 ,...,HN | ) [SS(g, h1 , . . . , hN )]
= EgP (G| ) Eh1 ,...,hN P (H1 ,...,HN |g, ) [SS(g, h1 , . . . , hN )]
1 X
wi Eh1 ,...,hN P (H1 ,...,HN |gi , ) [SS(g i , h1 , . . . , hN )] .

W
i

The expected sufficient statistics of the whole model are in two categories: those that depend
only on g, ESS(g), and those that depend on a port model k, ESS(g, hk , k ). ESS(g) is simply
the summation of counts (the amount of time G stays at some state, or the number of times G
transitions from one state to another) from the particles, weighted by the particle weights:

EgP (G| ) [SS(g)] 

1 X
wi SS(g i ) .
W
i

758

(6)

fiI NTRUSION D ETECTION USING CTBN S

Function Wholemodel Estep
input: current model t , evidence 
output: Expected sufficient statistics ESS
ESS := {ESS(g), ESS(s1 , g), . . . , ESS(sn , g)}
Initialize ESS as empty
For each particle g i  {g 1 , . . . , g M }, g i  P (G)
For each Sj  {S1 , . . . , SN }
[P (j |g i ), ESS(sj , g i )] = Submodel Estep(g i , t [Sj ], j )
For each Sj  {S1 , . . . , SN }
Q
ESS(sj , g) = ESS(sj , g) + k6=j P (k |g i )  ESS(sj , g i )
i)
ESSgi = CountGSS(gQ
ESS(g) = ESS(g) + j P (j |g i )  ESSgi
Return ESS

Figure 3: Rao-Blackwellized particle filtering Estep for the whole model
ESS(g, hk , k ) can be calculated for each submodel independently:
Eg,h1 ,...,hN P (G,H1 ,...,HN | ) [SS(g, hk , k )]
Z
1 X
wi

P (hk |g i , k )SS(g i , hk , k ) dhk
W
hk
i
Q
i Z
1 X j P (j |g )
P (hk |g i , k )SS(g i , hk , k ) dhk
=
W
P ( )
hk
i
Z
X
Y
1

P (j |g i )
P (hk , k |g i )SS(g i , hk , k ) dhk .
W
hk
i

(7)

j6=k

The integrals are over all possible trajectories for the hidden process Hk . The first line holds by
d-separation (we need only average over the submodel k, given an assignment to G). The second
line expands the weight. The last line combines the weight term for submodel k with the terms
in the integral to get the likelihood of hk and the submodel data. The constant of proportionality
P
will cancel in the subsequent maximization, or it can be reconstructed by noting that x TX|U [x|u]
should be the total time
R of the interval.
This last integral, hk P (hk , k |g i )SS(g i , hk , k ) dhk , and P (j |g i ) can be calculated using the
technique described by Nodelman et al. (2005), for exact ESS calculation. The calculations are
very similar the integrals of Section 3.7, except they intensity matrices can change from interval to
interval (they are a function of the sampled trajectory gi ).
The full E-step algorithm is shown in Figure 3 (sk represents all of the variables in submodel
k). Function Submodel Estep calculates the expected sufficient statistics and the likelihood for a
subnet model (Equation 7). Function CountGSS counts the empirical time and transition statistics
from the sampled trajectory of G (Equation 6).
In EM, we use the ESS as if they were the true sufficient statistics to maximize the likelihood
with respect to the parameters. For a regular CTBN variable X (such as our hidden variable G
and H), Equation 5 performs the maximization. For our toggle variables, e.g. Pi , the likelihood
759

fiX U & S HELTON

component for the toggle variable is
Y

MP

QPi |ui exp(QPi |u T [U = u])

u

which can be found by setting qx|u to be the same value (QPi |u ) for all x (tieing the parameters) and
simplifying the product over x in Equation 2. Thus the maximum likelihood parameter estimate is
QPi |u =

MPi
T [U = u]

where MPi is the number of events for variable Pi and QPi |u is the only parameter: the rate of
switching.
We synchronize the particles at the end of each window (see Section 6.1) and resample as
normal for a particle filter at those points. That is, we propagate the particles forward, but stop them
all at the end of the window, resample based on the weights, and then continue with the new set of
particles. In general, the particles are not aligned by time, except at these resampling points.
4.3 Online Testing Using Likelihood
Once the CTBN model has been fitted to historic data, we detect attacks by computing the likelihood
of a window of the data (see Section 6.1) under the model. If the likelihood falls below a threshold,
we flag the window as anomalous. Otherwise, we mark it as normal.
In our experiments, we fix the window to be of a fixed time length, Tw . Therefore, if the
window of interest starts at time T , we wish to calculate p( [T, T + Tw ] |  [0, T ]) where  [s, t]
represents the observed connections and packets from time s to time t. Again, we use a RBPF to
estimate this probability. The samples at time T represent the prior distribution P (G |  [0, T ]).
Propagating them forward across the window of length Tw produces a set of trajectories for G,
g i . Each submodel k can evalute P (k [T, T + Tw ] | g i ) by exact marginalization (the sum of the
vector T +Tw , the forward message). The weighted average (over samples g k ) of the product of the
submodel probabilities is our estimate of P ( [T, T + Tw ] |  [0, T ]).

5. Anomaly Detection Using System Calls
Now we turn to the problem of detecting anomalies using system call logs.
5.1 A CTBN Model for System Calls
System call logs monitor the kernel activities of machines. They record detailed information of the
sequence of system calls to operating system. Many malicious attacks on the host can be revealed
directly from the internal logs.
We analyze the audit log format of SUNs Solaris Basic Security Module (BSM) praudit audit
logs. Each user-level and kernel event record has at least three tokens: header, subject, and return.
An event begins with a header in the format of: header, record length in bytes, audit record version
number, event description, event description modifier, time and date. The subject line consists of:
subject, user audit ID, effective user ID, effective group ID, real user ID, real group ID, process
ID, session ID, and terminal ID consisting of a device and machine name. A return with a return
value indicating the success of the event closes the record.
760

fiI NTRUSION D ETECTION USING CTBN S

H

S1

S2

Sn

Figure 4: CTBN model for system call data

}

s1, s2, ..., sk

ti-1 ti-1+t

ti+t

ti

ti+1 ti+1+t

time

Figure 5: System call traces with a finite resolution clock (resolution = t )
We construct a CTBN model similar to our port-level network model. Individual system calls
S1 , ..., SN , which are the event description fields in the header token, are transiently observed: they
happen instantaneously with no duration. We treat them as toggle variables like packets in the
network model. We also introduce a hidden variable H as a parent of the system calls variables to
allow correlations among them. This hidden variable is designed to model the internal state of the
machine, although such a semantic meaning is not imposed by our method. Put together, our system
call model looks like Figure 4.
If the state space of the hidden variable H is of size m, the transition rate matrix of H is



QH = 


qh1
q h2 h1
..
.

q h1 h2
qh2
..
.

q hm h1

q hm h2

. . . q h1 hm
. . . q h2 hm
..
..
.
.
. . . qhm




.


and the transition intensity rate of the toggle variable s  S given the current value of its parent H
is qs|hi , i = 1, ..., m.
To estimate the CTBN model parameters, we again use the expectation maximization (EM)
algorithm. The expected sufficient statistics we need to calculate for our model are
 Mhi hj , the expected number of times H transitions from state i to j;
 Thi , the expected amount of time H stays in state i; and
 Ms|hi , the expected number of times system call s is evoked when H is in state i.
761

fiX U & S HELTON

The maximum likelihood parameters are
Mhi hj
Thi
Ms|hi
=
.
Thi

q hi hj =
qs|hi

5.2 Parameter Estimation with Finite Resolution Clocks
Because of the finite resolution of computer clocks, multiple instantaneous events (system calls)
occur within a single clock tick. Therefore in the audit logs, a batch of system calls may be recorded
as being executed at a same time point, rather than their real time stamp, as a result of this finite
time accuracy. However, the correct order of the events is kept in the logs. That is, we know exactly
that system call S2 follows S1 if they are recorded in this order in the audit logs. Thus all the system
call timings are only partially observed. This type of partial observation has not previously been
considered in CTBN inference. A typical trajectory  over [0, T ] of system call data is shown in
Figure 5: a batch of system calls are evoked at some time after ti but before the next clock tick,
followed by a quiet period of arbitrary length, and yet another bunch of events at some time after
ti+1 and so on.
Let t1:t2 denote the evidence over interval [t1, t2), t1:t2+ denote the evidence over [t1, t2], and
t1 :t2 denote the evidence over (t1, t2). We define the vectors
ti = p(Ht , 0:ti )
i

t+i = p(t+ :T |Ht+ )
i

i

where Ht is the value of H just prior to the transition at ti , and Ht+ is value just afterward. We
i
i
also define the vectors
ti = p(Hti , 0:t+ )
i

ti = p(ti :T |Hti )
where the evidence at the transition time ti is included. We follow the forward-backward algorithm
to compute ti and ti for all ti at which there is an event. To do this, we split any interval [ti , ti+1 )
into a spike period [ti , ti + t ) (t is one resolution clock), during which there is a batch of
system calls, and a quite period [ti + t , ti+1 ) over which no events exist, and do the propagations
separately.
For a spike period [ti , ti + t ), if the observed event sequence is s1 , s2 , ..., sk , we construct an
artificial Markov process X with the following intensity matrix.




QX = 



QH Q1 0
...
0
0 QH Q2 . . .
0
..
..
..
..
..
.
.
.
.
.
0
0
. . . QH Qk
0
0
...
0 QH
762





.



fiI NTRUSION D ETECTION USING CTBN S

where



QH = 


P
qh1  sS qs|h1
q h2 h1
..
.

qh2

q hm h1

q h1 h2
P
 sS qs|h2
..
.

...
...
..
.
. . . qhm

q hm h2

q h1 hm
q h2 hm
..
.
P
 sS qs|hm







and



Qi = 


qsi |h1
0
..
.
0

0
qsi |h2
..
.

...
...
..
.

0
0
..
.

0

0

qsi |hm







X tracks the evidence sequence s1  s2  ...  sk . QX is a square block matrix of dimension
m  (k + 1). Each block is an m  m matrix. The subsystem X has k + 1 blocks of states. The
first block represents the state of H before any events. The second block represents H after exactly
one event, s1 , happens. The third block represents H after s1 followed by s2 happens, and so
on. The last block represents H after all the events finish executing in order. The subsystem has
zero transition intensities everywhere except along the sequence pass. The diagonal of QH is the
same matrix as that of QH except that the transition intensities of all the system call variables are
subtracted. This is because the full system includes transitions that were not observed. While those
transition rates were set to zero (to force the system to agree with the evidence), such conditioning
does not change the diagonal elements of the rate matrix (Nodelman et al., 2002). Within each of
the k + 1 states of a block, H can freely change its value. Therefore, the non-diagonal elements
of QH have the same intensities as QH . Upon transitioning, X can only transit from some state to
another according to the event sequence. Therefore, most of the blocks are 0 matrices except those
to the immediate right of the diagonal blocks. The transition behavior is described by the matrix
Qi . Qi has 0 intensities on non-diagonal entries because H and S can not change simultaneously.
The diagonal element Qi (h, h) is the intensities of event si happening, given the current value of
the hidden state is h.
We take the forward pass as an example to describe the propagation; the backward pass can be
performed similarly. Right before ti , ti has m dimensions. We expand it to m(k + 1) dimensions
to form ti which only has non-zero probabilities in the first m states. ti now describes the
distribution over the subsystem X. ti eQX t represents the probability distribution at time ti + t ,
given that some prefix of the observed sequence occurred. We take only the last m state probabilities
to condition on the entire sequence happening, thus resulting in an m-dimensional vector, ti +t .
For a quiet period [ti + t , ti+1 ), no evidence is observed. Therefore ti +t is propagated to
ti+1 using QH , the rate matrix conditioned on only H events occuring:
ti+1 = ti +t exp(QH (ti+1  ti  t )) .
When we are done with the full forward-backward pass over the whole trajectory, we can calculate the expected sufficient statistics Mhi hj , Thi and Ms|hi . Again, we refer to the work of Nodelman
et al. (2005) for the algorithm.
763

fiX U & S HELTON

5.3 Testing Using Likelihood
Once we have learned the model from the normal process in the system call logs, we calculate
the log-likelihood of a future process under the model. The log-likelihood is then compared to a
predefined threshold. If it is below the threshold, a possible anomaly is indicated. With only a single
hidden variable, these calculations can be done exactly.

6. Evaluation
To evaluate our methodology, we constructed experiments on two different types of data: network
traffic traces and system call logs. In the following sections, we show the experiment results on both
tasks.
A dynamic Bayesian network (DBN) is another popular technique for graphical modeling of
temporal data. Because they slice time, events without state changes (instantaneous events) are
difficult to model. Any reasonable time resolution will result with multiple events for the same
variable over one time period. There is no standard way of encoding this in a DBN. If we use a toggle
variable, it only records the parity of the number of events over the time interval. Furthermore, for
the NIDS, events are very bursty. During active times, multiple packets are emited per second.
During inactive times, there may be no activity for hours. Finding a suitable sampling rate that
maintains the efficency of the model is difficult. For the HIDS, the problem is more acute. We do
not know of any way of modeling timing ambiguity in a DBN without throwing away all timing
information or adding a mathematical framework that essentially turns the DBN into the CTBN
described here. In general, we could not find a suitable way to apply a DBN to these problems
without essentially turning the DBN into a CTBN by very finely slicing time and then applying
numeric tricks to speed up inference that amount to converting the stochastic matrices into rate
matrices and using numeric integration for the matrix exponential.
We have compared against current adaptive methods for each problem individually. These include nearest neighbor, support vector machines, and sequence time-delaying embedding. We give
further details on these methods below.
6.1 Experiment Results on Network Traffic
In this section, we present our experiment results on NIDS.
6.1.1 DATASETS
We verify our approach on two publicly available real network traffic trace repositories: the MAWI
working group backbone traffic MAWI and the LBNL/ICSI internal enterprise traffic LBNL.
The MAWI backbone traffic is part of the WIDE project which has collected raw daily packet
header traces since 2001. It records the network traffic through the inter-Pacific tunnel between
Japan and the USA. The dataset uses tcpdump and IP anonymizing tools to record 15-minute
traces every day, and consists mostly of traffic from or to Japanese universities. In our experiment,
we use the traces from January 1st to 4th of 2008, with 36,592,148 connections over a total time of
one hour.
The LBNL traces are recorded from a medium-sized site, with emphasis on characterizing internal enterprise traffic. Publicly released in an anonymized form, the LBNL data collects more than
764

fiI NTRUSION D ETECTION USING CTBN S

# packets flowing from source to destination
# packets flowing from destination to source
# connections by the same source in the last 5 seconds
# connections to the same destination in the last 5 seconds
# different services from the same source in the last 5 seconds
# different services to the same destination in the last 5 seconds
# connections by the same source in the last 100 connections
# connections to the same destination in the last 100 connections
# connections with the same port and source in the last 100 connections
# connections with the same port and destination in the last 100 connections
Figure 6: Features for nearest neighbor approach from the work of (Lazarevic et al., 2003).
100 hours network traces from thousands of internal hosts. From what is publicly released, we take
one hour traces from January 7th, 2005 (the latest date available), with 3,665,018 total connections.
6.1.2 W ORM D ETECTION
We start with the problem of worm detection. We split traffic traces for each host: half for training
and half for testing. We learn a CTBN model from the training data for each of the hosts. Since the
network data available are clean traffic with no known intrusions, we inject real attack traces into
the testing data. In particular, we inject IP Scanner, W32.Mydoom, and Slammer. We then slide
a fixed-time window over the testing traces, report a single log-likelihood value for each sliding
window, and compare it with a predefined threshold. If it is below the threshold, we predict it as
an abnormal time period. We define the ground truth for a window to be abnormal if any attack
traffic exists in the interval, and normal otherwise. The window size we use is 50 seconds. We only
consider windows that contain at least one network event.
We compare our method employing RBPF with our previous factored CTBN model (Xu &
Shelton, 2008), connection counting, nearest neighbor, Parzen-window detector (Yeung & Chow,
2002), and one-class SVM with a spectrum string kernel (Leslie, Eskin, & Noble, 2002).
The connection counting method is straightforward. We score a window by the number of
initiated connections in the window. As most worms aggregate many connections in a short time,
this method captures this particular anomaly well.
To make nearest neighbor competitive, we try to extract a reasonable set of features. We follow
the feature selection of the work of Lazarevic et al. (2003), who use a total of 23 features. Not all
of their features are available in our data. Those available are shown in Figure 6. Notice that these
features are associated with each connection record. To apply the nearest neighbor method to our
window based testing framework, we first calculate the nearest distance of each connection inside
the window to the training set (which is composed of normal traffic only), and assign the maximum
among them as the score for the window. Similarly, for the Parzen window approach, we apply the
same feature set and assign the maximum density among all the connections inside a window to be
the score of that window.
Besides the above feature-based algorithms, we would also like to see how sequence-based
approaches compare against our methods. These algorithms are widely used in network anomaly
detection. Like our approach, they treat the traffic traces as stream data so that sequential contexts
765

fiX U & S HELTON

1

1

0.8

0.8

0.8

0.6

0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0.2

0
0

0.02

0.04
0.06
False Positive Rate

0.08

0.6

0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0.2

0
0

0.1

0.02

0.08

0.6

0.4

0
0

0.1

1

0.8

0.8

0.8

0.6

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0
0

0.02

0.04
0.06
False Positive Rate

IP Scanning

0.08

0.1

0.6

0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0.2

0
0

0.02

0.04
0.06
False Positive Rate

Mydoom

0.08

0.1

True Positive Rate

1

0.2

0.02

0.04
0.06
False Positive Rate

0.08

0.1

Slammer

1

0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0.2

Mydoom

True Positive Rate

True Positive Rate

LBNL

IP Scanning

0.04
0.06
False Positive Rate

True Positive Rate

1

True Positive Rate

True Positive Rate

MAWI

can be explored. One-class SVM with spectrum string kernel was chosen for comparison. We
implemented a spectrum kernel in the LIBSVM library (Chang & Lin, 2001). We give the network
activities (such as a connection starting or ending, or a packet emmision or receipt) inside each portlevel submodel a distinct symbol. The sequence of these symbols are fed to the algorithm as inputs.
A decision surface is trained from normal training traffic. In testing, for each sliding window, the
distance from this window string to the decision hyperplane is reported as the window score. We
also tried experiments using the edit distance kernel, but their results are dominated by the spectrum
kernel, so we do not report them here.

0.6

0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF100

0.2

0
0

0.02

0.04
0.06
False Positive Rate

0.08

0.1

Slammer

Figure 7: ROC curves of testing results on IP scanning attack, Mydoom attack and Slammer attack.
 = 0.001. Top: MAWI. Bottom: LBNL.

When injecting the attack traffic, we randomly pick a starting point somewhere in the first half
of the test trace and insert worm traffic for a duration equal to  times the length of the full testing
trace. The shorter  is, the harder it is to detect the anomaly. We choose  to be 0.02% for all
the experiments in this work to challenge the detection tasks. We also scaled back the rates of the
worms. When running at full speed, a worm is easy to detect for any method. When it slows down
(and thus blends into the background traffic better), it becomes more difficult to detect. We let  be
the scaling rate (e.g. 0.1 indicates a worm running at one-tenth of its normal speed).
For our method, we set the state space of variable G to be 4 and variable H to be 8. We use
100 samples for particle filtering, and resample the particles after every 50 seconds. For the SVM
spectrum kernel method, we choose the sub-sequence length to be 5 and the parameter  to be 0.8.
We show the ROC curves of all the methods in Figure 7. The curves show the overall performance on the 10 most active hosts for each dataset. Each point on the curves corresponds to a
766

fiI NTRUSION D ETECTION USING CTBN S

1

1

0.8

0.8
True Positive Rate

True Positive Rate

different threshold of the algorithm. Our CTBN method out-performs the other algorithms except
in the single case of the Mydoom attack against a background of the LBNL traffic. In many cases,
the advantages of the CTBN approach are pronounced.
For all of the MAWI data, the factored and non-factored CTBN models perform comparably.
We believe this is because the data only captures connections that traverse a trans-Pacific link.
Therefore, not all of the connections in or out of a machine are represented. This makes reasoning
about the global pattern of interaction for a machine difficult. For the LBNL data, one attack (IP
scanning) shows no advantage to a non-factored model. One attack (Mydoom) shows a distinct
advantage. And one attack (Slammer) indicates some advantage, depending on the desired false
positive rate. This demonstrate some advantage to jointly modeling the traffic across all ports,
although it is clear this advantage is not uniform over traffic patterns and attack types.

0.6

0.4

0.2

0.6

0.4

0.2
Connection Count
CTBN, RBPF100

0
0

0.02

0.04
0.06
False Positive Rate

0.08

Connection Count
CTBN, RBPF100

0.1

0
0

0.02

0.04
0.06
False Positive Rate

0.08

0.1

Figure 8: ROC curves of testing results for Slammer attack on MAWI dataset demonstrating the
effect of slowing the attack rate. Left:  = 0.01. Right:  = 0.001

We also show how the ROC curves shift as we scale back the worm running speed  in Figure 8.
As firewalls are built to be more sensitive to block malicious traffic, worms have to act more stealthy
to sneak through. We demonstrate the robustness of our method compared to the best competitor
(connection counts) to the speed of the worms attack.
6.1.3 H OST I DENTIFICATION
Identifying individual hosts based on their network traffic patterns is another useful application of
our model. For instance, a household usually installs a network router. Each family members
computer is connected to this router. To the outside Internet, the network traffic going out of the
router behaves as if it is coming from one peer, but it is actually coming from different people.
Dad will possibly read sports news while kids surf on social networks. It is interesting as well as
useful to tell which family member is contributing the current network traffic. Host identification
can also be used to combat identity theft. When a network identity is abused by the attacker, host
identification techniques can help the network administrator tell whether the current network traffic
of this host is consistent with its usual pattern or not.
767

fiX U & S HELTON

The first set of experiments we construct is a host model fitting competition. The same 10
hosts picked for the worm detection tasks from LBNL dataset compose our testing pool. We learn
the coupled CTBN model for each host. We split the test traces (clean) of a particular host into
segments with lengths of 15 seconds. For each of the segments, we compute the log-likelihood of
the segment under the learned model from all the hosts (including its own), and label the segment
with the host that achieves the highest value. We compute a confusion matrix C whose element Cij
equals the fraction of test traces of host i for which model j has highest log-likelihood. We expect
to see the highest hit rates fall on the diagonals because ideally a host should be best described by its
own model. Table 9 shows our results on the dataset of LBNL. The vast majority of traffic windows
are assigned to the correct host. With the exception of host 1, the diagonals are distinctly higher than
other elements in the same row. For comparison, we performed the same experiment using SVM
spectrum kernel method. Again, we selected the sub-sequence length to be 5 and the parameter
 to be 0.8. We tried multiple methods for normalization (of the distance to the hyperplane) and
variations of parameters. All produced very poor results with almost all of the windows assigned to
a single host. We omit the table of results.
Host
1
2
3
4
5
6
7
8
9
10

1
0.09
0.06
0
0
0
0
0.25
0
0.04
0

2
3
4
0.41 0.47 0
0.50 0.31 0
0
1
0
0
0
1
0.08 0.13 0
0.20 0.03 0
0
0.06 0.02
0.08 0.28 0
0.05 0.13 0.01
0.03 0.18 0.01

5
6
7
8
9
10
0
0
0
0.03 0
0
0
0.13 0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.68 0.06 0.03 0.01 0
0.01
0.01 0.74 0
0.02 0
0
0
0
0.66 0
0
0.01
0
0.05 0
0.59 0
0
0.01 0.01 0.02 0
0.73 0
0
0.15 0.03 0.03 0
0.57

Figure 9: Confusion matrix for LBNL for host identification using CTBN
Our second experiment is a host traffic differentiation task. We mingle the network traffic from
another host with the analyzed host. We expect the detection method to successfully tell apart the
two. To verify this idea, we pick one host among the 10 we choose above from LBNL dataset and
split its traffic evenly into training and testing. We again learn the model from training data. For
testing data, we randomly choose a period and inject another hosts traffic as if it were a worm.
Our goal is to identify the period as abnormal since the hosts traffic is no longer its own behavior.
Figure 10 displays the results from two such combination tests. The parameters for injecting the
traffic as a worm are  = 0.02,  = 0.001. In the left graph, the nearest neighbor and Parzen
window curve overlap, and both CTBN curves overlap. In the right graph, the coupled CTBN curve
substantially outperforms all the other curves.
6.2 Experiment Results on System Call Logs
In this section, we present our experiment results on HIDS.
768

fi1

1

0.8

0.8

0.6
0.4

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF10

0.2
0
0

0.02

0.04
0.06
False Positive Rate

0.08

True Positive Rate

True Positive Rate

I NTRUSION D ETECTION USING CTBN S

Nearest Neighbor
Connection Count
Parzen Window
SVMSpectrum
CTBN, factored
CTBN, RBPF10

0.6
0.4
0.2
0
0

0.1

0.02

0.04
0.06
False Positive Rate

0.08

0.1

Figure 10: ROC curves of testing results on host identification on the LBNL data. Left: host 1,
Nearest neighbor curve and Parzen window curve overlap, both CTBN curves overlap.
Right: host 2.

Week
1
2
3
4
5
6
7

# normal
processes
786
645
775
615
795
769
584

# attack
processes
2
4
20
331
10
24
0

System Call
close
ioctl
mmap
open
fcntl
stat
access

# occurrence
123403
68849
60886
42479
7416
6429
2791

System Call
execve
chdir
chroot
unlink
chown
mkdir
chmod

# occurrence
1741
1526
328
26
23
4
1

Figure 11: Left: DARPA BSM process summary. Right: DARPA BSM system call summary
6.2.1 DATASET
The dataset we used is the 1998 DARPA Intrusion Detection Evaluation Data Set from MIT Lincoln
Laboratory. Seven weeks of training data that contain labeled network-based attacks in the midst of
normal background data are publicly available at the DARPA website. The Solaris Basic Security
Module (BSM) praudit audit data on system call logs are provided for research analysis. We follow
Kang, Fuller, and Honavar (2005) to cross-index the BSM logs and produce a labeled list file that
labels individual processes. The resulting statistics are shown on the left table of Figure 11. The
frequency of all the system calls appearing in the dataset is summarized in descending order on the
right of Figure 11.
6.2.2 A NOMALY D ETECTION
Our experimental goal is to detect anomalous processes. We train our CTBN model on normal
processes only and test on a mixture of both normal and attack processes. The state space of the
769

fi1

1

0.8

0.8
True Positive Rate

True Positive Rate

X U & S HELTON

0.6
0.4
CTBN
SVMSpectrum
Stide
Nearest Neighbor

0.2
0
0

0.01

0.02
0.03
False Positive Rate

0.04

0.6
0.4
CTBN
SVMSpectrum
Stide
Nearest Neighbor

0.2

0.05

0
0

0.01

0.02
0.03
False Positive Rate

0.04

0.05

Figure 12: ROC curves for BSM data detection. Left: Training on week 1 and combined testing
results on week 2 to 7; Right: Training on week 3 and test on week 4, Stide curve and
CTBN curve overlap

hidden variable H is set to 2. The log-likelihood of a whole process under the learned model
represents the score of this process. We compare to the score with a predefined threshold to classify
the process as a normal one or a system abuse.
We implement sequence time-delaying embedding (stide) and stide with frequency threshold
(t-stide) for comparison (Warrender, Forrest, & Pearlmutter, 1999). These two algorithms build a
database of all previously seen normal sequences of system calls and compare the testing sequences
with it. They are straightforward and perform very well empirically on most of the system call log
datasets. We choose the parameter k, the sequence length to be 5, and h, the locality frame length, to
be 50. The results for t-stide are not shown in the following resulting graphs since they overlapped
with stide in almost all cases.
Other approaches we compare against are nearest neighbor and one-class SVM with spectrum
string kernel and edit distance kernel. We follow Hu et al. (2003) and transform a process into a
feature vector, consisting of the occurrence numbers of each system call in the process. The nearest
distance between a testing process and the training set of processes is assigned as the score. For
one-class SVM, processes are composed of strings of system calls. Normal processes are used for
learning the bounding surface and the signed distance to it is assigned as the score. We set the subsequence length to be 5 and the parameter  to be 0.5. Again, since the edit distance kernel results
are dominated by the spectrum kernel, we do not show them.
Figure 12 displays the results from two experiment settings. In the left graph, we train the
model on the normal processes from week 1 and test it on all the processes from weeks 2 to 7. In
the right graph, we train on normal processes from week 3 and test it on all the processes from week
4, the richest in attack processes volume. Because attacks are relatively rare compared to normal
traffic, we are most interested in the region of the ROC curves with small false positive rates. So
we only show the curves in the area where the false positive rate falls in the region [0, 0.05]. Our
CTBN method beats nearest neighbor and SVM with spectrum kernel in both experiments. stide
performs slightly better than our method in the combined test, but achieves the same accuracy in
770

fiI NTRUSION D ETECTION USING CTBN S

the experiment using only week 3 and testing on week 4. The advantage to the CTBN model over
stide is that it can be easily combined with other prior knowledge and other data sources (such as the
network data from NIDS). We demonstrate that there is no loss of performance from such flexibility.

7. Conclusions
In the realm of temporal reasoning, we have introduced two additions to the CTBN literature. First,
we demonstrated a Rao-Blackwellized particle filter with continuous evidence. Second, we demonstrated that we can learn and reason about data that contains imprecise timings, while still refraining
from discretizing time.
In the realm of intrusion detection, we have demonstrated a framework that performs well on two
related tasks with very different data types. By concentrating purely on event timing, without the
consideration of complex features, we were able to out-perform existing methods. The continuoustime nature of our model aided greatly in modeling the bursty event sequences that occur in systems
logs and network traffic. We did not have to resort to time slicing, either producing rapid slices that
are inefficient for quite periods, or lengthy slices that miss the timing of bursty events.
A combination of the two sources of information (system calls and network events) would be
straight-forward with the model we have produced. We believe it would result in more accurate
detection. The collection of such data is difficult, however; we leave it as an interesting next step.

Acknowledgments
This project was supported by Intel Research and UC MICRO, by the Air Force Office of Scientific
Research (FA9550-07-1-0076), and by the Defense Advanced Research Project Agency (HR001109-1-0030).

References
Agosta, J. M., Duik-Wasser, C., Chandrashekar, J., & Livadas, C. (2007). An adaptive anomaly
detector for worm detection. In Workshop on Tackling Computer Systems Problems with
Machine Learning Techniques.
A.Hofmeyr, S., Forrest, S., & Somayaji, A. (1998). Intrusion detection using sequences of system
calls. Journal of Computer Security, 6, 151180.
Cha, B. (2005). Host anomaly detection performance analysis based on system call of neuro-fuzzy
using soundex algorithm and n-gram technique. In Systems Communications (ICW).
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. http://
www.csie.ntu.edu.tw/cjlin/libsvm.
Cohn, I., El-Hay, T., Friedman, N., & Kupferman, R. (2009). Mean field variational approximation
for continous-time Bayesian networks. In Uncertainty in Artificial Intelligence.
Dewaele, G., Fukuda, K., & Borgnat, P. (2007). Extracting hidden anomalies using sketch and non
Gaussian multiresulotion statistical detection procedures. In ACM SIGCOMM.
Doucet, A., de Freitas, N., Murphy, K., & Russel, S. (2000). Rao-Blackwellised particle filtering
for dynamic Bayesian networks. In Uncertainty in Artificial Intelligence.
771

fiX U & S HELTON

El-Hay, T., Cohn, I., Friedman, N., & Kupferman, R. (2010). Continuous-time belief propagation.
In Proceedings of the Twenty-Seventh International Conference on Machine Learning.
El-Hay, T., Friedman, N., & Kupferman, R. (2008). Gibbs sampling in factorized continous-time
Markov processes. In Uncertainty in Artificial Intelligence.
Eskin, E. (2000). Anomaly detection over noisy data using learned probability distributions. In
International Conference on Machine Learning.
Eskin, E., Arnold, A., Prerau, M., Portnoy, L., & Stolfo, S. (2002). A geometric framework for
unsupervised anomaly detection: Detecting intrusions in unlabeled data. In Barbara, D., &
Jajodia, S. (Eds.), Applications of Data Mining in Computer Security. Kluwer.
Fan, Y., & Shelton, C. R. (2008). Sampling for approximate inference in continuous time Bayesian
networks. In Symposium on Artificial Intelligence and Mathematics.
Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics. In Proceedings of the Twenty-Fifth International Conference on Uncertainty in Artificial Intelligence.
Forrest, S., A.Hofmeyr, S., Somayaji, A., & A.Longstaff, T. (1996). A sense of self for unix processes. In IEEE Symposium on Security and Privacy, pp. 120128.
Hu, W., Liao, Y., & Vemuri, V. (2003). Robust support vector machines for anomaly detection in
computer security. In International Conference on Machine Learning and Applications.
Kang, D.-K., Fuller, D., & Honavar, V. (2005). Learning classifiers for misuse detetction using a
bag of system calls representation. In IEEE International Conferences on Intelligence and
Security Informatics.
Karagiannis, T., Papagiannaki, K., & Faloutsos, M. (2005). BLINC: Multilevel traffic classification
in the dark. In ACM SIGCOMM.
Kruegel, C., Mutz, D., Robertson, W., & Valeur, F. (2003). Bayesian event classification for intrusion
detection. In Annual Computer Security Applications Conference.
Lakhina, A., Crovella, M., & Diot, C. (2005). Mining anomalies using traffic feature distributions.
In ACM SIGCOMM, pp. 2126.
Lazarevic, A., Ertoz, L., Kumar, V., Ozgur, A., & Srivastava, J. (2003). A compare study of anomaly
detection schemes in network intrusion detection. In SIAM International Conference on Data
Mining.
LBNL.
LBNL/ICSI enterprise tracing project..
enterprise-tracing/Overview.html/.

http://www.icir.org/

Leslie, C., Eskin, E., & Noble, W. S. (2002). The spectrum kernel: A string kernel for SVM protein
classification. In Pacific Symposium on Biocomputing 7:566-575.
Malan, D. J., & Smith, M. D. (2005). Host-based detection of worms through peer to peer cooperation. In Workshop on Rapid Malcode.
MAWI. MAWI working group traffic archive.. http://mawi.nezu.wide.ad.jp/mawi/.
Moore, A. W., & Zuev, D. (2005). Internet traffic classification using Bayesian analysis techniques.
In ACM SIGMETRICS.
Ng, B., Pfeffer, A., & Dearden, R. (2005). Continuous time particle filtering. In National Conference
on Artificial Intelligence, pp. 13601365.
772

fiI NTRUSION D ETECTION USING CTBN S

Nodelman, U., & Horvitz, E. (2003). Continuous time Bayesian networks for inferring users presence and activities with extensions for modeling and evaluation. Tech. rep. MSR-TR-2003-97,
Microsoft Research.
Nodelman, U., Koller, D., & Shelton, C. R. (2005). Expectation propagation for continuous time
Bayesian networks. In Uncertainty in Artificial Intelligence, pp. 431440.
Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time Bayesian networks. In Uncertainty in Artificial Intelligence, pp. 378387.
Nodelman, U., Shelton, C. R., & Koller, D. (2003). Learning continuous time Bayesian networks.
In Uncertainty in Artificial Intelligence, pp. 451458.
Nodelman, U., Shelton, C. R., & Koller, D. (2005). Expectation maximization and complex duration
distributions for continuous time Bayesian networks. In Uncertainty in Artificial Intelligence,
pp. 421430.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical Recipes in C
(Second edition). Cambridge University Press.
Qin, X., & Lee, W. (2004). Attack plan recognition and prediction using causal networks. In Annual
Computer Security Application Conference, pp. 370379.
Rieck, K., & Laskov, P. (2007). Language models for detection of unknown attacks in network
traffic. In Journal in Computer Virology.
Saria, S., Nodelman, U., & Koller, D. (2007). Reasoning at the right time granularity. In Uncertainty
in Artificial Intelligence.
Simma, A., Goldszmidt, M., MacCormick, J., Barham, P., Black, R., Isaacs, R., & Mortier, R.
(2008). CT-NOR: Representing and reasoning about events in continuous time. In Uncertainty in Artificial Intelligence.
Soule, A., Salamatian, L., Taft, N., Emilion, R., & Papagiannali, K. (2004). Flow classification by
histogram. In ACM SIGMETRICS.
Soule, A., Salamatian, K., & Taft, N. (2005). Combining filtering and statistical methods for
anomaly detection. In Internet Measurement Conference, pp. 331344.
Tandon, G., & Chan, P. K. (2005). Learning useful system call attributes for anomaly detection. In
The Florida Artificial Intelligence Research Society Conference, pp. 405-410.
Warrender, C., Forrest, S., & Pearlmutter, B. (1999). Detecting intrusions using system calls: Alternative data models. In IEEE Symposium on Security and Privacy, IEEE Computer Society.
Xu, J., & Shelton, C. R. (2008). Continuous time Bayesian networks for host level network intrusion
detection. In European Conference on Machine Learning.
Xu, K., Zhang, Z.-L., & Bhattacharyya, S. (2005). Profiling internet backbone traffic: Behavior
models and applications. In ACM SIGCOMM.
Ye, N., Emran, S. M., Chen, Q., & Vilbert, S. (2002). Multivariate statistical analysis of audit trails
for host-based intrusion detection. IEEE Transactions of Computers, 51(7), 810820.
Yeung, D.-Y., & Chow, C. (2002). Parzen-window network intrusion detectors. In International
Conference on Pattern Recognition.
773

fiX U & S HELTON

Yeung, D.-Y., & Ding, Y. (2002). User profiling for intrusion detection using dynamic and static
behavioral models. Advances in Knowledge Discovery and Data Mining, 2336, 494505.
Zuev, D., & Moore, A. (2005). Internet traffic classification using Bayesian analysis techniques. In
ACM SIGMETRICS.

774

fiJournal of Artificial Intelligence Research 39 (2010) 217-268

Submitted 12/09; published 09/10

Narrative Planning: Balancing Plot and Character
Mark O. Riedl

riedl@cc.gatech.edu

School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA 30332 USA

R. Michael Young

young@csc.ncsu.edu

Department of Computer Science
North Carolina State University
Raleigh, NC 27695 USA

Abstract
Narrative, and in particular storytelling, is an important part of the human experience.
Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. One of the central challenges in
computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. There are many factors  logical and aesthetic  that contribute
to the success of a narrative artifact. Central to this success is its understandability. We
argue that the following two attributes of narratives are universal: (a) the logical causal
progression of plot, and (b) character believability. Character believability is the perception by the audience that the actions performed by characters do not negatively impact
the audiences suspension of disbelief. Specifically, characters must be perceived by the
audience to be intentional agents. In this article, we explore the use of refinement search
as a technique for solving the narrative generation problem  to find a sound and believable
sequence of character actions that transforms an initial world state into a world state in
which goal propositions hold. We describe a novel refinement search planning algorithm 
the Intent-based Partial Order Causal Link (IPOCL) planner  that, in addition to creating
causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why
those characters commit to their goals. We present the results of an empirical evaluation
that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional
partial-order planners.

1. Introduction
Narrative as entertainment, in the form of oral, written, or visual storytelling, plays a
central role in many forms of entertainment media, including novels, movies, television,
and theatre. Narrative is also used in education and training contexts to motivate and to
illustrate. One of the reasons for the prevalence of storytelling in human culture may be due
to the way in which narrative is a cognitive tool for situated understanding (Bruner, 1990;
McKoon & Ratcliff, 1992; Gerrig, 1993, 1994; Graesser, Singer, & Trabasso, 1994). There
is evidence that suggests that we, as humans, build cognitive structures that represent the
real events in our lives using models similar to the ones used for narrative in order to better
understand the world around us (Bruner, 1990). This narrative intelligence (Blair & Meyer,
c
2010
AI Access Foundation. All rights reserved.

fiRiedl & Young

1997; Mateas & Sengers, 1999) is central in the cognitive processes that we employ across
a range of experiences, from entertainment contexts to active learning.
Computational systems that reason about narrative intelligence are able to interact with
human users in a natural way because they understand collaborative contexts as emerging
narrative and are able to express themselves through storytelling. The standard approach to
incorporating storytelling into a computer system, however, is to script a story at design time
and then to have the storys script execute without variation at run-time. For a computer
system to use a scripted story means that the ability of the system to adapt to the users
preferences and abilities is limited. The story scripted into a system may not completely
engage the users interests or may be too challenging for the user to follow. Furthermore,
if stories are scripted at design time, a system can only have a limited number of stories
it can present to the user. In entertainment applications, a limited number of stories or a
limited number of permutations of a single story results in limited opportunities for user
interaction (or limited replay value if the computational system is a computer game). In
educational and training applications, a limited number of stories or a limited number of
permutations of a single story limits the ability of the system to adapt to a learners needs
and abilities.
An alternative approach is to generate stories either dynamically or on a per-session basis
(one story per time the system is engaged). Narrative generation is a process that involves
the selection of narrative content (the events that will be presented to an audience), ordering
of narrative content, and presentation of narrative content through discourse. A system that
can generate stories is capable of adapting narrative to the users preferences and abilities,
has expanded replay value, and is capable of interacting with the user in ways that were
not initially envisioned by system designers. While many entertainment, educational, and
training systems incorporate aspects of storytelling, very few systems exist that generate
novel narrative content in order to support the particular needs and preferences of the user.
The ability to customize narrative content to the user is the primary motivation of the
research effort described in this article.
Narrative content must be understandable, regardless of the purpose of the system that
utilizes a narrative generator and the needs of the system user. Of the many factors  both
logical and aesthetic  that relate to narrative understandability, we focus on two attributes
of narratives we consider to be relatively universal: (a) the logical causal progression of plot
and (b) character believability. Logical progression of plot refers to a property of narrative in
which the central events of the narrative obey the rules of the world in which the narrative
occurs. Character believability (Bates, 1994) is the perception by the audience that the
actions performed by characters do not negatively impact the audiences suspension of
disbelief. Specifically, characters must be perceived by the audience to be intentional agents
(Dennett, 1989). Thus a believable narrative sequence is one in which all characters can be
perceived to be intentional agents.
In this article we describe a narrative generation system that models the fictional narrative creation process as a search-based planning process. The resulting artifact  the
plan  is a description of the temporally ordered sequence of actions that story world characters will perform. This plan, when executed or rendered into natural language, tells
a story. Plans have been found to be good computational representations of narratives
because plans encode attributes central to narrative: action, temporality, and causality
218

fiNarrative Planning: Balancing Plot and Character

(Young, 1999). Unfortunately, solving the planning problem does not also solve the narrative generation problem because planners do not consider many of the logical and aesthetic
properties of narratives. Specifically, planners do not consider character believability. We
describe a novel refinement search planner  the Intent-based Partial Order Causal Link
(IPOCL) planner  that, in addition to creating causally sound plot progression, reasons
about character intentionality by (a) identifying possible character goals that explain their
actions and (b) creating plan structures that explain why those characters commit to their
goals. We begin with a brief background on narrative and lay the theoretical groundwork
for planning-based narrative generation (Section 2). Section 3 discusses related work in narrative generation. In Section 4, we lay out our algorithm, IPOCL, for narrative planning
in detail and illustrate its processing through examples. Finally, in Section 5, we describe
how we evaluated the system.

2. Narrative and Planning
In this section we cover some of the relevant background on narrative from the humanities
and from cognitive psychology. We use the introduced concepts related to narrative to build
an argument for using planning technologies to generate narratives and why off-the-shelf
planners, with their emphasis on goal satisfaction, are insufficient.
2.1 Narrative Background
Narrative and storytelling are terms that are widely understood but not often well defined.
One definition is given here:
Narrative: A narrative is the recounting of a sequence of events that have a continuant
subject and constitute a whole (Prince, 1987).
For a narrative to have a continuant subject and constitute a whole, the events described in
the narrative have a single point or relate to a single communicative goal (Chatman, 1993).
One can, however distinguish between narratives that tell a story and narratives that do
not (Herman, 2002). A narrative that tells a story has certain properties that one comes to
expect. In particular, a story is a narrative that has a plot  the outline of main incidents
in a narrative  that is structured to have a particular effect on the audience over time.
Narratologists break narrative down into two layers of interpretation: fabula and sjuzet
(Bal, 1998). The fabula of a narrative is an enumeration of all the events that occur in the
story world between the time the story begins and the time the story ends. The events in
the fabula are temporally sequenced in the order that they occur, which is not necessarily
the same order in which they are told. The sjuzet of a narrative is a subset of the fabula that
is presented via narration to the audience. If the narrative is written or spoken word, the
narration is in natural language. If the narrative is a cinematic presentation, the narration
is through the actions of actors and the camera shots that capture that action. While it is
the narrated sjuzet that is directly exposed to the audience, it is the fabula of a narrative
that is the content of the narrative, what the narrative is about. In this article, our work
is primarily concerned with the generation of a fabula. We assume that a sjuzet can be
generated from a fabula in a distinct process (e.g., Callaway & Lester, 2002; Young, 2006;
Jhala, 2009; Bae & Young, 2008; Cheong & Young, 2008).
219

fiRiedl & Young

There are many aspects that determine whether a story is accepted by the audience
as good. Many of these aspects are subjective in nature, such as the degree to which the
audience empathizes with the protagonist. Other aspects appear to be more universal
across a wide variety of genres. Cognitive psychologists have determined that the ability
of an audience to comprehend a narrative is strongly correlated with the causal structure
of the story (Trabasso & Sperry, 1985; van den Broek, 1988; Graesser, Lang, & Roberts,
1991; Graesser et al., 1994) and the attribution of intentions to the characters that are
participants in the events (Graesser et al., 1991; Gerrig, 1993; Graesser et al., 1994). Story
comprehension requires the audience (e.g. reader, hearer, viewer) to perceive the causal
connectedness of story events and to infer intentionality of characters. Accordingly, the
two attributes of narrative that we focus on in this work on narrative generation are logical
causal progression and character believability.
The causality of events is an inherent property of narratives and ensures a whole and
continuant subject (Chatman, 1993). Causality refers to the notion that there is a relationship between temporally ordered events such that one event changes the story world
in a particular way that enables future events to occur (Trabasso & van den Broek, 1985).
For a story to be considered successful, it must contain a degree of causal coherence that
allows the audience to follow the logical succession of events and predict possible outcomes.
Attesting to the importance of causality in story, Trabasso and Sperry (1985) found a significant correlation between recall of an event in a story and its existence as part of a causal
chain that terminates in the outcome of the story.
Character believability (Bates, 1994) is the perception by the audience that the actions
performed by characters do not negatively impact the audiences suspension of disbelief.
Character believability is partially dependent on the idiosyncrasies of a characters appearance and physical movements. Physical appearance is very important in visual media such
as animated film (Thomas & Johnson, 1981). Descriptions of character appearances are
also found in written and spoken presentations. Equally important is the way in which the
internal attributes of a character such as personality, emotion, desires, and intentions manifest themselves through the decisions the character makes and the behaviors the character
performs (Thomas & Johnson, 1981; Bates, 1994; Loyall, 1997).1 The definition of character believability places emphasis on the goal-oriented nature of characters. Goal-oriented
behavior is a primary requirement for believability (Loyall, 1997; Charles, Lozano, Mead,
Bisquerra, & Cavazza, 2003). Specifically, we, as humans, ascribe intentionality to agents
with minds (Dennett, 1989). The implication is that if a character is to be perceived as
believable, one should be able to, through observations of the character, infer and predict
its motivations and intentions. In this article, our approach to narrative generation focuses
explicitly on creating narrative sequences in which characters will be perceived to be intentional agents. Other research efforts have directly addressed other aspects of character
believability, including personality (e.g., Carbonell, 1980; Reilly, 1996; Rizzo, Veloso, Miceli,
& Cesta, 1999; Sengers, 2000), emotion (e.g., Gratch & Marsella, 2004; Seif El-Nasr, Yen,
& Ioerger, 2000), and appearance and physical performance (e.g., Blumberg & Galyean,
1995; Maes, Darrell, Blumberg, & Pentland, 1995; Perlin & Goldberg, 1996; Loyall, 1997;
Hayes-Roth, van Gent, & Huber, 1997; Lester, Voerman, Towns, & Callaway, 1999).
1. Loyall (1997) enumerates many of the elements that affect character believability in autonomous agents.

220

fiNarrative Planning: Balancing Plot and Character

2.2 Planning as a Model of Narrative Generation
There are many parallels between plans and narrative at the level of fabula. In particular,
a narrative is a sequence of events that describes how the story world changes over time.
In a fabula, change is instigated by intentional actions of story world characters, although
the story world can also be changed through unintentional acts such as accidents and forces
of nature. Likewise, a plan is a set of ordered operators that transforms a world from one
state to another state. If the operators of a plan are events that can happen in a story
world, then a plan can be a model of a fabula. Partially ordered plans allow operations
to remain temporally unconstrained if their relative execution order does not matter. The
semantics of the plan and the capabilities of the plan execution engine may determine
whether operations can, in fact, be executed in parallel (Knoblock, 1994). Similarly, the
events in a fabula can occur simultaneously in the story world, even though the narration
(e.g., sjuzet) of the events is necessarily linear.
Planners are implementations of algorithms that solve the planning problem: given a
domain theory, an initial state I, and a goal situation G consisting of a set of propositions,
find a sound sequence of actions that maps the initial state into a state where G is true.
The domain theory is a model of how the world can change. For example, one can use
STRIPS (Fikes & Nilsson, 1971) or STRIPS-like operators that specify what operations
can be performed in the world, when they are applicable, and how the world is different
afterward. Various algorithms have been developed that solve planning problems including
partial-order planners, constraint satisfaction planners, and heuristic search planners.
Since a plan can be used as a model of fabula, a planning algorithm can also be used
as a model of the dramatic authoring process that humans use to create narratives. Thus,
the creation of a narrative can be considered a problem solving activity if one considers the
fabula of a narrative to be the sequence of story-world events that achieves some outcome
desired by the author in order to have some effect or impact on an audience.
In this article, we present an algorithm for planning narratives. It specifically solves the
fabula planning problem.

Fabula Planning Problem: Given a domain theory, find a sound and believable sequence
of character actions that transforms an initial world state I into a world state in which
goal propositions G hold.

The domain theory, initial state, and goal situation are provided by the user of the fabula
generation system, whom we call the human author. The fabula generation system is tasked
with selecting and ordering a set of actions that, when told (as opposed to executed), is
considered a narrative.
The algorithm presented in subsequent sections can be considered one example of an
algorithm that solves the fabula generation problem. As with planning algorithms in general, we acknowledge that other algorithms may exist. In the next sections, we explore the
implications of searching for believable narrative plans.
221

fiRiedl & Young

2.2.1 Challenges of Planning as a Computational Model of Narrative
Generation
Algorithms that solve the planning problem find sequences of operations that are sound,
meaning that, in the absence of non-determinism, they are guaranteed to find a sequence of
operations that maps the initial state into a state in which the goal situation holds. When
generating a fabula, we assume that operations are actions to be performed by characters
that exist in a story world. A consequence of the planning problem definition is that
planners do not consider whether it is natural or believable for a character to perform an
action at any given time during the plan; they do not consider actions from the perspective
of the character or the audience, but from the context of whether it is necessary for the goal
situation to be achieved. We argue that this limits the applicability of off-the-shelf planners
as techniques for generating stories.
To illustrate this limitation, we present a simple example. Suppose we describe a world
with three characters: a king, a knight, and a princess. All characters live in a castle and the
castle has a tower in which characters can be locked up. Further suppose a goal situation
has been provided by the human author in which the princess is locked in the tower and
the king is dead. Given a reasonable domain theory  e.g., a set of possible actions  one
plan that can be found by a planner is:
1. The princess kills the king.
2. The princess locks herself in the tower.
This plan is valid from the perspective that it transforms the initial state into a state in
which the goal situation holds. But does it make sense as a story? A reader of this short
story will be left with many questions in mind. Why does the princess kill the king? Why
does the princess lock herself in the tower? Other plans exist that might make more sense,
such as:
1. The king locks the princess in the tower.
2. The knight kills the king.
Intuitively, it is easier for the reader to find an explanation that makes sense of this second
story: the princess must have upset the king and the knight must be avenging the princess.
Let us consider ways we can influence a planner to give us more favorable plans.
One possibility is that we could modify the problem definition. For example, we can
change the initial state and domain theory such that princesses cannot kill kings or that
characters cannot lock themselves in the tower. Do these modifications make sense? Suppose the king were attempting to harm the princess  the princess would be justified in
killing the king. To declare that princesses can never kill kings seems to impose an unnecessarily strong assumption on what narratives can be generated. Likewise, we can imagine
narratives in which it makes sense to lock oneself in a tower (perhaps to escape from danger). One of the advantages of automated generation of narratives is that an algorithm can
explore many possibilities and/or create narratives that were not envisioned by the human
author. This argument is further expanded by Riedl and Young (2006) in the context of
creativity on the part of a computational system.
222

fiNarrative Planning: Balancing Plot and Character

Another possibility is that we provide a heuristic function to a planner that favorably
ranks plans that demonstrate the quality of character believability. Such a heuristic will
increase the probability that the planner find a solution that is believable by ranking plans
as being closer to being a solution when they include actions that create the appearance
of intentionality. For example, a planner with a good heuristic could, in principal, find the
following plan:
1. The princess and the knight fall in love.
2. The king proposes to the princess.
3. The princess refuses the kings proposal.
4. The king locks the princess in the tower.
5. The knight kills the king.
Like the previous example, this plan has the king lock the princess in the tower and then has
the knight kill the king. The inclusion of actions 1 and 3, however, increase the likelihood
that a reader will find this story believable; the princesss refusal explains why the king locks
the princess in the tower (the kings proposal establishes the conditions for the princesss
refusal), and the princess and the knight falling in love explains why princess refuses the
kings proposal and why the knight kills the king.
A good heuristic that ranks on believability, however, only increases the probability that
a complete plan is found that has the desired properties by making it cost less to explore the
portions of the search space in which those solutions are likely to exist. It is still possible
for the planner to return a plan that is not believable in situations where it finds a shorter,
complete solution before it finds the longer, complete, and believable solution. This occurs
because the planning problem is solved when a sound sequence of actions that transforms
the initial state into one in which the goal situation holds.
We conclude that the fabula generation problem is sufficiently different than the planning
problem that if we wish to automatically plan the actions in a fabula, we can benefit from
new definitions for plan completeness and mechanisms for selecting actions that move the
planner toward complete solutions. We consider in greater detail what it means for a
character to appear believable with respect to intentionality, how we can detect character
believability in fabula plans, and how a planner can select actions that directly address both
logical causal progression and believability.
2.2.2 Intentionality in Character Believability
Character believability is partially due to character intentionality in that a story is more
likely to be considered believable if the story world characters appear to be motivated by
individual goals and desires. Stories are likely to be found more comprehensible when there
are well-formed relationships between character actions and recognizable character goals
(Graesser et al., 1991, 1994). However, unlike causal properties of story, there are no structures in plans or processes in planning algorithms that correspond directly to character
intentionality except the goal propositions in the planning problem. However, this is complicated by the fact that stories typically comprise of multiple characters who cannot be
223

fiRiedl & Young

assumed to have the same goals as the human author, or to want to achieve any of the goal
situation propositions provided by the human author.
The goal situation in fabula planning is thus reinterpreted as the outcome of the story.
However, all the propositions of the outcome are not necessarily intended by all story world
characters. Indeed, it is possible that none of the propositions of the goal situation are
intended by any of the story world characters. It is also not necessarily the case that any or
all of the story world characters have declared intentions at the beginning of the story (in
the initial state). That is, characters may only form intentions as a reaction to conditions
in the world or in response to the actions of other characters.
Achieving character intentionality in a fabula planner requires a decoupling of the characters intentions from the intentions of the human author and from the declaration of the
initial state and goal situation. Thus, we distinguish between the author goals (Riedl, 2004,
2009) and character goals. The author goal is a description of the world that the author
would like the fabula generator to achieve. For simplicity, we only consider a single author
goal encoded into the outcome, although it is often advantageous for the human author to
indicate several intermediate situations through which the fabula should pass as means of
providing additional guidance as to what he or she desires in a solution (cf., Riedl, 2009).
Character goals, on the other hand, are the goals that characters are perceived to pursue
through a portion of the overall fabula. Characters goals may be different from the outcome
in the sense that not all characters in a story desire the outcome state or seek to achieve
it. Character goals may also be adopted and resolved throughout a story. For example,
in the previous section the king appears to develop a goal of punishing the princess after
she refuses to marry him. The kings goal neither exists at the beginning of the story nor
persists through to the outcome.
Once agent intentions are decoupled from the initial world state and goal situation,
the planner must assume responsibility for determining character goals  why the agents
are performing the actions in the plan  and motivate those intentions with other actions.
Failure to distinguish between author goals and character goals results in the appearance
of collusion between characters to achieve the outcome situation when it does not makes
sense (e.g., a protagonist and antagonist) and/or to act inconsistently and erratically.2 Our
approach to incorporating character intentionality into narrative planning is described in
Section 4.
To illustrate the decoupling of character intentions, let us inspect the example planning
problem from Section 2.2.1 in more detail. The goal situation has two propositions: (a)
the princess is locked in the tower, and (b) the king is dead. This is an example where the
human authors intentions  that a particular outcome is reached  is not likely to be the
same as reasonable intentions of story world characters. That is, the princess is unlikely
to intend to be locked up and the king is unlikely to intend to be dead. Further, it is not
clear that there is any reason why the princess or the knight should intend that the king
be dead, although these declarations could be made by the human author at initialization
time. It would be reasonable for the reader to see the king appear to fight back against the

2. Sengers (2000) refers to this phenomenon as agent schizophrenia.

224

fiNarrative Planning: Balancing Plot and Character

knight to try to avoid death. But the planning problem is not adversarial in the sense that
there can be any uncertainty about whether the knight will prevail.3
We conclude that a fabula planner must select actions for characters that achieve the
outcome situation and that also create the appearance that the story world characters have
intentions that are potentially distinct from the human authors desires. Since characters
intentions are not necessarily provided in advance by the human author, reasonable goals
for characters must be discovered that explain their behaviors, and fabula structure must
be constructed that illustrates the formation of those intentions. After reviewing related
work, we will describe one algorithm that meets the requirements for solving the fabula
generation problem as laid out earlier.

3. Related Work
Narrative generation systems can often be classified as using one of two approaches to fictional fabula content creation. Simulation-based narrative generation systems (also referred
to as emergent systems in Aylett, 1999, 2000) are those that simulate a story world. The
simulation approach is to establish a set of characters and a world context. The narrative generation system then progressively determines the actions that the characters should
take over time as the situational context evolves. Often simulation-based narrative generation systems employ decentralized, autonomous embodied agents that represent story world
characters and react to the evolving world state. Deliberative narrative generation systems
are those that generate narratives by solving the problem of choosing a sequence of actions 
physical, mental, and dialogue  for all story world characters that meet certain constraints
and parameters (aesthetic, dramatic, or pedagogical). The narrative is the output of this
procedure. The primary distinction to simulation-based approaches is that a deliberative
narrative generation system uses a centralized reasoning algorithm  often a planner  to
determine the optimal actions for all characters. We limit our discussion of related narrative
generation research to how systems produce fabula content.
The simulation-based (emergent) approach to narrative generation is based on the assertion that the best way to generate a narrative is to model the behaviors and decision-making
processes of story world characters. Tale-Spin (Meehan, 1976) is a system that generates
Aesops Fables based on moment-to-moment inference about what each character should
do. The inference engine is based on theories of common-sense reasoning (Schank & Abelson, 1977). Meehan (1977) notes that in circumstances where character goals are not well
chosen or where the facts of the story world do not support the character actions the user
intends, generated narratives can be very short and oddly structured (see Meehan, 1977,
for examples of mis-spun narratives). The Carnegie Mellon University Oz project (Bates,
1992, 1994; Mateas, 1997; Loyall, 1997) uses autonomous, reactive, embodied agents to
represent characters in a virtual world. The agents use shallow and broad (Bates, Loyall,
& Reilly, 1992) decision-making routines to cover a wide repertoire of believable-looking activities. As first proposed by Laurel (1986) and later implemented by Bates and colleagues
(Bates, 1992; Kelso, Weyhrauch, & Bates, 1993; Weyhrauch, 1997), a special agent, called
3. This is an example in which agents be perceived to intentionally strive to avoid the human authors
desired outcome. Since the goal situation is the human authors intention, the goal situation must be
achieved.

225

fiRiedl & Young

a drama manager may be necessary to prevent uninteresting and poorly structured narratives from emerging. A drama manager oversees and coordinates character agent behavior
in order to coerce interesting and well-structured performances out of the autonomous
agents. The I-Storytelling system (Cavazza, Charles, & Mead, 2002) likewise relies on
autonomous, reactive agents to represent story world characters. Unlike the Oz project,
I-Storytelling system agents use hierarchical task network (HTN) planners to achieve
pre-determined goals. Cavazza et al. note that the way in which the virtual world is configured, including the initial position of character agents, and the initial goals of character
agents strongly influences the outcome of the story; poor initial configurations may result
in uninteresting narratives with no conflict.
Dehn (1981) asserts that the process of computational story generation must be a process
that includes the satisfaction of the intentions of the human author. Deliberative narrative
generation systems often consider the process of narrative creation from the perspective of
a singular author that has authority over the resulting narrative structure and is working to
achieve a narrative sequence that conforms to particular given constraints and parameters.
The Universe system (Lebowitz, 1984, 1985, 1987) uses a centralized hierarchical planner
to produce open-ended narrative soap-opera episodes that achieve the narratological intentions of the human author. The human author provides a goal situation that describes the
outcome of a particular episode and the Universe systems planner finds a sequence of
character actions that achieves that goal using hierarchically related task networks describing common activity. In general, hierarchical decomposition requires some form of grammar
or rules. Story grammars such as that by Rumelhart (1975) have been criticized as too restrictive (Black & Wilensky, 1979; Wilensky, 1983). Tailor (Smith & Witten, 1991) uses
state-space search to plan the actions of the storys protagonist. The protagonist is given a
goal to achieve and Tailor searches for a sequence of actions the protagonist can take to
achieve the goal. When an antagonist is present, Tailor uses adversarial search.
More recent work on deliberative narrative generation systems has focused on two areas:
the role of knowledge, and specialized search algorithms. The Minstrel system (Turner,
1994) implements a model of computational creativity based on adaptation and reuse of
existing concepts to create new stories. The Minstrel system uses specialized routines to
transform old stories into new stories. Mexica (Perez y Perez & Sharples, 2001) implements
a model of creative writing (cf., Sharples, 1999) that conceptualizes writing as a cycle
of cognitive engagement and reflection. The model employs a combination of case-based
reasoning  it probes a database of known existing stories for elements that match current
patterns of emotion and tension  and partial-order planning to ensure coherence between
story fragments. The ProtoPropp system (Gervas, Daz-Agudo, Peinado, & Hervas,
2005) uses a case-based reasoning approach to creating narratives. ProtoPropp encodes
examples of Russian folktales from Propp (1968) and functions based on regularities about
folktales identified by Propp into an ontology from which new folktales are created by
retrieving, adapting, and reusing parts of old folktales. More recently Porteous and Cavazza
(2009) have turned to planning narrative structures using a variation of FF (Hoffmann &
Nebel, 2001) to find a sequence of events that brings about a goal situation. The planner
does not consider character goals independent of the goal situation. Instead their generation
algorithm uses landmarks  partially ordered sets of first-order logic literals that must be
made true throughout the course of a solution  as a means of guiding the planner toward
226

fiNarrative Planning: Balancing Plot and Character

a solution consistent with the human authors vision. Landmarks are author goals (Riedl,
2004, 2009).
The goal of our research is to devise a narrative generation system that generates narratives that exhibit both causal coherence and character intentionality. We favor a deliberative
approach to narrative generation because a deliberative approach provides a mechanism for
ensuring that character actions are chosen with global structure in mind. Contrast this to
simulation-based approaches that choose character actions based on temporally localized
information. Deliberative systems avoid problems with logical causal progression because
they consider the narrative sequence as a whole. However, those deliberative narrative generation systems that have been designed to date conflate character goals and human author
goals without consideration for audience  reader, viewer, etc.  perspective.
Our algorithm, the Intent-Driven Partial Order Causal Link (IPOCL) planner, is an
algorithm that solves the fabula generation problem. The IPOCL algorithm is based on a
class of planning algorithms called Partial Order Causal Link (POCL) planners, of which
UCPOP (Penberthy & Weld, 1992; Weld, 1994) is a well known example. POCL planners
represent operators in a plan as STRIPS (Fikes & Nilsson, 1971) or STRIPS-like constructs
consisting of the operator name, a precondition  the conditions that must be true in a
world for an operator to be executable  and an effect  the conditions in the world that
are changed once an operator finishes execution. The precondition and effect consist of
zero or more first-order logic literals. Operators may be parameterized with variables that,
when bound to ground symbols, allows a single operator schema to represent many possible
ground operators. POCL planners use the following definition of a partially ordered plan,
using the term step to refer to operators that are instantiated into the plan structure:
Definition 1 (POCL Plan): A POCL plan is a tuple hS, B, O, Li such that S is a
set of plan steps, B is a set of binding constraints on the parameters of the steps in
S, O is a set of temporal orderings of the form s1 < s2 where s1 , s2  S, and L is a
set of causal links of the form hs1 , p, q, s2 i where s1 , s2  S and p is an effect of s1 and
q is a precondition of s2 and p unifies with q.
Note that we use the term step synonymously with action and operator. This differs
from the usage of the term in literature on non-POCL planners such as SATPLAN and
Graphplan.
POCL planners use an iterative process of identifying flaws in a plan and revising the
plan in a least-commitment manner. A flaw is any reason why a plan cannot be considered
a valid solution. An open condition flaw occurs when the precondition of a step or the goal
situation is not satisfied by the effects of a preceding step or the initial state. A POCL
planner solves for the open condition of the step or goal situation by non-deterministically
choosing existing steps or instantiating new steps that have effects that unify with the goal
conditions. A causal link (Penberthy & Weld, 1992) connects two plan steps s1 and s2 via
p
condition p, written s1 
 s2 , when s1 establishes the condition p in the story world needed
by subsequent step s2 in order for step s2 to execute. Causal links are used to record the
causal relationships between steps and record the satisfaction of open conditions. A causal
threat flaw occurs when the effects of one plan step possibly undo the effects of another plan
step. Causal threats are resolved by explicitly ordering the conflicting steps. We provide
227

fiRiedl & Young

POCL (hS, B, O, Li, F, )
The first parameter is a plan. On the initial call to POCL, there are only two steps in S  the dummy
initial step whose effect is the initial state and the final step. F is a set of flaws. On the initial call, F
contains an open condition flaw for each goal literal in the goal situation. B = L = .  is the set of action
schemata. Output is a complete plan or f ail.
I. Termination. If O or B is inconsistent, fail. Otherwise, if F is empty, return hS, B, O, Li.
II. Plan Refinement.
1. Goal selection. Select an open condition flaw f = hsneed , pi from F . Let F 0 = F  {f }.
2. Operator selection. Let sadd be a step that adds an effect e that can be unified with p (to
create sadd , non-deterministically choose a step sold already in S or instantiate an action schema
in ). If no such step exists, backtrack. Otherwise, let S 0 = S {sadd }, O0 = O {sadd < sneed },
B 0 = B  Bnew where Bnew are bindings (e.g., assignments of ground symbols to variables)
needed to make sadd add e, including the bindings of sadd itself, and L0 = L{hsadd , e, p, sneed i}.
If sadd 6= sold , add new open condition flaws to F 0 for every precondition of sadd .
3. Threat resolution. A step sthreat threatens a causal link hsj , e, p, sk i when it occurs between
sj and sk and it asserts e. For every used step sthreat that might threaten a causal link
hsj , e, p, sk i  L0 , non-deterministically do one of the following.
 Promotion. If sk possibly precedes sthreat , let O0 = O0  {sk < sthreat }.
 Demotion. If sthreat possibly precedes sj , let O0 = O0  {sthreat < sj }.
 Separation. Let O0 = O0 {sj < sthreat , sthreat < sk } and let B 0 = B 0  the set of variable
constraints needed to ensure that sthreat wont assert e.
III. Recursive invocation. Call POCL (hS 0 , B 0 , O0 , L0 i, F 0 , ).

Figure 1: The POCL algorithm.

the POCL planning algorithm in Figure 1 as a point of comparison for later discussion of
our fabula planning algorithm.
In the next section, we introduce our algorithm, the Intent-Driven Partial Order Causal
Link (IPOCL) planner, which creates narratives that, from the perspective of a reader, more
closely resemble the results of an emergent narrative generation system with regard to character intentionality. Specifically, IPOCL is a modification of existing search-based planning
algorithms to support character intentionality independent of author intentions. The goal
is to generate narratives through a deliberative process such that characters appear to the
audience to form intentions and act to achieve those intentions as if they were simulated. In
this way, IPOCL can produce narratives that have both logical causal progression, meaning
that they achieve author-indicated outcomes states, and have believable characters.

4. Intent-Driven Planning
The definition of character believability in this work is constrained to focus on the perceived
intentionality of character behavior in the story world. Perceived intentionality refers to
the way in which characters are observed by an audience to have goals and to act to achieve
those goals. In the context of computational storytelling systems, it is not sufficient for
a character to act intentionally if the audience is not capable of inferring that characters
228

fiNarrative Planning: Balancing Plot and Character

intentions from the circumstances that surround the character in the story world. The
audience of a story is not a collection of passive observers. Instead, the audience actively
performs mental problem-solving activities to predict what characters will do and how
the story will evolve (Gerrig, 1993). It makes sense, therefore, to reason about character
intentions and motivations at the time of generation from the perspective of the audience.
This will ensure that every character action considered for inclusion in the narrative will
appear motivated and intentional.
The Intent-Driven Partial Order Causal Link (IPOCL) planner that generates fabula
plans in which characters act intentionally and in which that intentionality is observable.
IPOCL extends conventional POCL planning to include an expanded plan representation,
definition of plan completeness, and action selection mechanisms that facilitate a fabula
planner to search for a solution in which the authors goal is achieved (e.g., the outcome)
and all characters appear to act intentionally. Conventionally, planners are means-ends
tools for solving problems. When employing a planning system to generate a fabula, the
system must produce the actions that make up the plot line of a story, along with a temporal
ordering  partial or total  over the execution times of those actions. We make the following
observations about the conventional planning problem:
 Plans being generated are created by or for a single agent (Bratman, Israel, and
Pollack, 1988) or for a collection of cooperating agents (Grosz & Sidner, 1990).4
 The goal situation is intended by one or more of the character agents and all agents
intend to execute a plan in support of achieving the goal state.
In order to facilitate the active mental processes of the audience suggested by Gerrig, we
observe that solving the fabula planning problem requires the following:
 Plans being generated are created for multiple character agents that are not necessarily
cooperating but also not necessarily adversarial.
 The goal situation describes properties of the world that are not necessarily intended
by any of the character agents that are to execute the plan.
The goal situation for the conventional planning problem is a partial description of the world
state that will be obtained at the end of the plans execution. In the context of narrative
planning, we refer to the goal situation as the outcome because it describes how the world
must be different after the narrative is completed.
The fabula generation algorithm described in the remainder of this section searches the
space of plans in which individual agent goals are potentially distinct from the outcome
4. The SharedPlans (Grosz & Sidner, 1990) formalism addresses the situation where more than one agent
collaborates to construct a joint plan for achieving some goal. Grosz and Sidners approach addresses
the cases where all agents intend that a joint goal is achieved or where one agent has a goal and
contracts out part of the task to another agent by communicating its intentions (Grosz & Kraus, 1996).
SharedPlans address the coordination of many individual plans into a single joint plan by defining how
agent intentions to perform actions and agent intentions that goals and sub-goals be achieved constrain
the behaviors of the individual agents working together. The formalism, however, does not address
situations where agents have different goals and are cooperating or contracting out.

229

fiRiedl & Young

ACTION ::= ACTION-NAME (VARIABLE )
actors: VARIABLE
happening: BOOLEAN
constraints: LITERAL
precondition: LITERAL
effect: LITERAL
LITERAL := PREDICATE ([VARIABLE | SYMBOL] )

Figure 2: Syntax for action schemata in IPOCL.
and in which agents are not necessarily cooperating. Character agents can either be given
intentions as part of the specification of the initial world state or develop them during
the course of the plan. The IPOCL planning algorithm accomplishes this by expanding
the representation of the plan structure to include information about the intentions of the
individual agents. Algorithmically, IPOCL simultaneously searches the space of plans and
the space of agent intentions. At any point in the process, agent intentions are ensured to be
plausible through the use of a special reasoning process that tests for character intentionality
from the perspective of the audience and attempts to revise the plan if the test fails.
4.1 Extensions to the Planning Problem Definition Language
The IPOCL planning problem is given in Definition 2.
Definition 2 (IPOCL Planning Problem): An IPOCL planning problem is a tuple,
hI, A, G, i, such that I is the initial state, A is a set of symbols that refer to character
agents, G is the goal situation, and  is a set of action schemata.
A significant factor in the IPOCL planning problem is A, the set of symbols that refer to
character agents in the world. These symbols are handled specially in processes determining
character intentionality. We have extended the traditional planning problem definition
language to use the character agents in two ways:
 Specification of which actions do not need to be intentional.
 Specification of which parameters of an action refer to the character agents that will
be intentionally performing the action.
The syntax for specifying action schemata for IPOCL is given in Figure 2. As with other
POCL planners, we use a STRIPS-like representation with preconditions and effects. Additionally, constraints are literals that must unify with those in the initial state and act as
a filter on applicable parameter bindings.
We distinguish between two types of actions: happenings (Prince, 1987) and nonhappenings. Happenings are actions that can occur without the intention of any character
such as accidents, involuntary reactions to stimuli, and forces of nature. Non-happening
events must be intended by a character. For clarity, we assume that, unless indicated otherwise, all actions are non-happenings and thus require an actor for whom the action fulfills
230

fiNarrative Planning: Balancing Plot and Character

Action: slay (?slayer, ?monster, ?place)
actors: ?slayer
constraints: knight(?slayer), monster(?monster), place(?place)
precondition: at(?slayer, ?place), at(?monster, ?place), alive(?slayer), alive(?monster)
effect: alive(?monster)
Action: marry (?groom, ?bride, ?place)
actors: ?groom, ?bride
constraints: male(?groom), female(?bride), place(?place)
precondition: at(?groom, ?place), at(?bride, ?place), loves(?groom, ?bride),
loves(?bride, ?groom), alive(?groom), alive(?bride)
effect: married(?groom), married(?bride), single(?groom), single(?bride),
married-to(?groom, ?bride), married-to(?bride, ?groom)
Action: appear-threatening (?monster, ?char, ?place)
actors: ?monster
happening: t
constraints: monster(?monster), character(?char), place(?place)
precondition: at(?monster, ?place), at(?char, ?place), scary(?monster), ?monster6=?char
effect: intends(?char, alive(?monster))

Figure 3: Example IPOCL action schemata.
an intention. If there are actions that can occur in the world without intent (for example,
falling down the stairs), they are marked by specifying the happening slot as true.
If the action is a non-happening, the actors slot specifies which of the parameters refer
to symbols representing characters that are acting intentionally to enact the particular
action. We say that an action is to be performed by character agent a when the actors
slot of the action references a. Figure 3 shows three action schemata involving a single
intentional actor, multiple intentional actors, and no intentional actor, respectively. The
action schema for Slay(?slayer, ?victim, ?place) specifies that ?slayer will refer to
the intentional actor. Note the implication that slaying cannot be performed accidentally.
The action schema for Marry(?groom, ?bride, ?place) specifies two intentional actors,
?groom and ?bride. Finally, Appear-threatening(?monster, ?char, ?place) indicates
that ?char will appear to be become frightened by a monster. This action does not need
to be intentional on the part of the ?monster or ?char.
A final note on action definition is the use of the special intends predicate, which can
only be used in the effect of an action. Semantically the intends predicate should be read
as meaning it is reasonable for a character to have the following goal as a response to
the action. Whether the intention is acted upon is determine by whether the proposition is
used, as described in the next section. When the narrative is told, there will be no mention
of facts that are unused. The intends predicate only occurs in action effects; it is not used in
action preconditions because that creates a strong commitment to how actions can be used.
For example, if Marry were to require intends(?groom, married-to(?groom, ?bride)),
it would preclude stories in which a character wants to not be single (but doesnt necessarily
want to be married) and also stories in which a character marries someone as revenge against
231

fiRiedl & Young

a third party (the marriage is just one action in a chain leading up to another goal that is
none of the actions effects).
4.2 Character Intentionality in Fabula Planning
Because a storys audience actively performs problem-solving as the story progresses in order
to predict the outcome and the fate of story world characters, a generated story should
support these cognitive processes. This means providing narrative structure that gives
enough information for the audience to infer the intentionality of character behavior. From
the fabula planners perspective, all character actions should be intentional (or happenings).
That is, for every character goal, a portion of the actions in the complete fabula plan describe
the actions to be performed by that character to achieve the character goal. We formalize
this as follows:
Definition 3 (Frame of Commitment): A frame of commitment is a tuple,
hS 0 , P, a, ga , sf i, such that S 0 is a proper subset of plan steps in a plan P = hS, B, O, Li,
a is a symbolic reference to a character agent such that the character agent is the actor
of all steps in S 0 , ga is a goal that character agent a is pursing by executing the steps
in S 0 , and sf  S 0  referred to as the final step  has ga for one of its effects and all
other steps in S 0 temporally precede sf in the step ordering O of plan P .
The purpose of the frame of commitment is to record a characters internal character goal
ga and the actions of the plan that the character will appear to perform during storytelling to
achieve that goal. However, from the perspective of the audience, it is not enough to declare
a character as having a goal; in order to make inferences about character intentions and
plans, the audience must observe the characters forming and committing to goals. Therefore,
each frame of commitment is associated with a condition, eg , of the form intends(a, ga ),
which indicates that for a character to commit to an internal character goal, a must be in
a state where it is reasonable to intend ga . The condition eg must be established in the
world by some plan step that has eg as an effect. That is, something in the world causes
character a to commit to ga . The plan step that causes eg and consequently causes the
frame of commitment is referred to as the motivating step for the frame of commitment. The
motivating step necessarily temporally precedes all plan steps in the frame of commitment.
Informally, the interval of intentionality is the set of actions S 0 that character a will perform
to achieve the internal character goal, ga .5
Character goals partially describe a world state that the character commits to achieving.
Commitments persist through time and a character will remain committed to the goal even
though the desired world state is undone (Bratman, 1987). IPOCL does not explicitly
represent the release of a commitment except to say that the interval of intentionality is
bounded temporally by the set of steps in the interval of intentionality.
5. An interval of intentionality roughly equates to the notion of a Full Individual Plan (FIP) in the SharedPlans formulation (Grosz & Sidner, 1990). A full individual plan is a portion of the larger Full Shared
Plan (FSP) that a single agent is responsible for executing. The distinction between a fabula plan and
an FSP is that the full fabula plan is not made up of many individual FIPs generated by collaborating
planning agents. Instead, a fabula plan is constructed as a whole and the individual character actions
that make up the whole plan are annotated to indicate what intention they might be used to achieve.

232

fiNarrative Planning: Balancing Plot and Character

Frame of commitment for a1
with goal ga1

intends(a1, ga1)

Intention level
Domain level
s5 (a2)

s3 (a1)

s2 (a1)

p2

ga1

s1 (a2)

p3

s4 (a1)

Figure 4: An IPOCL plan with a single frame of commitment and motivating step.
Character goals are captured in two ways. First, potential character intentions are
recorded in world states through the existence of world state propositions of the form
intends(a, ga ). These world state propositions record the fact that a character can have
an intention, but do not indicate whether an intention is acted upon, nor do they capture
which subsequent actions are executed in order for that character to act on the intention.
Second, character intentions are recorded in frame of commitment data structures. Frames
of commitment elaborate on the intention by also identifying which actions in the fabula
plan the character is to perform in pursuit of the intention. Note that an interval of
intentionality can contain more than one step with ga as an effect. This is necessary in the
case where another action in the fabula plan undoes ga in the world and the condition must
be reestablished.
IPOCL extends the definition of the POCL plan data structure to include frames of
commitment. The definition of an IPOCL plan is as follows.
Definition 4 (IPOCL Plan): An IPOCL plan is a tuple hS, B, O, L, Ci where S is a
set of plan steps, B is a set of binding constraints on the free variables in the steps
in S, O is the set of ordering constraints on the steps in S, L is a set of causal links
between steps in S, and C is a set of frames of commitment.
The sets S, B, O, and L are defined in the standard way (e.g., Penberthy & Weld, 1992).
The frames of commitment in C are defined in Definition 3. See Figure 4 for an illustration
of an IPOCL plan with a single frame of commitment and a motivating step for that frame.
The actor of each step si is indicated in parentheses. The IPOCL algorithm ensures that
all story world characters that participate in a fabula plan appear to act believably with
respect to intentionality. To satisfy this requirement, all character actions in an IPOCL
plan (except those marked as not needing to be intentional) must be intentional in the final
solution plan or happenings.
Definition 5 (Action Intentionality): An action in plan P is intentional if it belongs
to a frame of commitment in P .
Unintentional actions are not part of any interval of intentionality and are referred to as
orphans. In order for an IPOCL plan to be considered complete, all actions  except for
233

fiRiedl & Young

happenings  must be part of at least one frame of commitment. A character action can
belong to more than one interval of intentionality. The definition of plan completeness is as
follows:
Definition 6 (Complete IPOCL Plan): An IPOCL plan is complete if and only
if (1) all preconditions of all plan steps are established, (2) all causal threats6 are
resolved, and (3) all plan steps that are not happenings are intentional.
Conditions 1 and 2 together make up the conventional definition of plan completeness,
which can be termed causally complete. A fabula plan in IPOCL can be causally complete
without being fully complete under Definition 6. When a plan is causally complete but
not fully complete, then the plan contains orphans. If there are no ways to correct for
the orphans, IPOCL backtracks to find another possible complete solution plan. Taken
together, Definitions 4 and 6 directly address the high-level problem of finding a sound and
believable sequence of actions that transforms an initial world state into a world state in
which a goal situation holds.
4.3 Integrating Intentionality into Least-Commitment Planning
Frames of commitment are products of a process in which the planner tests the intentionality
of character actions and revises the plan if necessary. IPOCL, as a refinement search process,
uses an iterative, least-commitment process of identifying flaws in a plan and revising the
plan to repair the flaws. This creates a tree-like search space in which leaf nodes are either
complete plans (under Definition 6) or incomplete plans that cannot be repaired. Internal
nodes are incomplete plans that have one or more flaws.
In addition to open conditions and causal threat flaws adopted from POCL we define
three additional types of flaws:
Definition 7 (Open Motivation Flaw): An open motivation flaw in plan P is a
tuple, hc, pi, such that c is a frame of commitment in P and p is the sentence intends(a,
ga ) such that a is the character of c and ga is the internal character goal of c.
Definition 8 (Intent Flaw): An intent flaw in plan P is a tuple hs, ci where s is a
p
step in P and c is a frame of commitment in P such that s 
 sj is a causal link in
the plan, s is not part of c, and sj is a step in P , is part of c, and the character of s
is the same as the character of sj and c.
Definition 9 (Intentional Threat Flaw): An intentional threat flaw in plan P is a
tuple, hck , ci i, such that frame of commitment ck has an internal character goal that
negates the internal character goal of another frame of commitment ci .
Open motivation flaws reflect the fact that characters must appear motivated to have goals.
That is, something must cause a character to commit to a goal. An open motivation
flaw means that a plan has a frame of commitment whose interval of intentionality is not
preceded by a motivating step. Intent flaws reflect the fact that a plan step, s, to be
6. A causal threat occurs when, due to insufficient constraints of action ordering in a partially ordered plan,
the effects of one action can potentially undo the preconditions of another action.

234

fiNarrative Planning: Balancing Plot and Character

performed by a character can be part of a frame of commitment, c, held by that same
character. That is, step s causally establishes a precondition of some other step, sj , which
is part of c. The planner must non-deterministically decide whether the step is part of the
frame of commitment. The next sections describe algorithms for identifying and repairing
open motivation flaws and intent flaws. To facilitate this, the IPOCL algorithm, shown in
Figure 5, is broken up into three parts: causal planning, motivation planning, and intent
planning.
4.3.1 Causal Planning in IPOCL
The causal planning portion of the IPOCL algorithm implements the conventional POCL
algorithm with the addition of a frame of commitment discovery phase. Causal planning
occurs when there is an open condition that needs to be resolved. That is, some step sneed
has a precondition p that is not satisfied by any causal link. The planner chooses a plan
step sadd whose effect e can unify with p. This is accomplished by non-deterministically
choosing an existing plan step or by instantiating a new action.
The frame of commitment discovery process is triggered by the changes in the plan (e.g.
the addition of a causal link to the plan structure). If sadd is a newly instantiated step, then
there is the possibility that it is the final step (due to the backward-chaining nature of the
planning algorithm) of some previously undiscovered character intention. If this is the case,
then one of the effects of sadd , in addition to causally satisfying some open condition, is
intended by the character specified to perform sadd . IPOCL non-deterministically chooses
one of the effects of sadd (or no effect, in the case where sadd is not the final step of some
yet-to-be-discovered intention). If an effect is chosen, then a new frame of commitment
is constructed to record the characters commitment to achieving that effect in the world.
Step sadd is made to be the final step of the frames interval of intentionality and a new
open motivation flaw annotates the plan to indicate that the planner must find a motivating
step.
Regardless of whether sadd is newly instantiated or an existing plan step that is reused,
the planner must consider the possibility that sadd is part of an existing interval of intentionality. Steps can be performed as part of more than one intention; Pollack (1992) refers
to this as overloading. IPOCL performs a search of the plan node for frames of commitment
that sadd can be part of. The search routine finds a set of frames C 00 such that cj  C 00
when one of the two following conditions holds:
p

1. The frame of commitment cj contains step sj such that sadd 
 sj is a causal link in
the plan and sadd and sj are to be performed by the same character.
2. The frame of commitment cj contains step sj such that some frame ci 
/ C 00 is in
service of sj and sadd is a motivating step for ci . Frame ci is in service of step sj if
the final step of ci has an effect that establishes a precondition of sj , and sj is part of
frame ck , and ck 6= ci .
For each frame of commitment ci  C 00 , the plan is annotated with an intent flaw hsadd , cj i.
By resolving these flaws, the planner will determine whether step sadd becomes part of an
existing frames interval of intentionality.
235

fiRiedl & Young

IPOCL (hS, B, O, L, Ci, F, )
The first parameter is a plan, with steps S, variable bindings B, ordering constraints O, causal links L,
and frames of commitment C. F is a set of flaws (initially open conditions for each literal in the goal
situation).  is a set of action schemata. Output is a complete plan according to Definition 6 or f ail.
I. Termination. If O or B are inconsistent, fail. If F is empty and s  S, c  C | s is part of c,
return hS, B, O, L, Ci. Otherwise, if F is empty, fail.
II. Plan Refinement. Non-deterministically do one of the following.
 Causal planning
1. Goal selection. Select an open condition flaw f = hsneed , pi from F . Let F 0 = F  {f }.
2. Operator selection. Let sadd be a step that adds an effect e that can be unified with
p (to create sadd , non-deterministically choose a step sold already in S or instantiate an
action schema in ). If no such step exists, backtrack. Otherwise, let S 0 = S  {sadd },
O0 = O  {sadd < sneed }, B 0 = B  Bnew where Bnew are bindings (e.g., assignments of
ground symbols to variables) needed to make sadd add e, including the bindings of sadd
itself, and L0 = L  {hsadd , e, p, sneed i}. If sadd 6= sold , add new open condition flaws to F 0
for every precondition of sadd .
3. Frame discovery. Let C 0 = C.
a. If sadd 6= sold , non-deterministically choose an effect e of sadd or e = nil. If e 6=
nil, construct a new frame of commitment c with internal character goal e and the
character of sadd , let sadd be part of c, let C 0 = C  {c}, create a new open motivation
flaw f = hci, and let F 0 = F  {f }.
b. Let C 00 be the set of existing frames of commitment that can be used to explain sadd .
For all d  C 00 , create an intent flaw f = hsadd , di and let F 0 = F  {f }.
4. Threat resolution
 Causal threat resolution. Performed as in II.3 in the POCL algorithm (Figure 1)
 Intentional threat resolution. For all c1  C 0 and c2  C 0 , such that the character
of c1 is the same as the character of c2 , e1 is the goal of c1 , and e2 is the goal of c2 , if
e1 negates e2 , non-deterministically order c1 before c2 or vice versa and for all s1  c1
and all s2  c2 , O0 = O0  {s1 < s2 } or O0 = O0  {s2 < s1 }.
5. Recursive invocation. Call IPOCL (hS 0 , B 0 , O0 , L0 , C 0 i, F 0 , ).
 Motivation planning
1. Goal selection. Select an open motivation flaw f = hci from F . Let p be the condition
of c. Let F 0 = F  {f }.
2. Operator selection. Same as causal planning above, except
si  c, O0 = O0  {sadd < si }.
3. Frame discovery. Same as for causal planning, above.
4. Threat resolution. Same as for causal planning, above.
5. Recursive invocation. Call IPOCL (hS 0 , B 0 , O0 , L0 , C 0 i, F 0 , ).
 Intent planning
1. Goal selection. Select an intent flaw f = hs, ci from F . Let F 0 = F  {f }.
2. Frame selection. Let O0 = O. Non-deterministically choose to do one of the following.
 Make s part of c. Let sm be the motivating step of c. O0 = O0  {sm < s}. For all
ci  C such that ci is ordered with respect to c, then for all si  ci , O0 = O0  {si < s}
or O0 = O0  {s < si }. For each spred  S such that hspred , p, q, si  L and spred and
s have the same character, create an intent flaw f = hspred , ci and let F 0 = F 0  {f }.
 Do not make s part of c.
3. Recursive invocation. Call IPOCL (hS, B, O0 , L, Ci, F 0 , ).

Figure 5: The IPOCL algorithm.
236

fiNarrative Planning: Balancing Plot and Character

f1: Frame of commitment for a1 with goal ga1
f2: Frame of commitment
for a2 with goal ga2

intends(a2, ga2)

Intention level
Domain level
s2 (a1)

s4 (a2)

p3

s3 (a2)

ga2

s1 (a2)

Figure 6: An IPOCL plan where one character is contracted out by another character.
Condition 1 indicates that if two actions, sj and sadd , are to be performed by the same
character and the earlier action, sadd , establishes some condition in the world required
for the later action, sj , then a reasonable hypothesis is that both were part of the same
intention. The intent flaw on the earlier action indicates that the planner must, at some
point, decide whether to support this hypothesis by incorporating the actions into the
same interval of intentionality or to reject the hypothesis by leaving the plan structure
unchanged. Condition 2 indicates the situation where an agent requires a certain world
state to be achieved to make its intentional actions feasible and this sub-goal is contracted
out (e.g., Grosz & Kraus, 1996) to another agent. This occurs when a motivating action
to be performed by one character causes another character to have an intention that is in
service of the first characters actions, as illustrated in Figure 6. Character a1 is to perform
action s1 in pursuit of goal ga1 . Action s1 has a single precondition that is satisfied by an
action s3 performed by character a2 in pursuit of goal ga2 . Action s2 is the motivating action
that causes character a2 to have the goal to establish the precondition of step s1 . Since the
motivating step is to be performed by character a1 , it is a candidate under Condition 2 to
be incorporated into the frame of commitment of a1 .
Once frame discovery takes place, the planner must resolve any threats that were inadvertently introduced into the refined plan. There are two types of threats: causal threats
and intentional threats. The standard POCL means of detecting and correcting causal
threats is used (see Section 3). Intentional threats occur when a newly instantiated frame
of commitment, ck , has an internal character goal that negates the internal character goal
of some other frame of commitment ci for the same character. Character actions in the
fabula plan may be unordered with respect to one another and this allows for intervals of
intentionality that are interleaved. While it is possible for an agent  or character  to
hold conflicting desires, it is not rational for an agent to concurrently commit to conflicting
desires (Bratman, 1987). In the case that a character has two frames with goals that negate
each other, IPOCL corrects intentional threats by non-deterministically constraining the
ordering of ci and ck . The ordering of frames of commitment amounts to explicitly ordering
the actions that are part of each frame to correspond to the ordering of ci and ck . For more
complicated cases in which the goals of ci and ck do not negate each other but in which
plans causally interfere with each other, IPOCL relies on standard causal threat resolution
to either order the action sequences of each plan while leaving the frames unordered, or
237

fiRiedl & Young

to force the algorithm to backtrack. Some cases will not be identified or repaired without
additional semantic and contextual reasoning.
4.3.2 Motivation Planning in IPOCL
The motivation planning portion of the IPOCL algorithm is responsible for ensuring that
characters in the story world are motivated. A motivating step is a plan step in which one
of its effects causes a character to commit to a goal. Repairing an open motivation flaw
consists of non-deterministically finding a plan step with effect intends(a, ga )  either by
choosing an existing plan step or by instantiating an action schema and explicitly ordering
that step before the plan steps that are part of the frame of commitment. Motivation
planning is similar to causal planning except instead of establishing a causal link between
two plan steps, it establishes a motivation link between a motivating step and a frame of
commitment. Additionally, the motivating step for a frame of commitment is explicitly
ordered before all other steps in the frames interval of intentionality. In the work presented
here, a character agent cannot begin pursuing a character goal before it has committed to
the goal. Motivation planning involves frame discovery and threat resolution phases that
are identical to causal planning.
4.3.3 Intent Planning in IPOCL
The intent planning portion of the IPOCL algorithm determines interval membership for
all character actions except those that are final steps for their intervals of intentionality.
Intent planning repairs intent flaws. An intent flaw is a decision point that asks whether
a plan step s should be made part of the interval of some frame of commitment c. Unlike
other flaws that are repaired by refining the structure of the plan, intent flaws are resolved
by non-deterministically choosing one of the following:
 Make step s part of the interval of c and refine the plan structure to reflect the
association.
 Do not make step s part of the interval of c, remove the flaw annotation, and leave
the plan structure unchanged.7
When the former is chosen, step s becomes part of the interval of intentionality of frame
c. When this choice is made, the interval of frame c is updated appropriately and s is
explicitly ordered after the motivating step of frame c. Furthermore, the change in the
steps membership status can have an effect on the membership of plan steps that precede
s. Let spred be an establishing step of s  a step that precedes s and is causally linked to
s. The inclusion of step s in the interval of frame c also makes it possible for establishing
steps to be included in the interval of c if the following conditions hold:
 Step spred is to be performed by the same character as s.
7. Because an intent flaw can be addressed by not making any revisions to the plan structure, an intent
flaw is not strictly a flaw in the conventional sense. However, for the purpose of maintaining consistency
with existing revision mechanisms, we find it useful to treat an intent flaw as a flaw up until the point
that it is repaired.

238

fiNarrative Planning: Balancing Plot and Character

 Step spred is not a part of the interval of intentionality of c.
 The intent flaw, f = hspred , ci has not already been proposed and/or resolved.8
Intent flaws are created for each establishing step for which all three conditions hold. Intent planning thus operates in a spreading activation fashion. When one step becomes a
member of a frame of commitment, an entire sequence of establishing steps may follow.
This approach is necessary since frames of commitment can be created at any time during
plan refinement. Intent flaws are not standard flaws in the sense that they mark a potential
flaw instead of an actual flaw. We cannot determine at the time an action is instantiated
whether it is necessary for that action to be part of an interval of intentionality. The frame
of commitment it should belong to may not have been discovered yet, or it may not yet have
been discovered that the action can be part of a frame of commitment due to adjacency
requirements.
The propagation of intent flaws makes it possible for plan steps to become members of
more than one frame of commitment, which is a desirable property of the IPOCL algorithm.
Every time a character action  belonging to one frame of commitment  is used to satisfy
an open condition of a successor action that belongs to a different frame of commitment, the
system must non-deterministically decide whether the establishing action belongs to both
frames of commitment or remains only a member of its original frame. The decision about
interval membership also constrains the possible ordering of motivating steps for the frames
of commitment involved because motivating steps are temporally ordered before all actions
in the frame of commitment that the motivating step establishes. When a step becomes a
member of more than one frame of commitment, the possible placement of motivating steps
is constrained as in Figure 7 because the motivating step must occur before the earliest step
in a frame of commitment.
One thing we have not yet discussed is how to handle orphans. An orphan is a step in
the plan that does not belong to any interval of intentionality. Orphans are surreptitiously
repaired when they are adopted into intervals of intentionality. This can happen when they
causally establish conditions necessary for other, intentional actions. Orphaned actions
cannot be repaired directly because frames of commitment are discovered opportunistically
instead of instantiated in a least-commitment approach (as plan steps are). If there are
orphans remaining that have not been surreptitiously repaired by the time the planning
process completes, then the planner must backtrack.
4.4 An Example
The IPOCL algorithm is illustrated by the following story about an arch-villain who bribes
the President of the United States with a large sum of money. The example traces a
single path through the fabula plan search space generated by IPOCL. The initial plan
node contains only the initial state step and goal situation step. The initial state contains
propositions describing the state of the world before the story begins. The goal situation
contains a single proposition, corrupt(President), which describes what must be different
8. The inclusion of this condition ensures the systematicity of the algorithm since there can be more than
one causal link between spred and s. A search algorithm is systematic if it is guaranteed to never duplicate
a portion of the search space.

239

fiRiedl & Young

f1: Frame of commitment for a
Intention level
Domain level
s8

s5

s4: Pickup (a, gun)

s3: Load (a, gun)

s9

s2

s1: Shoot (a, deer, gun)

s7

s2: Rob (a, bank, gun)
Domain level
Intention level

f2: Frame of commitment for a

Figure 7: An IPOCL plan with overlapping intervals of intentionality for a single character.

about the world after the story is complete. The story that will be generated by IPOCL is,
in effect, the story about how the President becomes corrupt.
The goal proposition corrupt(President) is non-deterministically established by instantiating a new character action, Bribe(Villain, President, $), which states that the
Villain character will bribe the President character with some money. The Bribe action
was chosen because it has corrupt(President) as an effect. From the planners perspective, the Bribe action is causally motivated by the open condition of the goal situation.
Upon instantiation of the Bribe action, frame discovery is invoked. The effects of the Bribe
action are:
 corrupt(President)  the President is corrupt.
 controls(Villain, President)  the Villain exerts control over the President.
 has(President, $)  the President has the money.
 has(Villain, $)  the Villain does not have the money.
From the audiences perspective, any of these effects can be a reason why the Villain performs the actions in the story.
The planner non-deterministically chooses controls(Villain, President) as the character goal for the Villain character. Note that in this case the goal of the Villain differs from
the outcome of the story although the same action satisfies both conditions. There is no
reason why the planner could not have chosen corrupt(President) as the character goal
for the Villain. It is assumed here that either the plan cannot be completed if the alternative is chosen or that some heuristic function has evaluated all options and determined that
villains are more likely to want control over the President than anything else. Given the
choice made, the planner constructs a frame of commitment for the Villain character and
240

fiNarrative Planning: Balancing Plot and Character

Frame of commitment for Villain with goal
controls(vil, prez)
intends(vil, controls(vil, prez))

Intention level
Domain level
init

has(vil, $) Bribe (vil, prez, $)

corrupt(prez)

goal

Figure 8: Example narrative plan after discovering the one action and corresponding frame
of commitment.

makes the Bribe action the final step in the frames interval of intentionality. Even with
the new frame of commitment, the plan is still flawed since there is no reason for the Villain
character to have the goal of controlling the President. That is, the Villain needs to form
the intention to appear believable to the audience. An open motivation flaw indicates that
some action in the plan must satisfy the condition intends(Villain, controls(Villain,
President)) on the frame of commitment.
Since there are no other frames of commitment for the Villain, no intent flaws occur.
The Bribe action, however, has a precondition has(Villain, $) that becomes an open
condition; the Villain character must have the money if he is to bribe the President with
it. The planner chooses to repair the open motivation flaw on the single frame of commitment first and non-deterministically chooses the initial state to satisfy the open motivation
condition. This illustrates a situation where the intention of a character in the story world
is encoded as part of the initial conditions. While it does not have to be this way, the
domain engineer that specified the inputs to IPOCL has decided that no further motivation
for the Villain to want to control the President is needed. While this may not be the most
satisfactory solution, it is a valid solution. The partial plan at this point is shown in Figure
8.
The open condition has(Villain, $) on the Bribe action is considered next. To repair
this flaw, the planner non-deterministically instantiates a new character action Give(Hero,
Villain, $) in which the Hero character gives the Villain the money. The planner must
consider, from the audiences, perspective, why the Hero character gives the money to the
Villain. The planner inspects the effects of the Give action:
 has(Villain, $)  the Villain has the money.
 has(Hero, $)  the Hero does not have the money.
The planner non-deterministically chooses has(Villain, $) as the goal that the Hero is
attempting to achieve. A new frame of commitment for the Heros goal is created. Note
that the Heros intention matches the open condition that the Give action was instantiated
to satisfy. This indicates that the Heros commitment is in service to the Bribe action.
An open motivation flaw is created that corresponds to the new frame of commitment.
There are many actions that will establish the Heros intention that the Villain has the
money: the Villain might persuade the Hero if they are friends, or the Villain might coerce
241

fiRiedl & Young

Frame of commitment for Villain with goal
controls(vil, prez)

intends(vil, controls(vil, prez))

Frame of commitment for Hero
with goal has(vil, $)

has(vil, $)

Intention level
Domain level
init

has(hero, $)

Give (hero, vil, $)

has(vil, $)

Bribe (vil, prez, $)
corrupt(prez)

Coerce (vil, hero, (has vil $))

goal

Figure 9: Solution IPOCL plan graph for the example narrative.
the Hero. The latter, Coerce(Villain, Hero, has(Villain, $)), is chosen by the planner: the Villain character coerces the Hero character into having the goal has(Villain,
$).
At this point, the planner must determine why the Villain coerces the Hero. There
are several possibilities. First, frame discovery comes into play to determine if the Villain
intends any of the effects of the Coerce action. Assume the only effect of the Coerce action
is intends(Hero, has(Villain, $)). The planner can select this effect and construct a
new frame of commitment specifying that the Villain intends that the Hero intends that
the Villain has the money. Another option is to leave the Coerce action an orphan for the
time being. Let us suppose that this is the course that the planner chooses. A search of the
current plan structure indicates that the Coerce action can be part of the Villains existing
commitment to control the President. This is possible because Coerce is a motivating step
for the Heros frame of commitment and the Heros frame of commitment is in service to the
Bribe action, which is part of the Villains frame of commitment. An intent flaw associating
the Coerce action with the Villains existing frame of commitment is created. Eventually,
the spreading activation of intent planning will associate Coerce with the Villains frame of
commitment. The plan structure at this point is shown in Figure 9. Any remaining flaws
are handled by conventional causal planning.
4.5 Complexity of the IPOCL Algorithm
The computational complexity of the IPOCL algorithm is O(c(b(e + 1)a )n ), where
 n is the depth of the search space,
 b is the number of ways that an action can be instantiated (e.g., the number of
permutations of legal parameter bindings),
 e is the number of effects of an instantiated action, and
 a is the number of actors in an instantiated action.
242

fiNarrative Planning: Balancing Plot and Character

The worst-case branching factor of the IPOCL search space is b(e + 1)a . The factor,
(e + 1) signifies that if a new frame of commitment is being constructed, the planner must
choose between the e effects of the action (plus one to signify the condition where no effect
is chosen). The exponent a reflects the fact that if multiple characters are intentionally
participating in an action, then each of those characters can have distinct intentions for
performing that action. For example, the action Marry(?groom, ?bride, ?place) has six
effects (see the Appendix for the action schema) and two intentional actors (?groom and
?bride).
The depth of the IPOCL search space n is the number of open condition flaws, open
motivation flaws, intent flaws, causal threats, and intentional threats that are repaired. In
the worst-case, for every newly instantiated step in the plan IPOCL also creates a new
frame of commitment and a corresponding open motivation flaw. If nPOCL is the depth
of a solution in the search space of POCL planning problem and nIPOCL is the depth of
the corresponding solution in the search space on an IPOCL fabula planning problem, then
nIPOCL is bounded by the function nIPOCL = 2nPOCL .
A narrative was generated for evaluation purposes (see Section 5). The narrative, rendered into natural language, is given in Figure 13 and the plan data structure is presented
graphically in the Appendix (Figure 15). This complete fabula plan exists at a depth of
n = 82. The average branching factor for this domain is  6.56, with the worst branching
factor for any given node being 98. The node with 98 children is the first flaw that the
planner solves for: married(Jafar, Jasmine), which is solved by instantiating the action
Marry(Jafar, Jasmine, ?place). The operator schema has six effects. The two characters are both intentional actors meaning there can be up to two frames of commitment
generated. Finally, the parameter ?place can be bound in two ways. Note that our implementation of IPOCL uses constraint propositions to generate a child node for each legal
permutation of parameter bindings. When we provide a domain-specific heuristic evaluation function that favors plan structures with certain preferred character goals (for example,
Jafar intends that Jasmine is dead is not included), then the complete plan is generated
in approximately 12.3 hours (approximately 11.6 hours were spent in garbage collection)
on an Intel Core2 Duo 3GHz system with 3GB of RAM and 100GB of virtual memory
R
running Allegro CL
8.0. Under these conditions, IPOCL generates 1,857,373 nodes and
visits 673,079 nodes. When the algorithm is run with only a domain-independent heuristic
adopted from classical planning (e.g., number of flaws plus plan length), the problem cannot
be solved before the system runs out of virtual memory. The Appendix gives details on the
domain, fabula planning problem, and heuristic used.
Practical experience with the IPOCL algorithm suggests that better heuristic evaluation
functions are needed to guide the search process. Without sufficient heuristic functions, the
behavior of IPOCL devolves to nearly breadth-first. Practical experience with the algorithm
also suggests that it is difficult to write heuristic functions that practically distinguish
between sibling nodes. The problem of defining heuristic functions that distinguish between
sibling nodes in the plan search space arises in all POCL algorithms, but is exacerbated in
IPOCL due to the increased number of structural features that need to be distinguished.
243

fiRiedl & Young

4.6 Limitations and Future Work
As an algorithm that solves the fabula generation problem, IPOCL has been demonstrated
to generate sound narrative structures that support believability through the enforcement
of character intentions. IPOCL is able to achieve this by effectively decoupling the concept
of character intentions from those of author intentions. Consequently, intentionality of character actions must be opportunistically discovered at the time that actions are discovered.
The opportunistic discovery of character intentions during action instantiation significantly
increases the branching factor to the detriment of the ability to generate long narratives.
However, we feel that opportunistic discovery of intentions is a vital part of expanding the
space of narratives that can be searched to include those that have logical causal progression
and also have well-motivated and thus more believable characters. An alternative is to use a
grammar (cf., Rumelhart, 1975), hierarchical task networks (cf., Sacerdoti, 1977), or other
form of hierarchical decomposition (cf., Young, Pollack, & Moore, 1994) such that intentions are dealt with at one level of abstraction and specific character actions dealt with at
the primitive level. However, using grammars, HTNs, or other decompositional techniques
to generate narrative requires reasoning at higher levels of abstraction than the action and
introduces potentially rigid top-down structuring of plot that can limit the systems ability
to find solutions that might exist but cannot be described by the grammar/task-network.
There are additional limitations that need to be addressed. First, while the IPOCL
algorithm asserts that all non-happening character actions must be part of a frame of
commitment, and therefore motivated by an event (or the initial state), IPOCL also assumes
that each frame of commitments interval of intentionality terminates in an action that
successfully achieves the goal of the frame of commitment. Essentially, every character acts
according to an intention and every intention is achieved. This inherently limits the types
of narratives that can generated. For example, narratives in which a character tries to
achieve a goal but fails several times before finally succeeding are unlikely. Narratives in
which one character  a hero  defeats another  a villain  cannot be generated. Although,
it is possible to generate a narrative in which the villain first achieves his goal and then the
hero achieves his goal (thus defeating the villain).
The inability to consider actions that support intentions that are never achieved appears to be an inherent limitation of our partial-order planning approach. In particular,
the backward-chaining nature of the algorithm biases the approach toward explaining actions. To ensure soundness, causal threats are eliminated or backtracking occurs. It is
possible that a forward-chaining approach could resolve this issue, but only at the expense
of promiscuous intention generation. One way to force the algorithm to consider narrative
structures in which one character defeats another or in which a character fails several times
before succeeding is to seed the plan space with intermediate author goals indicating sets
of states through which all solutions must pass (Riedl, 2009). This approach, however,
presupposes that the human author knows, wants, or can predict some of the resultant
narrative structure.
As mentioned in Section 4.3.1, IPOCL currently only has weak mechanisms to detect
or prevent contradictory intentions for a character. Better heuristics may help control for
the situations that are not resolved through ordering of actions or ordering of frames of
244

fiNarrative Planning: Balancing Plot and Character

commitment. It is possible to extend the algorithm to include common-sense reasoning or
semantic analysis at the frame of commitment level. However, this work has not been done.
In general, better heuristics are needed. Heuristics can be divided into domain-dependent
and domain-independent heuristics. Domain-dependent heuristics, in this case, refer to
those that employ knowledge about the characters, setting, or preferences over the narrative structure. For example, to generate the example in Figure 13, we use a heuristic that
penalizes narratives that contain character goals that we thought unreasonable based on our
intuitions about characters and the types of stories that we sought. Domain-independent
heuristics are more difficult to identify but might include preferences for fewer frames of
commitment with longer action sequences. Domain-independent heuristics that can reward
narrative structures with dramatic arc will likely require complex models of narrative psychology such as those described by Gerrig and colleagues (Gerrig, 1993; Gerrig & Bernardo,
1994) and implemented by Fitzgerald, Kahlon, and Riedl (2009) and may not work on
intermediate, incomplete narratives.

5. An Evaluation of Character Intentionality in IPOCL-Generated
Fabula Plans
In order to perform an empirical evaluation of a readers perception of character intentionality in IPOCL-generated fabulas, we designed an objective evaluation procedure based on
question-answering in order to reveal a readers understanding of character intentions without the use of subjective questionnaires (Riedl & Young, 2005). The goal of the evaluation
was to determine if IPOCL-generated fabulas supported the cognitive processes that readers apply to comprehend character actions better than fabulas generated by conventional
planning algorithms. The evaluation procedure is outlined as follows. Two planning-based
algorithms were used to generate plans to be interpreted as fabulas: the IPOCL algorithm
and a conventional POCL planning algorithm. Each planner was provided identical initialization parameters. The first plan generated by each algorithm was selected to be presented
to study participants. Because the plans must be read, a simple natural language generation
process was used to produce natural language text from each fabula plan. Recall that the
purpose of a fabula plan is not to be executed by a plan execution system, but to contain
temporal event information to be told as a story. Participants were recruited and randomly
assigned to one of two groups. Participants in the POCL group read the POCL-generated
narrative text. Participants in the IPOCL group read the IPOCL-generated narrative text.
A variation of the question-answering protocol from the work of Graesser et al. (1991) was
used to elicit participants mental models of the narratives. In particular, we focused on
why questions that elicit understanding of story world character goals and motivations.
How do we evaluate question-answering performance across groups? QUEST (Graesser
et al., 1991) takes a graphical representation of a story and reliably predicts the questionanswering performance of a human who might also read the story. One of the implicit
assumptions behind QUEST is that it has been provided a well-structured story that has
contained within the storys narrative structure all the answers to any question one might
ask about character goals and motivations. We exploit that assumption as a means of measuring how well a story actually supports a human readers reasoning about character goals
and motivations. That is, if a story does not support human comprehension, we should see
245

fiRiedl & Young

Once there was a Czar who had three lovely daughters. One day the three daughters
went walking in the woods. They were enjoying themselves so much that they forgot
the time and stayed too long. A dragon kidnapped the three daughters. As they
were being dragged off, they cried for help. Three heroes heard the cries and set off
to rescue the daughters. The heroes came and fought the dragon and rescued the
maidens. Then the heroes returned the daughters to their palace. When the Czar
heard of the rescue, he rewarded the heroes.

Figure 10: An example story from the work of Graesser et al. (1991).
this manifested in human question-answering performance. QUEST knowledge structures
can be translated into fabula plans and vice versa (Christian & Young, 2004). From QUEST
knowledge structures automatically generated from fabula plans, we run QUEST to predict
question-answering performance and compare QUEST predictions to actual performance.
We expect to see that IPOCL-generated narratives are more understandable than the alternative; we should find a greater correspondence between QUEST and actual performance
in the IPOCL condition than we find in the POCL condition.
5.1 The QUEST Model of Question-Answering
The QUEST model (Graesser et al., 1991) accounts for the goodness-of-answer (GOA)
judgments for questions asked about passages of prose. One application of the QUEST
model is to show that people build cognitive representations of stories they read that capture
certain relationships between events in a story and the perceived goals of the characters in
the story (Graesser et al., 1991). QUEST knowledge structures can be represented visually
as directed graphs with nodes referring to either story events (typically character actions)
or character goals. Directed links capture the relationship between story event nodes in
terms of causality and the relationship between events and character goals in terms of
intentionality. A readers cognitive representation of the story is queried when the reader
answers questions about the story. The types of questions supported by the QUEST model
are: why, how, when, enablement, and consequence. For example, the story in Figure 10
(Graesser et al., 1991, Fig. 1) has the corresponding QUEST knowledge structure shown
in Figure 11 (Graesser et al., 1991, Fig. 2). There are two types of nodes in the QUEST
knowledge structure: event nodes, which correspond to occurrences in the story world,
and goal nodes, which correspond to goals that characters have. The links between nodes
capture the different types of relationships between events and character goals.
 Consequence (C): The terminal event node is a consequence of the initiating event
node.
 Reason (R): The initiating goal node is the reason for the terminal goal node.
 Initiate (I): The initiating event node initiates the terminal goal node.
 Outcome (O): The terminal event node is the outcome of the initiating goal node.
 Implies (Im): The initiating event node implies the terminal event node.
246

fiNarrative Planning: Balancing Plot and Character

Im
I

GOAL 10
Daughters get
help

O

C

R
GOAL 12
Daughters cry

EVENT 11
Daughters got
help

O

EVENT 14
Heroes heard
cries

I

GOAL 15
Heroes rescue
daughters

O

R

C

EVENT 13
Daughters
cried

GOAL 17
Heroes fight
dragon
R
GOAL 19
Heroes go to
daughters and
dragon

EVENT 16
Heroes
rescued
daughters

I
C

C
O

O

EVENT 18
Heroes
fought dragon
C
EVENT 20
Heroes came
to daughters
and dragon

Figure 11: An example of a QUEST model of the Czar and the Daughters story from
Graesser et al. (1991).

The QUEST model defines arc search procedures for each type of question (e.g. why, how,
when, enablement, and consequence). The arc search procedures, starting at the queried
node, distinguish between legal answer nodes and illegal answer nodes. That is, only nodes
reachable by the arc search procedures are legal answer nodes. The legality of answers and
the weight of structural distance correspond to GOA judgments of human story readers.
5.2 Procedure
The procedure involves comparing subject question-answering performance to QUEST
question-answering predictions in two conditions. The POCL condition is based on narrative structures generated by a conventional POCL planning algorithm. The IPOCL condition is based on narrative structures generated by the IPOCL algorithm. Both planners
were initialized with identical information defining a story world. The story world was
based loosely on the story of Aladdin. The initial parameters included the following:
 An initial state that defines the story world, including (a) locations, (b) objects, (c)
characters, and (d) the relevant initial relationships between all of the above. Story
world characters include Aladdin, King Jafar, Princess Jasmine, a dragon, and a genie.
 A library of operators defining the events that can be performed by story world
characters.
 An outcome: Jasmine and Jafar are married and the genie is dead.
Note that even though the initialization parameters were identical in both conditions,
there are differences in how the respective planners handle the parameters. In particular,
IPOCL makes use of additional information in action schemata about actors and whether
an action can be a happening. The POCL planner ignored this information, which did not
impact its ability to find a valid solution. The initialization information used by IPOCL
but ignored by the POCL planner are as follows. First, all operators must specify which of
247

fiRiedl & Young

the parameters are the intentional actors (and not characters being acted upon). Second,
some operators are tagged as happenings. Finally, some operators have effects of the form
intends(a, p) where a is a variable that can be bound to a ground symbol representing
a character, and p is a variable that can be bound to a literal that becomes one of the
characters internal goals. Effects of this form are used by the IPOCL implementation
during motivation planning to ensure there are actions that cause frames of commitment.
Since no operators have preconditions of this form, the POCL planner does not utilize
this information. The Appendix lists the entire set of initialization parameters used in the
evaluation.
The fabula plans generated by the two planning algorithms are shown in the Appendix.
For the plans to be human-readable, each plan was input into the Longbow discourse planner (Young et al., 1994). Longbow results in a plan consisting of communicative acts such
as describe-character and describe-event for conveying the temporally ordered information of the narrative. The discourse plan steps were then rendered into natural language
using a simple template-matching procedure. The resulting narrative texts for the POCL
condition and IPOCL condition are shown in Figures 12 and 13, respectively. Similarities
between the two narratives make a comparison study possible. Specifically, the set of events
in the IPOCL-generated narrative are a superset of the events in the POCL-generated narrative. There is one distinct action ordering difference between the two fabula plans: in
the IPOCL condition only the event where the King falls in love with Jasmine is temporally constrained to occur first, but in the POCL condition the ordering of this event is
under-constrained and falls late in the text. In the POCL condition, had this event come
earlier, some participants may have inferred a relationship between the king falling in love
and Aladdins actions even though there is no actual relationship in the generated QUEST
graph. We believe that the ordering had an insignificant impact on the results.
The fabula plans were also converted to structures that QUEST can use to predict
question-answering performance. We use a procedure described by Christian and Young
(2004). Their algorithm for generating QUEST graph structures from a plan has been only
evaluated for POCL plans involving a single character.9 IPOCL plans, however, contain
additional structures such as frames of commitment and motivation links that are not part
of conventional plan representations. Consequently, the algorithm for generating a QUEST
graph structure from a plan was extended to take into consideration IPOCL plans. An
additional study by the authors (not reported) determined that QUEST knowledge structures derived from IPOCL plans with the extended algorithm significantly predict questionanswering judgments when structural distance is ignored (p < 0.0005). The modifications
to Christian and Youngs (2004) algorithm are beyond the scope of this paper, but details
can be found in the work of Riedl (2004).
The evaluation involved a questionnaire in which participants read a story and then make
goodness-of-answer (GOA) judgments about pairs of question and answers. A questionanswer pair has a why question about an intentional action performed by a character in
the story and a possible answer. For example, the question, Why did Aladdin slay the
9. Christian and Young (2004) compare DPOCL plans to QUEST knowledge structures. DPOCL is a
decompositional, partial order causal link planning algorithm (Young et al., 1994) that extends the
conventional POCL algorithm by explicitly representing hierarchical relationships between abstract and
primitive planning operators.

248

fiNarrative Planning: Balancing Plot and Character

There is a woman named Jasmine. There is a king named Jafar. This is a story
about how King Jafar becomes married to Jasmine. There is a magic genie. This is
also a story about how the genie dies.
There is a magic lamp. There is a dragon. The dragon has the magic lamp. The
genie is confined within the magic lamp. There is a brave knight named Aladdin.
Aladdin travels from the castle to the mountains. Aladdin slays the dragon. The
dragon is dead. Aladdin takes the magic lamp from the dead body of the dragon.
Aladdin travels from the mountains to the castle. Aladdin hands the magic lamp to
King Jafar. The genie is in the magic lamp. King Jafar rubs the magic lamp and
summons the genie out of it. The genie is not confined within the magic lamp. The
genie casts a spell on Jasmine making her fall in love with King Jafar. Jasmine is
madly in love with King Jafar. Aladdin slays the genie. King Jafar is not married.
Jasmine is very beautiful. King Jafar sees Jasmine and instantly falls in love with her.
King Jafar and Jasmine wed in an extravagant ceremony.
The genie is dead. King Jafar and Jasmine are married. The end.

Figure 12: Text of story in control condition.

There is a woman named Jasmine. There is a king named Jafar. This is a story
about how King Jafar becomes married to Jasmine. There is a magic genie. This is
also a story about how the genie dies.
There is a magic lamp. There is a dragon. The dragon has the magic lamp. The
genie is confined within the magic lamp.
King Jafar is not married. Jasmine is very beautiful. King Jafar sees Jasmine and
instantly falls in love with her. King Jafar wants to marry Jasmine. There is a brave
knight named Aladdin. Aladdin is loyal to the death to King Jafar. King Jafar orders
Aladdin to get the magic lamp for him. Aladdin wants King Jafar to have the magic
lamp. Aladdin travels from the castle to the mountains. Aladdin slays the dragon.
The dragon is dead. Aladdin takes the magic lamp from the dead body of the dragon.
Aladdin travels from the mountains to the castle. Aladdin hands the magic lamp to
King Jafar. The genie is in the magic lamp. King Jafar rubs the magic lamp and
summons the genie out of it. The genie is not confined within the magic lamp. King
Jafar controls the genie with the magic lamp. King Jafar uses the magic lamp to
command the genie to make Jasmine love him. The genie wants Jasmine to be in love
with King Jafar. The genie casts a spell on Jasmine making her fall in love with King
Jafar. Jasmine is madly in love with King Jafar. Jasmine wants to marry King Jafar.
The genie has a frightening appearance. The genie appears threatening to Aladdin.
Aladdin wants the genie to die. Aladdin slays the genie. King Jafar and Jasmine wed
in an extravagant ceremony.
The genie is dead. King Jafar and Jasmine are married. The end.

Figure 13: Text of story in experimental condition.
dragon? might be paired with the answer, Because King Jafar ordered Aladdin to get
the magic lamp for him. The participants were asked to rate the goodness of the answer
for the given question on a four-point scale ranging from Very bad answer to Very good
answer. The participants were shown examples of a question-answer pairs before the rating
task began, but were not otherwise given a definition of good or poor or trained to make
the judgment. Participants rated the GOA of a question-answer pair for every combination
249

fiRiedl & Young

of goal nodes in the QUEST knowledge structure for the story. The POCL condition
questionnaire had 52 question-answer pairs while the IPOCL condition questionnaire had
82 question-answer pairs due to the increased story plan length. Participants were asked to
read the story text completely at least once before proceeding to the ratings task and were
allowed to refer back to the original text at any time during the rating task.
For each narrative, QUEST was used to predict whether question-answer pairs would
be considered as good or poor based on the arc search procedure following forward
reason arcs, backward initiate arcs, and backward outcome arcs (Graesser et al., 1991).
The hypotheses of the experiment were as follows.
Hypothesis 1 Participants in the IPOCL condition will have higher mean GOA judgment ratings for question-answer pairs identified by QUEST as being good than
participants in the POCL condition.
Hypothesis 2 Participants in the IPOCL condition will have lower mean GOA judgment ratings for question-answer pairs identified by QUEST as being poor than
participants in the POCL condition.
If the actual question-answering performance of participants results in statistically higher
GOA ratings for question-answer pairs judged by QUEST to be good, then there is greater
correspondence between QUEST predictions and actual performance. Likewise, if actual
question-answering performance of participants results in statistically lower GOA ratings
for question-answer pairs judged by QUEST to be poor, then there is greater correspondence between QUEST predictions and actual performance. Poor correspondence between
QUEST predictions and actual question-answering performance is an indication that a narrative lacks structure that supports human understanding of character goals, intentions,
and motivations.
Thirty-two undergraduate students in the Computer Science program at North Carolina
State University participated in the study. All participants were enrolled in the course Game
Design and Development and were compensated for their time with five extra credit points
on their final grade in the course.
5.3 Results and Discussion
Each question-answer pair in each questionnaire was assigned a good rating or a poor
rating based on the QUEST prediction. Good question-answer pairs were assigned a value
of 4 and poor question-answer pairs were assigned a value of 1. Human GOA ratings of
question-answer pairs were also assigned values from 1 to 4 with 1 corresponding to Very
poor answer and 4 corresponding to Very good answer. The results of participants
answers to questionnaire answers are compiled into Table 1. The numbers are the mean
GOA ratings for each category and each condition. The numbers in parentheses are standard deviations for the results. Mean human question-answering performance is more in
agreement with QUEST when the mean GOA ratings for question-answer pairs categorized
as good is closer to 4 and the mean GOA ratings for question-answer pairs categorized
as poor is closer to 1.
A standard one-tailed t-test was used to compare the mean GOA rating of good
question-answer pairs in the IPOCL condition to the mean GOA rating of good question250

fiNarrative Planning: Balancing Plot and Character

Condition
IPOCL
POCL

Mean GOA rating for good
question-answer pairs
(standard deviation)
3.1976 (0.1741)
2.9912 (0.4587)

Mean GOA rating for poor
question-answer pairs
(standard deviation)
1.1898 (0.1406)
1.2969 (0.1802)

Table 1: Results of the evaluation study.
answer pairs in the POCL condition. The result of the t-test with 15 degrees of freedom
yields t = 1.6827 (p < 0.0585). This result is strongly suggestive that Hypothesis 1 is
supported.
A standard one-tailed t-test was used to compare the mean GOA rating of poor
question-answer pairs in the IPOCL condition to the mean GOA rating of poor questionanswer pairs in the POCL condition. The result of the t-test with 15 degrees of freedom
yields t = 1.8743 (p < 0.05). Participants in the IPOCL condition had significantly lower
GOA ratings for poor question-answer pairs than participants in the POCL condition.
Hypothesis 2 is supported.
It is interesting to note that the standard deviation for results in the POCL condition for
good question-answer pairs was high. Further analysis reveals that human participants
are likely to judge a question-answer pair as good if there is lack of evidence against
the possibility that the character action might have been intentional. We speculate that
reader/viewers simultaneously consider multiple hypotheses explaining character behavior
until they are disproved. Regardless of the content of any communicative act, one will
always be able to provide a more or less plausible explanation of the meaning (Sadock,
1990).
There were a couple of limitations to note. We did not control for narrative length.
It is possible that the effects we measured were a result of narrative length instead of improved narrative structure generated by IPOCL. We believe this to be unlikely, but future
evaluations should add filler sentences to the POCL condition narrative that do not impact character intentionality so that the control narrative matches the length of the IPOCL
condition. According to the narrative comprehension theories of Graesser et al. (1994)
and Trabasso and colleagues (Trabasso & Sperry, 1985; Trabasso & van den Broek, 1985),
such filler sentences will not be included in the readers mental model in a meaningful way
because they will not be causally related to other concepts in the mental model of the
narrative. Consequently, we felt that it was safe to leave the filler sentences out. Another
limitation is that the model of discourse used in the Longbow discourse planner was simplistic. Specifically, explicit statements about character intentions were incorporated into
the narrative text in the experimental condition due to an overly promiscuous discourse
model used by the Longbow discourse planner. We believe that our results would be the
same if these explicit statements were excluded because human readers are very good at
inferring intentions from stories (Graesser et al., 1991, 1994). However, to be complete we
would have to control for such artifacts from discourse generation.
We conclude that there is strong evidence that the narrative in the experimental condition supported reader comprehension of character goals, intentions, and motivations better
than the narrative in the control condition. Since both were generated from identical initial251

fiRiedl & Young

ization parameters, the most significant independent variable is the generation algorithm.
We infer that the improvement of the IPOCL condition over the POCL condition is due
to enhancements to the automated story generation capability introduced in the IPOCL
algorithm.

6. Conclusions
The objective of the research presented here is to develop an approach to the generation
of narrative fabula that has the properties of supporting audience perception of character
intentionality and causal plot progression. An informal analysis of related work suggests
that narrative generation systems can be categorized as using simulation-based or deliberative approaches. Simulation-based approaches tend to produce narratives with reasonable
character believability but are unlikely to produce narrative with globally coherent plots.
This is due to the fact that simulation-based approaches model characters and attempt to
optimize character decisions in any given moment. Thus simulation-based approaches are
prone to local maxima. Deliberative approaches reviewed in this article do not directly consider character intentions but are otherwise more likely to produce narratives with causally
coherent plot progressions. This is due to the fact that deliberative narrative generation
systems tend to reason about the entire plot instead of separately about characters. In our
informal analysis, we did not see any evidence that a deliberative system cannot reliably produce narrative structures with character believability (especially character intentionality).
We use a refinement search approach to construct the entire fabula from the perspective of
the author. Our approach is consequently a deliberative one. We favor refinement search
because partial-order plans appear to be a good representation for the fabula of a narrative.
In our analysis, algorithms that solve the planning problem are not sufficient for generating narratives with character intentionality because planning algorithms are conventionally
designed to provide a singular agent with the ability to achieve a singular goal. Stories
are more likely than not to involve multiple agents  characters  who are not necessarily
cooperating to achieve a singular goal state. Accordingly, we developed a deliberative fabula generation algorithm that reasons about the understandability of characters from the
perspective of a hypothetical audience. The IPOCL fabula generation algorithm treats any
potential solution as flawed unless the audience is capable of understanding what individual
(and potentially conflicting) goals each character has and what motivated the characters
to adopt those goals throughout the progression of the narrative. IPOCL contains routines for repairing character intentionality flaws by non-deterministically attributing goals
to characters and then generating event sequences that motivate those goals.
In adopting the approach to narrative generation in the IPOCL algorithm, we realize
several limitations. First, IPOCL is incapable of producing narrative structures in which
a character fails to achieve their goals. Character failure of this sort is a natural part
of most stories and is especially important in comedy and tragedy (Charles et al., 2003).
Unfortunately, planners do not produce plans that fail  e.g. cannot execute to completion
 because the planner will prune that branch of the search space and backtrack. Conflict
can arise between characters when characters adopt contradictory goals. However, each
character will succeed in achieving their goal, although this will happen serially because
conflicting frames of commitment will be temporally ordered. Second, in our work to date,
252

fiNarrative Planning: Balancing Plot and Character

we have assumed that fabula and sjuzet can be reasoned about distinctly. That is, once
a fabula is generated indicating what a narrative is about, a separate process can reason
about how the narrative should be told. This may suffice for simple telling of generated
narratives. Intuitively, in order to achieve more sophisticated effects on an audience such
as suspense, one might have to consider how a narrative can be told while the generator is
determining what should be told.
We believe that the work reported here represents a step towards achieving greater capability in computer systems to generate fictional narratives for communication, entertainment, education, and training. It is an incremental step, building from established artificial
intelligence technologies  planning  and cognitive science principles. Non-subjective empirical evidence suggests that we have achieved improvement in narrative generation over
alternative, conventional planners. Furthermore, we believe we have created a framework on
which we can continue to make incremental improvements to narrative generation capabilities. For example, we have been able to incorporate the ability to handle folk psychological
models of character personality (Riedl & Young, 2006). A system that can generate stories
is capable of adapting narrative to the users preferences and abilities, has expanded replay
value, and is capable of interacting with the user in ways that were not initially envisioned
by system designers. Narrative generation is just one example of how instilling computational systems with the ability to reason about narrative can result in a system that is more
capable of communicating, entertaining, educating, and training.

253

fiRiedl & Young

Appendix A.
This appendix contains details about the Aladdin planning domain used for the evaluation,
including planning problem specification, heuristics, complete diagrams for the plans generated and their accompanying QUEST diagrams, and a partial trace generated during the
creation of the Aladdin narrative.
A.1 Planning Problem Specification for the Study
The POCL planning algorithm used in the evaluation study and our implementation of the
IPOCL algorithm use PDDL-like formulations. A problem describes the initial world state
and the goal situation. The operator library contains operator schemata. In the evaluation
study, the POCL planning algorithm and IPOCL algorithm were given the same inputs.
Note however that some parts of the inputs are not used by the POCL algorithm.
The following propositions define the initial state:
character(aladdin)
male(aladdin)
knight(aladdin)
at(aladdin, castle)
alive(aladdin)
single(aladdin)
loyal-to(aladdin, jafar)
character(jafar)
male(jafar)
king(jafar)
at(jafar, castle)
alive(jafar)
single(jafar)

character(jasmine)
female(jasmine)
at(jasmine, castle)
alive(jasmine)
single(jasmine)
beautiful(jasmine)
character(dragon)
monster(dragon)
dragon(dragon)
at(dragon, mountain)
alive(dragon)
scary(dragon)

character(genie)
monster(genie)
genie(genie)
in(genie, lamp)
confined(genie)
alive(genie)
scary(genie)
place(castle)
place(mountain)
thing(lamp)
magic-lamp(lamp)
has(dragon, lamp)

The following propositions define the outcome situation:
married-to(jafar, jasmine)

alive(genie)

The following action schemata were provided in the operator library for the evaluation study. Note the deviations from conventional PDDL. Constraints indicate immutable
propositions that must always be true. Constraints function like preconditions except that
they can only be satisfied by the initial state and no operators can negate a proposition
that is used as a constraint in an operator schema. The actors slot lists the parameters
that refer to the actors that intend the operation; we do not assume that the first parameter is always the intentional actor. Further, when there is more than one actor listed, the
operator is a joint operation, meaning that the operator can only be accomplished by that
many actors working as a team and that all actors intend one of the effects of the operator.
When happening is true, the operator is a is allowed to remain an orphan. Some operators
have effects of the form intends(?x, ?c) indicating that the effect of one of the characters
bound to ?x has an intention to achieve the literal bound to ?c. There are no operators that
have preconditions of that form; intention propositions are exclusively used by the IPOCL
algorithm implementation.

254

fiNarrative Planning: Balancing Plot and Character

Action: travel (?traveller, ?from, ?dest)
actors: ?traveller
constraints: character(?traveller), place(?from), place(?dest)
precondition: at(?traveller, ?from), alive(?traveller), ?from6=?dest
effect: at(?traveller, ?from), at(?traveller, ?dest)
Action: slay (?slayer, ?monster, ?place)
actors: ?slayer
constraints: knight(?slayer), monster(?monster), place(?place)
precondition: at(?slayer, ?place), at(?monster, ?place), alive(?slayer), alive(?monster)
effect: alive(?monster)
Action: pillage (?pillager, ?body, ?thing, ?place)
actors: ?pillager
constraints: character(?pillager), character(?body), thing(?thing), place(?place)
precondition: at(?pillager, ?place), at(?body, ?place), has(?body, ?thing),
alive(?body), alive(?pillager), ?pillager6=?body
effect: has(?body, ?thing), has(?pillager, ?thing)
Action: give (?giver, ?givee, ?thing, ?place)
actors: ?giver
constraints: character(?giver), character(?givee), thing(?thing), place(?place)
precondition: at(?giver, ?place), at(?givee, ?place), has(?giver, ?thing),
alive(?giver), alive(?givee), ?giver6=?givee
effect: has(?giver, ?thing), has(?givee, ?thing)
Action: summon (?char, ?genie, ?lamp, ?place)
actors: ?char
constraints: character(?char), genie(?genie), magic-lamp(?lamp), place(?place)
precondition: at(?char, ?place), has(?char, ?lamp), in(?genie, ?lamp),
alive(?char), alive(?genie), ?char6=?genie
effect: at(?genie, ?place), in(?genie, ?lamp), confined(?genie), controls(?char, ?genie, ?lamp)
Action: love-spell (?genie, ?target, ?lover)
actors: ?genie
constraints: genie(?genie), character(?target), character(?lover)
precondition: confined(?genie), loves(?target, ?lover), alive(?genie), alive(?target), alive(?lover),
?genie6=?target, ?genie6=?lover, ?target6=?lover
effect: loves(?target, ?lover), intends(?target, married-to(?target, ?lover))
Action: marry (?groom, ?bride, ?place)
actors: ?groom, ?bride
constraints: male(?groom), female(?bride), place(?place)
precondition: at(?groom, ?place), at(?bride, ?place), loves(?groom, ?bride), loves(?bride, ?groom),
alive(?groom), alive(?bride)
effect: married(?groom), married(?bride), single(?groom), single(?bride),
married-to(?groom, ?bride), married-to(?bride, ?groom)

255

fiRiedl & Young

Action: fall-in-love (?male, ?female, ?place)
actors: ?male
happening: t
constraints: male(?male), female(?female), place(?place)
precondition: at(?male, ?place), at(?female, ?place), single(?male), alive(?male), alive(?female),
loves(?male, ?female), loves(?female, ?male), beautiful(?female)
effect: loves(?male, ?female), intends(?male, married-to(?male, ?female))
Action: order (?king, ?knight, ?place, ?objective)
actors: ?king
constraints: king(?king), knight(?knight), place(?place)
precondition: at(?king, ?place), at(?knight, ?place), alive(?king), alive(?knight),
loyal-to(?knight, ?king)
effect: intends(?knight, ?objective)
Action: command (?char, ?genie, ?lamp, ?objective)
actors: ?char
constraints: character(?char), genie(?genie), magic-lamp(?lamp)
precondition: has(?char, ?lamp), controls(?char, ?genie, ?lamp), alive(?char), alive(?genie),
?char6=?genie
effect: intends(?genie, ?objective)
Action: appear-threatening (?monster, ?char, ?place)
actors: ?monster
happening: t
constraints: monster(?monster), character(?char), place(?place)
precondition: at(?monster, ?place), at(?char, ?place), scary(?monster), ?monster6=?char
effect: intends(?char, alive(?monster))

As mentioned in Section 4.5, we required domain-dependent and domain-independent
heuristics to generate the example fabula shown in Figure 13 (the plan structure diagram
is shown in Figure 15). In IPOCL, heuristics evaluate a plan node and return an integer
such that solutions are evaluated to 0 and the higher the number the farther the plan node
is from being a solution. We used two heuristic functions whose return values were added
together.
 Domain-independent heuristic
 1 for each action
 1 for each flaw
 10 for each frame of commitment if there is more than one frame per character
 1000 for orphans to be performed by characters for which there are no frames of
commitment
 Domain-dependent heuristic
 5000 for repeat actions
 5000 for frames of commitment for character-goal combinations that are not on
the following lists:
256

fiNarrative Planning: Balancing Plot and Character

 Aladdin intends has(king, lamp), alive(genie), alive(dragon), has(hero,
lamp), or married-to(hero, jasmine)
 Jafar intends married-to(jafar, jasmine)
 Jasmine intends married-to(jasmine, jafar), or married-to(jasmine,
jafar)
 Genie intends loves(jasmine, jafar), loves(jafar, jasmine), loves(aladdin,
jasmine), or loves(jasmine, jafar)
 1000 if the action marry is not associated with two frames of commitment
A.2 Plan Diagrams and QUEST Structures for the Study
The subsequent figures show the plan diagrams and corresponding QUEST structures for
fabula generated for the evaluation.
 Figure 14: a fabula plan automatically generated by a POCL planning algorithm for
the POCL condition of the evaluation study.
 Figure 15: a fabula plan automatically generated by our IPOCL algorithm implementation for the IPOCL condition on the evaluation study.
 Figure 16: the QUEST structure corresponding to the fabula plan for the POCL
condition of the evaluation study.
 Figure 17: the QUEST structure corresponding to the fabula plan for the IPOCL
condition of the evaluation study.
In Figures 14 and 15, solid boxes represent actions in the plan structure (where the last
action is the goal step). Solid arrows represent causal links and the associated text is both
an effect of the preceding action and a precondition of the successive action. Dashed arrows
are temporal constraints found by the planning algorithm, indicating a necessary temporally
ordering between two actions added due to promotion or demotion strategies for resolving
causal threats. In Figure 15, ovals are frames of commitment and horizontal dashed lines
represent membership of actions in frames of commitment.

257

fiRiedl & Young

Key:
A: Aladdin
J: Jasmine
K: King Jafar
D: Dragon
G: Genie

Travel (A, castle, mount)
at(A, mount)

Slay (A, D, mount)

at(A, mount)
at(A, mount)

alive(D)

Pillage (A, D, lamp, mount)
Travel (A, mount, castle)
at(A, castle)

Give (A, K, lamp, castle)
has(K, lamp)

at(A, castle)

Summon (K, G, lamp, castle)
confined(G)

Fall-In-Love (K, J, castle)
loves(J, K)

at(G, castle)

Love-Spell (G, K, J)
love(J, K)

Slay (A, G, castle)
Marry (K, J, castle)
married(K, J)

alive(G)

Goal: married(K, J), alive(G)

Figure 14: Fabula plan representation of the story used in the POCL condition of the
evaluation study.

258

fiNarrative Planning: Balancing Plot and Character

Fall-In-Love (K, J, castle)
intends(K, married(K, J))

Order (K, A, castle, (has K lamp))

intends(K, married(K, J))

at(A, mount)
at(A, mount)

Slay (A, D, mount)

at(A, mount)

alive(D)

Pillage (A, D, lamp, mount)

at(A, castle)
loves(J, K)

Give (A, K, lamp, castle)
has(K, lamp)

at(A, castle)
at(A, castle)

Summon (K, G, lamp, castle)
controls(K, G)

confined(G)

Command (K, G, castle, (loves J K))

at(G, castle)
at(G, castle)

Appear-Threat (G, A, castle)

intends(G, loves(J, K))

Genie intends
loves(J, K)

intends(A, alive(G))

Love-Spell (G, K, J)
loves(J, K)

Slay (A, G, castle)

Jasmine intends
married(J, K)

intends(J, married(J, K))

Marry (K, J, castle)

alive(G)

Aladdin intends
alive(G)

King intends married(K, J)

Travel (A, mount, castle)

Aladdin intends has(K, lamp)

Travel (A, castle, mount)

married(K, J)

Goal: married(K, J), alive(G)

Figure 15: Fabula plan representation of the story used in the experimental IPOCL of the
evaluation study.

259

fiRiedl & Young

C

GOAL 9
Aladdin give
lamp to King

R

R
GOAL 7
Aladdin
travel to
castle
GOAL 5
Aladdin
pillage lamp
from Dragon

R

O

O

O

R
GOAL 3
Aladdin slay
Dragon
R
GOAL 1
Aladdin
travel to
mountain

EVENT 10
Aladdin gave
lamp to King
C
EVENT 8
Aladdin
traveled to
castle

O

C

EVENT 4
Aladdin slew
Dragon
C
EVENT 2
Aladdin
traveled to
mountain

GOAL 13
Genie cast
love-spell

O

O

EVENT 12
King
summoned
Genie
C
EVENT 14
Genie cast
love-spell

C

C

EVENT 6
Aladdin
pillaged lamp
from Dragon
C

O

GOAL 11
King
summon
Genie

GOAL 15
Aladdin slay
Genie

O

EVENT 16
Aladdin slew
Genie

C
EVENT 17
King fell in
love with
Jasmine
GOAL 18
King marry
Jasmine

C
C
O

EVENT 20
King and
Jasmine
married

O

GOAL 19
Jasmine
marry King

Figure 16: The QUEST knowledge structure for the story plan from the POCL condition.

260

fiNarrative Planning: Balancing Plot and Character

EVENT 1
King fell in
love with
Jasmine
C

I
GOAL 12
Aladdin give
lamp to King

R

R
GOAL 10
Aladdin
travel to
castle
GOAL 8
Aladdin
pillage lamp
from Dragon

R

O

O

O

R
GOAL 6
Aladdin slay
Dragon
R
GOAL 4
Aladdin
travel to
mountain

EVENT 13
Aladdin gave
lamp to King
C
EVENT 11
Aladdin
traveled to
castle

GOAL 23
King marry
Jasmine

C

O

C
EVENT 5
Aladdin
traveled to
mountain

GOAL 24
Jasmine
marry King

O

C

GOAL 19
Genie cast
love-spell

C

R

EVENT 9
Aladdin
pillaged lamp
from Dragon
EVENT 7
Aladdin slew
Dragon

EVENT 25
King and
Jasmine
married

I

C
O

EVENT 20
Genie cast
love-spell

I

C
O

O

C

GOAL 17
King
command
Genie to cast
R
GOAL 14
King
summon
Genie
R
GOAL 2
King order
Aladdin to
get lamp

O

O
C
O

EVENT 18
King
commanded
Genie to cast
C
EVENT 15
King
summoned
Genie

EVENT 16
Genie
appeared
frightening

C

C
EVENT 3
King ordered
Aladdin to
get lamp

C

GOAL 21
Aladdin slay
Genie

I
C

O

EVENT 22
Aladdin slew
Genie

Figure 17: The QUEST knowledge structure for the story plan from the IPOCL condition.

261

fiRiedl & Young

A.3 Trace
The following is a portion of a trace generated by IPOCL initialized with the above fabula
planning problem specification and heuristic. The trace focuses on the generation of some
of the nodes in the plan space that contribute to the final solution (shown graphically in
Figure 15).
plan0
reason: initial plan
now working on: open condition married-to(jafar, jasmine) on step goal
children: 98 (visited 2; selecting 9)
plan 9
reason: created new step 1: marry(jafar, jasmine, castle) to solve married-to(jafar, jasmine)
now working on: open motivation intends(jafar, married-to(jafar, jasmine)) on frame 2
children: 9 (visited 2; selecting 105)
plan 105
reason: created new step 2: fall-in-love(jafar, jasmine, castle) to solve
intends(jafar, married-to(jafar, jasmine))
now working on: open motivation intends(jasmine, married-to(jasmine, jafar)) on frame 1
children: 3 (visited 1; selecting 122)
plan 122
reason: created new step 3: love-spell(genie, jasmine, jafar) to solve
intends(jasmine, married-to(jasmine, jafar))
now working on: open motivation intends(genie, loves(jasmine, jafar)) on frame 3
children: 8 (visited 2; selecting 141)
plan 141
reason: created new step 4: command(jafar, genie, lamp, loves(jasmine, jafar)) to solve
intends(genie, loves(jasmine, jafar))
now working on: open condition alive(genie) on step 4
children: 1 (visited 1; selecting 197)
...
plan 591
reason: created new step 6: give(aladdin, jafar, lamp, castle) to solve has(jafar, lamp)
now working on: open motivation intends(aladdin, has(jafar, lamp)) on frame 4
children: 4 (visited 2; selecting 4675)
plan 4675
reason: created new step 7: order(jafar, aladdin, castle, has(jafar, lamp)) to solve
intends(aladdin, has(jafar, lamp))
now working on: open condition loyal-to(aladdin, jafar) on step 7
children: 1 (visited 1; selecting 21578)
...
plan 21597
reason: created new step 8: pillage(aladdin, dragon, lamp, mountain) to solve has(aladdin, lamp)
now working on: open condition alive(aladdin) on step 8
children: 1 (visited 1; selecting 21653)

262

fiNarrative Planning: Balancing Plot and Character

...
plan 1398116
reason: created new step 12: slay(aladdin, genie, castle) to solve alive(genie)
now working on: causal threat on alive(genie) between 0 and 3, clobbered by step 12
children: 1 (visited 1; selecting 1398282)
...
plan 1398289
reason: created new step 13: appear-threatening(genie, aladdin, mountain) to solve
intends(aladdin, alive(genie))
now working on: open condition scary(genie) on step 13
children: 1 (visited 1; selecting 1398304)
...
plan 1398364
reason: adoption of step 4 by frame 2: jafar intends married-to(jafar, jasmine)
now working on: intent flaw for aladdin, to possibly link step 8 to frame 4: aladdin intends
has(jafar, lamp)
children: 2 (visited 2; selecting 1398368)
plan 1398368
reason: adoption of step 8 by frame 4: aladdin intends has(jafar, lamp)
now working on: intent flaw for aladdin, to possibly link step 10 to frame 4: aladdin intends
has(jafar, lamp)
children: 2 (visited 2; selecting: 1398376)
...
plan 1398384
reason: no adoption of step 2 by frame 2: jafar intends married-to(jafar, jasmine)
now working on: intent flaw for jasmine, to possibly link step 12 to frame 1: jasmine intends
married-to(jasmine, jafar)
children: 2 (visited 2; selecting 1398400)
...
plan 1398576
reason: adoption of step 7 by frame 2: jafar intends married-to(jafar, jasmine)
now working on: intent flaw for aladdin, to possibly link step 9 to frame 4: aladdin intends
has(jafar, lamp)
children: 2 (visited 1; selecting 1398640)
plan 1398640
reason: adoption of step 9 by frame 4: aladdin intends has(jafar, lamp)
solution found

263

fiRiedl & Young

References
Aylett, R. (1999). Narrative in virtual environments  towards emergent narrative. In
Mateas, M., & Sengers, P. (Eds.), Narrative Intelligence: Papers from the AAAI Fall
Symposium (Technical Report FS-99-01), pp. 8386. AAAI Press, Menlo Park.
Aylett, R. (2000). Emergent narrative, social immersion and storification. In Proceedings
of the 1st International Workshop on Narrative and Interactive Learning Environments.
Bae, B.-C., & Young, R. M. (2008). A use of flashback and foreshadowing for surprise arousal
in narrative using a plan-based approach. In Proceedings of the 1st International
Conference on Interactive Digital Storytelling, pp. 156167.
Bal, M. (1998). Narratology: An Introduction to the Theory of Narrative. University of
Toronto Press.
Bates, J. (1992). Virtual reality, art, and entertainment. Presence: The Journal of Teleoperators and Virtual Environments, 1 (1), 133138.
Bates, J. (1994). The role of emotion in believable agents. Communications of the ACM,
37 (7), 122125.
Bates, J., Loyall, A., & Reilly, W. (1992). Integrating reactivity, goals, and emotion in a
broad agent. In Proceedings of the 14th Annual Conference of the Cognitive Science
Society, pp. 696706.
Black, J. B., & Wilensky, R. (1979). An evaluation of story grammars. Cognitive Science,
3, 213230.
Blair, D., & Meyer, T. (1997). Tools for an interactive virtual cinema. In Trappl, R., &
Petta, P. (Eds.), Creating Personalities for Synthetic Actors: Towards Autonomous
Personality Agents, pp. 8391. Springer.
Blumberg, B., & Galyean, T. (1995). Multi-level direction of autonomous creatures for
real-time virtual environments. In Proceedings of the 22nd Annual Conference on
Computer Graphics, pp. 4754.
Bratman, M. (1987). Intentions, Plans, and Practical Reason. Harvard University Press,
Cambridge, MA.
Bruner, J. (1990). Acts of Meaning. Harvard University Press, Cambridge.
Callaway, C., & Lester, J. (2002). Narrative prose generation. Artificial Intelligence, 139 (2),
213252.
Carbonell, J. (1980). Towards a process model of human personality traits. Artificial
Intelligence, 15, 4974.
Cavazza, M., Charles, F., & Mead, S. (2002). Planning characters behaviour in interactive
storytelling. Journal of Visualization and Computer Animation, 13, 121131.
Charles, F., Lozano, M., Mead, S., Bisquerra, A., & Cavazza, M. (2003). Planning formalisms and authoring in interactive storytelling. In Proceedings of the 1st International Conference on Technologies for Interactive Digital Storytelling and Entertainment.
264

fiNarrative Planning: Balancing Plot and Character

Chatman, S. (1993). Reading Narrative Fiction. Macmillan Publishing Company, New
York.
Cheong, Y.-G., & Young, R. M. (2008). Narrative generation for suspense: Modeling and
evaluation. In Proceedings of the 1st International Conference on Interactive Digital
Storytelling, pp. 144155.
Christian, D., & Young, R. (2004). Comparing cognitive and computational models of
narrative structure. In Proceedings of the 19th National Conference on Artificial Intelligence, pp. 385390.
Dehn, N. (1981). Story generation after TALE-SPIN. In Proceedings of the 7th International
Joint Conference on Artificial Intelligence, pp. 1618.
Dennett, D. (1989). The Intentional Stance. MIT Press, Cambridge, MA.
Fikes, R., & Nilsson, N. (1971). STRIPS: a new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189208.
Fitzgerald, A., Kahlon, G., & Riedl, M. O. (2009). A computational model of emotional
response to stories. In Proceedings of the 2nd Joint International Conference on
Interactive Digital Storytelling, pp. 312315.
Gerrig, R. (1994). Narrative thought?. Personality and Social Psychology Bulletin, 20 (6),
712715.
Gerrig, R. J. (1993). Experiencing Narrative Worlds: On the Psychological Activities of
Reading. Yale University Press, New Haven.
Gerrig, R. J., & Bernardo, A. (1994). Readers as problem-solvers in the experience of
suspense. Poetics, 22, 459472.
Gervas, P., Daz-Agudo, B., Peinado, F., & Hervas, R. (2005). Story plot generation based
on CBR. Journal of Knowledge-Based Systems, 18 (45), 235242.
Graesser, A., Lang, K. L., & Roberts, R. M. (1991). Question answering in the context of
stories. Journal of Experimental Psychology: General, 120 (3), 254277.
Graesser, A., Singer, M., & Trabasso, T. (1994). Constructing inferences during narrative
text comprehension. Psychological Review, 101 (3), 371395.
Gratch, J., & Marsella, S. (2004). A domain-independent framework for modeling emotion.
Journal of Cognitive Systems Research, 5 (4), 269306.
Grosz, B., & Sidner, C. (1990). Plans for discourse. In Cohen, P., Morgan, J., & Pollack,
M. (Eds.), Intentions in Communication, pp. 417444. MIT Press.
Grosz, B., & Kraus, S. (1996). Collaborative plans for complex group action. Artificial
Intelligence, 86 (2), 269357.
Hayes-Roth, B., van Gent, R., & Huber, D. (1997). Acting in character. In Trappl, R.,
& Petta, P. (Eds.), Creating Personalities for Synthetic Characters: Towards Autonomous Personality Agents, pp. 92112. Springer.
Herman, D. (2002). Story Logic: Problems and Possibilities of Narrative. University of
Nebraska Press, Lincoln, NE.
265

fiRiedl & Young

Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Jhala, A. H. (2009). Cinematic Discourse Generation. Ph.D. thesis, North Carolina State
University.
Kelso, M., Weyhrauch, P., & Bates, J. (1993). Dramatic presence. Presence: The Journal
of Teleoperators and Virtual Environments, 2 (1), 115.
Knoblock, C. (1994). Generating parallel execution plans with a partial-order planner. In
Proceedings of the 2nd International Conference on Artificial Intelligence and Planning Systems, pp. 98103.
Laurel, B. (1986). Toward the Design of a Computer-Based Interactive Fantasy System.
Ph.D. thesis, Ohio State University.
Lebowitz, M. (1984). Creating characters in a story-telling universe. Poetics, 13, 171194.
Lebowitz, M. (1985). Story-telling as planning and learning. Poetics, 14, 483502.
Lebowitz, M. (1987). Planning stories. In Proceedings of the 9th Annual Conference of the
Cognitive Science Society, pp. 234242.
Lester, J., Voerman, J., Towns, S., & Callaway, C. (1999). Deictic believability: Coordinating gesture, locomotion, and speech in lifelike pedagogical agents. Applied Artificial
Intelligence, 13 (4-5), 383414.
Loyall, A. B. (1997). Believable Agents: Building Interactive Personalities. Ph.D. thesis,
School of Computer Science, Carnegie Mellon University.
Maes, P., Darrell, T., Blumberg, B., & Pentland, A. (1995). The ALIVE system: Fullbody interaction with autonomous agents. In Proceedings of the 1995 Conference on
Computer Animation.
Mateas, M. (1997). An Oz-centric review of interactive drama and believable agents. Technical report CMU-CS-97-156, School of Computer Science, Carnegie Mellon University.
Mateas, M., & Sengers, P. (1999). Narrative intelligence. In Mateas, M., & Sengers, P.
(Eds.), Narrative Intelligence: Papers from the 1999 Fall Symposium (Technical Report
FS-99-01), pp. 110. AAAI Press, Menlo Park, CA.
McKoon, G., & Ratcliff, R. (1992). Inference during reading. Psychological Review, 99,
440466.
Meehan, J. R. (1976). The Metanovel: Writing Stories by Computers. Ph.D. thesis, Yale
University.
Meehan, J. R. (1977). TALE-SPIN: An interactive program that writes stories. In Proceedings of the 5th International Joint Conference on Artificial Intelligence, pp. 9198.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial-order planner for
ADL. In Proceedings of the 3rd International Conference on Knowledge Representation
and Reasoning, pp. 103114.
Perez y Perez, R., & Sharples, M. (2001). MEXICA: A computer model of a cognitive
account of creative writing. Journal of Experimental and Theoretical Artificial Intelligence, 13, 119139.
266

fiNarrative Planning: Balancing Plot and Character

Perlin, K., & Goldberg, A. (1996). Improv: A system for scripting interactive actors in
virtual worlds. In Proceedings of the 23rd International Conference on Computer
Graphics and Interactive Techniques, pp. 205216.
Pollack, M. (1992). The uses of plans. Artificial Intelligence, 57 (1), 4368.
Porteous, J., & Cavazza, M. (2009). Controlling narrative generation with planning trajectories: the role of constraints. In Proceedings of the 2nd International Conference on
Interactive Digital Storytelling, pp. 234245.
Prince, G. (1987). A Dictionary of Narratology. University of Nebraska Press, Lincoln.
Propp, V. (1968). Morphology of the Folktale. University of Texas Press, Austin, TX.
Reilly, W. (1996). Believable Social and Emotional Agents. Ph.D. thesis, School of Computer
Science, Carnegie Mellon University.
Riedl, M. O. (2004). Narrative Generation: Balancing Plot and Character. Ph.D. thesis,
North Carolina State University.
Riedl, M. O. (2009). Incorporating authorial intent into generative narrative systems. In
Louchart, S., Roberts, D., & Mehta, M. (Eds.), Intelligent Narrative Technologies II:
Papers from the 2009 Spring Symposium (Technical Report SS-09-06), pp. 9194, Palo
Alto, CA. AAAI Press.
Riedl, M. O., & Young, R. M. (2005). An objective character believability evaluation procedure for multi-agent story generation systems. In Proceedings of the 5th International
Conference on Intelligent Virtual Agents (IVA), pp. 278291.
Riedl, M. O., & Young, R. M. (2006). Story planning as exploratory creativity: Techniques
for expanding the narrative search space. New Generation Computing, 24 (3), 303323.
Rizzo, P., Veloso, M., Miceli, M., & Cesta, A. (1999). Goal-based personalities and social
behaviors in believable agents. Applied Artificial Intelligence, 13, 239272.
Rumelhart, D. (1975). Notes on a schema for stories. In Bobrow, D., & Collins, A. (Eds.),
Representation and Understanding: Studies in Cognitive Science, pp. 185210. Academic Press, New York.
Sacerdoti, E. (1977). A Structure for Plans and Behavior. Elsevier, New York.
Sadock, J. (1990). Comments on Vanderveken and on Cohen and Levesque. In Cohen, P.,
Morgan, J., & Pollack, M. (Eds.), Intentions in Communication, pp. 257270. MIT
Press, Cambridge, MA.
Schank, R., & Abelson, R. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry
into Human Knowledge Structures. Lawrence Erlbaum Associates.
Seif El-Nasr, M., Yen, J., & Ioerger, T. (2000). FLAME  fuzzy logic adaptive model of
emotions. Autonomous Agents and Multi-Agent Systems, 3, 219257.
Sengers, P. (2000). Schizophrenia and narrative in artificial agents. In Proceedings of the
1st International Workshop on Narrative and Interactive Learning Environments.
Sharples, M. (1999). How We Write: Writing as Creative Design. Routledge, London.
Smith, T., & Witten, I. (1991). A planning mechanism for generating story texts. Literary
and Linguistic Computation, 6 (2), 119126.
267

fiRiedl & Young

Thomas, F., & Johnson, O. (1981). Disney Animation: The Illusion of Life. Abbeville
Press, New York.
Trabasso, T., & Sperry, L. (1985). Causal relatedness and importance of story events.
Journal of Memory and Language, 24, 595611.
Trabasso, T., & van den Broek, P. (1985). Causal thinking and the representation of
narrative events. Journal of Memory and Language, 24, 612630.
Turner, S. R. (1994). The Creative Process: A Computer Model of Storytelling. Lawrence
Erlbaum Associates, Hillsdale, NJ.
van den Broek, P. (1988). The effects of causal relations and hierarchical position on the
importance of story statements. Journal of Memory and Language, 27, 122.
Weld, D. (1994). An introduction to least commitment planning. AI Magazine, 15, 2761.
Weyhrauch, P. (1997). Guiding Interactive Fiction. Ph.D. thesis, Carnegie Mellon University.
Wilensky, R. (1983). Story grammars versus story points. The Behavioral and Brain Sciences, 6, 579623.
Young, R. M. (1999). Notes on the use of plan structures in the creation of interactive plot.
In Mateas, M., & Sengers, P. (Eds.), Narrative Intelligence: Papers from the AAAI
Fall Symposium (Technical Report FS-99-01), pp. 164167. AAAI Press, Menlo Park.
Young, R. (2006). Story and discourse: A bipartite model of narrative generation in virtual
worlds. Interaction Studies, 8 (2), 177208.
Young, R., Pollack, M., & Moore, J. (1994). Decomposition and causality in partial-order
planning. In Proceedings of the Second International Conference on Artificial Intelligence and Planning Systems, pp. 188193.

268

fiJournal of Artificial Intelligence Research 39 (2010) 483-532

Submitted 04/10; published 10/10

Kalman Temporal Differences
Matthieu Geist
Olivier Pietquin

matthieu.geist@supelec.fr
olivier.pietquin@supelec.fr

IMS research group
Supelec
Metz, France

Abstract
Because reinforcement learning suffers from a lack of scalability, online value (and Q-)
function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences
(KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based
algorithm is provided for deterministic Markov Decision Processes (MDP) which produces
biased estimates in the case of stochastic transitions. Than the eXtended KTD framework
(XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases
for both deterministic and stochastic transitions. Related algorithms are experimented on
classical benchmarks. They compare favorably to the state of the art while exhibiting the
announced features.

1. Introduction
Optimal control of stochastic dynamic systems is a trend of research with a long history. The
machine learning response to this recurrent problem is the Reinforcement Learning (RL)
paradigm (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Sigaud & Buffet, 2010). In
this general paragon, an artificial agent learns an optimal control policy through interactions
with the dynamic system (also considered as its environment). After each interaction, the
agent receives an immediate scalar reward information and the optimal policy it searches
for is the one that maximizes the cumulative reward over the long run.
Traditionally the dynamic system to be controlled is modeled as a Markov Decision
Process (MDP). An MDP is a tuple {S, A, P, R, }, where S is the state space, A the
action space, P : s, a  S  A  p(.|s, a)  P(S) the family of transition probabilities,
R : S  A  S  R the bounded reward function, and  the discount factor (weighting longterm rewards). According to these definitions, the system stochastically steps from state to
state conditionally on the actions the agent performed. To each transition (si , ai , si+1 ) is
associated an immediate reward ri . A policy  : S  A is a mapping from states to actions
which drives the action selection process of the agent. The optimal policy   is the one that
maximizes the cumulative reward over the long term.
This cumulative reward is locally estimated by the agent as a so-called value (respectively
Q-) function associating an expected cumulative reward to each state (respectively stateaction pair). The optimal policy is therefore the one that maximizes these functions for each
state or state-action pair. Many RL algorithms aim at estimating one of these functions so as
to infer the optimal policy. In the more challenging cases, the search for the optimal policy
c
2010
AI Access Foundation. All rights reserved.

fiGeist & Pietquin

is done online, while controlling the system. This requires a trial and error process and a
dilemma between immediate exploitation of the currently learnt policy and exploration to
improve the policy then occurs.
In this context, a fair RL algorithm should address some important features:
 allowing online learning;
 handling large or even continuous state spaces;
 being sample-efficient (learning a good control policy from as few interactions as possible);
 dealing with non-stationarity (even if the system is stationary, controlling it while
learning the optimal policy induces non-stationarities; other good reasons to prefer
tracking to convergence are given in Sutton, Koop, & Silver, 2007);
 managing uncertainty (which is a useful information for handling the dilemma between
exploration and exploitation);
 handling non-linearities (to deal with the max operator of the Bellman optimality
equation and for compact function representations such as neural networks).
All these aspects are rarely addressed at the same time by state-of-the-art RL algorithms.
We show that the proposed Kalman Temporal Differences (KTD) framework (Geist, Pietquin,
& Fricout, 2009a) addresses all these issues. It is based on the Kalman filtering paradigm
and uses an approximation scheme, namely the Unscented Transform (UT) of Julier and
Uhlmann (2004), to approximate the value function. Originally the Kalman (1960) filtering paradigm aims at tracking the hidden state (modeled as a random variable) of a
non-stationary dynamic system through indirect observations of this state. The idea underlying KTD is to cast value function approximation into a filtering problem, so as to benefit
from intrinsic advantages of Kalman filtering: online second order learning, uncertainty
estimation and non-stationarity handling. The UT is used to deal with non-linearities in
a derivative-free fashion, which notably allows deriving a second-order value iteration-like
algorithm (namely KTD-Q).
1.1 Formalism
The value function V  of a given policy  associates to each state the expected discounted
cumulative reward for starting in this state and then following :

X
V  (s) = E[
 i ri |s0 = s, ]

(1)

i=0

where ri is the reward observed at time i. The Q-function adds a degree of freedom for the
choice of the first action:

X
Q (s, a) = E[
 i ri |s0 = s, a0 = a, ]
i=0

484

(2)

fiKalman Temporal Differences

Reinforcement learning aims at finding (through interactions) the policy   which maximises
the value function for every state:
  = argmax(V  )

(3)



Despite the partial order (value functions are vectors), this maximum exists (Puterman,
1994). Two schemes (among others) can lead to the solution. First, policy iteration implies
learning the value function of a given policy, then improving the policy, the new one being
greedy respectively to the learnt value function. It requires solving the Bellman evaluation
equation (given here for the value function and the Q-function):


V  (s) = Es0 |s,(s) R(s, (s), s0 ) + V  (s0 ) , s  S


Q (s, a) = Es0 |s,a R(s, a, s0 ) + Q (s0 , (s0 )) , s, a  S  A

(4)
(5)

The expectations depend on the transition probability conditioned on current state-action
pair, the action being given by the policy in the case of value function evaluation. The second
scheme, called value iteration, aims at directly finding the optimal policy. It requires solving
the Bellman optimality equation (given here for the Q-function):


0
 0
Q (s, a) = Es0 |s,a R(s, a, s ) +  max Q (s , b) , s, a  S  A


bA

(6)

A parametric representation of either the value or the Q-function is supposed to be
available (possible representations are discussed hereafter) and Temporal Differences (TD)
algorithms are considered. TD algorithms form a class of online methods which consist in
correcting the representation of the value (or Q-) function according to the so-called TD
error i made on it. Although the formal definition of the TD error depends on the algorithm
(see Section 1.2), it can be intuitively defined as the difference between the predicted reward
according to the current estimate of the value or Q-function and the actual observed reward
at time step i. Most of TD algorithms can be generically written as:
i = i1 + Ki i

(7)

In this expression, i1 is the latest estimate of the value function (or of the set of parameters
defining it), i is an updated representation given an observed transition, i is the TD error,
and Ki is a gain indicating the direction in which the representation of the target function
should be corrected.
If the state space S and the action space A are finite and small enough, an exact
description of the value function is possible, and  is a vector with as many components as
the state (-action) space (tabular representation). In the case of large state and/or action
spaces, approximation is necessary. A classical choice in RL is the linear parameterization,
that is the value function is approximated by:
V (s) =

p
X

wj j (s) = (s)T 

j=1

485

(8)

fiGeist & Pietquin

where (j )1jp is a set of basis functions, which should be defined beforehand, and the
weights wj are the parameters:
 = w1 . . .

wp

T

and (s) = 1 (s) . . .

T
p (s)

(9)

Many function approximation algorithms require such a representation to ensure convergence (Tsitsiklis & Roy, 1997; Schoknecht, 2002), or even to be applicable (Bradtke &
Barto, 1996; Boyan, 1999; Geramifard, Bowling, & Sutton, 2006). Other representations
are possible such as neural networks where  is the set of synaptic weights (usually resulting
in a nonlinear dependency of the value function to its parameters).
Adopting this generic point of view, the problem addressed in this paper can be stated
as: given a representation of the value function (or of the Q-function) summarized by the
parameter vector  and given a Bellman equation to be solved, what is the best gain K?
Some state-of-the-art answers to this question are given in the following section.
1.2 State of the Art
This paper focuses on online methods. Standard RL algorithms such as TD evaluation,
SARSA and Q-Learning (Sutton & Barto, 1998) share the same features and a unified view
based on Equation (7) is adopted in the following. In this equation, the term i is the TD
error. Suppose that at step i a transition (si , ai , ri , si+1 , ai+1 ) is observed. For TD-like RL
algorithms, that is algorithms aiming at evaluating the value function of a given policy ,
the TD error is:
(10)
i = ri +  Vi1 (si+1 )  Vi1 (si )
For SARSA-like algorithms, that is algorithms which aim at evaluating the Q-function of a
given policy , the TD error is:
i = ri +  Qi1 (si+1 , ai+1 )  Qi1 (si , ai )

(11)

Finally, for Q-learning-like algorithms, that is algorithms which aim at computing the optimal Q-function Q , the TD error is:
i = ri +  max Qi1 (si+1 , b)  Qi1 (si , ai )
bA

(12)

The type of temporal difference determines the Bellman equation to be solved (evaluation
equation for (10-11), optimality equation for (12)), and thus if the algorithm belongs to the
policy iteration or value iteration family.
The gain Ki is specific to each algorithm. The most common are reviewed here. For TD,
SARSA and Q-learning (for example, see Sutton & Barto, 1998), the gain can be written
as
Ki = i ei
(13)
where i is a classical learning rate in stochastic approximation theory which should satisfy:

X

i =  and

i=0


X
i=0

486

i2 < 

(14)

fiKalman Temporal Differences

and ei is a unitary vector which is zero everywhere except in the component corresponding
to state si (or to state-action (si , ai )) where it is equal to one (Kronecker function). These
algorithms have been modified to consider so-called eligibility traces (again, see Sutton and
Barto), and the gain is then written as
Ki = i

i
X

ij ej

(15)

j=1

where  is the eligibility factor. Informally, this approach keeps memory of trajectories in
order to propagate updates to previously visited states.
These algorithms have also been extended to take into account approximate representation of the value function (Sutton & Barto, 1998), and are called direct algorithms (Baird,
1995). Without eligibility traces, the gain is written as
Ki = i i1 Vi1 (si )

(16)

where i1 Vi1 (si ) is the gradient following the parameter vector of the parameterized
value function in the current state. This gain corresponds to a stochastic gradient descent
according to the cost function kV   V k2 . As V  (si ) is not known nor directly observable,
it is replaced by ri +  V (si+1 ). This general approach is known as bootstrapping (Sutton
& Barto, 1998). The value function can be replaced straightforwardly by the Q-function
in this gain. The direct algorithms have also been extended to take into account eligibility
traces, which leads to the following gain:
Ki = i

i
X

ij i1 Vi1 (sj )

(17)

j=1

Another well known approach is the set of residual algorithms (Baird, 1995), for which the
gain is obtained through the minimization of the L2 -norm of the Bellman residual (i.e.,
the difference between the left side and the right side of the Bellman equation, possibly for
sampled transitions) using a stochastic gradient descent:


(18)
Ki = i i1 Vi1 (si )   Vi1 (si+1 )
The next reviewed approach is the (recursive form of the) Least-Squares Temporal Differences (LSTD) algorithm of Bradtke and Barto (1996), which is only defined for a linear
parameterization (8) and for which the gain is defined recursively:
Ci1 (si )
1 + ((si )  (si+1 ))T Ci1 (si )
Ci1 (si )((si )  (si+1 ))T Ci1
Ci = Ci1 
1 + ((si )  (si+1 ))T Ci1 (si )
Ki =

(19)
(20)

where (s) is defined in (9) and for which the matrix C0 must be initialized. LSTD also seeks
to minimize the L2 -norm of the Bellman residual, however using a least-squares approach
rather than a gradient descent and using the instrumental variable concept (Soderstrom
487

fiGeist & Pietquin

& Stoica, 2002) to cope with stochasticity of transitions1 . This algorithm has also been
extended to eligibility traces (for details, see Boyan, 1999).
The last reviewed approach, which is certainly the closest to this contribution, is the
Gaussian Process Temporal Differences (GPTD) algorithm of Engel (2005). A linear parameterization V (s) = (s)T  is assumed2 and the following statistical generative model
(obtained from the Bellman evaluation equation) is considered:


 

 
1  0 . . . 
T
r1
n1
0 1  0  (s1 )
  .. 
 ..  
 .. 
(21)
 .  +  . 
 .  =  .. . .
..
.
.
.  
T
ri
(si )
ni
0 ...
0
1
By assuming that the noise nj is white (and therefore centered), Gaussian and of variance j ,
and that the prior over parameters follows a normal distribution, the posterior distribution
of (|r1 , . . . , ri ) can be analytically computed. Moreover, by using the Sherman-Morrison
formula, a recursive algorithm satisfying the Widrow-Hoff update rule (7) can be obtained
(assuming a prior P0 ):
Ki =

i2

Pi = Pi1 

Pi1 ((si )  (si+1 ))
+ ((si )  (si+1 ))T Pi1 ((si )  (si+1 ))

(22)

Pi1 ((si )  (si+1 ))((si )  (si+1 ))T Pi1
i2 + ((si )  (si+1 ))T Pi1 ((si )  (si+1 ))

(23)

Alternatively, GPTD (with parametric representation) can be seen as the linear leastsquares solution of the L2 Bellman residual minimization.
Only the most classical value function approximation algorithms have been presented,
however many other exist. Nevertheless, to our knowledge none of them presents all the
features argued before as being desirable. Most of them assumes linearity, at least to
ensure convergence (Tsitsiklis & Roy, 1997; Schoknecht, 2002) and sometime even to be
applicable (Bradtke & Barto, 1996; Boyan, 1999; Geramifard et al., 2006). Some other
algorithms do not assume linearity, as residual ones (Baird, 1995), however they are not
often practical (eg., a value iteration-like residual algorithm is proposed by Baird, but this
method requires computing the gradient of the max operator). Some of these methods are
more sample efficient than others. Generally speaking, second order approaches tend to be
more efficient than first order one, and LSTD is usually recognized as being a sample efficient
approach. Algorithms which use a learning rate can partially cope with non-stationarity, by
using an adaptive learning rate for example. However the LSTD approach is known to not
1. This point of view is historical. Since then, it has been shown that LSTD actually minimizes the distance
between the value function and the projection onto the hypothesis space of its image through the Bellman
operator (Lagoudakis & Parr, 2003).
2. Actually, Engels work is more general. It models the value function itself as a Gaussian process and
uses a dictionary method to obtain a sparse representation (without this procedure, the value function
would be represented as a vector with as many components as visited states). However, if this dictionary
method is used in a preprocessing step, the Gaussian process nonparametric representation reduces to
the proposed parametric linear representation, basis functions being kernels. Constructing the parameterization automatically and online is surely of interest, but the proposed point of view makes further
comparisons easier.

488

fiKalman Temporal Differences

take into account non-stationarity (which explains that it is almost never used in optimistic
policy iteration or incremental actor-critic schemes), see for example the work of Phua
and Fitch (2007). Many recent approaches for handling the dilemma between exploration
and exploitation use some uncertainty information (eg., see Dearden, Friedman, & Russell,
1998 or Strehl, Li, Wiewiora, Langford, & Littman, 2006). However, as far as we know,
very few algorithms allow providing uncertainty information within a value approximation
context, and among them is the GPTD framework of Engel (2005). However, contrary to
this contribution the effective use of this information is left for future work. Like LSTD,
GPTD algorithms are sample efficient but they do not handle non-stationarity3 . Yet, GPTD
and KTD frameworks share some similarities, this is discussed throughout this paper. The
motivation behind KTD is to handle all these aspects at the same time.
1.3 Paper Outline
The next section introduces an alternative point of view of value function approximation
and introduces informally Kalman filtering and the state-space representation, upon which
our contribution is built.
Determinism of MDP is assumed in Section 3 and the general Kalman Temporal Differences framework is derived. Deterministic transitions are to be linked to a white noise
assumption which is necessary to KTD derivation. It is then specialized using an approximation scheme, the Unscented Transform (UT) of Julier and Uhlmann (2004) to derive a family
of practical algorithms. In Section 4, a colored noise model initially introduced by Engel,
Mannor, and Meir (2005) is used to extend the KTD framework to the case of stochastic transitions. An eXtended KTD (XKTD) framework is proposed, and its combination
with off-policy learning is discussed. Convergence is analysed in Section 5. Under white
noise assumption, it is shown that KTD minimizes a weighted square Bellman residual.
Under colored noise assumption, it is shown that XKTD indeed performs a least-squares
supervised learning associating state values to observed Monte Carlo returns of cumulative
rewards. This is the same solution as LSTD(1), which is an unbiased estimator of the value
function. Section 6 shows how to compute uncertainty about value estimates from this
framework and introduces a form of active learning scheme which aims at improving speed
of convergence of KTD-Q, the KTD value iteration-like algorithm. The proposed framework
is then experimented and compared to state of the art RL algorithms. Each experiment is
a classic RL benchmark which aims at highlighting a specific features of KTD. Last section
discusses position of the proposed framework to other related approaches and offers some
perspectives.

2. An Alternative Point of View
The previous section presented the standard vision of the reinforcement learning problem
and of its formulation under the MDP framework. Here an alternative point of view is
introduced.
3. LSTD and GPTD could certainly be extended to the non-stationary case, for example by introducing
some forgetting factor. However, this is not how they have been designed initially, and the aim of this
paper is not to provide LSTD nor GPTD variations.

489

fiGeist & Pietquin

2.1 Informal Idea
In this paper, a novel approach based on an alternative point of view is proposed. A
stochastic dynamic system is seen as possessing underlying value functions V  RS and
state-action value functions Q  RSA that an agent can observe by interacting with the
system. When an agent takes an action, it provokes a state change and the generation of a
reward. This reward is actually a local observation of the set of underlying value functions
ruling the behavior of the system. From a sequence of such observations, the agent can
infer information about any of the value functions. A good estimate of the value function
V (s) (resp. state-action value function Q(s, a)) is given by the conditional expectation over
all possible trajectories of V (s) (resp. Q(s, a)) given the sequence of observed rewards:
Vi (s) = E[V (s)|r1 , . . . , ri ]

(24)

Qi (s, a) = E[Q(s, a)|r1 , . . . , ri ]

(25)

Interacting with the system therefore becomes a mean to generate observations that
helps estimating value functions which are hidden properties of the system. From these value
function estimates, the followed policy can be modified to move towards the optimal policy.
It is also legitimate to adopt a behavior that allows gathering meaningful observations which
relates to the exploration versus exploitation dilemma.
Two special cases of value functions are the one associated to the followed policy 
and the one associated to the optimal policy   . The rest of this paper concentrates on
estimating these two particular value functions or associated Q-functions.
Equations (24) and (25) are not solvable in the general case but inferring hidden variables
from observations is typically treated by Kalman filtering in the signal processing and
optimal control communities. Value functions will be considered as generated by a set of
parameters and the search is for the optimal set of hidden parameters  that provides the
best estimate of the value function (see Section 3.1). In the following, Kalman filtering is
first introduced and a method casting (state-action) value function approximation into the
Kalman filtering framework and using Bellman equations to build a so-called state-space
representation of the problem is proposed.
2.2 Kalman Filtering
Originally, the Kalman (1960) filtering paradigm aims at tracking the hidden state X (modeled as a random vector) of a non-stationary dynamic system through indirect observations
{Y1 , . . . , Yi } of this state. To do so, at time i  1 the algorithm computes a prediction of
the state (Xi|i1 ) and observation (Yi|i1 ) at time i, knowing analytically how states evolve
and generate observations as clarified below. After the actual next observation Yi is known
(at time i), the state prediction is corrected to obtain the state estimate Xi|i using the
observation prediction error (ei = Yi  Yi|i1 ) according to the following Windrow-Hoff-like
equation:
Xi|i = Xi|i1 + Ki (Yi  Yi|i1 ) = Xi|i1 + Ki ei
(26)
where Ki is the Kalman gain which will be further described hereafter. In the original work
of Kalman, the linear form of equation (26) is a constraint: adopting a statistical point of
view, the goal of the Kalman filter is to recursively compute the best linear estimate Xi of
490

fiKalman Temporal Differences

the state at time i given the sequence of observations {Y1 , . . . , Yi }. Kalman considers the
best estimate to be the one that minimizes the quadratic cost function
Ji (X) = E[kXi  Xk2 |Y1 , . . . , Yi ]

(27)

To compute the optimal gain Ki under the constraints (26) and (27), several assumptions
are made.
First, the evolution of the system is supposed to be ruled by a so-called evolution equation
or process equation (using the possibly non-stationary fi function) which is known:
Xi+1 = fi (Xi ) + vi

(28)

Equation (28) links the next state Xi+1 with the current one Xi and vi is a random noise
usually named evolution noise or process noise modeling the uncertainty in the evolution.
Second, observations are supposed to be linked to states by another known function gi
used in the typically called observation equation or sensing equation:
Yi = gi (Xi ) + wi

(29)

Equation (29) relates the current observation Yi to the current state Xi and wi is a random
noise usually named observation noise modeling the uncertainty induced by the noisy observation. This noise together with the process noise are at the origin of the state estimation
problem (estimating the current state from history of observations).
Equations (28) and (29) provide the so-called state-space description of the system. The
major assumptions of Kalman is that vi and wi are additive, white and independent noises
of variance Pv and Pw respectively, meaning that:
E[vi ] = E[wi ] = 0
E[vi  wj ] = 0

(30)

i, j

E[vj  vi ] = E[wj  wi ] = 0

(31)
i 6= j

(32)

Given these assumptions and the constrains (26) and (27) and adopting a statistical
point of view, the Kalman filter algorithm provides the optimal quantities Xi|i1 , Yi|i1 and
Ki :
Xi|i1 = E[Xi |Y1 , . . . , Yi1 ] = E[fi1 (Xi1 ) + vi1 |Y1 , . . . , Yi1 ]
= E[fi1 (Xi1 )|Y1 , . . . , Yi1 ] = E[fi1 (Xi1|i1 )],

(33)

Yi|i1 = E[Yi |Y1 , . . . , Yi1 ] = E[gi (Xi ) + wi |Y1 , . . . , Yi1 ]
= E[gi (Xi )|Y1 , . . . , Yi1 ] = E[gi (Xi1|i1 )],
Ki =

PXei Pe1
.
i

(34)
(35)

where PXei = E[(Xi  Xi|i1 )ei |Y1 , . . . , Yi1 ] and Pei = cov(ei |Y1 , . . . , Yi1 ).
It is not in the scope of this paper to provide the complete development leading to these
general results which are provided by Kalman (1960). Yet, Section 3 will provide further
developments in the specific case of RL.
491

fiGeist & Pietquin

Several important comments can be made at this stage. First, no specific assumption
has been made about the distributions of the noises v and w except that they have a zeromean and known variances (Pv and Pw ). Given this, the Kalman filter provides the best
linear estimator (in the sense that the estimators update rule is linear) of the systems state
which may not be optimal. Yet, if these two noises have Gaussian distributions, they are
totally described by their mean and variance. In this specific case, the linear estimate is
thus the optimal estimate and the Kalman filter algorithm provides the optimal solution.
In this paper, the Gaussian assumption is never made and only the best linear estimator is
considered.
Second, no linear assumption has been made concerning functions fi and gi . Although
Kalman (1960) provides exact solutions to the estimation problem in the case of linear
state-space equations, only quantities involved in (33), (34) and (35) are required. There
exists approximation schemes to estimate these quantities even in the case of non-linear
equations. Extended Kalman filters and the unscented transform (see Section 3.2.2) are
such schemes.
Finally, Kalman filtering should not be mistaken for Bayesian filtering. Bayesian filtering would consist in computing the complete posterior distribution of the state given
the observations. Kalman filtering only focuses on the first and second moments of this
distribution (mean and variance) with a constrained linear update. In the case of Gaussian
distributions, Bayesian filtering reduces to Kalman filtering but is more complex in the
general case. In this paper, only Kalman filtering is considered.
2.3 State-space Formulation for the Value Function Evaluation Problem
Before providing the general framework, underlying ideas are introduced through the value
function V  (s) evaluation problem. As providing some uncertainty information about estimates is considered as a desired feature, a statistical point of view is adopted and the
parameter vector  is modeled as a set of random variables. Another desired feature is
to track the solution rather than converging to it. This suggests adopting some evolution
model for the value function (through the parameters). However, dynamics of the value
function are hard to model, as they depend on whether the dynamic system to be controlled is non-stationary or the value function evaluation takes place in a generalized policy
iteration scheme4 . Here a heuristic evolution model following the Occam razor principle is
adopted and parameters evolution is modeled as a random walk:
i = i1 + vi

(36)

In this equation, i is the (true) parameter vector at time i and vi is the evolution noise. It
is assumed white (that is centered, and at two different time steps, noises are independent),
but no hypothesis is done about its distribution. The parameter vector i is thus a random
process. As it is stationary (because E[i ] = E[i1 ]), it should not harm the case where the
value function is stationary. On the other hand, it should allow tracking a non-stationary
value function (even if this evolution model is not the true one, which cannot anyway be
obtained in the general case).
4. Each time the policy is improved, the associated value function changes too. Therefore, the value function
to be learnt is non-stationary.

492

fiKalman Temporal Differences

Another issue is to link what is observed (the reward) to what needs to be inferred (the
parameter vector representing the value function). The Bellman evaluation equation is a
good candidate to produce such an observation model:
ri = V  (si )  V  (si+1 )

(37)

However, the solution of the Bellman equation does not necessarily lie in the hypothesis
space (the set of functions which can be represented by the parameter vector, for a given
representation). Therefore there is some inductive bias ni , which is modeled here as a
centered noise:
ri = Vi (si )   Vi (si+1 ) + ni
(38)
Notice again that no Gaussian assumption is made about the distribution of this noise.
Evolution and observation models can be summarized in the following state-space formulation:
(
i = i1 + vi
(39)
ri = Vi (si )   Vi (si+1 ) + ni
This is a model of value function approximation. It is assumed that there exists some
parameter random process i which generates the rewards through the Bellman evaluation
equation, these observations being noisy due to some inductive bias and to the fact that
a sampled Bellman equation is used instead of the true one. States and actions can be
considered here as exogenous variables which are part of the definition of the observation
model at time i. Estimating the value function reduces here to the estimation of this
hidden random process. It can be addressed by Bayesian filtering, which aims at estimating
the whole distribution of i conditioned on past observed rewards. In this paper a more
restrictive point of view is adopted, the Kalman filtering one, and only mean and variance
of this distribution are estimated with a restriction to linear update rules.

3. KTD: the Deterministic Case
From now on and through the rest of this section the focus is on deterministic Markov
decision processes. Transitions become deterministic and Bellman equations (4-6) simplify
as follows:
V  (s) = R(s, (s), s0 ) + V  (s0 ), s
0





0

0

Q (s, a) = R(s, a, s ) + Q (s , (s )), s, a


0



0

Q (s, a) = R(s, a, s ) +  max Q (s , b), s, a
bA

(40)
(41)
(42)

In this section are provided the derivation of the most general KTD algorithm as well as
specializations to practical implementations.
3.1 The General Framework
A very general point of view is adopted now. A transition is generically noted as:


(si , si+1 )
ti = (si , ai , si+1 , ai+1 )


(si , ai , si+1 )
493

(43)

fiGeist & Pietquin

given that the aim is the value function evaluation, the Q-function evaluation or the Qfunction optimization (in other words, the direct evaluation of the optimal Q-function).
Similarly, for the same cases, the following shortcuts hold:


Vi (si )   Vi (si+1 )
gti (i ) = Qi (si , ai )   Qi (si+1 , ai+1 )


Qi (si , ai )   maxbA Qi (si+1 , b)

(44)

Then all TD errors can be written generically as
i = ri  gti (i )

(45)

A statistical point of view is adopted. As said before, the original Kalman (1960) filter
paradigm aims at tracking the hidden state (modeled as a random variable) of a nonstationary dynamic system through indirect observations of this state. The idea behind
KTD is to express value function approximation as a filtering problem: the parameters are
the hidden state to be tracked (modeled as random variables following a random walk),
the observation being the reward linked to the parameters through a Bellman equation.
The problem at sight can then be stated in a so-called state-space formulation (this term
comes from Kalman filtering literature and should not be confused with the state space of
an MDP):
(
i = i1 + vi
(46)
ri = gti (i ) + ni
This expression is fundamental for the proposed framework. Using the vocabulary of
Kalman filtering, the first equation is the evolution equation, it specifies that the real
parameter vector follows a random walk which expectation corresponds to the optimal estimate of the value function. The evolution noise vi is white, independent and of variance
matrix Pvi (to be chosen by the practitioner, this is further discussed in section 7). Notice
that this equation is not an update of the parameters (addressed later), but model their
natural evolution over time, according to the Kalman filtering paradigm described in Section 2.2; notably this allows handling non-stationarity of the targeted value function. The
second equation is the observation equation, it links the observed transition to the value
(or Q-) function through a Bellman equation, see (44). The observation noise ni is supposed white, independent and of (scalar) variance Pni (also to be chosen by the practitioner
and further discussed in section 7). Notice that this mandatory assumption does not hold
for stochastic MDP, that is why deterministic transitions are supposed here. More details
about this assumption and its consequences are given in Section 4. Given deterministic
transitions, this model noise arises because the solution of the Bellman equation does not
necessarily exists in the hypothesis space induced by the parameterization. Notice that the
choice of the nature of the approximator (choice of the structure of a neural network, of
basis functions for linear parameterization, etc.) is an important topic in reinforcement
learning and more generally in machine learning. Nevertheless, it is not addressed here,
and it has to be chosen by the practitioner.
494

fiKalman Temporal Differences

3.1.1 Minimized Cost Function
An objective could be to estimate the whole distribution of parameters conditioned on
past observed rewards, which can be addressed by Bayesian filtering. However, it is a
difficult problem in the general case. Here a more simple objective is chosen: estimating the
(deterministic) parameter vector which minimizes the expectation over true parameters of
the mean-squared error conditioned on past observed rewards. The idea is that information
is provided by observed transitions and associated rewards, and that knowing the mean of
the posterior distribution should be enough. The associated cost can be written as:


Ji () = E ki  k2 |r1:i with r1:i = r1 , . . . , ri

(47)

Notice that if i is a random vector (of which distribution is not known),  is a deterministic
vector. Generally speaking, the optimal solution or minimum mean square error (MMSE)
estimator is the conditional expectation5 :
argmin Ji () = i|i = E [i |r1:i ]

(48)



However, except in specific cases, this estimator is not analytically computable. Instead,
the aim is here to find the best linear estimator of i . It can be written in a form quite
similar to equation (7):
i|i = i|i1 + Ki ri
(49)
In Equation (49), i|i is the estimate of i at time i and i|i1 = E[i |r1:i1 ] is its prediction
according to past observed rewards r1:i1 , given the evolution equation. For a random walk
model the following holds (recall that the evolution noise is white):
i|i1 = E [i1 + vi |r1:i1 ] = E [i1 |r1:i1 ]
= i1|i1

(50)

The innovation
ri = ri  ri|i1

(51)

is the difference between the actual observed reward ri and its prediction ri|i1 based on
the previous estimate of the parameter vector and the observation equation (recall that the
observation noise is also white):
ri|i1 = E [ri |r1:i1 ] = E [gti (i ) + ni |r1:i1 ]
= E [gti (i )|r1:i1 ]

(52)

Note that the innovation ri is not exactly the temporal difference defined in Equation (45),
which is a random variable through its dependency to the random vector i . It is its
expectation conditioned on past observed data: ri = E[i |r1:i ].
5. This is quite intuitive, the best deterministic estimator (in a least-squares sens) of a random variable is
its mean.

495

fiGeist & Pietquin

3.1.2 Optimal Gain
Using classical equalities, the cost function can be rewritten as the trace of the matrix
variance of parameters error:


Ji () = E ki  k2 |r1:i


= E (i  )T (i  )|r1:i


= trace E (i  )(i  )T |r1:i
(53)
Recall that we restrict ourselves to the class of linear (and unbiased) estimators depicted
in Eq. (49). Therefore, the cost function Ji (i|i ) should be considered, and the unknown is
the gain Ki :



Ji (i|i ) = trace cov i  i|i |r1:i
(54)
A first step to the computation of the optimal gain is to express the conditioned covariance
over parameters as a function of the gain Ki . A few more notations are first introduced
(recall also (51), the definition of the innovation):

 = i i|i


 and i|i1 = i i|i1

 i|i
Pi|i = cov i|i |r1:i
and Pi|i1 = cov i|i1 |r1:i1
(55)
h
i


P = cov (r |r
r |r
) and P = E 
ri

i

1:i1

ri

i|i1 i

1:i1

The various estimators being unbiased, the covariance can be expanded as follows:


Pi|i = cov i  i|i |r1:i




= cov i  i|i1 + Ki ri |r1:i1


= cov i|i1  Ki ri |r1:i1
T
+ Ki Pri KiT
Pi|i = Pi|i1  Pri KiT  Ki Pr
i

(56)

The optimal gain can thus be obtained by zeroing the gradient with respect to Ki of the
trace of this matrix.
First note that the gradient being linear, for three matrices of ad hoc dimensions A,
B and C (that is products ABAT and AC T are well defined), B being symmetric, the
following algebraic identities hold:

A trace ABAT = 2AB
(57)


T
T
A trace AC
= A trace CA
=C
(58)
and thus using Equation (56) and previous identities:

Ki trace Pi|i = 0


2Ki Pri  2Pri = 0



Ki = Pri Pr1
i
496

(59)

fiKalman Temporal Differences

Using Equations (56) and (59), the covariance matrix Pi|i can be recursively computed as
follows:
Pi|i = Pi|i1  Ki Pri KiT
(60)
Recall that no Gaussian assumption has been made to derive these equations. Nevertheless,
under Gaussian (and linear) assumptions, the optimal update is actually linear6 (for example, see Chen, 2003). Please also notice that this variance matrix encodes the uncertainty
over parameter estimates, and not the intrinsic uncertainty of the considered MDP (it is
not the variance of the random process from which the value function is the mean).
3.1.3 General Algorithm
The most general KTD algorithm can now be derived. It breaks down in three stages. The
first step consists in computing predicted quantities i|i1 and Pi|i1 . These predictions
being made from past estimates, the algorithm has to be initialized with priors 0|0 and P0|0 .
Recall that for a random walk model, Equation (50) holds, and the predicted covariance
can also be computed analytically:


Pi|i1 = cov i|i1 |r1:i1


= cov i1|i1 + vi |r1:i1
= Pi1|i1 + Pvi

(61)

(recall that Pvi is the problem-dependent variance matrix of the evolution noise, to be
chosen by the practitioner).
The second step is to compute some statistics of interest. It will be specialized for
each algorithm in Section 3.2. The first statistic to compute is the prediction ri|i1 (52).
The second statistic to compute is the covariance between the parameter vector and the
innovation:
h
i
Pri = E (i  i|i1 )(ri  ri|i1 )|r1:i1
(62)
However, from the state-space model (46), ri = gti (i ) + ni , and the observation noise is
centered and independent, so
h
i
Pri = E (i  i|i1 )(gti (i )  ri|i1 )|r1:i1
(63)
The last statistic to compute is the covariance of the innovation, which can be written as
(using again the characteristics of the observation noise):


Pri = E (ri  ri|i1 )2 |r1:i1


= E (gti (i )  ri|i1 + ni )2 |r1:i1


= E (gti (i )  ri|i1 )2 |r1:i1 + Pni

(64)

(recall that Pni is the variance of the observation noise).
6. In other words, in this case, the Kalman filtering solution is actually the Bayesian filtering solution.

497

fiGeist & Pietquin

The third and last step of the algorithm is the correction step. It consists in computing
the gain (59), correcting the predicted parameter vector (49) and updating the associated
covariance matrix (60) accordingly. The proposed general framework is summarized in
Algorithm 1. Notice the similarity between the correction equation (i|i = i1|i1 + Ki (ri 
ri|i1 )) and the Widrow-Hoff equation where the approximated value is corrected in the
direction of the error (the innovation is indeed the TD error). The gain Ki can be seen as
a set of adaptive learning rates.
Algorithm 1: General KTD algorithm
Initialization: priors 0|0 and P0|0 ;
for i  1, 2, . . . do
Observe transition ti and reward ri ;
Prediction step;
i|i1 = i1|i1 ;
Pi|i1 = Pi1|i1 + Pvi ;
Compute statistics of interest;
ri|i1 = E[gti (i )|r1:i1 ] ;
h
i
Pri = E (i  i|i1 )(gti (i )  ri )|r1:i1 ;


Pri = E (gti (i )  ri|i1 )2 |r1:i1 + Pni ;
Correction step;
Ki = Pri Pr1
;
i

i|i = i|i1 + Ki ri  ri|i1 ;
Pi|i = Pi|i1  Ki Pri KiT ;

3.2 Specializations
The main difficulty in applying KTD is to compute the statistics of interest ri|i1 , Pri
and Pri (for which statistics i|i1 and Pi|i1 are necessary). First, the value function
evaluation in the case of a linear parameterization is considered. The related Bellman
equation is (40). In this case an analytical derivation is possible. Then an approximation
scheme, the unscented transform (UT) of Julier and Uhlmann (2004), is introduced. It
allows solving the same problem for a nonlinear parameterization. Q-function evaluation
and direct optimization follow.
3.2.1 KTD-V: Linear Parameterization
Here the linear parameterization of equation (8) is adopted, that is V (s) = (s)T . The
state-space formulation (46) can thus be rewritten as:
(
i = i1 + vi
ri = ((si )  (si+1 ))T i + ni
498

(65)

fiKalman Temporal Differences

Notice that as the problem at sight is the evaluation of a deterministic policy, no action has
to be observed. The policy being fixed, the MDP reduces to a valued Markov chain. To
shorten notations, Hi is defined as:
Hi = (si )  (si+1 )

(66)

As the observation equation is linear, the statistics of interest can be derived analytically.
The prediction is:
ri|i1 = E [gti (i )|r1:i1 ]


= E HiT i |r1:i1
= HiT E [i |r1:i1 ]
= HiT i|i1

(67)

The covariance between the parameter vector and the innovation can also be computed
analytically:
h
i

Pri = E i|i1 gti (i )  ri|i1 |r1:i1
h
i
= E i|i1 HiT i|i1 |r1:i1
h
i
T
= E i|i1 i|i1
|r1:i1 Hi
= Pi|i1 Hi

(68)

The covariance of the innovation is derived analytically as well:
h

i
2
gti (i )  ri|i1 |r1:i1 + Pni


2
T
= E Hi i|i1 |r1:i1 + Pni

Pri = E

= HiT Pi|i1 Hi + Pni

(69)

The optimal gain can thus be defined algebraically and recursively:
Ki =

Pi|i1 Hi
T
Hi Pi|i1 Hi +

Pni

(70)

The KTD-V approach for linear parameterization is summarized in Algorithm 2.
Notice that this gain shares similarities with the gain (19) of the LSTD algorithm
(Bradtke & Barto, 1996), which is not a surprise. LSTD is based on a least-squares
minimization (however with the introduction of instrumental variables in order to handle stochastic transitions), and the Kalman filter can be seen as a stochastic generalization
of the least-squares method. This gain shares also similarities with GPTD. Actually, if
the process noise is set to 0 (that is Pvi = 0), then KTD-V with linear parameterization
499

fiGeist & Pietquin

Algorithm 2: KTD-V: linear parameterization
Initialization: priors 0|0 and P0|0 ;
for i  1, 2, . . . do
Observe transition (si , si+1 ) and reward ri ;
Prediction step;
i|i1 = i1|i1 ;
Pi|i1 = Pi1|i1 + Pvi1 ;
Compute statistics of interest;
ri|i1 = HiT i|i1 ;
Pri = Pi|i1 Hi ;
Pri = HiT Pi|i1 Hi + Pni ;
/*
where Hi = (si )  (si+1 )

*/

Correction step;
;
Ki = Pri Pr1
i

i|i = i|i1 + Ki ri  ri|i1 ;
Pi|i = Pi|i1  Ki Pri KiT ;

and GPTD are the same algorithm7 , see Equation (22). This is not a surprise: under a
linear and Gaussian hypothesis, state-space (65) with zero evolution noise is equivalent to
the statistical generative model (21). An alternative point of view is that both approaches
provide the least-squares solution to the L2 Bellman residual minimization.
Although linear parameterization is widely used, one can be interested in using a nonlinear one (for optimal basis function search or more compact function representation for
instance). Another case of interest (addressed later) is to handle the max operator which is
inherent to the Bellman optimality equation. This is how the proposed approach notably
differs from Engels framework. Basically, the issue of computing the statistics of interest for
KTD can be stated as the following problem: given the mean and covariance of a random
variable (i|i1 and Pi|i1 for KTD), how can the mean and covariance of a nonlinear (and
perhaps non-differentiable) mapping (gti for KTD) of this random variable be computed?
The following section presents the unscented transform, which is an approximation scheme
designed to handle such a problem.

7. Once again, GPTD is more general than linear parameterization, the gain (22) being refereed to as
parametric GPTD by Engel (2005). Nevertheless, the non-parametric approach of GPTD actually
constructs online a kernel-based linear parameterization. At the end of learning, or if the parameterization is constructed in a preprocessing step, this non-parametric representation reduces to a linear
parametric representation. As the focus of this paper is how to learn parameters of a representation and
not the representation itself (which we totally recognize as being a problem of importance), GPTD is
always considered in its parametric form in this article.

500

fiKalman Temporal Differences

3.2.2 The Unscented Transform
Lets abstract from RL and Kalman filtering and consider the problem of non-linear mapping
of a random variable. Let X be a random vector, and let Y be a mapping of X. The problem
is to compute the mean and covariance of Y knowing the mapping and the first and second
order moments of X. If the mapping is linear, the relation between X and Y can be written
as Y = AX where A is a matrix of ad hoc dimension (that is number of row of Y times
number of rows of X). In this case, required mean and covariance can be analytically
computed as E[Y ] = AE[X] and E[Y Y T ] = AE[XX T ]AT . This result has been used to
derive the KTD-V algorithm of Section 3.2.1.
If the mapping is nonlinear, the relation between X and Y can be written as:
Y = f (X)

(71)

A first solution would be to approximate the nonlinear mapping by a first order Taylor
expansion around E[X]. This leads to the following approximations of the mean and covariance of Y :
E[Y ]  f (E[X])

(72)

E[Y Y T ]  (f (E[X])) E[XX T ] (f (E[X]))T

(73)

This approach is the basis of Extended Kalman Filtering (EKF) (for example, see Simon,
2006), which has been extensively studied and used in past decades. However it has some
limitations. First it cannot handle non-derivable nonlinearities, and thus cannot handle
the Bellman optimality equation (6) because of the max operator. It requires to compute
the gradient of the mapping f , which can be quite difficult even if possible (eg., neural
networks). It also supposes that the nonlinear mapping is locally linearizable in order to
have a good approximation, which is unfortunately not always the case and can lead to
quite bad results, as exemplified by Julier and Uhlmann (2004).
The basic idea of unscented transform is that it is easier to approximate an arbitrary
random vector (with samples) than an arbitrary nonlinear function. Its principle is to sample
deterministically a set of so-called sigma-points from the expectation and the covariance of
X. The images of these points through the nonlinear mapping f are then computed, and
they are used to approximate statistics of interest. It shares similarities with Monte-Carlo
methods, however here the sampling is deterministic and requires less samples to be drawn,
nonetheless guaranteeing a given accuracy (Julier & Uhlmann, 2004).
The original unscented transform is now described more formally (some variants have
been introduced since then, the basic principle being the same). Let n be the dimension of
X. A set of 2n + 1 so-called sigma-points is computed as follows:
x(0) = X
x(j)
x(j)

p

= X +
(n + )PX
j
p

= X 
(n + )PX

jn

j=0

(74)

1jn

(75)

n + 1  j  2n

(76)

as well as associated weights:
w0 =


n+

and

wj =
501

1
2 (n + )

j > 0

(77)

fiGeist & Pietquin

where X is the mean of X,pPX is its variance matrix,  is a scaling factor which controls
the sampling spread, and ( (n + )PX )j is the j th column of the Cholesky decomposition
of the matrix (n + )PX . Then the image through the mapping f is computed for each of
these sigma-points:
y (j) = f (x(j) ), 0  j  2n
(78)
The set of sigma-points and their images can finally be used to approximate first and second
order moments of Y , and even PXY , the covariance matrix between X and Y :
Y  y =

2n
X

wj y (j)

(79)

j=0

PY 

2n
X



wj y (j)  y



y (j)  y

T

(80)

j=0

PXY 

2n
X



T
wj x(j)  X y (j)  y

(81)

j=0

Thanks to the unscented transform, it is possible to address the value function evaluation problem with nonlinear parameterization, the random vector X being in this case the
parameter vector, and its nonlinear mapping Y the predicted reward.
3.2.3 KTD-V: Nonlinear Parameterization
In this section a generic parameterization of the value function V is considered: it can be
a neural network (Bishop, 1995), a semi-parametric kernel representation (Geist, Pietquin,
& Fricout, 2008), or any function representation of interest, as long as it can be described
by a set of p parameters. The general state-space formulation (46) can thus be written as:
(
i = i1 + vi
(82)
ri = Vi (si )   Vi (si+1 ) + ni
The problem is still to compute the statistics of interest, which becomes tractable with the
unscented transform. The first thing to compute is the set of sigma-points from known
statistics i|i1 and Pi|i1 as well as the associated weights using Equations (74-77), as
described in Section 3.2.2:
n
o
(j)
(83)
i|i1 = i|i1 , 0  j  2p
W = {wj , 0  j  2p}

(84)

Then the images of these sigma-points are computed (a predicted reward for each of the
sampled parameter vectors), using the observation function of state-space model (82), which
is linked to the Bellman evaluation equation (40):


(j)
Ri|i1 = ri|i1 = V(j) (si )   V(j) (si+1 ), 0  j  2p
(85)
i|i1

i|i1

502

fiKalman Temporal Differences

The sigma-points and their images being computed, the statistics of interest can be approximated by:
ri|i1 

2p
X

(j)

wj ri|i1

(86)


2
(j)
wj ri|i1  ri|i1 + Pni

(87)




(j)
(j)
wj i|i1  i|i1 ri|i1  ri|i1

(88)

j=0

Pr i 

2p
X
j=0

Pri 

2p
X
j=0

As the unscented transform is no longer an approximation for linear mapping, this formulation is still valid for value function evaluation with linear function approximation. KTD-V
with nonlinear function approximation is summarized in Algorithm 3. Notice that such
a general parameterization cannot be taken into account in GPTD nor LSTD. It is possible with direct algorithms (TD with function approximation), however there is a risk of
divergence. This is illustrated in Section 7.
3.2.4 KTD-SARSA
This section focuses on the Q-function evaluation of a fixed given policy. The associated
algorithm is called KTD-SARSA, which can be misleading. Indeed, SARSA is sometime
understood as a Q-function evaluation algorithm associated with an optimistic policy iteration scheme (eg., -greedy policy). Here the focus is on the Q-function evaluation problem,
and the control part is left apart. For a general parameterization Q , and considering the
Bellman evaluation equation (41), the state-space model (46) can be rewritten as:
(
i = i1 + vi
(89)
ri = Qi (si , ai )   Qi (si+1 , ai+1 ) + ni
For a fixed policy, the value function evaluation on the state space induced Markov chain8 is
quite similar to the Q-function evaluation on the state-action space induced Markov chain.
It is thus straightforward to extend KTD-V to Q-function evaluation. Recall that for a linear
parameterization, the unscented transform leads to an exact computation of statistics of
interest, and thus in this case Algorithm 3 (KTD-V) is equivalent to Algorithm 2. That
is why only the sigma-point formulation of KTD-SARSA is given, also summarized in
Algorithm 3.
LSTD and GPTD have also been generalized to the Q-function evaluation (see respectively Lagoudakis & Parr, 2003 and Engel, 2005). However, once again, these approaches
cannot handle a nonlinear parameterization, contrary to KTD-SARSA. Notice also that
if the parameterization is linear and the process noise is zero, KTD-SARSA is the same
algorithm as GPTD for Q-function evaluation (this is a direct extension of the equivalence between GPTD and KTD-V with linear parameterization and zero process noise, see
Sec. 3.2.1).
8. For a fixed policy, the MDP reduces to a Markov chain.

503

fiGeist & Pietquin

Algorithm 3: KTD-V, KTD-SARSA and KTD-Q
Initialization: priors 0|0 and P0|0 ;
for i  1, 2, . . . do


(si , si+1 ) (KTD-V)
Observe transition ti = (si , ai , si+1 , ai+1 ) (KTD-SARSA)


(si , ai , si+1 ) (KTD-Q)

and reward ri ;

Prediction Step;
i|i1 = i1|i1 ;
Pi|i1 = Pi1|i1 + Pvi ;
Sigma-points
n computation ; o
(j)
i|i1 = i|i1 , 0  j  2p (from i|i1 and Pi|i1 );
W = {wj , 0  j  2p } ;
Ri|i1 =
o
n
(j)

r
=
V
(KTD-V)
(j) (si )   V (j) (si+1 ), 0  j  2p

i|i1

i|i1
i|i1

o
n
(j)
ri|i1 = Q(j) (si , ai )   Q(j) (si+1 , ai+1 ), 0  j  2p (KTD-SARSA)
i|i1
i|i1

n
o


(j)

 ri|i1 = Q (j) (si , ai )   maxbA Q (j) (si+1 , b), 0  j  2p (KTD-Q)


i|i1

;

i|i1

Compute statistics of interest;
P
(j)
ri|i1 = 2p
j=0 wj ri|i1 ;
P
(j)
(j)
Pri = 2p
j=0 wj (i|i1  i|i1 )(ri|i1  ri|i1 );


2
P
(j)
Pri = 2p
+ Pni ;
j=0 wj ri|i1  ri|i1
Correction step;
Ki = Pri Pr1
;
i

i|i = i|i1 + Ki ri  ri|i1 ;
Pi|i = Pi|i1  Ki Pri KiT ;

3.2.5 KTD-Q
This section focuses on the Q-function optimization, that is on finding an approximate
solution to the Bellman optimality equation (42). A general parameterization Q is adopted.
The state-space model (46) can be specialized as follows:
(
i = i1 + vi
ri = Qi (si , ai )   maxbA Qi (si+1 , b) + ni

(90)

Here linear and nonlinear parameterizations are not distinguished, because of the nonlinearities induced by the max operator. It is tricky to handle, especially because of its
non-differentiability.
504

fiKalman Temporal Differences

Hopefully, as it approximates the random variable rather than the mapping, the unscented transform is a derivative-free approximation. Given the general KTD algorithm
introduced in Section 3.1.3 and the unscented transform described in Section 3.2.2, it is
possible to derive KTD-Q, the KTD algorithm for Q-function direct optimization. One has
first to compute the set of sigma-points associated with the parameter vector, as in equations (83-84). Then the mapping of these sigma-points through the observation equation of
state-space model (90), which contains the max operator, is computed:
o
n
(j)
(91)
Ri|i1 = ri|i1 = Q(j) (si , ai )   max Q(j) (si+1 , b), 0  j  2p
bA

i|i1

i|i1

Then, as usual, the sigma-points and their images are used to compute the statistics of
interest, as in equations (86-88). The proposed KTD-Q is summarized in Algorithm 3.
Notice that even if the parameterization is linear, there is no LSTD nor GPTD equivalent
to this algorithm. Actually, as linearity of the observation model is a mandatory assumption
for the derivation of these algorithms, the Bellman optimality operator cannot be taken
into account. As far as we know, KTD-Q is one of the first second order value iteration-like
algorithms. Choi and Van Roy (2006) propose a linear least-squares based bootstrapping
approach (to be discussed in Section 8) which can be used in a Q-learning-like setting.
Yu and Bertsekas (2007) also introduce a least-squares-based Q-learning. However, it is
designed for optimal stopping problems (which is a restrictive class of MDP) and it is
not truly online (to update the representation given a new observation, all the followed
trajectory are explicitly required). Roughly speaking, this algorithm is fitted-Q with a
least-squares for the supervised learning part and for which a new transition is added to
the learning basis at each iteration. Its computational complexity is cubic9 , which is higher
than the square complexity of KTD, as shown in the next section.
3.3 Algorithmic Complexity
Let p be the number of parameters. The unscented transform involves a Cholesky decomposition of which computational complexity is O(p3 ) in general. However, as the variance
update (60) is a rank one update, the Cholesky decomposition can be perfomed in O(p2 )
(eg., see Gill, Golub, Murray, & Saunders, 1974). The different algorithms imply to evaluate
2p + 1 times the gti function at each time-step. For KTD-V or KTD-SARSA and a general
parameterization, each evaluation is bounded by O(p). For KTD-Q, the maximum over
actions has to be computed. The notation A represents the cardinality of action space if finite, the computational complexity of the algorithm used to search the maximum otherwise
(eg., the number of samples times the evaluation complexity for Monte Carlo). Then each
evaluation is bounded by O(pA). Remaining operations are basic linear algebra, and are
thus bounded by O(p2 ). Therefore the global computational complexity (per iteration) of
KTD-V and KTD-SARSA is O(p2 ), and KTD-Q is in O(Ap2 ). As the mean and variance
matrix of parameters have to be maintained, the memory complexity is O(p2 ). Although
comparable to LSTD or GPTD complexity, this is higher than many other RL algorithms
which have a linear complexity. Nevertheless, most of value function approximation approaches assume a linear parameterization. KTD does not make this hypothesis (even to
9. However, the paper proposes some heuristics which reduce this complexity.

505

fiGeist & Pietquin

analyse convergence, as shown in Section 5.1) and so allows much more compact representations for the value function. Thus the quadratic complexity is a problem with important
counterparts.

4. KTD: the Stochastic Case
The KTD framework presented so far assumes deterministic transitions. If it is not the
case, the observation noise ni cannot be assumed as white (since it would include the MDP
stochasticity as well as the inductive bias), whereas it is a necessary condition for KTD
derivation. First it is shown that using KTD in a stochastic MDP involves a bias. Then a
colored noise model is introduced to alleviate this problem, and it is used to extend KTD.
The problem caused by off-policy learning, which prevents the derivation of an XKTD-Q
algorithm, is also discussed.
4.1 Stochastic Transitions and Bias
One can ignore this problem and use the cost function (47) linked to state-space model (46)
with stochastic transitions. However, similarly to approaches minimizing a squared Bellman
residual, such as residual algorithms of Baird (1995), this cost function is biased. More
precisely, it is biased relatively to stochasticity of transitions (parameters and transitions
are different sources of randomness). Additionally, this cost function being biased, the
estimator minimizing it (that is i|i ) is biased too.
Theorem 1. If the reward function only depends on the current state-action pair, and not
on the transiting state, then when used on a stochastic Markov decision process, the cost
function (47) is biased (relatively to stochasticity of transitions), its bias being given by:



2
0

 
kKi k E covs0 |si ,(si ) (ri + V (s )) |r1:i1

kKi k2 E cov (ri  gti ()) |r1:i1 = kKi k2 E covs0 |si ,(si ) (ri + Q (s0 , (s0 ))) |r1:i1

s0 |si ,ai



kKi k2 E covs0 |si ,ai (ri +  maxaA Q (s0 , a)) |r1:i1
(92)
It is clear that this bias is zero for deterministic transitions.
Proof. The assumption that the reward does not depend on the transiting state is made for
technically simplifying the demonstration, because of the conditioning of the cost function
on past observed rewards. Yet it is done without loss of generality. Under this hypothesis,
the state-space model to be considered for a stochastic MDP is:
(
i = i1 + vi
(93)
ri = Es0 |si ,ai [gti (i )] + ni
with ti now defined as the random quantity ti = (si , ai , s0 ). Notice that the observation
equation (minus the noise) is the Bellman equation for stochastic transitions. The difference
with state-space model (46) is that transitions are no more sampled but averaged. The
associated cost function is:


T
Ji () = trace Pi|i = trace Pi|i1  Pri KiT  Ki Pr
 Ki Pri KiT
(94)
i
506

fiKalman Temporal Differences

Calligraphic letters denote the same for state-space model (93) than notations (55) for
state-space model (46), eg.:
h
i


Pri = E i|i1 ri |r1:i1 with ri = ri  ri|i1 = ri  E Es0 |si ,ai [gti (i )] |r1:i1
(95)
Notice that the prediction of the reward is unbiased, thus the same holds for the innovation:




Es0 |si ,ai ri|i1 = ri|i1 and Es0 |si ,ai ri|i1 = ri|i1

(96)

The term Pi|i1 does not depend on transiting state s0 and the term Pri is linear in the
innovation, so they are unbiased:


Es0 |si ,ai Pi|i1 = Pi|i1 and Es0 |si ,ai [Pri ] = Pri

(97)

This is not the case for the variance of the innovation:
 

Es0 |si ,ai [Pri ] = Es0 |si ,ai E ri2 |r1:i1

 

= E Es0 |si ,ai ri2 |r1:i1
h
i


 
2
= E r2i |r1:i1 + E Es0 |si ,ai ri2  Es0 |si ,ai [ri ] |r1:i1


= Pri + E cov (ri ) |r1:i1
s0 |si ,ai

(98)

Thus the bias (Es0 |si ,ai [Ji ()]  Ji ()) can be computed:


Es0 |si ,ai [Ji ()]  Ji () = Es0 |si ,ai trace Ki (Pri  Pri ) KiT
= trace(Ki KiT )Es0 |si ,ai [Pri  Pri ]

= KiT Ki Es0 |si ,ai [Pri ]  Pri


2
= kKi k E cov (ri  gti ()) |r1:i1
s0 |si ,ai

(99)

Notice that neither V (si ) nor Q (si , ai ) depends on the transiting state s0 . Thus this proves
the result as expressed in Theorem 1.
This bias is quite similar to the one arising from the minimization of a square Bellman
residual. The result of Theorem 2 (see Section 5) even strengthen this parallel. A solution
could be to introduce an auxiliary filter to remove this bias, similarly to introduction of an
auxiliary function made by Antos, Szepesvari, and Munos (2008). However extension of this
work is not straightforward. Another approach could be to estimate this bias online so as
to remove it, similarly to what is done by Jo and Kim (2005) for least-mean square filtering.
However the Kalman filter is a much more complex framework than the least-squares filter,
especially when combined with unscented transform. Another interesting perspective could
be to introduce a colored observation noise as done by Engel (2005) in a Bayesian context
for Gaussian process-based algorithms. This last approach is presented and used to extend
KTD next.
507

fiGeist & Pietquin

4.2 A Colored Noise Model
First the focus is on value function evaluation. Extension to Q-function evaluation is
straightforward, and Q-function optimization is discussed later, because of its off-policy
aspect (the learnt policy is not the behaviorial one). The Bellman evaluation equation to
be solved is Equation (4): it has just been shown that directly using KTD in a stochastic problem induces a bias in the minimized cost function. A colored noise model which
was first proposed by Engel et al. (2005) (the basis of the so-called Monte-Carlo GPTD
algorithm) is first presented, before being adapted to extend the KTD framework.
The policy being fixed for evaluation, the MDP reduces in a valued Markov chain of
probability transition p (.|s) = p(.|s, (s)) and of reward R (s, s0 ) = R(s, (s), s0 ). The
value function can be defined as the expectation (over all possible trajectories) of the following discount return random process:


D (s) =


X

 i R (si , si+1 )|s0 = s, si+1  p (.|si )

(100)

i=0

This equation naturally leads to a Bellman-like anti-causal recurrence:
D (s) = R (s, s0 ) + D (s0 ), s0  p (.|s)

(101)

This random process can also be broken down in its mean plus a zero mean residual.
However by definition its mean is the value function V  (s) = E[D (s)], so by writing
V  (s) the residual:
D (s) = E[D (s)] + (D (s)  E[D (s)]) = V  (s) + V  (s)

(102)

Substituting Equation (102) into Equation (101), the reward can be expressed as a function
of the value plus a noise:
R (s, s0 ) = V  (s)  V  (s0 ) + N (s, s0 )

(103)

the noise being defined as:
N (s, s0 ) = V  (s)  V  (s0 )

(104)

As done by Engel et al. (2005), the residuals are supposed to be independent, which leads
to a colored noise model. This assumption is really strong, as transitions are likely to
render residuals dependent, however despite this some convergence guarantees are given in
Section 5.
Recall the observation equation of the state-space formulation (46): ri = gti (i ) + ni . In
the KTD framework, the observation noise ni is assumed white, which is necessary for the
algorithm derivation. In the eXtended Kalman Temporal Differences (XKTD) framework,
the colored noise model (104) is used instead.
The residual being centered and assumed independent, this noise is indeed a moving
average (MA) noise (here the sum of two white noises):
ni = ui + ui1 ,

ui  (0, i2 )

(105)

Notice that the white noise ui is centered with variance i2 , nevertheless no assumption is
made about its distribution (particularly no Gaussian assumption).
508

fiKalman Temporal Differences

4.3 Extending KTD
It is quite easy to use an autoregressive (AR) process noise in a Kalman filter by extending
the evolution equation (for example, see Simon, 2006). However, as far as we know, the
case of an MA observation noise has never been addressed before in the literature, whereas
it is necessary to extend KTD. Notice that this noise model is taken into account in a quite
different way in the GPTD framework. Basically, it is done using the partitioned matrix
inversion formula, which is not possible here due to the lack of linearity assumption.
4.3.1 eXtended Kalman Temporal Differences
Rederiving KTD in the case of an MA noise as done in Section 3.1 would be quite difficult.
Instead, it is proposed here to express the scalar MA noise ni as a vectorial AR noise. This
allows extending state-space model (46) to a new one for which Algorithm 1 applies rather
directly. Let i be an auxiliary random variable. Scalar MA noise (105) is equivalent to
the following vectorial AR noise:
  

  
i
i1
0 0
1
ui
(106)
=
+
ni
1 0
ni1

Indeed, from this vectorial AR noise, ni = i1  ui and i = ui , so ni = ui + ui1
T
which is the correct MA model. The noise u0i = ui ui is also centered and its variance
matrix is:


1 
2
(107)
Pu0i = i
  2
This new noise formulation having been defined, it is now possible to extend the statespace formulation (46):
(
xi = F xi1 + vi0
(108)
ri = gti (xi )
T
The parameter vector is now extended with the vectorial AR noise i ni :

xTi = iT i ni
(109)
Notice that as the observation noise ni is now a part of the extended parameter vector,
it is also estimated. The evolution matrix F takes into account the structure of the MA
observation noise. Let p be the number of parameters and Ip the identity matrix of size p,
the evolution matrix is written by bloc (0 denotes a zero p  1 column vector):


Ip 0 0
F =  0T 0 0 
(110)
0T 1 0
The process noise vi is also extended to take into account the MA observation noise. It is
still centered, however its variance matrix is extended using the variance matrix Pu0i (107):


Pv i
0
0
i2
i2 
(111)
Pvi0 =  0T
T
2
2
2
0
i  i
509

fiGeist & Pietquin

The observation equation remains the same:
ri = gti (xi ) = gti (i ) + ni

(112)

However now the observation noise is a part of the evolution equation, and it has to be
estimated.
Using this new state-space formulation, a general XKTD algorithm can be derived. It
is summarized in Algorithm 4. It is rather similar to Algorithm 1 with two slight changes:
the state-space to be considered is now given by Equation (108) and prediction of mean
and covariance of the extended random vector xi is done using the evolution matrix F
(which is the identity for KTD). Notice that the computational complexity is the same for
both algorithms, as the parameter vector is extended with only two scalars. As for KTD,
XKTD can be specialized to XKTD-V (value function evaluation) and XKTD-SARSA (Qfunction evaluation). The reasoning is the same as in Section 3.2 and practical approaches
are given in Algorithm 5. Yet, specialization to XKTD-Q is not straightforward because of
its off-policy nature, as explained in section 4.3.2.
Recall that KTD with zero process noise and linear parameterization is the same algorithm as GPTD (see Sec. 3.2.1). Actually, the same holds for XKTD with zero process noise
and linear parameterization and MC-GPTD (the algorithm obtained using the same colored
noise model in the GPTD framework, however in a different manner, see Engel et al., 2005).
This can be easily (but lengthly) checked by expanding XKTD equations in the linear case.
Once again, MC-GPTD can certainly be extended to handle non-stationarities, even if it is
less natural than for XKTD, but it cannot handle nonlinear parameterization. From this
point of view, XKTD extends MC-GPTD.
Algorithm 4: General XKTD algorithm
Initialization: priors x0|0 and P0|0 ;
for i  1, 2, . . . do
Observe transition ti and reward ri ;
Prediction step;
xi|i1 = F xi1|i1 ;
Pi|i1 = F Pi1|i1 F T + Pvi0 ;
Compute statistics of interest (using UT);
ri|i1 = E[gti (i ) + ni |r1:i1 ] ;


Pxri = E  (xi  xi|i1 )(gti (i ) + ni  ri|i1
 )|r1:i1 ;

Pri = E (gti (i ) + ni  ri|i1 )2 |r1:i1 ;
Correction step;
Ki = Pxri Pr1
;
i

xi|i = xi|i1 + Ki ri  ri|i1 ;
Pi|i = Pi|i1  Ki Pri KiT ;

510

fiKalman Temporal Differences

Algorithm 5: XKTD-V and XKTD-SARSA

T
T
0 0
and P0|0 ;
Initialization: priors x0|0 = 0|0
for i  1, 2, . . . do
(
(si , si+1 ) (XKTD-V)
Observe transition ti =
(si , ai , si+1 , ai+1 ) (XKTD-SARSA)

and reward ri ;

Prediction Step;
xi|i1 = F xi1|i1 ;
Pi|i1 = F Pi1|i1 F T + Pvi0 ;
Sigma-points
o
n computation ;
(j)
Xi|i1 = xi|i1 , 0  j  2p + 4 (from xi|i1 and Pi|i1 );
W = {wj , 0  j  2p + 4 } ;

(j)
(j)
(j)
(j)
/* notice that (xi|i1 )T = (i|i1 )T i|i1 ni|i1
*/
R
=
i|i1
n
o
(j)
(j)

 ri|i1 = V(j) (si )   V(j) (si+1 ) + ni|i1 , 0  j  2p + 4 (XKTD-V)
i|i1
i|i1
n
o
(j)
(j)

 ri|i1 = Q(j) (si , ai )   Q(j) (si+1 , ai+1 ) + ni|i1 , 0  j  2p + 4 (XKTD-SARSA)
i|i1

i|i1

Compute statistics of interest;
P
(j)
ri|i1 = 2p+4
j=0 wj ri|i1 ;
P
(j)
(j)
Pxri = 2p+4
j=0 wj (xi|i1  xi|i1 )(ri|i1  ri|i1 );

2
P
(j)
Pri = 2p+4
w
r

r
;
j
i|i1
j=0
i|i1
Correction step;
Ki = Pxri Pr1
;
i

xi|i = xi|i1 + Ki ri  ri|i1 ;
Pi|i = Pi|i1  Ki Pri KiT ;

4.3.2 XKTD and Off-policy Learning
Off-policy learning is the problem of learning the value of one policy (the target policy)
while following another one (the behavior policy). KTD-Q (or more generally Q-learninglike algorithms) is an example of off-policy learning: the behavior policy is any sufficiently
exploratory policy while the learnt policy is the optimal one. More generally, off-policy
learning is of interest, for example to reuse previous trajectories or if the behavioral policy
cannot be controlled.
Using a colored observation noise results in a memory effect, similarly to what happens
with eligibility traces for more classical TD algorithms (Sutton & Barto, 1998). As classical eligibility-trace algorithms, XKTD applied to off-policy learning should fail because
it includes some effect of multi-step transitions, which are contaminated by the behavior
policy and not compensated for in any way. For a discussion about off-policy learning and
511

;

fiGeist & Pietquin

memory effects, see for example the work of Precup, Sutton, and Singh (2000). The link
of this memory effect to Monte Carlo (and to eligibility traces when the eligibility factor
is set to 1) is shown in the convergence analysis of Section 5. Here it is analyzed through
XKTD equations by showing that parameters are updated according to all past temporal
differences errors, and not only the current one.
To show this, a first step is to expand the prediction equation:
xi|i1 = F xi1|i1

 

i|i1
i1|i1
 i|i1  =  0 
i1|i1
ni|i1

(113)

Let gti be defined as:
gti = E[gti (i )|r1:i1 ]

(114)

In the KTD framework, gti is actually the predicted reward. However, it is not the case in
the XKTD framework, because the estimated noise has also to be taken into account. The
predicted reward can be expanded using Eq. (113):
ri|i1 = E[gti (i ) + ni |r1:i1 ]
= gti + ni|i1
= gti + i1|i1
A blockwise notation is adopted for the Kalman gain:


Ki
Ki = Ki 
Kni

(115)

(116)

This being stated, the correction equation can be expanded:
xi|i = xi|i1 + Ki ri

 
 
Ki
i|i
i1|i1

 i|i  =  0  + Ki  ri  gti  i1|i1
Kni
i1|i1
ni|i


From the last equation a general update of the parameters can be derived:

i|i = i1|i1 + Ki ri  gti  Kwi1 ri1

(117)

(118)

The parameters are thus updated according to the temporal difference error at time i,
i = ri  gti , and to the innovation at time i  1, ri1 , which is itself (by recurrence) a combination of TD error at time i  1 and of innovation at time i  2, etc. This update equation
highlights the memory effect of XKTD which prevents its use in an off-policy learning scenario. Notably, this prevents the derivation of a XKTD-Q algorithm. A solution to combine
off-policy learning and the colored noise could be to use some importance sampling scheme,
a well known approach of the Monte Carlo literature which allows estimating quantities
linked to a distribution using samples drawn from another distribution.
512

fiKalman Temporal Differences

5. Convergence Analysis
This section provides a convergence analysis for both KTD (deterministic MDPs) and
XKTD (stochastic MDPs).
5.1 Deterministic Case
First a convergence analysis of the KTD algorithm is provided for deterministic MDP. It
leads to a result similar to the one of residual algorithms (Baird, 1995), that is the minimization of the squared Bellman residual. This theorem makes some strong assumptions
(actually the same as the GPTD framework, however without the linear hypothesis). However, it is important to remark that even if these hypotheses are not satisfied, the cost
function (47) is still minimized. The aim of this result is to link KTD to more classic RL
algorithms.
Theorem 2. Under the assumptions that posterior and noise distributions are Gaussian
and that the prior is Gaussian too (of mean 0 and variance P0 ), than the Kalman Temporal Differences algorithm (white observation noise assumption) minimizes the following
regularized empirical cost function:
Ci () =

i
X
2
1
rj  gtj () + (  0 )T P01 (  0 )
Pnj

(119)

j=0

Proof. First notice that KTD is indeed a specific form of Sigma-Point Kalman Filter
(SPKF). According to van der Merwe (2004, ch. 4.5), under the given assumptions, the
SPKF estimator (and thus the KTD one) is the maximum a posteriori (MAP) estimator:
i|i = iMAP = argmax p(|r1:i )

(120)



By applying the Bayes rule, the posterior distribution p(|r1:i ) can be written as the (normalized) product of the likelihood p(r1:i |) and of the prior distribution p():
p(|r1:i ) =

p(r1:i |)p()
p(r1:i )

(121)

The normalization factor p(r1:i ) does not depend on parameters, MAP thus reduces to
likelihood times prior:
i|i = argmax p(r1:i |)p()
(122)


Recall that, for KTD, the observation noise is assumed white. Therefore, the joint
likelihood is the product of local likelihoods:
i|i = argmax p(r1:i |)p() = argmax




i
Y

p(rj |)p()

Moreover, noise and prior are supposed to be Gaussian, thus:

rj |  N gtj (), Pnj and   N (0 , P0 )
513

(123)

j=1

(124)

fiGeist & Pietquin

On the other hand, maximizing a product of densities is equivalent to minimizing the sum
of the negatives of their logarithms:


i
X
i|i =  argmin 
ln(p(rj |)) + ln(p())
(125)


j=1

Under the Gaussian assumption, distributions are as follows:
!
1 (rj  gtj ())2
1
exp 
p(rj |) = p
2
Pnj
2Pnj


1
1
T
1
 (  0 ) P0 (  0 )
and p() =
p
1 exp
2
(2) 2 |P0 | 2

(126)
(127)

Consequently:

i|i



i
X
2
1
= argmin 
rj  gtj () + (  0 )T P01 (  0 )
P
nj


(128)

j=1

This proves the result.
Some remarks of importance have to be made. First, the memoryless channel assumption
does not hold for stochastic MDPs. Moreover, the form of the minimized cost function (119)
strengthens the parallel drawn in Section 4.1 between KTD and squared Bellman residual
minimization. Second, the chosen observation noise variance Pni allows weighting samples.
The evolution noise variance does not appear directly in the minimized cost function, nevertheless it empirically influences convergence and tracking abilities of the algorithm. For
example, it helps handling non-stationarity and avoiding local minima. The prior P0 acts
as a regularization terms, this can be of help to choose it. Notice that such a regularization
term also appears in the recursive form of the LSTD algorithm (eg., see Kolter & Ng, 2009).
Finally, it can be shown (again, see van der Merwe, 2004, ch. 4.5) that an SPKF (and thus
KTD) update is indeed an online form of a modified Gauss-Newton method, which is actually a variant of natural gradient descent. In this case, the Fisher information matrix
1
is Pi|i
, the inverse of the variance matrix of random parameters. The natural gradient
approach has been shown to be quite efficient for direct policy search (Kakade, 2001) and
actor-critics (Peters, Vijayakumar, & Schaal, 2005), so it lets envision good empirical results for KTD. This is experimented in Section 7. KTD is perhaps the first reinforcement
learning value (and Q-) function approximation algorithm (in a pure critic sense) involving
natural gradient.
5.2 Stochastic Case
Here a convergence analysis is provided for XKTD in stochastic MDPs. Again, this theorem
makes some strong assumptions, without harming the minimization of the cost function (47)
when they are not satisfied.
514

fiKalman Temporal Differences

Theorem 3. Assume that posterior and noise distribution are Gaussian, as well as prior
distribution (of mean 0 and variance P0 ). Then XKTD estimator minimizes the (weighted
and regularized) square error linking state values to Monte Carlo returns:

Ci () =

i
X


1

2
j=1 j1

V (sj ) 

i
X

2
 tj rt  + (  0 )T P01 (  0 )

(129)

t=j

Proof. Here again the result of van der Merwe (2004, ch. 4.5) is used. The corresponding
proof is made for a random walk evolution model (that is the identity evolution matrix),
however it can be easily extended to a linear evolution model. It can thus be applied to
state-space model (108):
xi|i = xMAP
= argmax p(x|r1:i )
(130)
i
x

State-space model (108) being equivalent to state-space model (46) with the MA noise (105),
the same holds for the (non-extended) parameter vector:
i|i = argmax p(|r1:i ) = argmin( ln(p(|r1:i )))


(131)



By applying the Bayes rule, the posterior distribution p(|r1:i ) is the (normalized) product
of likelihood p(r1:i |) and prior p():
p(|r1:i ) =

p(r1:i |)p()
p(r1:i )

(132)

The normalization factor p(r1:i ) does not depend on parameters, MAP therefore reduces to
likelihood times prior:
i|i = argmax p(r1:i |)p()
(133)


However, as the observation noise is no longer white, it is not possible to express the joint
likelihood as the product of local likelihoods. Nevertheless, the joint likelihood is still
computable. For this, a few notations are introduced. Let Vi (), Ri and Ni be the following
i  1 vectors:
Vi () = V (s1 ) V (s2 ) . . .
T
Ri = r1 r2 . . . ri
T
Ni = n1 n2 . . . ni

T
V (si )

(134)
(135)
(136)

Let Hi be the i  i bidiagonal matrix defined as:



1  0 . . .
0 1  0 


Hi =  . .

.
.
.
.
.
.
.  
0 ...
0
1
515

(137)

fiGeist & Pietquin

It is easy to check that its inverse is given by:

1 
0 1

H1
i =  ..
. ...
0 ...

...

..
.
0


 i1
... 


 
1

(138)

Eventually, let Ni = E[Ni NiT ] be the variance matrix of noise Ni , which takes into account
the coloration. Given the definition of noise ni (105), its a tridiagonal matrix given by:
 2

0 +  2 12
12
0
...


..
22
2
 12


+


.
1
2
2

Ni = 
(139)


..
..
..
2


.
.
.
i1
2
2
0
...
i1
i1
+  2 i2
As the noise is Gaussian, the likelihood is Gaussian too, and colored because of the observation noise. Its distribution is:
r1:i |  N (Ri  Hi Vi (), Ni )

(140)

Maximizing MAP is equivalent to minimizing the negative of its logarithm, so given the
distribution (140) the XKTD estimator satisfies:


T
1
(


))
(141)
(R

H
V
()
+
(


)
P
i|i = argmin (Ri  Hi Vi ())T 1
0
i
i i
0
0
Ni


The noise variance can be rewritten according to Hi and to a diagonal matrix containing
the residual variances:
2
)
Ni = Hi i HTi with i = diag(02 , . . . , i1

(142)

Using this last equation, the XKTD estimator can be rewritten as:


T
1
i|i = argmin (Ri  Hi Vi ())T 1
Ni (Ri  Hi Vi ()) + (  0 ) P0 (  0 )



= argmin (Ri  Hi Vi ())T (Hi i HTi )1 (Ri  Hi Vi ()) + (  0 )T P01 (  0 )



T
1
1
T 1
= argmin (H1
R

V
())

(H
R

V
())
+
(


)
P
(


)
(143)
i
i
i
i
0
0
0
i
i
i


Given the inverse (138) of the Hi matrix, this last equation proves the result.
This result shows that under some (strong) assumptions, XKTD minimizes the square
error linking state values to Monte Carlo returns, which strengthens the discussion about the
inability of XKTD to be used in an off-policy learning scenario of Section 4.3.2. As for KTD,
residuals variance weights the samples, and the prior acts as a regularization term, which
can help to choose it. An important fact is that this result shows that actually, under the
assumption that residuals variance is constant (that is j2 =  2 ), XKTD minimizes the same
516

fiKalman Temporal Differences

cost-function as (the recursive version of) LSTD(1), the eligibility traces-based extension of
LSTD with and eligibility factor of 1 (see Boyan, 1999 for a proof that LSTD(1) minimizes
cost-function (129)). As a consequence, XKTD is asymptotically an unbiased value function
estimator, as LSTD(1)10 .

6. An Active Learning Scheme
The parameters being modeled as random variables, and the value (or Q-) function being
a function of these parameters, it is a random variable for a given state (or state-action
pair). It is first shown how to compute its expectation and the associated uncertainty
thanks to the unscented transform. The dilemma between exploration and exploitation
should benefit from such uncertainty information. Few approaches in the literature allows
handling the value function approximation problem as well as computing uncertainty over
values meantime. The work of Engel (2005) is such an approach, however the effective use
of the obtained uncertainty information is left for future work. Here is a proposed form of
active learning which is a sort of totally explorative policy in the context of KTD-Q. This
contribution is shown to effectively speed up learning in Section 7.
6.1 Computing Uncertainty over Values
Let V be the approximated value function parameterized by the random vector  of mean
 and variance matrix P . Let V (s) and V2 (s) be the associated mean and variance for
a given state s. In order to propagate the uncertainty from the parameters to the value
function, a first step is to compute the sigma-points associated to the parameter vector
 = {(j) , 0  j  2p} as well as corresponding weights W = {wj , 0  j  2p} from  and
P , as described in Section 3.2. Then the images of these sigma-points are computed for
the given state s using the parameterized value function :
n
o
(j)
V (s) = V (s) = V(j) (s), 0  j  2p
(144)
Knowing these images and corresponding weights, it is possible to compute the statistics of
interest, namely mean and variance of the approximated value function:
V (s) =

2p
X

(j)

wj V (s)

and

V2 (s) =

j=0

2p
X


2
(j)
wj V (s)  V (s)

(145)

j=0

Thus, for a given representation of the value function and a random parameter vector, the
uncertainty can be propagated to the value function. Figure 1 illustrates the uncertainty
computation. Extension to Q-function is straightforward. The complexity (both computational and in memory) is here again quadratic. So, as at each time-step i an estimate i|i
and the associated variance Pi|i are known, uncertainty information can be computed in
the KTD framework.
An important remark has to be made here. The estimated variance provides some
information about the uncertainty about estimates, however it does not take into account
10. Notice that if LSTD(1) and KTD minimize the same cost function, they do it in a different way, thus
they provide the same estimates only asymptotically.

517

fiGeist & Pietquin

Figure 1: Uncertainty computation.
the stochasticity of the MDP. It will get lower as the number of samples increases. Roughly
speaking, it can be seen as an indirect and generalized counting of the number of visits of
a given state or state-action pair. Even in a stochastic MDP, it will vanish to zero as the
number of samples grows to infinity: it is an estimate of the uncertainty over the estimated
value function, not the variance of the stochastic process from which the value function is
the expectation.
6.2 A Form of Active Learning
A simple active learning scheme using this uncertainty information is provided here. KTDQ (determinism of transitions is assumed here) is an off-policy algorithm: it learns the
optimal policy   while following a different behaviorial policy b. A natural question is to
know what behaviorial policy to choose in order to speed up learning. A piece of response
is given here.
Let i be the current temporal index. The system is in a state si , and the agent has
to choose an action ai . The considered algorithm being KTD-Q, the estimates i1|i1
and Pi1|i1 are available. They can be used to approximate the uncertainty of the Q2
(si , a) be
function parameterized by i1 in the state si and for any action a. Let Q
i1
the corresponding variance. The action ai is chosen according to the following random
behaviorial policy:
Qi1 (si , ai )
b(ai |si ) = P
(146)
aA Qi1 (si , a)
A totally explorative policy is obtained, in the sense that it favorises less certain actions.
This is a way among others to use the available uncertainty information, nevertheless it is
shown in Section 7 to be quite efficient compared to a uniformly random behaviorial policy.
However, how to use wisely this variance information in the more general dilemma between
exploration and exploitation is still an open perspective.

7. Experiments
This section provides a set of classical RL benchmarks aiming at comparing KTD and
variants to state-of-the-art algorithms and at highlighting its different aspects. Atomic
benchmarks have been chosen in order to highlight separately unitary properties of KTD
(see Table 1), which should have been quite complex on a more difficult task. Compared
algorithms are TD, SARSA and Q-learning with function approximation as well as (recursive
518

fiKalman Temporal Differences

(non)stationarity
Tsitsiklis chain
Boyan chain
maze
inverted pendulum

(non)linearity
X

uncertainty

X
X
X

X

sample efficiency

stochasticity

X

X

X

Table 1: Experiments and highlighted properties.
form of) LSTD and (MC-) GPTD. For the sake of reproducibility, all parameter values are
provided for each experiment. Their extensions to eligibility traces are not considered
here, as LSTD performs better than TD() and varying  has small effect on LSTD()
performances, according to Boyan (1999).
7.1 Choosing KTD Parameters
In order to use the (X)KTD framework, parameters have to be chosen: the variance of the
observation noise (or the variance of residuals for XKTD), the priors and the variance of
the process noise. As they are less common and perhaps less intuitive than the choice of a
learning rate for example, they are discussed here. The evolution noise for KTD and the
residual for XKTD translate the confidence the practitioner has in the ability of the chosen
parameterization to represent the true value function. If it is known in advance that the
value function lies in the hypothesis space (which is the case for example in the tabular
case), the corresponding variance can be chosen very small (but never zero for numerical
stability reasons). Another way to choose these variances is to interpret them through their
weighting of samples, see Eq. (119) and (129). The prior 0 should be initialized to a value
close to the one the user thinks to be optimal, or to a default value, for example the zero
vector. The prior P0 quantifies the certainty the user has in the prior 0 , the lower the less
certain. Another way to interpret these priors is to consider them as regularization terms, as
shown in Eq. (119) and (129). How to choose the process noise variance is an open question.
If some knowledge about non-stationarity is available, it can be used to choose this matrix.
However, such a knowledge is generally difficult to obtain beforehand. In this article, a
process noise of the form Pvi = Pi1|i1 is used, with   1 a small positive constant.
Such an artificial process noise emphasizes recent observed data, the window of emphasized
observations being quantified by . Other artificial process noise can be chosen, see the
work of van der Merwe (2004, ch. 3.5.2) for a quick survey. In the following, parameters
are chosen by trial and error (for all algorithms). Theyre perhaps not the best ones, but
orders of magnitude are correct.
7.2 Tsitsiklis Chain
This first experiment aims at illustrating the ability of KTD to handle nonlinear parameterizations and its convergence property. It consists in a 3 states valued Markov chain first
proposed by Tsitsiklis and Roy (1997). State i transits to state i with probability 0.5 and to
state i  1 with probability 0.5 too (state 1 transiting to state 1 or 3 with equi-probability).
The reward is always zero, therefore the optimal value function is zero. This chain is very
simple, however a nonlinear parameterization which causes TD with function approximation divergence is considered. Let  = 0.05, let I be the 3  3 identity matrix and M the
519

fiGeist & Pietquin

3  3 matrix defined as:


1

M =  32
1
2

1
2

1
3
2

3
2
1
2


(147)

1

The value function is parameterized by a single scalar , its parameterization is given as
(notice that here V is a 3  1 vector):
T
V = exp ((M + I) ) V0 with V0 = 10 7 3

(148)

This parameterization has been proposed by Tsitsiklis and Roy (1997) to illustrate the
possible divergence of TD in the case of nonlinear parameterization. The optimal parameter
is obviously  = .

Figure 2: Tsitsiklis chain.
KTD is compared to TD with function approximation. LSTD and GPTD are not
considered here, as they are unable to handle a nonlinear parameterization. For TD, the
learning rate is chosen equal to i = 2.103 and the initial parameter is set to 0 = 0.
For KTD, priors are set to 0 = 0 and P0 = 10. The observation noise variance is set to
Pni = 103 . The process noise described in Section 7.2 is used with  = 101 . Results
are depicted in Figure 2 which shows the parameter estimates in function of the number
of observed transitions. TD estimates diverge, as expected. KTD handles the nonlinear
parameterization and converges toward the good value (despite stochasticity of transitions).
7.3 Boyan Chain
In this section KTD and XKTD are compared to two other second order value function approximation algorithms, namely (recursive) LSTD and (parametric) MC-GPTD on a simple
valued Markov chain, the Boyan (1999) chain. The objective is threefold: showing sample efficiency, demonstrating the bias removal (of XKTD compared to KTD) and showing
non-stationarity handling.
520

fierror

Kalman Temporal Differences

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

LSTD
MC-GPTD
KTD
XKTD

0

20

40

60
80
number of episodes

100

120

140

Figure 3: Boyan chain.
The Boyan chain is a 13-state Markov chain where state s0 is an absorbing state, s1
transits to s0 with probability 1 and a reward of -2, and si transits to either si1 or si2 ,
2  i  12, each with probability 0.5 and reward -3. The feature vector (s) for states
s12 , s8 , s4 and s0 are respectively [1, 0, 0, 0]T , [0, 1, 0, 0]T , [0, 0, 1, 0]T and [0, 0, 0, 1]T . The
feature vectors for other states are obtained by linear interpolation. The approximated
value function is thus V (s) = T (s). The optimal value function is exactly linear in
these features, and the corresponding optimal parameter vector is 1 = [24, 16, 8, 0]T .
To measure the quality of each algorithm the normalized Euclidian distance between the
current parameter vector estimate and the optimal one k1 k k   k is computed. Notice
that as the parameterization is linear, it is the same as measuring the error between the
true and the estimated value functions, up to a scaling factor. The discount factor  is set
to 1 in this episodic task. For all algorithms, the prior is set to P0|0 = I where I is the
identity matrix. Choosing the same prior should be fair, as it yields to choose the same
regularization term for all algorithms. For MC-GPTD and KTD variations, the residual
variance (observation noise for KTD) is set to i2 = 103 (Pni = 103 ). For KTD variations,
the process noise covariance is set to an RLS (recursive least-squares)-like adaptive process
noise as described in Section 7.1, that is Pvi = Pi1|i1 where Pi1|i1 denotes the variance
over parameters, and   1 is a small positive constant, chosen here equal to 102 . Choosing
these parameters requires some practice, but no more than choosing a learning rate for other
algorithms. For all algorithms the initial parameter vector is set to zero. To experiment
non-stationarity handling, a change in the MDP is simulated by multiplying the rewards
by ten from the 70th episode (rewards become 20 and 30 instead of 2 and 3). The
optimal value function is still linear in the feature vectors, and the optimal parameter vector
is 2 = 101 after the MDP change. Learning is done over 140 episodes, and results are
averaged over 300 trials. Results are presented in Figure 3.
Before the MDP change, KTD variations and MC-GPTD converge faster than LSTD
(and equally well). XKTD, as well as LSTD and MC-GPTD, is unbiased, contrary to
KTD. Thus XKTD does the job it has been designed for, that is removing the bias due to
stochastic transitions. After the MDP change, both LSTD and MC-GPTD fail to track the
value function. KTD manages to do it, but it is still biased. XKTD tracks the value function
without being biased. GPTD results are not presented here for the sake of readability.
521

fiGeist & Pietquin

However, its behavior is the same as KTD one before the MDP change, and it fails to track
the value function after the rewards switch (much like MC-GPTD). This experiment shows
that XKTD performs as well as KTD, however without the bias problem, which was the
motivation for introducing this new algorithm. It is sample-efficient and it tracks the value
function rather than converging to it (non-stationarity handling). It can be argued that
some forgetting factors can be added to LSTD or GPTD. However it is more naturally
done in the KTD framework, which moreover exhibits some other interesting aspects as
illustrated in the next sections.

7.4 Simple Maze
With the KTD framework, the parameters are modelled as random variables. Being a
function of the parameters, the approximated value (or Q-) function is a random function.
It is thus possible to compute a variance associated to the value of each state as shown in
Section 6.1. It is a necessary condition to handle the exploration-exploitation dilemma in a
value (or Q-) function approximation context. In this section the uncertainty information
which can be obtained from the KTD framework is illustrated on a simple maze problem.
The 2d continuous state space is the unit square: (x, y)  [0, 1]2 . Actions are to move
left, right, up or down, the magnitude being of 0.05 in each case. The reward is +1 if the
agent leaves the maze in y = 1 and x  [ 38 , 58 ], 1 if the agent leaves the maze in y = 1 and
x  [0, 38 [] 58 , 1], and 0 elsewhere. The algorithm is KTD-V. The parameterization is a set
of 9 equispaced Gaussian kernels (centered in {0, 0.5, 1}  {0, 0.5, 1}) and with a standard
deviation of 0.5. The forgetting factor  is set to 0.9. The agent starts in a random position
(x0 , y0 ) with x0 sampled from a Gaussian distribution, x0  N ( 21 , 18 ), and y0 sampled from
a uniform distribution, y0  U[0,0.05] . The behaviorial policy for which the value function
is learnt is going up with probability 0.9, and go in one of the three other directions with
probability 0.1
3 . The initial parameter vector is set to zero, the prior to P0|0 = 10I, and the
noise covariances to Pni = 1 and Pvi = 0I.
The value function is learnt quite well, however this is not the point here. The objective
is to illustrate the value function uncertainty. The learning is done over 30 episodes, and
results are given in Figure 4, which shows the standard deviation of the approximated value
function over the state space. Considering the x-axis, the uncertainty is lower in the middle
than in the border. This is explained by the fact that learning trajectories occur more
frequently in the center of the domain. Considering the y-axis, the uncertainty is lower
near the upper bound (y = 1) than near the lower bound (y = 0). This is explained by
the fact that retro-propagated values are less certain. Thus the uncertainty information
computed by KTD-V is meaningful on this simple example, and it should be useful to
speed up learning, eg., for exploration/exploitation dilemma. Another application example
is given in the Section 6.2 and is experimented in Section 7.5. GPTD also provides a
meaningful uncertainty information (Engel, Mannor, & Meir, 2003). However, as far as we
know, it has never been used practically. Most likely, such uncertainty information cannot
be derived from LSTD (the main reason for this belief is that the matrix maintained by
LSTD is not symmetric, therefore it cannot be interpreted as a variance matrix).
522

fiKalman Temporal Differences

1

0.9

0.9

0.8

0.8

0.7

y position

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3
0.2

0.2

0.1

0.1

0

0
0

0.1

0.2

0.3

0.4 0.5 0.6
x position

0.7

0.8

0.9

1

Figure 4: Simple maze, uncertainty illustration.
7.5 Inverted Pendulum
The last experiment is the inverted pendulum as described by Lagoudakis and Parr (2003).
The goal is here to compare two value-iteration-like algorithms, namely KTD-Q and Qlearning, which aim at learning directly the optimal policy. LSTD and GPTD cannot be
considered here: as they are unable to handle nonlinearities (the nonlinearity being the max
operator here), they cannot be used with the Bellman optimality operator. The proposed
active learning-like scheme is also experimented: it uses the uncertainty computed by KTD
to speed up convergence.
This task requires balancing a pendulum of unknown length and mass at the upright
position by applying forces to the cart it is attached to. Three actions are allowed: left
force (-1), right force (+1), or no force (0). The associated state space consists in vertical
angle  and angular velocity  of the pendulum. Deterministic transitions are computed
according to physical dynamics of the system, and depends on the current action a:
 =

g sin()  ml2 sin(2)/2  50 cos()a
4l/3  ml cos2 ()

(149)

where g is the gravity constant, m and l the mass and the length of the pendulum, M the
1
mass of the cart, and  = m+M
. A zero reward is given as long as the angular position is
 
in [ 2 , 2 ]. Otherwise, the episode ends and a reward of 1 is given. The parameterization
is composed of a constant term and a set of 9 equispaced Gaussian kernels (centered in
{ 4 , 0, 4 }  {1, 0, 1} and with a standard deviation of 1) for each action. Thus there is
a set of 30 basis functions. The discount factor  is set to 0.95.
7.5.1 Learning the Optimal Policy
First, algorithms ability to learn an optimal policy is compared. For Q-learning, the learning
rate is set to i = 0 nn00+1
+i with 0 = 0.5 and n0 = 200, according to Lagoudakis and Parr
523

fiGeist & Pietquin

(2003). For KTD-Q, the parameters are set to P0|0 = 10I, Pni = 1 and Pvi = 0I. For
all algorithms the initial parameter vector is set to zero. Training samples are collected
online with random episodes. The agent starts in a randomly perturbed state close to the
equilibrium (0, 0) and then follows a policy that selects actions uniformly at random. The
average length of such episodes was about 10 steps, and both algorithms learnt from the
same trajectories. Results are summarized in Figure 5.
10000

KTD-Q
Q-learning

steps

1000

100

10
0

100

200

300

400 500 600
number of episodes

700

800

900

1000

Figure 5: Inverted pendulum, optimal policy learning.
For each trial, learning is done over 1000 episodes. Every 50 episodes, learning is freezed
and the current policy is evaluated. For this, the agent is randomly initialized in a state
close to the equilibrium and the greedy policy is followed until the end of episode; this
is repeated 100 times and averaged. Performance is measured as the number of steps in
an episode. Maximum number of steps for one episode is bounded by 3000 steps, which
corresponds to 5 minutes of balancing the pole without failure. Results in Figure 5 are
averaged over 100 trials and presented in a semi-log scale.
KTD-Q learns an optimal policy (that is balancing the pole for the maximum number of
steps) asymptotically and near-optimal policies are learnt after only a few tens of episodes.
The results of KTD-Q are comparable to the ones of the LSPI algorithm (see Lagoudakis
& Parr, 2003, Fig. 16). With the same number of learning episodes, Q-learning with the
same linear parameterization fails to learn a policy which balances the pole for more than a
few tens of time steps. Similar results for Q-learning are obtained by Lagoudakis and Parr
(2003).
7.5.2 A Form of Active Learning
The parameters being random variables, as explained in Section 6 and illustrated in Section 7.4, the parameterized Q-function is a random function, and the KTD framework allows
computing a variance associated to the value of each state. Here is proposed an experiment
which aims at using this uncertainty information to speed up the learning. The learning
is still done from random trajectories. However, the form of active learning described in
Section 6 is considered now. The environment is initialized randomly as before. When the
system is in a given state, the standard deviation of the Q-function is computed for each
524

fiKalman Temporal Differences

action. These deviations are normalized, and the new action is sampled randomly according to the probabilities weighted by the deviations. Thus, an uncertain action will be more
likely sampled. The average length of such episodes was about 11 steps, which does not
differ much from uniformly random transitions. Consequently this can only slightly help to
improve speed of convergence (at most 10%, much less than the real improvement which is
about 100%). Results are summarized in Figure 6.
3000
2500
KTD-Q
Q-learning
active KTD-Q

steps

2000
1500
1000
500
0
0

50

100
150
200
number of episodes

250

300

Figure 6: Inverted pendulum, random and active learning.
For each trial, learning is done over 300 episodes. Less episodes are considered to show
the speed up of convergence, however both versions of KTD perform as well asymptotically.
Every 25 episodes, learning is freezed and the current policy is evaluated as before. Performance is measured as the number of steps of an episode, again for a maximum of 3000
steps. Results in Figure 6 are averaged over 100 trials. Notice that the scale is no longer
logarithmic. It compares KTD-Q with informed transitions (active KTD-Q) to KTD-Q
with uniformly random learning policy and Q-learning. When comparing the two versions
of KTD-Q, it is clear that sampling actions according to uncertainty speeds up convergence.
It is almost doubled in the first 100 episodes: for example, a performance of 1500 is obtained
after only 25 episodes with active-KTD, whereas it needs about 50 episodes for the basic
KTD. Thus the uncertainty information available thanks to the KTD framework can be
quite useful for reinforcement learning.

8. Discussion and Perspectives
In this section the proposed framework is discussed and linked to some related approaches.
Some perspectives are also given.
8.1 Discussion
Approaches related to the KTD framework have been proposed previously. Engel (2005)
proposes a Gaussian process approach to value function approximation. As explained before,
its principle is to model the value function as a Gaussian process and to adopt a generative
model linked to the Bellman evaluation equation. Links between Engels approach and the
525

fiGeist & Pietquin

proposed one have been discussed throughout the paper. Particularly, with a linear parameterization and a zero process noise KTD-V reduces to GPTD and XKTD-V to MC-GPTD.
However, KTD framework handle non-stationarities (even if we recognize that GPTD could
probably be extended to handle them too) and more importantly it handles non-linearities
in a derivative-free manner, which allows considering nonlinear parameterizations and the
Bellman optimality operator. Engels framework allows constructing automatically and
online a kernel-based linear parameterization, which is an advantage compared to the proposed framework. However, it can be easily incorporated in it (see Geist et al., 2008 where
it is used in a preprocessing step, using it online is not more difficult). As Kalman filtering is strongly linked to least-squares minimization (in the linear case, the former is a
generalization of the later), the proposed approach shares similarities with LSTD (Bradtke
& Barto, 1996). However, it does not take into account the instrumental variables concept (Soderstrom & Stoica, 2002), which is used to handle stochastic transitions (in the
KTD framework, it is done thanks to the colored noise model). Moreover, it has been
shown in Section 5.2 that XKTD-V (with linear parameterization and no evolution noise)
converges to the same solution as LSTD(1). Choi and Van Roy (2006) introduced a Kalman
filter designed to handle fixed-point approximation in the case of linear parameterization.
It can be roughly seen as a bootstrapping version of the proposed KTD-V. Instead of the
observation equation of state-space model (65), the following observation equation is used:
ri + (si+1 )T i1|i1 = (si )T i + ni . In other words, the reward is not considered as the
observation, but an approximation of the value function is used to compute a pseudoobservation ri + (si+1 )T i1|i1 . The update of the parameters  is made so as to match
the value function of the current state to this pseudo-observation (bootstrapping approach).
Alternatively, it can be seen as a linear least-squares variation of the classic TD with function approximation algorithm (which combines bootstrapping and gradient descent). Phua
and Fitch (2007) use a bank of classical Kalman filters to learn the parameters of a piecewise linear parameterization of the value function. It can be roughly seen as a special case
of the proposed approach, however differences exist: not one filter but a bank is used and
the parameterization is piecewise linear, which is exploited to develop specificities of the
algorithm (notably concerning the parameters update) while the proposed approach does
not make any assumption about the value function.
The proposed framework presents some interesting aspects. First, it does not suppose
stationarity. An immediate application is to take into account non-stationary MDP (Geist,
Pietquin, & Fricout, 2009b), as exemplified in Section 7.3. An even more interesting application is the control case. For instance, LSTD algorithm is known not to well behave when
combined with an optimistic policy iteration scheme (-greedy policy for example, see Phua
& Fitch, 2007), because of the non-stationarities induced by the fact that control and learning are interlaced. Similarly, Bhatnagar, Sutton, Ghavamzadeh, and Lee (2008) prefer TD
to LSTD as the actor of the incremental natural actor-critic approach they propose, despite
the fact that it is less sample efficient. Kalman filtering and thus proposed approaches are
robust to non-stationarity (to a certain extent). Quite few approaches aiming at approximating the value function take this non-stationary problem into account, the algorithm
of Phua and Fitch (2007) being one of them. Another related approach (designed to cope
with interlacing of control and learning in an actor-critic context) is the two-timescale
526

fiKalman Temporal Differences

stochastic approximation (for example, see Konda & Tsitsiklis, 2003 or Bhatnagar et al.,
2008).
Second, as KTD models parameters as a random vector, it is possible to compute uncertainty information about values, as explained in Section 6.1 and illustrated in Section 7.4.
It has been used to derive a form of active learning (Sections 6.2 and 7.5), however this uncertainty information could be useful to deal with the more general problem of the dilemma
between exploration and exploitation, following idea of what is done by Dearden et al. (1998)
or by Strehl et al. (2006). The point is that, as far as we know, rather few approaches allows
dealing with value function approximation and value uncertainty in the same time. One of
these approaches is the GPTD framework of Engel (2005), however the effective use of the
available uncertainty information is left for future work in the original publications and has
not been developed so far. It should also be noticed that without a probabilistic or statistical approach of the value function approximation problem such uncertainty information
would be more difficult to obtain.
Third, KTD also allows handling nonlinearities. It has been explicitly used for KTD-Q
(the max operator being a severe nonlinearity), which is illustrated in Section 7.5. Nonlinear
parameterization can be considered too, as illustrated in Section 7.2. A nonlinear parameterization has also been used by Geist et al. (2008) combined with a preliminary version of
KTD-Q. Moreover, nonlinear parameterization should allow more compact representation
of the value function approximator, which could somehow alleviate the square complexity
of the proposed framework.
KTD shares a drawback with other square Bellman residual minimization-based algorithms (which it is indeed according to Theorem 2): the value estimates are biased
if transitions of the dynamic system are not deterministic, as illustrated in Section 7.3.
Different algorithms propose various methods to cope with this problem. For residual algorithms (Baird, 1995), which consist in minimizing the square Bellman residual using a
gradient descent, it is proposed to use double sampling in order to obtain an unbiased
estimator. This approach has two major drawbacks: it needs a generative model, and it
is sample inefficient. For the LSTD algorithm (Bradtke & Barto, 1996), which consists
in minimizing the Bellman residual with a least-squares approach, an instrumental variable (Soderstrom & Stoica, 2002) is used to enforce unbiasedness of the estimator. Such an
approach is not easy to extend to nonlinearity or non-stationarity (and thus online control).
Another and generic approach to remove this sort of bias has been proposed by Antos et al.
(2008). It consists in introducing an auxiliary function (in add to the value function) which
role is to remove the bias. The resulting optimization problem is no longer quadratic, it consists in two interlocked square problems. When used with a linear function approximator, it
reduces to the LSTD algorithm, and it has been used with a neural network-based function
approximator by Schneega, Udluft, and Martinetz (2007). The GPTD framework (Engel,
2005) uses a colored noise model which has been adapted to extend the KTD framework.
8.2 Conclusion and Perspectives
A Kalman-filter-based Temporal Differences framework has been introduced to cope with
a number of problems at the same time: online learning, sample efficiency, non-stationarity
and non-linearity handling as well as providing uncertainty information. Being actually a
527

fiGeist & Pietquin

square-Bellman-minimization-based approach, the original framework cannot handle stochastic transitions. It has thus been extended using a colored observation noise model. A convergence analysis has been provided for both deterministic and stochastic cases. Finally,
various aspects of the proposed approach have been experimentally demonstrated on classical reinforcement learning benchmarks. Section 7.2 shows the ability to converge with
nonlinear parameterizations, Section 7.3 shows that the colored noise induces a unbiased
version of KTD and its ability to handle non-stationarities, Section 7.4 illustrates available
uncertainty information and Section 7.5 shows the value-iteration-like KTD-Q algorithm
as well as the learning speed-up obtained thanks to the proposed active learning scheme.
State-of-the-art algorithms were also considered, and KTD compares favorably to them.
The KTD framework presents some interesting perspectives. First, XKTD was shown
to effectively remove the bias. As noticed by Engel (2005, ch. 4.5), other noise models can
be envisioned (by analogy to LSTD() for example), however what noise models to choose
and how to incorporate them to the KTD framework are still open questions. More theoretical insights on the bias caused by the use of KTD on stochastic problems can also be
useful. Also, an interesting perspective to address the off-policy problem when considering
a colored noise is to combine XKTD with importance sampling. Another interesting perspective is to adapt the eligibility traces principle to the proposed framework in order to fill
the gap between KTD (local update) and XKTD (global update by its relation to Monte
Carlo) (Geist & Pietquin, 2010a).
Second, this KTD framework should be naturally extended to the partially observable
case. Indeed, inferring the state of a system given past observations is a problem which can
benefit from Bayesian filtering of which formalism is close to the one proposed. It is well
known that a partially observable MDP (POMDP) can be expressed as an MDP of which
states are distributions over states of the POMDP. If these distributions can be estimated
(by using a filtering approach for example), they should be naturally taken into account by
KTD: parameterization is already a function of the distribution over parameters, it can be
extended to be a function of the distribution over states in the same manner.
KTD framework handles well nonlinearities. An interesting perspective could be to use
it with a neural network based representation for the value (or Q-) function, which let hope
a more compact representation. This way, it can probably be easier to address real world
problems, for which scaling up is mandatory.
Another difficulty can be the choice of the different parameters, which are problemdependent. First it should be noticed that choosing this type of parameters is not more
difficult than choosing learning rates for example, it is just less usual in the RL community.
Concerning a more automatic choice of parameters, the adaptive filtering literature can
help (Goodwin & Sin, 2009). A form of adaptive evolution noise has been used in the
experimental part of this paper, however many other solutions can be envisioned.
As said before, KTD could be an interesting alternative to TD as the actor part of the
incremental natural actor-critic algorithms of Bhatnagar et al. (2008). Some preliminary
works on using KTD in an actor-critic architecture are provided by Geist and Pietquin
(2010c). Talking about natural gradient, a parallel has been drawn between the KTD
framework and natural gradient descent in Section 5.1, and this could benefit from more
theoretical insights.
528

fiKalman Temporal Differences

The value uncertainty available from this framework has been used for a form of active
learning scheme, and it is planned to be used to address the more general problem of
the dilemma between exploration and exploitation, either by adapting existing approaches
designed for the tabular case (Geist & Pietquin, 2010b) or by developing new methods.
Unscented Kalman filtering, on which this work is based, can be linked to nonlinear leastsquares problems solved using a statistical linearization approach (Geist & Pietquin, 2010e).
Underlying ideas can be used to extend the LSTD algorithm to nonlinear parameterizations
as well as to the Bellman optimality operator (Geist & Pietquin, 2010d).
Finally, it is planned to do more comparison with the state-of-the-art, both theoretically
and experimentally. Ultimately application of these ideas to a real world problem is needed
to asses their utility. Concerning this last point, we plan to apply the proposed framework
to a dialogue management problem.

Acknowledgments
The authors wish to thank the European Community (FP7/2007-2013, grant agreement
216594, CLASSiC project : www.classic-project.org) and the Region Lorraine for financial
support. Matthieu Geist also wish to thank ArcelorMittal Research for financial support
during his 2006-2009 PhD thesis.

References
Antos, A., Szepesvari, C., & Munos, R. (2008). Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. Machine
Learning, 71 (1), 89129.
Baird, L. C. (1995). Residual Algorithms: Reinforcement Learning with Function Approximation. In Proceedings of the International Conference on Machine Learning (ICML
95), pp. 3037.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., & Lee, M. (2008). Incremental Natural
Actor-Critic Algorithms. In Proceedings of the Twenty-First Annual Conference on
Advances in Neural Information Processing Systems (NIPS), Vancouver, Canada.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press,
New York, USA.
Boyan, J. A. (1999). Technical Update: Least-Squares Temporal Difference Learning. Machine Learning, 49 (2-3), 233246.
Bradtke, S. J., & Barto, A. G. (1996). Linear Least-Squares Algorithms for Temporal
Difference Learning. Machine Learning, 22 (1-3), 3357.
Chen, Z. (2003). Bayesian Filtering : From Kalman Filters to Particle Filters, and Beyond.
Tech. rep., Adaptive Systems Lab, McMaster University.
529

fiGeist & Pietquin

Choi, D., & Van Roy, B. (2006). A Generalized Kalman Filter for Fixed Point Approximation and Efficient Temporal-Difference Learning. Discrete Event Dynamic Systems,
16, 207239.
Dearden, R., Friedman, N., & Russell, S. J. (1998). Bayesian q-learning. In AAAI/IAAI,
pp. 761768.
Engel, Y. (2005). Algorithms and Representations for Reinforcement Learning. Ph.D. thesis,
Hebrew University.
Engel, Y., Mannor, S., & Meir, R. (2003). Bayes Meets Bellman: The Gaussian Process
Approach to Temporal Difference Learning. In Proceedings of the International Conference on Machine Learning (ICML 2003), pp. 154161.
Engel, Y., Mannor, S., & Meir, R. (2005). Reinforcement Learning with Gaussian Processes.
In Proceedings of International Conference on Machine Learning (ICML-05).
Geist, M., & Pietquin, O. (2010a). Eligibility Traces through Colored Noises. In Proceedings
of the IEEE International Conference on Ultra Modern Control systems (ICUMT
2010), Moscow (Russia). IEEE.
Geist, M., & Pietquin, O. (2010b). Managing Uncertainty within Value Function Approximation in Reinforcement Learning. In Active Learning and Experimental Design
workshop (collocated with AISTATS 2010), Sardinia, Italy.
Geist, M., & Pietquin, O. (2010c). Revisiting natural actor-critics with value function
approximation. In Torra, V., Narukawa, Y., & Daumas, M. (Eds.), Proceedings of
7th International Conference on Modeling Decisions for Artificial Intelligence (MDAI
2010), Vol. 6408 of Lecture Notes in Artificial Intelligence (LNAI), pp. 207218, Perpinya (France). Springer Verlag - Heidelberg Berlin.
Geist, M., & Pietquin, O. (2010d). Statistically Linearized Least-Squares Temporal Differences. In Proceedings of the IEEE International Conference on Ultra Modern Control
systems (ICUMT 2010), Moscow (Russia). IEEE.
Geist, M., & Pietquin, O. (2010e). Statistically Linearized Recursive Least Squares. In
Proceedings of the IEEE International Workshop on Machine Learning for Signal
Processing (MLSP 2010), Kittila (Finland).
Geist, M., Pietquin, O., & Fricout, G. (2008). Bayesian Reward Filtering. In et al., S. G.
(Ed.), Proceedings of the European Workshop on Reinforcement Learning (EWRL
2008), Vol. 5323 of Lecture Notes in Artificial Intelligence, pp. 96109. Springer Verlag,
Lille (France).
Geist, M., Pietquin, O., & Fricout, G. (2009a). Kalman Temporal Differences: the deterministic case. In Proceedings of the IEEE International Symposium on Adaptive Dynamic
Programming and Reinforcement Learning (ADPRL 2009), Nashville, TN, USA.
Geist, M., Pietquin, O., & Fricout, G. (2009b). Tracking in Reinforcement Learning. In
Proceedings of the 16th International Conference on Neural Information Processing
(ICONIP 2009), Bangkok (Thailande). Springer.
Geramifard, A., Bowling, M., & Sutton, R. S. (2006). Incremental Least-Squares Temporal
Difference Learning. In Proceedings of the 21st Conference, American Association for
Artificial Intelligence, pp. 356361.
530

fiKalman Temporal Differences

Gill, P. E., Golub, G. H., Murray, W., & Saunders, M. A. (1974). Methods for Modifying
Matrix Factorization. Mathematics of Computation, 28 (126), 505535.
Goodwin, G. C., & Sin, K. S. (2009). Adaptive Filtering Prediction and Control. Dover
Publications, Inc., New York, NY, USA.
Jo, S., & Kim, S. W. (2005). Consistent Normalized Least Mean Square Filtering with
Noisy Data Matrix. IEEE Transactions on Signal Processing, 53 (6), 21122123.
Julier, S. J., & Uhlmann, J. K. (2004). Unscented filtering and nonlinear estimation. Proceedings of the IEEE, 92 (3), 401422.
Kakade, S. (2001). A natural policy gradient. In Advances in Neural Information Processing
Systems 14 [Neural Information Processing Systems (NIPS 2001), pp. 15311538,
Vancouver, British Columbia, Canada.
Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction Problems.
Transactions of the ASMEJournal of Basic Engineering, 82 (Series D), 3545.
Kolter, J. Z., & Ng, A. Y. (2009). Regularization and Feature Selection in Least-Squares
Temporal Difference Learning. In proceedings of the 26th International Conference on
Machine Learning (ICML 2009), Montreal Canada.
Konda, V. R., & Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM J. Control
Optim., 42 (4), 11431166.
Lagoudakis, M. G., & Parr, R. (2003). Least-Squares Policy Iteration. Journal of Machine
Learning Research, 4, 11071149.
Peters, J., Vijayakumar, S., & Schaal, S. (2005). Natural Actor-Critic. In et al., J. G. (Ed.),
Proceedings of the European Conference on Machine Learning (ECML 2005), Lecture
Notes in Artificial Intelligence. Springer Verlag.
Phua, C. W., & Fitch, R. (2007). Tracking Value Function Dynamics to Improve Reinforcement Learning with Piecewise Linear Function Approximation. In Proceedings of the
International Conference on Machine Learning (ICML 07).
Precup, D., Sutton, R. S., & Singh, S. P. (2000). Eligibility Traces for Off-Policy Policy
Evaluation. In Proceedings of the Seventeenth International Conference on Machine
Learning (ICML00), pp. 759766, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience.
Schneega, D., Udluft, S., & Martinetz, T. (2007). Improving optimality of neural rewards
regression for data-efficient batch near-optimal policy identification.. In de Sa, J. M.,
Alexandre, L. A., Duch, W., & Mandic, D. P. (Eds.), ICANN, Vol. 4668 of Lecture
Notes in Computer Science, pp. 109118. Springer.
Schoknecht, R. (2002). Optimality of Reinforcement Learning Algorithms with Linear Function Approximation. In Proceedings of the Conference on Neural Information Processing Systems (NIPS 15).
Sigaud, O., & Buffet, O. (Eds.). (2010). Markov Decision Processes and Artificial Intelligence. Wiley - ISTE.
531

fiGeist & Pietquin

Simon, D. (2006). Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches
(1. Auflage edition). Wiley & Sons.
Strehl, A. L., Li, L., Wiewiora, E., Langford, J., & Littman, M. L. (2006). Pac modelfree reinforcement learning. In Proceedings of the 23rd International Conference on
Machine Learning (ICML 2006), pp. 881888, Pittsburgh, PA, USA.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction (3rd edition). The MIT Press.
Sutton, R. S., Koop, A., & Silver, D. (2007). On the role of tracking in stationary environments. In ICML 07: Proceedings of the 24th international conference on Machine
learning, pp. 871878, New York, NY, USA. ACM.
Soderstrom, T., & Stoica, P. (2002). Instrumental variable methods for system identification.
Circuits, Systems, and Signal Processing, 21, 19.
Tsitsiklis, J. N., & Roy, B. V. (1997). An analysis of temporal-difference learning with
function approximation. IEEE Transactions on Automatic Control, 42, 674690.
van der Merwe, R. (2004). Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models. Ph.D. thesis, OGI School of Science & Engineering, Oregon
Health & Science University, Portland, OR, USA.
Yu, H., & Bertsekas, D. P. (2007). Q-Learning Algorithms for Optimal Stopping Based on
Least Squares. In Proceedings of European Control Conference, Kos, Greece.

532

fiJournal of Artificial Intelligence Research 39 (2010) 663-687

Submitted 6/10; published 11/10

An Effective Algorithm for and Phase Transitions of the
Directed Hamiltonian Cycle Problem
Gerold Jager

gej@informatik.uni-kiel.de

Computer Science Institute,
Christian-Albrechts-University of Kiel,
D-24118 Kiel, Germany

Weixiong Zhang

weixiong.zhang@wustl.edu

Department of Computer Science and Engineering,
Washington University,
St. Louis, Missouri 63130, United States

Abstract
The Hamiltonian cycle problem (HCP) is an important combinatorial problem with
applications in many areas. It is among the first problems used for studying intrinsic properties, including phase transitions, of combinatorial problems. While thorough theoretical
and experimental analyses have been made on the HCP in undirected graphs, a limited
amount of work has been done for the HCP in directed graphs (DHCP). The main contribution of this work is an effective algorithm for the DHCP. Our algorithm explores and
exploits the close relationship between the DHCP and the Assignment Problem (AP) and
utilizes a technique based on Boolean satisfiability (SAT). By combining effective algorithms for the AP and SAT, our algorithm significantly outperforms previous exact DHCP
algorithms, including an algorithm based on the award-winning Concorde TSP algorithm.
The second result of the current study is an experimental analysis of phase transitions of
the DHCP, verifying and refining a known phase transition of the DHCP.

1. Introduction
An undirected graph G = (V, E) is Hamiltonian if it contains a Hamiltonian cycle (HC), a
cycle that visits each vertex exactly once. Given a graph, the Hamiltonian cycle problem
(HCP) is to find a HC or to prove that no HC exists in the graph. The decision version of the
HCP is among the first problems that were proven to be N P-complete (Karp, 1972). HCP is
a well-known problem with many applications in different areas, e.g., the Hamiltonian cycle
game in game theory (Stojakovic & Szabo, 2005), the problem of finding a knights tour on
a chessboard in artificial intelligence (Henderson & Apodaca, 2008), and the DNA Physical
Mapping in biology (Grebinski & Kucherov, 1996). Much research has been done on the
HCP in undirected graphs. For reviews, see the work of Bondy (1995), Christofides (1975),
Chvatal (1985), Gould (1991), Vandegriend (1998), and Gutin and Moscato (2000). In
particular, many algorithms have been developed for the HCP (Angluin & Valiant, 1979;
Bollobas, Fenner & Frieze, 1987; Frieze, 1988a; Posa, 1976; Vandegriend, 1998), as reviewed
in the Stony Brook Algorithm Repository (Skiena, 2008). One effective algorithm for the
HCP is based on the related Traveling Salesman Problem (TSP) in an undirected weighted
graph, which is the problem of finding a HC with minimum total weight.
c
2010
AI Access Foundation. All rights reserved.

fiJager & Zhang

The HCP is also a canonical problem for understanding intrinsic properties of combinatorial problems. One such problem property is the so called phase transition. Consider an
undirected graph Gn,m with m edges randomly chosen from all possible n(n  1)/2 edges
over n vertices. It is expected that when keeping the size n, i.e., the number of vertices, a
constant while increasing the number of edges m, the probability that a random graph Gn,m
is Hamiltonian increases from 0 to 1. Surprisingly, the probability of being Hamiltonian for
Gn,m exhibits a sharp, dramatic transition from 0 to 1, and the transition occurs approximately when m = dc  n  (log n + log log n)/2c (Bollobas, 1985; Cheeseman, Kanefsky &
Taylor, 1991; Komlos & Szemeredi, 1983). Furthermore, it was experimentally shown that
when the constant c is between 1.08 and 1.10, the probability that Gn,m is Hamiltonian is
1/2 (Vandegriend & Culberson, 1998). Phase transitions in the HCP have also been studied
under other different control parameters, for example, the so called general constrainedness
parameter (Frank, Gent & Walsh, 1998). The phase transition result of the HCP has motivated a substantial amount of research on phase transitions of other combinatorial problems,
particularly the TSP (Zhang & Korf, 1996) and Boolean satisfiability (Monasson, Zecchina,
Kirkpatrick & Selman, 1999).
In this study we consider the HCP in directed graphs, which we call directed HCP,
or DHCP for short. In addition to the known applications of the HCP mentioned above,
an interesting application of the DHCP is that DHCP heuristics can be used to solve the
Bottleneck TSP (Kabadi & Punnen, 2002). In contrast to the extensive amount of work
on the HCP for undirected graphs, the research on the DHCP is rather limited (Angluin
& Valiant, 1979; Bang-Jensen & Gutin, 2008; Kelly, 2007). The first exact algorithm for
the DHCP was developed by Martello (1983). This algorithm outputs a fixed number h
of HCs or reports that it cannot find h HCs in a given directed graph. By setting h = 1,
this gives rise to an algorithm for the DHCP. In recent years, algorithms based on SAT
encoding have been introduced to this problem, e.g., the absolute encoding (Hoos, 1999)
and the relative encoding (Prestwich, 2003; see also Velev & Gao, 2009). Furthermore, a
probabilistic heuristic for DHCP of complexity O(n1.5 ) was proposed (Frieze, 1988b). It
can be shown that for the random class Gn,m the probability, that for a given instance a
HC is found by this algorithm and therefore exists, changes from 0 to 1, when n grows to
infinity and m = n log n+cn, where c is a constant. For the DHCP, a phase transition result
similar to that of the HCP has been obtained as well, namely the phase transition occurs
at m = dc  n  (log n + log log n)c (McDiarmid, 1980), where the constant c was expected to
be close to 1.
Note that the research on the TSP has also alluded to a DHCP algorithm. Using the
technique of 2-point reduction, the asymmetric TSP (ATSP)  where the distance from city i
to city j may not be necessarily equal to that from j to i  can be converted to the symmetric
TSP, with the number of vertices being doubled (Jonker & Volgenant, 1983). Using this
transformation, we can determine whether a directed graph is Hamiltonian by solving the
symmetric TSP using the renowned Concorde algorithm (Applegate, Bixby, Chavatal &
Cook, 2005, 2006). Concorde has solved many large benchmark instances (Cook, 2010),
including a TSP instance with 85, 900 cities (Applegate et al., 2009), which up to date is
the largest solved practical TSP instance.
The main contribution of this paper is an effective exact algorithm for the DHCP. In our
algorithm, we utilize methods for two well-known combinatorial problems, i.e., the Assign664

fiAlgorithm for Directed Hamiltonian Cyce Problem

ment Problem (AP) and Boolean satisfiability (SAT); we therefore denote our algorithm by
AP-SAT. Using random graphs and many real world instances, we experimentally compare
the AP-SAT algorithm with the DHCP algorithm of Martello (1983), the TSP based approach that takes advantage of the TSP solver Concorde (Applegate et al., 2005, 2006)
and the above-mentioned SAT encodings for the DHCP (Hoos, 1999; Prestwich, 2003). The
results show that the AP-SAT algorithm significantly outperforms these algorithms.
The second contribution is an experimental study and refinement of the known phase
transition result on the existence of a HC in a random directed graph (McDiarmid, 1980),
as similarly done for the HCP (Vandegriend & Culberson, 1998).

2. The Algorithm
Consider a directed unweighted graph G = (V, E) with nodes V and edges E. For our
purpose of solving the DHCP, we consider the problem of determining whether or not
there exists a collection of cycles, which may not be necessarily complete cycles, visiting
each vertex exactly once. We call this problem directed Assignment Problem or DAP for
short. Our algorithm explores and exploits the intrinsic relationship between the DHCP
and the DAP. More precisely, the AP-SAT algorithm searches for a HC in the space of DAP
solutions. It first solves the DAP. If the DAP solution forms a HC, or no DAP solution
exists, the algorithm terminates. If the DAP solver returns a solution that is not a HC, the
algorithm then tries to patch the subcycles in the solution into a HC using the well-known
Karp-Steele patching method (Karp & Steele, 1985). If no HC is found either, these DAP
and patching steps are iterated, with the only difference that another DAP solution might
be found. For most cases that we considered in this study, the algorithm can find a HC or
determine that no solution exists after these two steps. If the algorithm fails to solve the
problem after these iterative steps, it then attempts to enumerate the DAP solutions by
formulating the DAP as a Boolean satisfiability problem and repeatedly solving the problem
using a SAT solver and adding constraints to eliminate the DAP solutions that have been
encountered. We discuss the details of these steps in the rest of the section.
2.1 Solving the Assignment Problem
Given n vertices and a matrix C = (cij )1i,jn  Rn,n of the costs between pairs of
vertices, the nAssignment Problemo(AP) is to find a vertex permutation   such that
Pn
  = arg min
i=1 ci,(i) :   n , where n is the set of all permutations of {1, . . . , n}.
Note that an AP solution can be viewed as a collection of cycles visiting each vertex exactly
once.
Many algorithms have been developed for the AP (Bertsekas, 1981; Goldberg & Kennedy,
1995; Jonker & Volgenant, 1987). (For an experimental comparison of AP algorithms see
DellAmico & Toth, 2000.) The most efficient one is the Hungarian algorithm, which is based
on Konig-Egervarys theorem and has a complexity of O(n3 ). In the AP-SAT algorithm we
use the implementation of the Hungarian algorithm by Jonker and Volgenant (1987, 2004).
665

fiJager & Zhang

For an unweighted directed graph G = (V, E), DAP can be solved by applying an AP
algorithm to the AP instance defined by the matrix C = (cij )1i,jn with

cij


 0,
1,
=

1,

if (i, j)  E, i 6= j
if (i, j) 
/ E, i =
6 j
if i = j

where we map the costs of arcs in G to 0 and the costs of the remaining arcs to 1. If the
AP algorithm returns a solution with cost 0, there is a DAP solution in G, since every arc
taken in the AP solution is an arc in G. On the other hand, if it returns a solution of cost
greater than 0, there is no DAP solution in G because at least one arc in the solution does
not belong to G.
The first step of the AP-SAT algorithm is this DAP algorithm. Then a HC of G, if one
exists, is a solution to the DAP. We have to distinguish three cases at the end of the first
step:
 If the cost of the AP solution is greater than 0, G does not have a HC, and the DHCP
instance is solved with no solution.
 If the AP solution has cost 0 and the solution consists of one cycle, we have found a
HC  and the DHCP instance is also solved.
 If the AP solution has cost 0 and the AP solution has more than one cycle, we cannot
determine, based on the AP solution, whether or not G is Hamiltonian. We then
continue to the next steps of the AP-SAT algorithm.
2.2 Karp-Steele Patching
If the DAP solution does not provide a definitive answer to the problem, i.e., the case where
the AP solution cost is 0 and the AP solution contains more than one cycle, we continue to
search for a HC in G. We first patch the subcycles in an attempt to form a HC, and we use
Karp-Steele patching (KSP) for this purpose, which is an effective ATSP heuristic (Glover,
Gutin, Yeo & Zverovich, 2001; Goldengorin, Jager & Molitor, 2006; Karp & Steele, 1985).
The operation of patching two cycles C1 and C2 in an AP solution is defined as follows:
two fixed arcs (v1 , w1 )  C1 and (v2 , w2 )  C2 are first deleted and two arcs (v1 , w2 ) and
(v2 , w1 ) joining the two cycles are added. The cost of patching C1 and C2 using (v1 , w2 )
and (v2 , w1 ) is equal to
(C1 , C2 ) = c(v1 , w2 ) + c(v2 , w1 )  (c(v1 , w1 ) + c(v2 , w2 ))
i.e., (C1 , C2 ) is the difference between the total cost of the inserted arcs and the total cost
of the deleted arcs. In each step we choose to patch the two cycles that have the largest
number of vertices. For these two cycles, the two arcs are chosen in such a way that the
patching cost is the minimum among all possible arc pairs. If we have k  2 cycles, we
repeat this patching step k  1 times to form one cycle at the end. We apply KSP to the
AP instance defined in Section 2.1. If the patching procedure provides a HC, the AP-SAT
algorithm can be terminated. Otherwise, we continue to the next step.
666

fiAlgorithm for Directed Hamiltonian Cyce Problem

2.3 Solving Variant APs
DAP may have multiple solutions, and some of the DAP solutions may be HCs. We can
increase the chance of finding a HC if we apply the AP step multiple times, since the
computational cost of the AP and the KSP algorithms is low. The key is to avoid finding
the same DAP solution again. To accomplish this, we slightly alter some of the arc costs of
the corresponding AP instance so as to find the other DAP solutions, enhanced by the KSP if
needed, to increase the possibility of finding a HC. In other words, we add a perturbation
component to create multiple variant AP instances to boost the overall chance of finding a
HC. Note that in the worst case when the DHCP instance contains no HC, this procedure
will not be productive.
The main idea to create a variant AP instance is to reduce the chance that the subcycles
in the current AP solution can be chosen in the subsequent rounds of solving the APs. This
is done by perturbing the costs of some of the arcs in G as follows. For each arc in the
current DAP solution we increase its cost by one. To create an AP instance different from
that in Section 2.1, we generalize the AP instance as follows. Let ci,j be the cost of the arc
(i, j)  E, and let
M

:= n  max {ci,j | (i, j)  E} + 1

i.e., M is greater than n times the largest cost of an arc in G. We then set the costs of the
edges not in E to M . The AP instance of Section 2.1 is a special case of this AP instance,
where the costs ci,j for all arcs (i, j)  E are 0. It is critical to notice that all DAP solutions,
including a HC, must have costs less than M . As before, if the solution contains a HC,
the algorithm terminates; otherwise, the subcycles are patched using the KSP to possibly
find a HC. We repeat this step multiple times so that an arc, which has appeared in many
previous DAP solutions, will be very unlikely to appear in the next DAP solution, and an
arc, which has never occurred in any previous DAP solution, will be more likely to appear
in the next DAP solution.
Let r be the maximal number of AP/KSP calls, i.e., the number of variant AP instances
solved. We observed in our experiments that r = n (see step 3 of the pseudo code of the
appendix) is a good choice. This will be discussed in detail in Section 3.1.
2.4 Implicitly Enumerating all DAP Solutions Using SAT
All the AP and patching based steps discussed above may still miss a solution to a DHCP
instance. We now consider how to implicitly enumerate all DAP solutions for finding a
solution to the DHCP, if it exists. The idea is to systematically rule out all the DAP
solutions that have been discovered so far during the search. To this end, we first formulate
a DAP as a Boolean satisfiability (SAT) problem (Dechter, 2003) and forbid a DAP solution
by adding new constraints to the SAT model. This elementary technique of adding new
constraints with the purpose of enumerating all SAT solutions can also be applied to a
general SAT problem (e.g., see Jin, Han & Somenzi, 2005). Notice that this cannot be
easily done under the AP framework because such constraints cannot be properly added to
the AP. Moreover, we can take advantage of the research effort that has been devoted to
SAT, in particular, we can use an effective SAT solver called MiniSat (Een & Sorensson,
2003, 2010).
667

fiJager & Zhang

In the conjunctive normal form (CNF), a SAT instance over a set of Boolean variables
is a conjunction of clauses, each of which is a disjunction of literals which are Boolean
variables or their negations. A clause is satisfied if one of its literals is True, and the
instance is satisfied if all its clauses are satisfied. The SAT problem is to find a truth
assignment of the variables to satisfy all clauses if they are satisfiable, or to determine no
such assignment exists. SAT was the first problem shown to be N P-complete (Cook, 1971;
Garey & Johnson, 1979; Karp, 1972).
We now formulate the DAP in SAT. A solution to a DAP must obey the following
restrictions:
 For each vertex i, i = 1, . . . , n, exactly one arc (i, j), i 6= j, exists in the DAP solution.
 For each vertex i, i = 1, . . . , n, exactly one arc (j, i), j 6= i, exists in the DAP solution.
We first introduce an integer decision variable xi,j to the arc (i, j)  E where xi,j = 1 holds
if and only if the arc (i, j) appears in the DAP solution. We represent the above constraints
in the following integer linear program (ILP).
( Pn
xi,j = 1 for i = 1, . . . , n
Pj=1,(i,j)E
(1)
n
i=1,(i,j)E xi,j = 1 for j = 1, . . . , n
where xi,j  {0, 1} for (i, j)  E. We thus have a total of 2n constraints. Note that we only
have to use m variables, one variable for each arc in the graph, which can be substantially
smaller than n2 variables for sparse graphs. We represent the integer linear program (1)
by a SAT model similar to the work of Lynce and Marques-Silva (2006), where we replace
integer variables xi,j with Boolean variables yi,j . To enforce the 2n restrictions in the SAT
formulation, we need to introduce constraints in clauses. One restriction in (1) means that
exactly one of the up to n involved Boolean variables for a vertex can be set to True and
the rest must be False. To represent this, we introduce at most 2n2 auxiliary variables
z1 , z2 , . . . , z2n2 , with up to n zs for one restriction. Without loss of generality, consider the
first restriction, which has z1 , z2 , . . . , zn associated. We use zk to represent that at least one
of y1,1 , y1,2 , . . . , y1,k is True. Precisely, the z variables are defined as follows.
 z1 = y1,1 or equivalently (y1,1  z1 )  (y1,1  z1 ).
 zk = y1,k  zk1 or equivalently (zk  y1,k )  (zk  zk1 )  (zk  y1,k  zk1 ) for
k = 2, 3, . . . , n.
In addition, we need to enforce that only one y1,i for i = 1, 2, . . . , n can be True. This
means that if y1,k is True, none of the y1,i for i < k can be True. This can be formulated
as
 zk1  y1,k for k = 2, 3, . . . , n.
Finally, zn must be True. The other restrictions in (1) are represented similarly.
The SAT based representation allows us to exclude a non-Hamiltonian DAP solution
previously found in the search. This can be done by introducing new clauses to explicitly
668

fiAlgorithm for Directed Hamiltonian Cyce Problem

forbidding all subcycles of this solution. Let such a subcycle be (v1 , v2 , . . . , vk , v1 ). Then
we add the clause
yv1 ,v2  . . .  yvk1 ,vk  yvk ,v1
to the current SAT instance. As a result, the updated SAT instance is not satisfiable,
meaning that the corresponding DHCP instance does not contain a HC, or gives rise to a
new DAP solution, as it does not allow the previous DAP solution.
In summary, after the AP- and patching-related steps failed to find a solution, the APSAT algorithm transforms the problem instance into a SAT instance. Then it collects all
previous DAP solutions, each of which includes at least two subcycles, and excludes these
subcycles for each of the DAP solutions by adding new clauses as described above. Then
the resulting SAT model is solved. If the SAT model is not satisfiable, then the DHCP
algorithm terminates with the result of the problem instance being not Hamiltonian. If the
SAT model is satisfiable and the solution has only one cycle, the algorithm stops with a HC.
If the SAT model is satisfiable, but the solution has more than one subcycle, new clauses
are introduced to the SAT model to rule out this solution, and the algorithm repeats to
solve the revised formula. Since there is a finite number of DAP solutions, the algorithm
terminates. In the worst case if the DAP solutions contain no HC, the SAT part of the
algorithm will enumerate all these DAP solutions. For an overview, we outline the main
steps of the AP-SAT algorithm in a pseudo code in the appendix.
2.5 Some General Remarks
Before we present our experimental results, we like to comment on the method we proposed
to help appreciate its features.
1. The AP-SAT algorithm consists of three main components, namely the AP step, the
KSP step and the SAT step. It might be interesting to know which of these components is the most important one. For this, we have to distinguish between completeness
and efficacy of the algorithm. The only necessary step for the completeness is the SAT
step of Section 2.4. This step without all previous steps leads also to a correct DHCP
algorithm. On the other hand, the AP-SAT algorithm is more effective if the AP and
the KSP steps are called often and the SAT step is not called or called only a few
times. For example, if for an instance no DAP solution exists or an existing HC is
found by the previous steps, the SAT part will not be invoked at all. Indeed, our
experiments showed that the SAT step is not invoked for most of the test instances.
Regarding the relative time needed by the AP and the KSP steps, we have to consider the density of problem instances. For an instance with a small number of arcs,
in most cases there is not only no HC solution, but also no DAP solution. In this
case the algorithm terminates after the first AP step and does not need to make any
KSP call. On the other hand, an instance with a large number of arcs should require
many AP steps, as many DAP solutions may exist which are not HCs, and thus a HC
solution may have to be found by KSP. This expected behavior could be validated by
experiments: the time for the KSP steps is smaller for instances with a small number
of arcs, but is larger for instances with a large number of arcs (see Figure 4).
669

fiJager & Zhang

2. The AP-SAT algorithm is also able to solve HCP as a special case of DHCP, but it
is less effective for this case. The reason is that for a symmetric case, an arc and its
reverse arc are often present in a DAP solution, resulting in many small cycles of two
vertices in the solution. Thus in general we have to enumerate a large number of DAP
solutions. In the worst case when no HC exists, all these DAP solutions have to be
enumerated, giving rise to a long running time.
3. We can easily revise the AP-SAT algorithm to identify all HCs in a directed graph.
Finding all solutions can be desirable for many applications, e.g., the problem of finding all knights tour on a chessboard (Henderson & Apodaca, 2008; Kyek, Parberry &
Wegener, 1997). For algorithms for this problem, see the already mentioned algorithm
of Martello (1983) and the algorithm of Frieze and Suen (1992). The revision works as
follows. If no HC exists, the algorithm remains the same. Consider now the case that
at least one HC exists. If the first HC has been found, the original AP-SAT algorithm
terminates in this case. The revised algorithm at this stage saves the first HC, and
then continues to search for the next HC. In the pseudo code of the appendix, we
only need to replace STOP with by SAVE in rows 8, 11, and 23. Note that
for the revised algorithm, the SAT part is always invoked if at least one HC exists.
Furthermore  like the original AP-SAT algorithm  this revised algorithm works also
for the symmetric case, but is less effective.
4. The AP-SAT algorithm used a restart scheme, i.e., it repeatedly solved a series of AP
instances, which were derived by modifying costs of the arcs appeared in the previous
AP solution. Although the restart scheme and the random restart scheme, which was
developed for constraint problems in artificial intelligence (Gomes, Selman & Kautz,
1998), follow the same design principle of trying to avoid to encounter the same
solutions again in subsequent runs, these two schemes are fundamentally different. As
its name indicated, the random restart scheme depends on random choices made for
variable and value selections in the process of search for a variable assignment for a
constraint problem. In contrast, our restart scheme is not random; the arcs in the
current AP solution will receive higher costs so that the subcycles in the current AP
solution will less likely be chosen again. In other words, the restart scheme we used
is somewhat deterministic and depends on solution structures of the problem.
5. The method we used to exclude the subcycles in the solution to the current DAP instance from the subsequent SAT solving process follows in principle the popular idea
of adding no-good constraints to a constraint satisfaction problem (Frost & Dechter,
1994; Richards & Richards, 2000; Zhang, Madigan, Moskewicz & Malik, 2001). Specifically, these subcycles are forbidden by introducing additional constraints.

3. Experimental Results
We have implemented the AP-SAT algorithm, the DHCP algorithm of Martello (1983),
the DHCP algorithms based on the absolute SAT encoding (Hoos, 1999) and the relative
SAT encoding (Prestwich, 2003) in C++ and compared them to an algorithm based on
the award-winning Concorde TSP program (Applegate et al., 2005, 2006). For the al670

fiAlgorithm for Directed Hamiltonian Cyce Problem

gorithm of Martello we have implemented a version which terminates whenever a HC, if
one exists, is found. For the SAT based algorithms we used the AP solver of Jonker and
Volgenant (1987, 2004) and the MiniSat SAT solver of Een and Sorensson (2003, 2010).
To apply Concorde, a DHCP instance was first transformed to an asymmetric TSP instance by the transformation in Section 2.1 and then to a symmetric TSP instance by the
2-point reduction method (Jonker & Volgenant, 1983). In our implementation, the 2-point
reduction works as follows for a graph G = (V, E) with V = {v1 , v2 , . . . , vn }.
1. Make a copy of the vertices v1 , v2 , . . . , vn , and create the vertex set V 0 := {v10 , v20 ,
. . . , vn0 }.
2. Define a new complete graph G0 on the vertex set V  V 0 with (symmetric) cost
function c0 : V  V 0  {0; 1; 2} by

 0 for 1  i = j  n
1 for 1  i 6= j  n, (vi , vj )  E
c0 (vi , vj0 ) :=

2 for 1  i 6= j  n, (vi , vj ) 
/E
c0 (vi , vj ) := 2 for 1  i 6= j  n
c0 (vi0 , vj0 ) := 2 for 1  i 6= j  n
Then a directed HC exists on G if and only if a TSP tour of cost n exists on G0 . Note that
 in contrast to the general version of the 2-point reduction  no value of  is required
here. We also tried the 3-point reduction method, which is in principle similar to the 2-point
reduction, but uses two (instead of one) copies of the vertex set and uses only cost values
from {0; 1}. For the details of the 3-point reduction, see the work of Karp (1972). Our
experimental results, which are not included here, showed that the 3-point reduction runs
slower on average than the 2-point reduction. Therefore, in the rest of the comparison, we
only consider the 2-point reduction.
After the 2-point reduction, Concorde started with the worst possible solution value
as the initial upper bound and was terminated as soon as its lower bound indicates a HC
is impossible.
In addition to this comparison, we also experimentally analyzed the AP-SAT algorithm
including its asymptotic behavior, and applied it to study phase transitions of the DHCP.
All our experiments were carried out on a PC with an Athlon 1900MP CPU with 2 GB of
memory.
3.1 Comparison of DHCP Algorithms
In our experiments we first tested random asymmetric instances Gn,m and parameters
n = 100, 200, 400, 800, 1600 and m = dcn(log n+log log n)c with c = 0.5, 0.6, . . . , 1.90, 2.00.
For each n and each c we generated 50 random instances and measured the CPU time for
these instances. Furthermore, we tested real-world and random instances from the Dimacs
challenge (Johnson et al., 2002, 2008) and non-random instances (Reinelt, 1991, 2008).
Whereas Tsplib contains 26 single asymmetric TSP instances with sizes from 17 to 443,
the Dimacs challenge contains 10 asymmetric problem generators called amat, coin, crane,
disk, rect, rtilt, shop, stilt, super, and tmat. Using each of these generators we generated 24
671

fiJager & Zhang

instances, 10 with 100 vertices, 10 with 316 vertices, 3 with 1000 vertices, and 1 with 3162
vertices, leading to 240 instances (for each of the 10 problem generators 24 instances) overall.
To transform asymmetric TSP instances back to DHCP instances, it seems to be reasonable
to only keep the arcs of small weights while ignoring the ones with large weights. In other
words, to generate a DHCP instance we chose the m smallest arcs in the corresponding
asymmetric TSP instance. It is interesting to note that the most difficult problem instances
for most problems in Tsplib and Dimacs appear when the degree parameter c is around
2, which is the value we used in our experiments. In contrast, the most difficult instances
of random graphs occur when the degree parameter c is 0.9 (see Section 3.3).
To investigate the variation of running time, we present one subfigure for each problem
class, i.e., for the 5 random classes with sizes 100, 200, 400, 800, 1600, and for the 10
Dimacs classes amat, coin, crane, disk, rect, rtilt, shop, stilt, super, and tmat. The y-axis
gives the average times plus their 95% confidence intervals, where all values are in seconds.
For the random classes the x-axis describes the degree parameter c, and for the Dimacs
classes it describes the size n. The results for the random instances are summarized in
Figure 1 and for the Dimacs instances in Figures 2, 3. As the Tsplib class consists only
of 26 single instances with completely different sizes, structures and difficulties, we present
these results in Table 1. If an experiment of a single algorithm on a single instance required
at least 1 hour or did not terminate due to a high memory requirement, we set the CPU
times as 3600 seconds.

Figures 1  3 and Table 1 show that the two SAT encodings are not competitive with
AP-SAT, Concorde or the Martello algorithm. Furthermore, AP-SAT and Concorde are
more stable than the Martello algorithm. Concorde failed to solve 16 Dimacs instances
(3 coin, 3 crane, 4 rect, 5 stilt, 1 super types) within the maximal allowed time of 1 hour,
whereas the AP-SAT algorithm failed only on 7 instances. Among these 7 instances on
which AP-SAT failed, 6 are stilt types, and the remaining instance (super3162) could be
solved if we increased the maximal allowed time from 1 hour to 4 hours (see Table 2). The
Martello algorithm was unable to solve the instances with 800 or larger size because of its
high memory requirement. For the other instances, it failed on 1 random instance of size
400 with degree parameter 0.9, on 51 Dimacs instances (10 coin, 12 crane, 11 disk, 11 rect,
7 stilt types), and 9 Tsplib instances (see Table 1). Nevertheless, the Martello algorithm
outperformed Concorde on smaller and easier instances, indicating that the former has
a worse asymptotic running time. Overall, we observed that the AP-SAT algorithm is
clearly superior to the four other algorithms. Among the 4266 instances (4000 random
instances, 240 Dimacs instances and 26 Tsplib instances) tested, only on 13 instances,
one of the other four algorithms is faster than AP-SAT. These problem instances include 4
random instances, namely 1 of size 400 with degree parameter 0.9, 3 of size 800 with degree
parameters 0.8, 0.9, 0.9, respectively, 8 Dimacs instances, namely coin1000-2, rect316-9,
stilt100-1, stilt100-5, stilt100-6, stilt100-7, stilt100-8, stilt316-2, and the Tsplib instance
br17 (see Table 1).
672

fiAlgorithm for Directed Hamiltonian Cyce Problem

Figure 1: Comparison of all algorithms on random instances.
Size 200
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

APSAT
Concorde
Martello

10,000

Average running time

Average running time

Size 100

1,000
100
10
1
0.1
0.01

0.01
0.001

0.001
0.0001

0.0001

0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

Degree parameter c

Degree parameter c

Size 400

Size 800
APSAT
Concorde
Martello

100
10
1
0.1
0.01

APSAT
Concorde

10,000

Average running time

1,000

1,000
100
10
1
0.1
0.01

0.001

0.001

0.0001

0.0001

0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

Degree parameter c

Degree parameter c
Size 1600
APSAT
Concorde

10,000

Average running time

Average running time

10,000

1,000
100
10
1
0.1
0.01
0.001
0.0001

0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

Degree parameter c

673

fiJager & Zhang

Figure 2: Comparison of all algorithms on Dimacs instances, part 1.
coin instances
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

Average running time

Average running time

amat instances

APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

0.01

0.01

0.001

0.001

0.0001

100

316

1000

0.0001

3162

100

316

Size

10,000
1,000
100
10
1
0.1

APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

0.01

0.01

0.001

0.001
100

316

1000

0.0001

3162

100

316

Size

1,000
100
10
1
0.1

Average running time

Average running time

10,000

APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

0.01

0.01

0.001

0.001
316

3162

rtilt instances
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding

100

1000

Size

rect instances

0.0001

3162

disk instances
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding

Average running time

Average running time

crane instances

0.0001

1000

Size

1000

0.0001

3162

Size

100

316

1000

Size

674

3162

fiAlgorithm for Directed Hamiltonian Cyce Problem

Figure 3: Comparison of all algorithms on Dimacs instances, part 2.
stilt instances
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

Average running time

Average running time

shop instances

APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

0.01

0.01

0.001

0.001

0.0001

100

316

1000

0.0001

3162

100

316

Size

10,000
1,000
100
10
1
0.1

APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding
10,000
1,000
100
10
1
0.1

0.01

0.01

0.001

0.001
100

316

3162

tmat instances
APSAT
Concorde
Martello
Abs. Encoding
Rel. Encoding

Average running time

Average running time

super instances

0.0001

1000

Size

1000

0.0001

3162

Size

100

316

1000

Size

675

3162

fiJager & Zhang

Table 1: Comparison of all algorithms on Tsplib instances.
Instance (Size)

AP-SAT

Concorde

br17 (17)
ftv33 (34)
ftv35 (36)
ftv38 (39)
p43 (43)
ftv44 (45)
ftv47 (48)
ry48p (48)
ft53 (53)
ftv55 (56)
ftv64 (65)
ft70 (70)
ftv70 (71)
kro124p (100)
ftv100 (101)
ftv110 (111)
ftv120 (121)
ftv130 (131)
ftv140 (141)
ftv150 (151)
ftv160 (161)
ftv170 (171)
rbg323 (323)
rbg358 (358)
rbg403 (403)
rbg443 (443)

4.05
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.01
0.02
0.02
0.02

0.07
0.13
0.15
0.23
0.42
0.19
0.16
0.07
1.56
0.09
0.23
0.87
0.29
3.74
0.56
2.42
0.8
3.04
0.84
1.13
1.14
2.13
4.81
13.55
4.52
6.73

Running time for algorithm
Martello Absolute Encoding
66.39
0
0
0
0
0
0
3600
0
0.01
0
0
0
0.04
3600
3600
3600
3600
3600
3600
3600
3600
0.09
0.14
0.18
0.21

676

0.85
3.29
5.59
2.96
65.58
1.97
5.23
53.96
20.31
11.13
119.96
34.18
1904.56
1993.33
1024.22
3600
3600
3600
593.65
2676.16
3600
3600
3600
3600
3600
3600

Relative Encoding
0.08
0
0
0.01
0.01
0.01
0.01
47.91
0.02
230.04
0.04
0.05
0.06
3600
3600
3600
3600
3600
3600
3600
3600
3600
5.12
6.98
10
13.24

fiAlgorithm for Directed Hamiltonian Cyce Problem

3.2 Analysis of AP-SAT
The efficacy of the AP-SAT algorithm may be due to the following reasons. Instances
with no HC are most likely to have no DAP solution either, and therefore the algorithm
terminates after the first AP call. On the other hand, instances with a HC are likely to
have multiple HCs, one of which can be found quickly by the AP or KSP steps. The only
difficult case is when there are many DAP solutions, but none or a very few of them are
HCs. In this case the AP and KSP steps may fail, and the SAT part will be invoked to find
a HC or to disprove the existence of a HC.
In the following we will analyze the instances where AP-SAT fails or requires much
time, and analyze the number r of computing variant AP instances (which we had set to
the size of the instance n; see the end of Section 2.3). Therefore we investigated the three
procedures in AP-SAT, namely AP, KSP and SAT. We observed that the SAT part was
invoked only on 14 out of all 4266 instances tested. We considered these 14 and other two
instances (stilt3162 and super3162), on which AP-SAT did not terminate in 1 hour, to be
hard. To further analyze these 16 hard instances we increased the maximal allowed time
from 1 hour to 4 hours. In Table 2 we present the running times of AP, KSP and SAT, and
the number of calls to the three procedures, where the numbers of AP and KSP calls are
given in the same column, as these two numbers are equal or different by only one (see the
pseudo code in the appendix). Furthermore, we add two additional pieces of information:
whether an instance has a HC or whether this is unknown, and whether AP-SAT terminated
on the instance in 4 hours. In Table 2, Memory means that this part terminated due to
a high memory requirement. Note that the solution status of the instance stilt316-2 (no
HC) was known, since Concorde  in contrast to AP-SAT  was able to solve it.
Table 2 shows that the running time of AP/KSP contributed to the majority of the
total running time of AP-SAT only on 4 out of the 16 hard instances, i.e., coin1000-2 and
rect316-9, and the two instances stilt3162 and super3162 on which SAT is not invoked at
all. On 6 instances, AP-SAT did not terminate. On 5 out of these 6 instances, i.e., stilt3162, stilt316-4, stilt316-5, stilt1000-1, and stilt1000-2, the SAT part did not terminate in a
reasonable amount of time or the algorithm stopped due to a high memory requirement of
SAT.
In order to determine r, we re-ran all instances in Table 2 with three different values of
r, i.e., r = 0, r = n/2, and r = 2n. The results (not presented) showed that when AP-SAT
was unable to terminate with r = n (i.e., on the 6 instances stilt316-2, stilt316-4, stilt316-5,
stilt1000-1, stilt1000-2, and stilt3162), it also failed to stop with other values of r. For all
remaining 10 instances, increasing r = n to r = 2n did not reduce the running times. This
is reasonable for the two instances coin1000-2 and rect316-9 with a large AP/KSP time,
as they have no HC. On the other hand, these two instances are the only ones on which
AP-SAT ran faster by using smaller values of r, namely coin1000-2 by using r = n/2 and
rect316-9 by using r = 0.
We thus conclude that r should not be increased, but rather be decreased. As it is hard
to estimate the memory requirements and the time of the SAT part, one alternative for
difficult instances would be to start AP-SAT with a smaller parameter r and then to stop
the SAT part after some time or after one unsuccessful call. After that the complete APSAT algorithm can be restarted with a larger r. For most instances, however, the choice of
677

fiJager & Zhang

Table 2: Comparison of the performance of AP, KSP, and SAT procedures in the AP-SAT
algorithm on 16 hard instances.
Instance
br17
coin1000-2
rect100-2
rect316-9
stilt100-1
stilt100-5
stilt100-6
stilt100-7
stilt100-8
stilt316-2
stilt316-4
stilt316-5
stilt1000-1
stilt1000-2
stilt3162
super3162

Running time
AP
KSP
SAT
0
0
4.1
352.73
47.54
1.82
0.07
0.01
0.27
3.69
0.61
0.35
0.17
0
0.07
0.2
0.01
28.84
0.17
0.02
0.07
0.17
0.05
0.06
0.15
0.03
0.07
20.21
1.37 14378.42
13.15
0.71
Memory
21.14
1.63
Memory
1446.63 107.06 12846.31
1457.76 102.21 12840.03
13832.40
567.60
0
13244.88
441.46
0

Number of calls
AK/KSP SAT
17
138
1000
1
100
1
316
1
100
1
100
41
100
1
100
1
100
1
316
1
316
1
316
1
1000
1
1000
1
650
0
1032
0

HC

Termin.

No
No
No
No
No
Yes
No
No
No
No
Unknown
Unknown
Unknown
Unknown
Unknown
Yes

Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
No
No
No
No
No
Yes

Figure 4: Comparison of the performance of AP and KSP procedures in AP-SAT on random
instances of size 1600.
AP
KSP

6

Average running time

5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2

Degree parameter c

r is not relevant. More difficult problem instances are required to perform a comprehensive
analysis of r.
Finally, in Figure 4 we compare the times used by AP and KSP on random instances of
size 1600 with degree parameter c = 0.5, 0.6, . . . , 1.90, 2.00.
We observe that AP is more time consuming than KSP. With a smaller c this effect is
more obvious because most instances can be solved with a result of No HC after the first
AP call, and thus the KSP does not need to be invoked at all.
678

fiAlgorithm for Directed Hamiltonian Cyce Problem

3.3 Phase Transitions of the DHCP
For random undirected graphs Gn,m , where m arcs are randomly chosen from all possible
n(n  1)/2 arcs over n vertices in the graph, Komlos and Szemeredi (1983) proved a phase
transition of c  dn  (log n + log log n)/2c with c = 1 for the HCP. Vandegriend and Culberson (1998) experimentally verified the theoretical result, where the constant c is between 1.08
and 1.10. For the DHCP, where m arcs are randomly chosen from all possible n(n  1) arcs,
McDiarmid proved a phase transition of m = cdn(log n+log log n)c with c = 1 (1980). Our
experiments were aimed to verify this result and determine the multiplicative constant c. As
a directed graph may contain twice as many arcs as the undirected counterpart, we would
expect the number of arcs to be doubled as well at the phase transition point. Therefore
we tested m = dc  n  (log n + log log n)c with c = 0.5, 0.6, 0.7, 0.8, 0.81, 0.82, . . . , 1.19, 1.20,
1.30, 1.40, 1.50, 1.60, 1.70, 1.80, 1.90, 2.00, where we expected the phase transition to occur
at c = 1. We considered problem instances with n = 128, 256, 512, 1024, 2048, 4096, 8192
vertices and chose 1000 independently generated random graphs for each n and for each c.
The phase transition result is shown in Table 3 and Figure 5, where the first parameter
is c and the second parameter the percentage of Hamiltonian graphs among all graphs
considered. We observe a phase transition of the DHCP similar to that of the HCP. In
particular, it is evident from Figure 5 that the phase transition becomes sharper, i.e., there
is a crossover among the phase transition curves, when the problem size increases, which
is characteristic for phase transitions in complex systems. This crossover occurs around
the degree parameter c = 0.9, which is substantially different from the expected value
of 1. In short, our observations verified the existence of a phase transition of the DHCP,
and the phase transition occurs at dc  n  (log n + log log n)c with approximately c = 0.9.
Furthermore, for the same constant c = 0.9, the probability that Gn,m is Hamiltonian
is 1/2. As a comparison, for undirected graphs, a constant between 1.08 and 1.10 was
found (Vandegriend & Culberson, 1998).

3.4 Asymptotic Behavior of AP-SAT
An interesting characteristic of an algorithm is its asymptotic behavior. To quantify this
behavior for the AP-SAT algorithm, we revisited the experiments of Section 3.3, i.e., the
experiments that verified the phase transitions of the DHCP. As described earlier, we considered random problem instances with n = 128, 256, 512, 1024, 2048, 4096, 8192 vertices
and chose 1000 independently generated random graphs for each n and for each c. To
measure the worst-case asymptotic behavior of AP-SAT, we only measured the CPU times
of the algorithm on the most difficult instances, i.e., the instances with degree parameter
c = 0.9 (see Section 3.3). The results can be found in Figure 6, where the x-axis is the
problem size and the y-axis the average time required. Since both, x- and y-axis are in
logarithmic scale and the log-log curve in Figure 6 is nearly linear, the average running
time of AP-SAT can be considered to be polynomial on the number n of vertices in the
graph. This is reasonable, as for random instances the SAT part was not called at all (see
Section 3.2), and the AP and KSP combined has a complexity not worse than O(n3 ).
679

fiJager & Zhang

Table 3: Phase transition of random instances.
c
0.5
0.6
0.7
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1
1.01
1.02
1.03
1.04
1.05
1.06
1.07
1.08
1.09
1.1
1.11
1.12
1.13
1.14
1.15
1.16
1.17
1.18
1.19
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2

128
0
0
5.1
23.3
25.5
27.8
30.6
32.4
33.6
37.6
39.8
44
46.8
49
52.8
55.5
59.2
60.1
62.7
63.6
67.2
68.8
69.4
71
72.8
74.2
75.8
76.9
77.4
78.4
81.7
81.6
83.3
85.1
85.4
86.3
86.6
86.7
87.3
88.5
88.7
87.8
88.5
89.1
95.7
96.4
99.1
99.7
99.7
99.8
100
100

256
0
0
3
21.9
24.2
28.4
28.9
31.8
34.1
36.5
39.1
43.5
47.3
52
53.2
54.4
58.4
60.1
61.7
65.3
65
67.9
71.8
74
74.3
75.4
75.4
76.6
78.6
79.3
79.2
82.4
83.2
84.4
87
87.4
88.6
88.9
89.2
89.2
91.2
92.6
92.9
93.8
97.2
98.8
99.1
99.6
99.8
99.8
100
100

512
0
0
2.6
21.5
23.9
27.4
33
34.9
35.6
36.9
38.3
42.4
47.3
49.8
52.3
56.9
59.5
61.5
60.8
64
66.2
68.3
72.1
72.1
74.5
76.3
79
82.1
84.4
86
87.3
88.4
88.6
88.8
89.3
89.7
90.1
90.7
90.9
92.6
93.1
93.6
93
93.3
97.5
99
99.9
99.8
100
99.9
99.9
100

Size
1024
0
0
1.3
18.7
20.6
23.1
25.8
32.9
30
35.4
40.8
43.8
45.8
52.5
54.7
54.1
60.8
63.6
64.9
66.2
67.9
71.2
73.8
73.6
78.3
81.1
81.9
83.2
85.2
85.9
88.1
87.1
87.2
86.8
90.9
90.3
89.5
92.2
93.2
93.9
94.1
95 0
95.5
96.2
98.8
99.5
99.9
99.9
100
99.9
99.9
100
680

2048
0
0
0.5
16.3
20.8
25
27
28.5
34.3
37.4
41
44.7
47.9
50.1
50.3
54.7
58.4
61.4
68.4
66.8
71.6
72.5
75
77.2
80.8
81.4
81
84.2
86.3
85.6
89.9
89.4
89.4
89.5
92
92.9
93
93.9
93.8
95.1
95.3
96.1
94.8
96.2
98.9
99.7
99.8
99.9
99.9
100
100
100

4096
0
0
0.1
14.1
17.9
20
23.3
29
33.4
34.7
35.9
40.2
47.7
48.6
52.7
59.4
60.7
65.6
68.3
72.7
71
75.7
77
79.8
78.7
82.4
83.4
85.3
88.3
85.9
90
90.3
92.6
92
93.8
93.3
93.9
94.1
94.2
95.5
94.7
95.8
97.3
97.6
98.7
99.5
99.9
100
99.9
100
100
100

8192
0
0
0.2
12.7
15.7
18.7
23.9
28.9
30
34.1
38.7
40.6
44.8
50.5
54.6
59.8
61.5
65.8
70.5
73.1
71.4
73.4
76.3
80
81.7
81.9
85.4
86.5
88.3
90.7
92.3
92
92.4
93.8
93.9
94
94.7
97.3
96.4
97.2
97.2
96.4
97.2
97.9
99.2
99.8
100
100
99.9
100
100
100

fiAlgorithm for Directed Hamiltonian Cyce Problem

Figure 5: Phase transition of random instances.
100
Size 128
Size 256
Size 512
Size 1024
Size 2048
Size 4096
Size 8192

Existence of HCs in %

80

60

40

20

0
0.4

0.6

0.8

1

1.2

1.4

Degree parameter c

681

1.6

1.8

2

fiJager & Zhang

Figure 6: Asymptotic behavior of the AP-SAT algorithm.
10,000
APSAT

Average running time

1,000
100
10
1
0.1
0.01
0.001
0.0001

128

256

512

1024

2048

4096

8192

Size

4. Summary
The Hamiltonian cycle problem (HCP) is an important, canonical combinatorial problem.
Surprisingly, for the HCP in directed graphs, which we called directed HCP or DHCP, no
effective exact algorithm has been developed. Our main result of this work is a novel and
effective exact algorithm for the DHCP. Our algorithm utilizes an existing algorithm for
the assignment problem and an existing method for Boolean satisfiability (SAT). Our work
includes a new SAT formulation of the HCP and the AP, which can be potentially extended
to other problems such as the TSP. Our experimental results on random and real problem
instances showed that our new algorithm is superior to four known algorithms including
one algorithm that takes advantage of the award-winning Concorde TSP algorithm. Furthermore, the first phase transition result on combinatorial problems was done on the HCP
and later was extended to the DHCP. In this paper we experimentally verified the existence
of a phase transition of the DHCP and refined the location where such a phase transition
appears using our new exact DHCP algorithm.

Acknowledgments
We thank David S. Johnson at AT&T Labs - Research and Gregory Gutin at Royal Holloway
University of London for many discussions related to this work and their insightful comments
on our manuscript. This research was supported in part by NSF grants IIS-0535257 and
DBI-0743797 to Weixiong Zhang.

682

fiAlgorithm for Directed Hamiltonian Cyce Problem

Appendix A. Pseudo Code of AP-SAT Algorithm
INPUT Directed non-complete graph G = (V, E) with |V | = n.
1 Define matrix C as in Section 2.1, M := 1.
2 Define subcycle collection set W := .
3 FOR s = 1, . . . , n
4
Solve AP on instance matrix C with solution value g, AP solution
(v1 , vi1 ), (v2 , vi2 ) . . . , (vn1 , vin1 ), (vn , vin ), number of cycles k.
5
IF g  M
6
THEN STOP with No HC.
7
ELSE IF k = 1
8
THEN STOP with HC being the AP solution.
9
Apply KSP to the cycles, and receive solution value h and complete
cycle (w1 , w2 , . . . , wn , w1 ).
10
IF h = 0
11
THEN STOP with HC (w1 , w2 , . . . , wn , w1 ).
12
FOR t = 1, . . . , n
13
cvt ,vit = cvt ,vit + 1
14
M = n  max {ci,j | (i, j)  E} + 1.
15
ci,j = M for all (i, j) 
/ E.
16
Add each subcycle of AP solution to W .
17 Start with the SAT model explained in Section 2.4.
18 For each subcycle (v1 , v2 , . . . , vk1 , vk , v1 ) of W add the clause
yv1 ,v2  . . .  yvk1 ,vk  yvk ,v1 to the SAT model.
19 Solve the SAT model.
20 IF Variable setting exists for the model.
21
THEN Add all k subcycles of the solution of the SAT model to W .
22
IF k = 1
23
THEN STOP with HC being the only
subcycle.
24
GOTO 19.
25
ELSE STOP with No HC.
OUTPUT HC of G, or proof that No HC exists in G.

References
Angluin, D. & Valiant, L.G. (1979). Fast Probabilistic Algorithms for Hamiltonian Circuits
and Matchings. J. Comput. System. Sci. 18(2), 155-193.
683

fiJager & Zhang

Applegate, D.L., Bixby, R.E., Chvatal, V. & Cook, W.J. (2005). Concorde Code:
http://www.tsp.gatech.edu/concorde.html
Applegate, D.L., Bixby, R.E., Chvatal, V. & Cook, W.J. (2006). The Traveling Salesman
Problem. A Computational Study. Princeton University Press.
Applegate, D.L., Bixby, R.E., Chvatal, V., Cook, W.J., Espinoza, D., Goycoolea, M.
& Helsgaun, K. (2009): Certification of an Optimal Tour through 85,900 Cities.
Oper. Res. Lett. 37(1), 11-15.
Bang-Jensen, J. & Gutin, G. (2008). Chapter 5 in: Digraphs: Theory, Algorithms and Applications. Springer, London. Free available:
http://www.cs.rhul.ac.uk/books/dbook/
Bertsekas, D.P. (1981). A New Algorithm for the Assignment Problem. Math. Program. 21,
152-171.
Bollobas, B. (1985). Random Graphs. Academic Press, London.
Bollobas, B., Fenner, T.I. & Frieze, A.M. (1987). An Algorithm for Finding Hamiltonian
Paths and Cycles in Random Graphs. Combinatorica 7(4), 327-341.
Bondy, J.A. (1995). Basic Graph Theory: Paths and Circuits. In Graham, R.L., Grotschel,
M., Lovasz, L. (Eds.): Handbook of Combinatorics I (3-110). North-Holland, Amsterdam.
Cheeseman, P., Kanefsky, B. & Taylor, W.M. (1991). Where the Really Hard Problems
Are. In Mylopoulos, J., Reiter, R. (Eds.): Proc. 12th International Conference on
Joint Artificial Intelligence (IJCAI), 331-337. Morgan Kaufmann.
Christofides, N. (1975). Graph Theory  An Algorithmic Approach. Academic Press, New
York.
Chvatal, V. (1985). Hamiltonian Cycles. Chapter 11 in Lawler, E.L., Lenstra, J.K., Rinnooy
Kan, A.H.G., Shmoys, D.B. (Eds.): The Traveling Salesman Problem. A Guided Tour
of Combinatorial Optimization. John Wiley & Sons, Chichester.
Cook, S.A. (1971). The Complexity of Theorem-Proving Procedures. Proc. 3rd Ann. ACM
Symp. on Theory of Computing (STOC), 151-158.
Cook, W.J. (2010). TSP Homepage:
http://www.tsp.gatech.edu/
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
DellAmico, M. & Toth, P. (2000). Algorithms and Codes for Dense Assignment Problems:
the State of the Art. Discrete Appl. Math. 100(1-2), 17-48.
Een, N. & Sorensson, N. (2003). An Extensible SAT-Solver. In Giunchiglia, E., Tacchella, A.
(Eds.): Proc. 6th International Conference on Theory and Applications of Satisfiability
Testing (SAT). Lecture Notes in Comput. Sci. 2919, 502-518.
Een, N. & Sorensson, N. (2010). MiniSat Code:
http://minisat.se
684

fiAlgorithm for Directed Hamiltonian Cyce Problem

Frank, J., Gent, I. & Walsh, T. (1998). Asymptotic and Finite Size Parameters for Phase
Transitions: Hamiltonian Circuit as a Case Study. Inform. Process. Lett. 65(5), 241245.
Frieze, A.M. (1988a). Finding Hamiltonian Cycles in Sparse Random Graphs. J. Combin. Theory Ser. B 44, 230-250.
Frieze, A.M. (1988b). An Algorithm for Finding Hamilton Cycles in Random Directed
Graphs. J. Algorithms 9, 181-204.
Frieze, A.M. & Suen, S. (1992). Counting Hamilton Cycles in Random Directed Graphs.
Random Structures Algorithms 9, 235-242.
Frost, D. & Dechter, R. (1994). Dead-End Driven Learning. Proc. 12th National Conference
on Artificial Intelligence (AAAI), 294-300. AAAI Press.
Johnson, D.S. (2008). 8th Dimacs Implementation Challenge: The Traveling Salesman Problem:
http://www.research.att.com/~dsj/chtsp/
Garey, M.R. & Johnson, D.S. (1979). Computers and Intractability. A Guide to the Theory
of N P-Completeness. Freeman, New York.
Glover, F., Gutin, G., Yeo, A. & Zverovich, A. (2001). Construction Heuristics for the
Asymmetric TSP. European J. Oper. Res. 129, 555-568.
Goldberg, A.V. & Kennedy, R. (1995). An Efficient Cost Scaling Algorithm for the Assignment Problem. Math. Program. 71, 153-177.
Goldengorin, B., Jager, G. & Molitor, P. (2006). Tolerance Based Contract-or-Patch Heuristic for the Asymmetric TSP. In Erlebach, T. (Ed.): Proc. 3rd Workshop on Combinatorial and Algorithmic Aspects of Networking (CAAN). Lecture Notes in Comput. Sci. 4235, 86-97.
Gomes, C.P., Selman, B. & Kautz, H. (1998). Boosting Combinatorial Search Through
Randomization. Proc. 15th National Conference on Artificial Intelligence (AAAI),
431-437. AAAI Press.
Gould, R.J. (1991). Updating the Hamiltonian Problem  a Survey. J. Graph Theory 15(2),
121-157.
Grebinski, V. & Kucherov, G. (1996). Reconstructing a Hamiltonian Circuit by Querying
the Graph: Application to DNA Physical Mapping. IR 96-R-123, Centre de Recherche
en Informatique de Nancy.
Gutin, G. & Moscato, P. (2000). Hamiltonian Page:
http://alife.ccp14.ac.uk/memetic/~moscato/Hamilton.html
Henderson, R. & Apodaca, E. (2008). A Knight of Egodeth: Zen Raptured Quietude. BookSurge Publishing.
Hoos, H.H. (1999). SAT-Encodings, Search Space Structure, and Local Search Performance.
Proc. 16th International Joint Conference on Artificial Intelligence (IJCAI), 296-303.
Morgan Kaufmann.
685

fiJager & Zhang

Jin, H., Han, H. & Somenzi, F. (2005). Efficient Conflict Analysis for Finding All Satisfying
Assignments of a Boolean Circuit. In Halbwachs, N., Zuck, L.D. (Eds.): Proc. 11th
International Conference on Tools and Algorithms for the Construction and Analysis
of Systems (TACAS). Lecture Notes in Comput. Sci. 3440, 287-300.
Johnson, D.S., Gutin, G, McGeoch, L.A., Yeo, A., Zhang, W. & Zverovich, A. (2002).
Experimental Analysis of Heuristics for the ATSP. Chapter 10 in: Gutin, G., Punnen,
A.P. (Eds.): The Traveling Salesman Problem and Its Variations. Kluwer.
Jonker, R. & Volgenant, A. (1983). Transforming Asymmetric into Symmetric Traveling
Salesman Problems. Oper. Res. Lett. 2(4), 161-163.
Jonker, R. & Volgenant, A. (1987). A Shortest Augmenting Path Algorithm for Dense and
Sparse Linear Assignment Problems. Computing 38, 325-340.
Jonker, R. & Volgenant, A. (2004). AP Code:
http://www.magiclogic.com/assignment.html
Kabadi, S.N. & Punnen, A.P. (2002). The Bottleneck TSP. Chapter 15 in: Gutin, G., Punnen, A.P. (Eds.): The Traveling Salesman Problem and Its Variations. Kluwer.
Karp, R.M. (1972). Reducibility Among Combinatorial Problems. In Miller, R.E., Thatcher,
J.W. (Eds.): Complexity of Computer Computations, 85-103. New York: Plenum.
Karp, R.M. & Steele, J.M. (1985). Probabilistic Analysis of Heuristics. Chapter 6 in: Lawler,
E.L., Lenstra, J.K., Rinnooy Kan, A.H.G., Shmoys, D.B. (Eds.): The Traveling Salesman Problem. A Guided Tour of Combinatorial Optimization. John Wiley & Sons,
Chicester.
Kelly, L. (2007). Hamilton Cycles in Directed Graphs. PhD Thesis, University of Birmingham, United Kingdom.
Komlos, M. & Szemeredi, E. (1983). Limit Distribution for the Existence of a Hamiltonian
Cycle in a Random Graph. Discrete Math. 43, 55-63.
Kyek, O., Parberry, I. & Wegener, I. (1997). Bounds on the Number of Knights Tours.
Discrete Appl. Math. 74(2), 171-181.
Lynce, I. & Marques-Silva, J. (2006). Efficient Haplotype Inference with Boolean Satisfiability. Proc. 21st National Conference on Artificial Intelligence (AAAI). AAAI Press.
Martello, S. (1983). An Enumerative Algorithm for Finding Hamiltonian Circuits in a Directed Graph. ACM Trans. Math. Software 9(1), 131-138.
McDiarmid, C.J.H. (1980). Cluster Percolation and Random Graphs. Math. Program. Stud. 13, 17-25.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B. & Troyansky, L. (1999). Determining Computational Complexity from Characteristic Phase Transitions. Nature 400,
133.
Prestwich, S. (2003). SAT Problems with Chains of Dependent Variables. Discrete
Appl. Math. 130(2), 329-350.
Posa, L. (1976). Hamiltonian Circuits in Random Graphs. Discrete Math. 14, 359-364.
686

fiAlgorithm for Directed Hamiltonian Cyce Problem

Reinelt, G. (1991). TSPLIB  a Traveling Salesman Problem Library. ORSA J. Comput. 3,
376-384.
Reinelt, G. (2008). Tsplib Library:
http://www.iwr.uni-heidelberg.de/groups/comopt/software/TSPLIB95/
Richards, E.T. & Richards, B. (2000). Non-Systematic Search and No-Good Learning.
J. Automat. Reason. 24(4), 483-533.
Skiena, S. (2008). Stony Brook Algorithm Repository:
http://www.cs.sunysb.edu/~algorith/files/hamiltonian-cycle.shtml
Stojakovic, M. & Szabo, T. (2005). Positional Games on Random Graphs. Random Structures Algorithms 26(1-2), 204-223.
Vandegriend, B. (1998). Finding Hamiltonian Cycles: Algorithms, Graphs and Performance.
Master Thesis, University of Alberta, Canada.
Vandegriend, B. & Culberson, J. (1998). The Gn,m Phase Transition is Not Hard for the
Hamiltonian Cycle Problem. J. Artificial Intelligence Res. 9, 219-245.
Velev, M.N. & Gao, P. (2009). Efficient SAT Techniques for Absolute Encoding of Permutation Problems: Application to Hamiltonian Cycles. Proc. 8th Symposium on Abstraction, Reformulation and Approximation (SARA), 159-166.
Zhang, W. & Korf, R.E. (1996). A Study of Complexity Transitions on the Asymmetric
Traveling Salesman Problem. Artificial Intelligence 81, 223-39.
Zhang, L., Madigan, C.F., Moskewicz, M.H. & Malik, S. (2009). Efficient Conflict Driven
Learning in a Boolean Satisfiability Solver. Proc. IEEE/ACM International Conference on Computer Aided Design (ICCAD), 279-285.

687

fiJournal of Artificial Intelligence Research 39 (2010) 429-481

Submitted 02/10; published 10/10

Nominals, Inverses, Counting, and Conjunctive Queries
or: Why Infinity is your Friend!
Sebastian Rudolph

rudolph@kit.edu

AIFB, Karlsruhe Institute of Technology, DE

Birte Glimm

birte.glimm@comlab.ox.ac.uk

Oxford University Computing Laboratory, UK

Abstract
Description Logics are knowledge representation formalisms that provide, for example,
the logical underpinning of the W3C OWL standards. Conjunctive queries, the standard
query language in databases, have recently gained significant attention as an expressive
formalism for querying Description Logic knowledge bases. Several different techniques for
deciding conjunctive query entailment are available for a wide range of DLs. Nevertheless,
the combination of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2
DL causes unsolvable problems for the techniques hitherto available. We tackle this problem
and present a decidability result for entailment of unions of conjunctive queries in the DL
ALCHOIQb that contains all three problematic constructors simultaneously. Provided
that queries contain only simple roles, our result also shows decidability of entailment of
(unions of) conjunctive queries in the logic that underpins OWL 1 DL and we believe
that the presented results will pave the way for further progress towards conjunctive query
entailment decision procedures for the Description Logics underlying the OWL standards.

1. Introduction
We present a decidability result for entailment of unions of conjunctive queries in the very
expressive Description Logic ALCHOIQb. The article is an extended version of the conference paper Status QIO: Conjunctive Query Entailment is Decidable, Proceedings of the
12th International Conference on the Principles of Knowledge Representation and Reasoning (KR 2010), May 0913, 2010 (Glimm & Rudolph, 2010).
Description Logics (DLs) are a family of logic based knowledge representation formalisms
(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Most DLs correspond
to the function-free two variable fragment of First-Order Logic (FOL) often extended with
counting quantifiers (e.g., xn y(R(x, y))) and DLs are also closely related to the (2variable) guarded fragment since DL formulae naturally result in guarded formulae when
translated into FOL. In line with the restriction to 2 variables, DL formulae contain only
unary and binary predicates, which are called concepts and roles in DLs. The constructors
for building complex expressions are usually chosen such that the key inference problems,
such as concept satisfiability, are decidable. A DL knowledge base (KB) consists of a TBox,
which contains intensional knowledge such as concept definitions and general background
knowledge (essentially a FOL theory), and an ABox, which contains extensional knowledge
and is used to describe individuals (a set of ground facts). Using a database metaphor, the
TBox corresponds to the schema, and the ABox corresponds to the data. In contrast to
c
2010
AI Access Foundation. All rights reserved.

fiRudolph & Glimm

databases, however, DL knowledge bases, as FOL in general, adopt an open world semantics,
i.e., they represent information about the domain in an incomplete way.
Standard DL reasoning services include testing concepts for satisfiability and retrieving
certain instances of a given concept. The latter retrieves, for a knowledge base consisting of an ABox A and a TBox T , all (ABox) individuals that are instances of the given
(possibly complex) concept expression C, i.e., all those individuals a such that T and A
entail that a is an instance of C. The underlying reasoning problems are well-understood,
and the computational complexity of the standard reasoning tasks given a knowledge base
as input range from PTime-complete for DLs with limited expresivity such as DL-Lite
(Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2005), EL (Baader, 2003), and ELP
(Krotzsch, Rudolph, & Hitzler, 2008) to 2-NExpTime-complete for very expressive DLs
such as SROIQ (Kazakov, 2008).
Despite the high worst case complexity of the standard reasoning problems for very
expressive DLs such as SROIQ, there are highly optimized implementations available,
e.g., FaCT++ (Tsarkov & Horrocks, 2006), Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007), and HermiT (Motik, Shearer, & Horrocks, 2009). These systems
are used in a wide range of applications, e.g., biology (Sidhu, Dillon, Chang, & Sidhu,
2005), bio informatics (Wolstencroft, Brass, Horrocks, Lord, Sattler, Turi, & Stevens, 2005),
medicine (Golbreich, Zhang, & Bodenreider, 2006), information integration (Calvanese,
De Giacomo, Lenzerini, Nardi, & Rosati, 1998b), geography (Goodwin, 2005), geology (Jet
Propulsion Laboratory, 2006), defense (Lacy, Aviles, Fraser, Gerber, Mulvehill, & Gaskill,
2005), and configuration (McGuinness & Wright, 1998). Most prominently, DLs are known
for their use as a logical underpinning of ontology languages, e.g., OIL, DAML+OIL, the
W3C standard OWL 1 (Bechhofer, van Harmelen, Hendler, Horrocks, McGuinness, PatelSchneider, & Stein, 2004), and its successor OWL 2 (W3C OWL Working Group, 2009).
There are three species of OWL 1: OWL Lite, OWL DL, and OWL Full. OWL 2 extends
OWL 1 and adds three further sublanguages (called OWL 2 profiles): OWL EL, OWL QL,
and OWL RL. OWL Lite corresponds to the DL SHIF in which the standard reasoning
tasks are ExpTime-complete, OWL 1 DL corresponds to the DL SHOIN , in which the
standard reasoning tasks are NExpTime-complete, and OWL 2 DL extends this to the DL
SROIQ. For OWL Full the standard reasoning tasks are no longer decidable. The new
QL, EL, and RL profiles are more restrictive than OWL DL and each of the profiles trades
off different aspects of OWLs expressive power in return for different computational and/or
implementational benefits. OWL EL corresponds to the DL EL ++ (Baader, Brandt, &
Lutz, 2005) and the basic reasoning problems can be performed in time that is polynomial
with respect to the size of the input knowledge base. OWL 2 QL is based on the DL-Lite
family of Description Logics, where the data complexity of conjunctive query entailment is
in AC0 . Thus, conjunctive query answering can be implemented using standard relational
database technology. OWL 2 RL enables the implementation of polynomial time reasoning
algorithms using rule-extended database technologies.
In data-intensive applications, querying KBs plays a central role. Instance retrieval
is, in some aspects, a rather weak form of querying: although possibly complex concept
expressions are used as queries, we can only query for tree-like relational structures, as
a DL concept cannot express arbitrary cyclic structures. This property is known as the
tree model property and is considered an important reason for the decidability of most
430

fiNominals, Inverses, Counting, and Conjunctive Queries

Modal and Description Logics (Gradel, 2001; Vardi, 1997) and we also heavily exploit a
variant of this property to establish our decidability result. Conjunctive queries (CQs)
and unions of conjunctive queries (UCQs) are well known in the database community and
constitute an expressive query language with capabilities that go well beyond standard
instance retrieval. In FOL terms, CQs and UCQs are formulae from the positive existential
fragment. Free variables in a query (not bound by an existential quantifier) are also called
answer variables or distinguished variables, whereas existentially quantified variables are
called non-distinguished.
If the query contains no distinguished variables, the query answer is just true or false
and the query is called a Boolean query. Given a knowledge base K and a Boolean UCQ
q, the query entailment problem is deciding whether q is true or false w.r.t. K, i.e., we
have to decide whether each model of K provides for a suitable assignment for the variables
in q. For a query with distinguished variables, the answers to the query are those tuples
of individual names (constants) for which the knowledge base entails the query that is
obtained by replacing the free variables with the individual names in the answer tuple.
These answers are also called certain answers. The problem of finding all answer tuples is
known as query answering. We present a decidability result for query entailment, which is
a decision problem, but this is no restriction since query answering can easily be reduced
to query entailment as we illustrate in more detail in Section 3.
1.1 Related Work
Conjunctive queries have been first mentioned in the context of Description Logics (DLs) by
Levy and Rousset (1996). The first account of conjunctive queries as main topic is given by
Calvanese, De Giacomo, and Lenzerini (1998a). In particular in recent years, the problem of
decidability of conjunctive query entailment and the complexity of the problem in different
logics has gained significant attention. For the DLs SHIQ and SHOQ decidability and
2-ExpTime-completeness of the problem is known (Glimm, Horrocks, Lutz, & Sattler,
2008a; Glimm, Horrocks, & Sattler, 2008b; Lutz, 2008; Eiter, Lutz, Ortiz, & Simkus,
2009). Conjunctive query entailment is already 2-ExpTime-hard in the relatively weak
DL ALCI (Lutz, 2008), which was initially attributed to inverse roles. Recently, it was
shown, however, that also transitive roles together with role hierarchies as in the DL SH
make conjunctive query entailment 2-ExpTime-hard (Eiter et al., 2009). The techniques by
Glimm et al. for SHIQ and SHOQ (Glimm et al., 2008a, 2008b) reduce query entailment
to the standard reasoning task of knowledge base satisfiability checking in the DL extended
with role conjunctions. An alternative technique is the so-called knots technique (Ortiz,
Simkus, & Eiter, 2008b), which is an instance of the mosaic technique originating in Modal
Logic. This technique also gives worst-case optimal algorithms for SHIQ and several of its
sub-logics. Further, there are automata-based decision procedures for positive existential
path queries (Calvanese, Eiter, & Ortiz, 2007, 2009). Positive existential path queries
generalize unions of conjunctive queries and, therefore, decision procedures for this kind of
query also provides decision procedures for unions of conjunctive queries. In particular the
most recent extension (Calvanese et al., 2009) is very close to a conjunctive query entailment
decision procedure for OWL 2, which corresponds to the DL SROIQ, because it covers
431

fiRudolph & Glimm

SRIQ, SROQ, and SROI. The use of the three problematic constructors for nominals,
inverses, and number restrictions is, however, not covered.
Regarding data complexity, i.e., the complexity with respect to the ABox (the data)
only, CQ entailment is usually coNP-complete for expressive logics. For example, for DLs
from ALE up to SHIQ this is the case (Glimm et al., 2008a) and this holds also for CQ
entailment in the two variable guarded fragment with counting (Pratt-Hartmann, 2009).
The latter work is quite closely related since many Description Logics can be translated into
the two variable guarded fragment with counting, i.e., the results of Pratt-Hartmann also
hold for SHIQ with only simple roles (roles that are not transitive and have no transitive
subrole) in the query. Given the same restriction on the query, also SHOQ and SHOI
were shown to have coNP-complete data complexity w.r.t. conjunctive query entailment
(Ortiz, Calvanese, & Eiter, 2008a).
Query entailment and answering have also been studied in the context of databases
with incomplete information (Rosati, 2006b; van der Meyden, 1998; Grahne, 1991). In this
setting, DLs can be used as schema languages, but the expressivity of the considered DLs is
usually much lower than the expressivity of the DL ALCHOIQb that we consider here and
reasoning in them is usually tractable. For example, the constructors provided by logics of
the DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007) are chosen
such that the standard reasoning tasks are in PTime regarding combined complexity and
query entailment is in AC0 with respect to data complexity. Thus, TBox reasoning can be
done independently of the ABox and the ABox can be stored and accessed using a standard
database SQL engine. Another tractable DL is EL (Baader, 2003). Conjunctive query
entailment in EL is, however, not tractable as the complexity increases to coNP-complete
(Rosati, 2007b). Moreover for EL++ (Baader et al., 2005), a still tractable extension of EL,
query entailment is even undecidable (Krotzsch, Rudolph, & Hitzler, 2007). This is mainly
because in EL++ , one can use unrestricted role compositions. This allows for encoding
context-free languages, and conjunctive queries can then be used to check the intersection
of such languages, which is known to be an undecidable problem. Since the logics used in
databases with incomplete information are considerable less expressive than ALCHOIQb,
the techniques developed in that area do not transfer to our setting.
Given that query entailment is a (computationally) harder task than, for example,
knowledge base satisfiability, it is not very surprising that decidability of the latter task
does not necessarily transfer to the problem of CQ entailment. Most of the undecidability results can be transferred from FOL since many DLs can directly be translated into
an equivalent FOL theory. For example, it is known that conjunctive query entailment
is undecidable in the two variable fragment of First-Order Logic L2 (Rosati, 2007a), and
Rosati identifies a relatively small set of constructors that cause the undecidability (most
notably role negation axioms, i.e., axioms of the form x, y (R(x, y)  P (x, y)) for R, P
binary predicates). Pratt-Hartmann (2009) recently established decidability for CQ entailment in the two variable guarded fragment with counting (GC2 ). It is worth noting that
Pratt-Hartmann assumes that the background theory (that is the knowledge base in our
case) is constant free and formulae of the form =1 x(P (x)), which can be used to simulate constants/nominals, are not considered guarded. His result covers, therefore, only
the DL ALCHIQb and is not applicable to the case, when the input knowledge base (the
background theory) contains nominals (individual constants).
432

fiNominals, Inverses, Counting, and Conjunctive Queries

Most of the implemented DL reasoners, e.g., KAON2,1 Pellet, and RacerPro,2 provide
an interface for conjunctive query answering, although KAON2 and RacerPro consider only
named individuals in the ABox for the assignments of variables. Under that restriction
queries do no longer have the standard FOL semantics and decidability is obviously not an
issue since conjunctive query answering with this restriction can be reduced to standard
instance retrieval by replacing the variables with individual names from the ABox and then
testing entailment of each conjunct separately. Pellet goes beyond that and also provides an
interface for conjunctive queries with FOL semantics under the restriction that the queries
have a kind of tree shape. Under this restriction decidability is known since CQs can then
be expressed as normal concepts (possibly by adding role conjunctions).
1.2 Contributions and Overview
Given all these results, which show a great interest in the problem of conjunctive query entailment over expressive DLs, it is very interesting that for the DLs SHIF, SHOIN , and
SROIQ that underpin the widely adopted standards OWL Lite, OWL 1 DL, and OWL 2
DL, respectively, decidability of conjunctive query entailment has only been established for
OWL Lite. The main obstacle in devising a decision procedure is the combination of inverse
roles (I), nominals (O), and number restrictions/counting quantifiers (F stands for functionality, N for unqualified number restrictions, and Q for qualified number restrictions).
The complications arising from the combination of these constructors caused also a major
hurdle in the development of implementable algorithms for knowledge base satisfiability in
SHOIN and extensions thereof, but Horrocks and Sattler (2005) devised a tableau-based
decision procedure that has since been extended to SROIQ. Meanwhile also alternative
approaches such as resolution (Kazakov & Motik, 2008), and hypertableau-based procedures
(Motik et al., 2009) are available and implemented.
The key obstacle in establishing a decision procedure is the existence of potentially
infinitely many new nominals, i.e., elements that are uniquely identifiable in any model of
a KB. For an example, consider the KB K given in Fig. 1. A concept of the form {o}
has to be interpreted as a singleton set, containing only the interpretation of the constant
o. For simplicity, we assume for now that a constant is always interpreted as itself, e.g.,
the interpretation of o is o. An axiom of the form {o1 } v f.s.f  .{o2 } can then be
understood as follows: For the constant o1 , there must be two elements, say d1 and d2 , such
that f (o1 , d1 ), s(d1 , d2 ), and f (o2 , d2 ) holds. Note that o2 occurs as the first element in
f (o2 , d2 ) since an inverse role (f  ) is used. Thus, an interpretation for the KB must contain
the three elements o1 , o2 , and o3 , which must be interconnected in the following way: paths
f

s

f

of the shape      have to lead from o1 to o2 as well as from o2 to o3 and from o3 to
o1 . Moreover, the role f is defined to be functional, meaning that every element can have
at most one f -successor. This also applies to all individuals oi , which forces the existence of
an s-cycle. Observe that a cyclic Boolean query such as {s(x, y), s(y, z), s(z, x)} that checks
for the existence of such a cycle cannot be answered by applying standard techniques such
as replacing variables with individual names (oi ) or rewriting the query into an equivalent
1. http://kaon2.semanticweb.org
2. http://www.racer-systems.com

433

fiRudolph & Glimm

{o1 } v f.s.f  .{o2 }
{o2 } v

f.s.f  .{o

{o1 }

3}

f

f

s
s

{o3 } v f.s.f  .{o1 }

{o2 }

s
f

{o3 }

func(f )

Figure 1: Example knowledge base K and a representation for a model, where the three
elements in the s-cycle are so-called new nominals.

tree-shaped query. The elements in the cycle behave as if they were nominals, but we do
not have names for them.
We tackle the problem of conjunctive query entailment in a very expressive DL that contains all the three problematic constructors simultaneously and prove decidability of (unions
of) conjunctive queries. The most challenging part is to establish finite representability of
countermodels in case the query given as input is not entailed by the knowledge base. Our
results also hold for SHOIQ knowledge bases, i.e., with some roles declared as transitive,
provided that the queries contain only simple roles (roles that are neither transitive nor
have a transitive subrole). This is essentially the same restriction that is placed on roles
that can occur in number restrictions since otherwise the standard reasoning tasks become
undecidable. Under this restriction, we can use standard techniques for eliminating transitivity (Kazakov & Motik, 2008). Hence, we also show decidability of conjunctive query
entailment in OWL DL, for queries with only simple roles.
We believe that our work is also valuable for understanding, in general, the structure of
models in DLs that contain nominals, inverse roles, and number restrictions. Furthermore,
we devise non-trivial extensions of standard techniques such as unraveling, which we believe
will prove useful when working with such expressive DLs.
The paper is organized as follows: in Section 2, we give a birds-eye view of the techniques
and ideas used to establish decidability. In Section 3, we give the necessary definitions and
introduce standard notations. In Sections 4, 5, and 6 we present the main results that we
then use in Section 7 to show how models that do not satisfy the query can be finitely
represented before we conclude in Section 8.

2. The Big Picture
Before going into the technical details, we will describe our overall line of argumentation
establishing decidability of conjunctive query entailment in ALCHOIQb.
2.1 Decidability via Finitely Representable Countermodels
Let K be an ALCHOIQb knowledge base and let q be the conjunctive query in question,
i.e., we aim to determine whether
K |= q.
Clearly, as ALCHOIQb is a fragment of first-order predicate logic with equality, K can be
translated into a FOL sentence F OL(K). Likewise we find a FOL sentence F OL(q) for
434

fiNominals, Inverses, Counting, and Conjunctive Queries

q being just an existentially quantified formula. Hence, checking the above entailment is
equivalent to determining whether the first-order theory F OL(K) entails F OL(q). As a
result of the completeness theorem for FOL (Godel, 1929), the consequences of a finite FOL
theory are recursively enumerable, which provides us with a procedure that terminates if
K |= q. Hence, we can establish decidability by providing another algorithm that terminates
iff the entailment above does not hold  i.e., if there is a so-called countermodel being a
model I of K for which I 6|= q.
We will provide such an algorithm by showing that, whenever such a countermodel I
exists at all, there is also a countermodel I that is finitely representable. More precisely,
I can be encoded into a word Rep(I) of finite length over a finite alphabet, whereby the
encoding Rep has the property that for every such finite word it can be effectively checked
whether it represents a countermodel for a given knowledge base and query.
As a consequence thereof, we can create the desired algorithm that enumerates all words,
checks each for being a countermodel, and terminates as soon as it has found one.
2.2 Finite Representability by Bounding Nominals and Blocking
We now outline how we are going to show that there is always a finitely representable
countermodel, if there is one at all. We do this by taking an arbitrary countermodel and
cautiously transforming it into a countermodel that is finitely representable. Cautiously
means that we have to make sure that the transformation does preserve the two properties
of 1) being a model of the underlying knowledge base K and 2) not entailing the considered
query q.
The result of the overall transformation is going to be a regular model, i.e., a structure
where substructures are being in a certain sense periodically repeated. It is common practice
in DL theory to construct this kind of models from arbitrary ones by blocking techniques,
whereby certain element configurations occurring twice in the original model are detected
and the new model is generated by infinitely stringing together the same finite substructure
that is delimited by those two configurations.
In the case we consider, this technique cannot be applied directly to the original countermodel. This is due to an intricate interplay of nominals, inverse roles and cardinality
constraints by which an arbitrary  even an infinite  number of domain elements can be
forced to behave like nominals; this is why those elements are usually referred to as new
nominals in a DL setting. In FOL, nominals are often called kings and the new nominals
are called the court. In our case, the presence of infinitely many new nominals in the model
may prevent the existence of repeated configurations needed for blocking.
We overcome this difficulty by first applying a transformation by means of which the
original countermodel is converted into a countermodel with only finitely many new nominals. This guarantees that the subsequent blocking-based transformation is applicable and
will yield the desired regular (and thus finitely representable) model.
2.3 Bounding Nominals by Transformations of Forest Quasi-Models
For our argumentation, we introduce the notion of forest quasi-models. These are structures
not satisfying the originally considered knowledge base but a weakened form of it. In
435

fiRudolph & Glimm

return to this concession, they exhibit a proper forest structure that is easier to handle and
manipulate.
We employ two techniques to turn proper models into forest quasi-models and vice
versa: a model can be unraveled yielding a forest quasi-model. A forest quasi-model can be
collapsed to obtain a proper model. Both techniques preserve certain structural properties.
Our strategy to construct a countermodel with finitely many nominals consists of the
following three steps:
 Take an arbitrary countermodel and unravel it.
 Transform the obtained forest quasi-model by substituting critical parts by wellbehaved ones,
 Collapse the obtained structure into a (proper) model.
The mentioned critical parts are those giving rise to new nominals. They have to
be  at least largely  avoided (we do not care about a finite set of those critical parts
remaining).
The central question is: where do these mysterious well-behaved substitutes come from?
Fortunately, the plethora of critical parts brings about its own remedy. We can use infinite
sets of critical parts to construct well-behaved ones in an infinite approximation process
(this is why infinity is your friend). We thereby obtain parts which have not been present
in our structure before, but are well compatible with it and can hence be used for its
reorganization.
After having informally introduced our main line of argumentation, we now move on to
the technical details.

3. Preliminaries
We first define the syntax and semantics of roles, and then go on to SHOIQb-concepts,
individuals, and knowledge bases. We do not actually use the full expressivity of SHOIQb,
but it is a convenient umbrella for all DLs we are working with and we can define less
expressive DLs of interest as restrictions of SHOIQb.
Definition 1 (Syntax of SHOIQb). Let NC , NR , and NI be countable, infinite, and
pairwise disjoint sets of concept names, role names, and individual names, respectively. We
call S = (NC , NR , NI ) a signature. The set rol(S) of SHOIQb-roles over S (or roles for
short) is NR  {r | r  NR }, where roles of the form r are called inverse roles. A role
inclusion axiom is of the form r v s with r, s roles. A transitivity axiom is of the form
trans(r) for r a role. A role hierarchy H is a finite set of role inclusion and transitivity
axioms.
For a role hierarchy H, we define the function inv over roles as inv(r) := r if r  NR and
inv(r) := s if r = s for a role name s  NR . Further, we define vH as the smallest transitive
reflexive relation on roles such that r v s  H implies r vH s and inv(r) vH inv(s). We
write r H s if r vH s and s vH r. A role r is transitive w.r.t. H (notation r+ vH r) if a
436

fiNominals, Inverses, Counting, and Conjunctive Queries

role s exists such that r vH s, s vH r, and trans(s)  H or trans(inv(s))  H. A role s is
called simple w.r.t. H if there is no role r such that r is transitive w.r.t. H and r vH s.
For r  rol(S) a simple role, a Boolean role expressions U is defined as follows:
U ::= r | U | U u U | U t U.
We use ` to denote standard Boolean entailment between a set of roles R  rol(S) and role
expressions. Let r  rol(S), and U a Boolean role expression over R. We inductively define:
 R ` r if r  R, and R 6` r otherwise,
 R ` U if R 6` U , and R 6` U otherwise,
 R ` U u V if R ` U and R ` V , and R 6` U u V otherwise,
 R ` U t V if R ` U or R ` V , and R 6` U t V otherwise.
A Boolean role expression U is safe if  6` U .
Given a signature S = (NC , NR , NI ), the set of SHOIQb-concepts (or concepts for
short) over S is the smallest set built inductively over symbols from S using the following
grammar, where o  NI , A  NC , n  IN0 , s is a simple role, and U is a role or a safe
Boolean role expression:
C ::= > |  | {o} | A | C | C1 u C2 | C1 t C2 |
4
U.C | U.C | 6 n s.C | > n s.C.
Alternatively, safeness can be characterized as follows: a Boolean role expression U is
safe if, after transforming it into disjunctive normal form, each disjunct contains at least
one non-negated role. Intuitively, this implies that a safe role expression can never relate
individuals that are not in a direct role relation with each other.
Definition 2 (Semantics of SHOIQb-concepts). An interpretation I = (I , I ) consists
of a non-empty set I , the domain of I, and a function I , which maps every concept name
A  NC to a subset AI  I , every role name r  NR to a binary relation rI  I  I ,
and every individual name a  NI to an element aI  I . For each role name r  NR ,
I
the interpretation of its inverse role (r ) consists of all pairs h,  0 i  I  I for which
h 0 , i  rI .
The semantics of SHOIQb-concepts over a signature S is defined as follows:
(r)I
>I
(C)I
(U.C)I
(U.C)I
(6 n s.C)I
(> n s.C)I

=
=
=
=
=
=
=

I  I \ rI
(r1 u r2 )I = r1I  r2I
(r1 t r2 )I = r1I  r2I
I
I

 =
({o})I = {oI }
I \ C I
(C u D)I = C I  DI
(C t D)I = C I  DI
I
0
I
0
I
{   | if h,  i  U , then   C }
{  I | there is a h,  0 i  U I with  0  C I }
{  I | ](sI (, C))  n}
{  I | ](sI (, C))  n}

where ](M ) denotes the cardinality of the set M and sI (, C) is defined as
{ 0  I | h,  0 i  sI and  0  C I }.
A concept C is in negation normal form (NNF) if negation occurs only in front of concept
names and we use nnf(C) to denote the negation normal form of a concept C.
4
437

fiRudolph & Glimm

Any concept can be transformed in linear time into an equivalent one in NNF by pushing
negation inwards, making use of de Morgans laws and the duality between existential and
universal restrictions, and between at-most and at-least number restrictions of the form
6 n r.C and > n r.C respectively (Horrocks, Sattler, & Tobies, 2000).
Definition 3 (Syntax and Semantics of Axioms and Knowledge Bases). A functionality
restriction is an expression func(f ) for f a role. For C, D concepts, a general concept
inclusion (GCI) is an expression C v D. We introduce C  D as an abbreviation for
C v D and D v C. A finite set of GCIs and functionality restrictions is called a TBox. An
.
.
(ABox) assertion is an expression of the form C(a), r(a, b), r(a, b), a = b, or a =
6 b, where
C is a concept, r is a role, and a, b  NI are individual names. An ABox is a finite set of
assertions. A knowledge base K is a triple (T , H, A) with T a TBox, H a role hierarchy,
and A an ABox.
We use con(K), rol(K), and nom(K) to denote, respectively, the set of concept names,
roles (including inverses), and individual names occurring in K. The closure cl(K) of K
is the smallest set containing nnf(C t D) if C v D  T ; D if D is a sub-concept of C
and C  cl(K); and nnf(C) if C  cl(K). A role f is functional in K if K contains the
functionality axiom func(f ) and it is inverse functional in K if K contains the functionality
axiom func(inv(f )).
Let I = (I , I ) be an interpretation. Then I satisfies a role inclusion axiom r v s if
rI  sI , I satisfies a transitivity axiom trans(r) if rI is a transitive binary relation, and a role
hierarchy H if it satisfies all role inclusion and transitivity axioms in H. The interpretation
I satisfies a functionality restriction func(f ) if, for each   I , ]({ 0 | h,  0 i  f I })  1; I
satisfies a GCI C v D if C I  DI ; and I satisfies a TBox T if it satisfies each functionality
restriction and each GCI in T . The interpretation I satisfies an assertion C(a) if aI  C I ,
.
.
r(a, b) if haI , bI i  rI , r(a, b) if haI , bI i 
/ rI , a = b if aI = bI , and a =
6 b if aI 6= bI ; I
satisfies an ABox if it satisfies each assertion in A. We say that I satisfies K if I satisfies
T , H, and A. In this case, we say that I is a model of K and write I |= K. We say that K
is consistent if K has a model.
4
If the knowledge base K is clear from the context, we simply say that a role f is (inverse)
functional instead of saying f is (inverse) functional in K.
The names of DLs indicate which constructors are supported. The basic DL ALC
supports Boolean concept constructors and GCIs, but no role hierarchies, functionality
restrictions et cetera. If transitivity axioms are added, we use S instead of ALC. Inverse
roles are indicated by the letter I, role inclusion axioms by H, nominals, i.e., concepts of
the form {o} for o  NI , by O, functionality restrictions by F, qualified number restrictions,
i.e., concepts of the form 6 n s.C and > n s.C, by Q, and safe Boolean role expressions by
b. If number restrictions are limited to concepts of the form 6 n s.> and > n s.>, we use
the letter N .
We mostly refer to a few particular DLs in this paper: the DL SHOIQ is obtained from
SHOIQb by disallowing Boolean role expressions. The DLs SHIQ, SHOQ, and SHOI
are obtained from SHOIQ by disallowing nominals, inverse roles, and number restrictions
(incl. functionality restrictions), respectively. Finally, the DL ALCOIFb is obtained from
SHOIQb by disallowing transitivity axioms (we use ALC instead of S in the name of the
DL to indicate this), role inclusion axioms, and concepts of the form 6 n s.C and > n s.C.
438

fiNominals, Inverses, Counting, and Conjunctive Queries

3.1 Conjunctive Queries and Unions of Conjunctive Queries
We now introduce Boolean conjunctive queries since they are the basic form of queries we
are concerned with. We later also define non-Boolean queries and show how they can be
reduced to Boolean queries. Finally, unions of conjunctive queries are just a disjunction of
conjunctive queries.
Definition 4 (Syntax and Semantics of Conjunctive Queries). Let S = (NC , NR , NI ) be
a signature and NV a countably infinite set of variables disjoint from NC , NR , and NI .
A term t is an element from NV  NI . Let A  NC be a concept name, r  NR a role
name, and t, t0 terms. An atom is an expression A(t) or r(t, t0 ) and we refer to these two
types of atoms as concept atoms and role atoms respectively. A Boolean conjunctive query
q is a non-empty set of atoms. We use var(q) to denote the set of (existentially quantified)
variables occurring in q and term(q) to denote the set of variables and individual names
occurring in q. As usual, we use ](q) to denote the cardinality of q, which is simply the
number of atoms in q, and we use |q| for the size of q, i.e., the number of symbols necessary
to write q.
Let I = (I , I ) be an interpretation. A total function  : term(q)  I is an evaluation
if (a) = aI for each individual name a occurring in q. For A(t), r(t, t0 ) atoms, we write
 I |= A(t) if (t)  AI ;
 I |= r(t, t0 ) if ((t), (t0 ))  rI .
If, for an evaluation , I |= At for all atoms At  q, we write I |= q. We say that I
satisfies q and write I |= q if there exists an evaluation  such that I |= q. We call such a
 a match for q in I.
Let K be a knowledge base and q a conjunctive query. If I |= K implies I |= q, we say
that K entails q and write K |= q.
4
The query entailment problem is defined as follows: given a knowledge base K and a
query q, decide whether K |= q.
Definition 5 (Unions of Conjunctive Queries). A union of Boolean conjunctive queries is
a formula q1  . . .  qn , where each disjunct qi is a Boolean conjunctive query.
A knowledge base K entails a union of Boolean conjunctive queries q1  . . .  qn , written
as K |= q1  . . .  qn , if, for each interpretation I such that I |= K, there is some i such that
I |= qi and 1  i  n.
4
We now clarify the connection between query entailment and query answering. For
query answering, let the variables of a conjunctive query be typed: each variable can either
be existentially quantified (also called non-distinguished ) or free (also called distinguished
or answer variables). Let q be a query in n variables (i.e., ](var(q)) = n), of which v1 , . . . , vm
(m  n) are answer variables. The answers of K to q are those m-tuples (a1 , . . . , am ) of
individual names such that, for all models I of K, I |= q for some  that satisfies (vi ) = aIi
for all i with 1  i  m. Recall that we use nom(K) to denote the set of individual names
occurring in K (in the form of nominals or ABox individuals). It is not hard to see (cf.
Chandra & Merlin, 1977) that the answers of K to q can be computed by testing, for each
439

fiRudolph & Glimm

(a1 , . . . , am )  nom(K)m , whether the query q[v1 ,...,vm /a1 ,...,am ] obtained from q by replacing
each occurrence of vi with ai for 1  i  m is entailed by K. The set of certain answers
to q is then the set of all m-tuples (a1 , . . . , am ) for which K |= q[v1 ,...,vm /a1 ,...,am ] . Let
k = ](nom(K)) be the number of individual names occurring in K. Since K is finite, clearly
k is finite. Hence, deciding which tuples belong to the set of answers can be checked with
at most k m entailment tests.
The algorithm that we present in this paper decides query entailment. The reasons
for devising a decision procedure for query entailment instead of query answering are twofold: first, query answering can be reduced to query entailment as shown above; second, in
contrast to query answering, query entailment is a decision problem and can be studied in
terms of complexity theory.
3.2 Simplifying Assumptions
In the following, we make several assumptions that are without loss of generality, but
simplify the presentation of the decision procedure.
3.2.1 From SHOIQ and ALCHOIQb to simplified ALCOIFb Knowledge Bases
In the following, we only work with ALCOIFb knowledge bases. Nevertheless, our results
also hold for SHOIQ knowledge bases and queries with only simple roles in the query and
for ALCHOIQb knowledge bases, i.e., when the knowledge base contains safe Boolean role
expressions, but no transitivity. The restriction to ALCOIFb is without loss of generality,
as we show now.
Provided the query contains only simple roles, we can use the elimination techniques for
transitivity (Kazakov & Motik, 2008) to reduce a SHOIQ knowledge base to an ALCHOIQ
knowledge base with extended signature. We can further eliminate qualified number restrictions and role inclusion axioms by transforming an ALCHOIQb knowledge base into an
ALCOIFb knowledge base that is equivalent to the original one up to an extension of the
signature (Rudolph, Krotzsch, & Hitzler, 2008). We do not repeat a formal proof here, but
rather give an informal argument as to how this reduction works.
We assume that the knowledge base is in negation normal form, i.e., all GCIs are of
the form > v C with C a concept in NNF. Now, consider a concept expression of the form
> n r.C with r a role and C a concept. This means that there are at least n distinct rneighbors satisfying C. However, this situation can be enforced by introducing n new roles
r1 , . . . , rn each of which is deemed to have r as a superrole (ri v r) and which are pairwise
disjoint (> v (ri u rj ).). Under those side conditions, the above concept expression
can be replaced by r1 .C u . . . u rn .C.
A somewhat dual argumentation is possible for concept expressions of the form 6 n r.C
restricting the number of r-neighbors satisfying C to at most n. Again we extend the
signature by introducing new roles r1 , . . . , rn , but this time, we let them cover all outgoing
r-links in the following sense: whenever an r-link leads to some domain element  which
satisfies C, then one of the roles r1 , . . . , rn also leads there. Indeed, safe Boolean role
expressions allow for expressing this correspondence via the concept description (r u r1 u
. . . u rn ).C. It is now easy to see, that this concept expression can replace the above if
we additionally demand all roles r1 , . . . , rn to be functional.
440

fiNominals, Inverses, Counting, and Conjunctive Queries

{o} v r.A

A v r.A

A v s.B

func(f  )

func(g  )

B vC tD

C v f.E

D v g.E

E v B t {o}

r
{o}
E

A

r

s
f

B
C E

A

r

s
g

B
DE

A

r

s
f

B
C E

A

r

s
g

B
DE

A r



s
f

B
C E

g



Figure 2: Knowledge base for our running example and a representation of a model for the
knowledge base.

Finally consider a role hierarchy statement r v s, stating that whenever two domain
elements 1 and 2 are connected by role r, they are also interconnected via s. Clearly, this
statement can be reformulated as: there are no two domain elements connected by r and
by s. This, in turn, can be equivalently rephrased by saying that no domain element has
an r u s-neighbor or, expressed as GCI, > v (r u s)..
These transformations can be applied to an ALCHOIQb knowledge base, whereby all
cardinality constraints and role inclusion axioms are eliminated. This leaves us with an
equivalent ALCOIFb knowledge base up to an extension of the signature.
Figure 2 displays an ALCOIFb knowledge base and an according model, which we will
refer to as a running example throughout the paper.
Furthermore, we assume that the ABox is internalized (e.g., C(a) is replaced by the
equivalent GCI {a} v C, r(a, b) by {a} v r.{b}, etc.). Thus, we effectively decide query
entailment with respect to a TBox only since knowledge bases in this setting have an empty
ABox.
For T an ALCOIFb TBox, it is always possible to transform T into an equivalent TBox
T 0 up to signature extension such that all GCIs in T 0 have one of the following simplified
forms:
l
G
Ai v
Bj | A  {o} | A v U.B | A v U.B | func(f ),
(1)
where A(i) and B(j) are concept names, o is andindividual name, U is a safe Boolean
F role
expression, and f is a role. If i = 0, we interpret Ai as > and if j = 0, we interpret Bj as
. An ALCOIFb knowledge base K = (T , A) is simplified if T is simplified and A is empty.
Every ALCOIFb knowledge base, which is not in this form, can be transformed in polynomial time into the desired form by using the standard structural transformation, which
iteratively introduces definitions for compound sub-concepts (Kazakov & Motik, 2008).
Thus, we assume in the remainder that any knowledge base is rewritten into a simplified
ALCOIFb knowledge base.
441

fiRudolph & Glimm

3.2.2 Connected and Constant-free Queries
We assume that queries are connected. More precisely, let q be a conjunctive query. We
say that q is connected if, for all t, t0  term(q), there exists a sequence t1 , . . . , tn such that
t1 = t, tn = t0 and, for all 1  i < n, there exists a role name r such that r(ti , ti+1 )  q or
r(ti+1 , ti )  q. A collection q1 , . . . , qn of queries is a partitioning of q if q = q1  . . .  qn ,
term(qi )  term(qj ) =  for 1  i < j  n, and each qi is connected.
Lemma 6. Let K be a knowledge base, q a conjunctive query, and q1 , . . . , qn a partitioning
of q. Then K |= q iff K |= qi for each i with 1  i  n.
A proof is given by Tessaris (2001) and, with this lemma, it is clear that the restriction
to connected queries is indeed without loss of generality since entailment of q can be decided
by checking entailment of each qi at a time. In what follows, we therefore assume queries
to be connected without further notice.
In unions of conjunctive queries, we assume that the variable names in each disjunct are
different from the variable names in the other disjuncts. This can always be achieved by
naming variables apart. We further assume that each disjunct in a UCQ is a connected conjunctive query. This is without loss of generality since a UCQ which contains unconnected
disjuncts can always be transformed into conjunctive normal form; we can then decide entailment for each resulting conjunct separately and each conjunct is a union of connected
conjunctive queries (Glimm et al., 2008a). Note that, due to the transformation into conjunctive normal form, the resulting number of unions of connected conjunctive queries for
which we have to test entailment can be exponential in the size of the original query.
We further assume that queries do not contain constants (individual names) to occur in
the position of variables. In the presence of nominals this is without loss of generality: for
each individual name a occurring in q, we extend the knowledge base K with the axioms
{a}  Na for Na  NC a fresh concept name, and replace each occurrence of a in q with a
fresh variable xa  NV and add a concept atom Na (xa ) to q.
3.2.3 General Notation
Throughout this paper, concept names and role expressions are written in upper case, while
roles and individual names are written in lower case. Unless stated otherwise, we use A
and B for concept names; C and D for possibly complex concepts; r and s for roles, f
for functional or inverse functional roles; U and V for safe Boolean role expressions; and o
for nominals that are used in TBox axioms or that occur in complex concepts. Sub- and
superscripts might be appended if necessary. If not stated otherwise, we use q (possibly
with subscripts) for a connected Boolean conjunctive query, K for a simplified ALCOIFb
knowledge base, I for an interpretation (I , I ), and ,  for evaluations.

4. Model Construction
In this section, we introduce interpretations and models that have a kind of forest shape.
The main notion of a forest is, however, very weak since we do also allow for arbitrary
relations between tree elements and roots. Without such relations, we call the result a
strict forest. We exploit the nice properties of trees and forests in the following sections,
442

fiNominals, Inverses, Counting, and Conjunctive Queries

when we replace parts in interpretations that give rise to an infinite number of new nominals.
Since even models of an ALCOIFb knowledge base that have a kind of forest shape are not
really forests, we also introduce approximations of models in which nominals are no longer
interpreted as singleton sets. We call these structures quasi-interpretations or quasi-models
and such interpretations can have the form of real forests. Further, we provide a way of
unraveling an arbitrary model into a forest that is a quasi-model for the knowledge base
and a way of collapsing such forest quasi-models back into real models of the knowledge
base that still have a kind of forest shape.
Definition 7 (Forest (Quasi-)Interpretations and (Quasi-)Models). A tree T is a nonempty, prefix-closed subset of IN . For w, w0  T , we call w0 a successor of w if w0 = w  c
for some c  IN, where  denotes concatenation. We call w0 a predecessor of w if w = w0  c
for some c  IN, and w0 is a neighbor of w if w0 is a successor of w or vice versa. The empty
word  is called the root of the tree. We use |w| to denote the length of w.
A forest F is a subset of R IN , where R is a countable, possibly infinite set of elements
such that, for each   R, the set {w | (, w)  F } is a tree. Each pair (, )  F is called
a root of F . For (, w), (0 , w0 )  F , we call (0 , w0 ) a successor of (, w) if 0 =  and w0
is a successor of w; (0 , w0 ) is a predecessor of (, w) if 0 =  and w0 is a predecessor of w;
(0 , w0 ) is a neighbor of (, w) if (0 , w0 ) is a successor of (, w) or vice versa. A node (, w)
is an ancestor of a node (0 , w0 ) if  = 0 and w is a prefix of w0 and it is a descendant if
 = 0 and w0 is a prefix of w.
A forest interpretation of a knowledge base K is an interpretation I = (I , I ) that
satisfies the following conditions:
FI1 I is a forest with roots R;
FI2 there is a total and surjective function  : nom(K)  R  {} such that (o) = (, )
iff oI = (, );
FI3 for each role r  rol(K), if h(, w), (0 , w0 )i  rI , then either
(a) w =  or w0 = , or
(b) (, w) is a neighbor of (0 , w0 ).
If I |= K, we say that I is a forest model for K. If I has a single root, we call I a tree
interpretation and a tree model for K, respectively.
Let K be an ALCOIFb knowledge base. With nomFree(K), we denote the ALCIFb
knowledge base obtained from K by replacing each nominal concept {o} with o  nom(K)
with a fresh concept name No . A forest quasi-interpretation for K is an interpretation
J = (J , J ) of nomFree(K) that satisfies the following properties:
FQ1 J is a forest with roots R;
FQ2 there is a total and surjective function  : nom(K)  R  {} such that (o) = (, )
iff (, )  NoJ
FQ3 for each role r  rol(K), if h(, w), (0 , w0 )i  rI , then either
(a) w =  or w0 = , or
443

fiRudolph & Glimm

(b) (, w) is a neighbor of (0 , w0 ).
Note that condition FQ2 allows for elements (, w)  J with w 6=  such that (, w)  NoJ .
We call J strict if in condition FQ3, only FQ3(b) is allowed. If J |= nomFree(K) we say
that J is a forest quasi-model for K.
The branching degree d(w) of a node w in a tree T is the number of successors of w. Let
I = (I , I ) be a forest (quasi) interpretation for K. If there is a k such that d(w)  k for
each (, w)  I , then we say that I has branching degree k.
4
In the remainder, when we use the concept name No , we mean the fresh concept name
that was introduced in nomFree(K) for the nominal concept {o} with o  nom(K). Elements
in the extension of a concept No are called nominal placeholders. Please note that, in a
forest quasi-interpretations J , we can have several elements (, w) with w 6=  such that
(, w)  NoJ .
In the following, we define a notion of isomorphism between forest interpretations. Note
that we demand not only structural identity w.r.t. concepts and roles but also w.r.t. the
successor relation.
Definition 8 (Isomorphism between Forest Interpretations). Let I, I 0 be two forest inter0
pretations of K with 1 , 2  I , 10 , 20  I . The pairs h1 , 2 i, h10 , 20 i are isomorphic
w.r.t. K, written h1 , 2 i 
=K h10 , 20 i iff
0

1. h1 , 2 i  rI iff h10 , 20 i  rI for each r  rol(K),
0

2. i  AI iff i0  AI for i  {1, 2} and each A  con(K),
0

3. i = oI iff i0 = oI for i  {1, 2} and each o  nom(K).
We say that I and I 0 are isomorphic w.r.t. K, written: I 
=K I 0 , if there is a bijection
0
 : I  I such that, for each 1 , 2  I , h1 , 2 i 
=K h(1 ), (2 )i and 1 is a
successor of 2 iff (1 ) is a successor of (2 ).
4
If clear from the context, we omit the subscript K of 
=K . We extend the above definition
in the obvious way to forest quasi-interpretations, i.e., by omitting condition 3 and defining
the isomorphism with respect to K0 = nomFree(K).
Forest quasi-models have, intuitively, the purpose of an intermediate step between arbitrary models of K and forest models of K. When identifying each  in the interpretation of
a concept No in the knowledge base K0 with a root that is in the interpretation of No , we
obtain an interpretation that would be a model for K apart from functionality restrictions
for some nominals that might be violated. We show later how we can eliminate those relations from the forest back to the roots that violate functionality restrictions and how we
can eventually obtain a forest model from a forest quasi-model.
Another useful property of quasi-interpretations is that, for simplified ALCIFb knowledge bases, it can be checked locally whether an interpretation I is actually a model of
K.
Definition 9 (Local K-consistency). Let I = (I , I ) be an interpretation for a simplified
ALCIFb knowledge base K with   I . We define local satisfaction for  and concepts
that can occur in simplified ALCIFb axioms as follows:
444

fiNominals, Inverses, Counting, and Conjunctive Queries

1. for A1 , . . . , An  con(K):
(a) I,  |=

d

(b) I,  |=

F

Ai if   AIi for each i with 1  i  n; I,  6|=

d

Ai otherwise;
F
Ai if   AIi for some i with 1  i  n; I,  6|= Ai otherwise;

2. for U a safe Boolean role expression over rol(K), A  con(K):
(a) I,  |= U.A if there is some  0  I such that h,  0 i  U I and I,  0 |= A;
I,  6|= U.A otherwise;
(b) I,  |= U.A if, for each  0  I such that h,  0 i  U I , I,  0 |= A; I,  6|= U.A
otherwise;
3. for f  rol(K), I,  |= func(f ) if ]({ 0  I | h,  0 i  f I })  1; I,  6|= func(f )
otherwise.
An element   I locally satisfies a GCI C v D with C, D ALCIFb-concepts if I,  |= C
implies I,  |= D. It locally satisfies a functionality restriction func(f ) if I,  |= func(f ). An
element   I is locally K-consistent if it locally satisfies each axiom in K.
4

Lemma 10. Let K be a simplified ALCIFb knowledge base and I = (I , I ) an interpretation for K. Then I is a model for K iff each element   I is locally K-consistent.

Proof. For simplified ALCIFb knowledge bases, only axioms of the form A v U.B and
A v U.B involve checking neighbors of an element  and, since B is a concept name in
simplified knowledge bases, it is immediate that satisfaction of B can be checked locally for
the neighbor of  in question.
For a knowledge base K with nominals, we can also use local K-consistency, but we
need an additional global condition that ensures that nominals are interpreted as singleton
sets. The following is an immediate consequence of Lemma 10 and the extra condition 2
for nominals:
Proposition 11. Let K be a simplified ALCOIFb knowledge base and I = (I , I ) an
interpretation for K. Then I is a model for K iff
1. each element   I is locally K-consistent and,
2. for each o  nom(K), there is exactly one element   I such that oI = .

445

fiRudolph & Glimm

We now show how we can obtain a forest quasi-model from a model of K by using an
adapted version of unraveling.
Definition 12 (Unraveling). Let K be a consistent ALCOIFb knowledge base and I =
(I , I ) a model for K. Let choose be a function that returns, for a concept C = U.B 
cl(K) and an element   (U.B)I an element C,  I such that h, C, i  U I and
C,  B I .
Without loss of generality, we assume that, for all   I and concepts C1 = U1 .B1 , C2 =
U2 .B2  cl(K) such that   C1I  C2I , if choose(C1 , ) = 1 , choose(C2 , ) = 2 , and
h, 1 i 
= h, 2 i, then 1 = 2 .
An unraveling for some element   I , denoted as (I, ), is an interpretation that
is obtained from I and  as follows: we define the set S  (I ) of sequences to be the
smallest set such that
  is a sequence;
 1    n  n+1 is a sequence, if
 1    n is a sequence,
 if n > 2 and hn , n1 i  f I for some functional role f , then n+1 6= n1 ,
 n+1 = choose(C, n ) for some C = U.B  cl(K).
Now fix a set F  {}  IN and a bijection  : F  S such that
(i) F is a forest,
(ii) (, ) = ,
(iii) if (, w), (, w  c)  F with w  c a successor of w, then (, w  c) = (, w)  n+1 for
some n+1  I .
Such a forest F and bijection  exist because S is a prefix-closed set with root . Thus, we
just map from the notion of sequences to that of forests.
For each o  nom(K), let No  NC be a fresh concept name. For each (, w)  F , set
Tail(, w) = n if (, w) = 1    n . Now, we define the unraveling for  as the interpretation
J = (J , J ) with J = F and, for each (, w)  J , we define the interpretation of
concept and role names as follows:
(a) for each o  nom(K), NoJ = {(, w)  J | Tail(, w)  oI };
(b) for each concept name A  con(K), AJ = {(, w)  J | Tail(, w)  AI };
(c) for each role name r  rol(K), h(, w), (, w0 )i  rJ iff w0 is a neighbor of w, and
hTail(, w), Tail(, w0 )i  rI .
Let R be the subset of I that contains exactly those   I such that oI =  for some
o  nom(K). Let U be a set containing an unraveling of I starting from each   R. The
union of all interpretations from U is called an unraveling for I, denoted as (I), where
unions of interpretations are defined in the natural way.
4
446

fiNominals, Inverses, Counting, and Conjunctive Queries

No E
A
A

BC
E

BD
E

f

s

BC
E

g

BC
E

A
A

s

r

BD
E

f

BD
E

f

g

BC
E

g

BC
E

s

..

.

r
s

BD
E
g

BC
E
f
..

r
s

r

r

r
s

A
A

r

BD
E
g

.

f
..

.

BD
E
g

f
..

No E
..
.
No E
..
.
No E
..
.

.

Figure 3: Unraveling of the model displayed in Figure 2.
Figure 3 shows the unraveling for our example knowledge base and model. The dotted
lines under the non-root elements labeled No indicate that a copy of the whole tree should
be appended since we do not stop unraveling at nominal placeholders.
It might be helpful to think of the function Tail as a homomorphism (up to signature
extension) from the elements in the unraveling J to elements in the original model I.
Indeed, Tail satisfies the following properties: For each (, w), ( 0 , w0 )  J ,
 Tail(, w) = oI iff (, w)  NoJ , for all o  nom(K),
 Tail(, w)  AI iff (, w)  AJ , for all A  con(K), and
 hTail(, w), Tail( 0 , w0 )i  rI iff h(, w), ( 0 , w0 )i  rJ , for all r  rol(K).
Unravelings are the first step in the process of transforming an arbitrary model of K
into a forest model since the resulting model is a forest quasi-model of K, as we show in the
next lemma.
Lemma 13. Let K be a consistent ALCOIFb knowledge base and I = (I , I ) a model of
K. Then J = (J , J ) = (I) is a strict forest quasi-model for K.
Proof. Let K0 = nomFree(K). By construction, J satisfies conditions FQ1 and FQ3 of
forest quasi-models and the strictness condition. Since J is obtained from a model I of
K, by definition of unravelings as starting from each   I such that oI =  for some
o  nom(K), and by condition (a) of unravelings, there is, for each o  nom(K), one root
(, )  J such that (, )  NoJ . Thus, J satisfies also property FQ2 and J is a forest
quasi-interpretation for K. We show that J is a model of K0 by demonstrating that each
(, w)  J is locally K0 -consistent. Since we assume all knowledge bases to be simplified,
we only have to consider axioms of form (1).
447

fiRudolph & Glimm

d
F
d
Let Ax be an axiom of the form Ai v Bj and assume
that (, w)  ( Ai )J . By
d
condition (b) of unravelings, we have w = Tail(, w)  ( Ai )I and, since I |= K, we have
w  BjI for some j. Again by condition (b) of unravelings, we then have (, w)  BjJ as
required.
Axioms of the form A  {o} in K are rewritten into A  No in K0 . We consider A v No
and No v A separately. Let Ax be of the form A v No for o  nom(K) and assume that
(, w)  AJ . By condition (b), we have that w = Tail(, w)  AI and, since I |= K,
we have w  {oI }. By condition (a) of unravelings, we then have that (, w)  NoJ as
required. For No v A with o  nom(K), assume that (, w)  NoJ . By condition (a), we
have w = Tail(, w)  {oI } and, since I |= K, we have w  AI . By condition (b) of
unravelings, we then have (, w)  AJ as required.
Let Ax be an axiom of the form A v U.B and assume that (, w)  AJ . By condition (b), we have w = Tail(, w)  AI and, since I |= K, we have each w0  I such
that hw , w0 i  U I is such that w0  B I . Let ( 0 , w0 ) be such that h(, w), ( 0 , w0 )i  U J
and ( 0 , w0 ) 
/ B J . By condition (c) of unravelings, we then have that hw , w0 i  U I for
w0 = Tail( 0 , w0 ) and by condition (b) that w0 
/ B I , which is a contradiction.
Let Ax be an axiom of the form A v U.B and assume that (, w)  AJ . By condition (b), we have w = Tail(, w)  AI and, since I |= K, we have there is at least one
w0  I such that hw , w0 i  U I and w0  B I . In case there is more than one such
element, let w0 be such that w0 = choose(C, w ). Then, by definition of sequences, there is
some neighbor (, w0 ) of (, w) with Tail(, w0 ) = w0 . Let (, w) = 1    n , i.e., n = w .
We distinguish two cases:
1. The element w0 is such that w0 = n1 . By definition of the bijection , w =
w0  c, by definition of J from I (condition (c)) and since hw , w0 i  U I , we have
h(, w), (, w0 )i  U J . Then, since B is a concept name and w0  B I , we have by
condition (b) that (, w0 )  B J , which proves the claim.
2. The element w0 is such that w0 6= n1 . By definition of sequences and the bijection
, we have that (, w0 ) = 1    n  w0 . Now, by definition of J from I (in particular
properties (b) and (c)), we have h(, w), (, w0 )i  U J and, again since B is a concept
name, (, w0 )  B J , which proves the claim.
Let Ax be an axiom of the form func(r) for r  rol(K). Assume, to the contrary
of what is to be shown, that (, w) has two distinct neighbors (, w1 ), (, w2 ) such that
h(, w), (, w1 )i, h(, w), (, w2 )i  rJ . Since the function  introduced in the unraveling is a
bijection, there are two distinct sequences s1 and s2 such that (, w1 ) = s1 and (, w2 ) = s2
and Tail(, w1 ) = 1 , Tail(, w2 ) = 2 with 1 6= 2 . Since h(, w), (, w1 )i, h(, w), (, w2 )i 
rJ we get, due to condition (c), that hTail(, w), 1 i, hTail(, w), 2 i  rI , which is a contradiction since I |= K.
Since (, w) was arbitrarily chosen, we have that each element in the domain of J is
locally K0 -consistent as required and J |= K0 by Lemma 10.
Lemma 14. Let K be a consistent ALCOIFb knowledge base, I = (I , I ) a model of K,
and J = (J , J ) = (I) an unraveling for I. Then J has a branching degree bounded in
|cl(K)|.

448

fiNominals, Inverses, Counting, and Conjunctive Queries

Proof. Let m be the number of axioms in K. Each axiom of a simplified knowledge base
can contain at most one existential restriction and, due to the definition of the function
choose used in the unraveling, there are, for each sequence s  S, at most m elements
1 , . . . , m  I such that s  i with 1  i  m is a sequence in S. Since the mapping 
from the forest J to sequences is a bijection, J is a forest with branching degree m.
In the following steps, we traverse a forest quasi-model in an order in which elements
with smaller tree depth are always of smaller order than elements with greater tree depth.
Elements with the same tree depth are ordered lexicographically. The bounded branching
degree of unravelings then guarantees that, after a finite number of steps, we go on to
the next level in the forest and process all nodes eventually. Further, we can merge nodes
such that, finally, all nominal placeholders (in the extension of some No ) can be interpreted
as nominals without violating functionality restrictions. In fact, we do not only have to
merge nominal placeholders, but also elements that are related to a nominal placeholder
by an inverse functional role since, by definition of the semantics, these elements have to
correspond to the same element in a model. In order to identify such elements, we define
the notion of backwards counting paths as follows:
Definition 15 (Paths and BCPs). Let I = (I , I ) be an interpretation. We call 1  . . .  n
a path from 1 to n if, for each i with 1  i < n, hi , i+1 i  riI for some role ri  rol(K).
The length |p| of a path p = 1  . . .  n is n  1. Each element   I is a path of length
U

Un1

0. We write 1 1 2 . . .  n to denote a path from 1 to n such that hi , i+1 i  UiI for
each 1  i < n and Ui a safe Boolean role expression.
Let K be an ALCOIFb knowledge base and I = (I , I ) a forest model (a forest quasimodel) of K. A path p = 1 . . .n in I is a descending path if there is some root (, )  I
such that, for each i with 1  i  n, i = (, wi ) and, for 1  i < n, |wi | < |wi+1 |. The
path p is a backwards counting path (BCP) in I if n = oI (n  NoI ) for some nominal
o  nom(K) and, for each i with 1  i < n, hi , i+1 i  fiI for some inverse functional role
fi  rol(K). The path p is a descending BCP if it is a BCP and a descending path. Given a
f1

fn

BCP p = 1  2 . . .  n+1 with n+1  oJ (n+1  NoJ ), we call the sequence f1    fn o
a path sketch of p.
4
Please note that an element  in the domain of J already counts as a (descending) BCP
if   oJ (NoJ ) for some o  nom(K).
We now define the order that guarantees that in an iterative parsing process, we not only
process all nodes, but also that we can merge nodes as required so that, finally, all nominal
placeholders can be interpreted as nominals without violating functionality restrictions.
Definition 16 (Ordering). For convenience and without loss of generality, we assume that
the set of individual names NI is ordered. Let K be a consistent ALCOIFb knowledge
base and J a forest quasi-interpretation for K. We extend the order to elements in J


as follows: let w1 = wp  c11    cn1 , w2 = wp  c12    cm
2  IN where wp  IN is the longest
common prefix of w1 and w2 , then w1 < w2 if either n < m or both n = m and c11 < c12 . For
(1 , ), (2 , )  J , let o1  nom(K) be the smallest nominal such that (1 , )  NoJ1 and
o2  nom(K) the smallest nominal such that (2 , )  NoJ2 . Now (1 , w1 ) < (2 , w2 ) if either
(i) |w1 | < |w2 | or (ii) |w1 | = |w2 | and o1 < o2 or (ii) |w1 | = |w2 |, o1 = o2 and w1 < w2 .
449

fiRudolph & Glimm

In the following, we are merging elements in an unraveling and, in this process, create
new roots of the form (w, ) from elements of the form (, w) and elements of the form
(w, w0 ) from (, ww0 ). We extend, therefore, the order to elements of this form as follows:
(1 w1 , w10 ) < (2 w2 , w20 ) if (1 , w1 w10 ) < (2 , w2 w20 ).
4
Roughly speaking, we proceed as follows in order to transform a quasi-forest model
J into a forest model I: we work our way downwards the trees level by level along the
descending BCPs and use the above defined order for this purpose. By definition of the
semantics, elements that start the same descending BCP or, more precisely, that start
BCPs with identical path sketches, have to correspond to the same element in the forest
model I that we produce. During the traversal of the forest quasi-model, we distinguish
two situations: (i) we encounter an element (, w) that starts a descending BCP and we
have not seen another element before that starts a descending BCP with the same path
sketch. In this case, we promote (, w) to become a new root node of the form (w, ) and
we shift the subtree rooted in (, w) with it; (ii) we encounter a node (, w) that starts a
descending BCP, but we have already seen a node (0 , w0 ) that starts a descending BCP
with that path sketch and which is now a root of the form (0 w0 , ). In this case, we delete
the subtree rooted in (, w) and identify (, w) with (0 w0 , ). If (, w) is an f -successor of
its predecessor for some inverse functional role f , we delete all f  -successors of (0 w0 , ) and
their subtrees in order to satisfy the functionality restriction. We use a notion of collapsing
admissibility to characterize models in which the predecessor of (, w) satisfies the same
atomic concepts as the deleted successor of (0 , w0 ), which ensures that local consistency is
preserved. By virtue of this notion, we can characterize forest quasi-models that can be
collapsed into proper models irrespective of whether they have been obtained by unraveling
of a model or not.
In order to keep the domain as a forest when promoting an element (, w) to a new root,
we build the new domain with elements of the form (w, ) for (, w) and elements of the
form (w, w0 ) for descendants (, ww0 ) of (, w).
Definition 17 (Equivalence Relation  and Collapsings). Let K be an ALCOIFb knowledge base, K0 = nomFree(K), and J = (J , J ) a forest quasi-interpretation of K. We
define  as the smallest equivalence relation on J that satisfies 1  2 if 1 , 2 start
descending BCPs with identical path sketches.
Let J be a strict forest quasi-interpretation for K, J0 = (J0 , J0 ) = J and (0 , w0 ) 
J
0
 the smallest element with w0 6=  that starts a descending BCP. We call J0 an initial
collapsing for J and (0 , w0 ) the focus of J0 .
Let Ji be a collapsing for J and (i , wi )  Ji the focus of Ji . We obtain a collapsing
Ji+1 = (Ji+1 , Ji+1 ) with focus (i+1 , wi+1 ) for J from Ji according to the following two
cases:
1. There is no element (, )  Ji such that (, ) is smaller than the focus (i , wi ) and
(, )  (i , wi ). Then Ji+i is obtained from Ji by renaming each element (i , wi wi0 ) 
Ji to (i wi , wi0 ).
2. There is an element (, )  Ji such that (, ) is smaller than the focus (i , wi ) and
(, )  (i , wi ). Let (, ) be the smallest such element.
450

fiNominals, Inverses, Counting, and Conjunctive Queries

(a) Ji+1 = Ji \ ({(i , wi wi0 ) | wi0  IN }  {(, w) | w = c  w0 , c  IN, w0 
IN , (i , wi ) has a predecessor (i , wi0 ) such that h(i , wi0 ), (i , wi )i  f Ji for an
inverse functional role f in rol(K) and h(, c), (, )i  f Ji });
(b) for each concept name A  con(K) and (, w)  Ji+1 , (, w)  AJi+1 iff (, w) 
AJi ;
(c) for each role name r  rol(K) and (1 , w1 ), (2 , w2 )  Ji+1 , h(1 , w1 ), (2 , w2 )i 
rJi+1 iff
i. h(1 , w1 ), (2 , w2 )i  rJi or
ii. (1 , w1 ) is the predecessor of (i , wi ) in Ji (i.e., 1 = i and wi = w1  c for
some c  IN), (2 , w2 ) = (, ), and h(1 , w1 ), (i , wi )i  rJi .
The focus (i+1 , wi+1 ) in Ji+1 is the smallest descending BCP with (i , wi ) < (i+1 , wi+1 ).
For a collapsing Ji , let safe(Ji ) be the restriction of Ji to elements (, w) such that
(, w)  Jj for all j  i. With J we denote the non-disjoint union of all interpretations
safe(Ji ) obtained from subsequent collapsings Ji for J . The interpretation obtained from
J by interpreting each o  nom(K) as (, )  NoJ is denoted by collapse(J ) and called a
purified interpretation with respect to J. If collapse(J ) |= K, we call collapse(J ) a purified
model of K.
4
In Figures 4 to 7 we illustrate the first collapsing steps for the unraveling depicted in
Figure 3. Apart from the nominal placeholder concepts, the concept interpretations are
not shown in the figures, but are assumed to be as indicated in Figure 3. The edges for
descending BCPs are shown in red color, and the dashed lines in Figure 4 indicate the levels
of the tree because, within a tree, the order in which the nodes are processed depends firstly
on their level. Within a level, we assume that the order increases from left to right. The
numbers next to nodes in Figure 4 indicate, which elements are used as focus element in
a collapsing step and their order. For the initial collapsing (Figure 4) the focus is on the
first non-root element that starts a BCP, which we indicate with a black border around the
node and a black triangle pointing to the focus.
In the first collapsing step we just rename elements to promote the focus from Figure 4
to a root. Because the focus element highlighted in Figure 5 starts a BCP with path sketch
different from the ones started by smaller elements, we again only rename elements to
obtain a new root (Figure 6). Now, the focus is on a nominal placeholder and since nominal
placeholder are BCPs, we have a root with the same path sketch and use the second case
of Definition 17. The resulting collapsing is depicted in Figure 7.
Finally, we obtain a collapsing for the unraveling shown in Figure 3 as the one depicted
in Figure 8.
We can now show that the collapsing of an unraveling results in a forest model for
K. Our aim is, however, to show something more general. We want to collapse not only
unravelings into forest models, but also forest quasi-models which have been obtained in
another way. Unfortunately, it is not the case that the collapsing of any forest quasi-model
results in a forest model for K since the elements that we merge in the collapsing process
do not necessarily satisfy the same atomic concepts. We define, therefore, the following
admissibility criterion that characterizes forest quasi-models that can be collapsed into
forest models.
451

fiRudolph & Glimm

No

No

r

r



H
1 No

r
3

4
f

f
r s
g







No
f

r s



g

g

5 No
f
g

r s

H

g


r s

r s


2
g



r s



f



r s

No
f

s

Figure 4: An initial collapsing for the un- Figure 5: In the first collapsing step we just
raveling depicted in Figure 3.
rename elements to promote the
focus from Figure 4 to a root.

r

r

No
f

r
r

r s

r s

f





H

r s



g

g



f

No
f

s

r

r s

g

s

g

g


s



N

f



g

f

s



r

No



No

Figure 6: A collapsing obtained from the Figure 7: A collapsing obtained by using
one depicted in Figure 5.
the second case of Definition 17
on the collapsing from Figure 6.

Definition 18 (Collapsing-admissibility). Let J be a forest quasi-interpretation for some
ALCOIFb knowledge base K. Then J is collapsing-admissible if there exists a function
ch : (cl(K)  J )  J such that
1. for each concept C = U.B  cl(K) and each   C J , we have h, ch(C, )i  U J and
ch(C, )  B J . Moreover, if there is no functional role f for which h, ch(C, )i  f J
then ch(C, ) is a successor of ,
2. for each concept C = U.B  cl(K) and elements ,  0  C J that start descending
BCPs with identical path sketches, we have h, ch(C, )i 
= h 0 , ch(C,  0 )i.
4
Lemma 19. Let K be an ALCOIFb knowledge base. Any unraveling J of a model I for
K is collapsing-admissible.
452

fiNominals, Inverses, Counting, and Conjunctive Queries

B
B
B
B
B
{o} E f C E g D E f C E g D E f C E g



r
s

A

s

r
A

s

r
s
A
s

r
A
r
A
r
..
.

Figure 8: Result of collapsing the unraveling from Fig. 3. The infinitely many new root
elements are displayed in the top line.

Proof. We define a function ch directly from the function choose used in the unraveling as follows: for each C  cl(K) and (, w)  J with (, w) = 1  . . .  n and
choose(C, Tail(, w)) = { 0 }, we set ch(C, (, w)) = (, w0 ) for (, w0 ) =  (1  . . .  n   0 )
if 1  . . .  n   0 is a sequence in S and (, w0 ) =  (1  . . .  n1 ) otherwise. This is
well-defined since the function  in unravelings is total and bijective and it is as required
for admissibility since elements that start BCPs with identical path sketches are always
generated from the same element in I. The first condition of collapsing-admissibility holds
since in unravelings, we always add 1  . . .  n   0 to the set of sequences unless the pair
hn , n1 i is in the interpretation of some functional role. In that case, the function ch uses
the predecessor instead of the successor, which is still admissible.
Lemma 20. Let K be a consistent ALCOIFb knowledge base, J = (J , J ) a strict forest
quasi-model for K with branching degree b that is collapsing-admissible. Then collapse(J )
is a forest model for K with branching degree b.
Proof. Let K0 = nomFree(K). Since J is a forest quasi-model of K by assumption, J |= K0 .
We first show that each collapsing Ji for J is a forest quasi-model for K, i.e., Ji |= K0 .
We then show that each collapsing Ji+1 produced from an admissible collapsing Ji is again
collapsing-admissible. Finally, we show that, for each o  nom(K), there is exactly one
node in J of the form (, ) such that (, )  NoJ , which implies by Proposition 11 that
collapse(J ) is a forest model for K.
We start with the first claim: For the initial collapsing this is immediate since J is a
forest quasi-model for K. In particular, J0 is locally K0 -consistent. Assume that Ji is a
locally K0 -consistent collapsing and (i , wi ) is the focus in Ji . We show that Ji+1 is locally
453

fiRudolph & Glimm

K0 -consistent. Since K0 is simplified by assumption, we only have to consider axioms of
form (1).
If Ji+1 is obtained according to the first case of Definition 17, we only rename elements
of the domain in order to create a new root node and local K0 -consistency is immediate.
We assume, thus, that Ji+1
according to the second case of Definition 17.
d is obtained
F
Axioms of the form Ai v Bj and A  {o} (rewritten into A  No in K0 ) hold
immediately due to condition 2.b of collapsings.
Let Ax be an axiom of the form A v U.B and assume that (, w)  AJ . The only
interesting elements are the predecessor (i , wi0 ) of the focus (i , wi ) and (, ). However,
(i , wi )  (, ) and, since J is collapsing-admissible, (i , wi ) and (, ) satisfy the same
atomic concepts with respect to con(K). Further, the interpretation of atomic concepts is
not changed due to 2.b, which again implies local K0 -consistency for this kind of axioms.
Let Ax be an axiom of the form A v U.B and assume that (, w)  AJi+1 . We concentrate on the three interesting cases where the direct neighborhoods of elements change:
1. We start with the case where the focus (i , wi ) is the corresponding U -successor of
(, w), i.e., i = , wi = wc for some c  IN, h(, w), (i , wi )i  U Ji , and (i , wi )  B Ji .
Since (, ) and (i , wi ) are in the same equivalence class for  by assumption, (, )
starts a BCP with the same path sketch as (i , wi ) and both (, ) and (i , wi ) satisfy
the same atomic concepts with respect to con(K). Then condition 2.(c)ii. ensures that
(, ) is the required U -successor for (, w) in Ji+1 .
2. Assume that (, w) = (, ), h(, ), (, c)i  U Ji , (, c)  B Ji , (, c) 
/ Ji+1 , and
J
i+1
(, ) 
/ (U.B)
. Due to 2.a, the focus (i , wi ) has a predecessor (i , wi0 ) such that
0
h(i , wi ), (i , wi )i  f Ji for an inverse functional role f  rol(K) and h(, ), (, c)i 
(f  )Ji . Since f is inverse functional and Ji is, by assumption, locally K0 -consistent,
there is no successor (i , wi  ci ) of (i , wi ) such that h(i , wi ), (i , wi  ci )i  (f  )Ji .
Similarly, there is no element (0 , w0 ) such that h(, ), (0 , w0 )i  (f  )Ji . Then,
since Ji is collapsing-admissible, we have that (i , wi0 )  ch(U.B, (i , wi )), (, c) 
ch(U.B, (, )), and h(i , wi ), (i , wi0 )i 
= h(, ), (, c)i since (i , wi ) and (, ) start
descending BCPs with identical path sketches. In particular, h(i , wi ), (i , wi0 )i  U Ji
and (i , wi0 )  B Ji . Then, by condition 2.(c)ii., h(, ), (i , wi0 )i  U Ji+1 , by condition 2.b, (i , wi0 )  B Ji+1 , and, thus, (, )  (U.B)Ji+1 as required.
3. We assume that (i , wi ) has a predecessor (i , wi0 ) such that h(i , wi0 ), (i , wi )i  f Ji
for an inverse functional role f in rol(K) and h(, c), (, )i  f Ji , causing the deletion
of (, c) and its descendants, one of which, say (, v) is connected to some (, ), such
that h(, ), (, v)i  U Ji and (, v)  B Ji . Now, if there is no inverse functional role
g for which h(, ), (, v)i  g Ji , then the strictness and collapsing-admissibility of Ji
ensure the existence of a c0  IN for which h(, ), (, c)i  U Ji and (, c)  B Ji and,
consequently, also h(, ), (, c)i  U Ji+1 and (, c)  B Ji+1 . If h(, ), (, v)i  g Ji
for some inverse functional role g, then strictness of the initial collapsing implies that
(, v) itself started a descending BCP and, due to the defined order, it must have
been a focus before and now be a root itself. This contradicts, however, the initial
assumption that (, v) is a descendant of (, ) and we are done.
In all cases, the elements in Ji+1 have the required successors.
454

fiNominals, Inverses, Counting, and Conjunctive Queries

Let Ax be an axiom of the form func(f ) for f  rol(K). We concentrate on relations
between the predecessor (i , wi0 ) of the focus and (, ) since otherwise local K0 -consistency
is immediate. A predecessor exists for the focus since we process elements in ascending order starting with non-root nodes. Assume h(i , wi0 ), (i , wi )i  f Ji , in which case
h(i , wi0 ), (, )i  f Ji+1 due to 2.(c)ii. and assume (, ) has a successor (, c) in Ji such
that h(, c), (, )i  f Ji . In this case, (, c) 
/ Ji+1 according to 2.a and together with
0
local K -consistency of Ji , this implies that (i , wi0 ) is the only element in Ji+1 such that
h(i , wi0 ), (, )i  f Ji+1 .
We now show that each Ji+1 produced from an admissible collapsing Ji is again admissible for collapsing. By assumption, the initial collapsing is admissible, so let Ji be an
admissible collapsing and chi the required function. We distinguish two cases:
1. Let Ji+1 be produced according to the first case of collapsings. We define a function
chi+1 for Ji+1 as follows: For each C  cl(K) and   Ji+1 , we set chi+1 (C, ) =  0
for  0 = (i wi , w10 ) if chi (C, ) = (i , wi wi0 ) for (i , wi ) the focus in Ji and  0 = chi (C, )
otherwise. Since we just change the names of the elements and leave the interpretation
of concepts and roles as before, the function is as required for admissibility.
2. Let Ji+1 be produced according to the second case of collapsings. We define a function
chi+1 for Ji+1 as follows: For each C  cl(K),
(a) for each   Ji+1 
/ {(i , wi0 ), (, )} with (i , wi0 ) the predecessor of the focus
(i , wi ), we set chi+1 (C, ) =  0 for  0 such that  0  Ji+1 and  0 = chi (C, );
this is well-defined since only successors of (, ) and (i , wi0 ) are deleted in Ji+1 .
(b) for  = (, ) and (i , wi0 ) the predecessor of the focus, chi+1 (C, ) =  0 for
 0 = (i , wi0 ) if chi (C, ) = (, c) and (, c) 
/ Ji+1 and  0 = chi (C, ) otherwise;
(c) for  = (i , wi0 ) the predecessor of the focus, we set chi+1 (C, ) =  0 for  0 = (, )
if chi (C, ) = (i , wi ) and  0 = chi (C, ) otherwise.
For elements apart from the predecessor of the focus (i , wi0 ) and the root (, )
that replaces (i , wi ), the interpretation of concepts and roles remains as before
by properties 2.b and 2.c and the function is as required. For (i , wi0 ), we change
the function so that in cases where (i , wi ) was returned, (, ) is returned. Since
(i , wi )  (, ), this is admissible. Similarly, if a successor (, c) of (, ) is not contained in Ji+1 , then (i , wi0 ) is used instead. This is admissible since, in this case,
h(i , wi ), (i , wi0 )i 
= h(, ), (, c)i as argued above for axioms of the form A v U.B.
We now show that, for each o  nom(K), there is exactly one node in J of the form
(, ) such that (, )  NoJ . Nominal placeholders are descending BCPs by definition and,
when a nominal placeholder becomes the focus, it is merged into a root that is in the same
equivalence class of  and which is by definition of lower order. Such a root exists because
of property FQ2 of forest quasi-interpretations.
The interpretation J is obtained by building the non-disjoint union of the safe parts
for all collapsings, which contain only elements which will neither be renamed nor deleted.
Thus, J does not contain nominal placeholders as required. Considering any one element
(, w)  J , we find that there is an i  IN such that all successors and all root neighbors
455

fiRudolph & Glimm

of (, w) in Ji are the same as in J . As we have shown, Ji is locally K0 -consistent and
therefore (, w) has a consistent neighborhood. Hence J is a forest quasi-model of K.
Now, when interpreting each o  nom(K) as {(, )  J | (, )  NoJ } in collapse(J ),
we obtain a forest model for K, where the set of roots is {(, ) | (, )  J }.
The bounded branching degree is an immediate consequence of the construction since
we never add successors during the construction and the starting forest quasi-interpretation
J has a bounded branching degree by assumption.
Since unravelings of a model I for K are strict forest quasi-models of K with branching degree bounded in |cl(K)| by Lemma 13, and unravelings are collapsing-admissible by
Lemma 19, it is an immediate consequence of Lemma 20 that the collapsing of an unraveling
yields a forest model branching degree bounded in |cl(K)|.
Corollary 21. Let K be an ALCOIFb knowledge base and I an interpretation such that
I |= K, then the purified interpretation collapse((I)) is a forest model for K with branching
degree b bounded in |cl(K)|.
Since the number of roots might still be infinite in purified models, we could, up to
now, have obtained the same result by unraveling an arbitrary model, where we take all
elements on BCPs as roots instead of taking just the nominals and creating new roots
in the collapsing process. In the next sections, however, we show how we can transform
an unraveling of a counter-model for the query such that it remains collapsing-admissible
and such that it can in the end be collapsed into a forest model with a finite number of
roots that is still a counter model for the query. For this transformation it is much more
convenient to work with real (strict) trees and forests, which is why we use (strict) forest
quasi-interpretations.
In the next sections, we also use the following alternative characterization of the result
of a collapsing, which comes in handy for the subsequent proofs.
We start by defining the so-called pruning of a forest quasi-interpretation, which is,
roughly speaking, the structure obtained by just deleting all the nodes, which will be erased
in the course of the collapsing process anyway.
Definition 22 (Pruning). Let J be a strict forest quasi-model for an ALCOIFb knowledge
base K that is collapsing-admissible and let J0 , J1 , . . . , J be as defined in Definition 17.
The pruning of J (written prune(J )) is obtained by restricting J to a set   J which
is defined as follows:  contains all hw1 , w2 w3 i  J for which hw1 w2 , w3 i  J or
hw1 w2 , w3 i is the focus in some Ji .
4
We again use the equivalence relation  for elements that start descending BCPs with
identical path sketches from Definition 17 and construct an interpretation from a pruning
by identifying equivalent nodes, also known as factorization.
Definition 23 (Factorization). Let K be an ALCOIFb knowledge base, J a strict forest
quasi-interpretation for K that is collapsing-admissible, and L = prune(J ).
The factorization of L by  (denoted by L/ ) is now defined as the forest quasi-interpretation M = (M , M ) with
 M = {[] |   L };
456

fiNominals, Inverses, Counting, and Conjunctive Queries

 for each A  con(K), AM = {[] |   AL },
 for each r  rol(K), rM = {h[] , [ 0 ] i | h,  0 i  rL }, and
 for each o  nom(K), oM = [] for   NoL .
4
Note that the interpretation of nominals in M is well defined as, by definition, all
No -instances are in the same -equivalence class.
Now we are ready to establish the wanted correspondence: the collapsing of a forest
quasi-interpretation can essentially be obtained by first pruning and then factorizing it.
Lemma 24. Let J be a strict forest quasi-model for an ALCOIFb knowledge base K and
let J be collapsing-admissible. Then collapse(J ) 
= prune(J )/ . Moreover the new roots in
collapse(J ) correspond to those -equivalence classes that contain J -elements which start
descending BCPs in J .
Proof. Considering the first claim, note that by definition of the collapsing procedure, for
every (w, w0 )  collapse(J ) there is exactly one pair w1 , w2 with w = w1 w2 such that
(w1 , w2 w0 )  prune(J ) . Moreover, case 1 of the construction assures that collapse(J ) contains one element from every -equivalence class from prune(J )/ . Hence the mapping
 : collapse(J )  prune(J )/ with (w, w0 ) = [(w1 , w2 w0 )] is a bijection and, as a consequence of the construction, an isomorphism.
The second claim is also a direct consequence of the construction of the collapsing.

5. Quasi-Entailment in Quasi-Models
In this section, we will provide a characterization for forest quasi-models that mirrors query
entailment for the corresponding proper models. In our further argumentation, we will
talk about the initial part of a tree, i.e., the part that is left if branches are cut down to a
fixed length. For a forest interpretation I = (I , I ) and an n  IN, we therefore denote
with cutn (I) the interpretation obtained from I by restricting I to those pairs (, w) for
which |w|  n.
The following lemma ensures that in the case of purified models, we find only finitely
many unraveling trees of depth n that look different.
Lemma 25. Let K be a consistent ALCOIFb knowledge base. Then there is a purified
interpretation I such that I |= K and, for every n  IN, there are only finitely many
non-isomorphic trees of depth n.
Proof. Since K has a model by assumption, Corollary 21 guarantees that there is some
purified model I of K. In particular, I is a forest model with the branching degree bounded
in the size of cl(K).
We now compute the maximal number of non-isomorphic trees of depth n over the
domain of I. We denote this bound by Tn . The argumentation is close to the one used by
Levy and Rousset (1998) for their definition of tree blocking.
457

fiRudolph & Glimm

Let c = |cl(K)| and r = |rol(K)|. We first consider trees of depth n = 0. We have 2c
choices for the different subsets of concepts in cl(K). For n > 0, each concept at level 0
can trigger the generation of a new successor and we can have any number of successors
between 0 and c. Assume, for now, that we have only a single role name r  rol(K) and
that each node in a level smaller than n is the root of a tree with depth n  1 with exactly
c ) non-isomorphic sub-trees of
c successors for each node. In this case, there are O(2c Tn1
depth n. Taking into account that a node does not necessarily have c successors, but we can
c ) for the number of nonchoose any number between 0 and c, we get a bound of O(2c cTn1
isomorphic sub-trees of depth n. Finally, since we have not only one but a choice of r roles,
c )r ). We now abbreviate 2c cr with x and rc with a and rewrite
we get a bound of O(2c (cTn1
n1
n
the obtained bound as Tn = O(x(Tn1 )a ). Unfolding yields Tn = O((x1+a+...+a )(T0 )a )
n
n
n
which is bounded by O((xa )(2c )a ) = O((x2c )a ). By expanding the abbreviated symbols,
n
we obtain a bound for Tn of O((2c cr )(rc) ).
For our further considerations, we introduce the notion of anchored n-components.
These are meant to be certain substructures of forest quasi-interpretations. In the first
place, these substructures contain a connected set of nodes W 0 which are situated closely
together in the original structure, this closeness being witnessed by the fact that all elements of W 0 are descendants of some node  and have a distance  n to . Moreover for
each of those nodes  0 from W 0 , the anchored n-component may (but does not need to)
contain a finite number of descending BCPs starting from  0 .
Definition 26 (Anchored Components). Let J be a forest quasi-interpretation and   J .
An interpretation C will be called anchored n-component of J with witness  if C can be
created by restricting J to a set W  J obtained as follows:
 Let J be the subtree of J that is started by  and let J,n := cutn (J ). Select a
subset W 0  J,n that is closed under predecessors.
 For every  0  W 0 , let P be a finite set (possibly empty) of descending BCPs p starting
from  0 and let W0 contain all nodes from all p  P .
S
 Set W = W 0  0 W 0 W0 .
4
Now think of a forest quasi-model J and some query q. The following definition and
lemma employ the notion of anchored n-components to come up with the notion of quentailment (short for quasi-entailment), a criterion that reflects query-entailment in the world
of forest quasi-models.
Definition 27 (Quentailment). Let q be a conjunctive query with ](q) = n and J some
forest quasi-model of an ALCOIFb knowledge base K. We say that J quentails q (written
J | q) if, for V = var(q), J contains connected anchored n-components C1 , . . . , C` and there
C
are according functions i : V  2 i such that the following hold:
Q1 For every x  V , there is at least one Ci , such that i (x) 6= 
Q2 For all A(x)  q, we have i (x)  AJ for all i.
458

fiNominals, Inverses, Counting, and Conjunctive Queries

Q3 For every r(x, y)  q there is a Ci such that there are 1  i (x) and 2  i (y) such
that h1 , 2 i  rJ .
Q4 If, for some x  V , there are connected anchored n-components Ci and Cj with  
i (x) and  0  j (x), then there is
 a sequence Cn1 , . . . , Cnk with n1 = i and nk = j and
 a sequence 1 , . . . , k with 1 =  and k =  0 as well as m  nm (x) for all
1  m < k,
such that, for every m with 1  m < k, we have that
 Cnm contains a descending BCP p1 started by m ,
 Cnm+1 contains a descending BCP p2 started by m+1 ,
 p1 and p2 have the same path sketch.
For a union of conjunctive queries u = q1  . . .  qh , we say that J quentails u (written
J | u) if J | q for a q  {q1 , . . . , qh }.
4
Note that an anchored component may contain none, one or several instantiations of a
variable x  V . Intuitively, the definition ensures, that we find matches of query parts which
when fitted together by identifying BCP-equal elements yield a complete query match.
Lemma 28. Let u = q1  . . .  qh be a union of conjunctive queries and K an ALCOIFb
knowledge base. Then
1. For any model I of K, (I) | u implies I |= u.
2. For any strict forest quasi-model J of K that is collapsing-admissible, collapse(J ) |= u
implies J | u.
Proof.
1. Let q be a disjunct of u such that  (I) | q, V = var(q), and C1 , . . . , C` the
connected anchored n-components witnessing the quentailment. We use the function
Tail from Definition 12 and exploit its properties as a homomorphism. Note that Tail
maps nodes in (I) to the same individual in I, if they start descending BCPs with
the same path sketches. Due to condition Q4 from Definition 27, this implies, for
every x  V and every 1  i (x) and 2  j (x), that Tail(1 ) = Tail(2 ). Hence, the
total function  : V  I defined by letting (x) =  whenever Tail() =  for some
  i (x) and some i with 1  i  `, is well-defined. We now show that  is a query
match for q in I by examining the atoms of q:
 For every unary atom A(x), the definition of quentailment ensures that there
exist a Ci and a   Ci with   i (x) and   AJ . Then, by definition of Tail,
it follows that (x) = Tail()  AI .
 Likewise, for every binary atom r(x, y), the definition of quentailment ensures
that there exists a Ci and 1 , 2  Ci such that 1  i (x) and 2  i (y) as
well as h1 , 2 i  rJ . Again, by definition of Tail, it follows that h(x), (y)i =
hTail(1 ), Tail(2 )i  rI .
459

fiRudolph & Glimm

2. To prove this, we employ the alternative characterization of collapsings as established
0
0
0
in Lemma 24. Let I 0 = (I , I ) = prune(J )/ and let  : V  I be a match for
q in I 0 . We use  to construct the anchored n-components and functions needed to
show that J quentails q.
Let V   V contain those variables that  maps to a singleton -equivalence class.
We now define V = {V1 , . . . , Vm } as the finest partitioning on V  such that, for any
x, y  V  , x and y are in the same partition whenever r(x, y)  q for some r  rol(K).
Next, we assign to every partition V 0  V the set QV 0 of query atoms containing
variables from V 0 . We now construct for every V 0 an anchored n-component CV 0 and
a function V 0 (initialized as yielding  for all inputs) as follows:
 For every x  V 0 , let CV 0 contain the J -element  for which (x) = {}. Note
0
that I consists of the -equivalence classes over elements from J , i.e., {} is
0
one of the -equivalence classes from I . Moreover, set V 0 (x) = V 0 (x)  {}.
 For every r(x, y)  QV 0 with y 6 V 0 and (x) = {}, let CV 0 contain an additional element  0  (y) for which h,  0 i  rJ (existence assured by definition
of collapsing via factorization) and all elements from some descending BCP in
prune(J ) starting from  (existence assured since [ 0 ] is not a singleton class).
Moreover set V 0 (y) = V 0 (y)  { 0 }.
 Likewise, for every r(x, y)  QV 0 with x 6 V 0 and (y) = {}, let CV 0 contain an
additional element  0  (x) for which h 0 , i  rJ (existence assured by definition of collapsing via factorization) and all elements from the shortest descending
BCP in prune(J ) starting from  0 (existence assured since [ 0 ] is not a singleton
class). Moreover set V 0 (x) = V 0 (x)  { 0 }.
We furthermore construct, for each query atom a that contains no variables from V  ,
its own anchored n-component Ca and function a (again initialized to always return
) as follows:
 If a = r(x, y), let Ca contain two nodes 1 and 2 for which 1  (x) and
2  (y) and h1 , 2 i  rJ (existence assured by definition via factorization)
as well as some prune(J )-descending BCP starting from 1 and the shortest
prune(J )-descending BCP starting from 2 .
 If a = A(x), let Ca contain a node  for which   (x) and   AJ (existence
assured by definition via factorization) as well as the shortest prune(J )-descending BCP starting from .
Let C contain all CV 0 and Ca defined so far. Note that C already satisfies the conditions
Q1-Q3 of Definition 27. We now have to add some more anchored n-components in
order to satisfy condition Q4 as well. Let C0 be initially empty. For any x  V where
(x) is a non-singleton equivalence class and any two C , C  C with    (x) and
 0   (x), we have that, since  and  0 are in the same -equivalence class (x),
there is a sequence 1 , . . . , k of J -nodes with  = 1 and  0 = k and every i and
i+1 start a descending BCP having the same path sketch. We enhance C0 by one
anchored component per i which contains just i and the corresponding descending
460

fiNominals, Inverses, Counting, and Conjunctive Queries

(x1 )

r

r
A
s

{o}
E

A
s

BC
E

f

(x2 )

g

(x5 )

r

A
s

BD
E

(x3 ) f

r

r
A
s

BC
E

BD
E

g

(x4 )

r
A
s

BC
E
g

f





No E
r

C1

A
1 (x1 )
r
s
A
1 (x2 )

C2

BC
E

r
A

s
BD
2 (x5 )1 (x3 )
E

f

g

BC
E

BD
f
2 (x3 )
E

f

r
s
BC
2 (x4 )
E

A

No E
..
.

r
s

A

BD
E

r
A

s

BC
E

g

BC
E

g

BC
E

BD
E

f

BD
E

f

BD
E

f

r
..

.

s

g

g
..

No E
..
.

g
..

.

.

No E
..
.

..

.

Figure 9: Correspondence between entailment and quentailment.
BCPs. Then, by construction, the elements of CC0 constitute the necessary anchored
n-components to justify that J quentails q and, thus, J quentails u.

As an example for the correspondence between (query) entailment and quentailment,
consider the conjunctive query
q = {r(x1 , x2 ), s(x2 , x3 ), f (x4 , x3 ), s(x5 , x4 )}.
A match  for this query into our example model from Figure 2 is displayed in the upper
part of Figure 9, which witnesses I |= q. In the lower part, the anchored components C1
and C2 and according functions 1 and 2 establish (I) | q.

6. Limits and Forest Transformations
Before introducing the following constructions in detail, we will try to provide some highlevel explanation to convey the intuition behind the subsequent steps. As mentioned before,
one of the major obstacles for a decision procedure for conjunctive query entailment is that
461

fiRudolph & Glimm

for DLs including inverses, nominals, and cardinality restrictions (or alternatively functionality), there are potentially infinitely many so-called new nominals: domain elements
which can be identified by being linked to a proper nominal via a BCP.
However, we will show that for every model of a knowledge base that does not satisfy a
conjunctive query (i.e., for every countermodel), there is a nice countermodel with only
finitely many new nominals (in the subsequent section, we will then argue that this ensures
the existence of a procedure that terminates when the query is not entailed by the knowledge
base in question). We provide a construction which transforms an arbitrary countermodel
into a nice one by first unraveling it into a quasi forest model, then substituting new
nominals by uncritical elements and finally collapsing the result back into a proper model.
For doing this, we have to find appropriate substitutes for most of the new nominals. Those
substitutes have to fit in their environment without themselves introducing new nominals.
Due to the global cardinality constraints that BCPs impose on their elements, the existence of infinitely many new nominals implies that their witnessing BCPs must get
longer and longer, such that by just looking at some finite-distance neighborhood, most
of those new nominals just look like non-nominal domain elements. This state of affairs
can be exploited by essentially constructing new domain elements as environment-limits.
In a way, those new domain elements are characterized by the property that they can be
approximated with arbitrary precision by already present domain elements  possibly without themselves being present in the domain.3 We will see in the following that those new
domain elements can serve as the substitutes we are looking for.
Definition 29 (Limits of a Model). Let I = (I , I ) with   I be some model of an
ALCOIFb knowledge base K. A tree interpretation J is said to be generated by  (written:
J C ), if it is isomorphic to the restriction of (I, ) to elements of {(, cw) | (, cw) 
(I,) , c 6 H} for some H  IN.
The set of limits of I (written lim I) is the set of all tree interpretations J such that
for every k  IN, there are infinitely many   I such that cutk (L) 
= cutk (J ) for some
L C .
4
Figure 10 displays one limit element of our example model.
The following lemma gives some useful properties of limits. Besides some rather obvious compatibility considerations with respect to knowledge base satisfaction, claim 3 of
Lemma 30 provides us with the as pleasant as useful insight that the root node of a limit
can never be part of a BCP at all.
Lemma 30. Let K be an ALCOIFb knowledge base, K0 = nomFree(K), I a purified model
of K, and n some fixed natural number. Then the following hold:

1. Let L0 be a tree interpretation such that there are infinitely many   I with L0 =
cutn (L) for some LC. Then, there is at least one limit J  lim I such that cutn (J ) 
=
L0 .
2. Every J  lim I is locally K0 -consistent apart from its root (, ).
3. As an analogy, consider the fact that any real number can be approximated by a sequence of rational
numbers, even if it is itself irrational.

462

fiNominals, Inverses, Counting, and Conjunctive Queries

A
A

s

BC
E

s

BD
E

f

BD
E

s

BC
E

g

BC
E

g

BC
E

s

BD
E

f

BD
E

f

BD
E

f

BD
E

BC
E

g

BC
E

g

BC
E

g

BC
E

g

A
A
A
.

r
..

r

r

r

s

r

f

..

f
.

..

f
.

..

f
.

..

BC
E
f

.

..

.

Figure 10: One limit for the model from Fig. 2
3. For every J  lim I it holds that its root (, ) has no BCP to any (, w)  J .
4. If J  lim I contains a node  starting two backwards counting paths with path sketches
s1 and s2 , then for any element   in any unraveling holds: if the direct neighborhood
of   is isomorphic to that of  and   starts a descending BCP with path sketch s1
then it also starts a descending BCP with path sketch s2 .
5. Every J  lim I is collapsing-admissible.
Proof.
1. Let b be the branching degree of I, let Dn be the (by assumption infinite) set
of all   I such that L0 
= cutn (L) for some L C , and let Jn contain all those L.
Starting with k = n, we now iteratively increase k and construct sets Jk and Dk and
tree interpretations Lk . On our way, we inductively prove some properties.
By induction hypothesis we know that Dk is infinite and there is some Lk with Lk 
=
cutk (M) for all M  Jk . By Lemma 25, there are only finitely many non-isomorphic
tree interpretations of depth k + 1 with branching degree b, and we can partition
Jk into finitely many sets Jk,1 , . . . , Jk,m such that every two M, M0 from any Jk,i
satisfy cutk+1 (M) 
= cutk+1 (M0 ). Likewise, we can define classes Dk,1 , . . . , Dk,m with
Dk = Dk,1  . . .  Dk,m such that   Dk,i if there is an L C  with L  Jk,i . Now,
as Dk is infinite, one of the Dk,i must be infinite as well and we can set Dk+1 = Dk,i
and Jk+1 = Jk,i . Hence, we know that Dk+1 is infinite and there is some Lk+1 with
Lk+1 
= cutk+1 (M) for all M  Jk+1 .
Thus, we have established an infinite sequence Ln , Ln+1 , . . . with Li 
= cuti (Lj ) for all
j > i. Without loss of generality, we can assume that isomorphic nodes are named
identically, i.e., we even have Li = cuti (Lj ) for all j > i. Now we can define J as the
(non-disjoint) union of all Li . This way we have established the structure J for which
cutk (J ) = Lk and we know that for every k there are infinitely many  (namely all
elements from Dk ) such that cutk (L) 
= Lk for some L C . Hence this J is the limit
element with the desired properties.
2. Let (, w) with w 6=  be a node in J . Now choose a   I such that cut|w|+1 (L) 
=
cut|w|+1 (J ) for some L C  (by definition, there are even infinitely many such s to
463

fiRudolph & Glimm

choose from). Then L contains a node   whose direct neighborhood is isomorphic to
that of (, w). However, as L is contained in (I, ) and I |= K by assumption, it is
locally K0 -consistent and hence   is. Therefore (, w) is locally K0 -consistent in J .
3. Assume the contrary, i.e., that some J  lim I has a BCP from the root (, ) to some
(, w)  J with (, w)  NoJ for some o  nom(K). Since we have only functionality
and by definition of BCPs, a BCP uniquely identifies one domain individual. By
definition of lim I, however, there are infinitely many   I satisfying cut|w| (L) 
=
cut|w| (J ) for some J C  and we have an infinite number of individuals with the same
counting path to oI . This is a contradiction.
4. Choose k to be the maximum length of the two BCPs. By definition of the limit,
I contains an element  such that cut|w|+k (L) 
= cut|w|+k (J ) for some L C . Now,
let  0  L be the element that (with respect to this isomorphism) corresponds to
  J . Then,  0 is the origin of two descending BCPs with path sketches s1 and
s2 . Let Tail( 0 ) =  0 . Since path sketches of descending BCPs uniquely identify one
domain individual, every node   in any unraveling that starts a descending BCP
with path sketch s1 must have been caused by  0 as well. Furthermore (as their
direct neighborhoods are isomorphic and by the specific design of the choose function
from Definition 12 which renders all successors non-isomorphic), all successors of  
uniquely correspond to neighbors of  0 and in turn to successors of  0 .
This in turn implies that, for every successor of   , one finds a successor of  0 with
isomorphic direct neighbourhood. Yet, this synchronicity argument can be inductively
applied and thereby iterated down the BCP. Thus, we obtain that   also starts a
descending BCP with path sketch s2 .
5. We define the function ch : (cl(K)  J )  J essentially like in the proof of
Lemma 19, namely by referring to the function choose. For a given element   J
that starts a BCP of length ` in J , choose a  0  I such that cut||+` (L) 
= cut||+` (J )
for some L C  0 . As L is contained in (I,  0 ), we can now proceed and define ch(C, )
as demonstrated in the proof of Lemma 19.

Having defined limit elements as convenient building blocks for restructuring forest
quasi-interpretations, the following definition provides the first hints on where inside such
a structure one existing node (and all its successors) can safely be exchanged by a limit
element.
Definition 31 (n-Secure Replacement). Let K be an ALCOIFb knowledge base, I a model
for K, J some forest quasi-model for K with   J . A strict tree quasi-interpretation
J 0  lim I is called an n-secure replacement for  if
 cutn ((J , )) is isomorphic to cutn (J 0 ),
 for every anchored n-component of J 0 with witness  0 , there is an isomorphic anchored
n-component of J with witness .
464

fiNominals, Inverses, Counting, and Conjunctive Queries

No E
A
A

A
A
A

.
..

BC
E

BD
E

f

BD
E


=

s

BC
E

g

BC
E

g

BC
E

s

BD
E

f

BD
E

f

BD
E

f

BD
E

BC
E

g

BC
E

g

BC
E

g

BC
E

g

r
/

s

r

r

s

r

s

f

..

f
.

..

f
.

..

f
.

A
A

s

r

..

BC
E
f

.

..

s

BD
E
g

s

r

BC
E

BD
E

f

s

BC
E

g

BC
E

s

BD
E

f

BD
E

f

BC
E

g

BC
E

g

BC
E

r

r

r

f
..

A

r

s

A

.

A

..

A
r



r

.

BD
E
g

f
..

.

BD
E
g

f
..

No E
..
.
No E
..
.
No E
..
.

.

.

Figure 11: Forest quasi-model (right) and according 3-secure replacement for  (left).
If a   J has an n-secure replacement in lim I, we call  n-replaceable w.r.t. I, otherwise
we call  n-irreplaceable w.r.t. I.
4
Figure 11 displays a 3-secure replacement in the considered unraveling of our example
model.
After having defined which elements of a forest quasi-model are eligible for being replaced
by a limit element, we have to make sure that not too many elements (actually defined in
terms of the original model) are exempt from being replaced.
Lemma 32. Every purified model I of an ALCOIFb knowledge base K contains only
finitely many distinct elements that start a BCP and are the cause for n-irreplaceable nodes
in the unraveling of I.

Proof. Assume the converse: let a purified model I of K contain an infinite set D of elements
giving rise to n-irreplaceable nodes in (I). Then there must be an L0 such that there is an
infinite set D0  D such that every d0  D0 generates an L for which cutn (L) 
= L (since by
Lemma 25, there are only finitely many non-isomorphic choices for L0 ). This set D0 can be
used to guide the construction of a specific limit element J  lim I according to Lemma
30.1. Now, for an element (, w) from J starting a BCP, let l(,w)  IN be the length of
the shortest such BCP starting from (, w). Then, let k be the maximum over all l(,w) of
individuals (, w) from J that start a BCP and for which |w|  n. By construction, D0
contains one element d00 generating an L with cutk (L) 
= cutk (J ) (actually infinitely many).
By the choice of k and Lemma 30.4, we can conclude that J is an n-secure replacement for
the irreplaceable (I)-node caused by d00 which contradicts the fact that d00  D.
Now we know, which elements of a forest quasi-model can be replaced by a suitable limit
element. The following definition exactly tells us, how such a replacement is carried out:
the respective element and all its successors are deleted and the limit element (together
with its successors) is inserted at the same position.
465

fiRudolph & Glimm

No E
A
A

BC
E

BD
E

f

s

BC
E

g

BC
E

BD
E

f

BD
E

f

s

BC
E

g

BC
E

g

BC
E

s

BD
E

f

BD
E

f

BD
E

f

BD
E

BC
E

g

BC
E

g

BC
E

g

BC
E

g

A
A
A

..

.

r

f

..

r
s

r

r
s

r

r

s

r
s

A

A

r

f

..

.

f
.

..

f
.

No E
..
.
No E
..
.

..

BC
E
f

.

..

.

Figure 12: Result of replacing the element  by the 3-secure replacement depicted in Figure 11. The inserted component is highlighted.

Definition 33 (Replacement Step). Let K be an ALCOIFb knowledge base, I a model
of K, and J a forest quasi-model of K, i.e., J |= K0 = nomFree(K). Let (, w)  J be
n-replaceable w.r.t. I and J 0 an according n-replacement for (, w) from lim I with root
(, ).
We define the result of replacing (, w) by J 0 as the interpretation R where
0

J
00
00
J
J
0
0
 R = J
red  {(, ww ) | (, w )   } with red = ( \ {(, ww ) | |w | > 1})
0

0
0
J
 for each A  con(K0 ), AR = (AJ  J
red )  {(, ww ) | (, w )  A }
J
0
00
0
00
 for each r  rol(K0 ), rR = (rJ J
red red ){h(, ww ), (, ww )i | h(, w ), (, w )i 
0
rJ }
4

Figure 12 displays the result of carrying out this replacement step on our example.
The following lemma assures that during a replacement as described above, no new
anchored n-components are introduced, instead all anchored n-components present after an
n-secure transformation were present before or completely contained in the inserted limit
element.
Lemma 34. Let K be an ALCOIFb knowledge base, I a model for K, J a forest quasimodel for K, i.e., J |= K0 = nomFree(K), and let (, w)  J be n-replaceable w.r.t. I. Let
J 0 be an n-replacement for (, w) with root (, ) and R be the result of replacing (, w) by
J 0 . Then the following hold:
1. cutn ((J , (, w))) is isomorphic to cutn ((R, (, w))).
466

fiNominals, Inverses, Counting, and Conjunctive Queries

2. If n  1, then R is locally K0 -consistent.
3. Whenever R contains an anchored n-component C, then one of J or J 0 contains an
anchored n-component isomorphic to C.
Proof.

1. This is a direct consequence from Definitions 31 and 33.

2. We make a case distinction when element-wise investigating local consistency of R
(note that K and K0 are simplified and that local consistency of a node (, v)  R
depends only on this node and its direct neighbors):
 v = ww0 for some w0 6= : then the direct neighborhood of (, v) in R is isomorphic to the direct neighborhood of (, w0 ) in J 0 (recall that (, ) is the root of
J 0 ). By Lemma 30.2, J 0 is locally K0 -consistent except possibly for (, ). Hence
also (, v) is locally K0 -consistent in R.
 v 6= ww0 for any w0 , i.e., (, v) was not affected by the replacement: then the
direct neighborhood of (, v) has not changed by the replacement and, therefore,
the neighborhoods of (, v) in J and R coincide. As J is locally K0 -consistent
by assumption, so is (, v) in R.
 v = w: in that case, the direct neighborhood of (, v) has changed but remained
isomorphic. This follows from the preceding statement (34.1).
3. Let (0 , w0 ) be the witness of C. We distinguish three cases:
 0 =  and w is a prefix of w0 . Then, clearly C is completely contained in J 0 .
 0 =  and w0 is a prefix of w. Let C 0 be the structure obtained by restricting C
to all elements of the form (, ww00 ) and then renaming every element (, ww00 )
to (, w00 ), where (, ) is the root of J 0 . Then C 0 is an anchored n-component
in J 0 with witness (, ). Now, by definition of replacing, J must contain an
isomorphic copy of C 0 with witness (, w). Since the other part of C (consisting
of those nodes (0 , w0 ) such that w is not a prefix of w0 ) has not been altered by
the replacement, we can conclude that J must contain an isomorphic copy of C.
 Neither of the above. Then, (0 , w0 ) and the subtree rooted in (0 , w0 ) is contained
in J as this part of J has not been affected by the replacement. Then, clearly
also C is contained in J .

We are now ready for defining the whole process of restructuring a forest quasi-model
essentially by substituting as many nodes as possible by appropriate limit elements.
Definition 35 (n-Secure Transformation). Let I be a model of some ALCOIFb knowledge
base K and J an unraveling for I. An interpretation J 0 is called an n-secure transformation
of J if it is obtained by (possibly infinitely) repeating the following step:
Choose one unvisited and w.r.t. tree-depth minimal node (, w) that is n-replaceable
w.r.t. I. Replace (, w) with one of its n-secure replacements from lim I and mark (, w)
as visited.
4
467

fiRudolph & Glimm

B
B
No E f C E g D E
A
A
A
A

BD
E

f

BD
E

s

BC
E

g

BC
E

g

BC
E

s

BD
E

f

BD
E

f

BD
E

f

BD
E

BC
E

g

BC
E

g

BC
E

g

BC
E

g

r

r

r
.

r

s

A

..

s

r

BC
E

r

s

s

r

s

A

A

r

f

..

f
.

..

f
.

..

f
.

..

BC
E
f

.

..

.

Figure 13: Result of collapsing the forest quasi-model displayed in Figure 12.
Note that this is well-defined as every node is visited at most once and no formerly
irreplaceable node ever becomes replaceable. Hence for every k  IN, the initial segment
cutk (J ) of the current intermediate structure J is already isomorphic to the initial segment
cutk (J 0 ) of J 0 after a bounded number of replacement steps, due to the fact that all involved
structures have bounded branching degree.
By now, the whole effort might still look a bit contrived and pointless, however, the
following lemma establishes a bunch of properties that in the end allow us to deduce the
existence of a very well-behaved countermodel whenever there is any at all.
We show that the process of unraveling, n-secure transformation and collapsing preserves the property of being a model of a knowledge base and (with the right choice of n)
also preserves the property of not entailing a conjunctive query. Moreover, this model conversion process ensures that the resulting model contains only finitely many new nominals
(witnessed by a bound on the length of BCPs). Figure 13 illustrates these properties for our
example model. Note that only two new nominals are left whereas collapsing the original
unraveling yields infinitely many.
Lemma 36. Let I be a purified model of some ALCOIFb knowledge base K, J an unraveling of I, and J 0 an n-secure transformation of J . Then the following hold:
1. J 0 is a strict forest quasi-model for K.
2. J 0 is collapsing-admissible.
3. collapse(J 0 ) is a model of K.
4. There is a natural number m such that J 0 does not contain any node whose shortest
descending BCP has a length greater than m.
468

fiNominals, Inverses, Counting, and Conjunctive Queries

5. If J 0 contains an anchored n-component C, then J contains an anchored n-component
isomorphic to C.
6. If, for some union of conjunctive queries u = q1  . . .  qh , we have J |6 u and n >
maxq{q1 ,...,qh } ](q), then J 0 |6 u.
7. If, for some union of conjunctive queries u = q1  . . .  qh , we have I 6|= u and
n > maxq{q1 ,...,qh } ](q), then collapse(J 0 ) 6|= u.
Proof.
1. Let K0 = nomFree(K). Due to Lemma 13, J is a strict forest quasi-model
for K. By Lemma 34.2, each replacement step preserves local K0 -consistency and results, thus, in a forest quasi-model for K. Since each n-replacement is a strict tree
quasi-interpretation also strictness is preserved. By induction it follows that every interpretation produced in the n-secure transformation procedure is a strict forest quasimodel for K. For every node in J 0 , its direct predecessor and direct successors have
not changed any more after finitely many replacement steps and local K0 -consistency
depends solely on those neighbors. Hence J 0 is also locally K0 -consistent.
2. By Lemma 19, J is collapsing-admissible, by Lemma 30.5 every limit of I is. Moreover, as is obvious from the proofs of both propositions, it is possible to define the
respective ch-functions recurring to the original choose-function on I, hence every
two elements from (even different) unravelings or limits that start descending BCPs
with identical path sketches correspond to the very same element in I whence the
separate ch-functions are compatible with each other. Therefore, replacing an element in the unraveling yields a strict forest quasi-model that is collapsing-admissible.
Applying the same argument inductively yields that every intermediate strict forest
quasi-model during the n-secure transformation is collapsing-admissible. Finally, as
the according ch-function stabilizes after finitely many replacement steps (together
with the neighborhood of the considered elements), also J 0 is collapsing-admissible.
3. This follows from the two previous facts (36.1 and 36.2) together with Lemma 20.
4. Consider the set D of all   I causing n-irreplaceable nodes in J . By Lemma 32,
D is finite. We obtain D0 by removing all  from D that do not start any descending
BCPs.
For   D0 , let dBCP() denote the set of descending BCPs starting in  and choose

m := max0
D

min


|p|

pdBCP()

Now assume there were a  0  I having a shortest descending BCP of length greater
than m. Obviously, as  0 6 D0 , there must be a (, w) generated by  0 that is nreplaceable. However, during the n-secure transformation all n-replaceable elements
have been replaced by elements that do not start any descending BCPs at all due to
Lemma 30.3.
469

fiRudolph & Glimm

5. We prove this by induction on the replacement steps of the n-secure transformation
process by showing that this is true for every intermediate replacement result R0 .
The claim for J 0 then follows from the fact that, for every considered C (which is
always finite), only a finite part cut` (J 0 ) is relevant and that for every `, there is a
bounded number of replacement steps after which we have cut` (R0 ) = cut` (J 0 ) for
every further intermediate R0 .
As base case (zero replacement steps carried out), we find that for R0 = J , the claim
is trivially true.
Now assume that the claim has been established for R and has to be shown for R0
that is created by replacing (, w) in R with some J 00 . By Lemma 34.3, we then know
that one of the following is the case:
 R contains C. Yet, we can apply the induction hypothesis and conclude that also
J contains C as claimed.
 J 00 contains C. But, since C is finite, it is already contained in cutk (J 00 ) for some
k  IN and, as J 00 is a limit element, we find one   I with cutk ((I, )) =
cutk (J 00 ). Since I is purified, we find a (, w)  J that corresponds to , i.e.,
J contains an isomorphic copy of (I, ) which in turn contains an isomorphic
copy of C.
6. This is actually a straightforward consequence from the preceding proposition and the
definition of quentailment.
For the indirect proof, we suppose J |6 u and n > maxq{q1 ,...,qh } ](q) and J 0 | u, the
latter witnessed by J 0 | q for a q  {q1 , . . . , qh }. By definition, the latter assures the
existence of adequate anchored n-components in J 0 . Then, applying the preceding
proposition (36.5), we obtain that isomorphic copies of all those anchored n-components are contained in J which, by definition, just means J | q and, therefore, J | u.
Hence, we have a contradiction, which proves the claim.
7. We prove this indirectly, so assume I 6|= u, n > maxq{q1 ,...,qh } ](q), and collapse(J 0 ) |=
u, witnessed by collapse(J 0 ) |= q for a q  {q1 , . . . , qh }.
Then, from Lemma 28.2, it follows that J 0 | q. By the previous proposition (36.6),
we conclude J | q, which in turn implies I |= q by Lemma 28.1. This implies I |= u,
a contradiction.

Now we are able to establish our first milestone on the way to showing finite representability of countermodels.
Theorem 37. For every ALCOIFb knowledge base K with K 6|= u, there is a forest model
I of K with finitely many roots such that I 6|= u. Moreover, I has bounded branching degree.

470

fiNominals, Inverses, Counting, and Conjunctive Queries

Proof. Let u = q1  . . .  qh . Since an inconsistent knowledge base entails every query, we
can assume that K is consistent and, since K 6|= u, there is a model I of K with I 6|= u.
Choose an n > maxq{q1 ,...,qh } ](q) and let J 0 be obtained by carrying out an n-secure
transformation on (I) and let I 0 = collapse(J 0 ). We know that I 0 is a model of K (via
Lemma 36.3) and that I 0 6|= u (by Lemma 36.7).
By Lemma 36.4, we know that there is a fixed natural number m such that the shortest
descending BCP started by any node in J 0 is shorter than m. Note that there are only
finitely many path sketches of length  m. This means that every node in J 0 that starts
a descending BCP at all can be assigned to one such path sketch. However, this entails
that there are only finitely many elements (i.e., -equivalence classes) in I 0 that contain
J 0 -elements starting descending BCPs in J 0 . This implies, via Lemma 24, that I 0 contains
only finitely many roots.
The fact that I 0 has bounded branching degree is a direct consequence from the fact
that the initial unraveling has bounded branching degree, that replacement do not change
the branching degree nor do collapsings as assured by Lemma 20.

7. Finite Representations of Models
In this section, we show how we can construct a finite representation of a forest model
of a knowledge base that has only a finite number of roots. We then show that these
finite representations can be used to check query entailment. In order to do this, we use a
technique that is very similar to the blocking techniques used in tableau algorithms (see, e.g.,
Horrocks & Sattler, 2007). A tableau algorithm builds a so-called completion graph that is
a finite representation of a model. A completion graph has essentially the same structure
as our forest quasi-models. It contains root nodes for the nominals occurring in the input
knowledge base plus further root nodes for new nominals that start BCPs. Each (new and
old) nominal is the root of a tree, and relations only occur between direct neighbors within a
tree, between elements within a tree and a root, or between the roots. An initial completion
graph contains only nodes for the nominals occurring in the input knowledge base. Concepts
are expanded according to a set of expansion rules, and new nodes are added to the graph
when expanding existential restrictions. New nominals are added by the so-called NN-rule
whenever an element from within a tree has a relationship with an inverse functional role
to a root node that represents a nominal from the input knowledge base, i.e., when a BCP
is created. In order to obtain a finite representation, tableau algorithms usually employ
some cycle detection mechanism, called blocking. Otherwise the depth of the trees and
the number of new nominals might grow infinitely. For logics as expressive as ALCOIFb,
blocking usually requires two pairs of elements. In our notation, a (non-root) node n with
predecessor n0 blocks a node m with predecessor m0 , if hn0 , ni 
= hm0 , mi. In order to
obtain a real model from the finite representation, the part between n and m is copied
and appended infinitely often. We use a similar technique to obtain a finite representation
for a forest model. Since we want to preserve non-entailment, working with just pairs of
elements is not sufficient. Instead, we take the length n of the query into account and use
isomorphic trees of depth n to define blocking. This technique has first been employed for
deciding query entailment in ALCN with role conjunctions (Levy & Rousset, 1998) and
has recently been extended to the logics ALCHIQ, ALCHOQ, and ALCHOI (Ortiz, 2008;
471

fiRudolph & Glimm

Ortiz et al., 2008a) and extends, as our result, to the DLs SHIQ, SHOQ, and SHOI (i.e.,
with transitivity) as long as the query contains only simple roles.
As for forest quasi-interpretations, we use isomorphisms between forest interpretations
or parts of them.
Definition 38 (Isomorphism between Forest Interpretations). Let K be an ALCOIFb
0
0
knowledge base and I = (I , I ), I 0 = (I , I ) two forest interpretations of K. Without
loss of generality, we assume from now on that each root  = (, )  I is in the extension
of a unique concept N that does not occur in con(K). Then I and I 0 are called isomorphic
0
w.r.t. K, written: I 
=K I 0 , iff there is a bijection  : I  I such that:
 1 is a successor of 2 iff (1 ) is a successor of (2 ) for all 1 , 2  I ,
0

 h1 , 2 i  rI iff h(1 ), (2 )i  rI for all 1 , 2  I and r  rol(K),
0

   AI iff ()  AI for all   I and A  con(K)  {N |  = (, )  I }.
0

  = oI iff () = oI for all   I and o  nom(K).
4
Usually, we omit the subscript K from 
=K and assume that it is clear from the context.
Definition 39 (n-Blocking). Let n  IN be a fixed natural number and I = (I , I ) with
(, w)  I , w 6=  a forest interpretation for some ALCOIFb knowledge base K. An
n-blocking-tree w.r.t. (, w), denoted blocknI (, w), is the interpretation obtained from I by
restricting I to elements in {(, ww0 ) | |w0 |  n}  {(, ) | (, )  I }. An n-blocking-tree
blocknI (, w) n-blocks an n-blocking-tree blocknI (, ww0 ) if
1. blocknI (, w) and blocknI (, ww0 ) have disjoint domains except for root elements,
2. there is a bijection  from elements in blocknI (, w) to elements in blocknI (, ww0 ) that
witnesses blocknI (, w) 
= blocknI (, ww0 ), and
3. for each descendant (, wv) of (, w), there is no inverse functional role f and root
(, )  I such that h(, wv), (, )i  f I .
A node (, v)  I is n-blocked, if (, v) is either directly or indirectly n-blocked ; (, v) is
indirectly n-blocked, if one of its ancestors is n-blocked; (, v) is directly n-blocked if none
of its ancestors is n-blocked and (, v) is a leaf of some n-blocking-tree blocknI (, ww0 ) in I
that is n-blocked; in this case we say that (, v) is (directly) n-blocked by  (, ww0 ) for 
the bijection witnessing 
=.
Without loss of generality, we assume that the n-blocking-trees used above are minimal
w.r.t. the order of elements in I (cf. Definition 16).
A forest interpretation I = (I , I ) for K is an n-representation of K if
1. I is finite,
2. I contains no indirectly n-blocked nodes,
3. for each o  nom(K), there is one element of the form (, )  I such that oI =
(, )I ,
472

fiNominals, Inverses, Counting, and Conjunctive Queries

4. each element that is not directly n-blocked is locally K-consistent.
4
Note that n = 1 is more restrictive than standard pairwise blocking since two trees
of depth one need to be isomorphic before blocking occurs, whereas standard blocking
already occurs for two isomorphic pairs of nodes. For DLs as expressive as ALCOIFb,
however, n has to be greater than 0 (at least trees of depth 1) if we want to transform
n-representations into models of the knowledge base. We now show that each knowledge
base has an n-representation for some fixed n  IN and, afterwards, that we can use an
n-representation to build a model for the knowledge base.
Lemma 40. Let K be a consistent ALCOIFb knowledge base and u = q1  . . .  qh a union
of conjunctive queries and n a fixed natural number greater than max1ih |qi |. If K 6|= u,
then there is an n-representation of K that does not satisfy u.
Proof. By assumption, K is consistent and K 6|= u. Then, by Theorem 37, there is a forest
model I of K with finitely many roots and branching degree bounded in |cl(K)|, and for all
q  {q1 , . . . , qh } holds I 6|= q. We show that we can find an n-representation R for I.
We use a similar argumentation as in Lemma 25 to show that there are only finitely many
non-isomorphic n-blocking trees. We again denote this bound by Tn . Let c = |cl(K)|, r =
|rol(K)|, and m the (finite) number of roots in I. Each root   I is annotated with a
special concept N by assumption. For n = 0, we again have 2c choices. For n > 0, each
element can have between 0 and c successors and between 0 and m relations with roots.
For roots we have 2c+m choices for the concepts. We use 2cm as bound for the choice of
concepts for roots and this clearly bounds the choice for non-roots as well. Each non-root
node in a level smaller than n is the root of a tree with depth n  1 and each node in the
sub-tree can again have up to m relations to a root. Assuming that we have only a single
cm ) for the number of non-isomorphic
role name r  rol(K), we get a bound of O(2c cmTn1
sub-trees of depth n with relations to the at most m roots. Since we have not only one
cm )r ). We now abbreviate 2c (cm)r
but a choice of r roles, we get a bound of O(2c (cmTn1
with x and cmr with a and rewrite the obtained bound as Tn = O(x(Tn1 )a ). Unfolding
n
n1
n
n
n
yields Tn = O((x1+a+...+a )(T0 )a ) which is bounded by O((xa )(2c )a ) = O((x2c )a ).
n
By expanding the abbreviated symbols, we obtain a bound for Tn of O((2c (cm)r )(cmr) ).
Together with the fact that I is obtained from a collapsing and relations from elements
within a tree to a root in collapsings are never for inverse functional roles, this shows
that there is an n-representation of I because for each tree rooted in a node (, )  I
with depth greater than Tn , there are two nodes (, w) and (, ww0 ) such that blocknI (, w)
n-blocks blocknI (, ww0 ), and we can simply discard indirectly n-blocked nodes from I to
obtain the desired n-representation.
Since I 6|= q and the n-representation is a restriction of I, non-entailment of q is clearly
preserved.
Please note that we would not obtain such a bound if we had not fixed a bound on the
number of new nominals (roots) beforehand and that we cannot use the standard tableau
algorithms to obtain this result. The reason for this is that the number of new nominals
(roots) in the tableau algorithms depends on the length of the longest path before blocking
473

fiRudolph & Glimm

occurs. For our n-blocking-trees, however, we also have to consider relations back to the
roots, which means that blocking occurs the later the more roots we have. On the other
hand, delaying blocking may lead to the introduction of more and more new roots. Due to
this cyclic argument, termination cannot be guaranteed for the tableau algorithms unless
we have fixed a bound on the number of new nominals beforehand. This is also the reason
why the tableau algorithm for entailment of conjunctive queries with only simple roles in the
query of Calvanese et al. (2009) is sound, complete, and terminating on SHIQ, SHOQ,
and SHOI knowledge bases, but is not guaranteed to terminate on SHOIQ knowledge
bases (transitivity, i.e., having a DL with S instead of ALC does not have any impact on
this).
We now show, how we can obtain a model for a knowledge base K from some nrepresentation of K. We use a technique that is directly inspired from tableau algorithms
and resembles the process of building a tableau from a complete and clash-free completion
graph. In particular the tableau algorithm by Ortiz et al. (Ortiz, 2008; Ortiz et al., 2008a)
is very similar as it also uses tree blocking.
Definition 41 (Models for n-Representations). Let R = (R , R ) be an n-representation
0
0
of some ALCOIFb knowledge base K. Let s = 11 , . . . , m
be a sequence of pairs of elements
m
R
0
from  . With |s| we denote the length m of s. For such a sequence s, we set last (s) = m
0

0

0

0

m+1
and last (s) = m . By s | m+1
we denote the sequence 11 , . . . , m
, m+1 .
m m+1
The set of R-induced elements, denoted elem(R), is inductively defined as follows:

 If  = (, )  R , then




 elem(R).

 If s  elem(R),  = (, w)  R ,  is not n-blocked, and  is a successor of last (s),
then s |   elem(R).
 If s  elem(R),  = (, w)  R ,  is directly n-blocked by some  0  R , and  is a
0
successor of last (s), then s |   elem(R).
We define the interpretation I = (I , I ) induced by R as follows:
 I = elem(R),
 for each s  I and A  con(K), s  AI iff last (s)  AR ,
 for each s  I and o  nom(K), s = oI iff last (s) = oR ,
 for each s, s0  I and r  rol(K), rI =
{hs, s0 i | s0 = s |
{hs, s0 i | s = s0 |
{hs, s0 i | s0 =
{hs, s0 i | s =

0

0


and hlast (s), last (s0 )i  rR }
and hlast (s), last (s0 )i  rR }



 and hlast (s), i

 0
 and h, last (s )i

 rR }
 rR }.
4

The interpretation of nominals is well-defined since n-representations are forest interpretations for K (hence, there is a unique root for each nominal) and pairs  with  = (, )
are never appended to sequences in elem(R).
474

fiNominals, Inverses, Counting, and Conjunctive Queries

Lemma 42. Let K be a consistent ALCOIFb knowledge base, u = q1  . . .  qh a union of
conjunctive queries, and n  1 a fixed natural number greater than max1ih |qi |. If R is
an n-representation of K such that R 6|= u, then there is a model I of K such that I 6|= u.
The proof is essentially as the one by Ortiz et al. (2008a), but adapted to our case, where
we work completely on interpretations. Our n-representations correspond to completion
graphs and our models to tableaux in their case.
Proof. Let I be an interpretation induced by R. Since n-representations do not contain
relations from an element within a tree to a root for an inverse functional role by definition,
functionality restrictions are not violated in I. Further, since K is simplified and R is a forest
interpretation for K such that all elements apart from (directly) n-blocked ones are locally
K-consistent, it is quite straightforward that each element in the induced interpretation is
locally K-consistent. Together with the restriction on nominals (property 3), this implies
that I is a model for K. This is essentially the same principle as the one used to prove that
tableaux constructed from completion graphs are proper representations of models of the
input knowledge base.
Assume, to the contrary of what is to be shown, that I |= u. Then there is a disjunct
q  {q1 , . . . , qh } and a match  for q such that I |= q. We use  to construct a match  for
q in R by shifting the mapping for variables into parts that have no direct counterpart
in R upwards.
We define the match graph G for q in I as an undirected graph containing a node s for
each s  I such that (x) = s for some x  var(q) and containing an edge hs, s0 i for each
s, s0  I such that there is an atom r(x, y)  q, (x) = s, and (y) = s0 . We call nodes of
G that correspond to roots in I root nodes of G (i.e., nodes s such that s =  ) and we
call all other nodes tree nodes. Note that the restriction of G to tree nodes is a set of trees
that we refer to as G1 , . . . , Gk and that each such tree has a depth smaller than n.
For each x  var(q) such that (x) =  (  is a root node in G), we set (x) = last (  ).
Note that  is a root node in R.
For each Gi  {G1 , . . . , Gk }, we distinguish two situations:
1. Gi contains a node s such that last (s) 6= last (s) (i.e., Gi contains a path from within
an n-blocking tree to a copy of the path starting from the node that blocks). Due
to the use of n-blocking, a single tree Gi can never cover more than one n-blocking
tree and it can use at most nodes from two n-blocking trees (leaving one and then
entering the next one in less than n steps). For each node s0 in Gi such that |s0 | < |s|
and x  var(q) such that (x) = s0 , we set (x) = (last (s0 )). For each s0 in Gi with
|s0 |  |s| and x  var(q) such that (x) = s0 , we set (x) = last (s0 ).
2. Gi contains no node s such that last (s) 6= last (s) (i.e., Gi contains a path that lies
completely within an n-blocking tree or from a path outside of an n-blocking-tree into
an n-blocking-tree). For each node s in Gi and x  var(q) such that (x) = s, we set
(x) = last (s).
By definition of , I as an induced model of R, and n-blocking, we immediately have
that, for each A(x)  q, (x)  AR . We show that, for each r(x, y)  q, h(x), (y)i  rR ,
which proves R |= q. We distinguish three cases:
475

fiRudolph & Glimm

1. (x) =  for some   R . Then (x) =  = (, )  R . We distinguish three cases
for (y):
0

(a) (y) = 0 is also a root, then (y) =  0 = (0 , )  R and, since  is a match
for q in I and by definition of I as an induced interpretation of R, we have that
h(x), (y)i = h,  0 i  rR .
0

(b) (y) is a successor of (x) in I, i.e., (y) = s =  | 0 . Then s is not n-blocked
and (y) =  0 = (0 , c)  R for c  IN. Again, since  is a match for q
in I and by definition of I as an induced interpretation of R, we have that
h(x), (y)i = h,  0 i  rR .
0

(c) (y) is neither a root ((y) 6= 0 for any  0  R ) nor a successor of (x) in I
0
((y) 6=  | 0 for any  0  R ). Then (y) belongs to some graph match component Gi and (y) = last ((y)) or (y) =  (last ((y))). Since the isomorphism
between n-blocking trees also takes the relations to root nodes into account and
other parts have direct counterparts in R, we have that h(x), (y)i  rR .
0

2. (x) = s 6=  for any   R . The cases when (y) = 0 for some  0  R is as
above. We assume, therefore, that (y) = s0 with |s0 | > 1. By definition of I, this
means that either s = s0 | 0 or s0 = s| 0 for some ,  0  R . We assume s0 = s| 0 . The
opposite case is analogous. By definition of the match graph G, there is a component
Gi of G that contains both s and s0 . We distinguish two cases:
(a) The component Gi contains a node s such that last (s) 6= last (s). The most
interesting case is when last ((y)) 6= last ((y)), i.e., s = s0 . Then (x) =
 (last (s)) and (y) = last (s0 ). Since last (s0 ) 6= last (s0 ), we have that last (s0 )
is the node that directly n-blocks last (s0 ) and, by definition of the bijection
, which witnesses the isomorphism, we have that (x) =  (last (s)) is the
predecessor of (y) = last (s0 ) and, by definition of I from R, that h(x), (y)i 
rR .
(b) The component Gi contains no node s such that last (s) 6= last (s). Then (x) =
last ((x)) and (y) = last ((y)). By definition of I from R, we immediately
have that h(x), (y)i  rR .
In any case, we have that h(x), (y)i  rR , which implies R |= q contradicting the initial
assumption.
Now Lemma 40 guarantees that, in case K 6|= q, there is always a finite n-representation
R for K such that R 6|= q and Lemma 42 guarantees that R can be transformed into a
model I of K such that I 6|= q. This suffices to show that we can enumerate all (finite)
n-representations for K and check whether they entail a disjunct of the union of conjunctive
queries. Together with the semi-decidability result for FOL, we get the following theorem.
Theorem 43. Let K be an ALCOIFb knowledge base and u = q1  . . .  qh a union of
conjunctive queries. The question whether K |= u is decidable.

476

fiNominals, Inverses, Counting, and Conjunctive Queries

8. Conclusions
We have solved the long-standing open problem of deciding conjunctive query entailment in
the presence of nominals, inverse roles, and qualified number restrictions. We have shown
that the problem is decidable by providing a decision procedure and proving its correctness.
Since the approach is purely a decision procedure, the computational complexity of the
problem remains open.
Our result also shows decidability of entailment of unions of conjunctive queries in
SHOIQ and SROIQ (underlying OWL DL and OWL 2) if we disallow non-simple roles
as binary query predicates. We thereby have reached a first important milestone towards
tackling the problem of conjunctive queries for OWL 1 DL and OWL 2 DL.
Entailment of unions of conjunctive queries is also closely related to the problem of
adding rules to a DL knowledge base, e.g., in the form of Datalog rules. Augmenting a
DL KB with an arbitrary Datalog program easily leads to undecidability (Levy & Rousset,
1998). In order to ensure decidability, the interaction between the Datalog rules and the
DL knowledge base is usually restricted by imposing a safeness condition. The DL+log
framework (Rosati, 2006a) provides the least restrictive integration proposed so far and
Rosati presents an algorithm that decides the consistency of a DL+log knowledge base
by reducing the problem to entailment of unions of conjunctive queries. Notably, Rosatis
results (2006a, Thm. 11) imply that the consistency of an ALCHOIQb knowledge base
extended with (weakly-safe) Datalog rules is decidable if and only if entailment of unions
of conjunctive queries in ALCHOIQb is decidable, which we have established.
Corollary 44. The consistency of ALCHOIQb+log-knowledge bases (both under FOL
semantics and under non-monotonic semantics) is decidable.
Another related reasoning problem is query containment. Given a schema (or TBox) S
and two queries q and q 0 , we have that q is contained in q 0 w.r.t. S iff every interpretation
I that satisfies S and q also satisfies q 0 . It is well known that query containment w.r.t.
a TBox can be reduced to deciding entailment for unions of conjunctive queries w.r.t.
a knowledge base (Calvanese et al., 1998a). Decidability of unions of conjunctive query
entailment in ALCHOIQb implies, therefore, also decidability of query containment w.r.t.
an ALCHOIQb TBox.
There are two obvious avenues for future work. We will embark on extending our
results in order to allow non-simple roles as query predicates. This is a non-trivial task
as our current approach heavily relies on a certain locality of query matches, which has
to be relinquished when considering non-simple roles. On the other hand, we are eager to
determine the associated computational complexities and provide techniques that can form
the basis for implementable algorithms.

Acknowledgments
During his stay in Oxford where our collaboration started, Sebastian Rudolph was supported
by a scholarschip of the German Academic Exchange Service (DAAD). Continuative work on
the subject was enabled by funding through the ExpresST project of the German Research
Foundation (DFG).
477

fiRudolph & Glimm

Birte Glimm was supported by EPSRC in the project HermiT: Reasoning with Large
Ontologies.
We thank the three anonymous reviewers for their numerous helpful comments.
We thank Ian Pratt-Hartmann for (unknowingly) smashing our graphomata, Maria Magdalena Ortiz de la Fuente for establishing the competitive atmosphere, Yevgeny Kazakov
for breath-taking discussions on black holes, Boris Motik for his motivating considerations
on the value of the academic life, and last not least God for providing us with extraordinary
weather and  most notably  infinity.

References
Baader, F. (2003). Terminological cycles in a description logic with existential restrictions.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI 2003), pp. 325330.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proceedings of the
19th International Joint Conference on Artificial Intelligence (IJCAI 2005). Morgan
Kaufmann, Los Altos.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (2003).
The Description Logic Handbook. Cambridge University Press.
Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & Stein, L. A. (2004). OWL web ontology language reference. Tech.
rep., World Wide Web Consortium. http://www.w3.org/TR/2004/REC-owl-ref-20040210/.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:
Tractable description logics for ontologies. In Veloso, M. M., & Kambhampati, S.
(Eds.), Proceedings of the 20th National Conference on Artificial Intelligence (AAAI
2005), pp. 602607. AAAI Press/The MIT Press.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and efficient query answering in description logics: The DL-Lite family.
Journal of Automated Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998a). On the decidability of query
containment under constraints. In Proceedings of the 17th ACM SIGACT SIGMOD
Symposium on Principles of Database Systems (PODS 1998), pp. 149158. ACM Press
and Addison Wesley.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998b). Description
logic framework for information integration. In Proceedings of the 6th International
Conference on the Principles of Knowledge Representation and Reasoning (KR 1998).
Calvanese, D., Eiter, T., & Ortiz, M. (2007). Answering regular path queries in expressive description logics: An automata-theoretic approach. In Proceedings of the 22th
National Conference on Artificial Intelligence (AAAI 2007).
Calvanese, D., Eiter, T., & Ortiz, M. (2009). Regular path queries in expressive description
logics with nominals. In Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI 2009), pp. 714720. AAAI Press/The MIT Press.
478

fiNominals, Inverses, Counting, and Conjunctive Queries

Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation of conjunctive queries
in relational data bases. In Proceedings of the 9th ACM Symposium on Theory of
Computing (STOC 1977), pp. 7790. ACM Press and Addison Wesley.
Eiter, T., Lutz, C., Ortiz, M., & Simkus, M. (2009). Query answering in description logics
with transitive roles. In Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI 2009), pp. 759764. AAAI Press/The MIT Press.
Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2008a). Conjunctive query answering for
the description logic SHIQ. Journal of Artificial Intelligence Research, 31, 151198.
Glimm, B., Horrocks, I., & Sattler, U. (2008b). Unions of conjunctive queries in SHOQ.
In Proceedings of the 11th International Conference on the Principles of Knowledge
Representation and Reasoning (KR 2008). AAAI Press/The MIT Press.
Glimm, B., & Rudolph, S. (2010). Status QIO: Conjunctive query entailment is decidable.
In Proceedings of the 12th International Conference on the Principles of Knowledge
Representation and Reasoning (KR 2010), pp. 225235. AAAI Press/The MIT Press.
Godel, K. (1929). Uber die Vollstandigkeit des Logikkalkuls. Ph.D. thesis, Universitat Wien.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). The foundational model of anatomy in
OWL: Experience and perspectives. Journal of Web Semantics, 4 (3).
Goodwin, J. (2005). Experiences of using OWL at the ordnance survey. In Proceedings
of the 1st OWL Experiences and Directions Workshop (OWLED 2005), Vol. 188 of
CEUR Workshop Proceedings. CEUR (http://ceur-ws.org/).
Gradel, E. (2001). Why are modal logics so robustly decidable?. In Paun, G., Rozenberg,
G., & Salomaa, A. (Eds.), Current Trends in Theoretical Computer Science, Entering
the 21th Century, Vol. 2, pp. 393408. World Scientific.
Grahne, G. (1991). Problem of Incomplete Information in Relational Databases. Lecture
Notes in Computer Science. Springer-Verlag.
Horrocks, I., & Sattler, U. (2005). A tableaux decision procedure for SHOIQ. In Proceedings
of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005).
Horrocks, I., & Sattler, U. (2007). A tableau decision procedure for SHOIQ. Journal of
Automated Reasoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning with Individuals for the Description
Logic SHIQ. In McAllester, D. (Ed.), Proceedings of the 17th Conference on Automated Deduction (CADE 2000), No. 1831 in Lecture Notes in Artificial Intelligence,
pp. 482496. Springer-Verlag.
Jet Propulsion Laboratory, C. I. o. T. (2006). Semantic web for earth and environmental
terminology (SWEET).. http://sweet.jpl.nasa.gov/.
Kazakov, Y. (2008). RIQ and SROIQ are harder than SHOIQ. In Proceedings of the
11th International Conference on the Principles of Knowledge Representation and
Reasoning (KR 2008). AAAI Press/The MIT Press.
Kazakov, Y., & Motik, B. (2008). A resolution-based decision procedure for SHOIQ.
Journal of Automated Reasoning, 40 (23), 89116.
479

fiRudolph & Glimm

Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Conjunctive queries for a tractable fragment
of OWL 1.1. In Proceedings of the 7th International Semantic Web Conference (ISWC
2007), Vol. 4825 of Lecture Notes in Computer Science, pp. 310323. Springer-Verlag.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2008). ELP: Tractable rules for OWL 2. In
Proceedings of the 8th International Semantic Web Conference (ISWC 2008), Vol.
5318 of Lecture Notes in Computer Science, pp. 649664. Springer-Verlag.
Lacy, L., Aviles, G., Fraser, K., Gerber, W., Mulvehill, A., & Gaskill, R. (2005). Experiences
using OWL in military applications. In Proceedings of the 1st OWL Experiences and
Directions Workshop (OWLED 2005). CEUR (http://ceur-ws.org/).
Levy, A. Y., & Rousset, M.-C. (1996). CARIN: A representation language combining horn
rules and description logics. In Proceedings of the 12th European Conference on Artificial Intelligence (ECAI 1996), pp. 323327.
Levy, A. Y., & Rousset, M.-C. (1998). Combining horn rules and description logics in
CARIN. Artificial Intelligence, 104 (12), 165209.
Lutz, C. (2008). The complexity of conjunctive query answering in expressive description
logics. In Proceedings of the International Joint Conference on Automated Reasoning
(IJCAR 2008), pp. 179193. Lecture Notes in Computer Science.
McGuinness, D. L., & Wright, J. R. (1998). An industrial strength description logic-based
configuration platform. IEEE Intelligent Systems, 13 (4).
Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning for description logics. Submitted to a journal. http://www.hermit-reasoner.com/publications/
msh08hypertableau-journal.pdf.
Ortiz, M. (2008). Extending CARIN to the description logics of the SH family. In Proceedings of Logics in Artificial Intelligence, European Workshop (JELIA 2008), pp.
324337. Lecture Notes in Artificial Intelligence.
Ortiz, M., Calvanese, D., & Eiter, T. (2008a). Data complexity of query answering in
expressive description logics via tableaux. Journal of Automated Reasoning, 41 (1),
6198.
Ortiz, M., Simkus, M., & Eiter, T. (2008b). Conjunctive query answering in sh using
knots. In Proceedings of the 2008 Description Logic Workshop (DL 2008). CEUR
(http://ceur-ws.org/).
Pratt-Hartmann, I. (2009). Data-complexity of the two-variable fragment with counting
quantifiers. Forthcoming in Information and Computation. http://arxiv.org/abs/0806.
1636.
Rosati, R. (2006a). DL+log: Tight integration of description logics and disjunctive datalog.
In Proceedings of the 10th International Conference on the Principles of Knowledge
Representation and Reasoning (KR 2006), pp. 6878.
Rosati, R. (2006b). On the decidability and finite controllability of query processing in
databases with incomplete information. In Proceedings of the 25th ACM SIGACT
SIGMOD Symposium on Principles of Database Systems (PODS 2006), pp. 356365.
ACM Press and Addison Wesley.
480

fiNominals, Inverses, Counting, and Conjunctive Queries

Rosati, R. (2007a). The limits of querying ontologies. In Proceedings of the 11th International Conference on Database Theory (ICDT 2007), Vol. 4353 of Lecture Notes in
Computer Science, pp. 164178. Springer-Verlag.
Rosati, R. (2007b). On conjunctive query answering in EL. In Proceedings of the 2007
Description Logic Workshop (DL 2007). CEUR Workshop Proceedings.
Rudolph, S., Krotzsch, M., & Hitzler, P. (2008). Terminological reasoning in SHIQ with
ordered binary decision diagrams. In Proc. 23rd National Conference on Artificial
Intelligence (AAAI 2008), pp. 529534. AAAI Press/The MIT Press.
Sidhu, A., Dillon, T., Chang, E., & Sidhu, B. S. (2005). Protein ontology development using
OWL. In Proceedings of the 1st OWL Experiences and Directions Workshop (OWLED
2005), Vol. 188 of CEUR Workshop Proceedings. CEUR (http://ceur-ws.org/).
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: A practical
OWL-DL reasoner. Journal of Web Semantics, 5 (2).
Tessaris, S. (2001). Questions and answers: Reasoning and querying in Description Logic.
PhD thesis, University of Manchester.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.
In Proceedings of the International Joint Conference on Automated Reasoning (IJCAR
2006), Vol. 4130 of Lecture Notes in Computer Science, pp. 292  297. Springer-Verlag.
van der Meyden, R. (1998). Logical approaches to incomplete information: A survey. In
Logics for Databases and Information Systems, pp. 307356. Kluwer Academic Publishers.
Vardi, M. Y. (1997). Why is modal logic so robustly decidable?. In Descriptive Complexity
and Finite Models: Proceedings of a DIMACS Workshop, Vol. 31 of DIMACS: Series
in Discrete Mathematics and Theoretical Computer Science, pp. 149184. American
Mathematical Society.
W3C OWL Working Group (2009).
OWL 2 web ontology language document
overview. Tech. rep., World Wide Web Consortium. http://www.w3.org/TR/2009/
REC-owl2-overview-20091027/.
Wolstencroft, K., Brass, A., Horrocks, I., Lord, P., Sattler, U., Turi, D., & Stevens, R.
(2005). A Little Semantic Web Goes a Long Way in Biology. In Proceedings of the
5th International Semantic Web Conference (ISWC 2005).

481

fiJournal of Artificial Intelligence Research 39 (2010) 301-334

Submitted 2/10; published 9/10

A Model-Based Active Testing Approach to
Sequential Diagnosis
Alexander Feldman

a.b.feldman@tudelft.nl

Delft University of Technology
Mekelweg 4, 2628 CD, Delft, The Netherlands

Gregory Provan

g.provan@cs.ucc.ie

University College Cork
Department of Computer Science
College Road, Cork, Ireland

Arjan van Gemund

a.j.c.vangemund@tudelft.nl

Delft University of Technology
Mekelweg 4, 2628 CD, Delft, The Netherlands

Abstract
Model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses. The set of diagnoses can be reduced by taking into account extra observations (passive
monitoring), measuring additional variables (probing) or executing additional tests (sequential diagnosis/test sequencing). In this paper we combine the above approaches with
techniques from Automated Test Pattern Generation (ATPG) and Model-Based Diagnosis
(MBD) into a framework called Fractal (FRamework for ACtive Testing ALgorithms).
Apart from the inputs and outputs that connect a system to its environment, in active
testing we consider additional input variables to which a sequence of test vectors can be
supplied. We address the computationally hard problem of computing optimal control assignments (as defined in Fractal) in terms of a greedy approximation algorithm called
FractalG . We compare the decrease in the number of remaining minimal cardinality
diagnoses of FractalG to that of two more Fractal algorithms: FractalATPG and
FractalP . FractalATPG is based on ATPG and sequential diagnosis while FractalP is
based on probing and, although not an active testing algorithm, provides a baseline for comparing the lower bound on the number of reachable diagnoses for the Fractal algorithms.
We empirically evaluate the trade-offs of the three Fractal algorithms by performing
extensive experimentation on the ISCAS85/74XXX benchmark of combinational circuits.

1. Introduction
Combinational Model-Based Diagnosis (MBD) approaches (de Kleer & Williams, 1987)
often lead to a large number of diagnoses, which is exponential in the number of components,
in the worst-case. Combining multiple sensor readings (observation vectors) (Pietersma &
van Gemund, 2006) helps in a limited number of cases because the approach is inherently
passive, i.e., there are situations in which the observations repeat themselves (for example,
in systems that are stationary, pending a reconfiguration).
Sequential diagnosis algorithms (Shakeri, 1996) can be used as an alternative to the
above passive approach, with better decay of the number of diagnostic hypotheses. The
decay rate depends on the tests and test dictionary matrix, and is bounded from below
c
2010
AI Access Foundation. All rights reserved.

fiFeldman, Provan, & van Gemund

by results for tests with binary outcomes. Algorithms for sequential diagnosis suffer from
a number of other limitations. Early approaches assume single-faults while multiple-fault
sequential diagnosis is super-exponential (p2 or harder) (Shakeri, Raghavan, Pattipati, &
Patterson-Hine, 2000).
As observations (test outcomes) are not known in advance, the goal of a diagnostician
is to create a policy that minimizes the diagnostic uncertainty on average, i.e., one aims at
minimizing the average depth of a test tree. Pattipati and Alexandridis (1990) have shown
that under certain conditions (e.g., unit test costs and equal prior fault probabilities) a onestep look-ahead policy leads to an optimal average depth of the test tree; de Kleer, Raiman,
and Shirley (1992) have shown that one-step look-ahead delivers good practical results for
a range of combinational circuits.
This paper proposes a framework, called Fractal (FRamework for ACtive Testing
ALgorithms) for comparing different computational vs. optimality trade-offs in various
techniques for reducing the diagnostic uncertainty. All Fractal algorithms start from an
initial set of multiple-fault diagnostic hypotheses (this initial set can contain all possible hypotheses) and compute actions for reducing this initial set to, if possible, a single diagnostic
hypothesis (candidate). In the case of probing (de Kleer & Williams, 1987), this action consists of measuring an internal (hidden) variable. In the case of sequential diagnosis (ATPG),
the action consists of applying a set of input (control) assignments that disambiguate the
health state of the component that appears faulty in most of the initial diagnostic hypotheses. In the case of active testing the action consists of applying a set of input (control)
assignments that optimally reduce the initial set of hypotheses. In our framework the active testing and sequential diagnosis approaches differ only in how they compute the input
(control) assignments. We measure the optimality of the algorithms by computing the speed
with which they decay the initial set of hypotheses and the computational efficiency.
environment
IN

OUT
system
CTL
FRACTAL

Figure 1: Active testing dataflow for Fractal
In Fractal, we study the influence of not just the input (IN) and output (OUT) variables,
but also control (CTL) variables. Controls are similar to inputs, except they can be modified
by users while the system is connected to its environment. Use of models from first principles
and controls in Fractal allows us to eliminate the need of designing explicit tests and test
dictionaries. Our algorithms implicitly create test matrices leading to optimal decay based
on the built-in testing capabilities of the system. We call the above approach active testing;
it is a technique using models from first principles and controls for creating test sequences
that reduce the diagnostic uncertainty of a system. The architecture in which we use
Fractal and active testing is shown in Fig. 1.
302

fiA Model-Based Active Testing Approach to Sequential Diagnosis

As reliable component failure rates may be problematic to obtain, we assume equally
likely and small prior probabilities of failure and measure the diagnostic uncertainty as the
number of Minimal Cardinality (MC) diagnoses. Fractal can be modified to use arbitrary
failure probabilities and even components that are more likely to be faulty than healthy.
This would necessitate modifications of some of the algorithms (e.g., change of bias in the
importance sampling, etc.). In addition to simplifying the modeling, the equiprobable failure
rates assumption also has computational advantages. It can be shown that with equal and
small prior probabilities of failure, the diagnostic entropy, e.g., as used by de Kleer and
Williams (1987), can be computed directly from the number of MC diagnoses.
The computational complexity of deterministic algorithms for sequential diagnosis increases with respect to both the fault-cardinality and the number of tests (the size of the
test dictionary). To enable performance to scale up to real-world problems, which may have
high fault-cardinality and a large number of tests, we propose FractalGa low-cost greedy
stochastic approach that maintains exponential decay of the number of MC diagnoses. Instead of assuming single faults or timing out, FractalG may result in suboptimal but still
exponential decay.
We study the performance of FractalG compared to two alternatives: (1) FractalATPG , which implements sequential diagnosis based on Automated Test Pattern Generation (ATPG), and (2) FractalP, which implements probing (de Kleer & Williams, 1987).
ATPG has been successfully used in the electronic industry to compute sets of inputs that
test each component in a VLSI circuit. We have considered an ATPG-based approach because it is natural to attempt to reduce the diagnostic ambiguity by computing inputs that
can disambiguate the status of the single component that appears in the majority of the
diagnostic hypotheses.
FractalATPG is derived from sequential testing, is deterministic and myopic, and allows us to evaluate how well a single-step lookahead approach works on the given model.
Although probing is not classified as a technique for sequential diagnosis, it can be viewed
as a process for generating tests using additional control circuitry (machine or human) to
execute a probe such that some output reveals the internal variable. Its significance is that
it shows a lower bound on the number of diagnoses achievable for a model extended with
unlimited CTL circuitry.
Our contributions are as follows:
 We devise an approach for reducing the diagnostic uncertainty, called active testing,
that generalizes sequential diagnosis and MBD, allows combination of multiple passive
sensor readings, and does not require explicit tests and test dictionaries.
 We design FractalATPGa single-step look-ahead algorithm based on ATPGfor
solving the active testing problem.
 We design and implement FractalGa greedy approximation algorithm for active
testing that overcomes the limitations of FractalATPG and offers a trade-off in computational complexity vs. optimality for reducing the diagnostic uncertainty. We
compare FractalG and FractalATPG .
 We implement FractalP and use it as a computationally efficient, myopic (one-step
lookahead), easy-to-analyze baseline technique for reducing diagnostic uncertainty.
303

fiFeldman, Provan, & van Gemund

Although FractalP is technically not an active testing algorithm, the implementation
of probing and active testing in a common framework and the unified experimentation
help to understand the cost vs. performance trade-offs in (active and passive) testing
vs. probing strategies.
 We present extensive empirical data on 74XXX/ISCAS85 circuits, which enable us
to evaluate FractalATPG, FractalG, and FractalP in terms of their ability to
reduce the number of remaining diagnoses according to a geometric decay function.
This paper is organized as follows. Section 2 introduces related work. Section 3 presents basic MBD notions, the concept of remaining number of diagnoses and a framework for sequential diagnosis. Section 4 introduces a stochastic sampling-based algorithm for computing the
expected number of cardinality-minimal diagnoses. Section 5 describes the FractalATPG,
FractalG, and FractalP algorithms. Section 6 shows experimental results. Finally,
Sec. 7 summarizes this paper and discusses future work.

2. Related Work
Early work aimed at diagnostic convergence by de Kleer and Williams (1987) compute
a probe sequence for reducing diagnostic entropy using a myopic search strategy. Unlike
their work, in active testing we assume that probes are not available, other than indirectly
exposed through diagnosis based on test vectors, which offers an automated solution.
Generating test vectors to deduce faults has received considerable attention. Automatic
test pattern generation (ATPG) aims at verifying particular, single-faults (Stephan, Brayton, & Sangiovanni-Vincentelli, 1996). ATPG differs from active testing in that the vectors
are specific for particular single-faults, whereas active testing generates a sequence of vectors
to isolate unknown, multiple-faults, a much harder problem.
Table 1: Properties of techniques for sequential diagnosis
Technique

Model

User Actions

Automatic
Tests

Performance

Cost

Passive monitoring
Sequential diagnosis
FractalATPG
FractalG
Probing (FractalP )

first principles
test dictionary
first principles
first principles
first principles

apply test
apply controls
apply controls
measure internals

no
yes
yes
-

variable1
good
variable3
good
binary search

low
variable2
medium
high
medium

1
2
3

Depends on the environment (IN/OUT data).
Speed deteriorates rapidly with multiple-faults.
Depends on the model topology.

Table 1 summarizes the properties of the various techniques for sequential diagnosis discussed in this paper. Fractal eliminates the need for using tools for building tests and test
dictionaries, such as the ones proposed by Deb, Ghoshal, Malepati, and Kleinman (2000).
In our approach tests and test dictionaries are automatically constructed from design speci304

fiA Model-Based Active Testing Approach to Sequential Diagnosis

fications and models. At the same time, Fractal delivers comparable or better diagnostic
convergence at reasonable computational price.
Active testing bears some resemblance with sequential diagnosis, which also generates a
sequence of test vectors (Pattipati & Alexandridis, 1990; Raghavan, Shakeri, & Pattipati,
1999; Tu & Pattipati, 2003; Kundakcioglu & Unluyurt, 2007). The principal difference is
that in sequential diagnosis a fault dictionary is used (fault matrix). This pre-compiled
dictionary has the following drawback: in order to limit the (exponential) size of the dictionary, the number of stored test vectors is extremely small compared to the test vector
space. This severely constrains the optimality of the vector sequence that can be generated;
in contrast, active testing computes arbitrary test vectors on the fly using a model-based
approach. Furthermore, the matrix specifies tests that only have a binary (pass/fail) outcome, whereas active testing exploits all the systems outputs, leading to faster diagnostic
convergence. In addition, we allow the inputs to be dynamic, which makes our framework
suitable for online fault isolation.
The sequential diagnosis problem studies optimal trees when there is a cost associated
with each test (Tu & Pattipati, 2003). When costs are equal, it can be shown that the
optimization problem reduces to a next best control problem (assuming one uses information entropy). In this paper a diagnostician who is given a sequence S and who tries to
compute the next optimal control assignment would try to minimize the expected number
of remaining diagnoses |(S)|.
Our task is harder than that of Raghavan et al. (1999), since the diagnosis task is NPhard, even though the diagnosis lookup uses a fault dictionary; in our case we compute a
new diagnosis after every test. Hence we have an NP-hard sequential problem interleaved
with the complexity of diagnostic inference at each step (in our case the complexity of
diagnosis is p2 -hard). Apart from the above-mentioned differences, we note that optimal
test sequencing is infeasible for the size of problems in which we are interested.
Model-Based Testing (MBT) (Struss, 1994) is a generalization of sequential diagnosis.
The purpose of MBT is to compute inputs manifesting a certain (faulty) behavior. The main
differences from our active testing approach are that MBT (1) assumes that all inputs are
controllable and (2) MBT aims at confirming single-fault behavior as opposed to maximally
decreasing the diagnostic uncertainty.
Brodie, Rish, Ma, and Odintsova (2003) cast their models in terms of Bayesian networks.
Our notion of entropy is the size of the diagnosis space, whereas Brodie et al. use decisiontheoretic notions of entropy to guide test selection. Brodie et al. extend their past Bayesian
diagnostic approach (Rish, Brodie, & Ma, 2002) with sequential construction of probe sets
(probe sets are collections of, for example, pings to a subset of the nodes in a computer network). The approach of Brodie et al. is limited to networks although it can be extended by
modifying the type of Bayesian network shown by Rish et al.; such a modification, however,
would necessitate more computationally expensive Bayesian reasoning for achieving good
approximation results for the most probable explanations.
The approach of Brodie et al. (2003) does not compute modifications in the target
network topology and does not propose control actions (for example, a network server
that fails to respond can be dialed-up through a modem or checked by a technician at a
higher cost). The similarity between Fractal and active probing is that both approaches
attempt at reducing the diagnostic uncertainty by analyzing the future state of the system
305

fiFeldman, Provan, & van Gemund

as a function of some action (sending a set of probes for active probing or an application of
control inputs for FractalG and FractalATPG).
We solve a different problem than that of Heinz and Sachenbacher (2008), Alur, Courcoubetis, and Yannakakis (1995). Both of these approaches assume a non-deterministic
model defined as an automaton. In contrast, our framework assumes a static system (plant
model) for which we must compute a temporal sequence of tests to best isolate the diagnosis.
Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this
automaton to a relational specification, and apply their framework to software diagnosis.
This automaton-based framework accommodates more general situations than does ours,
such as the possibility that the systems state after a transition may not be uniquely determined by the state before the transition and the input, and/or the systems state may
be associated with several possible observations. In our MBD framework, a test consists
of an instantiation of several variables, which corresponds to the notion of test sequence
within the automaton framework of Heinz and Sachenbacher. The framework of Esser and
Struss requires modeling of the possible faults, whereas Fractal works both with weak
and strong-fault models1 . Interestingly, as shown by Esser and Struss, modeling of abnormal software behavior can be derived to some extent from software functional requirements.
This makes their framework suitable for software systems.
A recent approach to active diagnosis is described by Kuhn, Price, de Kleer, Do, and
Zhou (2008), where additional test vectors are computed to optimize the diagnosis while the
system (a copier) remains operational. Their work differs from ours in that plans (roughly
analogous to test sequences) with a probability of failure T are computed statically, and
a plan remains unmodified even if it fails to achieve its desired goal (a manifestation of a
failure with probability close to T ). Conversely, Fractal dynamically computes next-best
control settings in a game-like manner. The biggest difference between Fractal and the
approach of Kuhn et al. is in the use of models. Fractal is compatible with traditional
MBD (de Kleer & Williams, 1987) and can reuse existing models from first principles while
the pervasive approach of Kuhn et al. uses an automaton and a set of possible actions.
The approach of Kuhn et al. (2008) uses existing MBD and planning algorithms, and as
such integrates existing approaches; in contrast, Fractal introduces new control algorithms
and reuses an external diagnostic oracle. An advantage of the pervasive diagnosis approach
is that the use of a planning engine generates a complete sequence of actions, as opposed to
the one-step lookahead of FractalG. Depending on the planning formalism, the complexity
of pervasive diagnosis can be dominated by the planning module, while the most complex
computational task in Fractal is that of diagnosis. Both pervasive diagnosis and this
paper, however, report good average-case computational efficiency for benchmark problems.
Last, the paper of Kuhn et al. is limited to single-fault diagnoses, although the pervasive
diagnosis framework can be generalized to multiple faults.
Feldman, Provan, and van Gemund (2009a) introduce an early version of FractalG.
This paper (1) generalizes the Fractal framework, (2) introduces FractalATPG and
FractalP, (3) extends the experimental results, and (4) provides a comparison of the
different Fractal approaches.
1. Weak-fault models (also known as models with ignorance of abnormal behavior) and strong-fault models
are discussed by Feldman, Provan, and van Gemund (2009b).

306

fiA Model-Based Active Testing Approach to Sequential Diagnosis

3. Concepts and Definitions
Our discussion starts by introducing relevant MBD notions. Central to MBD, a model of
an artifact is represented as a propositional Wff over a set of variables V . We will define
four subsets of these variables: assumable, observable 2 , control, and internal variables. This
gives us our initial definition:
Definition 1 (Active Testing System). An active testing system ATS is defined as ATS =
hSD, COMPS, CTL, OBSi, where SD is a propositional Wff over a variable set V , COMPS
OBS  CTL  V , and COMPS, OBS, and CTL are subsets of V containing assumable,
observable, and control variables, respectively.
The set of internal variables is denoted as INT, INT = V \ {COMPS  OBS  CTL}.
Throughout this paper we assume that OBS, COMPS, and CTL are disjoint, and SD 6|=.
Sometimes it is convenient (but not necessary) to split OBS into non-controllable inputs IN
and outputs OUT (OBS = IN  OUT, IN  OUT = ).
3.1 Running Example
We will use the Boolean circuit shown in Fig. 2 as a running example for illustrating all
notions and the algorithm shown in this paper. The 2-to-4 line demultiplexer consists of
four Boolean inverters and four and-gates.
a

b

h1

h3

p

q

h2

h4

r

s

h5

i

h6

h7

h8

o1

o2

o3

o4

Figure 2: A demultiplexer circuit
The expression h  (o  i) models an inverter, where the variables i, o, and h represent input, output, and health respectively. Similarly, an and-gate is modeled as h 
(o  i1  i2  i3 ). The above propositional formulae are copied for each gate in Fig. 2 and
their variables subscripted and renamed in such a way as to ensure a proper disambiguation
2. In the MBD literature the assumable variables are also referred to as component, failure-mode, or
health variables. Observable variables are also called measurable variables.

307

fiFeldman, Provan, & van Gemund

and to connect the circuit. The result is the following propositional model:

[h1  (a  p)]  [h2  (p  r)]




[h

3  (b  q)]  [h4  (q  s)]


h5  (o1  i  p  q)
SD =
h

6  (o2  i  r  q)



 h7  (o3  i  p  s)


h8  (o4  i  r  s)

(1)

The set of assumable variables is COMPS = {h1 , h2 , . . . , h8 }, the observable variables are
OBS = {a, b, o1 , o2 , o3 , o4 }, and the set of control variables is the singleton CTL = {i}. Note
the conventional selection of the sign of the health variables h1 , h2 , . . . , hn . Other authors
use ab for abnormal.
3.2 Diagnosis
The traditional query in MBD computes terms of assumable variables, which are explanations for the system description and an observation.
Definition 2 (Diagnosis). Given a system ATS, an observation  over some variables in
OBS, and an assignment  to all variables in COMPS,  is a diagnosis iff SD     6|=.
The set of all diagnoses of SD and an observation  is denoted as (SD, ). The cardinality
of a diagnosis, denoted as ||, is defined as the number of negative literals in .
Continuing our running example, consider an observation vector 1 = a  b  i  o4 .
There are a total of 256 possible assignments to all variables in COMPS and |(SD, 1 )| =
200. Example diagnoses are 1 = h1  h2  . . .  h7  h8 and 2 = h1  h2  h3  h4 
h5  h6  h7  h8 . We will write sometimes a diagnosis in a set notation, specifying the set
of negative literals only. Thus 2 would be represented as D2 = {h1 , h4 }.
Definition 3 (Minimal-Cardinality Diagnosis). A diagnosis   is defined as MinimalCardinality (MC) if no diagnosis   exists such that |  | < |  |.
Our selection of minimality criterion is such that it is impossible to compute all diagnoses
from the set of all MC diagnoses without further inference. MC diagnoses, however, are often
used in practice due to the prohibitive cost of computing a representation of all diagnoses
of a system and an observation (e.g., all subset-minimal diagnoses).
Consider an observation vector 2 = a  b  i  o1  o4 . There are 6 MC diagnoses
of cardinality 2 consistent with SD  2 , and counting these MC diagnoses is a common
problem in MBD.
The number of MC diagnoses of a system ATS and an observation  is denoted as
| (SD, )|, where  (SD, ) is the set of all MC diagnoses of SD  . Given a system
ATS, an observation sequence S is defined as a k-tuple of terms S = h1 , 2 , . . . , k i, where
i (1  i  k) is an instantiation of variables in OBS.
Throughout this paper, we assume that the health of the system under test does not
change during the test (i.e., the same inputs and a fault produce the same outputs) and call
this assumption stationary health.
308

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Lemma 1. Given a system ATS, a stationary health state for its components , and an
observation sequence S, it follows that   (SD, 1 )  (SD, 2 )  . . .  (SD, k ).
Proof. The above statement follows immediately from the stationary health assumption and
Def. 2.
Lemma 1 can be applied only in the cases in which all diagnoses are considered. If we
compute subset-minimal diagnoses in a weak-fault model, for example, the intersection
operator has to be redefined to handle subsumptions. To handle non-characterizing sets of
diagnoses3 (e.g., MC or first m diagnoses), we provide the following definition.
Definition 4 (Consistency-Based Intersection). Given a set of diagnoses D of SD  , and
an a posteriori observation  , the intersection of D with the diagnoses of SD   , denoted
as  (D,  ), is defined as the set D (D   D) such that for each   D it holds that
SD     6|=.
It is straightforward to generalize the above definition to an observation sequence S.
Definition 5 (Remaining Minimal-Cardinality Diagnoses). Given a diagnostic system ATS
and an observation sequence S, the set of remaining diagnoses (S) is defined as (S) =
 ( (    ( (SD, 1 ), 2 ),    ), k ).
We use |(S)| instead of the more precise diagnostic entropy as defined by de Kleer and
Williams (1987) and subsequent works, as this allows low-complexity estimations (discussed
in Sec. 4). In particular, if all diagnoses are of minimal-cardinality and the failure probability
of each component is the same, then the gain in the diagnostic entropy can be directly
computed from |(S)|.

4. Computing the Expected Number of MC Diagnoses
Active testing aims to minimize the expected number of diagnoses that result from the
possible set of outputs that may occur from a given control vector. In this section we
present an algorithm to approximate this expectation.
We will compute the expected number of diagnoses for a set of observable variables M
(M  OBS). The initial observation  and the set of MC diagnoses D =  (SD, ) modify
the probability density function of subsequent outputs (observations), i.e., a subsequent
observation  changes its likelihood. The (non-normalized) a posteriori probability of an
observation  , given a function  that computes the set of MC diagnoses and an initial
observation , is:
Pr( |SD, ) =

| ( (SD, ),  )|
| (SD, )|

(2)

The above formula computes the probability of a given a priori set of diagnoses restricting
the possible outputs, i.e., we assume that the probability is the ratio of the number of
3. A characterizing set of diagnoses, for example the set of all subset-minimal diagnoses, is loosely defined
as a set of diagnoses from which the (complete) set of all diagnoses can be constructed without using the
system description or any other information.

309

fiFeldman, Provan, & van Gemund

remaining diagnoses to the number of initial diagnoses. In practice, there are many  for
which Pr( |SD, ) = 0, because a certain fault heavily restricts the possible outputs of a
system (i.e., the set of the remaining diagnoses in the numerator is empty).
The expected number of remaining MC diagnoses for a variable set M , given an initial
observation , is then the weighted average of the intersection sizes of all possible instantiations over the variables in M (the weight is the probability of an output):

E  (SD, M |) =

X

 M 

| (D,  )|  Pr( |SD, )
X

 M 

Pr( |SD, )

(3)

where D =  (SD, ) and M  is the set of all possible assignments to the variables in M .
Replacing (2) in (3) and simplifying gives us the following definition:
Definition 6 (Expected Minimal-Cardinality Diagnoses Intersection Size). Given a system ATS and an initial observation , the expected remaining number of MC diagnoses
E  (SD, OBS|) is defined as:

E  (SD, OBS|) =

X

 OBS

X

| ( (SD, ),  )|2

 OBS

| ( (SD, ),  )|

(4)

where OBS is the set of all possible assignments to all variables in OBS.
Two of the algorithms presented in this paper compute the expected number of remaining
MC diagnoses for one variable. As a result the expectation expression in (4) simplifies to:
E  (SD, v|) =

| ( (SD, ), v)|2 + | ( (SD, ), v)|2
| ( (SD, ), v)| + | ( (SD, ), v)|

(5)

The complexity of computing (5) depends only on the length of the sequence S, the complexity of the MC oracle computing  (SD, ), and the complexity of the intersection
algorithm.
4.1 Computing the Expectation Using Importance Sampling
To overcome the computational complexity of evaluating an expectation, we employ a
stochastic algorithm based on importance sampling. The key insight that allows us to
build a fast method for computing the expected number of remaining diagnoses is that the
prior observation (and respectively the set of MC diagnoses) shifts the probability of the
outputs. Hence, an algorithm that samples the possible input assignments (recall that it
is a basic modeling assumption that inputs are equally likely) and counts the number of
different observations, given the set of prior diagnoses, can produce a good approximation.
We next introduce an algorithm for approximating the expected number of remaining
diagnoses.
310

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Algorithm 1 Approximate expectation of (S)
1: function Expectation(ATS, , D) returns a real
inputs: ATS (active testing system): model
 (term): control vector
D (set of diagnoses): prior diagnoses
local variables: , ,  (terms): observation
s (integer): sum of the remaining diagnoses, initially 0
q (integer): sum of squares of the remaining diagnoses, initially 0
Z (set of terms): samples
E (real): expectation
2:
Z
3:
repeat
4:
  RandomInputs(SD, IN)
5:
for all   D do
6:
  InferOutputs(SD, OUT,   , )
7:
if    6 Z then
8:
Z  Z  {  }
9:
q  q + | (D,     )|2
10:
s  s + | (D,     )|
11:
E  q/s
12:
end if
13:
end for
14:
until Terminate(E)
15:
return E
16: end function

Algorithm 1 uses a couple of auxiliary functions: RandomInputs assigns random values to
all inputs and InferOutputs computes all outputs from the system model, all inputs and
a diagnosis.4 The computation of the intersection size | (D,  )| can be implemented
by counting those   D for which SD         6|=.
The algorithm terminates when a termination criterion (checked by Terminate) is
satisfied. In our implementation, Terminate returns success when the last n iterations
(where n is a small constant) leave the expected number of diagnoses, E, unchanged, in
terms of its integer representation. Our experiments show that for all problems considered,
n < 100 yields a negligible error.
The complexity of Alg. 1 is determined by the complexity of consistency checking (line
9  10) and the size of D. If we denote the complexity of a single consistency check with
, then the complexity of Alg. 1 becomes O(|D|). Although consistency checking for
diagnostic problems is NP -hard in the worst case, for average-case problems it is easy. In
our implementation of Expectation we overcome the complexity of consistency checking
4. This is not always possible in the general case. In our framework, we have a number of assumptions,
i.e., a weak-fault model, well-formed circuit, etc. The complexity of InferOutputs thus depends on
the framework and the assumptions.

311

fiFeldman, Provan, & van Gemund

by using an incomplete Logic-Based Truth Maintenance System (LTMS) (Forbus & de Kleer,
1993).

5. Algorithms for Reducing the Diagnostic Uncertainty
In this section we introduce three algorithms: FractalATPG, FractalG, and FractalP.
5.1 Problem Definition and Exhaustive Search
Our AT problem is defined as follows:
Problem 1 (Optimal Control Sequence). Given a system ATS, a sequence (of past observations and controls) S = h1  1 , 2  2 ,    , k  k i, where i (1  i  k) are OBS
assignments and j (1  j  k) are CTL assignments, compute a new CTL assignment
k+1 , such that:
k+1 = argmin E  ( (SD, S), {IN  OUT}|)

(6)

CTL

where CTL is the space of all possible control assignments.
Problem 1 is different from the general sequential testing problem, as formulated by Shakeri
(1996). In the Shakeri formulation, there are different test costs and different prior failure
probabilities, where Problem 1 assumes equal costs and equal small prior probabilities of
failure. Pattipati and Alexandridis (1990) show that under those assumptions, minimizing
the test cost at each step constitutes an optimal policy for minimizing the expected test cost.
Hence, solving Problem 1 is solving the lesser problem of generating an optimal test strategy
given unit costs and equal prior failure probability. Note that we can use an algorithm that
optimizes Problem 1 as a heuristic algorithm for solving the sequential testing problem. In
this case the expected cost would be arbitrarily far from the optimum one, depending on
the cost distribution and the tests.
Consider our running example with an initial observation vector (and control assignment) 3  3 = a  b  i  o1  o2  o3  o4 , where 3 = i is chosen as the initial
control input. The four MC diagnoses of SD  3  3 are  = {{h1 , h3 }, {h2 , h5 },
{h4 , h5 }, {h5 , h8 }}.
An exhaustive algorithm would compute the expected number of diagnoses for each of
the 2|CTL| next possible control assignments. In our running example we have one control
variable i and two possible control assignments (5 = i and 6 = i). To compute the
expected number of diagnoses, for each possible control assignment  and for each possible
observation vector , we have to count the number of initial diagnoses which are consistent
with   .
Computing the intersection sizes for our running example gives us Table 2. Note that,
in order to save space, Table 2 contains rows only for those    for which Pr(  ) 6= 0,
given the initial diagnoses  (and, as a result, | ( (SD, 3  3 ),   )| =
6 0). It is
straightforward to compute the expected number of diagnoses for any control assignment
with the help of this marginalization table. In order to do this we have to (1) filter out
those lines which are consistent with the control assignment  and (2) compute the sum
and the sum of the squares of the intersection sizes (the rightmost column of Table 2).
312

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Table 2: Marginalization table for SD and 3
i

a

b o1 o2 o3 o4

F
F
F
F
F
F
F
F
F
F
F

F
F
F
F
F
F
T
T
T
T
T

F
F
F
T
T
T
F
F
F
T
T

F
T
T
F
T
T
F
T
T
F
T

F
F
F
F
F
F
F
F
F
F
F

F
F
F
F
F
F
F
F
F
F
F

F
F
T
F
F
T
F
F
T
F
F

Pr

| |

0.03125
0.0625
0.03125
0.03125
0.0625
0.03125
0.03125
0.0625
0.03125
0.03125
0.0625

1
2
1
1
2
1
1
2
1
1
2

i a
F
T
T
T
T
T
T
T
T
T
T

T
F
F
F
F
F
F
T
T
T
T

b o1 o2 o3 o4
T
F
F
F
T
T
T
F
F
F
T

T
F
F
F
F
T
T
F
T
T
T

F
F
F
T
T
F
F
F
F
T
F

F
F
T
F
F
F
T
T
F
F
F

T
T
F
F
F
F
T
F
F
T
F

Pr

| |

0.03125
0.0625
0.03125
0.03125
0.03125
0.03125
0.0625
0.03125
0.03125
0.0625
0.125

1
2
1
1
1
1
2
1
1
2
4

To compute E(SD, OBS|3  i), we have to find the sum and the sum of the squares of
the intersection sizes of all rows in Table 2 for which column i is F. It can be checked that
E(SD, OBS|3 , i) = 24/16 = 1.5. Similarly, E(SD, OBS|3  i) = 34/16 = 2.125. Hence
an optimal diagnostician would consider a second measurement with control setting  = i.
The obvious problem with the above brute-force approach is that the size of the marginalization table is, in the worst-case, exponential in |OBS|. Although many of the rows in the
marginalization table can be skipped as the intersections are empty (there are no consistent prior diagnoses with the respective observation vector and control assignment), the
construction of this table is computationally so demanding that we will consider an approximation algorithm (to construct Table 1 for our tiny example, the exhaustive approach had
to perform a total of 512 consistency checks).
5.2 FractalATPG
Consider the running example from Sec. 3 and an observation 4 = a  b  i  o1  o4 . This
leads to the 6 double-fault MC diagnoses, shown in Fig. 3.

1
2
3
4
5
6
E

h1

h2

h3

h4

h5

h6

h7

h8




































13
3

10
3

13
3

13
3







6








10
3







6

13
3

Figure 3: ATPG-Based active testing example
Instead of searching through the space of all possible control assignments, we directly compute a control assignment that tests a specific component c by using an approach from
313

fiFeldman, Provan, & van Gemund

ATPG. We choose this component c to be the one that most decreases the expected number of remaining MC diagnoses by minimizing E  (SD, c|  ). If we look at Fig. 3 we can
see that knowing the health of h1 and h3 leads to E   3.33, for h2 , h4 , h5 , and h7 , we have
E   4.33, and for h6 and h7 we have E  = 6. Choosing a control setting that computes
the state of h1 or h3 is intuitive as the state of this component makes the most balanced
partition of the prior diagnoses.
We next present the FractalATPG algorithm that uses the approach illustrated above.
Algorithm 2 ATPG-Based active testing algorithm
1: function FractalATPG (ATS, , ) returns a control term
inputs: ATS (active testing system): model
 (term): initial (non-modifiable) observation
 (term): initial control
local variables: c (variable): component
f (integer): remaining diagnoses
d (term): diagnosis
 (term): control setting
H (set of pairs): component/expectation pairs
D (set of terms): diagnoses

2:
D   (SD,   )
3:
for all c  COMPS do
4:
f 0
5:
for all d  D do
6:
if c  d then
7:
f f +1
8:
end if
9:
end for
10:
H  hc, f 2 + (|D|  f )2 i
11:
end for
12:
H  SortByExpectation(H)
13:
for i = 1 . . . |H| do
14:
if   ATPG(ATS, , Hi hci) then
15:
return 
16:
end if
17:
end for
18:
return RandomControls()
19: end function
Algorithm 2 counts the number of prior diagnoses that each component appears in (lines
4 - 8) and the result is saved in the variable f . This number is then used to compute the
expected number of remaining MC diagnoses given the component health (line 10). For
each component the expected number of diagnoses is stored in the set H (line 10). The set
H is then sorted in increasing order of expectation (line 12). We then iterate over the set of
components in order of expectation (lines 13  17). For each component we try to compute
314

fiA Model-Based Active Testing Approach to Sequential Diagnosis

an ATPG vector that tests it. In some cases such a vector may not exist. In the worst case
there is no ATPG vector that can test any component, and Alg. 2 has no better strategy
but to return a random control assignment (line 18).
The time complexity of Alg. 2 is determined by the complexity of the diagnostic search
(line 2) and the complexity of ATPG (line 14). If we denote the former with  and the
latter with  then the complexity of FractalATPG becomes O(|COMPS|). As the
complexity of ATPG is usually lower than that of diagnosis (abductive reasoning) ( < ),
the complexity of FractalATPG is determined by the time for computing MC diagnoses.
Computing ATPG vectors has been extensively studied (Bushnell & Agrawal, 2000)
and although it is known to be an NP -hard problem (Ibarra & Sahni, 1975), there exists
evidence that ATPG is easy for practical problems (Prasad, Chong, & Keutzer, 1999). Some
efficient ATPG algorithms integrate randomized approach and Boolean difference (Bushnell
& Agrawal, 2000). The former approach efficiently computes test vectors for the majority of
components, while the latter computes test vectors for the remaining components by using
a DPLL-solver.
We implement ATPG as follows. First we duplicate the system description SD by
renaming each variable v : v 6 {IN  CTL} to v  , thus generating SD (SD and SD share
the same input and control variables). Then we create the all healthy assignment (for all
assumable variables)  0 and the single fault assignment  I such that  0 and  I differ only in
the sign of the literal whose component we want to test. Finally, we construct the following
propositional expression:
#
"
_
(7)
o  o
    SD  SD   0   I 
oOUT

where the operator  denotes an exclusive or, hence o  o  (o  o )  (o  o ).
The propositional expression in (7) leaves unconstrained only the controls  that we
need. There are two instances of the system: healthy (SD and  0 ) and faulty (SD and
 I ). The last term in  forces the output of the healthy and the faulty system to be
different in at least one bit. To compute an ATPG control vector we need one satisfiable
solution of . Note that an ATPG control vector may not exist ( |=), i.e., a component
may not be testable given CTL and SD  . Often there are multiple satisfying control
assignments. In this case FractalATPG chooses an arbitrary one. The latter does not
mean that all satisfiable ATPG control vectors achieve the same uncertainty reduction.
FractalATPG becomes suboptimal when there is no control testing a given component, or
when there are multiple controls. FractalATPG becomes completely random when there
are no components that can be tested with the given choice of controls.
There are two problems with FractalATPG. First, FractalATPG assumes stationary
inputs, i.e., FractalATPG ignores a source of uncertainty. The non-modifiable inputs, however, can only help in the decay process, hence FractalATPG is conservative in choosing
the control assignmentsa feature that leads to suboptimality. A bigger problem is that
FractalATPG decreases the expected number of remaining MC diagnoses by computing
the exact health of one component. Here, the problem is not that FractalATPG tests one
component per step, but that it tries to compute a control assignment that computes the
exact state of this component. An active testing algorithm can decrease the diagnostic uncertainty by computing a probability distribution function for the state of each component.
315

fiFeldman, Provan, & van Gemund

A natural extension of FractalATPG is an algorithm that computes the state of k
components simultaneously. The latter approach assumes that the system is k-component
testablean unrealistic assumption. In our experiments we have seen that systems are often
even not single-component testable. Note that computing the exact states of components
is not a requirement for decreasing the diagnostic uncertainty. Instead of computing the
exact state of one or more components, the algorithm shown in the next section implicitly
builds a probability density function for the health state of each component, and does not
suffer from the problems of FractalATPG .
5.3 FractalG
Consider SD from the example started in Sec. 3, input variables IN = {i}, control variables CTL = {a, b}, initial input values  = i, and an initial observation 3 =   (a 
b)  (o1  o2  o3  o4 ). The initial observation 3 leads to 5 triple-fault MC diagnoses:  (SD, 3 ) = {{h1 , h4 , h7 }, {h1 , h7 , h8 }, {h2 , h3 , h6 }, {h2 , h4 , h5 },
{h3 , h6 , h8 }}. We also write D =  (SD, 3 ) and choose one of the faults in D to be
the truly injected fault   (let   = {h1 , h7 , h8 }).
Exhaustive

Greedy

k

1

E  (SD, IN|1 )

k

1

E  (SD, IN|1 )

1
2
3
4

a  b
a  b
a  b
ab

4.33
1.57
1.57
1.33

1
2
3

a  b
a  b
ab

4.33
1.57
1.33

k

2

E  (SD, IN|2 )

k

2

E  (SD, IN|2 )

1
2
3
4

a  b
a  b
a  b
ab

1.67
1
1
1.67

1
2
3

a  b
a  b
ab

1.67
1
1.67

| (D,   1 )| = 2

| (D,   1 )| = 2

| ( (D,   1 ),   2 )| = 1

| ( (D,   1 ),   2 )| = 1

Figure 4: Exhaustive and greedy search for an optimal control assignment
The left and right parts of Fig. 4 show two possible scenarios for locating   . On the left
we have an exhaustive approach which considers all the 2|CTL| control assignments, hence it
cannot be used to solve practical problems. The greedy scenario on the right side of Fig. 4
decreases the number of computations of expected number of remaining MC diagnoses from
2|CTL| to |CTL|. The idea is to flip one control variable at a time, to compute the expected
number of remaining MC diagnoses and to keep the flip (shown in bold in Fig. 4) if E 
decreases. Given an initial control assignment  we consider the space of possible control
flips. This space can be visualized as a lattice (Fig. 5 shows a small example). Figure 5
316

fiA Model-Based Active Testing Approach to Sequential Diagnosis

shows the expected number of MC diagnoses for each control assignment. Note that probing
can be visualized in a similar way.
{i2 , i4 , i6 , i7 } 17.54

{i2 , i4 , i6 } 17.54 {i2 , i4 , i7 } 14.5 {i2 , i6 , i7 } 21.05 {i4 , i6 , i7 } 21.05

{i2 , i4 } 14.65 {i2 , i6 } 21.05 {i2 , i7 } 21.05 {i4 , i6 } 21.05 {i4 , i7 } 18.84 {i6 , i7 } 25

{i2 } 21.05 {i4 } 18.84 {i6 } 25 {i7 } 25

{} 25

Figure 5: Example of an expectation optimization lattice (74182, |CTL| = 4, |IN| = 5).
Each node shows the set of control flips and the expected number of MC diagnoses.

In practice, control literals are mostly independent and even though the space of control
assignments is not continuous in general, it has large continuous subspaces. The greedy
approach is shown in Alg. 3, which computes a control assignment for a given active testing
system and a prior observation.
Algorithm 3 Greedy active testing algorithm
1: function Fractal(ATS, ) returns a control term
inputs: ATS (active testing system): model
 (term): initial observation
local variables: ,   (terms): control configurations
E, E  (reals): expectations
D (set of terms): diagnoses
l (literal): control literal

2:
D   (SD, )
3:
E  Expectation(ATS, , D)
4:
for all l   do
5:
   FlipLiteral(, l)
6:
E   Expectation(ATS,   , D)
7:
if E  < E then
8:
  
9:
E  E
10:
end if
11:
end for
12:
return 
13: end function
317

fiFeldman, Provan, & van Gemund

The set of initial diagnoses is computed from the initial observation in line 2. In line 5,
Alg. 3 flips the next literal in the current control assignment. The auxiliary FlipLiteral
subroutine simply changes the sign of a specified literal in a term. After each flip the
expected intersection size is computed with a call to Expectation (cf. Alg. 1). If the
new expected intersection size is smaller than the current one, then the proposed control
assignment is accepted as the current control assignment, and the search continues from
there.
The complexity of FractalG is determined by the complexity of the diagnostic search
(line 2) and the complexity of Expectation (line 3 and line 6). If we denote the former
with  and the latter with  then the complexity of FractalG becomes O(|CTL|). As
  , the complexity of FractalG is the same as FractalG. In practice FractalG
requires more computation to compute a sufficient decay. This is due to the design of
Expectation (Alg. 1).
While the active-testing problem is worst-case NP -hard (it can be reduced to computing
a diagnosis), as we will see in the experimentation section, it is possible to achieve very good
average-case performance by choosing an appropriate MBD oracle. The advantage of the
greedy approach, in particular, is that the number of computations of the expected number
of diagnoses is linear in the number of literals in the control assignment. This is done at
the price of some optimality (i.e., the effect of combinations of controls is neglected).
5.4 FractalP
Probing is related to active testing as measuring internal variables can be thought of as
revealing internal control circuits. Alternatively, one can add control circuitry to a model
that reveals the values of internal variables. To reveal this hidden control potential we
implement GDE probing (de Kleer & Williams, 1987) in FractalP. Our approach is
different from GDE in two ways. First, we compute the expected number of remaining
MC diagnoses instead of expected diagnostic entropy. Second, Fractal does not use an
Assumption-Based Truth Maintenance System (ATMS) (de Kleer, 1986).
Consider the running example from Sec. 3 and an observation 5 = a  b  i  o1 
o2  o3  o4 . This leads to 5 triple-fault MC diagnoses:  (SD, 3 ) = {{h1 , h4 , h7 },
{h1 , h7 , h8 }, {h2 , h3 , h6 }, {h2 , h4 , h5 }, {h3 , h6 , h8 }}. Subsequent measurement of p gives us | ( (SD, 3 ), p)| = 3 if p is positive and | ( (SD, 5 ), p)| = 2
otherwise. The expected number of MC diagnoses is E  (SD, {p}|3 ) = 2.6. Repeating this
for the remaining internal variables results in E  (SD, {q}|3 ) = 2.6, E  (SD, {r}|3 ) = 3.4,
and E  (SD, {s}|3 ) = 3.4. As a result we can see that measuring p and q is less informative
than measuring r and s, which is intuitive as r and s give a more balanced partitioning of
the circuit.
Problem 2 (Probe Sequencing). Given a system ATS, an observation  and a partial
assignment to the internal variables , choose a variable p from the set U of unassigned
internal variables , such that:
p = argmin E  (SD, p|  )

(8)

pU

Algorithm 4 solves Problem 2. Algorithm 4 computes the expected number of diagnoses
for each unobserved variable (lines 3 - 11). Starting from the set D of initial diagnoses
318

fiA Model-Based Active Testing Approach to Sequential Diagnosis

(computed in line 2), Alg. 4 perform a total of 2|D||V \{OBSCOMPS}| consistency checks
(lines 4 and 5) to determine the expected number of MC diagnoses for each unobserved
variable.
We next show the probing algorithm as introduced by de Kleer and Williams (1987) and
adapted for the Fractal framework.
Algorithm 4 Probing algorithm
1: function FractalP(ATS, ) returns a variable
inputs: ATS (active testing system): model
 (term): observation
local variables: v, R (variables): probes
E, E  (reals): expectations
p, q (reals): remaining diagnoses
D (set of terms): diagnoses

2:
D   (SD, )
3:
for all v  V \ {COMPS  OBS} do
4:
p  | (D,   v)|
5:
q  | (D,   v)|
6:
E   (p2 + q 2 )/(p + q)
7:
if E  < E then
8:
Rv
9:
E  E
10:
end if
11:
end for
12:
return R
13: end function
Instead of computing the expected number of remaining MC diagnoses for a single variable
p, it is possible to consider measuring all pairs of variables hp1 , p2 i, or in general, all k-tuples
of internal variables hp1 , p2 , . . . , pm i for m  |V \{OBSCOMPS}|. We will refer to probing
involving more than 1 variable as k-probing. Although it has been shown that users do not
significantly benefit in terms of diagnostic uncertainty by performing k-probing (de Kleer
et al., 1992), we can easily modify FractalP to consider multiple probes. Note that for
m = |V \ {OBS  COMPS}| there is no probing problem, as there is only one way to pick
all internal variables.
The most complex operation in FractalP is again computing the initial set of MC
diagnoses. In addition to that, we have |V \ {COMPS  OBS}| consistency checks. Consistency checking is, in general, easier than diagnosis. Note that all Fractal algorithms
(FractalATPG , FractalG, and FractalP) start with computing the set of initial MC
diagnoses. Hence, the difference in their performance is determined by the complexity of
reducing the initial set  (SD, ). According to this criterion, the fastest algorithm is
FractalP as it only performs a small number of consistency checks, followed closely by
FractalATPG (computing ATPG vectors). The slowest algorithm is FractalG, as it
computes the expected number of MC diagnoses given multiple variables.
319

fiFeldman, Provan, & van Gemund

6. Experimental Results
We have implemented Fractal in approximately 3 000 lines of C code (excluding the
diagnostic engine and the Logic Based Truth Maintenance System). All experiments were
run on a 64-node dual-CPU cluster (each node configured with two 2.4 GHz AMD Opteron
DP 250 processors and 4 Gb of RAM).
6.1 Experimental Setup
We have experimented on the well-known benchmark models of ISCAS85 combinational
circuits (Brglez & Fujiwara, 1985). As models derived from the ISCAS85 circuits are computationally intensive (from a diagnostic perspective), we have also considered four mediumsized circuits from the 74XXX family (Hansen, Yalcin, & Hayes, 1999). In order to use the
same system model for both MC diagnosis counting and simulation, the fault mode of each
logic gate is stuck-at-opposite, i.e., when faulty, the output of a logic gate assumes the
opposite value from the nominal. Without loss of generality, only gates are allowed to fail
in our models. This is different from ATPG where gates typically do not fail but wires are
modeled as components that can fail with failure modes stuck-at-zero and stuck-at-one.
The ATPG and MBD modeling approaches achieve the same results.

Table 3: An overview of the 74XXX/ISCAS85 circuits (V is the total number of variables
and C is the number of clauses)

Name

Description

74182
74L85
74283
74181

4-bit
4-bit
4-bit
4-bit

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

27-channel interrupt ctl.
32-bit SEC circuit
8-bit ALU
32-bit SEC circuit
16-bit SEC/DEC
12-bit ALU
8-bit ALU
9-bit ALU
32-bit multiplier
32-bit adder

CLA
comparator
adder
ALU

|IN|

|OUT|

Original
|COMPS|

9
11
9
14

5
3
5
8

36
41
60
41
33
233
50
178
32
207

7
32
26
32
25
140
22
123
32
108

Reduced
|COMPS|

V

C

19
33
36
65

47
77
81
144

150
236
244
456

6
15
14
21

160
202
383
546
880
1 193
1 669
2 307
2 416
3 512

356
445
826
1 133
1 793
2 695
3 388
4 792
4 864
7 232

514
714
1 112
1 610
2 378
3 269
4 608
6 693
7 216
9 656

59
58
77
58
160
167
353
385
1 456
545

In addition to the original 74XXX/ISCAS85 models, we have performed cone reductions as
described by Siddiqi and Huang (2007) and de Kleer (2008). Recall that from the perspective
of the MBD diagnostic engine, faults inside a cone (where a cone is a set of components)
cannot be distinguished, hence it is enough to provide a single health variable per cone. We
call models with a single health variable per cone reduced. Table 3 describes all models.
320

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Both initial observation vectors and control settings are used in the first step of the
Fractal inference. To illustrate the significant diagnostic convergence that is possible, we
use initial observations leading to high numbers of initial MC diagnoses.
To average over the diagnostic outcomes of the observations, we repeat each experiment
with a range of initial observation vectors. The cardinality of the MC diagnosis is of no
significance to Fractal, but it produces a significant burden on the diagnostic oracle (Feldman, Provan, & van Gemund, 2008). In order to overcome this computational difficulty, we
have limited our experiments to observation vectors leading to double faults only.
For each circuit we have generated 1 000 non-masking double-faults, and for each observation we have computed the number of initial MC diagnoses. From the 1 000 observation
vectors we have taken the 100 with the largest number of MC diagnoses. The resulting
observations are summarized in Table 4. For example, we can see a staggering number of
46 003 double faults for the most under-constrained c7552 observation.
Table 4: Number of MC diagnoses per observation vector
Name

Min

Original
Max
Mean

Min

Reduced
Max Mean

74182
74L85
74283
74181

25
32
48
93

25
88
60
175

25
50.2
51.8
113.5

2
5
6
10

2
13
9
19

2
7
7
13.3

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

88
168
783
1 200
1 782
2 747
1 364
3 312
6 246
16 617

370
292
1 944
1 996
5 614
7 275
2 650
17 423
15 795
46 003

165.8
214.1
1 032.5
1 623.3
2 321.9
3 621.7
1 642.2
6 202.1
8 526.1
23 641.2

21
4
20
4
40
10
158
15
2 928
45

127
31
190
31
341
90
576
192
6 811
624

47.5
15.8
39.7
15
84.8
21.3
226
34.5
3 853
121.6

Since the 74XXX/ISCAS85 circuits have no control variables we abuse the benchmark by
designating a fraction of the input variables as controls.
We define two policies for generating next inputs: random and stationary. The latter
input policy (where the input values do not change in time) is a typical diagnostic worst-case
for system environments which are, for example, paused pending diagnostic investigation,
and it provides us with useful bounds for analyzing Fractals performance.
Note that the use of non-characterizing sets of diagnoses (see Def. 4) may lead to a situation in which the real (injected) fault is not in the initial set of diagnoses. In such a case the
set of remaining diagnoses (S) may become an empty set after some number of Fractal
steps. Although this gives us some diagnostic information, this is an undesirable situation
and non-characterizing sets of diagnoses should represent most of the diagnostic probability mass to minimize the likelihood of such cases. We have constructed our experimental
benchmark of initial observations in such a way as to avoid such cases.
321

fiFeldman, Provan, & van Gemund

6.2 Expected Number of MC Diagnoses
We have observed that the error of Alg. 1 is insensitive to the number or the composition
of the input variables. It can be seen that the value of the expected number of diagnoses
E approaches the exact value E when increasing the number of samples n. In particular,
E is equal to the exact value of the expected number of MC diagnoses E, when all possible
input values are considered. Figure 6 shows examples of E approaching E for three of our
benchmark models.
c432

c880

200

c1908

160

E

180

40

E

150

38

140
160

130

36

120

140
0

100

200 300
step

400

500

110

E
E
0

100

200 300
step

400

500

34

0

100

200 300
step

400

500

Figure 6: Convergence of expected number of MC diagnoses with increasing sample size
Terminate approximates the intermediate value of E by computing the sequence E = hE1 ,
E2 , . . ., En i. The standard error of the mean of E is defined as:
s
SEME =  ,
n

(9)

where s is the standard deviation of E. We have set Terminate to terminate Alg. 1 when
n > 15 and SEME < , where  is a circuit-dependent threshold constant. Table 5 shows 
for the various circuits we have experimented on.
Table 5: Termination parameters for Alg. 1
Original
Mean n Max n



74182
74L85
74283
74181

0.1
0.11
0.2
0.4

52.7
176.2
139.9
169.1

110
246
225
203

0.01
0.03
0.01
0.07

151.6
170.3
211.3
143.2

223
212
243
181

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.72
0.77
3.57
4.3
14.01
12.77
13.6
23.35
33.18
68.11

48.2
36.4
93.9
51.3
19.5
40.5
78.9
34.2
37.2
68.7

99
61
163
93
35
78
196
39
144
91

0.18
0.02
0.1
0.01
0.62
0.1
0.66
0.09
19.1
3.73

108.6
55.7
156.3
121.4
18.8
65.2
89.6
36.0
39.4
73

158
92
204
148
25
102
132
48
74
122

322



Reduced
Mean n Max n

Name

fiA Model-Based Active Testing Approach to Sequential Diagnosis

We have determined  using the following procedure. First, for each circuit, we choose an
arbitrary initial observation and a small set IN of input variables (|IN| = 8). The small
cardinality of IN allows us to compute true values of E. Next, for each circuit we run 10
pseudo-random experiments. For  we choose the smallest value of SEME such that its
corresponding E is within 95% of E. Table 5 shows the average and maximum number of
steps in which Alg. 1 reaches this value. In all cases an upper bound of n = 100 is a safe
termination criterion.
6.3 Comparison of Algorithms
Consider a weak-fault model of a chain of n inverters and a set of MC diagnoses D (initially,
|D| = n). At each step single-variable probing can eliminate at most 0.5|D| diagnoses. It can
also be shown that halving the expected number of remaining MC diagnoses is a theoretical
bound of any one-step lookahead strategy. As a result we use the geometric decay curve
N (k) = N0 pk + N

(10)

as a model of the diagnosis decay. In this case, N0 is the initial number of diagnoses, N is
the value to which |(S)| converges, and p is the decay rate constant. For probing, N = 1.
In all our experiments we will fit both the expected number of remaining MC diagnoses E
and the actual number or remaining MC diagnoses (S) to Eqn. 10.
6.3.1 FractalATPG
Figure 7 shows the reduction of the expected number of MC diagnoses as a function of (1)
the number of control variables |CTL| and (2) the time k. One can easily see that a global
optimum is reached quickly on both independent axes. This decay is shown for both c432
(Fig. 7, left) and the reduced c880 (Fig. 7, right). The number of control variables |CTL|
varies from 0 to 36 for c432 (|IN| = 36) and from 0 to 60 for c880 (|IN| = 60).
c432

c880 (reduced)

300
300
200
E

E

200

100

100
0
2

4

6

8 10
12 14
k

30

20

10

0

0

0
2

|CTL|

4

6

20
8 10
12 14
k

60

40
|CTL|

Figure 7: Decay of E, stationary inputs, FractalATPG
Using |(S)| instead of E results in similar plots (there is high correlation between E
and |(S)|), hence we have omitted the |(S)| plots. The minimum, maximum and mean
Pearsons linear correlation coefficient between E from Fig. 7 and the respective |(S)| for
each number of control variables in c432 is min = 0.713, max = 0.999, and avg = 0.951,
323

fiFeldman, Provan, & van Gemund

respectively. The corresponding correlation coefficients for the reduced c880 are min =
0.834, max = 1, and avg = 0.972.
It can be seen that the expected number of remaining diagnoses E quickly reaches a
global optimum when increasing |CTL|, which means that turning even a small number of
input variables into controls allows for a geometric decay of the diagnostic entropy. The
results for the reduced c880 are similar to the non-reduced c432. Hence, identification of
cones helps the performance of the diagnostic oracle, but does not change the convergence
behavior or the effect of the control variables.
Fitting geometric decay curves (Eqn. 10) on the |CTL| axes of Fig. 7 produces better
fits for c880 than for c432. Similarly, the values of N for fits alongside the k-axis are
larger for c432 than for c880. The reason for that is the small number of outputs in c432
(cf. Table 3). In circuits with few outputs, randomly turning a limited number of inputs
into controls may not lead to a fast decay or a small N , as the control-output connectivity
of a model is essential for decreasing the diagnostic uncertainty.
Table 6 and Table 7 summarize a total of 14 000 FractalATPG experiments over the
whole 74XXX/ISCAS85 benchmark. Table 6 shows the correlation between the expected
number of remaining MC diagnoses and the actual number of remaining MC diagnoses. In
the second and third columns of Table 6 we can see the minimum and average correlations
between E and  (S). The third and fourth cases specify the fraction of observations for
which we have  > 0.95 and  > 0.975, respectively. Columns 6  9 repeat this data for the
reduced 74XXX/ISCAS85 circuits.
Table 6: Linear correlation coefficient  of the expected number of remaining MC diagnoses
E and the actual number of remaining diagnoses  (S), |CTL| = 14 |IN|, stationary
inputs, FractalATPG

Name

min

avg

74182
74L85
74283
74181

0.55
0.46
0.46
0.46

0.98
0.91
0.91
0.88

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0
0.5
0.51
0
0.38
0
0.48
0.54
0.42
0.62

0.83
0.87
0.86
0.88
0.87
0.89
0.86
0.93
0.9
0.88

Original
 > 0.95

Reduced
 > 0.95

 > 0.975

min

avg

 > 0.975

0.82
0.52
0.69
0.48

0.79
0.44
0.61
0.39

1
0.45
0.45
0.45

1
0.81
0.84
0.86

1
0.4
0.38
0.44

1
0.39
0.31
0.35

0.24
0.32
0.28
0.32
0.31
0.31
0.29
0.48
0.41
0.44

0.16
0.15
0.18
0.18
0.18
0.16
0.19
0.39
0.24
0.13

0
0
0.51
0
0
0
0.06
0.47
0.4
0.45

0.81
0.86
0.85
0.87
0.79
0.79
0.82
0.88
0.9
0.89

0.29
0.42
0.3
0.47
0.22
0.19
0.3
0.44
0.36
0.43

0.23
0.33
0.19
0.34
0.15
0.12
0.23
0.32
0.21
0.23

Table 7 summarizes the parameters of the geometric decay curves fitted to (S). We can see
that although (S) is well approximated by a geometric decay curve (the average goodness324

fiA Model-Based Active Testing Approach to Sequential Diagnosis

of-fit criterion R2 is 0.84) the average decay constant p is low (0.13 for the non-reduced and
0.22 for the reduced 74XXX/ISCAS85 circuits).
Table 7: Decay rate p (minimal, maximal, and average) and goodness-of-fit R2 (average) of
geometric decay best-fit to (S), |CTL| = 41 |IN|, stationary inputs, FractalATPG

pmin

Original
pmax pavg

2
Ravg

pmin

Reduced
pmax pavg

2
Ravg

74182
74L85
74283
74181

0.3
0.06
0.18
0.11

0.52
0.75
0.68
0.71

0.43
0.48
0.57
0.5

0.95
0.88
0.78
0.86

0.5
0.35
0.31
0.18

0.5
0.64
0.6
0.64

0.5
0.5
0.48
0.5

1
0.92
0.94
0.9

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.03
0.1
0.06
0.03
0.02
0.05
0.02
0.22
0.02
0.56

0.8
0.79
0.84
0.9
0.93
0.91
0.87
0.91
0.91
0.95

0.56
0.64
0.54
0.62
0.65
0.63
0.52
0.65
0.52
0.76

0.81
0.84
0.83
0.81
0.68
0.77
0.85
0.8
0.88
0.61

0.03
0.48
0.06
0.39
0.51
0.14
0.04
0.06
0.02
0.01

0.79
0.76
0.8
0.76
0.74
0.75
0.76
0.79
0.9
0.88

0.52
0.65
0.53
0.63
0.64
0.6
0.45
0.54
0.51
0.58

0.82
0.84
0.87
0.85
0.77
0.8
0.89
0.84
0.89
0.85

Average

0.13

0.82

0.58

0.81

0.22

0.74

0.55

0.87

Name

The decay rate p depends mostly on the circuit topology, hence the large variance in Table 7.
Consider, for example, an artificial topology, where there are n components, and n output
variables that produce the health-state of each component for a specific control assignment
(e.g., a self-test). In this topology p would be very small as a diagnostician needs at most
one test (control assignment) to decrease the number of MC diagnoses to one.
The performance of FractalATPG is determined by the size of the model and the
diagnostic oracle. In the above experiments the overall time for executing a single scenario
varied from 3.4 s for 74182 to 1 015 s for c6288. The satisfiability problems in the ATPG part
were always easy and the DPLL solver spent milliseconds in computing control assignments.
The decay rate of FractalATPG depends on the number and composition of controls.
In what follows we will see that FractalG can achieve a similar decay rate with a smaller
number of control variables.
6.3.2 FractalG
Figure 8 shows the decay in the expected number of remaining MC diagnoses for FractalG.
While the reduction is similar for c432, we can see a steeper reduction in the number of
remaining MC diagnoses on both independent axes. Hence, the greedy algorithm is better
than FractalATPG in identifying control combinations of small size, thereby leading to a
better decay rate.
325

fiFeldman, Provan, & van Gemund

c432

c880 (reduced)

250

100

200
^

^

E

E

150
100

50

50
0
5

10
k

15

30

10

20

0

0

0
20

5

|CTL|

10
k

40
15

60

|CTL|

Figure 8: Decay of E (left) and (S) (right), stationary inputs, FractalG
Table 8 and Table 9 summarize the whole 74XXX/ISCAS85 benchmark. Table 8 shows that
FractalG, similar to FractalATPG, results in high average correlation between (S) and
E (avg > 0.79 for all circuits).
Table 8: Linear correlation coefficient  of the expected number of remaining MC diagnoses
E and the actual number of remaining diagnoses  (S), |CTL| = 14 |IN|, stationary
inputs, FractalG

Name

min

avg

74182
74L85
74283
74181

0.03
0
0
0

0.88
0.72
0.5
0.56

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.01
0.01
0.05
0.08
0.05
0.01
0.34
0.27
0.09
0.78

0.75
0.88
0.77
0.86
0.81
0.83
0.73
0.78
0.81
0.86

Original
 > 0.95

Reduced
 > 0.95

 > 0.975

min

avg

 > 0.975

0.39
0.12
0.08
0.05

0.18
0.06
0.03
0.02

1
0.01
0
0

1
0.66
0.48
0.55

1
0.16
0.12
0.09

1
0.14
0.11
0.07

0.07
0.29
0.09
0.36
0.25
0.38
0.09
0.05
0.11
0.06

0.02
0.08
0.06
0.21
0.14
0.22
0.05
0
0.05
0.06

0.01
0
0
0.42
0
0.01
0
0
0.1
0.21

0.68
0.85
0.73
0.9
0.8
0.76
0.7
0.6
0.78
0.83

0.07
0.33
0.08
0.39
0.4
0.37
0.04
0.1
0.09
0.13

0.05
0.2
0.04
0.16
0.3
0.26
0.01
0.07
0.04
0.01

The decay rates of FractalATPG and FractalG are similar (cf. Table 7 and Table 9),
but, as is visible from Fig. 8, FractalG reduces the number of remaining MC diagnoses
more quickly, with fewer control variables. The c432 combinational circuit is difficult for
active testing because it has a small number of outputs compared to the number of inputs
(cf. Table 3), hence reducing the diagnostic utility.
To summarize the effect of the number of controls on the diagnostic convergence, we
again fit the geometric decay curve (Eqn. 10) to (S) for each of the 100 initial observation
326

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Table 9: Decay rate p (minimal, maximal, and average) and goodness-of-fit R2 (average) of
geometric decay best-fit to (S), |CTL| = 14 |IN|, stationary inputs, FractalG
pmin

Original
pmax pavg

2
Ravg

pmin

Reduced
pmax pavg

2
Ravg

74182
74L85
74283
74181

0.24
0.05
0.12
0.15

0.53
0.74
0.67
0.75

0.43
0.47
0.42
0.48

0.95
0.9
0.9
0.9

0.5
0.25
0.35
0.15

0.5
0.65
0.58
0.69

0.5
0.49
0.44
0.44

1
0.93
0.96
0.93

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.04
0.09
0.12
0.19
0.32
0.21
0.34
0.3
0.04
0.08

0.88
0.88
0.67
0.87
0.73
0.74
0.63
0.83
0.81
0.54

0.56
0.71
0.42
0.63
0.53
0.53
0.53
0.61
0.5
0.34

0.83
0.81
0.9
0.87
0.87
0.87
0.91
0.83
0.9
0.92

0.03
0.34
0.07
0.11
0.05
0.15
0.01
0.06
0.08
0.16

0.86
0.85
0.83
0.82
0.84
0.81
0.8
0.86
0.77
0.83

0.59
0.68
0.52
0.68
0.59
0.6
0.44
0.58
0.47
0.59

0.8
0.85
0.88
0.85
0.83
0.8
0.9
0.82
0.89
0.84

Average

0.16

0.73

0.51

0.88

0.17

0.76

0.54

0.88

Name

vectors and various |CTL|. In this case, N0 is the initial number of diagnoses, N is the
value to which |(S)| converges, and p is the decay constant (the most important parameter
of our fits). For an easy circuit with chain topology, for p = 12 , N0 halves every k steps,
as in binary search, hence p corresponds to one bit. For p = 41 , p corresponds to two bits.
Table 10: Mean p for various numbers of control bits, stationary input policy, FractalG
Name

3 bits

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.61
0.79
0.5
0.71
0.68
0.45
0.39
0.52
0.31
0.62

Original
4 bits 5 bits
0.69
0.83
0.55
0.72
0.7
0.49
0.38
0.62
0.41
0.77

0.42
0.77
0.62
0.59
0.41
0.39
0.43
0.67
0.23
0.3

3 bits
0.7
0.58
0.49
0.8
0.54
0.39
0.79
0.81
0.64
0.59

Reduced
4 bits 5 bits
0.71
0.62
0.47
0.82
0.52
0.44
0.8
0.72
0.7
0.34

0.57
0.52
0.44
0.75
0.3
0.42
0.61
0.79
0.59
0.38

Table 10 shows the average p over all initial observations and for various numbers of control
bits b = lg |CTL|. Table 10 does not include data for the 74XXX circuits as they do not
have enough inputs (we need circuits with at least 32 inputs). From Table 10 it is visible
327

fiFeldman, Provan, & van Gemund

that an exponential increase in the number of control variables does not lead to a significant
decrease in p. Hence, for ISCAS85, even turning a small number of the input variables into
controls leads to a near-optimal decrease in the number of remaining MC diagnoses.
The performance of FractalG was worse than that of FractalATPG due to the multivariable expectation. The running time varied between 7.1 s for 74182 and 2 382 s for
c6288. Most of the CPU time was spent in the Expectation subroutine (cf. Alg. 1).
Each consistency check was computationally easy, but for each circuit there were thousands
of them. Hence, improving the performance of LTMS would lead to an increase of the
performance of FractalG.
6.3.3 FractalP
We next discuss FractalP. As mentioned earlier, probing is different from active testing
as it assumes full observability of the model, i.e., all internal variables can be measured (cf.
Sec. 5). Furthermore, probing considers one internal variable per step, while active testing
assigns value to all control variables.5
The value of the decay rate p depends on (1) the topology of the circuit, (2) the initial
observation and (3) the values of the subsequent probes. For probing in ISCAS85 we see
that the values of the decay rate p are close to 0.5 for both (S) and E. Figure 9 shows
the actual and expected number of remaining MC diagnoses ( (S) and E, respectively)
and a geometric fit to E for three probing scenarios.
c3540 (reduced)

c432

c5315

300

300

k

k

N0 p + N
(S )
E

200

N0 p + N
(S )
E

200

8000

N0 p k + N
(S )
E

6000
4000

100

100

2000
0

2

4

6
k

8

10

12

0

2

4

6
k

8

10

12

0

2

4

6
k

8

10

12

Figure 9: Actual number of remaining MC diagnoses (S), expected number of remaining
MC diagnoses E, and a geometric decay fit to (S), stationary inputs, FractalP

Each plot in Fig. 9 shows a single probing session with a single initial observation. Figure 10
shows the goodness-of-fit criterion R2 vs. the decay rate constant p for all 100 observations
and each of the 10 multiple runs of the Fig. 9 circuits.
It is visible from Fig. 10 that the absolute values of R2 are (in most of the cases) close
to 1. This is an indicator that the probing experiments fit the geometric decay model given
in Eqn. 10 well. Figure 10 shows a bad topology (c432 on the left), and a good topology
(c5315 on the right) that achieves decay rate p close to 0.5 (0.38 < p < 0.58) with very high
accuracy of the fit (0.9896  R2  1).
The expected number of remaining MC diagnoses is a good predictor of the actual number of
MC diagnoses for all ISCAS85 circuits, as is shown in Table 11. The absolute values, again
5. There exist multi-probe generalizations of probing (de Kleer et al., 1992).

328

fiA Model-Based Active Testing Approach to Sequential Diagnosis

c5315

c3540 (reduced)
1

0.95

0.995

0.995

0.9

0.99

2

R

0.985

0.985

0.85

0.99

R

2

1

R

2

c432
1

0.8

0.98

0.98

0.75

0.975

0.975

0

0.5
p

1

0

0.5
p

0

1

0.5
p

1

Figure 10: Geometric decay rate vs. goodness-of-fit for (S), FractalP
depend on the topology, and we can see a smaller correlation  for some c432 observations. In
most of the cases, however, the correlation is significant, e.g., for all circuits and observations
except c432 we have  > 0.95.
Table 11: Linear correlation coefficient  of the expected number of remaining MC diagnoses E and the actual number of remaining diagnoses  (S), stationary inputs,
FractalP

Name

min

avg

74182
74L85
74283
74181

0.83
0.77
0.97
0.96

0.95
0.97
0.99
0.99

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.66
0.97
0.98
0.99
0.98
0.98
0.97
0.99
0.92
0.95

0.97
1
1
1
1
1
1
1
1
1

Original
 > 0.95

Reduced
 > 0.95

 > 0.975

min

avg

 > 0.975

0.64
0.87
1
1

0.6
0.67
1
0.97

1
0.83
0.83
0.92

1
0.99
0.98
0.99

1
0.92
0.83
0.95

1
0.87
0.76
0.92

0.83
1
1
1
1
1
1
1
1
1

0.67
1
1
1
1
1
1
1
1
0.99

0.62
0.91
0.92
0.86
0.65
0.7
0.97
0.7
0.98
0.82

0.96
0.98
0.99
0.98
0.97
0.96
1
0.98
1
0.96

0.76
0.87
0.99
0.88
0.86
0.72
1
0.91
1
0.7

0.62
0.76
0.96
0.79
0.68
0.55
1
0.81
1
0.51

In the second and third columns of Table 11 we can see the minimum and average correlations between E and  (S). The third and fourth cases specify the fraction of observations
for which we have  > 0.95 and  > 0.975, respectively. Columns 6  9 repeat this data for
the reduced 74XXX/ISCAS85 circuits.
Table 12 summarizes the decay rate p and the goodness-of-fit criterion R2 for all observations and circuits. For c432, the values of p and R2 are more dispersed, while in the
other experiments p strongly resembles that of chained-elements (i.e., p is close to 0.5).
The minimum, maximum and average values of p (per circuit) are given in columns pmin ,
pmax , and pavg , respectively.
329

fiFeldman, Provan, & van Gemund

Table 12: Decay rate p (minimal, maximal, and average) and goodness-of-fit R2 (average)
of geometric decay best-fit to (S), stationary inputs, FractalP

pmin

Original
pmax pavg

2
Ravg

pmin

Reduced
pmax pavg

2
Ravg

74182
74L85
74283
74181

0.26
0.21
0.31
0.3

0.64
0.7
0.64
0.66

0.54
0.52
0.49
0.5

0.95
0.97
0.99
0.99

0.5
0.25
0.4
0.27

0.5
0.55
0.58
0.56

0.5
0.45
0.49
0.42

1
0.97
0.96
0.99

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

0.1
0.4
0.36
0.39
0.39
0.37
0.38
0.4
0.92
0.95

0.82
0.57
0.61
0.6
0.58
0.6
0.58
0.59
1
1

0.58
0.5
0.51
0.51
0.5
0.51
0.5
0.5
1
1

0.96
1
1
1
1
1
1
1
1
0.99

0.11
0.25
0.2
0.25
0.13
0.22
0.37
0.18
0.98
0.82

0.84
0.6
0.67
0.59
0.81
0.85
0.59
0.89
1
0.96

0.55
0.46
0.46
0.46
0.55
0.65
0.49
0.52
1
0.7

0.95
0.98
0.99
0.98
0.96
0.89
1
0.96
1
0.51

Average

0.41

0.69

0.58

0.99

0.35

0.71

0.55

0.94

Name

6.4 Experimental Summary
If we compare Table 6 and Table 11 we can see that the average correlation avg decreases
significantly. Hence, assuming limited observability (i.e., assuming that not all internals are
measurable) decreases the quality of E as a predictor of (S). The increased statistical
dispersion of  is visible from the increased range max  min (cf. Table 6, where max is
always 1). For example, if we consider c2670, the standard deviation of all E vs. (S)
correlation coefficients  is  = 0.0031 for FractalP and  = 0.0783 for FractalATPG.
The difference in dispersion of correlation coefficients is significant for all circuits, with
smallest values for c432, where it is 0.0038 for FractalP and 0.0825 for FractalATPG .
By comparing Table 7, Table 9, and Table 12 we can see that the mean decay rates
of FractalATPG, FractalG, and FractalP are similar (the average p of FractalG is
0.7 while the average p of FractalATPG is 0.73). The average goodness-of-fit criterion R2
for exponential decays is always good (0.88 for FractalG, 0.84 for FractalATPG), and
almost perfect in probing (0.97).
The summary of our experiments is best shown in Fig. 11. To factor out sampling
error and to be able to perform exhaustive computations, we have chosen the smallest
74182 circuit. The original 74182 (a 4-bit carry-lookahead generator) has 19 components,
9 inputs, and 5 outputs. We have turned four of the inputs into controls (hence, |IN| = 4
and |CTL| = 4).
We have considered a random control policy in addition to FractalP, FractalATPG,
and FractalG. With a random control policy, at each step, a random value is assigned to
each control variable. We have also shown an exhaustive control search where the expected
330

fiA Model-Based Active Testing Approach to Sequential Diagnosis

number of remaining MC diagnoses is computed at each step, and for each possible control
combination. This works with 74182 but leads to a combinatorial blow-up with any other
(larger) circuit.
74182, geometric decay fit

74182, remaining number of MC diagnoses
40

40
Fractal P
Fractal ATPG
Fractal G
random controls
best controls

30

(S)

25

30

20
15

20
15
10

5

5
0
2

4

6

8

random controls
best controls

25

10

0

Fractal P
Fractal ATPG
Fractal G

35

N0 p k + N1

35

2

4

6

8

k

k

Figure 11: Comparison of all control policies
To reduce the stochastic error when plotting Fig. 11, we have replaced the sampling (for
computing an expected number of remaining MC diagnoses) with an exhaustive method;
this is possible as |IN| = 5. The only randomized decision is to choose the actual fault from
the initial ambiguity group. To reduce the error due to this stochastic fault injection, we
have tested each of the 5 control policies 100 times.
We can see in Fig. 11 that the least informed control policy (the random control policy
simply does not use E) shows the worst decay in the number of remaining diagnoses. On
the other extreme, the exhaustive control policy achieves the best decay. The price for
this policy in terms of computational effort, however, is prohibitive. FractalG achieves
decay rates comparable to the exhaustive policy with affordable average-case complexity.
FractalATPG has better complexity than FractalG, but the whole decay rate curve of
FractalATPG is bounded from below by the one computed by FractalG.
Probing does not compare to active testing as both approaches have different assumptions on the observability of the model. Figure 11 shows the decay rate of probing to
illustrate the different decay curves depending on the observability assumptions. In this
experiment the probing decay rate geometric fit with p = 12 almost perfectly fits the actual
number of remaining MC diagnoses.

7. Conclusions
We have devised an algorithm, FractalG, for active testing that is (1) computationally
efficient and (2) rapidly reduces the diagnostic uncertainty (measured as the number of
remaining MC diagnoses) by manipulating a set of control variables. As fully optimizing (2)
leads to a combinatorial blow-up, FractalG achieves a compromise between (1) and (2) by
using a greedy approximation approach for searching over the space of control assignments
and a stochastic sampling method for computing the number of remaining MC diagnoses.
The result is a fast algorithm (optimizing a whole Fractal scenario takes between 1 s
331

fiFeldman, Provan, & van Gemund

for 74182 and 40 min for c6288) that decreases the diagnostic uncertainty according to
a geometric decay curve. This geometric decay curve fits the Fractal data well (the
goodness-of-fit criterion R2 is 0.88 on average) and provides steep decay (the average decay
rate p is 0.7).
We have applied FractalG to the real-world problem of reducing the diagnostic uncertainty of a heavy-duty printer (Feldman, 2010). For that purpose, we have modeled the
Paper Input Module (PIM). In the PIM case-study, FractalG computed the most informative tests in troubleshooting multiple sensor and component failures. This happens even
with a coarse-grained device model (only a few constraints per component), which shows
an unexpected benefit of Fractal: trade-off of modeling complexity vs. test effort.
The optimality of FractalG depends on the topology of and constraints on the input
model. We can create models leading to arbitrarily bad optimality of FractalG by, for
example, directly encoding truth tables in SD. In practical situations, however, controls are
independent. That means that applying a single control rarely undoes the effect of the
previous ones. This also happens when arbitrary inputs are converted to controls, as in our
experimentation benchmark. Consider, for example, a multiplier (c6288). Leaving out some
of the inputs leads to dont cares in the output and hence some components (full-adders,
and-gates) will remain untested. Subsequently assigning values to these left-out inputs will
unambiguously exonerate or blame these untested components, which will help narrowing
down the set of diagnostic hypotheses.
The most important benefit in applying Fractal to industrial cases is that active
testing trade-offs modeling fidelity for computational complexity and extra testing. This
enables users to achieve good diagnostic certainty without the large cost traditionally associated with developing high fidelity models based on physics of failure and other precision
approaches.
We have compared the optimality and performance of FractalG to an ATPG-based
algorithm for sequential diagnosis, FractalATPG. While the average decay rate of both
algorithms is similar (average p of FractalATPG is 0.73), the average goodness-of-fit criterion R2 of FractalATPG is lower (0.84), which means that FractalG is consistently
closer to the optimal solution than is FractalATPG . FractalG has achieved better exponential decay compared to all algorithms except exhaustive control search. For example,
the difference in the decay rate p between FractalG and exhaustive search for 74182 is
5.4%. The exhaustive control approach, however, takes minutes to complete even for a
circuit as simple as 74182, and times-out with any model having more than 20 controls. As
a result, we can conclude that FractalG trades off a small decrease in p for a significant
performance speedup.

References
Alur, R., Courcoubetis, C., & Yannakakis, M. (1995). Distinguishing tests for nondeterministic and probabilistic machines. In Proc. ACM Symposium on Theory of Computing,
pp. 363372.
Brglez, F., & Fujiwara, H. (1985). A neutral netlist of 10 combinational benchmark circuits
and a target translator in Fortran. In Proc. ISCAS85, pp. 695698.
332

fiA Model-Based Active Testing Approach to Sequential Diagnosis

Brodie, M., Rish, I., Ma, S., & Odintsova, N. (2003). Active probing strategies for problem
diagnosis in distributed systems. In Proc. IJCAI03, pp. 13371338.
Bushnell, M. L., & Agrawal, V. D. (2000). Essentials of Electronic Testing for Digital,
Memory and Mixed-Signal VLSI Circuits. Kluwer Academic Publishers, Boston.
de Kleer, J. (1986). Problem solving with the ATMS. Artificial Intelligence, 28 (2), 197224.
de Kleer, J. (2008). An improved approach for generating Max-Fault Min-Cardinality diagnoses. In Proc. DX08, pp. 247252.
de Kleer, J., Raiman, O., & Shirley, M. (1992). One step lookahead is pretty good. In
Readings in Model-Based Diagnosis, pp. 138142. Morgan Kaufmann Publishers, San
Francisco.
de Kleer, J., & Williams, B. (1987). Diagnosing multiple faults. Artificial Intelligence,
32 (1), 97130.
Deb, S., Ghoshal, S., Malepati, V. N., & Kleinman, D. L. (2000). Tele-diagnosis: Remote
monitoring of large-scale systems. In Proc. AEROCONF00, Vol. 6, pp. 3142.
Esser, M., & Struss, P. (2007). Fault-model-based test generation for embedded software.
In Proc. IJCAI07, pp. 342347.
Feldman, A. (2010). Approximation Algorithms for Model-Based Diagnosis. Ph.D. thesis,
Delft University of Technology.
Feldman, A., Provan, G., & van Gemund, A. (2008). Computing observation vectors for
Max-Fault Min-Cardinality diagnoses. In Proc. AAAI08, pp. 919924.
Feldman, A., Provan, G., & van Gemund, A. (2009a). FRACTAL: Efficient fault isolation
using active testing. In Proc. IJCAI09, pp. 778784.
Feldman, A., Provan, G., & van Gemund, A. (2009b). Solving strong-fault diagnostic models
by model relaxation. In Proc. IJCAI09, pp. 785790.
Forbus, K., & de Kleer, J. (1993). Building Problem Solvers. MIT Press.
Hansen, M., Yalcin, H., & Hayes, J. (1999). Unveiling the ISCAS-85 benchmarks: A case
study in reverse engineering. IEEE Design & Test, 16 (3), 7280.
Heinz, S., & Sachenbacher, M. (2008). Using model counting to find optimal distinguishing
tests. In Proc. of COUNTING08, pp. 91106.
Ibarra, O. H., & Sahni, S. K. (1975). Polynomially complete fault detection problems. IEEE
Trans. on Computers, 24 (3), 242249.
Kuhn, L., Price, B., de Kleer, J., Do, M., & Zhou, R. (2008). Pervasive diagnosis: Integration
of active diagnosis into production plans. In Proc. DX08, pp. 106119.
Kundakcioglu, O. E., & Unluyurt, T. (2007). Bottom-up construction of minimum-cost
and/or trees for sequential fault diagnosis. IEEE Trans. on SMC, 37 (5), 621629.
Pattipati, K., & Alexandridis, M. (1990). Application of heuristic search and information
theory to sequential fault diagnosis. IEEE Trans. on SMC, 20 (4), 872887.
Pietersma, J., & van Gemund, A. (2006). Temporal versus spatial observability in modelbased diagnosis. Systems, Man and Cybernetics, 2006, 6, 53255331.
333

fiFeldman, Provan, & van Gemund

Prasad, M. R., Chong, P., & Keutzer, K. (1999). Why is ATPG easy. In Proc. DAC99, pp.
2228.
Raghavan, V., Shakeri, M., & Pattipati, K. (1999). Optimal and near-optimal test sequencing algorithms with realistic test models. IEEE Trans. on SMC, 29 (1), 1126.
Rish, I., Brodie, M., & Ma, S. (2002). Accuracy vs. efficiency trade-offs in probabilistic
diagnosis. In Proc. AAAI02, pp. 560566.
Shakeri, M. (1996). Advances in System Fault Modeling and Diagnosis. Ph.D. thesis,
University of Connecticut.
Shakeri, M., Raghavan, V., Pattipati, K. R., & Patterson-Hine, A. (2000). Sequential testing
algorithms for multiple fault diagnosis. IEEE Trans. on SMC, 30 (1), 114.
Siddiqi, S., & Huang, J. (2007). Hierarchical diagnosis of multiple faults. In Proc. IJCAI07,
pp. 581586.
Stephan, P., Brayton, R., & Sangiovanni-Vincentelli, A. (1996). Combinational test generation using satisfiability. IEEE Trans. on CAD of Integrated Circuits and Systems,
15 (9), 11671176.
Struss, P. (1994). Testing physical systems. In Proc. AAAI94, pp. 251256.
Tu, F., & Pattipati, K. (2003). Rollout strategies for sequential fault diagnosis. IEEE Trans.
on SMC, 33 (1), 8699.

334

fiJournal of Artificial Intelligence Research 39 (2010) 581632

Submitted 12/09; published 11/10

Which Clustering Do You Want?
Inducing Your Ideal Clustering with Minimal Feedback
Sajib Dasgupta
Vincent Ng

sajib@hlt.utdallas.edu
vince@hlt.utdallas.edu

Human Language Technology Research Institute
University of Texas at Dallas
800 West Campbell Road; Mail Station EC31
Richardson, TX 75080-3021 U.S.A.

Abstract
While traditional research on text clustering has largely focused on grouping documents
by topic, it is conceivable that a user may want to cluster documents along other dimensions,
such as the authors mood, gender, age, or sentiment. Without knowing the users intention,
a clustering algorithm will only group documents along the most prominent dimension,
which may not be the one the user desires. To address the problem of clustering documents
along the user-desired dimension, previous work has focused on learning a similarity metric
from data manually annotated with the users intention or having a human construct a
feature space in an interactive manner during the clustering process. With the goal of
reducing reliance on human knowledge for fine-tuning the similarity function or selecting
the relevant features required by these approaches, we propose a novel active clustering
algorithm, which allows a user to easily select the dimension along which she wants to
cluster the documents by inspecting only a small number of words. We demonstrate the
viability of our algorithm on a variety of commonly-used sentiment datasets.

1. Introduction
Text clustering is one of the major application domains for demonstrating the viability of
a clustering algorithm. While traditional research on text clustering has largely focused
on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment. Since
virtually all existing text clustering algorithms can produce just one clustering of a given
set of documents, a natural question is: is this clustering necessarily the one the user desires? In other words, can a text clustering algorithm always produce a clustering along the
user-desired dimension?
The answer to this question depends to a large extent on whether the user can successfully communicate her intention to the clustering algorithm. Traditionally, this can be
achieved by designing a good similarity function that can capture the similarity between
a pair of documents, so that her ideal clustering can be produced. This typically involves
having her identify a set of features that is useful for inducing the desired clusters (Liu, Li,
Lee, & Yu, 2004). However, manually identifying the right set of features is both timeconsuming and knowledge-intensive, and may even require a lot of domain expertise. The
fact that the resulting similarity function is typically not easily portable to other domains is
particularly unappealing from a machine-learning perspective. To overcome this weakness,
c
2010
AI Access Foundation. All rights reserved.

fiDasgupta & Ng

researchers have attempted to learn a similarity metric from side information (Xing, Ng,
Jordan, & Russell, 2002), such as constraints on which pairs of documents must or must
not appear in the same cluster (Wagstaff, Cardie, Rogers, & Schrodl, 2001).
By contrast, recent work has focused on active clustering, where a clustering algorithm
can incorporate user feedback during the clustering process to help ensure that the documents are grouped according to the user-desired dimension. One way to do this is to have
the user incrementally construct a set of relevant features in an interactive fashion (Bekkerman, Raghavan, Allan, & Eguchi, 2007; Raghavan & Allan, 2007; Roth & Small, 2009).
Another way is to have the user correct the mistakes made by the clustering algorithm in
each clustering iteration by specifying whether two existing clusters should be merged or
split (Balcan & Blum, 2008). A major drawback associated with these active clustering
algorithms is that they involve a considerable amount of human feedback, which needs to
be provided in each iteration of the clustering process. Furthermore, identifying clusters
for merging or splitting in Balcan and Blums algorithm may not be as easy as it appears:
for each merge or split decision the user makes, she has to sample a large number of
documents from the cluster(s), read through the documents, and base her decision on the
extent to which the documents are (dis)similar to each other.
In this article, we attack the problem of clustering documents according to user interest
from a different angle. We aim to have a knowledge-lean approach to this problem  an
approach that can produce a clustering of the documents along the user-desired dimension
without relying on human knowledge for fine-tuning the similarity function or selecting
the relevant features, unlike existing approaches. To this end, we propose a novel active
clustering algorithm, which assumes as input a simple feature representation (composed of
unigrams only) and a simple similarity function (i.e., the dot product), and operates by
(1) inducing the important clustering dimensions1 of a given set of documents, where each
clustering dimension is represented by a (small) number of automatically selected words
that are representative of the dimension; and (2) have the user choose the dimension along
which she wants to cluster the documents by examining these automatically selected words.
In comparison to the aforementioned feedback mechanisms, ours is arguably much simpler:
we only require that the user have a cursory look at a small number of features for each
dimension once and for all, as opposed to having the user generate the feature space in an
interactive manner or identify clusters that need to be merged or split in each clustering
iteration.
We evaluate our active clustering algorithm on the task of sentiment-based clustering,
where the goal is to cluster a set of documents (e.g., reviews) according to the polarity
(e.g., thumbs up or thumbs down) expressed by the author without using any labeled
data. Our decision to focus on sentiment-based clustering is motivated by several reasons.
One reason is that there has been relatively little work on sentiment-based clustering. As
mentioned before, existing work on text clustering has focused on topic-based clustering,
where high accuracies can be achieved even for datasets with a large number of classes
(e.g., 20 Newsgroups); and despite the large amount of recent work on sentiment analysis
1. We use the term clustering dimension to refer to a dimension along which a set of documents can be
clustered. For example, a set of movie reviews can be clustered according to genre (e.g., action, romantic,
or documentary) or sentiment (e.g., positive, negative, neutral).

582

fiInducing Your Ideal Clustering with Minimal Feedback

Review 1
The sound from my system did seem to be a little better
(the CDs were not skipping as much). But the bottom line is it
didnt fix the problem as the CDs are still skipping noticeably,
although not as bad as before. ...
Review 2
John Lynch wrote a classic in Spanish-American Revolutions 1808-1826.
He describes all the events that led to the independence of Latin America from Spain.
The book starts in Rio de La Plata and ends in Mexico and Central America.
Curiously one can note a common pattern of highly stratified societies lead by Spanish ...
The reluctance of Spanish Monarchy (and later even of liberals) led to independence ...
For all of those who are interested in a better understanding of Latin ??this great book is a must.
Lynch cleverly combines historical and economic facts about the Hispanic American societies ...

Table 1: Snippets of two reviews that illustrate the two challenges of polarity classification.
One is that reviews are sentimentally ambiguous (Review 1), and the other is that the
objective materials in a review can significantly outnumber their subjective counterparts
(Review 2).

and opinion mining, much of it has focused on supervised methods (see Pang & Lee, 2008,
for a comprehensive survey of the field).
Another equally important reason for our focus on sentiment-based clustering is concerned with the challenges that this task presents to natural language processing (NLP)
researchers. Broadly speaking, the complexity of sentiment-based clustering arises from
two sources. First, reviews are sentimentally ambiguous, containing both positive and negative sentiment-bearing words and phrases. Review 1 of Table 1 shows a snippet of a
review from the DVD domain that illustrates the sentimental ambiguity problem: while
the phrases a little better, not skipping, and not as bad convey a positive sentiment,
the phrases didnt fix and skipping noticeably are negative sentiment-bearing. Hence,
unless a sentiment analyzer performs deeper linguistic analysis, it will be difficult for the
analyzer to determine the polarity of the review. Second, the objective materials in a review tend to significantly outnumber their subjective counterparts, as a reviewer typically
devotes a large portion of the review to describing the features of a product before assigning a rating to it; consequently, any sentiment analyzer that uses a word- or phrase-based
feature representation will be composed of mostly features that are irrelevant with respect
to polarity determination. Shown in Review 2 of Table 1 is a snippet of a book review
that illustrates this problem. As we can see, all but three words/phrases (classic, great
book, cleverly) in this review correspond to objective materials.
The aforementioned complications present significant challenges even to supervised polarity classification systems, let alone sentiment-based clustering algorithms, which do not
have access to any labeled data. To further illustrate the difficulty that these two complications impose on sentiment-based clustering, consider the task of clustering a set of movie
reviews. Since each review may contain a description of the plot and the authors sentiment,
a clustering algorithm may cluster reviews along either the plot dimension or the sentiment
dimension; and without knowing the users intention, they will be clustered along the most
583

fiDasgupta & Ng

prominent dimension. Assuming the usual bag-of-words representation, the most prominent
dimension will more likely be plot, as it is not uncommon for a review to be devoted almost
exclusively to the plot, with the author briefly expressing her sentiment only at the end of
the review. Even if the reviews contain mostly subjective materials, the most prominent
dimension may still not be sentiment owing to the aforementioned sentimental ambiguity
problem: the presence of both positive and negative sentiment-bearing words in these reviews renders the sentiment dimension hidden (i.e., less prominent) as far as clustering is
concerned.
In sum, our contributions in this article are five-fold.
 We propose a novel active clustering algorithm that can cluster a set of documents
along the user-desired dimension without any labeled data or side information such as
manually specified or automatically acquired must-link and cannot-link constraints.
In comparison to existing active clustering approaches, our algorithm has the appeal
of requiring much simpler human feedback.
 We demonstrate the viability of our algorithm not only by evaluating its performance
on sentiment datasets, but also via a set of human experiments, which is typically
absent in papers that involve algorithms for incorporating user feedback.
 Our results have led to a deeper understanding of spectral clustering. Specifically, we
propose a novel application of the top eigenvectors produced by a spectral clustering
algorithm, where we use them to unveil the important clustering dimensions of a text
collection.
 Our results also have implications for domain adaptation, a topic that has recently
received a lot of attention in the NLP community. Specifically, we show that the
sentiment dimension manually identified for one domain can be used to automatically
identify the sentiment dimension for a new, but similar, domain.
 Preliminary results on datasets that possess more than one clustering dimension (e.g.,
a collection of book and DVD reviews, which can be clustered by sentiment or by the
type of the product concerned) indicate that our algorithm is capable of producing
multiple clusterings of a dataset, one along each dimension. Hence, our algorithm can
potentially reveal more information from a dataset than is possible with traditional
clustering algorithms, which can only produce a single clustering of the data. The
ability to produce multiple clusterings is a particularly useful feature for a user who
does not have any idea of how she wants the documents to be clustered (due to the
lack of knowledge of the data, for instance). Even if a user has some knowledge of the
data and knows how she wants the documents to be clustered, our algorithm can help
unveil other hidden dimensions that she is not previously aware of but may also be
of interest to her.
The rest of this article is organized as follows. Section 2 presents the basics of spectral
clustering, which will facilitate the discussion of our active clustering algorithm in Section
3. We describe our human experiments and evaluation results on several sentiment datasets
in Section 4 and the significance of our work in Section 5. Finally, we discuss related work
in Section 6 and conclude in Section 7.
584

fiInducing Your Ideal Clustering with Minimal Feedback

2. Spectral Clustering
When given a clustering task, an important question to ask is: which clustering algorithm
should we use? A popular choice is k-means. Nevertheless, it is well-known that k-means has
the major drawback of not being able to separate data points that are not linearly separable
in the given feature space (e.g., see Dhillon, Guan, & Kulis, 2004; Cai, He, & Han, 2005).
Moreover, since k-means clusters documents directly in the given feature space, which for
text applications typically comprises hundreds of thousands of features, its performance
could be adversely affected by the curse of dimensionality. Spectral clustering algorithms
were developed in response to these problems with k-means. In this section, we first present
one of the most commonly-used algorithms for spectral clustering (Section 2.1). Then, we
provide the intuition behind spectral clustering (Section 2.2). Finally, we describe two ways
to use the resulting eigenvectors to produce a clustering (Section 2.3).
2.1 Algorithm
Let X={x1 , . . . , xn } be a set of n data points to be clustered, s : X  X   be a similarity
function defined over X, and S be a similarity matrix that captures pairwise similarities
(i.e., Si,j = s(xi , xj )). Like many other clustering algorithms, a spectral clustering algorithm
takes S as input and outputs a k-way partition C = {C1 , C2 , .., Ck } (i.e., ki=1 Ci = X and
i, j : i 6= j = Ci  Cj = ). Equivalently, one can think of spectral clustering as learning
a partitioning function f , which, in the rest of this article, will be represented as a vector
such that f (i)  {1, . . ., k} indicates the cluster to which xi should be assigned. Note
that the cluster labels are interchangeable and can even be renamed without any loss of
generality.
Among the well-known spectral clustering algorithms (e.g., Weiss, 1999; Shi & Malik,
2000; Kannan, Vempala, & Vetta, 2004), we adopt the one proposed by Ng, Jordan, and
Weiss (2001), as it is arguably the most widely-used. Below are the main steps of Ng et
al.s spectral clustering algorithm:
1. Create the diagonal matrix D whose (i,i)-th entry is the sum of the i-th row of S,
and then construct the Laplacian matrix2 L = D 1/2 SD 1/2 .
2. Find the eigenvalues and the eigenvectors of L.
3. Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.
4. Each data point is now rank-reduced to a point in the m-dimensional space. Normalize
each point to unit length (while retaining the sign of each value).
5. Apply k-means to cluster the data points using the resulting m eigenvectors.
In other words, spectral clustering clusters data points in a low-dimensional space, where
each dimension corresponds to a top eigenvector of the Laplacian matrix.
2. We follow Ng et al. (2001) and employ a normalized dual form of the usual Laplacian D  S.

585

fiDasgupta & Ng

2.2 Intuition behind Spectral Clustering
It may not be immediately clear why spectral clustering produces a meaningful partitioning of a set of points. There are theoretical justifications behind spectral clustering, but
since the mathematics is quite involved, we will only provide an intuitive justification of
this clustering technique in a way that is sufficient for the reader to understand our active
clustering algorithm in Section 3, and refer the interested reader to Shi and Maliks (2000)
seminal paper on spectral clustering for details. Since we will only apply spectral clustering
to produce a 2-way clustering of a given set of data points in the rest of this article, we will
center our discussion on 2-way clustering in this subsection.
Spectral clustering employs a graph-theoretic notion of grouping. Specifically, a set of
data points in an arbitrary feature space is represented as an undirected weighted graph,
where each node corresponds to a data point, and the edge weight between two nodes xi
and xj is their similarity, Si,j .
Given this graph formulation, a reasonable way to produce a 2-way partitioning of the
data points is to minimize the similarity between the resulting two clusters, C1 and C2 .
Hence, a reasonable objective function to minimize is the cut value, where
X
Cut(C1 , C2 ) =
Si,j (f (i)  f (j))2 .
i,j

Without loss of generality, we can define f as follows.

1 : i  C1
f (i) =
1 : i  C2
As mentioned before, while we use 1 and 1 as cluster labels here, they are interchangeable
and can in fact be renamed in whatever way we want.
One problem with minimizing the cut value, as noticed by Wu and Leahy (1993), is
that this objective favors producing unbalanced clusters in which one of them contains a
very small number of nodes. In other words, there is a bias towards isolating a small set
of nodes. As mentioned by Shi and Malik (2000), this should not be surprising, since the
number of edges involved in the cut (and hence the cut value) tends to increase as the sizes
of the two clusters become relatively balanced.
A closer examination of the minimum cut criterion reveals the problem: while it minimizes inter-cluster similarity, it makes no attempt to maximize intra-cluster similarity. To
address this weakness, Shi and Malik (2000) propose to minimize instead the normalized
cut value, N Cut, which takes into account both inter-cluster dissimilarity and intra-cluster
similarity. More specifically,
Cut(C1 , C2 )
Cut(C1 , C2 )
+
,
assoc(C1 , C1  C2 ) assoc(C2 , C1  C2 )
P
where assoc(A, B), computed as xi A,xj B Si,j , is the total connection from the nodes in
A to the nodes in B. Given this definition, a cut resulting from unbalanced clusters will no
longer have a small N Cut value. To see the reason, consider the case where C1 consists of
just one node. In this case, assoc(C1 , C1  C2 ) = Cut(C1 , C2 ), making N Cut(C1 , C2 ) large.
N Cut(C1 , C2 ) =

586

fiInducing Your Ideal Clustering with Minimal Feedback

After some algebra, we can express N Cut as follows:
N Cut(C1 , C2 ) =

f T (D  S)f
f T Df

subject to the constraints that (Df )T 1 = 0 and
 rP
d(i)


PiC2

iC1 d(i)
rP
f (i) =
d(i)


  PiC1 d(i)
iC2

: i  C1
: i  C2

where d(i) = D(i, i), as defined in Section 2.1.3 The first constraint, which specifies that
Df is orthogonal to 1, can be intuitively understood as follows: since 1, being a constant
vector where all of its entries are 1, cannot be used to induce a partition, this constraint
avoids the trivial solution in which all points are assigned to the same cluster.
Unfortunately, Papadimitriou proves that minimizing normalized cut is an NP-complete
problem, even for the special case of graphs on regular grids (see Shi & Malik, 2000, for the
proof). Hence, following Shi and Malik, we relax this minimization problem by dropping
the second constraint and allowing each entry of f to take on a real value rather than one
of two discrete values, seeking a real-valued solution to the following problem:
minn

f 

f T (D  S)f
f T Df

(1)

subject to
Df  1.
Assuming that g = D 1/2 f , we can rewrite Problem (1) as
minn

g

gT D 1/2 (D  S)D 1/2 g
gT g

(2)

subject to
g  D 1/2 1.
Following the standard Rayleigh-Ritz theorem, one can prove that the solution to
Problem (2), g, is the eigenvector that corresponds to the second smallest eigenvalue of
D 1/2 (D  S)D 1/2 , or equivalently, the eigenvector that corresponds to the second largest
eigenvector of D1/2 SD 1/2 , which is the Laplacian matrix L defined in Section 2.1. For
simplicity, we will henceforth refer to the eigenvector that corresponds to the n-th largest
eigenvalue of L simply as its n-th eigenvector and denote it as en .4
3. Besides normalized cut, ratio cut (Chan, Schlag, & Zien, 1994), average association (Shi & Malik, 2000),
and min-max cut (Ding, He, Zha, Gu, & Simon, 2001) have also been used as objective functions for
spectral clustering algorithms.
4. Given that Problem (2) involves minimizing a Rayleigh quotient, it may seem somewhat unintuitive that
its solution is the second eigenvector of L rather than its first eigenvector. The reason can be attributed
to the constraint associated with the problem, which specifies that the solution g is perpendicular to
D1/2 1, the first eigenvector of L.

587

fiDasgupta & Ng

This is the idea behind spectral clustering: the second eigenvector of L is an approximate solution to the problem of minimizing normalized cut.5 Of course, since the second
eigenvector is a real-valued solution, we will have to convert it into a partitioning function
so that it can be used to cluster the data points. Section 2.3 explains two simple ways of
converting this eigenvector into a partitioning function.
It turns out that the other eigenvectors of L also convey useful information about the
data. Specifically, if we impose an additional constraint to Problem (2) forcing the solution to be orthogonal to the second eigenvector of L, then the solution becomes the third
eigenvector. Hence, the third eigenvector can be thought of as a suboptimal solution to
Problem (2), meaning that it can also be used to impose a reasonably good partition of
the data points. Perhaps more importantly, since the eigenvectors of L are orthogonal to
each other (because L is symmetric), the clustering produced by using the third eigenvector
is likely to correspond to a different dimension of the data than that produced by the second
eigenvector.
More generally, if we limit the solution space to only those real-valued vectors that are
orthogonal to the first m eigenvectors of L, then the solution to our constrained optimization
problem is the (m + 1)-th eigenvector of L. In other words, each of the top eigenvectors of
L can intuitively be thought of as revealing an important dimension of the data, although
subsequent eigenvectors are progressively less ideal as far as clustering is concerned.
2.3 Clustering with Eigenvectors
As Ng et al. (2001) point out, different authors still disagree on which eigenvectors to
use, and how to derive clusters from them. In this subsection, we describe two common
methods for determining which eigenvectors to use, and for each method, we show how to
derive clusters using the selected eigenvector(s). These methods will serve as baselines in
our evaluation.
2.3.1 Method 1: Using the Second Eigenvector Only
Since Shi and Malik (2000) show that the second eigenvector, e2 , is the approximate solution
to the problem of minimizing the normalized cut, it should perhaps not be surprising that
e2 is commonly chosen as the only eigenvector for deriving a partition. However, since e2
is a real-valued solution to the constrained optimization problem, we need to specify how
we can derive clusters from it.
Clustering using e2 is trivial: since we have a linearization of the points, one simple way
is to determine the threshold for partitioning them. However, we follow Ng et al. (2001)
and cluster the points using 2-means in this one-dimensional space.
2.3.2 Method 2: Using the Top m Eigenvectors
Recall from Section 2.1 that after eigen-decomposing the Laplacian matrix, each data point
is represented by m co-ordinates. In the second method, we use 2-means to cluster the data
points in this m-dimensional space, effectively exploiting all of the top m eigenvectors.
5. In fact, since f = D1/2 g, we have to pre-multiply the second eigenvector of L by D1/2 to get the
solution to Problem (1), but following Ng et al. (2001), we employ the second eigenvector of L directly
for clustering, ignoring the term D1/2 .

588

fiInducing Your Ideal Clustering with Minimal Feedback

3. Our Active Clustering Algorithm
As mentioned before, sentiment-based clustering is challenging, in part due to the fact that
reviews can be clustered along more than one dimension. In this section, we describe our
active clustering algorithm, which makes it easy for a user to specify that the dimension
along which she wants to cluster the data points is sentiment. Recall that our algorithm first
applies spectral clustering to reveal the most important dimensions of the data, and then
lets the user select the desired dimension (i.e., sentiment). To motivate the importance of
user feedback, it helps to understand why the two baseline clustering algorithms described
in Section 2.3, which are also based on spectral methods but do not rely on user feedback, may not always yield a sentiment-based clustering. To begin with, consider the first
method, where only the second eigenvector is used to induce the partition. Recall that the
second eigenvector reveals the most prominent dimension of the data. Hence, if sentiment is
not the most prominent dimension (which can happen if the non-sentiment-bearing words
outnumber the sentiment-bearing words in the bag-of-words representation of a review),
then the resulting clustering of the reviews may not be sentiment-oriented. A similar line
of reasoning can be used to explain why the second baseline clustering algorithm, which
clusters based on all of the top eigenvectors, may not always work well. Since each eigenvector corresponds to a different dimension (and, in particular, some of them correspond to
non-sentiment dimensions), using all of them to represent a review may hamper the accurate computation of the similarity of two reviews as far as clustering along the sentiment
dimension is concerned. In the rest of this section, we discuss in detail the major steps of
our active clustering algorithm, which allows easy incorporation of user feedback.
3.1 Step 1: Identify the Important Clustering Dimensions
We rely on a simple method for identifying the important clustering dimensions of a given
text collection: we employ the top eigenvectors of the Laplacian as the important clustering dimensions. This method is motivated by the fact that e2 , the second eigenvector of
the Laplacian, is the optimal real-valued solution to the objective function that spectral
clustering minimizes (i.e., normalized cut, Shi & Malik, 2000), and is therefore an optimal
clustering dimension. More importantly, we exploit a rarely-utilized observation discussed
in Section 2.2: while the remaining eigenvectors are all suboptimal solutions (with ei being more suboptimal as i increases), the top eigenvectors (i.e., those with small i values),
being less suboptimal, may still yield reasonably good (though not optimal) clusterings of
the data and can therefore serve as good clustering dimensions. Existing applications of
spectral clustering have mainly clustered data points in the space defined by all of the top
eigenvectors, and have not attempted to use each of the ei s (with i > 2) separately to
produce clusterings, unlike ours. Note that the first eigenvector, being a constant vector,
simply assigns all data points to the same cluster and therefore is typically ignored.
3.2 Step 2: Identify the Relevant Features for Each Partition
Given the eigen-decomposition from Step 1, we first obtain the second through the m-th
eigenvectors, which correspond to the most important dimensions of the data. The next
question is: how can we determine which dimension captures the user interest? One way to
589

fiDasgupta & Ng

do this is to have the user inspect each of the m1 partitions of the reviews and decide which
corresponds most closely to a sentiment-based clustering. The main drawback associated
with this kind of user feedback is that the user may have to read a large number of reviews
in order to make a decision. Hence, to reduce human effort, we employ an alternative
procedure: we (1) identify the most informative features for characterizing each partition,
and (2) have the user inspect just the features rather than the reviews. To make it easy for
a human to identify a clustering dimension, the features should be chosen so that they are
useful for distinguishing the reviews in the two clusters.
To identify and rank the informative features, we employ a method that we call maximum
margin feature ranking (MMFR).6 Recall that a maximum margin classifier (e.g., a support
vector machine) separates data points from two classes while maximizing the margin of
separation. Specifically, a maximum margin hyperplane is defined by w  x  b = 0, where
x is a feature vector representing an arbitrary data point, and w (a weight vector) and b (a
scalar) are parameters that are learned by solving the following constrained optimization
problem:
X
1
i
min kwk2 + C
2
i
subject to
ci (w  xi  b)  1  i ,

1  i  n,

where ci  {+1, 1} is the class of the i-th training point xi , i is the degree of misclassification of xi , and C is a regularization parameter that balances training error and model
complexity.
We use w to identify the most informative features for a partition. Note that the most
informative features are those with large absolute weight values: a feature with a large
positive (negative) weight is strongly indicative of the positive (negative) class.7 We exploit
this observation and identify the most informative features for a partition by (1) training a
binary SVM classifier8 on the partition, where data points in the same cluster are assumed
to have the same class value; (2) sorting the features according to the SVM-learned feature
weights; and (3) generating two ranked lists of informative features using the top and bottom
F features, respectively.
Given the ranked lists generated for each of the m  1 partitions, the user will select one
of the partitions/dimensions as most relevant to sentiment by inspecting as many features
in the ranked lists as needed. After picking the most relevant dimension, the user will
label one of the two feature lists associated with this dimension as positive and the other
as negative. Since each feature list represents one of the clusters, the cluster associated
with the positive list is labeled positive and the cluster associated with the negative list
is labeled negative.
6. Note that other commonly-used feature selection techniques such as log-likelihood ratio and information
gain can also be applied to identify these informative features (see Yang & Pedersen, 1997, for an
overview).
7. The notion of using SVM feature weights as measures of feature informativeness has also been explored
in other work. See, for instance, the work of Fung (2003), Gilad-Bachrach, Navot, and Tishby (2004),
and Kugler, Aoki, Kuroyanagi, Iwata, and Nugroho (2005) for details.
8. All the SVM classifiers in this article are trained using the SVMlight package (Joachims, 1999a), with
the learning parameters set to their default values.

590

fiInducing Your Ideal Clustering with Minimal Feedback

In comparison to existing user feedback mechanisms for assisting a clustering algorithm,
ours requires comparatively little human intervention: we only require that the user select a
dimension by examining a small number of features, as opposed to having the user construct
the feature space or identify clusters that need to be merged or split as is required with
other methods.
3.3 Step 3: Identify the Unambiguous Reviews
There is a caveat, however. As mentioned in the introduction, many reviews contain both
positive and negative sentiment-bearing words. These ambiguous reviews are more likely
to be clustered incorrectly than their unambiguous counterparts. Since the ranked lists
of features are derived from each partition, the presence of these ambiguous reviews can
adversely affect the identification of informative features using MMFR. As a result, we
remove the ambiguous reviews before deriving informative features from a partition.
We employ a simple method for identifying unambiguous reviews. In the computation
of eigenvalues, each data point factors out the orthogonal projections of each of the other
data points with which they have an affinity. Ambiguous data points receive the orthogonal
projections from both the positive and negative data points, and hence they have near zero
values in the pivot eigenvectors. In other words, the points with near zero values in the
eigenvectors are more ambiguous than those with large absolute values. We therefore sort
the data points according to their corresponding values in the eigenvector, and keep only
the top n/8 and the bottom n/8 data points. We induce the informative features only from
the resulting 25% of the data points, and present them to the user so that she can select
the desired partition.9
3.4 Step 4: Cluster Along the Selected Eigenvector
Finally, we employ 2-means to cluster all the reviews along the eigenvector selected by the
user, regardless of whether a review is ambiguous or not.

4. Evaluation
In this section, we describe experiments that aim to evaluate the effectiveness of our active
clustering algorithm and provide insights into it.
4.1 Experimental Setup
We begin by discussing the details on the datasets, the document preprocessing method,
the implementation of spectral clustering, and the evaluation metrics.
9. Note that 25% is a somewhat arbitrary choice. Underlying this choice is merely the assumption that a
fraction of the reviews is unambiguous. As we will see in the evaluation section, these reviews can be
classified according to their polarity with a high accuracy; consequently, the features induced from the
resulting clusters are also of high quality. Additional experiments revealed that the list of top-ranking
features does not change significantly when induced from a smaller number of unambiguous reviews.

591

fiDasgupta & Ng

4.1.1 Datasets
We use five sentiment datasets, including the widely-used movie review dataset [MOV]
(Pang, Lee, & Vaithyanathan, 2002) as well as four datasets containing reviews of four
different types of products from Amazon [Books (BOO), DVDs (DVD), Electronics (ELE),
and Kitchen Appliances (KIT)] (Blitzer, Dredze, & Pereira, 2007). Each dataset has 2000
labeled reviews (1000 positives and 1000 negatives). To illustrate the difference between
topic-based clustering and sentiment-based clustering, we will also show topic-based clustering results on POL, a dataset created by taking all the documents from the two sections
of 20 Newsgroups that discuss issues in cryptography and politics, namely, sci.crypt and
talks.politics.
4.1.2 Document Preprocessing
To preprocess a document, we first tokenize and downcase it, and then represent it as a
vector of unstemmed unigrams, each of which assumes a value of 1 or 0 that indicates its
presence or absence in the document. In addition, we remove from the vector punctuation,
numbers, words of length one, and words that occur in only a single review.
Following common practice in the information retrieval community, we also exclude
words with a high document frequency, many of which are stopwords or domain-specific
general-purpose words (e.g., movies in the movie domain). A preliminary examination of
our evaluation datasets reveals that these words typically comprise 12% of a vocabulary.
The decision of exactly how many terms to remove from each dataset is subjective: a large
corpus typically requires more removals than a small corpus. To be consistent, we simply
sort the vocabulary by document frequency and remove the top 1.5%. We will henceforth
refer to this document representation as the bag-of-words (BOW) representation.
4.1.3 Spectral Learning Setup
Following common practice in spectral learning for text domains (e.g., Kamvar, Klein,
& Manning, 2003; Cai et al., 2005), we compute the similarity between two reviews by
taking the dot product of their feature vectors. As in Ng et al.s (2001) spectral clustering
algorithm, we set the diagonal entries of the similarity matrix to 0. In addition, we set m
to 5. In other words, we consider the second through fifth eigenvectors, assuming that they
are sufficient for capturing the desired clusterings.10
4.1.4 Evaluation Metrics
We employ two evaluation metrics. First, we report results for each dataset in terms of
accuracy, which is the percentage of documents for which the label assigned by our system
is the same as the gold-standard label. Second, following Kamvar et al. (2003), we evaluate
the clusters produced by our approach against the gold-standard clusters using the Adjusted
Rand Index (ARI), which is the corrected-for-chance version of the Rand Index. More
specifically, given a set of N data points and two clusterings of these points, U and V ,
10. Note that setting m to 5 is a somewhat arbitrary choice, and that any number of eigenvectors can be
used in our active clustering algorithm.

592

fiInducing Your Ideal Clustering with Minimal Feedback

where U = {U1 , U2 , . . . , Um } has m clusters and V = {V1 , V2 , . . . , Vn } has n clusters, ARI
is computed as follows:
nij 
2

 P bj 
P
 [ i a2i
j 2 ]/
ARI(U, V ) = 1 P a  P b 
P
ai  P
j
i
j 2 ][ i 2
j
2[ i 2 +
P

ij

N
2
bj 
N
2 ]/ 2



In this formula, nij is the number of common objects in Ui and Vj ; whereas ai and bj are the
number of objects in Ui and Vj , respectively. ARI ranges from 1 to 1; better clusterings
have higher ARI values.
4.2 Baseline Systems
In this subsection, we describe our baseline results. The first two baseline systems are the
ones described in Section 2.3, and the last two are arguably more sophisticated clustering
algorithms that are employed in an attempt to strengthen our baseline results.
4.2.1 Clustering Using the Second Eigenvector Only
As our first baseline, we adopt Shi and Maliks (2000) approach and cluster the reviews
using only the second eigenvector, e2 , as described in Section 2.3. Results on POL and the
sentiment datasets, expressed in terms of accuracy and ARI, are shown in row 1 of Tables 2a
and 2b, respectively. Owing to the randomness in the choice of seeds for 2-means, these and
all other experimental results involving 2-means are averaged over ten independent runs.11
As we can see, this baseline achieves an accuracy of 93.7% on POL, but much lower
accuracies (of 5070%) on the sentiment datasets. The same performance trend can be
observed with ARI. These results provide suggestive evidence that producing a sentimentbased clustering requires different features than producing a topic-based clustering, and that
in many cases, the more salient features tend to be topic-based. The difference between
sentiment-based clustering and topic-based clustering will be further illuminated by the
experiments in Section 4.7.
In addition, it is worth noting that this baseline achieves much lower accuracies and
ARI values on BOO, DVD, and ELE than on the remaining two sentiment datasets. Since
e2 captures the most prominent dimension, these results suggest that sentiment dimension
is not the most prominent dimension in these three datasets. In fact, this is intuitively
plausible. For instance, in the book domain, positive book reviews typically contain a
short description of the content, with the reviewer only briefly expressing her sentiment
somewhere in the review. Similarly for the electronics domain: electronic product reviews
are typically aspect-oriented, with the reviewer talking about the pros and cons of each
aspect of the product (e.g., battery, durability). Since the reviews are likely to contain both
positive and negative sentiment-bearing words, the sentiment-based clustering is unlikely
to be captured by e2 .
11. Note that clustering in a one-dimensional space (as in this baseline) yields very stable results regardless
of the choice of seeds: our results over the ten runs exhibit nearly zero variance.

593

fiDasgupta & Ng

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

POL
93.7
95.9
98.7
70.3
93.7

MOV
70.9
58.9
61.8
71.3
70.9

Accuracy
KIT BOO
69.7 58.9
64.0 59.9
62.2 52.5
66.9 52.1
69.7 69.5

DVD
55.3
60.4
50.6
50.3
70.8

ELE
50.8
63.8
50.2
63.8
65.8

(ARI)
DVD
0.01
0.03
0.01
0.01
0.17

ELE
0.01
0.07
0.01
0.08
0.10

(a)

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

POL
0.76
0.84
0.94
0.16
0.76

Adjusted Rand Index
MOV KIT BOO
0.17 0.15 0.03
0.03
0.05 0.04
0.05
0.06 0.01
0.18 0.11 0.01
0.17 0.15 0.15

(b)

Table 2: Results in terms of (a) accuracy and (b) Adjusted Rand Index for the six datasets
obtained using the bag-of-words document representation. The strongest result(s) for each
dataset are boldfaced.

4.2.2 Clustering Using the Top Five Eigenvectors
As our second baseline, we represent each data point using the top five eigenvectors (i.e., e1
through e5 ), and cluster them using 2-means in this five-dimensional space, as described in
Section 2.3. Hence, this can be thought of as an ensemble approach, where the clustering
decision is collectively made by the five eigenvectors.12
Results are shown in row 2 of Tables 2a and 2b.13 In comparison to the first baseline,
we see improvements in accuracy and ARI for POL and the three sentiment datasets on
which the first baseline performs poorly (i.e., BOO, DVD, and ELE), with the most drastic
improvement observed on ELE. However, performance on the remaining two sentiment
datasets deteriorates. These results can be attributed to the fact that for BOO, DVD,
and ELE, e2 does not capture the sentiment dimension, but since some other eigenvector
in the ensemble does, we see improvements. On the other hand, e2 has already captured
the sentiment dimension in MOV and KIT; as a result, employing additional dimensions,
which may not be sentiment-related, may only introduce noise into the computation of the
similarities between the reviews.
12. While the first eigenvector can only produce a trivial clustering in which all data points reside in the
same cluster, it is commonly used in combination with other top eigenvectors to create a low-dimensional
space in which data points are clustered. See the work of Ng et al. (2001) for more details.
13. When clustering in a five-dimensional space, we observe that the results can be highly sensitive to the
choice of seeds. For instance, the variances in the accuracy observed over the ten runs for POL, MOV,
KIT, BOO, DVD, and ELE are 0, 2.38, 19.90, 24.70, 12.76, and 4.43, respectively.

594

fiInducing Your Ideal Clustering with Minimal Feedback

4.2.3 Clustering Using the Interested Reader Model
Our third baseline is Kamvar et al.s (2003) unsupervised clustering algorithm, which, according to the authors, is ideally suited for text clustering, and has recently been proved to
be a special case of ratio-cut optimization (Kulis, Basu, Dhillon, & Mooney, 2009). Specifically, they introduce a new Laplacian inspired by the Interested Reader Model. This
Laplacian is computed as (S + dmax I  D)/dmax , where D and S are defined as in Section
2.1, except that Si,j =0 if i is not one of js k nearest neighbors and j is not one of is k
nearest neighbors; dmax is the maximum rowsum of S; and I is the identity matrix. Since
its performance is highly sensitive to k, we tested values of 10, 15, . . ., 500 for k and report in row 3 of Tables 2a and 2b the best results. Somewhat disappointingly, despite its
algorithmic sophistication and the fact that we are reporting the best results, this baseline
does not offer consistent improvements over the previous two. In comparison to the first
baseline, it achieves better performance on POL but worse performance on all the sentiment
datasets. Like the first baseline, its results on BOO, DVD and ELE are particularly poor.
4.2.4 Clustering Using Non-Negative Matrix Factorization
Non-negative matrix factorization (NMF) has recently been shown by Xu, Liu, and Gong
(2003) to be effective for document clustering. After re-implementing this algorithm, we
evaluate it on our six datasets.14 Shown in row 4 of Tables 2a and 2b are the best results obtained after running the algorithm five times. In comparison to the first baseline,
NMF achieves better performance on ELE, comparable performance on MOV, and worse
performance on the remaining datasets.
4.3 Our Active Clustering Algorithm
In this subsection, we describe human and automatic experiments for evaluating our active
clustering algorithm.
4.3.1 Human Experiments
Unlike the four baselines, our active clustering algorithm requires users to specify which
of the four dimensions (defined by the second through fifth eigenvectors) are most closely
related to sentiment by inspecting a set of features derived from the unambiguous reviews
for each dimension using MMFR. To better understand how easy it is for a human to select
the desired dimension given the features, we performed the experiment independently with
five humans (all of whom are computer science graduate students not affiliated with this
research) and computed the agreement rate.
Specifically, for each dataset, we showed each human judge the top 100 features for
each cluster according to MMFR (see Tables 38 for a subset of these 100 features induced
for each of the six datasets, where the lightly shared columns correspond to the sentiment
dimension selected by the majority of the human judges).15 In addition, we informed her
14. For matrix factorization we use the code downloaded from http://www.csie.ntu.edu.tw/cjlin/nmf/index.html.
15. While all human judges reported that inspecting the top 100 features is sufficient for identifying the
sentiment dimension, we note that a user of our clustering algorithm may request to inspect as many
features as she wants.

595

fiDasgupta & Ng

e2
C1
serder
armenian
turkey
armenians
muslims
sdpa
argic
davidian
dbd@ura
troops
C2
sternlight
wouldn
pgp
crypto
algorithm
isn
likely
access
idea
cryptograph

POL
e3
e4
C1
C1
beyer
serbs
arabs
palestinians
andi
muslims
research
wrong
israelis
department
tim
bosnia
uci
live
ab
matter
z@virginia
freedom
holocaust
politics
C2
escrow
sternlight
algorithm
access
net
des
privacy
uk
systems
pgp

C2
standard
sternlight
des
escrow
employer
net
york
jake
code
algorithm

e5
C1
escrow
serial
algorithm
chips
ensure
care
strong
police
omissions
excepted
C2
internet
uucp
uk
net
quote
ac
co
didn
ai
mit

Table 3: Top ten features induced for each dimension for the POL domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

596

fiInducing Your Ideal Clustering with Minimal Feedback

e2
C1
relationship
son
tale
husband
perfect
drama
focus
strong
beautiful
nature
C2
worst
stupid
waste
bunch
wasn
video
worse
boring
guess
anyway

MOV
e3
e4
C1
C1
production
jokes
earth
kids
sequences
live
aliens
animation
war
disney
crew
animated
alien
laughs
planet
production
horror
voice
evil
hilarious
C2
sex
romantic
school
relationship
friends
jokes
laughs
sexual
cute
mother

C2
thriller
killer
murder
crime
police
car
dead
killed
starts
violence

e5
C1
starts
person
saw
feeling
lives
told
happen
am
felt
happened
C2
comic
sequences
michael
supporting
career
production
peter
style
latest
entertaining

Table 4: Top ten features induced for each dimension for the MOV domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

597

fiDasgupta & Ng

BOO
e2
C1
history
must
modern
important
text
reference
excellent
provides
business
both

e3
C1
series
man
history
character
death
between
war
seems
political
american

e4
C1
loved
highly
easy
enjoyed
children
again
although
excellent
understand
three

e5
C1
must
wonderful
old
feel
away
children
year
someone
man
made

C2
plot
didn
thought
boring
got
character
couldn
ll
ending
fan

C2
buy
bought
information
easy
money
recipes
pictures
look
waste
copy

C2
money
bad
nothing
waste
buy
anything
doesn
already
instead
seems

C2
boring
series
history
pages
information
between
highly
page
excellent
couldn

Table 5: Top ten features induced for each dimension for the BOO domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

598

fiInducing Your Ideal Clustering with Minimal Feedback

ELE
e2
C1
mouse
cable
cables
case
red
monster
picture
kit
overall
paid

e3
C1
music
really
ipod
too
little
headphones
hard
excellent
need
fit

e4
C1
easy
used
card
fine
using
problems
fine
drive
computer
install

e5
C1
amazon
cable
card
recommend
dvd
camera
fast
far
printer
picture

C2
working
never
before
phone
days
headset
money
months
return
second

C2
worked
problem
never
item
amazon
working
support
months
returned
another

C2
money
worth
amazon
over
return
years
much
headphones
sony
received

C2
phone
off
worked
power
battery
unit
set
phones
range
little

Table 6: Top ten features induced for each dimension for the ELE domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

599

fiDasgupta & Ng

e2
C1
love
clean
nice
size
set
kitchen
easily
sturdy
recommend
price
C2
months
still
back
never
worked
money
did
amazon
return
machine

KIT
e3
e4
C1
C1
works
really
water
nice
clean
works
work
too
ice
quality
makes
small
thing
sturdy
need
little
keep
think
best
item
C2
price
item
set
ordered
amazon
gift
got
quality
received
knives

C2
ve
years
love
never
clean
months
over
pan
been
pans

e5
C1
pan
oven
cooking
made
pans
better
heat
cook
using
clean
C2
love
coffee
too
recommend
makes
over
size
little
maker
cup

Table 7: Top ten features induced for each dimension for the KIT domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

600

fiInducing Your Ideal Clustering with Minimal Feedback

e2
C1
worth
bought
series
money
season
fan
collection
music
tv
thought
C2
young
between
actors
men
cast
seems
job
beautiful
around
director

DVD
e3
e4
C1
C1
music
video
collection
music
excellent
found
wonderful
feel
must
bought
loved
workout
perfect
daughter
highly
recommend
makes
our
special
disappointed
C2
worst
money
thought
boring
nothing
minutes
waste
saw
pretty
reviews

C2
series
cast
fan
stars
original
comedy
actors
worth
classic
action

e5
C1
money
quality
video
worth
found
version
picture
waste
special
sound
C2
saw
watched
loved
enjoy
whole
got
family
series
season
liked

Table 8: Top ten features induced for each dimension for the DVD domain. The shaded
columns correspond to the dimensions selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

601

fiDasgupta & Ng

Judge
1
2
3
4
5
Agreement

POL
2,3,4
2,4
4
2,3
2
80%

MOV
2
2
2,4
2
2
100%

KIT
2
2
4
2
2
80%

BOO
4
4
4
4
4
100%

DVD
3
3
3
3
3
100%

ELE
3
3
3
3,4
3
100%

Table 9: Human agreement rate. Also shown are the eigenvectors selected by the five judges.
of the intended dimension: for example, for POL, the judge was told that the intended
clustering was Politics vs. Science. Also, if she determined that more than one dimension
was relevant to the intended clustering, she was instructed to rank these dimensions in
terms of relevance, where the most relevant one would appear first in the list.
The dimensions (expressed in terms of the IDs of the eigenvectors) selected by each of
the five judges for each dataset are shown in Table 9. The agreement rate (shown in the
last row of the table) was computed based on only the highest-ranked dimension selected
by each judge. As we can see, perfect agreement is achieved for four of the five sentiment
datasets, and for the remaining two datasets, near-perfect agreement is achieved. These
results, together with the fact that it took five to six minutes to identify the relevant
dimension, indicate that asking a human to determine the intended dimension based on
solely the informative features is a viable task.
4.3.2 Clustering Results
Next, we cluster all 2000 documents for each dataset using the dimension selected by the
majority of the human judges. The clustering results are shown in row 5 of Tables 2a
and 2b. In comparison to the best baseline for each dataset, we see that our algorithm
performs substantially better on BOO, DVD and ELE, at almost the same level on MOV
and KIT, but slightly worse on POL. Note that the improvements observed for BOO, DVD
and ELE can be attributed to the failure of e2 to capture the sentiment dimension. Perhaps
most importantly, by exploiting human feedback, our algorithm has achieved more stable
performance across the datasets than the four baselines.16
4.3.3 Identification of Unambiguous Documents
Recall that the features with the largest MMFR were computed from the unambiguous
documents only. To get an idea of how accurate our algorithm for identifying unambiguous
documents is, we show in Table 10 the accuracy obtained when the unambiguous documents
in each dataset were clustered using the eigenvector selected by the majority of the judges.
As we can see, the accuracy on each dataset is higher than the corresponding accuracy
shown in row 5 of Table 2a. In fact, an accuracy of more than 85% was achieved on all
16. As in the first baseline, since we are clustering in a one-dimensional space here, the results are not
sensitive to the choice of seeds, yielding zero variance over the ten independent runs.

602

fiInducing Your Ideal Clustering with Minimal Feedback

Accuracy

POL
99.8

MOV
87.0

KIT
87.6

BOO
86.2

DVD
87.4

ELE
77.6

Table 10: Accuracies on unambiguous documents.

# labels

POL
400

MOV
150

KIT
200

BOO
350

DVD
350

ELE
200

Table 11: Transductive SVM results.
but one dataset. This suggests that our method of identifying unambiguous documents is
reasonably accurate.
Note that it is crucial to be able to achieve a high accuracy on the unambiguous documents: if clustering accuracy is low, the features induced from the clusters may not be an
accurate representation of the corresponding dimension, and the human judge may have a
difficult time identifying the intended dimension. In fact, some human judges reported difficulty in identifying the correct dimension for the ELE dataset, and this can be attributed
in part to the low accuracy achieved on the unambiguous documents.
4.3.4 User Feedback Versus Labeled Data
Recall that our four baselines are unsupervised, whereas our algorithm can be characterized
as semi-supervised, as it relies on user feedback to select the intended dimension. Hence, it
should not be surprising to see that the average clustering performance of our algorithm is
better than that of the baselines.
To do a fairer comparison, we conduct another experiment in which we compare our
algorithm against a semi-supervised sentiment classification system, which uses a transductive SVM as the underlying semi-supervised learner. More specifically, the goal of this
experiment is to determine how many labeled documents are needed in order for the transductive learner to achieve the same level of performance as our algorithm. To answer
this question, we first give the transductive learner access to the 2000 documents for each
dataset as unlabeled data. Next, we randomly sample 50 unlabeled documents and assign
them the true label. We then re-train the classifier and compute its accuracy on the 2000
documents. We keep adding more labeled data (50 in each iteration) until it reaches the
accuracy achieved by our algorithm. Results of this experiment are shown in Table 11.
Owing in the randomness involved in the selection of unlabeled documents, these results
are averaged over ten independent runs. As we can see, our user feedback is equivalent to
the effort of hand-annotating 275 documents per dataset on average.
4.3.5 Multiple Relevant Eigenvectors
As seen from Table 9, some human judges selected more than one eigenvector for some
datasets (e.g., {2,3,4} for POL; {2,4} for MOV; and {3,4} for ELE). However, we never took
into account these extra eigenvectors in our previous experiments. To better understand
603

fiDasgupta & Ng

Our system

POL
Acc ARI
95.9 0.84

MOV
Acc ARI
69.1 0.16

ELE
Acc ARI
65.1 0.10

Table 12: Results obtained using multiple relevant eigenvectors for the POL, MOV and
ELE datasets.

Accuracy

POL
99.3

MOV
86.1

KIT
81.7

BOO
79.3

DVD
77.6

ELE
80.6

Table 13: Supervised classification accuracies.
whether these extra eigenvectors can help improve accuracy and ARI, we conduct another
experiment in which we apply 2-means to cluster the documents in the space defined by all
of the selected eigenvectors. Table 12 shows the accuracy and ARI results that are averaged
over ten independent runs. As we can see, the results for POL are considerably better
than those obtained when only the highest-ranked eigenvector is used, suggesting that the
extra eigenvectors contain useful information. However, the results on MOV and ELE drop
slightly with the addition of the extra eigenvectors, indicating that the extra sentiment
dimensions are not useful.
4.3.6 Supervised Classification Results
Next, we present results for supervised classification on our five sentiment datasets. While
one should not expect our largely unsupervised approach to offer comparable performance
to a fully-supervised approach, we believe that having fully-supervised results will enable
the reader to get a sense of where our work stands among existing work on identifying
sentiment in these datasets. Specifically, we report in Table 13 averaged 10-fold crossvalidation accuracies, where an SVM classifier is trained on nine folds and tested on the
remaining fold in each fold experiment. As we can see, our results lag behind the supervised
results by 8.115.2% on these datasets.
4.4 Alternative Document Representations
In the above experiments, we represented each document as a bag of words with the most
frequent 1.5% of the words removed. This is, of course, not the only way to represent a
document. In this subsection, we examine two alternative document representations in an
attempt to better understand the effect of document representation on classification results.
In our first document representation, we represent a document using all of the unigrams
that appear in it and do not remove the frequent words from the document vector. This
bag-of-all-words (BOAW) representation is motivated by the fact that the frequencies of
function words and the like have been shown in many studies to be useful features for various kinds of non-topic-based classification (e.g., Finn & Kushmerick, 2006; Stein, Argamon,
& Frieder, 2006; Abbasi, Chen, & Salem, 2008; Koppel, Schler, & Argamon, 2009). The
604

fiInducing Your Ideal Clustering with Minimal Feedback

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

POL
70.6
94.7
61.2
59.2
84.3

MOV
54.3
60.6
61.1
54.6
65.9

Accuracy
KIT BOO
51.6 52.4
58.0 56.1
57.8 52.4
50.8 50.1
64.8 60.1

DVD
51.2
53.7
50.4
52.9
58.6

ELE
53.1
57.1
50.3
51.4
64.1

(a)

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

POL
0.17
0.80
0.05
0.03
0.47

Adjusted Rand Index (ARI)
MOV KIT
BOO DVD
0.01
0.01
0.01
0.01
0.04
0.03
0.01
0.01
0.05
0.02
0.01
0.01
0.01 0.01 0.01 0.01
0.10
0.09
0.04 0.03

ELE
0.01
0.03
0.01
0.01
0.08

(b)

Table 14: Results in terms of (a) accuracy and (b) Adjusted Rand Index for the six datasets
obtained using the bag-of-all-words document representation. The strongest result(s) for each
dataset are boldfaced.

accuracy and ARI results obtained by re-running our four baselines as well as our system
using this document representation are shown in Tables 14a and 14b, respectively. Comparing Tables 2a and 14a, we can see that when all words are used as features, the best
accuracy achieved for each dataset drops by 311% than when the high-frequency words are
removed before spectral clustering is applied. Similar trends can be observed with the ARI
results shown in Tables 2b and 14b. Overall, these results substantiate our hypothesis that
retaining the high-frequency words in the document representation has an adverse effect on
the performance of these clustering algorithms.
Next, we experiment with another representation, specifically one in which each document is represented using only the sentiment-bearing words it contains. To understand the
motivation behind this bag-of-sentiment-words (BOSW) representation, recall from the introduction that one way to encourage the clustering algorithm to produce the user-desired
clustering is to design the feature space so that it contains all and only those features that
are useful for producing the user-desired clustering. Since we desire a sentiment-based clustering, we can design a feature space composed of solely sentiment-bearing words. Since a
hand-crafted subjectivity lexicon (i.e., a lexicon where each word is manually labeled with
its prior polarity17 ) for English is readily available, we can automatically construct a feature
space that consists of only those words that have a (positive or negative) polarity according
to the subjectivity lexicon, and represent a document using the resulting feature space. The
17. The prior polarity of a word is its polarity computed without regard to the context in which the word
appears.

605

fiDasgupta & Ng

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

MOV
69.1
60.7
54.6
68.8
69.1

KIT
62.3
57.9
50.3
59.0
62.3

Accuracy
BOO DVD
60.2 61.4
57.6
63.1
54.4
56.0
59.2 63.3
60.2 61.4

ELE
63.9
62.7
50.6
60.5
63.9

(a)

System Variation
2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

Adjusted Rand Index (ARI)
MOV KIT BOO DVD ELE
0.15 0.06 0.04 0.05 0.08
0.04
0.03 0.03 0.07 0.06
0.01
0.01 0.01
0.01 0.01
0.14
0.03 0.03 0.07 0.04
0.15 0.06 0.04 0.05 0.08
(b)

Table 15: Results in terms of (a) accuracy and (b) Adjusted Rand Index for the five
sentiment datasets obtained using the bag-of-sentiment-words document representation.
The strongest result(s) for each dataset are boldfaced.

goal, then, is to determine whether the BOSW document representation can improve the
sentiment-based clustering results obtained using the BOW representation.
To identify sentiment-bearing words in our experiment, we employ the subjectivity lexicon introduced in the work of Wilson, Wiebe, and Hoffmann (2005).18 The lexicon contains
8221 words, each of which is hand-labeled with a prior polarity of Positive, Negative,
or Neutral. We create a new subjectivity lexicon L in which we retain only those words
in Wilson et al.s lexicon that have either a Positive or Negative polarity. The BOSW
representation of a document is then composed of all and only those words that appear in
both L and the document.
The accuracy and ARI results of our baselines and our system obtained when employing
the BOSW representation are shown in Tables 15a and 15b, respectively. Consider first the
second eigenvector only baseline, NMF, and the Interested Reader Model. In comparison
to their corresponding results in Tables 2a and 2b, where the BOW representation was
used, we can see that performance improves on the BOO, DVD, and ELE datasets in most
cases, but drops on the MOV and KIT datasets. For the top five eigenvectors baseline,
performance increases on DVD and slightly on MOV, but drops on the remaining datasets.
Finally, using the BOSW representation causes the performance of our system to drop on
all datasets.
Overall, these results seem to suggest that whether the BOSW representation of a document yields better clustering results than its BOW representation is rather dependent on
the underlying domain and clustering algorithm. Nevertheless, we can see that the best
18. See http://www.cs.pitt.edu/mpqa/.

606

fiInducing Your Ideal Clustering with Minimal Feedback

clustering accuracy/ARI achieved for each sentiment dataset using the BOSW representation is significantly lower than that obtained using the BOW representation. We speculate
two reasons for the poorer results. First, the general-purpose subjectivity lexicon does not
cover all of the sentiment-bearing words. In particular, words that are sentiment-oriented
in the context of a particular domain but have a neutral polarity otherwise may be omitted from the BOSW document representation. Second, some non-sentiment-bearing words
might be useful for identifying sentiment.
4.5 Domain Adaptation
As mentioned in the introduction, the majority of existing approaches to sentiment classification is supervised. One weakness of these supervised approaches is that when given a
new domain, one needs to go through the expensive process of collecting a large amount of
annotated data in order to train an accurate polarity classifier.19 One may argue that our
active clustering algorithm suffers from the same weakness: the user needs to identify the
sentiment dimension for each domain. One way to address this weakness is through domain
adaptation. Specifically, we investigate whether the sentiment dimension manually identified for one domain (henceforth the source domain) can be used to automatically identify
the sentiment dimension for a new domain (henceforth the target domain). We hypothesize
that domain adaptation is feasible, especially if the two domains are sentimentally similar (i.e., there is a significant overlap between the features that characterize the sentiment
dimensions of the two domains).
As a result, we propose the following method for automatically identifying the sentiment
dimension for the target domain, y, using the sentiment dimension manually identified for
the source domain, x. Assume that the sentiment dimension of domain x is defined by
x
x
eigenvector ex . Moreover, assume that C1e and C2e are the two vectors of the top-ranked
features (obtained using MMFR) that characterize the two clusters induced by ex (with 100
features in each cluster). Now, given the target domain y, we first compute the similarity
between ex and each of ys top eigenvectors, ey2 , . . ., ey5 , where the similarity between two
eigenvectors ex and ey is defined as
x

y

x

y

x

y

x

y

max((C1e , C1e ) + (C2e , C2e ), (C1e , C2e ) + (C2e , C1e ))
Here,  is a similarity function that computes the similarity between two feature vectors.
In our experiments, we simply set it to be the dot product, which allows us to capture the
degree of overlap between the two feature vectors. Then, we posit the eigenvector from
{ey2 , . . . , ey5 } that has the highest overlap as the one that defines the sentiment dimension.20
To determine the effectiveness of our method, we compare the automatically selected
eigenvector with the human-selected eigenvector for each domain. Results are shown in
Table 16, where a Y in row i and column j indicates that the sentiment dimension for
target domain j has been successfully identified by using the sentiment dimension manually
19. While collecting annotated data is trivial when dealing with review data, the same is not necessarily
true for other kinds of data. For instance, people express their opinions and sentiment in political blogs
and floor debates, but the associated postings and transcripts may not be explicitly annotated with
sentiment labels.
20. Note that the two arguments to the max function correspond to the two different ways of creating the
mapping between the feature vectors in the two domains.

607

fiDasgupta & Ng

Domain
MOV
DVD
BOO
ELE
KIT

MOV

Y
N
N
N

DVD
Y

Y
N
Y

BOO
N
Y

N
N

ELE
N
N
N

Y

KIT
N
Y
Y
Y


Table 16: Domain adaptation results.

identified for source domain i, and an N indicates a failure. For instance, if we know
the sentiment dimension of the DVD domain (through human feedback), then our domain
adaptation method can be used to correctly identify the sentiment domain of MOV and
vice versa. However, domain adaptation using our method is not always successful. For
instance, knowing the sentiment dimension of MOV does not allow us to correctly predict
the sentiment dimension of ELE. Interestingly, if we ignore the BOO/KIT pair, then domain
adaptation exhibits symmetry. By symmetry, we mean that if domain x can be used to
identify the correct sentiment dimension for domain y, then domain y can be used to
identify the correct sentiment dimension for domain x. This intuitively makes sense: if
x can successfully be used to identify the sentiment dimension for y, it is likely that the
two domains share a lot of sentiment words. Consequently, using y to adapt to x is also
likely to be successful. The BOO/KIT pair represents a case in which domain adaptation is
successful in only one direction: while domain adaptation is successful from BOO to KIT,
the similarity between the sentiment dimensions of the two domains is not high (see the
discussion in the next paragraph for details), which contributes to the failure to adaptation
in the other direction.
As mentioned at the beginning of this subsection, we hypothesize that domain adaptation is likely to be successful if the two domains under consideration are similar to each
other. To test this hypothesis, we show in Table 17a the similarity between the manually
identified eigenvector and the corresponding automatically identified eigenvector for each
pair of domains. Three points deserve mention. First, as long as the similarity value is at
least 14, domain adaptation is successful; also, as long as the similarity value is at most 6,
domain adaptation is unsuccessful. Hence, these results substantiate our hypothesis that
domain adaptation is more likely to be successful if the two domains under consideration
are more similar to each other. It would be interesting to see if these two thresholds can
be used to predict whether domain adaptation is successful given a new pair of domains.
Second, domain adaptation in both directions are likely to be successful if the similarity
value is sufficiently high. As mentioned before, if the similarity value is high, then the
two domains share many sentiment words in common, which may in turn contribute to
successful domain adaptation in both directions. For the five domains we are considering,
as long as the similarity value is at least 14, then domain adaptation in both directions will
be successful. Third, it is worth reiterating that even if the similarity value falls below this
threshold, it does not imply that domain adaptation will fail. As mentioned before, the
sentiment dimension for domain y will be (correctly) identified as long as its similarity with
608

fiInducing Your Ideal Clustering with Minimal Feedback

Domain
MOV
DVD
BOO
ELE
KIT

MOV

14
(6)
(3)
(1)

DVD
14

21
(8)
10

BOO
(6)
21

(6)
(11)

ELE
(2)
(10)
(10)

32

KIT
(3)
10
8
32


BOO
(4)
13

(5)
(8)

ELE
(2)
(9)
(6)

27

KIT
(3)
7
6
23


BOO
(2)
8

(1)
(3)

ELE
(0)
(1)
(4)

5

KIT
(0)
3
2
9


(a)

Domain
MOV
DVD
BOO
ELE
KIT

MOV

5
(4)
(2)
(1)

DVD
10

14
(8)
7
(b)

Domain
MOV
DVD
BOO
ELE
KIT

MOV

9
(2)
(0)
(1)

DVD
4

7
(0)
3
(c)

Table 17: Similarity results for domain adaptation. (a) shows the similarity between the
sentiment eigenvector in the source domain and the eigenvector most similar to it in the target
domain. (b) shows the similarity between the sentiment eigenvector in the source domain and the
second most similar eigenvector in the target domain. (c) shows the similarity gap, which is the
difference between the corresponding entries in (a) and (b).

the sentiment dimension for domain x is highest among the four eigenvectors for y, as is
the case with the BOO/KIT domain pair.
So far we have attempted to correlate the success of domain adaptation with the similarity between the manually selected eigenvector in the source domain and the eigenvector
most similar to it in the target domain. It may be worth to also consider the similarity
between the manually selected eigenvector and the second most similar eigenvector in the
target domain, as the gap in similarity may give an indication as to the success of domain adaptation. To determine whether there is a better correlation between the success
of domain adaptation and this similarity gap, we compute (1) the similarity between the
eigenvector manually selected for the source domain and its second most similar eigenvector
in the target domain (see Table 17b) as well as (2) the similarity gap (see Table 17c), which
is simply the difference between the corresponding entries in Tables 17a and 17b. As we
can see from Table 17c, there also appears to be some correlation between the success of
domain adaptation and the gap values. In particular, if the gap value is at least 5, domain
609

fiDasgupta & Ng

adaptation is successful; however, if the gap value is at most 1, domain adaptation is unsuccessful. Nevertheless, these gap values do not help to predict the domain pairs where the
success of domain adaptation cannot be predicted using the similarity values in Table 17a
(e.g., the domain pairs that have low similarity and yet are domain-adaptable). Moreover,
they fail to predict the success of domain adaptation for many domain pairs, specifically
those where the gap value is between 1 and 5.
4.6 Subjectivity Lexicon versus Human Feedback
One might argue that if we had access to a subjectivity lexicon, we could use it to automatically identify the right sentiment dimension, thus obviating the need for human feedback
altogether. In this subsection, we investigate whether it is indeed feasible to use a handbuilt general-purpose sentiment lexicon to identify the eigenvector that corresponds to the
sentiment dimension in a new domain.
For our experiment, we use the subjectivity lexicon L described in Section 4.4. As
mentioned before, L contains all and only those words in Wilson et al.s (2005) subjectivity
lexicon that are marked with a prior polarity of Positive or Negative. The procedure for
automatically identifying the sentiment dimension using L is similar to the one described
in the domain adaptation section: for each of the second through fifth eigenvectors, we first
compute the similarity between the eigenvector and L, and then choose the eigenvector
that has the highest similarity with L. As in domain adaptation, we compute the similarity
between L and an eigenvector ex as
x

x

x

x

max((C1L , C1e ) + (C2L , C2e ), (C1L , C2e ) + (C2L , C1e ))
where C1L and C2L represent the words in L that are labeled as positive and negative rex
x
spectively, and C1e and C2e are the top-ranked features (obtained using MMFR) that
characterize the two clusters induced by ex (with 100 features in each cluster).  is a similarity function that computes the similarity between two feature vectors. As in domain
adaptation, we simply set it to be the dot product.
Our results indicate that we successfully identified the right eigenvector using L for
each of the five domains. Note that while L is a general-purpose (i.e., domain-independent)
lexicon containing only generic sentiment-bearing words, it is good enough to identify the
correct sentiment dimension for five different domains. It is worth noting that the sentiment
dimension of the MOV domain has the highest similarity with L (i.e., 34) out of the five
domains, suggesting that the highest-ranked sentiment features of the MOV domain (according to MMFR) are largely generic. DVD has the second largest similarity with L (33),
followed by BOO (26), KIT (16) and ELE (16). The comparatively low similarity values
for KIT and ELE are indicative of the fact that their highest-ranked sentiment features are
largely domain-specific.
Finally, although a subjectivity lexicon obviates the need for human feedback, we should
emphasize that this does not undermine the contribution of our feedback-oriented clustering
technique, for the following reasons. First, thinking from a text mining perspective, it would
be good to have an approach that is as knowledge-free as possible. Employing a handcrafted subjectivity lexicon makes our system resource-dependent; in fact, a subjectivity
lexicon may not be readily available for the vast majority of natural languages. Second, we
610

fiInducing Your Ideal Clustering with Minimal Feedback

want our method to be potentially applicable to non-sentiment domains (e.g., spam vs. not
spam), where we are again faced with the same problem that a hand-built lexicon may not
be available.
4.7 Single Data, Multiple Clusterings
As mentioned previously, a set of documents can be clustered along different dimensions.
For example, movie reviews can be clustered by sentiment (positive vs. negative) or genre
(e.g., action, romantic or documentary). A natural question is: can we produce different
clusterings of a given set of documents, each of which corresponds to a different dimension?
For the vast majority of existing text clustering algorithms, the answer is no: they can
only cluster along exactly one dimension, which is typically the most prominent dimension.
On the other hand, since our algorithm induces the important clustering dimensions of
a dataset, each of which can in principle be used to produce a (distinct) clustering, we
hypothesize that it can generate multiple clusterings of a given dataset along its important
dimensions.
To test our claim that our algorithm can produce multiple clusterings, we evaluate it
on four datasets that possess multiple clustering dimensions, namely MOV-DVD, BOODVD, DVD-ELE, and MOV-KIT.21 For example, the BOO-DVD dataset consists of all
the reviews taken from the BOO and DVD domains. Hence, each augmented dataset
is composed of 4000 reviews (2000 from each of the two contributing domains), which can
be clustered according to either topic (e.g., Book vs. DVD) or sentiment.22 Note that
the four pairs of domains used to create the augmented datasets were chosen carefully.
Specifically, two augmented datasets (MOV-DVD and BOO-DVD) were created such that
their constituent domains are mutually domain-adaptable according to Table 16, and the
remaining two (DVD-ELE and MOV-KIT) were created such that their constituent domains
are not domain-adaptable. Our goal is to see whether our active clustering algorithm is able
to produce both topic- and sentiment-based clusterings for datasets with different levels of
sentimental similarity.
The clustering procedure is almost identical to the one described in Section 3. In essence,
we (1) compute the top five eigenvectors of the Laplacian matrix; (2) learn the top-ranked
features corresponding to e2 through e5 according to MMFR; (3) ask the human judges
to identify the eigenvectors corresponding to both the topic dimension and the sentiment
dimension; and (4) use 2-means to produce two clusterings of the reviews, one according
to the selected topic dimension and the other the selected sentiment dimension. As in
Section 4.3, we conducted human and automatic experiments to determine the viability of
our algorithm.
21. The reason for our employing these augmented datasets is that they not only obviate the need for
additional human annotations, but also guarantee that there are at least two dimensions along which
clusters can be formed, thus allowing us to directly test its ability to produce multiple clusterings. While
it is also possible to evaluate our algorithms ability to generate multiple clusterings using the MOV
dataset (by clustering along genre and sentiment), we decided to leave this for future investigation, since
the documents in MOV are not annotated with genre information.
22. This is not to be confused with topic-sentiment mixture models (Mei, Ling, Wondra, Su, & Zhai, 2007),
where the goal is to first use topic models to mine the major aspects of a product from an online review
and then assign ratings to each extracted aspect. On the other hand, our goal is to design a clustering
algorithm that is capable of generating multiple clusterings of a dataset.

611

fiDasgupta & Ng

Judge
1
2
3
4
5
Agreement

MOV-DVD
2
2
2
2
2
100%

BOO-DVD
2
2
2
2
2
100%

DVD-ELE
2
2
2
2
2
100%

MOV-KIT
2
2
2
2
2
100%

DVD-ELE
3
3,5
5,3
3
3
80%

MOV-KIT
3
3,5
3
5
3
80%

(a)

Judge
1
2
3
4
5
Agreement

MOV-DVD
3
3,4
3,4
3
3
100%

BOO-DVD
4,5
4,5
4,5
4,5
4,5
100%
(b)

Table 18: Human agreement rate for selecting (a) the topic dimension and (b) the sentiment
dimension for the augmented datasets. Also shown are the eigenvectors selected by the
human judges.

4.7.1 Human Experiments
We employed the same five human judges involved in the human experiments in Section 4.3
to independently determine the topic dimension and the sentiment dimension for each of
the four augmented datasets using only the top features according to MMFR. As before, if
a human judge identifies more than one relevant eigenvector for a particular dimension, we
ask her to rank the eigenvectors according to relevance. Finally, we take the topic/sentiment
dimension that is the ranked first by the largest number of judges as the human-selected
topic/sentiment dimension.
Tables 18a and 18b show respectively the topic and sentiment dimensions (expressed in
terms of the IDs of the eigenvectors) selected by each of the five judges for each augmented
dataset. Also shown in the tables is the the human agreement rate, which was computed
based on only the highest-ranked dimension selected by each judge. Several points from
these human experiments deserve mention.
First, for each dataset, all human judges managed to find one eigenvector (out of the
top five) that corresponds to topic and at least one other eigenvector that corresponds
to sentiment. Perhaps more importantly, a human agreement rate of at least 80% was
achieved on all four datasets with respect to selecting the eigenvector(s) that correspond
to the topic and sentiment dimensions. These results together provide suggestive evidence
that (1) the eigen-decomposition procedure in our active clustering algorithm is effective
enough to unearth both the topic and sentiment dimensions when both of them are present
612

fiInducing Your Ideal Clustering with Minimal Feedback

in a dataset, and (2) our proposal for incorporating user feedback via inspecting a small
number of features is viable.
Second, while both topic and sentiment are prominent dimensions in these datasets, the
fact that the second eigenvector captures the topic dimension for all four datasets suggests
that topic is a more prominent dimension than sentiment. In fact, all of our human judges
reported that the topic dimension can be identified quite easily, achieving perfect agreement
on identifying the topic dimension. This provides empirical evidence for our speculation
that topic is typically (though not always) a more prominent dimension than sentiment
when both dimensions exist in a dataset.
Third, while reasonably high human agreement rate for identifying the sentiment dimension was achieved (perfect agreement on two datasets and 80% agreement rate on the
remaining two; see Table 18b for details), the human judges have reported that it was difficult to identify the sentiment dimension(s), especially for the two datasets composed of
sentimentally dissimilar domains.
In an attempt to gain insight into why the judges found it difficult to identify the
sentiment dimension(s), we show in Tables 1922 the top-ranked features induced for each
dimension using MMFR for the four augmented datasets, where the lightly shaded columns
correspond to the eigenvectors chosen for the topic dimension and the darkly shaded columns
correspond to the eigenvectors chosen for the sentiment dimension. After examining these
results, we believe that a few points deserve mention.
First, the top features generated from the sentiment eigenvector(s) for MOV-DVD and
BOO-DVD, the two datasets composed of sentimentally similar constituent domains, are
clearly sentiment-oriented, making it relatively easy for the human judges to determine the
sentiment eigenvector(s). Not so is the case for DVD-ELE and MOV-KIT, the two datasets
composed of dissimilar domains, where the top features are noisier (i.e., many of them
are not necessarily sentiment-oriented), thus making it tougher for the judges to locate
the sentiment eigenvector(s). In fact, one can see from just the top features generated by
the sentiment eigenvector(s) in Tables 1922 that those for MOV-DVD and BOO-DVD are
clearly more sentiment-oriented than those for DVD-ELE and MOV-KIT.
It should not be surprising that the more sentimentally dissimilar the constituent domains are, the noisier the top features generated from the sentiment eigenvector(s) are,
however. If the constituent domains are sentimentally similar, they tend to have many
sentiment-bearing words in common. This implies that these sentiment-bearing words will
appear more frequently in the augmented datasets than in each of the constituent datasets.
Hence, combining the two domains helps to boost the influence of these sentiment-bearing
words, increasing the chance of their appearing higher up in the list of features ranked
by MMFR. This reinforcement effect intuitively explains why the sentiment eigenvector is
clearly dominated by sentiment words for datasets composed of sentimentally similar domains. On the other hand, if the constituent domains are sentimentally dissimilar, they tend
not to have many sentiment-bearing words in common. As a result, the influence of the
sentiment-bearing words that are present in only one of the two constituent domains will be
diluted by the larger number of non-sentiment-bearing words that result from combining
the two domains. In other words, the features that are clearly sentiment-oriented in just
one rather than both domains may no longer appear sufficiently high in the ranked list of
features. In fact, as we saw in Tables 21 and 22, the sentiment eigenvector is contaminated
613

fiDasgupta & Ng

e2
C1
roles
drama
murder
meets
crime
supporting
involving
convincing
tale
lead
C2
bought
season
buy
disappointed
fan
amazon
buying
copy
dvds
watched

MOV-DVD
e3
e4
C1
C1
wonderful recommend
excellent
fan
beautiful
liked
personal
book
collection
read
view
excellent
art
amazing
highly
definitely
fantastic
highly
deal
absolutely
C2
stupid
boring
dull
mean
terrible
save
lame
run
guys
except

C2
buy
house
rent
waste
wait
kill
murder
obvious
season
dvds

e5
C1
kids
children
loved
child
son
daughter
boy
school
wonderful
heart
C2
quality
dark
war
horror
release
fan
earth
production
suspense
sound

Table 19: Top ten features induced for each dimension for the MOV-DVD domain. The
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

614

fiInducing Your Ideal Clustering with Minimal Feedback

e2
C1
reader
important
subject
understanding
modern
information
examples
political
business
nature
C2
saw
watched
actors
liked
music
season
humor
comedy
favorite
ending

BOO-DVD
e3
e4
C1
C1
bought
excellent
disappointed wonderful
easy
highly
information collection
price
music
waste
special
workout
classic
helpful
video
expected
perfect
reviews
amazing

e5
C1
loved
enjoyed
children
year
wonderful
child
fun
son
friends
highly

C2
young
men
cast
role
actors
script
scene
war
performance
action

C2
version
quality
waste
worst
review
original
edition
collection
amazon
format

C2
boring
ending
waste
reviews
couldn
novel
maybe
pages
stupid
finish

Table 20: Top ten features induced for each dimension for the BOO-DVD domain. The
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

615

fiDasgupta & Ng

e2
C1
funny
acting
family
actors
action
plot
enjoy
young
wonderful
comedy
C2
unit
battery
purchased
device
problems
tried
working
plug
charge
computer

DVD-ELE
e3
e4
C1
C1
easy
fine
small
problems
perfect
worked
excellent
months
highly
easy
nice
working
low
computer
comfortable
day
ipod
card
headphones
drive
C2
amazon
item
review
company
return
took
check
saw
card
worked

C2
amazon
tv
purchase
disappointed
item
purchased
reviews
wanted
received
ipod

e5
C1
video
card
camera
fast
easy
cable
picture
pictures
paper
digital
C2
phone
waste
unit
battery
getting
low
power
hear
worst
batteries

Table 21: Top ten features induced for each dimension for the DVD-ELE domain. The
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

616

fiInducing Your Ideal Clustering with Minimal Feedback

e2
C1
james
directed
sex
hour
drama
relationship
death
direction
tv
michael
C2
food
recommend
pot
purchased
mine
kitchen
mixer
handle
size
store

MOV-KIT
e3
e4
C1
C1
pan
coffee
cooking
clean
clean
machine
pans
ice
cook
maker
heat
plastic
oven
cup
heavy
fill
food
months
stick
working
C2
months
purchased
worked
broke
amazon
coffee
replacement
month
tried
service

C2
item
price
sheets
ordered
amazon
received
beautiful
dishes
arrived
sets

e5
C1
price
clean
kitchen
knife
knives
size
sharp
dishwasher
cutting
attractive
C2
pan
toaster
oven
pans
heat
return
bottom
worked
read
toast

Table 22: Top ten features induced for each dimension for the MOV-KIT domain. The
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges. e2 , . . ., e5 are the top eigenvectors; C1 and C2 are the clusters.

617

fiDasgupta & Ng

2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

MOV-DVD
Acc ARI
77.1 0.29
62.4 0.08
84.2 0.53
56.3 0.02
77.1 0.29

BOO-DVD
Acc ARI
77.8 0.31
77.2 0.31
63.1 0.07
69.2 0.15
77.8 0.31

DVD-ELE
Acc ARI
94.2 0.78
93.9 0.78
94.8 0.80
94.4 0.79
94.2 0.78

MOV-KIT
Acc ARI
99.3 0.97
99.3 0.97
99.6 0.99
70.6 0.17
99.3 0.97

DVD-ELE
Acc ARI
50.9 0.00
50.4 0.00
50.9 0.00
51.1 0.00
61.1 0.05

MOV-KIT
Acc ARI
50.0 0.00
50.0 0.00
50.1 0.00
61.6 0.05
59.2 0.03

(a)

2nd eigenvector only
Top five eigenvectors
Interested Reader Model
NMF
Our system

MOV-DVD
Acc ARI
54.4 0.01
68.3 0.13
53.4 0.01
66.9 0.11
71.4 0.18

BOO-DVD
Acc ARI
52.3 0.01
52.0 0.00
52.1 0.01
51.7 0.00
68.8 0.14
(b)

Table 23: Results on (a) topic-based clustering and (b) sentiment-based clustering for the
four augmented datasets. The strongest results for each dataset are boldfaced.

by a number of features that are not necessarily sentiment-bearing, which make it difficult
for the human judges to identify the sentiment dimension.
Another interesting point to note is that for some datasets, there seems to be more than
one eigenvector that correspond to sentiment. For instance, for the BOO-DVD dataset, all
five human judges agreed that both e4 and e5 correspond to the sentiment dimension. A
closer examination of these two eigenvectors (shown in Table 20) reveals a very interesting
pattern: in e4 , the positive features (in C1 ) came from the DVD domain and the negative
features (in C2 ) came from the BOO domain; whereas in e5 , the positive features (in
C1 ) came from BOO domain and the negative features (in C2 ) came from DVD. In other
words, e4 partitions the reviews according to the positive of DVD and the negative of
BOO, whereas e5 does the reverse. This suggests that the eigen-decomposition procedure
is smart enough not to merge the positive and negative sentiment-bearing words from
the two domains together. Perhaps even more importantly, both e4 and e5 are not only
partitioning the reviews along the sentiment dimension but also the topic dimension.
4.7.2 Clustering Results
Rows 14 of Tables 23a and 23b show the topic- and sentiment-based clustering results for
the same four baseline text clustering algorithms that were described in Section 4.2. Note
that each of these baselines can only produce one clustering of the documents per dataset.
Hence, for each baseline, the topic-based clustering results are produced by comparing
this clustering against the gold-standard topic-based clustering, and the sentiment-based
618

fiInducing Your Ideal Clustering with Minimal Feedback

clustering results are produced by comparing the same clustering against the gold-standard
sentiment-based clustering.
As we can see from these topic-based results in Table 23a, the baseline in which we cluster
using only the second eigenvector achieves the best average clustering results over the four
augmented datasets. This can potentially be attributed to the fact that e2 corresponds to
the topic dimension for all four datasets according to the human judges, as described in the
human experiments. However, clustering using only e2 does not produce the best clustering
results on all four datasets. In fact, the Interested Reader Model achieves the best results
on MOV-DVD, DVD-ELE, and MOV-KIT. Nevertheless, its results on BOO-DVD are the
worst among the baselines. The same is true for the top five eigenvectors baseline and
NMF: both of them have yielded poor results on MOV-DVD; in addition, NMFs results on
BOO-DVD and MOV-KIT are not promising either.
As far as the sentiment-based baseline clustering results are concerned (see rows 14
of Table 23b), the best average performance is achieved by NMF. Except for three cases
(NMF on MOV-DVD and MOV-KIT, as well as top five eigenvectors on MOV-DVD),
these baseline results are not particularly promising, with accuracy results in the low fifties
and ARI results close to zero.
The topic- and sentiment-based clustering results produced by our algorithm are shown
in row 5 of Tables 23a and 23b. Specifically, these results are obtained by grouping the
reviews according to the eigenvectors manually selected for the topic and sentiment dimensions, respectively. Hence, unlike the baselines, the topic-based clustering and the
sentiment-based clustering produced by our algorithm are different from each other. As
before, in cases where the human judges selected more than one eigenvector for each dimension, we use the eigenvector that is ranked first most frequently. As we can see, the
accuracies for topic-based clustering are reasonably high, ranging from 77.1% to 99.3%.
These results suggest that it is possible to achieve high-performance topic-based (or more
precisely, domain-based) clustering for a dataset even when another prominent clustering dimension (i.e., sentiment) is present. On the other hand, despite the existence of eigenvectors
that clearly capture the sentiment dimension for these datasets (e.g., e3 for the MOV-DVD
dataset), the sentiment-based clustering accuracies and ARI values are lower than those of
topic-based clustering. This can potentially be attributed to the reason mentioned in the
introduction: the fact that reviews are sentimentally ambiguous makes them non-trivial
to classify. In comparison to the four baselines, our algorithm achieves not only the best
average performance over the four datasets but also comparatively very stable performance
across these datasets.
It is worth noting that the sentiment-based clustering results produced by our algorithm
for MOV-DVD and BOO-DVD are higher than those for DVD-ELE and MOV-KIT. This is
perhaps not surprising: as discussed before, the human judges have found it more difficult
to identify the sentiment eigenvector for DVD-ELE and MOV-KIT than for MOV-DVD and
BOO-DVD, owing in part to the fact that many of the top-ranked features in the sentiment
eigenvector for DVD-ELE and MOV-KIT are not sentiment-oriented, which in turn can
be attributed to the fact that both of these datasets correspond to domain pairs that
are sentimentally dissimilar. As mentioned above, two sentimentally dissimilar constituent
domains tend not to have many sentiment-bearing words in common, and consequently, the
influence of the sentiment-bearing words that are present in only one of the two constituent
619

fiDasgupta & Ng

domains will be diluted by the larger number of non-sentiment-bearing words that result
from combining the two domains, making it difficult to produce a good sentiment-based
clustering. On the other hand, combining the two domains helps to boost the influence of
these sentiment-bearing words, increasing the chance of their appearing higher up in the
list of features ranked by MMFR and producing a good sentiment-based clustering.
Interestingly, our algorithm achieves better topic-based clustering results on the two
datasets  DVD-ELE and MOV-KIT  where it achieves poorer sentiment-based clustering results. In fact, the topic-based clustering accuracies on DVD-ELE and MOV-KIT are
near perfect: 94.2% and 99.3% for DVD-ELE and MOV-KIT respectively. This is by no
means a coincidence: when the constituent domains of an augmented dataset are highly
dissimilar (i.e., their word usage tends to differ considerably from each other), the topic clusters are well-separated from each other and hence high topic-based clustering results can be
achieved. A similar line of reasoning can explain why our algorithm finds it comparatively
more difficult to produce a good topic-based clustering for MOV-DVD and BOO-DVD,
where the constituent domains are similar.
These results seem to suggest that a higher topic-based accuracy/ARI implies a lower
sentiment-based accuracy/ARI and vice versa. We speculate that when the constituent
domains are similar, their sentiment-bearing features tend to be similar and as a result,
sentiment-based results tend to be good and topic-based results tend to be poor. Additional
experiments are needed to determine the reason.
Overall, these results provide supporting evidence that our feedback-oriented algorithm
can produce multiple clusterings of a dataset. In particular, even though the sentimentbased clustering accuracies are not as high as the topic-based clustering accuracies for the
augmented datasets, the current level of performance of our algorithm is arguably reasonable, especially considering the fact that sentiment-based clustering is a challenging task
and that traditional clustering algorithms fail to even produce more than one clustering.
4.7.3 Multiple Relevant Eigenvectors
Recall from Table 18b that for each of the four augmented datasets, there is at least one
judge who indicated that more than one eigenvector is relevant to the sentiment dimension.
However, when producing the sentiment-based clustering results using our system in Table 23b, we only used the eigenvector that was ranked most frequently by the human judges.
To better understand whether using more relevant eigenvectors can help improve the results for sentiment-based clustering, we repeat the experiment in which we apply 2-means
to cluster the documents in the space defined by all the eigenvectors that were determined
as relevant by at least one judge. More specifically, we cluster with the following set of
eigenvectors: {3,4} for MOV-DVD, {4,5} for BOO-DVD, {3,5} for DVD-ELE, and {3,5}
for MOV-KIT.
The accuracy and ARI results of this experiment are shown in Table 24. In comparison to
the results in the last row of Table 23b, we see that using additional relevant eigenvectors
yields better results for all but the BOO-DVD dataset. While it may not be easy to
determine the reason, we believe that the poorer results observed on BOO-DVD can be
attributed to the impurity of e5 , which captures not only sentiment but also topic, as
discussed before. On the other hand, the additional sentiment eigenvectors chosen for the
620

fiInducing Your Ideal Clustering with Minimal Feedback

Our system

MOV-DVD
Acc ARI
72.2 0.19

BOO-DVD
Acc ARI
55.7 0.01

DVD-ELE
Acc ARI
66.2 0.10

MOV-KIT
Acc ARI
59.8 0.04

Table 24: Results on sentiment-based clustering obtained using multiple relevant eigenvectors for the four augmented datasets.
other three augmented datasets do not seem to have this impurity problem, as they all
capture the sentiment dimension for only one of the constituent domains.

5. Significance of Our Work
We believe that our approach is significant in the following aspects.
1. Producing a clustering according to user interest. We proposed a novel framework in which we enabled a spectral clustering algorithm to take into account human
feedback and produce a clustering along the dimension of interest to the user. A
particularly appealing aspect of our approach is concerned with the relatively minimal human feedback it demands, where the user just needs to take a cursory look
at a small number of features that are representative of each induced dimension. It
is worth noting that having a human inspect and select an automatically induced
clustering dimension is a new form of interaction between a human and a clustering
algorithm. It enables a human to easily engage in various clustering tasks to help improve their performance in an easy, low-effort manner. We believe that our approach,
which belongs to an emerging family of interactive algorithms that allows the user to
make small, guiding tweaks and thereby get results much better than would otherwise
be possible, is the future of information retrieval.
2. Inducing human-interpretable clustering dimensions. The dimensions produced by spectral clustering or other dimensionality reduction algorithms (e.g., Latent
Semantic Indexing (LSI), Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990)
are generally considered non-interpretable (Sebastiani, 2002), unlike a dimension in
the original feature space, which typically corresponds to a word type and can therefore be interpreted by a human easily. The results of our preliminary study challenge
this common wisdom. We show in the context of text clustering that a dimension
in the low-dimensional space induced by spectral clustering can be interpreted by a
human. We believe the ability to produce human-interpretable dimensions enables us
to employ spectral clustering (and perhaps other dimensionality reduction-based clustering algorithms) for text processing in a more intelligent manner. This is especially
the case with respect to selecting the dimensions that are pertinent to the task at
hand. For example, in existing applications of spectral clustering to the topic-based
clustering task (e.g., Xu et al., 2003; He, Cai, Liu, & Ma, 2004; Hu, Deng, Guo, & Xu,
2007), all of the dimensions in the low-dimensional space are typically used. Since
we showed that not all dimensions produced by spectral clustering for a dataset are
necessarily topic-related, we can potentially improve topic-based clustering results by
621

fiDasgupta & Ng

not employing the non-topic-related dimensions in the clustering process. In addition,
since some of these induced dimensions correspond to non-topic dimensions, we can
use them to produce non-topic-based clusterings. In particular, given the recent surge
of interest in the NLP community in text classification along non-topic dimensions
such as sentiment and gender (e.g., Garera & Yarowsky, 2009; Jurafsky, Ranganath, &
McFarland, 2009), our approach offers a solution to these tasks that does not rely on
labeled data, unlike the majority of existing approaches to non-topic-based text classification, which are supervised in nature. Overall, we believe that NLP researchers
have not fully exploited the power of spectral clustering, and hence the rewards of
understanding spectral clustering in light of our results may be significant.
3. Producing multiple clusterings. While the majority of existing text clustering
algorithms can produce a single clustering of a dataset, our approach can potentially
be used to produce multiple clusterings, one along each of the important clustering
dimensions induced via a novel application of spectral clustering.
Finally, it is worth mentioning that the task of inducing clustering dimensions is reminiscent of the influential topic modeling task (Blei, Ng, & Jordon, 2003), whose goal is to
discover the major topics of a set of documents in an unsupervised manner. Note that the
two tasks are fundamentally different: while a topic model attempts to discover the major
topics in a set of documents, our dimension model aims to discover the major clustering
dimensions. Nevertheless, the two models bear resemblance to each other in many ways.
First, they both employ clustering to discover information from a text collection in an unsupervised manner. Second, they both display the learned information to a human using
representative words: a topic model represents each induced topic using words that are
representative of each topic, and our dimension model represents each induced clustering
dimension using words representative of the two document clusters involved in the dimension. Finally, not all induced topics and clustering dimensions are human-recognizable, but
for those that are, a human is needed to assign labels to them. We believe that the induction
of clustering dimensions has the potential to substantially enhance the capability of existing
text analysis algorithms to discover knowledge from a text collection in an unsupervised
manner by complementing the information induced by a topic model.

6. Related Work
In the introduction, we discussed related work on producing a user-desired clustering. In this
section, we focus on discussing related work on topic-based clustering and classification, sentiment classification, active learning, and producing multiple clusterings in computational
stylistics.
Topic-based text clustering. Traditional research on text clustering has focused primarily on topic-based clustering, owing in large part to DARPAs Topic Detection and
Tracking initiative in the 1990s. Many different clustering algorithms have been used, including non-hierarhical algorithms such as k-means and Expectation-Maximization (EM)
and hierarchical algorithms such as single-link, complete-link, group-average, and singlepass (Hatzivassiloglou, Gravano, & Maganti, 2000). These algorithms cluster a given set of
622

fiInducing Your Ideal Clustering with Minimal Feedback

documents in a feature space that is typically spanned by all of the unigrams. However,
clustering in such a high-dimensional space does not allow the distance between two documents to be reliably computed due to the curse of dimensionality. Consequently, more
recent work has focused on the development of algorithms that cluster documents in a lowdimensional space constructed via dimensionality reduction. Representative members of
this family of dimensionality reduction-based clustering algorithms include traditional algorithms that are based on LSI (Deerwester et al., 1990), as well as more recently proposed
(and arguably better performing) algorithms such as spectral clustering (Shi & Malik, 2000;
Ng et al., 2001), non-negative matrix factorization (Xu et al., 2003), locality preserving indexing (He et al., 2004), and locality discriminating indexing (Hu et al., 2007). Despite the
development of these new clustering algorithms, they have primarily been evaluated with
respect to their ability to produce topic-based clusterings.
Topic-based text classification. As Yang and Liu (1999) put it, text classification is
inherently a supervised learning task. In fact, it is arguably one of the most popular
tasks to which supervised learning techniques were applied in the information retrieval
community in the 1990s (see Sebastiani, 2002, for a comprehensive overview of related work
on machine learning for text classification). Nevertheless, the annotated documents that
are needed for training a high-performance supervised text classifier can be expensive to
obtain. As a result, some researchers have investigated the possibility of performing text
classification with little or even no labeled data. Such attempts have led to the development
of general-purpose semi-supervised text classification algorithms that combine labeled and
unlabeled data using transduction (Joachims, 1999b) or EM (Nigam, McCallum, Thrun, &
Mitchell, 2000), the latter of which has been used in combination with active learning (McCallum & Nigam, 1998). More recently, Sandler (2005) has proposed an unsupervised text
classification algorithm that is based on mixture modeling and LSI-based dimensionality
reduction.
Sentiment classification. As mentioned in the introduction, despite the large amount of
recent work on sentiment analysis and opinion mining, much of it has focused on supervised
methods (see Pang & Lee, 2008, for a comprehensive survey of the field). One weakness of these existing supervised polarity classification systems is that they are typically
domain- and language-specific. Hence, when given a new domain or language, one needs to
go through the expensive process of collecting a large amount of annotated data in order
to train a high-performance polarity classifier. Some recent attempts have been made to
leverage existing sentiment corpora or lexicons to automatically create annotated resources
for new domains or languages. However, such methods require the existence of either a
parallel corpus/machine translation engine for projecting/translating annotations/lexicons
from a resource-rich language to the target language (Banea, Mihalcea, Wiebe, & Hassan,
2008; Wan, 2008), or a domain that is similar enough to the target domain (Blitzer et al.,
2007). When the target domain or language fails to meet this requirement, sentiment-based
clustering and unsupervised polarity classification become appealing alternatives. Unfortunately, with a few exceptions (e.g., semi-supervised sentiment analysis, Riloff & Wiebe,
2003; Sindhwani & Melville, 2008; Dasgupta & Ng, 2009a; Li, Zhang, & Sindhwani, 2009),
these tasks are largely under-investigated in the NLP community. Turneys (2002) work is
perhaps one of the most notable examples of unsupervised polarity classification. However,
623

fiDasgupta & Ng

while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically.
Domain adaptation. Domain adaptation, also known as transfer learning, has been one
of the focal research areas in machine learning and NLP in recent years, where the goal
is to leverage the labeled data available for one domain (the source domain) to build a
classifier for another domain (the target domain). Techniques for domain adaptation has
been applied to various NLP tasks, including part-of-speech tagging, noun phrase chunking,
syntactic parsing, named entity recognition, and word sense disambiguation (e.g., Daume III
& Marcu, 2006; Chan & Ng, 2007; Duame III, 2007; Jiang & Zhai, 2007a, 2007b). Of
particular relevance to our work are domain adaptation techniques specifically developed
for text and sentiment classification (e.g., Blitzer, McDonald, & Pereira, 2006; Finn &
Kushmerick, 2006; Blitzer et al., 2007; Gao, Fan, Jiang, & Han, 2008; Ling, Dai, Xue, Yang,
& Yu, 2008; Tan, Cheng, Wang, & Xu, 2009). It is worth noting that our domain adaptation
setting is different from the traditional setting. Traditionally, sophisticated classifiers and/or
an automatically constructed mapping of features between the two domains are used in the
adaptation process. In our setting, however, we simply utilize the sentiment dimension
that is manually selected for the source domain to automatically identify the sentiment
dimension for the target domain.
Active clustering. Active learning is a heavily investigated machine learning paradigm
that aims to achieve better generalization bounds with lower annotation costs (Cohn, Atlas,
& Ladner, 1994). While in a traditional active learning setting, a human is requested to
annotate the data points that a classifier is most uncertain about (e.g., Cohn et al., 1994),
recent research in active learning has involved asking a human to identify or label the
features that are useful for the classification task at hand (e.g., Bekkerman et al., 2007;
Raghavan & Allan, 2007; Druck, Settles, & McCallum, 2009; Roth & Small, 2009). As
mentioned in the introduction, active learning has been applied in a clustering setting, with
the goal of encouraging an algorithm to produce the user-intended clustering when the
data can be clustered along multiple dimensions. Different variants of active clustering
have been proposed. Some request a human to label a pair of data points as must-link or
cannot-link to indicate whether the two points must or must not reside in the same cluster
(e.g., Wagstaff et al., 2001; Bilenko, Basu, & Mooney, 2004), while others have a human
determine whether two clusters should be merged or split during a hierarchical clustering
process (e.g., Balcan & Blum, 2008). Our active clustering algorithm is yet another variant:
we ask a human to select the clustering she desires from a set of automatically produced
clusterings.
Generation of multiple clusterings. The notion that text collections may be clustered
in multiple independent ways has been discussed in the literature on computational stylistics
(see Lim, Lee, & Kim, 2005; Biber & Kurjian, 2006; Grieve-Smith, 2006; Tambouratzis &
Vassiliou, 2007; Gries, Wulff, & Davies, 2010, for example). In machine learning, there
have been attempts to design algorithms for producing multiple clusterings of a dataset.
While some of them operate in a semi-supervised setting (e.g., Gondek & Hofmann, 2004;
Davidson & Qi, 2007), some are totally unsupervised (e.g., Caruana, Elhawary, Nguyen,
& Smith, 2006; Jain, Meka, & Dhillon, 2008). For instance, Caruana et al.s (2006) meta
clustering algorithm produces m different clusterings of a dataset by running k-means m
624

fiInducing Your Ideal Clustering with Minimal Feedback

times, each time with a random selection of seeds and a random weighting of features. Its
goal is to present each local minimum found by k-means as a possible clustering. However,
they do not propose any mechanism for determining which of these m clusterings is the one
the user desires. Our approach, which relies on spectral clustering rather than k-means for
producing multiple clusterings, fills this gap by soliciting user feedback to determine the
user-desired clustering.

7. Conclusions and Future Work
Unsupervised clustering algorithms typically group objects along the most prominent dimension, in part owing to their objective of simultaneously maximizing inter-cluster similarity and intra-cluster dissimilarity. Hence, if the users intended clustering dimension
is not the most prominent dimension, these unsupervised clustering algorithms will fail
miserably. To address this problem, we proposed an active clustering algorithm, which
allows us to mine the user-intended, possibly hidden, dimension of the data and produce
the desired clustering. This mechanism differs from competing methods in that it requires
very limited feedback: to select the intended dimension, the user only needs to inspect a
small number of features. We demonstrated its viability via a set of human and automatic
experiments with the challenging, yet under-investigated task of sentiment-based clustering, obtaining promising results. Additional experiments provided suggestive evidence that
(1) domain adaptation can be successfully applied to identify the sentiment dimension for a
new domain if the domains under consideration are sentimentally similar; (2) a hand-crafted
subjectivity lexicon, if available, can be used to replace the user feedback needed to select
the sentiment eigenvector of a domain; and (3) our algorithm can potentially be used to
produce multiple clusterings for datasets that possess multiple clustering dimensions.
Equally importantly, we empirically demonstrated that it is possible for a human to
interpret a dimension produced by a spectral clustering algorithm, contrary to the common
wisdom that the dimensions in an automatically constructed rank-reduced space are noninterpretable. We believe that NLP researchers have not fully exploited the power of spectral
clustering, and hence the rewards of understanding spectral clustering in light of our results
may be significant. Finally, our proposal to represent an induced clustering dimension as
sets of informative features facilitates exploratory text analysis, potentially enhancing the
capability of existing text analysis algorithms by complementing the information provided
by other unsupervised models (e.g., a topic model).
In future work, we plan to explore several extensions to our active clustering algorithm.
First, as our active clustering algorithm can potentially be used to produce multiple clusterings of a dataset, one interesting future direction would be to examine its theoretical
guarantees, determining whether it is able to produce distinct clusterings that are qualitatively strong (see Dasgupta & Ng, 2010a, 2010b, for example). Second, we plan to use our
algorithm in combination with existing feedback-oriented methods (e.g., Bekkerman et al.,
2007; Roth & Small, 2009) for improving its performance. For instance, instead of having
the user construct a relevant feature space from scratch, she can simply extend the set of
informative features identified for the user-selected dimension. Third, since none of the
steps in our algorithm is specifically designed for sentiment classification, we plan to apply
it to other non-topic-based text classification tasks that have recently received a lot of in625

fiDasgupta & Ng

terest in the NLP community, such as gender classification (i.e., the task of determining the
gender of the author of a document). Finally, we plan to adopt a richer representation of a
document that exploits features such as polarity-oriented words obtained from hand-built
or machine-learned sentiment lexicons (e.g., Hu & Liu, 2004; Wiebe, Wilson, Bruce, Bell,
& Martin, 2004; Andreevskaia & Bergler, 2006; Mohammad, Dunne, & Dorr, 2009; Rao
& Ravichandran, 2009), or those derived from finer-grained (i.e., sentential, sub-sentential,
phrase-based) sentiment analysis methods (e.g., Wilson et al., 2005; Kennedy & Inkpen,
2006; Polanyi & Zaenen, 2006; McDonald, Hannan, Neylon, Wells, & Reynar, 2007; Choi
& Cardie, 2008), as richer features may make it further easier for the user to identify the
desired dimension when using our method.

Bibliographic Note
Portions of this work were previously presented in a conference publication (Dasgupta &
Ng, 2009b). The current article extends this work in several ways, most notably: (1) a
detailed introduction to spectral clustering (Section 2.2); (2) the inclusion of two more
baseline systems (Section 4.2); (3) an investigation of the effect of document representation
on clustering performance (Section 4.4); (4) the addition of three new sections focusing on
issues in domain adaptation (Section 4.5), employing a manually constructed subjectivity
lexicon (Section 4.6), and producing multiple clusterings of a dataset (Section 4.7); as well
as (5) a description of the significance of our work (Section 5).

Acknowledgments
The authors acknowledge the support of National Science Foundation (NSF) grant IIS0812261. We thank the four anonymous reviewers for their helpful comments and for
unanimously recommending this article for publication in JAIR. Any opinions, findings,
conclusions or recommendations expressed in this article are those of the authors and do
not necessarily reflect the views or official policies, either expressed or implied, of NSF.

References
Abbasi, A., Chen, H., & Salem, A. (2008). Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Transactions on Information
Systems, 26 (3).
Andreevskaia, A., & Bergler, S. (2006). Mining WordNet for a fuzzy sentiment: Sentiment
tag extraction from WordNet glosses. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computational Linguistics (EACL), pp. 209
216.
Balcan, M.-F., & Blum, A. (2008). Clustering with interactive feedback. In Proceedings of
the 19th International Conference on Algorithmic Learning Theory (ALT), pp. 316
328.
Banea, C., Mihalcea, R., Wiebe, J., & Hassan, S. (2008). Multilingual subjectivity analysis using machine translation. In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 127135.
626

fiInducing Your Ideal Clustering with Minimal Feedback

Bekkerman, R., Raghavan, H., Allan, J., & Eguchi, K. (2007). Interactive clustering of
text collections according to a user-specified criterion. In Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 684689.
Biber, D., & Kurjian, J. (2006). Towards a taxonomy of web registers and text types: A
multidimensional analysis. Language and Computers, 59 (1), 109131.
Bilenko, M., Basu, S., & Mooney, R. J. (2004). Integrating constraints and machine learning
in semi-supervised clustering. In Proceedings of the 21st International Conference on
Machine Learning (ICML), pp. 8188.
Blei, D. M., Ng, A. Y., & Jordon, M. I. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3, 9931022.
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics (ACL), pp. 440447.
Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 120128.
Cai, D., He, X., & Han, J. (2005). Document clustering using locality preserving indexing.
IEEE Transactions on Knowledge and Data Engineering, 17 (12), 16241637.
Caruana, R., Elhawary, M. F., Nguyen, N., & Smith, C. (2006). Meta clustering. In
Proceedings of 6th IEEE International Conference on Data Mining (ICDM), pp. 107
118.
Chan, P. K., Schlag, D. F., & Zien, J. Y. (1994). Spectral k-way ratio-cut partitioning and
clustering. IEEE Transactions on Computer-Aided Design, 13, 10881096.
Chan, Y. S., & Ng, H. T. (2007). Domain adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 4956.
Choi, Y., & Cardie, C. (2008). Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 793801.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine Learning, 15 (2), 201221.
Dasgupta, S., & Ng, V. (2009a). Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL-IJCNLP), pp. 701709.
Dasgupta, S., & Ng, V. (2009b). Topic-wise, sentiment-wise, or otherwise? Identifying the
hidden dimension for unsupervised text classification. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
580589.
Dasgupta, S., & Ng, V. (2010a). Mining clustering dimensions. In Proceedings of the 27th
International Conference on Machine Learning (ICML), pp. 263270.
627

fiDasgupta & Ng

Dasgupta, S., & Ng, V. (2010b). Towards subjectifying text clustering. In Proceedings of
the 33rd Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR), pp. 483490.
Daume III, H., & Marcu, D. (2006). Domain adaptation for statistical classifiers. Journal
of Artificial Intelligence Research, 26, 101126.
Davidson, I., & Qi, Z. (2007). Finding alternative clusterings using constraints. In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM), pp. 773778.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing by latent semantic analysis. Journal of American Society of Information
Science, 41 (6), 391407.
Dhillon, I., Guan, Y., & Kulis, B. (2004). Kernel k-means, spectral clustering and normalized cuts. In Proceedings of the 10th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pp. 551556.
Ding, C., He, X., Zha, H., Gu, M., & Simon, H. D. (2001). A min-max cut algorithm
for graph partitioning and data clustering. In Proceedings of the 2001 International
Conference on Data Mining (ICDM), pp. 107114.
Druck, G., Settles, B., & McCallum, A. (2009). Active learning by labeling features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 8190.
Duame III, H. (2007). Frustratingly easy domain adaptation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics (ACL), pp. 256263.
Finn, A., & Kushmerick, N. (2006). Learning to classify documents according to genre.
Journal of the American Society for Information Science and Technology, 57 (11),
15061518.
Fung, G. (2003). The disputed Federalist Papers: SVM feature selection via concave minimization. In Proceedings of the 2003 Conference on Diversity in Computing, pp.
4246.
Gao, J., Fan, W., Jiang, J., & Han, J. (2008). Knowledge transfer via multiple model local
structure mapping. In Proceeding of the 14th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD), pp. 283291.
Garera, N., & Yarowsky, D. (2009). Modeling latent biographic attributes in conversational
genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP), pp. 710718.
Gilad-Bachrach, R., Navot, A., & Tishby, N. (2004). Margin based feature selection  theory
and algorithms. In Proceedings of the 21st International Conference on Machine
Learning (ICML), pp. 4350.
Gondek, D., & Hofmann, T. (2004). Non-redundant data clustering. In Proceedings of the
4th IEEE International Conference on Data Mining (ICDM), pp. 7582.
Gries, S., Wulff, S., & Davies, M. (2010). Corpus-linguistic Applications: Current Studies,
New Directions. Rodopi.
628

fiInducing Your Ideal Clustering with Minimal Feedback

Grieve-Smith, A. (2006). The envelope of variation in multidimensional register and genre
analyses. Language and Computers, 60 (1), 2142.
Hatzivassiloglou, V., Gravano, L., & Maganti, A. (2000). An investigation of linguistic
features and clustering algorithms for topical document clustering. In Proceedings of
the 23rd Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR), pp. 224231.
He, X., Cai, D., Liu, H., & Ma, W.-Y. (2004). Locality preserving indexing for document
representation. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp. 96103.
Hu, J., Deng, W., Guo, J., & Xu, W. (2007). Locality discriminating indexing for document
classification. In Proceedings of the 30th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR) (Poster), pp. 689
690.
Hu, M., & Liu, B. (2004). Mining opinion features in customer reviews. In Proceedings of
the 19th National Conference on Artificial Intelligence (AAAI), pp. 755760.
Jain, P., Meka, R., & Dhillon, I. S. (2008). Simultaneous unsupervised learning of disparate
clusterings. In Proceedings of SIAM International Conference on Data Mining (SDM),
pp. 858869.
Jiang, J., & Zhai, C. (2007a). Instance weighting for domain adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics
(ACL), pp. 254271.
Jiang, J., & Zhai, C. (2007b). A two-stage approach to domain adaptation for statistical
classifiers. In Proceedings of the 16th Conference on Information and Knowledge
Management (CIKM), pp. 401410.
Joachims, T. (1999a). Making large-scale SVM learning practical. In Scholkopf, B., &
Smola, A. (Eds.), Advances in Kernel Methods - Support Vector Learning, pp. 4456.
MIT Press.
Joachims, T. (1999b). Transductive inference for text classification using support vector
machines. In Proceedings of the 16th International Conference on Machine Learning
(ICML), pp. 200209.
Jurafsky, D., Ranganath, R., & McFarland, D. (2009). Extracting social meaning: Identifying interactional style in spoken conversation. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL HLT), pp. 638646.
Kamvar, S., Klein, D., & Manning, C. (2003). Spectral learning. In Proceedings of the 19th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 561566.
Kannan, R., Vempala, S., & Vetta, A. (2004). On clusterings: Good, bad and spectral.
Journal of the ACM, 51 (3), 497515.
Kennedy, A., & Inkpen, D. (2006). Sentiment classifiation of movie reviews using contextual
valence shifters. Computational Intelligence, 22 (2), 110125.
629

fiDasgupta & Ng

Koppel, M., Schler, J., & Argamon, S. (2009). Computational methods in authorship attribution. Journal of the American Society for Information Science and Technology,
60 (1), 926.
Kugler, M., Aoki, K., Kuroyanagi, S., Iwata, A., & Nugroho, A. (2005). Feature subset
selection for support vector machines using confident margin. In Proceedings of the
2005 IEEE International Joint Conference on Neural Networks (IJCNN), pp. 907912.
Kulis, B., Basu, S., Dhillon, I., & Mooney, R. (2009). Semi-supervised graph-based clustering: A kernel approach. Machine Learning, 74 (1), 122.
Li, T., Zhang, Y., & Sindhwani, V. (2009). A non-negative matrix tri-factorization approach
to sentiment classification with lexical prior knowledge. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pp. 244
252.
Lim, C., Lee, K., & Kim, G. (2005). Multiple sets of features for automatic genre classification of web documents. Information Processing and Management, 41 (5), 12631276.
Ling, X., Dai, W., Xue, G., Yang, Q., & Yu, Y. (2008). Spectral domain-transfer learning.
In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pp. 488496.
Liu, B., Li, X., Lee, W. S., & Yu, P. S. (2004). Text classification by labeling words. In
Proceedings of the 19th National Conference on Artificial Intelligence (AAAI), pp.
425430.
McCallum, A. K., & Nigam, K. (1998). Employing EM and pool-based active learning for
text classification. In Proceedings of the 15th International Conference on Machine
Learning (ICML), pp. 350358, Madison, WI. Morgan Kaufmann.
McDonald, R., Hannan, K., Neylon, T., Wells, M., & Reynar, J. (2007). Structured models
for fine-to-coarse sentiment analysis. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics (ACL), pp. 432439.
Mei, Q., Ling, X., Wondra, M., Su, H., & Zhai, C. (2007). Sentiment mixture: Modeling
facets and opinions in weblogs. In Proceedings of the 16th World Wide Web Conference
(WWW), pp. 171180.
Mohammad, S., Dunne, C., & Dorr, B. (2009). Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the
2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 599608.
Ng, A., Jordan, M., & Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm.
In Advances in Neural Information Processing Systems 14 (NIPS).
Nigam, K., McCallum, A., Thrun, S., & Mitchell, T. (2000). Text classification from labeled
and unlabeled documents using EM. Machine Learning, 39 (2/3), 103134.
Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends
in Information Retrieval, 2 (12), 1135.
630

fiInducing Your Ideal Clustering with Minimal Feedback

Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 7986. Association for Computational Linguistics.
Polanyi, L., & Zaenen, A. (2006). Contextual valence shifters. In Computing Attitude and
Affect in Text: Theory and Applications. Springer Verlag.
Raghavan, H., & Allan, J. (2007). An interactive algorithm for asking and incorporating
feature feedback into support vector machines. In Proceedings of the 30th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR), pp. 7986.
Rao, D., & Ravichandran, D. (2009). Semi-supervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pp. 675682.
Riloff, E., & Wiebe, J. (2003). Learning extraction patterns for subjective expressions.
In Proceedings of the 2003 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 105112.
Roth, D., & Small, K. (2009). Interactive feature space construction using semantic information. In Proceedings of the 13th Conference on Computational Natural Language
Learning (CoNLL), pp. 6674.
Sandler, M. (2005). On the use of linear programming for unsupervised text classification.
In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pp. 256264.
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing
Surveys, 34 (1), 147.
Shi, J., & Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 22 (8), 888905.
Sindhwani, V., & Melville, P. (2008). Document-word co-regularization for semi-supervised
sentiment analysis. In Proceedings of the 8th IEEE International Conference on Data
Mining (ICDM), pp. 10251030.
Stein, S., Argamon, S., & Frieder, O. (2006). The effect of OCR errors on stylistic text
classification. In Proceedings of the 29th Annual International ACM SIGIR conference
on Research and Development in Information Retrieval (SIGIR) (Poster), pp. 701
702.
Tambouratzis, G., & Vassiliou, M. (2007). Employing thematic variables for enhancing classification accuracy within author discrimination experiments. Literary and Linguistic
Computing, 22 (2), 207224.
Tan, S., Cheng, X., Wang, Y., & Xu, H. (2009). Adapting naive Bayes to domain adaptation
for sentiment analysis. In Proceedings of the 31st European Conference on Information
Retrieval (ECIR), pp. 337349.
Turney, P. (2002). Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 417424.
631

fiDasgupta & Ng

Wagstaff, K., Cardie, C., Rogers, S., & Schrodl, S. (2001). Constrained k-means clustering
with background knowledge. In Proceedings of the 18th International Conference on
Machine Learning (ICML), pp. 577584.
Wan, X. (2008). Using bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 553561.
Weiss, Y. (1999). Segmentation using eigenvectors: A unifying view. In Proceedings of the
International Conference on Computer Vision (ICCV), pp. 975982.
Wiebe, J. M., Wilson, T., Bruce, R., Bell, M., & Martin, M. (2004). Learning subjective
language. Computational Linguistics, 30 (3), 277308.
Wilson, T., Wiebe, J. M., & Hoffmann, P. (2005). Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of the Joint Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pp. 347354.
Wu, Z., & Leahy, R. M. (1993). An optimal graph theoretic appproach to data clustering
and its application to image segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 15 (11), 11011113.
Xing, E. P., Ng, A. Y., Jordan, M. I., & Russell, S. J. (2002). Distance metric learning with
application to clustering with side-information. In Advances in Neural Information
Processing Systems 15 (NIPS), pp. 505512.
Xu, W., Liu, X., & Gong, Y. (2003). Document clustering based on non-negative matrix
factorization. In Proceedings of the 26th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR), pp. 267273.
Yang, Y., & Liu, X. (1999). A re-examination of text categorization methods. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR), pp. 4249.
Yang, Y., & Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. In Proceedings of the 14th International Conference on Machine Learning
(ICML), pp. 412420.

632

fiJournal of Artificial Intelligence Research 39 (2010) 1-49

Submitted 05/10; published 09/10

Planning with Noisy Probabilistic Relational Rules
Tobias Lang
Marc Toussaint

tobias.lang@tu-berlin.de
mtoussai@cs.tu-berlin.de

Machine Learning and Robotics Group
Technische Universitat Berlin
Franklinstrae 28/29, 10587 Berlin, Germany

Abstract
Noisy probabilistic relational rules are a promising world model representation for several reasons. They are compact and generalize over world instantiations. They are usually
interpretable and they can be learned effectively from the action experiences in complex
worlds. We investigate reasoning with such rules in grounded relational domains. Our algorithms exploit the compactness of rules for efficient and flexible decision-theoretic planning.
As a first approach, we combine these rules with the Upper Confidence Bounds applied to
Trees (UCT) algorithm based on look-ahead trees. Our second approach converts these
rules into a structured dynamic Bayesian network representation and predicts the effects
of action sequences using approximate inference and beliefs over world states. We evaluate
the effectiveness of our approaches for planning in a simulated complex 3D robot manipulation scenario with an articulated manipulator and realistic physics and in domains of
the probabilistic planning competition. Empirical results show that our methods can solve
problems where existing methods fail.

1. Introduction
Building systems that act autonomously in complex environments is a central goal of Artificial Intelligence. Nowadays, A.I. systems are on par with particularly intelligent humans
in specialized tasks such as playing chess. They are hopelessly inferior to almost all humans, however, in deceivingly simple tasks of everyday-life, such as clearing a desktop,
preparing a cup of tea or manipulating chess figures: The current state of the art in reasoning, planning, learning, perception, locomotion, and manipulation is so far removed from
human-level abilities, that we cannot yet contemplate working in an actual domain of interest (Pasula, Zettlemoyer, & Kaelbling, 2007). Performing common object manipulations
is indeed a challenging task in the real world: we can choose from a very large number of
distinct actions with uncertain outcomes and the number of possible situations is basically
unseizable.
To act in the real world, we have to accomplish two tasks. First, we need to understand
how the world works: for example, a pile of plates is more stable if we place the big plates
at its bottom; it is a hard job to build a tower from balls; filling tea into a cup may lead to a
dirty table cloth. Autonomous agents need to learn such world knowledge from experience
to adapt to new environments and not to rely on human hand-crafting. In this paper, we
employ a recent solution for learning (Pasula et al., 2007). Once we know about the possible
effects of our actions, we face a second challenging problem: how can we use our acquired
knowledge in reasonable time to find a sequence of actions suitable to achieve our goals?
c
2010
AI Access Foundation. All rights reserved.

fiLang & Toussaint

This paper investigates novel algorithms to tackle this second task, namely planning. We
pursue a model-based approach for planning in complex domains. In contrast to modelfree approaches which compute policies directly from experience with respect to fixed goals
(also called habit-based decision making), we follow a purposive decision-making approach
(Botvinick & An, 2009) and use learned models to plan for the goal and current state at
hand. In particular, we simulate the probabilistic effects of action sequences. This approach
has interesting parallels in recent neurobiology and cognitive science results suggesting that
the behavior of intelligent mammals is driven by internal simulation or emulation: it has
been found that motor structures in the cortex are activated during planning, while the
execution of motor commands is suppressed (Hesslow, 2002; Grush, 2004).
Probabilistic relational world model representations have received significant attention
over the last years. They enable to generalize over object identities to unencountered situations and objects of similar types and to account for indeterministic action effects and noise.
We will review several such approaches together with other related work in Section 2. Noisy
indeterministic deictic (NID) rules (Pasula et al., 2007) capture the world dynamics in an
elegant compact way. They are particularly appealing as they can be learned effectively
from experience. The existing approach for planning with these rules relies on growing
full look-ahead trees in the grounded domain. Due to the very large action space and the
stochasticity of the world, the computational burden to plan just a single action with this
method in a given situation can be overwhelmingly large. This paper proposes two novel
ways for reasoning efficiently in the grounded domain using learned NID rules, enabling fast
planning in complex environments with varying goals. First, we apply the existing Upper
Confidence bounds applied to Trees (UCT) algorithm (Kocsis & Szepesvari, 2006) with NID
rules. In contrast to full-grown look-ahead trees, UCT samples actions selectively, thereby
cutting suboptimal parts of the tree early. Second, we introduce the Probabilistic Relational
Action-sampling in DBNs planning Algorithm (PRADA) which uses probabilistic inference
to cope with uncertain action outcomes. Instead of growing look-ahead trees with sampled successor states like the previous approaches, PRADA applies approximate inference
techniques to propagate the effects of actions. In particular, we make three contributions
with PRADA: (i) Following the idea of framing planning as a probabilistic inference problem (Shachter, 1988; Toussaint, Storkey, & Harmeling, 2010), we convert NID rules into
a dynamic Bayesian network (DBN) representation. (ii) We derive an approximate inference method to cope with the state complexity of a time-slice of the resulting network.
Thereby, we can efficiently predict the effects of action sequences. (iii) For planning based
on sampling action-sequences, we propose a sampling distribution for plans which takes predicted state distributions into account. We evaluate our planning approaches in a simulated
complex 3D robot manipulation environment with realistic physics, with an articulated humanoid manipulating objects of different types (see Fig. 4). This domain contains billions of
world states and a large number of potential actions. We learn NID rules from experience
in this environment and apply them with our planning approaches in different planning
scenarios of increasing difficulty. Furthermore, we provide results of our approaches on
the planning domains of the most recent international probabilistic planning competition.
For this purpose, we discuss the relation between NID rules and the probabilistic planning
domain definition language (PPDDL) used for the specification of these domains.
2

fiPlanning with Noisy Probabilistic Relational Rules

We begin this paper by discussing the related work in Section 2 and reviewing the
background of our work, namely stochastic relational representations, NID rules, the formalization of decision-theoretic planning and graphical models in Section 3. In Section 4,
we present two planning algorithms that build look-ahead trees to cope with stochastic
actions. In Section 5, we introduce PRADA which uses approximate inference for planning.
In Section 6, we present our empirical evaluation demonstrating the utility of our planning
approaches. Finally, we conclude and outline future directions of research.

2. Related Work
The problem of decision-making and planning in stochastic relational domains has been approached in different ways. The field of relational reinforcement learning (RRL) (Dzeroski,
de Raedt, & Driessens, 2001; van Otterlo, 2009) investigates value functions and Q-functions
that are defined over all possible ground states and actions of a relational domain. The key
idea is to describe important world features in terms of abstract logical formulas enabling
generalization over objects and situations. Model-free RRL approaches learn value functions
for states and actions directly from experience. Q-function estimators include relational
regression trees (Dzeroski et al., 2001) and instance-based regression using distance metrics between relational states such as graph kernels (Driessens, Ramon, & Gartner, 2006).
Model-free approaches enable planning for the specific problem type used in the training
examples, e.g. on(X, Y ), and thus may be inappropriate in situations where the goals of
the agent change quickly, e.g. from on(X, Y ) to inhand(X). In contrast, model-based RRL
approaches first learn a relational world model from the state transition experiences and
then use this model for planning, for example in the form of relational probability trees
for individual state attributes (Croonenborghs, Ramon, Blockeel, & Bruynooghe, 2007) or
SVMs using graph kernels (Halbritter & Geibel, 2007). The stochastic relational NID rules
of Pasula et al. (2007) are a particularly appealing action model representation, as it has
been shown empirically that they can learn the dynamics of complex environments.
Once a probabilistic relational world model is available (either learned or handcrafted),
one can pursue decision-theoretic planning in different ways. Within the machine learning
community, a popular direction of research formalizes the problem as a relational Markov
decision process (RMDP) and develops dynamic programming algorithms to compute solutions, i.e. policies over complete state and action spaces. Many algorithms reason in
the lifted abstract representation without grounding or referring to particular problem instances. Boutilier, Reiter, and Price (2001) introduce Symbolic Dynamic Programming,
the first exact solution technique for RMDPs which uses logical regression to construct
minimal logical partitions of the state space required to make all necessary value function
distinctions. This approach has not been implemented as it is difficult to keep the firstorder state formulas consistent and of manageable size. Based on these ideas, Kersting, van
Otterlo, and de Raedt (2004) propose an exact value iteration algorithm for RMDPs using
logic-programming, called ReBel. They employ a restricted language to represent RMDPs
so that they can reason efficiently over state formulas. Holldobler and Skvortsova (2004)
present a first-order value iteration algorithm (FOVIA) using a different restricted language.
Karabaev and Skvortsova (2005) extend FOVIA by combining first-order reasoning about
actions with a heuristic search restricted to those states that are reachable from the initial
3

fiLang & Toussaint

state. Wang, Joshi, and Khardon (2008) derive a value iteration algorithm based on using
first-order decision diagrams (FODDs) for goal regression. They introduce reduction operators for FODDs to keep the representation small, which may require complex reasoning;
an empirical evaluation has not been provided. Joshi, Kersting, and Khardon (2009) apply
model checking to reduce FODDs and generalize them to arbitrary quantification.
All these techniques form an interesting research direction as they reason exactly about
abstract RMDPs. They employ different methods to ensure exact regression such as theorem proving, logical simplification, or consistency checking. Therefore, principled approximations of these techniques that can discover good policies in more difficult domains are
likewise worth investigating. For instance, Gretton and Thiebaux (2004) employ first-order
regression to generate a suitable hypothesis language which they then use for policy induction; thereby, their approach avoids formula rewriting and theorem proving, while still
requiring model-checking. Sanner and Boutilier (2007, 2009) present a first-order approximate linear programming approach (FOALP). Prior to producing plans, they approximate
the value function based on linear combinations of abstract first-order value functions,
showing impressive results on solving RMDPs with millions of states. Fern, Yoon, and
Givan (2006) consider a variant of approximate policy iteration (API) where they replace
the value-function learning step with a learning step in policy space. They make use of a
policy-space bias as described by a generic relational knowledge representation and simulate trajectories to improve the learned policy. Kersting and Driessens (2008) describe a
non-parametric policy gradient approach which can deal with propositional, continuous and
relational domains in a unified way.
Instead of working in the lifted representation, one may reason in the grounded domain.
This makes it straightforward to account for two special characteristics of NID rules: the
noise outcome and the uniqueness requirement of rules. When grounding an RMDP which
specifies rewards only for a set of goal states, one might in principle apply any of the traditional A.I. planning methods used for propositional representations (Weld, 1999; Boutilier,
Dean, & Hanks, 1999). Traditionally, planning is often cast as a search problem through
a state and action space, restricting oneself to the portion of the state space that is considered to contain goal states and to be reachable from the current state within a limited
horizon. Much research within the planning community has focused on deterministic domains and thus cant be applied straightforwardly in stochastic worlds. A common approach
for probabilistic planning, however, is to determinize the planning problem and apply deterministic planners (Kuter, Nau, Reisner, & Goldman, 2008). Indeed, FF-Replan (Yoon,
Fern, & Givan, 2007) and its extension using hindsight optimization (Yoon, Fern, Givan, &
Kambhampati, 2008) have shown impressive performance on many probabilistic planning
competition domains. The common variant of FF-Replan considers each probabilistic outcome of an action as a separate deterministic action, ignoring the respective probabilities.
It then runs the deterministic Fast-Forward (FF) planner (Hoffmann & Nebel, 2001) on the
determinized problem. FF uses a relaxation of the planning problem: it ignores the delete
effects of actions and applies clever heuristics to prune the search space. FF-Replan outputs
a sequence of actions and expected states. Each time an action execution leads to a state
which is not in the plan, FF-Replan has to replan, i.e., recompute a new plan from scratch
in the current state. The good performance of FF-Replan in many probabilistic domains
has been explained by the structure of these problems (Little & Thiebaux, 2007). It has
4

fiPlanning with Noisy Probabilistic Relational Rules

been argued that FF-Replan should be less appropriate in domains in which the probability
of reaching a dead-end is non-negligible and where the outcome probabilities of actions need
to be taken into account to construct a good policy.
Many participants of the most recent probabilistic planning competition (IPPC, 2008)
extend FF-Replan to deal with the probabilities of action outcomes (see the competition
website for brief descriptions of the algorithms). The winner of the competition, RFF
(Teichteil-Konigsbuch, Kuter, & Infantes, 2010), computes a robust policy offline by generating successive execution paths leading to the goal using FF. The resulting policy has
low probability of failing. LPPFF uses subgoals generated from a determinization of the
probabilistic planning problem to divide it into smaller manageable problems. HMDPPs
strategy is similar to the all-outcomes-determinization of FF-Replan, but accounts for the
probability associated with each outcome. SEH (Wu, Kalyanam, & Givan, 2008) extends
a heuristic function of FF-Replan to cope with local optima in plans by using stochastic
enforced hill-climbing.
A common approach to reasoning in a more general reward-maximization context which
avoids explicitly dealing with uncertainty is to build look-ahead trees by sampling successor
states. Two algorithms which follow this idea, namely SST (Kearns, Mansour, & Ng, 2002)
and UCT (Kocsis & Szepesvari, 2006), are investigated in this paper.
Another approach by Buffet and Aberdeen (2009) directly optimizes a parameterized
policy using gradient descent. They factor the global policy into simple approximate policies
for starting each action and sample trajectories to cope with probabilistic effects.
Instead of sampling state transitions, we propose the planning algorithm PRADA in this
paper (based on Lang & Toussaint, 2009a) which accounts for uncertainty in a principled
way using approximate inference. Domshlak and Hoffmann (2007) propose an interesting
planning approach which comes closest to our work. They introduce a probabilistic extension of the FF planner, using complex algorithms for building probabilistic relaxed planning
graphs. They construct dynamic Bayesian networks (DBNs) from hand-crafted STRIPS operators and reason about actions and states using weighted model counting. Their DBN
representation, however, is inadequate for the type of stochastic relational rules that we use,
for the same reasons why the naive DBN model which we will discuss in Sec. 5.1 is inappropriate. Planning by inference approaches (Toussaint & Storkey, 2006) spread information
also backwards through DBNs and calculate posteriors over actions (resulting in policies
over complete state spaces). How to use backward propagation or even full planning by
inference in relational domains is an open issue.
All approaches working in the grounded representation have in common that the number
of states and actions will grow exponentially with the number of objects. To apply them in
domains with very many objects, these approaches need to be combined with complementary
methods that reduce the state and action space complexity in relational domains. For
instance, one can focus on envelopes of states which are high-utility subsets of the state
space (Gardiol & Kaelbling, 2003), one can ground the representation only with respect to
relevant objects (Lang & Toussaint, 2009b), or one can exploit the equivalence of actions
(Gardiol & Kaelbling, 2007), which is particularly useful in combination with ignoring
certain predicates and functions of the relational logic language (Gardiol & Kaelbling, 2008).
5

fiLang & Toussaint

3. Background
In this section, we set up the theoretical background for the planning algorithms we will
present in subsequent sections. First, we describe relational representations to define world
states and actions. Then we will present noisy indeterministic deictic (NID) rules in detail
and thereafter define the problem of decision-theoretic planning in stochastic relational
domains. Finally, we briefly review dynamic Bayesian networks.
3.1 State and Action Representation
A relational domain is represented by a relational logic language L: the set of logical
predicates P and the set of logical functions F contain the relationships and properties that
can hold for domain objects. The set of logical predicates A comprises the possible actions
in the domain. A concrete instantiation of a relational domain is made up of a finite set of
objects O. If the arguments of a predicate or function are all concrete, i.e. taken from O, we
call it grounded. A concrete world state s is fully described as a conjunction of all grounded
(potentially negated) predicates and function values. Concrete actions a are described by
positive grounded predicates from A. The arguments of predicates and functions can also
be abstract logical variables which can represent any object. If a predicate or function
has only abstract arguments, we call it abstract. Abstract predicates and functions enable
generalization over objects and situations. We will speak of grounding a formula  if we
apply a substitution  that maps all of the variables appearing in  to objects in O.
A relational model T of the transition dynamics specifies P (s0 |a, s), the probability
of a successor state s0 if action a is performed in state s. In this paper, this is usually
a non-deterministic distribution. T is typically defined compactly in terms of formulas
over abstract predicates and functions. This enables abstraction from object identities and
concrete domain instantiations. For instance, consider a set of N cups: the effects of trying
to grab any of these cups may be described by the same single abstract model instead of
using N individual models. To apply T in a given world state, one needs to ground T with
respect to some of the objects in the domain. NID rules are an elegant way to specify such
a model T and are described in the following.
3.2 Noisy Indeterministic Deictic Rules
We want to learn a relational model of a stochastic world and use it for planning. Pasula
et al. (2007) have recently introduced an appealing action model representation based on
noisy indeterministic deictic (NID) rules which combine several advantages:
 a relational representation enabling generalization over objects and situations,
 indeterministic action outcomes with probabilities to account for stochastic domains,
 deictic references for actions to reduce action space,
 noise outcomes to avoid explicit modeling of rare and overly complex outcomes, and
 the existence of an effective learning algorithm.
6

fiPlanning with Noisy Probabilistic Relational Rules

Table 1 shows an exemplary NID rule for our complex robot manipulation domain.
Fig. 1 depicts a situation where this rule can be used for prediction. Formally, a NID rule
r is given as

pr,1
: r,1 (X )




..
.
(1)
ar (X ) : r (X ) 

:
r,mr (X )
p

r,m
r


pr,0
: r,0
where X is a set of logical variables in the rule (which represent a (sub-)set of abstract
objects). In the rules which define our world models all formulas are abstract, i.e., their
arguments are logical variables. The rule r consists of preconditions, namely that action
ar is applied on X and that the state P
context r is fulfilled, and mr +1 different outcomes
with associated probabilities pr,i  0, i=0 pr,i = 1. Each outcome r,i (X ) describes which
predicates and functions change when the rule is applied. The context r (X ) and outcomes
r,i (X ) are conjunctions of (potentially negated) literals constructed from the predicates in
P as well as equality statements comparing functions from F to constant values. Besides the
explicitely stated outcomes r,i (i > 0), the so-called noise outcome r,0 models implicitly
all other potential outcomes of this rule. In particular, this includes the rare and overly
complex outcomes typical for noisy domains, which we do not want to cover explicitly for
compactness and generalization reasons. For instance, in the context of the rule depicted in
Fig. 1 a potential, but highly improbable outcome is to grab the blue cube while pushing all
other objects of the table: the noise outcome allows to account for this without the burden
of explicitly stating it.
The arguments of the action a(Xa ) may be a true subset Xa  X of the variables X
of the rule. The remaining variables are called deictic references D = X \ Xa and denote
objects relative to the agent or action being performed. Using deictic references has the
advantage to decrease the arity of action predicates. This in turn reduces the size of the
action space by at least an order of magnitude, which can have significant effects on the
planning problem. For instance, consider a binary action predicate which in a world of
n objects has n2 groundings in contrast to a unary action predicate which has only n
groundings.
As above, let  denote a substitution that maps variables to constant objects,  : X  O.
Applying  to an abstract rule r(X ) yields a ground rule r((X )). We say a ground rule r
covers a state s and a ground action a if s |= r and a = ar . Let  be a set of ground NID
rules. We define (a) := {r | r  , ar = a} to be the set of rules that provide predictions for
action a. If r is the only rule in (a) to cover a and state s, we call it the unique covering rule
for a in s. If a state-action pair (s, a) has a unique covering rule r, we calculate P (s0 | s, a)
by taking all outcomes of r into account weighted by their respective probabilities,
r

0

0

P (s |s, a) = P (s |s, r) =

m
X

pr,i P (s0 |r,i , s) + pr,0 P (s0 |r,0 , s),

(2)

i=1

where, for i > 0, P (s0 | r,i , s) is a deterministic distribution that is one for the unique
state constructed from s taking the changes of r,i into account. The distribution given
7

fiLang & Toussaint

Table 1: Example NID rule for a complex robot manipulation scenario, which models to
try to grab a ball X. The cube Y is implicitly defined as the one below X (deictic
referencing). X ends up in the robots hand with high probability, but might
also fall on the table. With a small probability something unpredictable happens.
Confer Fig. 1 for an example application.

grab(X) : on(X, Y ), ball(X), cube(Y ), table(Z)

 0.7 : inhand(X), on(X, Y )
0.2 : on(X, Z), on(X, Y )


0.1 : noise

Figure 1: The NID rule defined in Table 1 can be used to predict the effects of action
grab(ball) in the situation on the left side. The right side depicts the possible
successor states as predicted by the rule. The noise outcome is indicated by a
question mark and does not define a unique successor state.

the noise outcome, P (s0 | r,0 , s), is unknown and needs to be estimated. Pasula et al. use a
worst case constant bound pmin  P (s0 |r,0 , s) to lower bound P (s0 |s, a). Alternatively, to
come up with a well-defined distribution, one may assign very low probability to very many
successor states. As described in more detail in Sec. 5.2, our planning algorithm PRADA
exploits the factored state representation of a grounded relational domain to achieve this
by predicting each state attribute to change with a very low probability.
If a state-action pair (s, a) does not have a unique covering rule r (e.g. two rules cover
(s, a) providing conflicting predictions), one can predict the effects of a by means of a
noisy default rule r which explains all effects with changing state attributes as noise:
P (s0 |s, r ) = P (s0 | r ,0 , s). Essentially, using r expresses that we do not know what
will happen. This is not meaningful and thus disadvantageous for planning. (Hence, one
should bias a NID rules learner to learn rules with contexts which are likely to be mutually
exclusive.) For this reason, the concept of unique covering rules is crucial in planning with
NID rules. Here, we have to pay the price for using deictic references: when using an
abstract NID rule for prediction, we always have to ensure that its deictic references have
unique groundings. This may require examining a large part of the state representation, so
8

fiPlanning with Noisy Probabilistic Relational Rules

that proper storage of the ground state and efficient indexing techniques for logical formula
evaluation are needed.
The ability to learn models of the environment from experience is a crucial requirement
for autonomous agents. The problem of learning rule-sets is in general NP-hard, but efficiency guarantees on the sample complexity can be given for many learning subtasks with
suitable restrictions (Walsh, 2010). Pasula et al. (2007) have proposed a supervised batch
learning algorithm for complete NID rules. This algorithm learns the structure of rules
as well as their parameters from experience triples (s, a, s0 ), stating the observed successor
state s0 after action a was applied in state s. It performs a greedy search through the space
of rule-sets. It optimizes the tradeoff between maximizing the likelihood of the experience
triples and minimizing the complexity of the current hypothesis rule-set  by optimizing
the scoring metric
X
X
S() =
log P (s0 | s, rs,a )  
P EN (r) ,
(3)
(s,a,s0 )

r

where rs,a is either the unique covering rule for (s, a) or the noisy default rule r and 
is a scaling parameter that controls the influence of regularization. P EN (r) penalizes the
complexity of a rule and is defined as the total number of literals in r.
The noise outcome of NID rules is crucial for learning. The learning algorithm is initialized with a rule-set comprising only the noisy default rule r and then iteratively adds
new rules or modifies existing ones using a set of search operators. The noise outcome
allows avoiding overfitting, as we do not need to model rare and overly complex outcomes
explicitly. Its drawback is that its successor state distribution P (s0 | r,0 , s) is unknown.
To deal with this problem, the learning algorithm uses a lower bound pmin to approximate
this distribution, as described above. This algorithm uses greedy heuristics in its attempt
to learn complete rules, so no guarantees on its behavior can be given. Pasula et al., however, report impressive results in complex noisy environments. In Sec. 6.1, we confirm their
results in a simulated noisy robot manipulation scenario. Our major motivation for employing NID rules is that we can learn them from observed actions and state transitions.
Furthermore, our planning approach PRADA can exploit their simple structure (which is
similar to probabilistic STRIPS operators) and convert them into a DBN representation.
We provide a detailed comparison of NID rules and PPDDL in Appendix B. While NID
rules do not support all features of a sophisticated domain description language such as
PPDDL, they can compactly capture the dynamics of many interesting planning domains.
3.3 Decision-Theoretic Planning
The problem of decision-theoretic planning is to find actions a  A in a given state s which
are expected to maximize future rewards for states and actions (Boutilier et al., 1999).
In classical planning, this reward is usually defined in terms of a clear-cut goal which is
either fulfilled or not fulfilled in a state. This can be expressed by means of a logical
formula . Typically, this formula is a partial state description so that there exists more
than one state where  holds. For example, the goal might be to put all our romance
books on a specific shelf, no matter where the remaining books are lying. In this case,
planning involves finding a sequence of actions a such that executing a starting in s will
9

fiLang & Toussaint

result in a world state s0 with s0 |= . In stochastic domains, however, the outcomes of
actions are uncertain. Probabilistic planning is inherently harder than its deterministic
counterpart (Littman, Goldsmith, & Mundhenk, 1997). In particular, achieving a goal
state with certainty is typically unrealistic. Instead, one may define a lower bound  on
the probability for achieving a goal state. A second source of uncertainty next to uncertain
action outcomes is the uncertainty about the initial state s. We will ignore the latter in the
following and always assume deterministic initial states. As we will see later, however, it is
straightforward to incorporate uncertainty about the initial state using one of our proposed
planning approaches.
Instead of a classical planning task which is finished once we have achieved a state
where the goal is fulfilled, our task may also be ongoing. For instance, our goal might be to
keep the desktop tidy. This can be formalized by means of a reward function over states,
which yields high reward for desirable states (for simplicity, here we assume rewards do
not depend on actions). This is the approach taken in reinforcement learning formalisms
(Sutton & Barto, 1998). Classical planning goals can easily be formalized with such a
reward function. We cast the scenario of planning in a stochastic relational domain in a
relational Markov decision process (RMDP) framework (Boutilier et al., 2001). We follow
the notation of van Otterlo (2009) and define an RMDP as a 4-tuple (S, A, T, R). In contrast
to enumerated state spaces, here the state space S has a relational structure defined by
logical predicates P and functions F, which yield the ground atoms with arguments taken
from the set of domain objects O. The action space A is defined by positive predicates A
with arguments from O. T : S  A  S  [0, 1] is a transition distribution and R : S  R
the reward function. Both T and R can make use of the factored relational representation
of S and A to abstract from states and actions, as discussed in the following. Typically, the
state space S and the action space A of a relational domain are very large. Consider for
instance a domain of 5 objects where we use 3 binary predicates to represent states: in this
2
case, the number of states is 235 = 275 . Relational world models encapsulate the transition
probabilities T in a compact way exploiting the relational structure. For example, NID rules
as described in Eq. (2) achieve this by generalized partial world state descriptions in the
form of conjunctions of abstract literals. The compactness of these models, however, does
not carry over directly to the planning problem.
A (deterministic) policy  : S  A tells us which action to take in a given state. For
a fixed horizon d and a discount
0 <  < 1, we are interested in maximizing the
Pd factor
t r . The value of a factored state is defined as the
discounted total reward r =

t
t=0
expected return from state s following policy :
V  (s) = E[r | s0 = s; ] .

(4)

A solution to an RMDP, and thus to the problem of planning, is an optimal policy   which
maximizes the expected return. It can be defined by the Bellman equation:
X


V  (s) = R(s) +  max[
P (s0 | s, a)V  (s0 )] .
aA

10

s0

(5)

fiPlanning with Noisy Probabilistic Relational Rules

Similarly, one can define the value Q (s, a) of an action a in state s as the expected return
after action a is taken in state s, using policy  to select all subsequent actions:
Q (s, a) = E[r | s0 = s, a0 = a; ]
X
= R(s) + 
V  (s0 )P (s0 | s, a) .

(6)
(7)

s0

The Q-values for the optimal policy   let us define the optimal action a and the optimal
value of a state as


a = argmax Q (s, a)

and

(8)

aA




V  (s) = max Q (s, a) .
aA

(9)

In enumerated unstructured state spaces, state and Q-values can be computed using dynamic programming methods resulting in optimal policies over the complete state space.
Recently, promising approaches exploiting relational structure have been proposed that apply similar ideas to solve or approximate solutions in RDMPs on an abstract level (without
referring to concrete objects from O) (see related work in Sec. 2). Alternatively, one may
reason in the grounded relational domain. This makes it straightforward to account for the
noise outcome and the uniqueness requirement of NID rules. Usually, one focuses on estimating the optimal action values for the given state. This approach is appealing for agents
with varying goals, where quickly coming up with a plan for the problem at hand is more
appropriate than computing an abstract policy over the complete state space. Although
grounding simplifies the problem, decision-theoretic planning in the propositionalized representation is a challenging task in complex stochastic domains. In Sections 4 and 5, we
present different algorithms reasoning in the grounded relational domain for estimating the
optimal Q-values of actions (and action-sequences) for a given state.
3.4 Dynamic Bayesian Networks
Dynamic Bayesian networks (DBNs) model the development of stochastic systems over
time. The PRADA planning algorithm which we introduce in Sec. 5 makes use of this
kind of graphical model to evaluate the stochastic effects of action sequences in factored
grounded relational world states. Therefore, we will briefly review Bayesian networks and
their dynamic extension here.
A Bayesian network (BN) (Jensen, 1996) is a compact representation of the joint probability distribution over a set of random variables X by means of a directed acyclic graph
G. The nodes in G represent the random variables, while the edges define their dependencies and thereby express conditional independence assumptions. The value x of a variable
X  X depends only on the values of its immediate ancestors in G, which are called the
parents P a(X) of X. Conditional probability functions at each node define P (X | P a(X)).
In case of discrete variables, they may be defined in form of conditional probability tables.
A BN is a very compact representation of a distribution over X if all nodes have only few
parents or their conditional probability functions have significant local structure. This will
play a crucial role in our development of the graphical models for PRADA.
11

fiLang & Toussaint

A DBN (Murphy, 2002) extends the BN formalism to model a dynamic system evolving
over time. Usually, the focus is on discrete-time stochastic processes. The underlying
system itself (in our case, a world state) is represented by a BN B, and the DBN maintains
a copy of this BN for every time-step. A DBN can be defined as a pair of BNs (B0 , B ),
where B0 is a (deterministic or uncertain) prior which defines the state of the system at the
initial state t = 0, and B is a two-slice BN which defines the dependencies between two
successive time-steps t and t + 1. This implements a first-order Markov assumption: the
variables at time t + 1 depend only on other variables at time t + 1 or on variables at t.

4. Planning with Look-Ahead Trees
To plan with NID rules, one can treat the domain described by the
ulary as a relational Markov decision process as discussed in Sec.
we present two value-based reinforcement learning algorithms which
generative model to build look-ahead trees starting from the initial
used to estimate the values of actions and states.

relational logic vocab3.3. In the following,
employ NID rules as a
state. These trees are

4.1 Sparse Sampling Trees
The Sparse Sampling Tree (SST) algorithm (Kearns et al., 2002) for MDP planning samples
randomly sparse, but full-grown look-ahead trees of states starting with the given state as
root. This suffices to compute near-optimal actions for any state of an MDP. Given a
planning horizon d and a branching factor b, SST works as follows (see Fig. 2): In each tree
node (representing a state), (i) SST takes all possible actions into account, and (ii) for each
action it takes b samples from the successor state distribution using a generative model for
the transitions, e.g. the transition model T of the MDP, to build tree nodes at the next
level. Values of the tree nodes are computed recursively from the leaves to the root using
the Bellman equation: in a given node, the Q-value of each possible action is estimated
by averaging over all values of the b children states for this action; then, the maximizing
Q-value over all actions is chosen to estimate the value of the given node. SST has the
favorable property that it is independent of the total number of states of the MDP, as it
only examines a restricted subset of the state space. Nonetheless, it is exponential in the
time horizon taken into account.
Pasula et al. (2007) apply SST for planning with NID rules. When sampling the noise
outcome while planning with SST, they assume to stay in the same state, but discount
the estimated value. We refer to this adaptation when we speak of SST planning in the
remainder of the paper. If an action does not have a unique covering rule, we use the noisy
default rule r to predict its effects. It is always better to perform a doN othing action
instead where staying in the same state does not get punished. Hence, in SST planning one
can discard all actions for a given state which do not have unique covering rules.
While SST is near-optimal, in practice it is only feasible for very small branching factor
b and planning horizon d. Let the number of actions be a. Then the number of nodes at
horizon d is (ba)d . (This number can be reduced if the same outcome of a rule is sampled
multiple times.) As an illustration, assume we have 10 possible actions per time-step and
set parameters d = 4 and b = 4 (the choice of Pasula et al. in their experiments). To plan a
single action for a given state, one has to visit (10  4)4 = 2, 560, 000 states. While smaller
12

fiPlanning with Noisy Probabilistic Relational Rules

Figure 2: The SST planning algorithm samples sparse, but full-grown look-ahead trees to
estimate the values of actions and states.

choices of b lead to faster planning, they result in a significant accuracy loss in realistic
domains. As Kearns et al. note, SST is only useful if no special structure that permits
compact representation is available. In Sec. 5, we will introduce an alternative planning
approach based on approximate inference that exploits the structure of NID rules.
4.2 Sampling Trees with Upper Confidence Bounds
The Upper Confidence Bounds applied to Trees (UCT) algorithm (Kocsis & Szepesvari,
2006) also samples a search tree of subsequent states starting with the current state as root.
In contrast to SST which generates b successor states for every action in a state, the idea of
UCT is to choose actions selectively in a given state and thus to sample selectively from the
successor state distribution. UCT tries to identify large subsets of suboptimal actions early
in the sampling procedure and to focus on promising parts of the look-ahead tree instead.
UCT builds its look-ahead tree by repeatedly sampling simulated episodes from the
initial state using a generative model, e.g. the transition model T of the MDP. An episode is a
sequence of states, rewards and actions until a limited horizon d: s0 , r0 , a1 , s1 , r1 , a2 . . . sd , rd .
After each simulated episode, the values of the tree nodes (representing states) are updated
online and the simulation policy is improved with respect to the new values. As a result, a
distinct value is estimated for each state-action pair in the tree by Monte-Carlo simulation.
More precisely, UCT follows the following policy in tree node s: If there exist actions
from s which have not been explored yet, then UCT samples one of these using a uniform
distribution. Otherwise, if all actions have been explored at least once, then UCT selects
the action that maximizes an upper confidence bound QO
U CT (s, a) on the estimated action
13

fiLang & Toussaint

value QU CT (s, a),
s
QO
U CT (s, a)

= QU CT (s, a) + c

log ns
,
ns,a

U CT (s) = argmax QO
U CT (s, a) ,

(10)
(11)

a

where ns,a counts the number of times that actionPa has been selected from state s, and ns
counts the total number of visits to state s, ns = a ns,a . The bias parameter c defines the
influence of the number of previous action selections and thereby controls the extent of the
upper confidence bound.
At the end of an episode, the value of each encountered state-action pair (st , at ), 0 
t < d, is updated using the total discounted rewards:
nst ,at  nst ,at + 1 ,
QU CT (st , at )  QU CT (st , at ) +

(12)
1
nst ,at

d
X

[

0

 t t rt0  QU CT (st , at )] .

(13)

t0 =t

The policy of UCT implements an exploration-exploitation tradeoff: It balances between
exploring currently suboptimal-looking actions that have been selected seldom thus far and
exploiting currently best-looking actions to get more precise estimates of their values. The
total number of episodes controls the accuracy of UCTs estimates and has to be balanced
with its overall running time.
UCT has achieved remarkable results in challenging domains such as the game of Go
(Gelly & Silver, 2007). To the best of our knowledge, we are the first to apply UCT for
planning in stochastic relational domains, using NID rules as a generative model. We adapt
UCT to cope with noise outcomes in the same fashion as SST: we assume to stay in the
same state and discount the obtained rewards. Thus, UCT takes only actions with unique
covering rules into account, for the same reasons as SST does.

5. Planning with Approximate Inference
Uncertain action outcomes characterize complex environments, but make planning in relational domains substantially more difficult. The sampling-based approaches discussed in
the previous section tackle this problem by repeatedly generating samples from the outcome
distribution of an action using the transition probabilities of an MDP. This leads to lookahead trees that easily blow up with the planning horizon. Instead of sampling successor
states, one may maintain a distribution over states, a so-called belief. In the following,
we introduce an approach for planning in grounded stochastic relation domains which propagates beliefs over states in the sense of state monitoring. First, we show how to create
compact graphical models for NID rules. Then we develop an approximate inference method
to efficiently propagate beliefs. With this in hand, we describe our Probabilistic Relational
Action-sampling in DBNs planning Algorithm (PRADA), which samples action-sequences
in an informed way and evaluates these using approximate inference in DBNs. Then, an
example is presented to illustrate the reasoning of PRADA. Finally, we discuss PRADA in
comparison to the approaches of the previous section, SST and UCT, and present a simple
extension of PRADA.
14

fiPlanning with Noisy Probabilistic Relational Rules

(a)

(b)

Figure 3: Graphical models for NID rules: (a) Naive DBN; (b) DBN exploiting NID factorization

5.1 Graphical Models for NID Rules
Decision-theoretic problems where agents need to choose appropriate actions can be represented by means of Markov chains and dynamic Bayesian networks (DBNs) which are
augmented by decision nodes to specify the agents actions (Boutilier et al., 1999). In the
following, we discuss how to convert NID rules to DBNs which the PRADA algorithm will
use to plan with probabilistic inference. We denote random variables by upper case letters
(e.g. S), their values by the corresponding lower case letters (e.g., s  dom(S)), variable
vectors by bold upper case letters (e.g. S = (S1 , S2 , S3 )) and value vectors by bold lower
case letters (e.g. s = (s1 , s2 , s3 )). We also use column notation, e.g. s2:4 = (s2 , s3 , s4 ).
A naive way to convert NID rules to DBNs is shown in Fig. 3(a). States are represented
by a vector S = (S1 , . . . , SN ) where for each ground predicate in P there is a binary Si
and for each ground function in F there is an Sj with range according to the represented
function. Actions are represented by an integer variable A which indicates the action out
of a vector of ground action predicates in A. The reward gained in a state is represented
by U and may depend only on a subset of the state variables. It is possible to express
arbitrary reward expectations P (U | S) with binary U (Cooper, 1988). How can we define
the transition dynamics using NID rules in this naive model? Assume we are given a set of
fully abstract NID rules. We compute all groundings of these rules w.r.t. the objects of the
domain and get the set  of K different ground NID rules. The parents of a state variable
Si0 at the successor time-step include the action variable A and the respective variable Si
at the predecessor time-step. The other parents of Si0 are determined as follows: For each
rule r   where the literal corresponding to Si0 appears in the outcomes of r, all variables
Sk corresponding to literals in the preconditions of r are parents of Si0 . As typically Si0 can
be manipulated by several actions which in turn are modeled by several rules, the total
number of parents of Si0 can be very large. This problem is worsened by the usage of deictic
references in the NID rules, as they increase the total number K of ground rules in . The
resulting local structure of the conditional probability function of Si0 is very complex, as one
has to account for the uniqueness of covering rules. These complex dependencies between
two time-slices make this representation unfeasible for planning.
15

fiLang & Toussaint

Therefore, we exploit the structure of NID rules to model a state transition with the
compact graphical model shown in Fig. 3(b) representing the joint distribution
P (u0 , s0 , o, r,  | a, s)

=

P (u0 | s0 ) P (s0 | o, r, s) P (o | r) P (r | a, ) P ( | s) ,

(14)

which we will explain in detail in the following. As before, assume we are given a set of
fully abstract NID rules, for which we compute the set  of K different ground NID rules
w.r.t. the objects in the domain. In addition to S, S0 , A, U and U 0 as above, we use a
binary random variable i for each rule to model the event that its context holds, which
is the case if all required literals hold. Let I() be the indicator function which is 1 if the
argument evaluates to true and 0 otherwise. Then, we have


K
K
Y
Y
^
P ( | s) =
P (i |s(i ) ) =
Sj = sri ,j  .
(15)
I
i=1

i=1

j(i )

V

We use i i to express a logical conjunction 1  n . The function () yields the set of
indices of the state variables in s, on which  depends. sri denotes the configuration of the
state variables corresponding to the literals in the context of ri . We use an integer-valued
variable R ranging over K +1 possible values to identify the rule which predicts the effects
of the action. If it exists, this is the unique covering rule for the current state-action pair,
i.e., the only rule r  (a) modeling action a whose context holds:


^
P (R = r|a, ) = I r  (a)  r = 1 
r 0 = 0  .
(16)
r0 (a)\{r}

If no unique covering rule exists, we predict no changes as indicated by the special value
R = 0 (assuming not to execute the action, similarly as SST and UCT do):


^
^
P (R = 0 | a, ) =
I r = 1 
r 0 = 0 .
(17)
r0 (a)\{r}

r(a)

The integer-valued variable O represents the outcome of the action as predicted by the
rule. It ranges over M possible values where M is the maximum number of outcomes all
rules in  have. To ensure a sound semantics, we introduce empty dummy outcomes with
zero-probability for those rules whose number of outcomes is less than M . The probability
of an outcome is defined as in the corresponding rule:
P (O = o | r) = pr,o .
We define the probability of the successor state as
Y
P (s0 | o, s, r) =
P (s0i | o, si , r) ,

(18)

(19)

i

which is one for the unique state that is constructed from s taking the changes according
to r,o into account: if outcome o specifies a value for Si0 , this value will have probability
16

fiPlanning with Noisy Probabilistic Relational Rules

one. Otherwise, the value of this state variable persists from the previous time-step. As
rules usually change only a small subset of s, persistence most often applies. The resulting
dependency P (s0i | o, r, si ) of a variable Si0 at time-step t + 1 is compact. In contrast to the
naive DBN in Fig. 3(a), it has only three parents, namely the variables for the outcome,
the rule and its predecessor at the previous time-step. This simplifies the specification of
a conditional probability function for S 0 significantly and enables efficient inference, as we
will see later. The probability of the reward is given by


^
P (U 0 = 1 | s0 ) = I 
Sj0 = j  .
(20)
j(U 0 )

The function (U 0 ) yields the set of indices of the state variables in s0 , on which U 0 depends.
The configuration of these variables that corresponds to our planning goal is denoted by
 . Uncertain initial states can be naturally accounted for by specifying priors P (s0 ). We
renounce the specification of a prior here, however, as the initial state s0 will always be given
in our experiments later to enable comparison to the look-ahead tree based approaches SST
and UCT which require deterministic initial states (which might also be sampled from a
prior). Our choice for the distribution P (a) used for sampling actions will be described in
Sec. 5.3.
For simplicity we have ignored derived predicates and functions which are defined in
terms of other predicates or functions in the presentation of our graphical model. Derived
concepts may increase the compactness of rules. If dependencies among concepts are acyclic,
it is straightforward to include derived concepts in our model by intra-state dependencies
for the corresponding variables. Indeed, we will use derived predicates in our experiments.
We are interested in inferring posterior state distributions P (st | a0:t1 ) given the sequence of previous actions (where we omit conditioning on the initial state for simplicity).
Exact inference is intractable in our graphical model. When constructing a junction tree,
we will get cliques that comprise whole Markov slices (all variables representing the state at
a certain time-step): consider eliminating all state variables St+1 . Due to moralization, the
outcome variable O will be connected to all state variables in St . After elimination of O,
all variables in St will form a clique. Thus, we have to make use of approximate inference
techniques. General loopy belief propagation (LBP) is unfeasible due to the deterministic
dependencies in small cycles which inhibit convergence. We also conducted some preliminary tests in small networks with a damping factor, but without success. It is an interesting
open question whether there are ways to alternate between propagating deterministic information and running LBP on the remaining parts of the network, e.g., whether methods such
as MC-SAT (Poon & Domingos, 2007) can be successfully applied in decision-making contexts as ours. In the next subsection, we propose a different approximate inference scheme
using a factored frontier (FF). The FF algorithm describes a forward inference procedure
that computes exact marginals in the next time-step subject to a factored approximation
of the previous time-step. Here, our advantage is that we can exploit the structure of the
involved DBNs to come up with formulas for these marginals. FF is related to passing only
forward messages. In contrast to LBP, information is not propagated backwards. Note that
our approach does not condition on rewards (as in full planning by inference) and samples
actions, so that backward reasoning is uninformative.
17

fiLang & Toussaint

5.2 Approximate Inference
In the following, we present an efficient method for approximate inference in the previously
proposed DBNs exploiting the factorization of NID rules. We focus on the mathematical
derivations. An illustrative example will be provided in Sec. 5.4.
We follow the idea of the factored frontier (FF) algorithm (Murphy & Weiss, 2001) and
approximate the belief with a product of marginals:
Y
P (st | a0:t1 ) 
P (sti | a0:t1 ) .
(21)
i

We define
(sti ) := P (sti | a0:t1 ) and
(st ) := P (st | a0:t1 ) 

N
Y

(22)
(sti )

(23)

i=1

and derive a FF filter for the DBN model in Fig. 3(b). We are interested in inferring the
state distribution at time t + 1 given an action sequence a0:t and calculate the marginals of
the state attributes as
t+1
(st+1
| a0:t )
i ) = P (si
X
=
P (st+1
| rt , a0:t1 ) P (rt | a0:t ) .
i

(24)
(25)

rt

In Eq. (25), we use all rules for prediction, weighted by their respective posteriors P (rt | a0:t ).
This reflects the fact that depending on the state we use different rules to model the same
action. The weight P (rt | a0:t ) is 0 for all rules not modeling action at . For the remaining
rules which do model at , the weights correspond to the posterior over those parts of the
state space where the according rule is used for prediction.
We compute the first term in (25) as
X
P (st+1
| rt , a0:t1 ) =
P (st+1
| rt , sti ) P (sti | rt , a0:t1 )
i
i
sti



X

P (st+1
| rt , sti ) (sti ) .
i

(26)

sti

Here, we sum over all possible values of the variable Si at the previous time-step t. Intuitively, we take into account all potential pasts to arrive at value st+1
at the next
i
t , st ) enables us to easily predict the probabilities
time-step. The resulting term P (st+1
|
r
i
i
at the next time-step as discussed below. Each such prediction is weighted by the marginal
(sti ) of the respective previous value. The approximation in (26) assumes that sti is conditionally independent of rt . This is not true in general as the choice of a rule for prediction
depends on the current state and thus also on attribute Si . To improve on this approximation one can examine whether sti is part of the context of rt : if this is the case, we can infer
the state of sti from knowing rt . However, we found our approximation to be sufficient.
18

fiPlanning with Noisy Probabilistic Relational Rules

As one would expect, we calculate the successor state distribution P (st+1
| rt , sti ) by
i
t
taking the different outcomes o of r into account weighted by their respective probabilities
P (o | rt ),
X
P (st+1
| rt , sti ) =
P (st+1
| o, rt , sti ) P (o | rt ) .
(27)
i
i
o

This shows us how to update the belief over Sit+1 if we predict with rule rt . P (st+1
| o, rt , sti )
i
t+1
is a deterministic distribution. If o changes the value of Si , si is set accordingly. Otherwise, the value sti persists.
Lets turn to the computation of the second term in Eq. (25), P (rt | a0:t ), the posterior
over rules. The trick is to use the context variables  and to exploit the assumption that a
rule r models the state transition if and only if it uniquely covers (at , st ), which is indicated
by an appropriate assignment of the . This can then be further reduced to an expression
involving only the marginals (). We start with
X
P (Rt = r | a0:t ) =
P (Rt = r | t , a0:t ) P (t | a0:t )
t




^

= I(r  (at )) P tr = 1,

tr0 = 0 | a0:t1 

r0 (at )\{r}




^

= I(r  (at )) P (tr = 1 | a0:t1 ) P 

tr0 = 0 | tr = 1, a0:t1  .

r0 (at )\{r}

(28)
To simplify the summation over t , we only have to consider the unique assignment of the
context variables when r is used for prediction: provided it models the action, as indicated
by I(r  (at )), this is the case if its context tr holds, while the contexts tr0 of all other
competing rules r0 for action at do not hold.
We calculate the second term in (28) by summing over all states s as
X
Y
X
P (tr = 1 | a0:t1 ) =
P (tr = 1 | st ) (st ) 
P (tr = 1 | st )
(stj )
(29)
st

=

Y

st

(Sjt = sr,j )

.

j

(30)

j(tr )

The approximation in (29) is the FF assumption. In (30), sr denotes the configuration of
the state variables according to the context of r like in (15). We sum out all variables not in
the context of r. Only the variables in rs context remain: the terms (Sjt = sr,j ) correspond
to the probabilities of the respective literals.
The third term in (28) is the joint posterior over the contexts of the competing rules r0
given that rs context already holds. We are interested in the situation where none of these
other contexts hold. We calculate this as


^
Y
P
tr0 = 0 | tr = 1, a0:t1  
P (tr0 = 0 | tr = 1, a0:t1 ) ,
(31)
r0 (at )\{r}

r0 (at )\{r}

19

fiLang & Toussaint

approximating it by the product of the individual posteriors. The latter are computed as
X
P (tr0 = 0 | tr = 1, a0:t1 ) =
P (tr0 = 0 | st ) P (st | tr = 1, a0:t1 )
(32)
t

s
if r r0  
 1.0 Q
t
 1.0  i(t ), (Si = sr0 ,i ) otherwise
,

r0
t

(33)

i6(r )

where the if-condition expresses a logical contradiction of the contexts of r and r0 . If their
contexts contradict, then r0 s context will surely not hold given that rs context holds.
Otherwise, we know that the state attributes apppearing in the contexts of both r and r0
do hold as we condition on r = 1. Therefore, we only have to examine the remaining state
attributes of r0 s context. Again, we approximate this posterior with the FF marginals.
Finally, we compute the reward probability straightforwardly as
X
Y
P (U t = 1 | st )P (st | a0:t1 , s0 ) 
(Sit = i ) ,
(34)
P (U t = 1 | a0:t1 ) =
st

i(U t )

where  denotes the configuration of state variables corresponding to the planning goal as
in (20). As above, the summation over states is simplified by the FF assumption resulting
in a product of the marginals of the required state attributes.
The overall computational costs of propagating the effects of an action are quadratic in
the number of rules for this action (for each such rule we have to calculate the probability
that none of the others applies) and linear in the maximum numbers of context literals and
manipulated state attributes of those rules.
Our inference framework requires an approximation for the distribution P (s0 | r,0 , s)
(cf. Eq. (2)) to cope with the noise outcome of NID rules. From the training data used to
learn rules, we estimate which predicates and functions change value over time as follows: let
Sc  S contain the corresponding variables. We estimate for each rule r the average number
N r of changed state attributes when the noise outcome applies. Due to our factored frontier
approach, we can consider the noise effects for each variable independently. We approximate
r
the probability that Si  Sc changes in rs noise outcome by | SNC | . In case of change, all
changed values of Si have equal probability.
5.3 Planning
The DBN representation in Fig. 3(b) together with the approximate inference method described in the last subsection enable us to derive a novel planning algorithm for stochastic
relational domains: The Probabilistic Relational Action-sampling in DBNs planning Algorithm (PRADA) plans by sampling action sequences in an informed way based on predicted
beliefs over states and evaluating these action sequences using approximate inference.
More precisely, we sample sequences of actions a0:T 1 of length T . For 0 < t  T , we
infer the posteriors over states P (st | a0:t1 , s0 ) and rewards P (ut | a0:t1 , s0 ) (in the sense
of filtering or state monitoring). Then, we calculate the value of an action sequence with a
discount factor 0 <  < 1 as
Q(s0 , a0:T 1 ) :=

T
X

 t P (U t = 1 | a0:t1 , s0 ) .

t=0

20

(35)

fiPlanning with Noisy Probabilistic Relational Rules

We choose the first action of the best sequence a = argmaxa0:T 1 Q(a0:T 1 , s0 ), if its
value exceeds a certain threshold  (e.g.,  = 0). Otherwise, we continue sampling actionsequences until either an action is found or planning is given up. The quality of the found
plan can be controlled by the total number of action-sequence samples and has to be traded
off with the time that is available for planning.
We aim for a strategy to sample good action sequences with high probability. We
propose to choose with equal probability among the actions that have a unique covering
rule for the current state. Thereby, we avoid the use of the noisy default rule r which
models action effects as noise and is thus of poor use in planning. For the action at time t,
PRADA samples from the distribution


^
X
t
tr0 = 0 | a0:t1 .
(36)
P tr = 1,
Psample
(a) 
r(a)

r0 (a)\{r}

This is a sum over all rules for action a: for each such rule we add the posterior that it is the
unique covering rule, i.e. that its context tr holds, while the contexts tr0 of the competing
rules r0 do not hold. This sampling distribution takes the current state distribution into
account. Thus, the probability to sample an action sequence a predicting the state sequence
s0 , . . . , sT depends on the likelihood of the state sequence given a: the more likely the required outcomes are, the more likely the next actions will be sampled. Using this policy,
PRADA does not miss actions which SST and UCT explore, as the following proposition
states (proof in Appendix A).
Proposition 1: The set of action sequences PRADA samples with non-zero probability
is a super-set of the ones of SST and UCT.
In our experiments, we replan after each action is executed without reusing the knowledge of previous time-steps. This simple strategy helps to get a general impression of
PRADAs planning performance and complexity. Other strategies are easily conceivable.
For instance, one might execute the entire sequence without replanning, trading off faster
computation times with a potential loss in the achieved reward. In noisy environments, it
might seem a better strategy to combine the reuse of previous plans with replanning. For
instance, one could omit the first action of the previous plan, which has just been executed,
and examine the suitability of the remaining actions in the new state. While we consider
only the single best action sequence, in many planning domains it might also be beneficial
to marginalize over all sequences with the same first action. For instance, an action a1
might lead to a number of reasonable sequences, none of which are the best, while another
action a2 is the first of one very good sequence, but also many bad ones  in which case one
might favor a1 .
5.4 Illustrative Example
Let us consider the small planning problem in Table 2 to illustrate the reasoning procedure
of PRADA. Our domain is a noisy cubeworld represented by predicates table(X), cube(X),
on(X, Y ), inhand(X) and clear(X)  Y.on(Y, X) where a robot can perform two types
of actions: it may either lift a cube X by means of action grab(X) or put the cube which is
21

fiLang & Toussaint

held in hand on top of another object X using puton(X). The start state s0 shown in 2(a)
contains three cubes a, b and c stacked in a pile on table t. The goal shown in 2(b) is to
get the middle cube b on-top of the top cube a. Our world model provides three abstract
NID rules to predict action effects, shown in Table 2(c). Only the first rule has uncertain
outcomes: it models to grab an object which is below another object. In contrast, grabbing
a clear object (Rule 2) and putting an object somewhere (Rule 3) always leads to the same
successor state.
First, PRADA constructs a DBN to represent the planning problem. For this purpose,
it computes the grounded rules with respect to the objects O = {a, b, c, t} shown in 2(d).
Most potential grounded rules can be ignored: one can deduce from the abstract rules which
predicates are changeable. In combination with the specifications in s0 , this prunes most
grounded rules. For instance, we know from s0 that t is the table. Thus, no ground rule
with action argument X = t needs to be constructed as all rules require cube(X).
Based on the DBN, PRADA samples action-sequences and evaluates their expected
rewards. In the following, we investigate this procedure for the sampling of action-sequence
(grab(b), puton(a)). Table 2(e) presents the inferred values of the DBN variables and
other auxiliary quantities. The marginals  (Eq. (22)) of the state variables at t = 0 are
set deterministically according to s0 . We calculate the posteriors over context variables
P ( | a0:t1 ) according to Eq. (30). In our example, at t = 0 there is one rule with
probability 1.0 for each of the actions grab(a), grab(b) and grab(c). In contrast, there are
no rules with non-zero probability for the various puton() actions. By the help of Eq. (33),
we calculate the probability of each rule r to be the unique covering rule for the respective
action (listed under Unique rule; note that we do not condition on a fixed action at thus
far): this is the case if context r of r holds, while all contexts r0 of the competing rules
r0 for the same action do not hold. At t = 0, this is the same as the posterior of r alone.
The resulting probabilities are used to calculate the sampling distribution of Eq. (36): first,
we compute the probability for each action to have a unique covering rule which is a simple
sum over probabilities of the previous step (listed under Action coverage in the table); then,
we normalize these values to get a sampling distribution Psample (). At t = 0, this results in
a sampling distribution which is uniform over the three actions with unique rules. Assume
we sample a0 = grab(b) (grabbing blue cube b). Variable R specifies the ground rules to
use for predicting the state marginals at the next time-step. We can infer its posterior
according to Eq. (28). Here, P (R0 = (1, b/act) | a0 ) = 1.0.
Things get more interesting at t = 1. Here, we observe the effects of the factored
frontier. For instance, consider calculating the posterior over context r for ground rule
r = (1, b/att) (grabbing blue cube b which is below yellow a) using Eq. (30),
P ((1,b/att) | a0 )  (on(a, b))  (on(b, t))  (cube(a))  (cube(b))  (table(t))
= 0.2  0.2  1.0  1.0  1.0 = 0.04.
In contrast, the exact value is P ((1,b/att) | a0 ) = 0.2, according to the third outcome of
abstract Rule 1 used to predict a0 . The imprecision is due to ignoring the correlations: FF
regards the marginals for on(a, b) and on(b, t) as independent, while in fact they are fully
correlated.
At t = 1, the action grab(a) has three ground rules with non-zero context probabilities
(grabbing a from either b, c or t). This is due to the three different outcomes of abstract
22

fiPlanning with Noisy Probabilistic Relational Rules

Table 2: Example of PRADAs factored frontier inference
(a) Start state
s0 = {on(a, b), on(b, c), on(c, t),
cube(a), cube(b), cube(c), table(t)}

(b) Goal
 = {on(b, a)}

(c) Abstract NID rules with example situations
Rule 1:
grab(X) : on(Y, X), on(X, Z), cube(X), cube(Y ), table(T )

 0.5 : inhand(X), on(Y, Z), on(Y, X), on(X, Z)
0.3 : inhand(X), on(Y, T ), on(Y, X), on(X, Z)


0.2 : on(X, T ), on(X, Z)

Rule 2:
grab(X) : cube(X), clear(X), on(X, Y )

1.0 : inhand(X), on(X, Y )


(e) Inferred posteriors in PRADAs
FF
inference
for
action-sequence
(grab(b), puton(a))
t=0

t=1

t=2

State marginals 
on(a, b)
on(a, c)
on(a, t)
on(b, a)
on(b, c)
on(b, t)
on(c, t)
inhand(b)
clear(a)
clear(b)
clear(c)
Goal U

1.0
0.0
0.0
0.0
1.0
0.0
1.0
0.0
1.0
0.0
0.0
0.0

0.2
0.5
0.3
0.0
0.0
0.2
1.0
0.8
1.0
0.8
0.5
0.0

0.2
0.5
0.3
0.8
0.0
0.2
1.0
0.16
0.2
0.8
0.5
0.8

P ( | a0:t1 )
(1,b/act)
(1,b/att)
(1,c/btt)
(2,a/b)
(2,a/c)
(2,a/t)
(2,b/t)
(2,c/t)
(3,a/b)
(3,c/b)
(3,t/b)

1.0
0.0
1.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.04
0.5
0.2
0.5
0.3
0.16
0.5
0.8
0.8
0.8

Unique rule
(1, b/act)
1.0
0.0
(1, b/att)
0.0 0.0336
(1, c/att)
0.0
0.25
(1, c/btt)
1.0
0.0
(2, a/b)
1.0
0.07
(2, a/c)
0.0
0.28
(2, a/t)
0.0
0.12
(2, b/t)
0.0 0.154
(2, c/t)
0.0
0.25
(3, a/b)
0.0
0.8
(3, c/b)
0.0
0.8
(3, t/b)
0.0
0.8
Action coverage
grab(a)
1.0
0.47
grab(b)
1.0 0.187
grab(c)
1.0
0.5
puton(a)
0.0
0.8
puton(c)
0.0
0.8
puton(t)
0.0
0.8
Sample distribution
Psample (grab(a))
0.33 0.132
Psample (grab(b))
0.33 0.0526
Psample (grab(c))
0.33 0.141
Psample (puton(a))
0.0 0.225
Psample (puton(c))
0.0 0.225
Psample (puton(t))
0.0 0.225
P (Rt = rt | a0:t )
Rt = (1, b/act)
1.0
0.0
Rt = (3, a/b)
0.0
0.8
Rt = 0
0.0
0.2

Rule 3:
puton(X) : inhand(Y ), cube(Y )

1.0 : on(Y, X), inhand(X)


(d) Grounded NID rules
Grounded Rule Action
Substitution
(1, a/bbt)
grab(a) {X  a, Y  b, Z  b, T  t}
(1, a/bct)
grab(a) {X  a, Y  b, Z  c, T  t}
...
(1, c/bbt)
grab(c) {X  c, Y  b, Z  b, T  t}
(2, a/b)
grab(a)
{X  a, Y  b}
(2, a/c)
grab(a)
{X  a, Y  c}
(2, a/t)
grab(a)
{X  a, Y  t}
...
(2, c/t)
grab(c)
{X  c, Y  t}
(3, a/b)
puton(a)
{X  a, Y  b}
(3, a/c)
puton(a)
{X  a, Y  c}
...
(3, t/c)
puton(t)
{X  a, Y  c}

23

fiLang & Toussaint

Rule 1. As an example, we calculate the probability of rule (2, a/c) (grabbing a from c) to
be the unique covering rule for grab(a) at t = 1 as
P ((2,a/c) ,(2,a/b) , (2,a/t) | a0 )
 P ((2,a/c) | a0 )  (1.  P ((2,a/b) | a0 ))  (1.  P ((2,a/t) | a0 ))
= 0.5  (1.  0.2)  (1.  0.3) = 0.28 .
After some more calculations, we determine the sampling distribution at t = 1. Assume
we sample action puton(a). This results in rule (3/a, b) (putting b on a) being used for
prediction with 0.8 probability  since this is its probability to be the unique covering rule
for action puton(a). The remaining mass 0.2 of the posterior is assigned to those parts of
the state space where no unique covering rule is available for puton(a). In this case, we use
the default rule R = 0 (corresponding to not performing the action) so that with probability
0.2 the values of the state variables persist.
Finally, let us infer the marginals at t = 2 using Eq. (25). As an example, we calculate
(inhand(b)t=2 ). Let i(b) be brief for inhand(b). We sum over the ground rules rt=1 taking
the potential values i(b)t=1 and i(b)t=1 at the previous time-step t = 1 into account,
X
(i(b)t=2 ) 
P (rt=1 | a0:1 ) ( P (i(b)t=2 | rt=1 , i(b)t=1 ) (i(b)t=1 )
rt=1

+ P (i(b)t=2 | rt=1 , i(b)t=1 ) (i(b)t=1 ) )
= 0.8 (0.0  0.2 + 0.0  0.8) + 0.2 (0.0  0.2 + 1.0  0.8) = 0.16 .
As discussed above, only the ground rule (3/a, b) and the default rule play a role in this
prediction. In effect, the belief that b is inhand decreases from 0.8 to 0.16 after having tried
to put b on a, as expected. Similarly, we calculate the posterior of on(b, a) as 0.8. This is
also the expected probability to reach the goal when performing the actions grab(b) and
puton(a). (Here, PRADAs inferred value coincides with the true posterior.)
For comparison, the probability to reach the goal is 1.0 when performing the actions
grab(a), puton(t), grab(b) and puton(a), i.e., when we clear b before we grab it. This plan
is safer, i.e., has higher probability, but takes more actions.
5.5 Comparison of the Planning Approaches
The most prominent difference between the presented planning approaches is in their way
to account for the stochasticity of action effects. On the one hand, SST and UCT repeatedly take samples from successor state distributions and estimate the value of an action by
building look-ahead trees. On the other hand, PRADA maintains beliefs over states and
propagates indetermistic action effects forward. More precisely, PRADA and SST follow
opposite approaches: PRADA samples actions and calculates the state transitions approximately by means of probabilistic inference, while SST considers all actions (and thus is exact
in its action search) and samples state transitions. The price for considering all actions is
SSTs overwhelmingly large computational cost. UCT remedies this issue and samples action sequences and thus state transitions selectively: it uses previously sampled episodes to
build upper confidence bounds on the estimates for action values in specific states, which
are used to adapt the policy for the next episode. It is not straightforward to translate
24

fiPlanning with Noisy Probabilistic Relational Rules

this adaptive policy to PRADA since PRADA works on beliefs over states instead of states
directly. Therefore, we chose the simple policy for PRADA to sample randomly from all
actions with a unique covering rule in a state (in the form of a sampling distribution to
account for beliefs over states).
PRADA returns a whole plan that will transform the world state into one where the goal
is fulfilled with a probability exceeding a given threshold , in the spirit of conformant planning or probabilistic planning with no observability (Kushmerick, Hanks, & Weld, 1995).
Due to their outcome-sampling, SST and UCT cannot return such a plan in a straightforward way. Instead, they provide a policy for many successor states based on their estimates
of the action-values in their look-ahead tree. The estimates of states deeper in the tree are
less reliable as they have been built from less episodes. If an action has been executed and a
new state is observed, these estimates can be reused. Thus far, PRADA does not take any
knowledge gained in previous action-sequence samples into account to adapt its policy. An
elegant way to achieve this and to better exploit goal knowledge might use backpropagation
through our DBNs to plan completely by inference (Toussaint & Storkey, 2006). This is
beyond the scope of this paper, as it is not clear how to do this in a principled way in the
large state and action spaces of relational domains. Alternatively, PRADA could give high
weight to the second action of the previous best plan. Below in Sec. 5.6, we show another
simple way to make use of previous episodes to find better plans.
PRADA can afford its simple action-sampling strategy as it evaluates large numbers
of action-sequences efficiently and does not have to grow look-ahead trees to account for
indeterministic effects. This points at an important difference: all three algorithms are faced
with search spaces of action sequences which are exponential in the horizon. To calculate
the value of a given action sequence, however, SST and UCT still need exponential time due
to their outcome sampling. In contrast, PRADA propagates the state transitions forward
and thus is linear in the horizon.
Like all approximate planning algorithms, neither SST, UCT nor PRADA can be expected to perform ideally in all situations. SST and UCT sample action outcomes and hence
face problems if important outcomes only have small probability. For instance, consider an
agent that wants to escape a room with two locked doors. If it hits the first door which is
made of wood it has a chance of 0.05 to break it and escape. The second door is made of
iron and has only a chance of 0.001 to break. SST and UCT may take a very long time to
detect that it is 50 times better to repeatedly hit the wooden door. In contrast, PRADA
recognizes this immediately after having reasoned about each of the actions once as it takes
all outcomes into account. On the other hand, in PRADAs approximate inference procedure the correlations among state variables get lost while SST and UCT preserve them as
they sample complete successor states. This can impair PRADAs planning performance in
situations where correlations are crucial. Consider the following simple domain with two
state attributes a and b. The agent can choose from two actions modeled by the rules

action1 :
action2 :






0.5 : a, b
, and
0.5 : a, b



0.5 : a, b
0.5 : b, a




25

.

fiLang & Toussaint

The goal is to make both attributes either true or false, i.e.,  = (a  b)  (a  b). For
both actions, the resulting marginals will be (a) = 0.5, (a) = 0.5, (b) = 0.5 and
(b) = 0.5. Due to its factored frontier, PRADA cannot distinguish between both actions
although action1 will achieve the goal, while action2 will not.
PRADAs estimated probabilities of states and rewards may differ significantly from
their true values. This does not harm its performance in many domains as our experiments
indicate (Sec. 6). We suppose the reason for this is that while PRADAs estimated probabilities can be imprecise, they enable a correct ranking of action sequences  and in planning,
we are interested in choosing the best action instead of calculating correctly its value.
A further difference between the proposed algorithms is in their way to handle the noise
outcome of rules: PRADA assigns very small probability to all successor states  in the spirit
of the noise outcome. In contrast, for SST and UCT it does not make sense to sample from
such a distribution, as any single successor state has extremely low probability and will be
inadequate to estimate state and action values. Hence, they use the described workaround
to assume to stay in the same state, while discounting obtained rewards.
It is straightforward for PRADA to deal with uncertain initial states. Uncertainty of
initial states is common in complex environments and may for instance be caused by partial
observability or noisy sensors. This uncertainty has its natural representation in the belief
state PRADA works on. In contrast, SST and UCT cannot account for uncertain initial
states directly, but would have to sample from the prior distribution.
5.6 An Extension: Adaptive PRADA
We present a very simple extension of PRADA to increase its planning accuracy. We
exploit the fact that PRADA evaluates complete sequences of actions  in contrast to SST
and UCT where the actions taken at t > 0 depend on the sampled outcomes. Adaptive
PRADA (A-PRADA) examines the best action sequence found by PRADA. While PRADA
chooses the first action of this sequence without further reasoning, A-PRADA inspects each
single action of this sequence and decides by simulation whether it can be deleted. The
resulting shortened sequence may lead to an increased expected reward. This is the case if
actions do not have significant effects on achieving the goal or if they decrease the success
probability. If such actions are omitted, the states with high reward are reached earlier and
their rewards are discounted less. For instance, consider the goal to grab a blue ball: an
action sequence that grabs a red cube, puts it onto the table and only then grabs the blue
ball can be improved by omitting the first two actions which are unrelated to the goal.
More precisely, A-PRADA takes PRADAs action sequence aP with the highest value
and investigates iteratively for each action whether it can be deleted. An action can be
deleted from the plan if the resulting plan has a higher reward likelihood. This idea is
formalized in Algorithm 1. The crucial calculation of this algorithm is to compute values
Q(s0 , a0:T 1 ) as defined in Eq. (28) and restated here for convenience:
0

Q(s , a

0:T 1

)=

T
X

 t P (U t = 1 | a0:t1 , s0 ) .

t=1

PRADAs approximate inference procedure is particularly suitable for calculating all required P (U t = 1 | a0:t1 , s0 ). It performs this calculation in time linear in the length T of
26

fiPlanning with Noisy Probabilistic Relational Rules

Algorithm 1 Adaptive PRADA (A-PRADA)
Input: PRADAs plan aP
Output: A-PRADAs plan aA
1: aA  aP
2: for t = 0 to t = T  1 do
3:
while true do
4:
Let a be a plan of length T .
5:
a0:t1  a0:t1
B Omit at
A
t+1:T 1
t:T 2
6:
a
 aA
7:
aT 1  doN othing
8:
if Q(s0 , a) > Q(s0 , aA ) then
9:
aA  a
10:
else
11:
break
12:
end if
13:
end while
14: end for
15: return aA

the action sequence, while SST and UCT would require time exponential in T because of
their outcome sampling.

6. Evaluation
We have implemented all presented planning algorithms and the learning algorithm for
NID rules in C++. Our code is available at www.user.tu-berlin.de/lang/prada/. We
evaluate our approaches in two different scenarios. The first is an intrinsically noisy complex simulated environment where we learn NID rules from experience and use these to
plan. Second, we apply our algorithms on the benchmarks of the Uncertainty Part of the
International Planning Competition 2008.
6.1 Simulated Robot Manipulation Environment
We perform experiments in a simulated complex robot manipulation environment where a
robot manipulates objects scattered on a table (Fig. 4). Before we report our results in three
series of experiments on different tasks of increasing difficulty, we first describe this domain
in detail. We use a 3D rigid-body dynamics simulator (ODE) that enables a realistic behavior of the objects. This simulator is available at www.user.tu-berlin.de/lang/DWSim/.
Objects are cubes and balls of different sizes and colors. The robot can grab objects and
put them on top of other objects or on the table. The actions of the robot are affected by
noise. In this domain, towers of objects are not straight-lined; it is easier to put an object
on top of a big cube than on top of a small cube while it is difficult to put something on
top of a ball; piles of objects may topple over; objects may fall off the table in which case
they become out of reach for the robot.
We represent this domain with predicates on(X, Y ), inhand(X), upright(X), out(X) (if
an object has fallen off the table), function size(X) and unary typing predicates cube(X),
ball(X), table(X). These predicates are obtained by querying the state of the simulator and
27

fiLang & Toussaint

Figure 4: A simulated robot plays with cubes and balls of different sizes scattered on a
table. Objects that have fallen off the table cannot be manipulated anymore.

translating it according to simple hand-made guidelines, thereby sidestepping the difficult
problem of converting the agents observations into an internal representation. For instance,
on(a, b) holds if a and b exert friction forces on each other and as z-coordinate is greater
than the one of b, while their x- and y-coordinates are similar. Besides these primitive
concepts, we also use the derived predicate clear(X)  Y.on(Y, X). We found this
predicate to enable more compact and accurate rules, which is reflected in the values of the
objective function of the rule learning algorithm given in Eq. (3).
We define three different types of actions. These actions correspond to motor primitives
whose effects we want to learn and exploit. The grab(X) action triggers the robot to open
its hand, move its hand next to X, let it grab X and raise the robot arm again. The
execution of this action is not influenced by any further factors. For example, if a different
object Y has been held in the hand before, it will fall down on either the table or a third
object just below Y ; if there are objects on top of X, these are very likely to fall down.
The puton(X) action centers the robots hand at a certain distance above X, opens it and
raises the hand again. For instance, if there is an object Z on X, the object Y that was
potentially inhand may end up on Z or Z might fall off X. The doN othing() action triggers
no movement of the robots arm. The robot might choose this action if it thinks that any
other action could be harmful with respect to its expected reward. We emphasize again
that actions always execute, regardless of the state of the world. Also, actions which are
rather unintuitive for humans such as trying to grab the table or to put an object on top of
itself are carried out. The robot has to learn by itself the effects of such motor primitives.
Due to its intrinsic noise and its complexity, this simulated robot manipulation scenario
is a challenging domain for both learning compact world models as well as planning. If there
are o objects and f different object sizes, the action space contains 2o+1 actions while the
2
state space is huge with f o 2o +6o different states (not excluding states one would classify as
impossible given some intuition about real world physics).
We use the rule learning algorithm of Pasula et al. (2007) with the same parameter
settings to learn three different sets of fully abstract NID rules. Each rule-set is learned
28

fiPlanning with Noisy Probabilistic Relational Rules

from independent training sets of 500 experience triples (s, a, s0 ) that specify how the world
changed from state s to successor state s0 when an action a was executed, assuming full
observability. Training data to learn rules are generated in a world of six cubes and four
balls of two different sizes by performing random actions with a slight bias to build high
piles. Our resulting rule-sets contain 9, 10 and 10 rules respectively. These rule-sets provide
approximate partial models to the true world dynamics. They generalize over the situations
of the experiences, but may not account for situations that are completely different from
what the agent has seen before. To enforce compactness and avoid overfitting, rules are
regularized; hence, the learning algorithm may sometimes favor to model rarely experienced
state transitions as low-probability outcomes in more general rules, thereby trading off
accuracy for compactness. This in combination with the general noisiness of the world
causes the need to carefully account for the probabilities of the world when reasoning with
these rules.
We perform three series of experiments with planning tasks of increasing difficulty. In
each series, we test the planners in different worlds with varying numbers of cubes and
balls. Thus, we transfer the knowledge gained in the training world to different, but similar
worlds by using abstract NID rules. For each object number, we create five different worlds.
Per rule-set and world, we perform three independent runs with different random seeds.
To evaluate the different planning approaches, we compute the mean performances and
planning times over the fixed (but randomly generated) set of 45 trials (3 learned rule-sets,
5 worlds, 3 random seeds).
We choose the parameters of the planning algorithms as follows. For SST, we report results for different branching factors b, as far as the resulting runtimes allow. Similarly, UCT
and (A-)PRADA each have a parameter that balances their planning time and the quality
of their found actions. For UCT, this is the number of episodes, while for (A-)PRADA
this is the number of sampled action-sequences. Depending on the experiment, we set both
heuristically such that the tradeoff between planning time and quality is reasonable. In
particular, for a fair comparison we pay attention that UCT, PRADA and A-PRADA get
about the same planning times, if not reported otherwise. Furthermore, for UCT we set
the bias parameter c to 1.0 which we found heuristically to perform best. For all planners
and experiments, we set the discounting factor for future rewards to  = 0.95. A crucial
parameter is the planning horizon d, which heavily influences planning time. Of course, d
cannot be known a-priori. Therefore, if not reported otherwise, we deliberately set d larger
than required for UCT and (A-)PRADA to suggest that our algorithms are also effective
when d can only be estimated. Indeed, we found in all our experiments that as long as d is
not too small, its exact choice does not have significant effects on UCTs and (A-)PRADAs
planning quality  unlike its effects on planning times. In contrast, we set the horizon d
for SST always as small as possible, in which case its planning times are still very large.
If a planning algorithm does not find a suitable action in a given situation, we restart the
planning procedure: SST builds a new tree, UCT runs more episodes and (A-)PRADA
takes new action-sequence samples. If in a given situation after 10 planning runs a suitable
action still is not found, the trial fails.
Furthermore, we use FF-Replan (Yoon et al., 2007) as a baseline. As we discuss in
more detail with the related work in Sec. 2, FF-Replan determinizes the planning problem,
thereby ignoring outcome probabilities. FF-Replan has shown impressive results on the
29

fiLang & Toussaint

domains of the probabilistic planning competitions. These domains are carefully designed
by humans: their action dynamics definitions are complete, accurate and consistent and are
used as the true world dynamics in the according experiments  in contrast to the learned
NID rules we use here which estimate approximate partial models of our robot manipulation
domain. To be able to use the derived predicate clear(X) in the FF-Replan implementation
of our experiments, we included the appropriate literals of this predicate by hand in the
outcomes of the rules  while our SST, UCT and (A-)PRADA implementations infer these
values automatically from the definition of clear(X). We report results of FF-Replan with
these (almost original) learned rules using the all-outcomes determinization scheme, denoted
by FF-Replan-All below. (Using single-outcome schemes always led to worse performance.)
Some of these rules are very general (putting only few restrictions on the arguments and
deictic references); in this case, more actions appear applicable in a given state than make
sense from an intuitive human perspective which hurts FF-Replan much more than the other
methods, resulting in large planning times for FF-Replan. For instance, a rule may model
the toppling over of a small tower including object X when trying to put an object Y on top
of the tower: one outcome might specify Y to end up below X. While this is only possible
if Y is a cube, of course, the learning algorithm may choose to omit a typing predicate
cube(X) due to regularization, as it prefers compact rules and none of its experiences might
require this additional predicate. Therefore, we created modified rule-sets by hand where we
introduced typing predicates where appropriate to make contexts more distinct. Below, we
denote our results with these modified rule-sets as FF-Replan-All* and FF-Replan-Single*,
using all-outcomes and single most-probable outcome determinization schemes.
6.1.1 High Towers
In our first series of experiments, we investigate building high towers which was the planning
task in the work of Pasula et al. (2007). More precisely, the reward in a state is defined as
the average height of objects. This constitutes an easy planning problem as many different
actions may increase the reward (object identities do not matter) and a small planning
horizon d is sufficient. We set SST to horizon d = 4 (Pasula et al. s choice) with different
branching factors b and UCT and (A-)PRADA to horizon d = 6. In our experiments, initial
states do not contain already stacked objects, so the reward for performing no actions is
0. Table 3 and Fig. 5 present our results. SST is not competitive. For a branching factor
b > 1, it is slower than UCT and (A-)PRADA by at least an order of magnitude. For
b = 1, its performance is poor. In this series of experiments, we designed the worlds of 10
objects to contain many big cubes. This explains the relatively good performance of SST in
these worlds, as the number of good plans is large. As mentioned above, we control UCT,
PRADA and A-PRADA to have about the same times available for planning. All three
approaches perform far better than SST in almost all experiments. The difference between
UCT, PRADA and A-PRADA is never significant.
This series of experiments indicates that planning approaches using full-grown lookahead trees like SST are inappropriate even for easy planning problems. In contrast, approaches that exploit look-ahead trees in a clever way such as UCT seem to be the best
choice for easy tasks which require a small planning horizon and can be solved by many
alternative good plans. The performance of the planning approaches using approximate
30

fiPlanning with Noisy Probabilistic Relational Rules

Table 3: High towers problem. Reward denotes the discounted total reward for different
numbers of objects (cubes/balls and table). The reward for performing no actions
is 0. All data points are averages over 45 trials created from 3 learned rule-sets,
5 worlds and 3 random seeds. Standard deviations of the mean estimators are
shown. FF-Replan-All* and FF-Replan-Single* use hand-made modifications of
the original learned rule-sets. Fig. 5 visualizes these results.
Objects

Planner
FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

6+1

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

6.65  1.01
6.29  0.80
4.48  0.94

41.07  9.63
7.54  4.09
4.61  2.75

1.19
1.01
0.94
0.99
1.25
1.27

9.03  0.80
121.40  11.12
595.43  55.95
7.45  0.19
6.01  0.07
6.36  0.07

5.10  1.01
3.08  0.87
2.82  0.87

76.86  20.98
28.65  16.81
1.72  0.27








1.07
1.21
0.87
1.07
1.21
1.47

23.57  3.48
335.5  52.4
1613.3  249.2
15.54  0.40
15.24  0.27
16.30  0.27

6.97  1.21
7.36  1.07
5.76  1.21

121.99  27.43
33.45  12.80
4.14  1.08








119.26  10.59
1748.7  170.2
8424  851
31.71  5.83
31.58  1.14
35.22  0.40

9.62
12.36
11.09
17.11
16.10
16.29

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
10+1

Trial time (s)

11.68
12.90
12.80
16.01
15.54
16.12

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
8+1

Reward

15.12
14.48
16.48
17.71
16.21
16.78

31








1.34
1.20
1.19
1.08
1.07
1.14

fiLang & Toussaint

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
SST b=1
SST b=2
SST b=3
UCT
PRADA
A-PRADA

15
10
5

1000

Trial time (s)

Discounted total reward

10000

100
10
1

6

8
Objects

10

6

(a) Reward

8
Objects

10

(b) Time

Figure 5: High towers problem Visualization of the results presented in Table 3. The reward
for performing no actions is 0. All data points are averages over 45 trials created
from 3 learned rule-sets, 5 worlds and 3 random seeds. Error bars for the standard
deviations of the mean estimators are shown. Please note the log-scale in (b).

inference, PRADA and A-PRADA, however, comes close to the one of UCT, showing also
their suitability for such scenarios.
FF-Replan focuses on exploiting conjunctive goal structures and cannot deal with quantified goals. As the grounded reward structure of this task consists of a disjunction of
different tower combinations, FF-Replan has to pick an arbitrary tower combination as its
goal. Therefore, to apply FF-Replan we sample tower combinations according to the rewards they achieve (i.e., situations with high towers are more probable) and do not exclude
combinations with balls at the bottom of towers as they are not prohibited by the reward
structure. As Yoon et al. note, the obvious pitfall of this [goal formula sampling] approach
is that some groundings of the goal are not reachable or are much more expensive to reach
from the initial state. When FF-Replan cannot find a plan, we do not execute an action,
but sample a new ground goal formula at the next time-step, preserving already achieved
tower structures.
FF-Replan performs significantly worse than the previous planning approaches. The
major reason for this is that FF-Replan often comes up with plans exploiting low-probability
outcomes of rules  in contrast to SST, UCT and (A-)PRADA which reason over the
probabilities. To illustrate this, consider the example rule in Fig. 1 which models putting
a ball on top of a cube. It has two explicit outcomes: the ball usually ends up on the
cube; sometimes, however, it falls on the table. FF-Replan can misuse this rule as a tricky
way to put a ball on the table  ignoring that this often will fail. As the results of FFReplan-Single* show, taking only most probable outcomes into account does not remedy
this problem: there are often two to three outcomes with similar probabilities so such a
choice seems unjustified; sometimes, the intuitively expected outcome is split up into
different outcomes with low probabilities, which however vary only in features irrelevant for
the planning problem (such as upright()).
32

fiPlanning with Noisy Probabilistic Relational Rules

Table 4: Desktop clearance problem. Reward denotes the discounted total reward for different numbers of objects (cubes/balls and table). The reward for performing no
actions is 0. All data points are averages over 45 trials created from 3 learned rulesets, 5 worlds and 3 random seeds. Standard deviations of the mean estimators
are shown. FF-Replan-All* and FF-Replan-Single* use hand-made modifications
of the original learned rule-sets. Fig. 6 visualizes these results.
Obj.

Planner
FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

6+1

SST (b=1)
UCT
PRADA
A-PRADA

SST (b=1)
UCT
PRADA
A-PRADA

SST (b=1)
UCT
PRADA
A-PRADA

3.81  0.67
5.86  0.87
6.53  1.07

19.1  6.5
1.1  0.7
0.7  0.8

0.75
0.86
0.86
0.80

1382.6  80.4
52.2  0.7
40.9  0.7
42.3  0.7

5.93  1.00
6.21  1.05
6.02  0.94

29.8  8.7
3.5  0.6
0.8  0.7






2.01
1.08
1.54
1.57

8157  978
151.4  2.0
154.5  1.9
157.4  2.0

3.30  0.74
3.53  0.87
3.91  0.86

60.9  12.1
20.7  5.4
5.2  1.3


10.13  0.80
12.81  1.14
13.91  1.12

> 8h
415.7  7.4
385.3  4.7
394.5  4.0

8.43
10.29
14.63
14.87

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
10+1

Trial time (s)

5.35
9.60
10.94
12.79

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
8+1

Reward






6.1.2 Desktop Clearance
The task in our second series of experiments is to clear up the desktop. Objects are lying
splattered all over the table in the beginning. An object is cleared if it is part of a tower
containing all other objects of the same class. An object class is simply defined in terms of
color which is additionally provided to the state representation of the robot. The reward of
the robot is defined as the number of cleared objects. In our experiments, classes contain
2-4 objects with at most 1 ball (in order to enable successful piling). Our starting situations contain some piles, but only with objects of different classes. Thus, the reward for
performing no actions is 0. Desktop clearance is more difficult than building high towers,
as the number of good plans yielding high rewards is significantly reduced.
We set the planning horizon d = 6 optimal for SST which is required to clear up a
class of 4 objects, namely grabing and putting three objects. As above, by contrast we set
d = 10 for UCT and (A-)PRADA to show that they can deal with overestimation of the
usually unknown optimal horizon d. Table 4 and Fig. 6 present our results. The horizon
d = 6 overburdens SST as can be seen from its large planning times. Even for b = 1, SST
takes almost 40 minutes on average in worlds of 6 objects, while over 2 hours in worlds of
8 objects. Therefore, we did not try SST for greater b. In contrast, the planning times
33

fi16

10000

14

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
SST b=1
UCT
PRADA
A-PRADA

12
10
8
6
4
2
6

8
Objects

1000
Trial time (s)

Discounted total reward

Lang & Toussaint

100
10
1
6

10

(a) Reward

8
Objects

10

(b) Time

Figure 6: Desktop clearance problem. Visualization of the results presented in Table 4. The
reward for performing no actions is 0. All data points are averages over 45 trials
created from 3 learned rule-sets, 5 worlds and 3 random seeds. Error bars for the
standard deviations of the mean estimators are shown. Note the log-scale in (b).

of UCT, PRADA and A-PRADA, again controlled to be about the same and to enable
reasonable performance, are two orders of magnitude smaller, although overestimating the
planning horizon: for a trial they take on average about 45s in worlds of 6 objects, 2 12
minutes in worlds of 8 objects and 6-7 minutes in worlds of 10 objects. Nonetheless, UCT,
PRADA and A-PRADA perform significantly better than SST. In all worlds, PRADA and
A-PRADA in turn outperform UCT, in particular in worlds with many objects. A-PRADA
finds the best plans among all planners. All planners gain more reward in worlds of 8 objects
in comparison to worlds of 6 objects, as the number of objects that can be cleared increases
as well as the number of classes and thus of good plans. The worlds of 10 objects contain
the same numbers of object classes like the worlds of 8 objects, but with more objects,
making planning more difficult.
Overall, our findings in the Desktop clearance experiments indicate that while SST is
inappropriate, UCT achieves good performance in planning scenarios which require medium
planning horizons and where there are several, but not many alternative plans. Approaches
using approximate inference like PRADA and A-PRADA, however, seem to be more appropriate in such scenarios of intermediate difficulty.
Furthermore, our results indicate that FF-Replan is inadequate for the clearance task.
We sample target classes randomly to provide a goal structure to FF-Replan; the tower
structure within a target class in turn is also randomly chosen. The bad performance of
FF-Replan is due to the reasons described in the previous experiments; in particular the
plans of FF-Replan often rely on low-probability outcomes.
34

fiPlanning with Noisy Probabilistic Relational Rules

Table 5: Reverse tower problem. The trial times and numbers of executed actions are given
for the successful trials for different numbers of objects (cubes and table). All
data points are averages over 45 trials created from 3 learned rule-sets, 5 worlds
and 3 random seeds. Standard deviations of the mean estimators are shown. FFReplan-All* and FF-Replan-Single* use hand-made modifications of the original
learned rule-sets.
Objects

5+1

6+1

7+1

Planner

Success rate

Trial time (s)

Executed actions

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.02
1.00
0.67

7.1  0.0
26.7  2.7
7.0  0.9

12.0  0.10
13.1  0.9
13.6  1.1

SST (b=1)
SST (b=2)
UCT
PRADA
A-PRADA

0.00
0.00
0.38
0.71
0.82

> 1 day
2504.9  491.1
27.0  1.8
25.4  0.8

19.5  4.0
13.2  0.7
10.9  0.8

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.00
1.00
0.64

589.2  73.7
52.7  5.3

12.0  0.8
17.3  2.1

UCT
PRADA
A-PRADA

0.00
0.47
0.56

>4 h
66.4  3.9
77.5  8.3

13.6  0.9
14.4  2.5

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.00
0.42
0.56

2234.2  81.1
687.4  86.4

15.1  1.3
17.5  2.0

PRADA
A-PRADA

0.24
0.23

871.3  126.6
783.7  132.6

18.2  1.2
15.1  1.8

6.1.3 Reverse Tower
To explore the limits of UCT, PRADA and A-PRADA, we conducted a final series of
experiments where the task is to reverse towers of C cubes which requires at least 2C
actions (each cube needs to be grabbed and put somewhere at least once). Apart from the
long planning horizon, this is difficult due to the noise in the simulated world: towers can
become unstable and topple over with cubes falling off the table. To decrease this noise
slightly to obtain more reliable results, we forbid the robot to grab objects that are not clear
(i.e., below other objects). We set a limit of 50 executed actions on each trial. If thereafter
the reversed tower still is not built, the trial fails. The trial also fails if one of the required
objects falls off the table.
Table 5 presents our results. We cannot get SST with optimal planning horizon d = 10
to solve this problem even for five cubes. Although the space of possible actions is reduced
due to the mentioned restriction, SST has enormous runtimes. With b = 1, SST does not find
suitable actions (no leaves with the goal state) in several starting situations  the increased
planning horizon leads to a high probability of sampling at least one unfavorable outcome
for a required action. For b  2, a single tree traversal of SST takes more than a day. We
found UCT to also require large planning times in order to achieve a reasonable success
rate. Therefore, we set the planning horizons optimal for UCT. In worlds of 5 cubes, UCT
with optimal d = 10 has a success rate of about 40% while taking on average more than 40
35

fiLang & Toussaint

minutes in case of success. For 6 cubes, however, UCT with optimal d = 12 never succeeds
even when planning times exceed 4 hours. In contrast, we can afford an overestimating
horizon d = 20 for PRADA and A-PRADA. In worlds of 5 cubes, PRADA and A-PRADA
achieve success rates of 71% and 82% respectively in less than half a minute. A-PRADAs
average number of executed actions in case of success is almost optimal. In worlds of 6
cubes, the success rates of PRADA and A-PRADA are still about 50%, taking a bit more
than a minute on average in case of success. When their trials fail, this is most often due
to cubes falling off the table and not because they cannot find appropriate actions. Cubes
falling off the table is also a main reason why the success rates of PRADA and A-PRADA
drop to 23% and 24% respectively in worlds of 7 cubes when towers become rather unstable.
Planning times in successful trials, however, also increase to more than 13 minutes indicating
the limitations of these planning approaches. Nonetheless, the mean number of executed
actions in successful trials is still almost optimal for A-PRADA.
Overall, the Reverse tower experiments indicate that planning approaches using lookahead trees fail in tasks that require long planning horizons and can only be achieved by
very few plans. Given the huge action and state spaces in relational domains, the chances
that UCT simulates an episode with exactly the required actions and successor states are
very small. Planning approaches using approximate inference like PRADA and A-PRADA
have the crucial advantage that the stochasticity of actions does not affect their runtime
exponentially in the planning horizon. Of course, their search space of action-sequences still
is exponential in the planning horizon so that problems requiring long horizons are hard to
solve also for them. Our experiments show that by using the very simple, though principled
extension A-PRADA, we can gain significant performance improvements.
Our results also show that FF-Replan fails to provide good plans when using the original
learned rule-sets. This is surprising as the characteristics of the Reverse tower task seem
to favor FF-Replan in comparison to the other methods: there is a single conjunctive goal
structure and the number of good plans is very small while these plans require long horizons.
As the results of FF-Replan-All* and FF-Replan-Single* indicate, FF-Replan can achieve
a good performance with the adapted rule-sets that have been modified by hand to restrict
the number of possible actions in a state. While this constitutes a proof of concept of
FF-Replan, it shows the difficulty of applying FF-Replan with learned rule-sets.

6.1.4 Summary
Our results demonstrate that successful planning with learned world models (here in the
form of rules) may require to explicitly account for the quantification of predictive uncertainty. More concretely, methods applying look-ahead trees (UCT) and approximate
inference ((A-)PRADA) outperform FF-Replan on different tasks of varying difficulty. Furthermore, (A-)PRADA can solve planning tasks with long horizons, where UCT fails. Only
if one post-processes the learned rules by hand to clarify their application contexts and
the planning problem uses a conjunctive goal structure and requires few and long plans,
FF-Replan performs better than UCT and (A-)PRADA.
36

fiPlanning with Noisy Probabilistic Relational Rules

6.2 IPPC 2008 Benchmarks
In the second part of our evaluation, we apply our proposed approaches on the benchmarks
of the latest international probabilistic planning competition, the Uncertainty Part of the
International Planning Competition in 2008 (IPPC, 2008). The involved domains differ in
many characteristics, such as the number of actions, the required planning horizons and
the reward structures. As the competition results show, no planning algorithm performs
best everywhere. Thus, these benchmarks give an idea for what types of problems SST,
UCT and (A-)PRADA may be useful. We convert the PPDDL domain specifications into
NID rules along the lines described in Sec. B.1. The resulting rule-sets are used to run our
implementations of SST, UCT and (A-)PRADA on the benchmark problems.
Each of the seven benchmark domains consists of 15 problem instances. An instance
specifies a goal and a starting state. Instances vary not only in problem size, but also
in their reward structures (including action costs), so a direct comparison is not always
possible. In the competition, each instance was considered independently: planners were
given a restricted amount of time (10 minutes for problems 1-5 of each domain and 40
minutes for the others) to cover as many repetitions of the very same problem instance as
possible up to a maximum of a 100 trials. Trials differed in the random seeds resulting
in potentially different state transitions. The planners were evaluated with respect to the
number of trials ending in a goal state and the collected reward averaged over all trials.
Eight planners entered in the competition, including FF-Replan which was not an official participant. They are discussed with the related work in Sec. 2. For their results, which
are too voluminous to be presented here, we refer the reader to the website of the competition. Below, we provide a qualitative comparison of our methods to the results of these
planners. We do not attempt a direct quantitative comparison for several reasons. First,
the different hardware prevents timing comparisons. Second, competition participants have
frequently not been able to successfully cover trials of a single or all instances of a domain.
It is difficult to tell the reasons for this from the results tables: the planner might have
been overburdened by the problem, might have faced temporary technical problems with
the client-server architecture framework of the competition or could not cope with certain
PPDDL constructs which could have been rewritten in a simpler format.
Third and most importantly, we have not optimized our implementations to reuse previous planning efforts. Instead, we fully replan for each single action (within a trial and
across trials). The competition evaluation scheme puts replanners at a disadvantage (in
particular those which replan each single action). Instead of replanning, a good strategy for
the competition is to spend most planning time before starting the first trial and then reuse
the resulting insights (such as conditional plans and value functions) for all subsequent trials
with a minimum of additional planning. Indeed, this strategy has often been adopted as
many trial time results indicate. We acknowledge that this is a fair procedure to evaluate
planners which compute policies over large parts of the state-space before acting. We feel,
however, that this is counter to the idea of our approaches: UCT and (A-)PRADA are
meant for flexible planning with varying goals and different situations. Thus, what we are
interested in is the average time to compute good actions and successfully solve a problem
instance when there is no prior knowledge available.
37

fiLang & Toussaint

Table 6: Benchmarks of the IPPC 2008. The first column of a table specifies the problem
instance. Suc. is the success rate. The trial time and the number of executed
actions are given for the successful trials. Where applicable, the reward for all
trials is shown. All results are achieved with full replanning within a trial and
across trials.
(a) Search and Rescue
Planner

Suc. Trial Time (s)

Actions

(c) Blocksworld

Reward

SST
UCT
01
PRADA
A-PRADA

100
54
100
100

37.90.1
1.40.1
1.10.1
1.10.1

SST
UCT
PRADA
A-PRADA

100
56
100
100

220.20.1
4.10.3
1.60.1
1.60.1

9.80.2
12.20.6
12.90.7
12.80.4

SST
UCT
PRADA
A-PRADA

71
57
99
99

955.50.5
12.90.6
1.40.1
1.40.1

9.80.2 166285
13.60.6 68063
18.01.0 148088
17.91.1 148088

Actions

Reward

UCT
04 PRADA
A-PRADA

61
100
100

24.91.6
1.40.0
1.40.0

16.10.8 720057
11.90.4 146089
11.50.3 150087

SST
UCT
01
PRADA
A-PRADA

0
0
100
100





257.86.3 46.81.0
143.83.1 43.11.1



1.000.0
1.000.0

05

UCT
PRADA
A-PRADA

46
89
92

40.12.1
6.80.3
6.50.3

16.81.4 60064
21.80.9 124083
21.00.9 132081

02

PRADA
A-PRADA

100
100

285.27.8 46.21.3
215.84.2 39.60.9

20.000.0
20.000.0

06

UCT
PRADA
A-PRADA

39
83
84

71.75.6
10.10.9
10.00.9

19.51.3 41059
24.31.3 124090
23.71.2 124090

03

UCT
PRADA
A-PRADA

100
100
50

07

UCT
PRADA
A-PRADA

53
98
98

230.313.2
10.10.4
9.90.4

21.51.4 54062
18.50.8 147088
18.00.8 149087

04

PRADA
A-PRADA

28
60

959.035.5 76.13.2
519.215.3 72.02.4

0.30.5
0.60.1

UCT
PRADA
A-PRADA

34
59
59

332.924.1 21.711.5
20.20.8 30.41.7
19.90.8 29.91.7

05

08

UCT
PRADA
A-PRADA

54
61
2

9972776 37.93.5
345.48.5 68.41.6
528.638.8 38.00.0

606149
46524
41134

UCT
09 PRADA
A-PRADA

30
63
65

752.872.3
30.21.2
30.01.1

08

PRADA
A-PRADA

3
10

336188 87.02.3
157948 85.32.7

0.190.1
0.290.3

09

PRADA
A-PRADA

28
0

144925 85.91.5
 (1750.3)


136531
112630

10

PRADA
A-PRADA

21
21

97.910.2
92.19.8

26.82.8
26.72.8

18027
18027

11

PRADA
A-PRADA

17
18

151.712.3
154.111.9

302.5
30.22.6

25029
25029

12

PRADA
A-PRADA

38
21

210.872.1 30.110.5 636253
219.828.5 30.72.8 55655

02

03

9.20.2 144090
11.40.3 90070
10.50.4 146089
10.40.4 146089

Planner

156083
880100
146089
144090

36059
91082
91082

0
100
100
100


9.90.3
8.50.2
8.00.2

02

UCT
PRADA
A-PRADA

100
57
65

64.12.2 12.40.3
30.10.7
90.2
33.70.8 11.40.3

03

UCT
PRADA
A-PRADA

89
19
21

390.58.5 18.60.4
119.24.9 12.30.5
121.05.3 14.30.7

UCT
04 PRADA
A-PRADA

82
6
4

149719 26.00.5
2967143 17.51.1
244.243.6 15.52.8



0.80.0
0.60.0

03

10

57.03.3 21.51.8 -9.60.0

Suc. Trial Time (s)

1285.28.1 32.80.0 929.82.1
165.72.9 52.51.1 865.13.3
457.87.1 35.00.7 754.121.5

(e) Exploding Blocksworld
Planner

Actions

SST
UCT
PRADA
A-PRADA

01





17.80.4 23.00.7
18.40.5 22.30.8

Planner

26.42.4 36048
27.51.6 93080
27.51.6 101084

Suc. Trial Time (s)

Reward

0
0
53
63

PRADA

Suc. Trial Time (s)

(d) Boxworld

(b) Triangle-Tireworld
Planner

Actions

SST
UCT
01
PRADA
A-PRADA


6.90.2
6.40.2
6.10.2

38

Suc. Trial Time (s)
86071224
111.814.0
3.60.0
3.90.0

Actions

SST
UCT
01
PRADA
A-PRADA

5
3
62
61

02

PRADA
A-PRADA

28
29

11.90.3 14.40.5
12.70.2 13.20.5

03

PRADA
A-PRADA

36
30

14.30.3 12.60.6
16.80.3 12.50.5

04

PRADA
A-PRADA

27
26

30.31.2 14.80.5
14.91.1 15.20.5

05

PRADA
A-PRADA

100
100

06

PRADA
A-PRADA

51
61

128.52.9 16.90.7
97.55.3 17.30.8

07

PRADA
A-PRADA

14
72

125.06.9 15.30.4
154.85.5 17.61.0

5.50.1
5.50.1

9.60.6
9.30.4
8.60.8
8.40.8

6.60.1
6.60.1

fiPlanning with Noisy Probabilistic Relational Rules

Therefore, for each single problem instance we perform 100 trials with different random
seeds using full replanning. A trial is aborted if a goal state is not reached within some
maximum number of actions varying slightly for each benchmark (about 50 actions). We
present the success rates and the mean estimators of trial times, executed actions and
rewards with their standard deviations in Table 6 for the problem instances where at least
one trial was successfully covered in reasonable time.
Search and Rescue (Table 6(a)) is the only domain where SST (with branching factor
1) is able to find plans within reasonable time  with significantly larger runtimes than
UCT and (A-)PRADA. The success rates and the rewards indicate that PRADA and APRADA are superior to UCT and scale up to rather big problem instances. To give an idea
w.r.t. the IPPC evaluation scheme: UCT solves successfully 54 trials of the first instance
within 10 minutes with full replanning, while PRADA and A-PRADA solve all trials with
full replanning. In fact, despite of replanning each single action, PRADA and A-PRADA
show the same success rates as the best planners of the benchmark except for the very large
problem instances (within the competition, only the participants FSP-RBH and FSP-RDH
achieved comparably satisfactory results). We conjecture that the success of our methods is
due to that fact that this domain requires to account carefully for the outcome probabilities,
but does not involve very long planning horizons.
Triangle-Tireworld (Table 6(b)) is the only domain where UCT outperforms PRADA
and A-PRADA, although at a higher computational cost. The more depth-first-like style of
planning of UCT seems useful in this domain. To give an idea w.r.t. the IPPC evaluation
scheme: UCT performs 60 successful trials of the first instance within 10 minutes, while
PRADA and A-PRADA achieve 72 and 74 trials resp. using full replanning; but UCT solves
more trials in the more difficult instances. The required planning horizons increase quickly
with the problem instances. Our approaches cannot cope with the large problem instances,
which only three competition participants (RFF-BG, RFF-PG, HMDPP) could cover.
Our methods face problems when the required planning horizons are very large, while
the number of plans with non-zero probability is small. This becomes evident in the
Blocksworld benchmark (Table 6(c)). This domain is different from the robot manipulation environment of our first evaluation in Sec. 6.1. The latter is considerably more
stochastic and provides more actions in a given situation (e.g., we may grab objects within
a pile). Blocksworld is the only domain where our approaches are inferior to FF-Replan. To
give an idea w.r.t. the IPPC evaluation scheme: UCT does not perform a single successful
trial of the first instance within 10 minutes, while PRADA and A-PRADA achieve 16 and
17 trials resp. using full replanning.
In the Boxworld domain (Table 6(d)), our approaches can exploit the fact that the
delivery of boxes is (almost) independent of the delivery of other boxes (in most problem
instances this is further helped by the intermediate rewards for delivered boxes). In contrast
to UCT, PRADA and A-PRADA scale up to relatively large problem instances. PRADA
and A-PRADA solve all 100 trials of the first problem instance, requiring on average 4.3
min and 2.4 min resp. with full replanning. Only two competition participants solved
trials successfully in this domain (RFF-BG and RFF-PG). To give an idea w.r.t. the IPPC
evaluation scheme: UCT does not perform a single successful trial within 10 minutes, while
PRADA completes 2 and A-PRADA 4 trials. This small number can be explained by the
large plan lengths where each single action is computed with full replanning.
39

fiLang & Toussaint

Finally, in the Exploding Blocksworld domain (Table 6(e)) PRADA and A-PRADA
perform better or as good as the competition participants. To give an idea w.r.t. the IPPC
evaluation scheme: UCT achieves only a single successful trial within 10 minutes, while
PRADA and A-PRADA complete 56 and 61 trials resp..
We did not perform any experiments in either the SysAdmin or the Schedule domain. Their PPDDL specifications cannot be converted into NID rules due to the involved
universal effects. In contrast, this has been possible for the Boxworld domain despite of
the universal effects there: in the Boxworld problem instances, the universally quantified
variables always refer to exactly one object which we exploit for conversion to NID rules.
(Note that this can be understood as a trick to implement deictic references in PPDDL
by means of universal effects. The according action operator, however, has odd semantics:
boxes could end up in two different cities at the same time.) Furthermore, we ignored the
Rectangle-Tireworld domain, which together with the Triangle-Tireworld domain makes
up the 2-Tireworlds benchmark, as its problem instances have faulty goal descriptions: They
should include not(dead) (this has not been critical to name a winner in the competition as
personally communicated by Olivier Buffet).
6.2.1 Summary
The majority of the PPDDL descriptions of the IPPC benchmarks can be converted into
NID rules, indicating the broad spectrum of planning problems which can be covered by
NID rules. Our results demonstrate that our approaches perform comparably to or better
than state-of-the-art planners on many traditional hand-crafted planning problems. This
hints at the generality of our methods for probabilistic planning beyond the type of robotic
manipulation domains considered in Sec. 6.1. Our methods perform particularly well in
domains where outcome probabilities need to be carefully accounted for. They face problems
when the required planning horizons are very large, while the number of plans with non-zero
probability is small; this can be avoided by intermediate rewards.

7. Discussion
We have presented two approaches for planning with probabilistic relational rules in grounded
domains. Our methods are designed to work on learned rules which provide approximate
partial models of noisy worlds. Our first approach is an adaptation of the UCT algorithm
which samples look-ahead trees to cope with action stochasticity. Our second approach,
called PRADA, models the uncertainty over states explicitly in terms of beliefs and employs
approximate inference in graphical models for planning. When we combine our planning
algorithms with an existing rule learning algorithm, an intelligent agent can (i) learn a
compact model of the dynamics of a complex noisy environment and (ii) quickly derive appropriate actions for varying goals. Results in a complex simulated robotics domain show
that our methods outperform the state-of-the-art planner FF-Replan on a number of different planning tasks. In contrast to FF-Replan, our methods reason over the probabilities
of action outcomes. This is necessary if the world dynamics are noisy and only partial and
approximate world models are available.
However, our planners also perform remarkably well on many traditional probabilistic
planning problems. This is demonstrated by our results on IPPC benchmarks, where we
40

fiPlanning with Noisy Probabilistic Relational Rules

have shown that PPDDL descriptions can be converted to a large extent to the kind of rules
our planners use. This hints at the general-purpose character of particularly PRADA and
the potential benefits of its techniques for probabilistic planning. For instance, our methods
can be expected to perform similarly well in large propositional MDPs which do not exhibit
a relational structure.
So far, our planning approaches deal in reasonable time with problems containing up
to 10-15 objects (implying billions of world states) and requiring planning horizons of up
to 15-20 time-steps. Nonetheless, our approaches are still limited in that they rely on
reasoning in the grounded representation. If very many objects need to be represented or if
the representation language gets very rich, our approaches need to be combined with other
methods that reduce state and action space complexity (Lang & Toussaint, 2009b).
7.1 Outlook
In its current form, the approximate inference procedure of PRADA relies on the specific
compact DBNs compiled from rules. The development of similar factored frontier filters
for arbitrary DBNs, e.g. derived from more general PPDDL descriptions, is promising.
Similarly, the adaptation of PRADAs factored frontier techniques into existing probabilistic
planners is worth of investigation.
Using probabilistic relational rules for backward planning appears appealing. It is
straightforward to learn NID rules that regress actions by providing reversed triples (s0 , a, s)
to the rule learning algorithm, stating the predecessor state s for a state s0 if an action a has
been applied before. Backward planning, which can be combined with forward planning,
has received a lot of attention in classical planning and may be fruitful for both planning
with look-ahead trees as well as planning using approximate inference. By means of propagating backwards through our DBNs, one may ultimately derive algorithms that calculate
posteriors over actions, leading to true planning by inference (instead of sampling actions).
An important direction for improving our PRADA algorithm is to make it adapt its
action-sequence sampling strategy to the experience of previous samples. We have introduced a very simple extension, A-PRADA, to achieve this, but more sophisticated methods
are conceivable. Learning rule-sets online and exploiting them immediately by our planning method is also an important direction of future research in order to enable acting in
the real world, where we want to behave effectively right from the start. Improving the
rule framework for more efficient and effective planning is another interesting issue. For
instance, instead of using a noisy default rule, one may use mixture models to deal with
actions with several (non-unique) covering rules, or in general use parallel rules that work
on different hierarchical levels or different aspects of the underlying system.

Acknowledgments
We thank the anonymous reviewers for their careful and thorough comments which have
greatly improved this paper. We thank Sungwook Yoon for providing us an implementation
of FF-Replan. We thank Olivier Buffet for answering our questions on the probabilistic
planning competition 2008. This work was supported by the German Research Foundation
(DFG), Emmy Noether fellowship TO 409/1-3.
41

fiLang & Toussaint

Appendix A. Proof of Proposition 1
Proposition 1 (Sec. 5.3) The set of action sequences PRADA samples with non-zero
probability is a super-set of the ones of SST and UCT.
Proof: Let a0:T 1 be an action sequence that was sampled by SST (or UCT). Thus,
there exists a state sequence s0:T and a rule sequence r0:T 1 such that in every state st
(t < T ), action at has a unique covering rule rt that predicts the successor state st+1 with
probability pt > 0. For, if pt = 0, then st+1 would never be sampled by SST (or UCT).
We have to show that t, 0  t < T : P (st | a0:t1 , s0 ) > 0. If this is the case then
t
Psample (at ) > 0 as at has the unique covering rule rt in st and at will eventually be sampled.
P (s0 ) = 1 > 0 is obvious. Now assume P (st | a0:t1 , s0 ) > 0. If we execute at , we will
get P (st+1 | a0:t , s0 )  pt P (st | a0:t1 , s0 ) > 0. The posterior P (st+1 | a0:t , s0 ) can be greater
(first inequality) due to persistence or to previous states having non-zero probability that
also lead to st+1 given at .
The set of action sequences PRADA samples is larger than that of SST (or UCT) as
SST (or UCT) refuses to model the noise outcomes of rules. Assume an action a and state
s to be the only state where a has a unique covering rule. If an episode to s can only be
simulated by means of rule predictions with the noise outcome, this action will never be
sampled by SST (or UCT) (as the required states are never sampled). In contrast, PRADA
also models the effects of the noise outcome by giving very low probability to all possible
successor states with the heuristic described above. 

Appendix B. Relation between NID rules and PPDDL
We use NID rules (Sec. 3.2) as relational model of the transition dynamics of probabilistic actions. Besides allowing for negative literals in the preconditions, NID rules extend
probabilistic STRIPS operators (Kushmerick et al., 1995; Blum & Langford, 1999) by two
special constructs, namely deictic references and noise outcomes, which are crucial for learning compact rule-sets. An alternative language to specify probabilistic relational planning
problems used by the International Probabilistic Planning Competitions (IPPC, 2008) is
the probabilistic planning domain definition language (PPDDL) (Younes & Littman, 2004).
PPDDL is a probabilistic extension of a subset of PDDL, derived from the deterministic
action description language (ADL). ADL, in turn, introduced universal and conditional
effects and negative precondition literals into the (deterministic) STRIPS representation.
Thus, PPDDL allows for the usage of syntactic constructs which are beyond the expressive
power of NID rules; however, many PPDDL descriptions can be converted into NID rules.
Before taking a closer look at how to convert PPDDL and NID rule representations
into each other, we clarify what is meant by action in each of the formalisms, giving an
intuition of the line of thinking when using either of these. We understand by abstract
action an abstract action predicate, e.g. pickup(X). Intuitively, this defines a certain type
of action. The stochastic state transitions according to an abstract action can be specified by
both abstract NID rules as well as abstract PPDDL action operators (also called schemata).
Typically, several different abstract NID rules model the same abstract action, specifying
state transitions in different contexts. In contrast, usually only one abstract PPDDL action
42

fiPlanning with Noisy Probabilistic Relational Rules

operator is used to model an abstract action: context-dependent effects are modeled by
means of conditional and universal effects.
To make predictions in a specific situation for a concrete action (a grounded action
predicate such as pickup(greenCube)), the strategy within the NID rule framework is to
ground the set of abstract NID rules and examine which ground rules cover this state-action
pair. If there is exactly one such ground rule, it is chosen for prediction. If there is no such
rule or if there is more than one (the contexts of NID rules do not have to be mutually
exclusive), one chooses the noisy default rule, essentially saying that one does not know
what will happen (other strategies are conceivable, but not pursued here). In contrast, as
there is usually exactly one operator per abstract action in PPDDL domains, there is no
need of the concept of operator uniqueness and to distinguish between ground actions and
operators.
B.1 Converting PPDDL to NID rules
In the following, we discuss how to convert PPDDL features into a NID rule representation.
While it may be impossible to convert a PPDDL action operator into a single NID rule,
one may often translate it into a set of rules with at most a polynomial increase in the size
of representation. Table 7 provides an example of a converted PPDDL action operator of
the IPPC domain Exploding Blocksworld. As NID rules support many, but not all of the
features a sophisticated domain description language such as PPDDL provides, using rules
will not lead to compact representations in all possible domains. Our experiments, however,
show that the dynamics of many interesting planning domains can be specified compactly.
Furthermore, additional expressive power in rule contexts can be gained by using derived
predicates which allow to bring in various kinds of logical formulas such as quantification.
Conditional Effects A conditional effect in a PPDDL operator takes the form when C
then E. It can be accounted for by two NID rules: the first rule adds C to its context and
E to its outcomes, while the second adds C to its context and ignores E.
Universal Effects PPDDL allows to define universal effects. These specify effects for all
objects that meet some preconditions. An example is the reboot action of the SysAdmin
domain of the IPPC 2008 competition: it specifies that every computer other than the one
rebooted can independently go down with probability 0.2 if it is connected to a computer
that is already down. This cannot be expressed in a NID rule framework. While we can
refer to objects other than the action arguments via deictic references, we require these
deictic references to be unique. For the reboot action, we would need a unique way to refer
to each other computer which cannot be achieved without significant modifications (for
example, such as enumerating the other computers via separate predicates).
Disjunctive Preconditions and Quantification PPDDL operators allow for disjunctive preconditions, including implications. For instance, the Search-and-rescue domain
of the IPPC 2008 competition defines an action operator goto(X) with the precondition
(X 6= base)  humanAlive(). A disjunction A  B ( A  B) can be accounted for
by either using two NID rules, with the first rule having A in the context and the second
rule having A  B. Alternatively, one may introduce a derived predicate C  A  B. In
general, the trick of derived predicates allows to overcome syntactical limitations of NID
43

fiLang & Toussaint

Table 7: Example for converting a PPDDL action operator into NID rules. The putDownoperator of the IPPC benchmark domain Exploding Blocksworld (a) contains a
conditional effect which can be accounted for by two NID rules which either exclude
(b) or include (c) this condition in their context.
( : action putDown

(a)

: parameters (?b  block)
: precondition (and (holding ?b) (noDestroyedT able))
: ef f ect (and (emptyhand) (onT able ?b) (not (holding ?b))
(probabilistic 2/5 (when (noDetonated ?b) (and (not (noDestroyedT able)) (not (noDetonated?b))))))
)
(b)
putDown(X) : block(X), holding(X), noDestroyedT able(), noDetonated(X)

1.0 : emptyhand(X), onT able(X), holding(X)

(c)
putDown(X) : block(X), holding(X), noDestroyedT able(), noDetonated(X)

0.6 : emptyhand(X), onT able(X), holding(X)

0.4 : emptyhand(X), onT able(X), holding(X), noDestroyedT able(), noDetonated(X)

rules and bring in various kinds of logical formulas such as quantifications. As discussed by
Pasula et al. (2007), derived predicates are an important prerequisite to being able to learn
compact and accurate rules.
Types Terms may be typed in PPDDL, e.g. driveT o(C  city). Typing of objects and
variables in predicates and functions can be achieved in NID rules by the usage of typing
predicates within the context, e.g. using an additional predicate city(C).
State Transition Rewards In PPDDL, one can encode Markovian rewards associated
with state transitions (including action costs as negative rewards) using fluents and update
rules in action effects. One can achieve this in NID rules by associating rewards with the
outcomes of rules.
B.2 Converting NID rules to PPDDL
We show in the following that the way NID rules are used in SST, UCT and PRADA at
planning time can be handled via at most a polynomial blowup in representational size.
The basic building blocks of a NID rule, i.e. the context as well as the outcomes, transfer
one-to-one to PPDDL action operators. The deictic references, the uniqueness requirement
of covering rules and the noise outcome need special attention.
Deictic References Deictic references in NID rules allow to refer to objects which are
not action arguments. In PPDDL, one can refer to such objects by means of universal
conditional effects. There is an important restriction, however: a deictic reference needs to
pick out a single unique object in order to apply. If it picks out none or many, the rule fails
to apply. There are two ways to ensure this uniqueness requirement within PPDDL. First,
44

fiPlanning with Noisy Probabilistic Relational Rules

if allowing quantified preconditions, an explicit uniqueness precondition for each deictic
reference D can be introduced. Using universal quantification, it constrains all objects
satisfying the preconditions D of D to be identical, i.e., X, Y : D (X, )  D (Y, ) 
X = Y , where  are some other variables. Alternatively, uniqueness of deictic references
can be achieved by a careful planning problem specification, which however cannot be
guaranteed when learning rules.
Uniqueness of covering rules The contexts of NID rules do not have to be mutually
exclusive. When we want to use a rule for prediction (as in planning), we need to ensure that
it uniquely covers the given state-action pair. The procedural evaluation process for NID
rules can be encoded declaratively in PPDDL using modified conditions which explicitly
negate the contexts of competing rules. For instance, if there are three NID rules with
potentially overlapping contexts A, B, and C (propositional for simplicity), the PPDDL
action operator may define four conditions: c1 = {A  B  C}, c2 = {A  B  C},
c3 = {A  B  C}, c4 = {(A  B  C)  (A  B)  (A  C)  (B  C)}. Conditions c1 ,
c2 and c3 test for uniqueness of the corresponding NID rules and subsume their outcomes.
Condition c4 tests for non-uniqueness (either no covering rule or multiple covering rules)
and models potential changes as noise, analogous to the situations in a NID rule context in
which the noisy default rule would be used.
Noise outcome The noise outcome of a NID rule subsumes seldom or utterly complex
outcomes. It relaxes the frame assumption: even not explicitly stated things may change
with a certain probability. This comes at the price of the difficulty to ensure a well-defined
successor state distribution P (s0 | s, a). In contrast, PPDDL needs to explicitly specify
everything that might change. This may be an important reason why it is difficult to come
up with an effective learning algorithm for PPDDL.
While in principle PPDDL does not provide for a noise outcome, the way our approaches
account for it in planning can be encoded in PPDDL. We either treat the noise outcome
as having no effects (in SST and UCT; basically a noop operator then) which is trivially
translated to PPDDL; or we consider the probability of each state attribute to change
independently (in PRADA) which can be encoded in PPDDL with independent universal
probabilistic effects.
The noise outcome allows to always make predictions for an arbitrary action: if there
are no or multiple covering rules, we may use the (albeit not very informative) prediction
of the default rule. Such cases can be dealt with in PPDDL action operators using explicit
conditions as described in the previous paragraph.

References
Blum, A., & Langford, J. (1999). Probabilistic planning in the graphplan framework. In
Proc. of the Fifth European Conference on Planning (ECP), pp. 319332.
Botvinick, M. M., & An, J. (2009). Goal-directed decision making in prefrontal cortex:
a computational framework. In Advances in Neural Information Processing Systems
(NIPS), pp. 169176.
45

fiLang & Toussaint

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 194.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for first-order
MDPs. In Proc. of the Int. Conf. on Artificial Intelligence (IJCAI), pp. 690700.
Buffet, O., & Aberdeen, D. (2009). The factored policy-gradient planner. Artificial Intelligence Journal, 173 (5-6), 722747.
Cooper, G. (1988). A method for using belief networks as influence diagrams. In Proc. of
the Fourth Workshop on Uncertainty in Artificial Intelligence, pp. 5563.
Croonenborghs, T., Ramon, J., Blockeel, H., & Bruynooghe, M. (2007). Online learning and
exploiting relational models in reinforcement learning. In Proc. of the Int. Conf. on
Artificial Intelligence (IJCAI), pp. 726731.
Domshlak, C., & Hoffmann, J. (2007). Probabilistic planning via heuristic forward search
and weighted model counting. Journal of Artificial Intelligence Research, 30, 565620.
Driessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels and Gaussian processes for
relational reinforcement learning. Machine Learning, 64 (1-3), 91119.
Dzeroski, S., de Raedt, L., & Driessens, K. (2001). Relational reinforcement learning.
Machine Learning, 43, 752.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration with a policy language
bias: solving relational markov decision processes. Journal of Artificial Intelligence
Research, 25 (1), 75118.
Gardiol, N. H., & Kaelbling, L. P. (2003). Envelope-based planning in relational MDPs. In
Proc. of the Conf. on Neural Information Processing Systems (NIPS).
Gardiol, N. H., & Kaelbling, L. P. (2007). Action-space partitioning for planning. In Proc. of
the AAAI Conf. on Artificial Intelligence (AAAI), pp. 980986.
Gardiol, N. H., & Kaelbling, L. P. (2008). Adaptive envelope MDPs for relational
equivalence-based planning. Tech. rep. MIT-CSAIL-TR-2008-050, MIT CS & AI Lab,
Cambridge, MA.
Gelly, S., & Silver, D. (2007). Combining online and offline knowledge in UCT. In Proc. of
the Int. Conf. on Machine Learning (ICML), pp. 273280.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order rgeression in inductive policy
selection. In Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI), pp.
217225.
Grush, R. (2004). Conscious thought as simulation of behaviour and perception. Behaviorial
and brain sciences, 27, 377442.
46

fiPlanning with Noisy Probabilistic Relational Rules

Halbritter, F., & Geibel, P. (2007). Learning models of relational MDPs using graph kernels.
In Proc. of the Mexican Conference on Artificial Intelligence (MICAI), pp. 409419.
Hesslow, G. (2002). Conscious thought as simulation of behaviour and perception. Trends
in Cognitive Science, 6 (6), 242247.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Holldobler, S., & Skvortsova, O. (2004). A logic-based approach to dynamic programming.
In AAAI-Workshop: Learning and planning in MDPs, pp. 3136.
IPPC

(2008).
Sixth International Planning Competition,
http://ippc-2008.loria.fr/wiki/index.php/Main Page.

Uncertainty

Part..

Jensen, F. (1996). An introduction to Bayesian networks. Springer Verlag, New York.
Joshi, S., Kersting, K., & Khardon, R. (2009). Generalized first-order decision diagrams for
first-order MDPs. In Proc. of the Int. Conf. on Artificial Intelligence (IJCAI), pp.
19161921.
Karabaev, E., & Skvortsova, O. (2005). A heuristic search algorithm for solving first-order
MDPs. In Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI), pp.
292299.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). A sparse sampling algorithm for nearoptimal planning in large Markov decision processes. Machine Learning, 49 (2-3),
193208.
Kersting, K., & Driessens, K. (2008). Nonparametric policy gradients: A unified treatment of propositional and relational domains. In Proc. of the Int. Conf. on Machine
Learning (ICML), pp. 456463.
Kersting, K., van Otterlo, M., & de Raedt, L. (2004). Bellman goes relational. In Proc. of
the Int. Conf. on Machine Learning (ICML), pp. 465472.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. In Proc. of the
European Conf. on Machine Learning (ECML), pp. 837844.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 78 (1-2), 239286.
Kuter, U., Nau, D. S., Reisner, E., & Goldman, R. P. (2008). Using classical planners to
solve nondeterministic planning problems. In Proc. of the Int. Conf. on Automated
Planning and Scheduling (ICAPS), pp. 190197.
Lang, T., & Toussaint, M. (2009a). Approximate inference for planning in stochastic relational worlds. In Proc. of the Int. Conf. on Machine Learning (ICML), pp. 585592.
Lang, T., & Toussaint, M. (2009b). Relevance grounding for planning in relational domains.
In Proc. of the European Conf. on Machine Learning (ECML), pp. 736751.
47

fiLang & Toussaint

Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. In ICAPS-Workshop
International Planning Competition: Past, Present and Future.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1997). The computational complexity of
probabilistic planning. Journal of Artificial Intelligence Research, 9, 136.
Murphy, K. P. (2002). Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D. thesis, UC Berkeley.
Murphy, K. P., & Weiss, Y. (2001). The factored frontier algorithm for approximate inference in DBNs. In Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI),
pp. 378385.
Pasula, H. M., Zettlemoyer, L. S., & Kaelbling, L. P. (2007). Learning symbolic models of
stochastic domains. Journal of Artificial Intelligence Research, 29, 309352.
Poon, H., & Domingos, P. (2007). Sound and efficient inference with probabilistic and
deterministic dependencies. In Proc. of the AAAI Conf. on Artificial Intelligence
(AAAI).
Sanner, S., & Boutilier, C. (2007). Approximate solution techniques for factored first-order
MDPs. In Proc. of the Int. Conf. on Automated Planning and Scheduling (ICAPS),
pp. 288295.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques for first-order MDPs.
Artificial Intelligence, 173 (5-6), 748788.
Shachter, R. (1988). Probabilistic inference and influence diagrams. Operations Research,
36, 589605.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT
Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Aggregation for generating
policies in MDPs. In To appear in Proc. of Int. Conf. on Autonomous Agents and
Multiagent Systems.
Toussaint, M., & Storkey, A. (2006). Probabilistic inference for solving discrete and continuous state Markov decision processes. In Proc. of the Int. Conf. on Machine Learning
(ICML), pp. 945952.
Toussaint, M., Storkey, A., & Harmeling, S. (2010). Expectation-maximization methods
for solving (PO)MDPs and optimal control problems. In Chiappa, S., & Barber, D.
(Eds.), Inference and Learning in Dynamic Models. Cambridge University Press.
van Otterlo, M. (2009). The Logic of Adaptive Behavior. IOS Press, Amsterdam.
Walsh, T. J. (2010). Efficient learning of relational models for sequential decision making.
Ph.D. thesis, Rutgers, The State University of New Jersey, New Brunswick, NJ.
48

fiPlanning with Noisy Probabilistic Relational Rules

Wang, C., Joshi, S., & Khardon, R. (2008). First order decision diagrams for relational
MDPs. Journal of Artificial Intelligence Research, 31, 431472.
Weld, D. S. (1999). Recent advances in AI planning. AI Magazine, 20 (2), 93123.
Wu, J.-H., Kalyanam, R., & Givan, R. (2008). Stochastic enforced hill-climbing. In Proc. of
the Int. Conf. on Automated Planning and Scheduling (ICAPS), pp. 396403.
Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: A baseline for probabilistic planning.
In Proc. of the Int. Conf. on Automated Planning and Scheduling (ICAPS), pp. 352
359.
Yoon, S. W., Fern, A., Givan, R., & Kambhampati, S. (2008). Probabilistic planning via
determinization in hindsight. In Proc. of the AAAI Conf. on Artificial Intelligence
(AAAI), pp. 10101016.
Younes, H. L., & Littman, M. L. (2004). PPDDL1.0: An extension to PDDL for expressing
planning domains with probabilistic effects. Tech. rep., Carnegie Mellon University.

49

fiJournal of Artificial Intelligence Research 39 (2010) 689743

Submitted 05/10; published 12/10

Best-First Heuristic Search for Multicore Machines
Ethan Burns
Sofia Lemons
Wheeler Ruml

EABURNS AT CS . UNH . EDU
SOFIA . LEMONS AT CS UNH . EDU
RUML AT CS . UNH . EDU

Department of Computer Science
University of New Hampshire
Durham, NH 03824 USA

Rong Zhou

RZHOU AT PARC . COM

Embedded Reasoning Area
Palo Alto Research Center
Palo Alto, CA 94304 USA

Abstract
To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a
shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state
space and to detect duplicate states without requiring frequent locking. PBNF allows speculative
expansions when necessary to keep threads busy. We identify and fix potential livelock conditions
in our approach, proving its correctness using temporal logic. Our approach is general, allowing it
to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS
planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that
A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than
improved versions of previous parallel search proposals.

1. Introduction
It is widely anticipated that future microprocessors will not have faster clock rates, but instead
more computing cores per chip. Tasks for which there do not exist effective parallel algorithms
will suffer a slowdown relative to total system performance. In artificial intelligence, heuristic
search is a fundamental and widely-used problem solving framework. In this paper, we compare
different approaches for parallelizing best-first search, a popular method underlying algorithms such
as Dijkstras algorithm and A* (Hart, Nilsson, & Raphael, 1968).
In best-first search, two sets of nodes are maintained: open and closed. Open contains the search
frontier: nodes that have been generated but not yet expanded. In A*, open nodes are sorted by their
f value, the estimated lowest cost for a solution path going through that node. Open is typically
implemented using a priority queue. Closed contains all previously generated nodes, allowing the
search to detect states that can be reached via multiple paths in the search space and avoid expanding
them multiple times. The closed list is typically implemented as a hash table. The central challenge
in parallelizing best-first search is avoiding contention between threads when accessing the open
and closed lists. We look at a variety of methods for parallelizing best-first search, focusing on
algorithms which are based on two techniques: parallel structured duplicate detection and parallel
retracting A*.
c
2010
AI Access Foundation. All rights reserved.

fiB URNS , L EMONS , RUML , & Z HOU

Parallel structured duplicate detection (PSDD) was originally developed by Zhou and Hansen
(2007) for parallel breadth-first search, in order to reduce contention on shared data structures by
allowing threads to enjoy periods of synchronization-free search. PSDD requires the user to supply
an abstraction function that maps multiple states, called an nblock, to a single abstract state. We
present a new algorithm based on PSDD called Parallel Best-N Block-First (PBNF1 ). Unlike PSDD,
PBNF extends easily to domains with non-uniform and non-integer move costs and inadmissible
heuristics. Using PBNF in an infinite search space can give rise to livelock, where threads continue
to search but a goal is never expanded. We will discuss how this condition can be avoided in
PBNF using a method we call hot nblocks, as well as our use of bounded model checking to test its
effectiveness. In addition, we provide a proof of correctness for the PBNF framework, showing its
liveness and completeness in the general case.
Parallel retracting A* (PRA*) was created by Evett, Hendler, Mahanti, and Nau (1995). PRA*
distributes the search space among threads by using a hash of a nodes state. In PRA*, duplicate
detection is performed locally; communication with peers is only required to transfer generated
search-nodes to their home processor. PRA* is sensitive to the choice of hashing function used
to distribute the search space. We show a new hashing function, based on the same state space
abstraction used in PSDD, that can give PRA* significantly better performance in some domains.
Additionally, we show that the communication cost incurred in a naive implementation of PRA* can
be prohibitively expensive. Kishimoto, Fukunaga, and Botea (2009) present a method that helps to
alleviate the cost of communication in PRA* by using asynchronous message passing primitives.
We evaluate PRA* (and its variants), PBNF and other algorithms empirically using dual quadcore Intel machines. We study their behavior on three popular search domains: STRIPS planning,
grid pathfinding, and the venerable sliding tile puzzle. Our empirical results show that the simplest
parallel search algorithms are easily outperformed by a serial A* search even when they are run
with eight threads. The results also indicate that adding abstraction to the PRA* algorithm can give
a larger increase in performance than simply using asynchronous communication, although using
both of these modifications together may outperform either one used on its own. Overall, the PBNF
algorithm often gives the best performance.
In addition to finding optimal solutions, we show how to adapt several of the algorithms to
bounded suboptimal search, quickly finding w -admissible solutions (with cost within a factor of w
of optimal). We provide new pruning criteria for parallel suboptimal search and prove that algorithms using them retain w -admissibility. Our results show that, for sufficiently difficult problems,
parallel search may significantly outperform serial weighted A* search. We also found that the
advantage of parallel suboptimal search increases with problem difficulty.
Finally, we demonstrate how some parallel searches, such as PBNF and PRA*, lead naturally
to effective anytime algorithms. We also evaluate other obvious parallel anytime search strategies
such as running multiple weighted A* searches in parallel with different weights. We show that the
parallel anytime searches are able to find better solutions faster than their serial counterparts and
they are also able to converge more quickly on optimal solutions.

1. Peanut Butter N (marshmallow) Fluff, also known as a fluffernutter, is a well-known childrens sandwich in the
USA.

690

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

2. Previous Approaches
There has been much previous work in parallel search. We will briefly summarize selected proposals
before turning to the foundation of our work, the PRA* and PSDD algorithms.
2.1 Depth- and Breadth-first Approaches
Early work on parallel heuristic search investigated approaches based on depth-first search. Two
examples are distributed tree search (Ferguson & Korf, 1988), and parallel window search (Powley
& Korf, 1991).
Distributed tree search begins with a single thread, which is given the initial state to expand.
Each time a node is generated an unused thread is assigned to the node. The threads are allocated
down the tree in a depth-first manner until there are no more free threads to assign. When this occurs,
each thread will continue searching its own children with a depth-first search. When the solution
for a subtree is found it is passed up the tree to the parent thread and the child thread becomes free
to be re-allocated elsewhere in the tree. Parent threads go to sleep while their children search, only
waking once the children terminate, passing solutions upward to their parents recursively. Because
it does not keep a closed list, depth-first search cannot detect duplicate states and does not give
good search performance on domains with many duplicate states, such as grid pathfinding and some
planning domains.
Parallel window search parallelizes the iterative deepening A* (IDA*, see Korf, 1985) algorithm. In parallel window search, each thread is assigned a cost-bound and will perform a costbounded depth-first search of the search space. The problem with this approach is that IDA* will
spend at least half of its search time on the final iteration and since every iteration is still performed
in only a single thread, the search will be limited by the speed of a single thread. In addition, nonuniform costs can foil iterative deepening, because there may not be a good way to choose new
upper-bounds that give the search a geometric growth.
Holzmann and Bosnacki (2007) have been able to successfully parallelize depth-first search for
model checking. The authors are able to demonstrate that their technique that distributes nodes
based on search depth was able to achieve near linear speedup in the domain of model checking.
Other research has used graphics processing units (GPUs) to parallelize breadth-first search for
use in two-player games (Edelkamp & Sulewski, 2010). In the following sections we describe
algorithms with the intent of parallelizing best-first search.
2.2 Simple Parallel Best-first Search
The simplest approach to parallel best-first search is to have open and closed lists that are shared
among all threads (Kumar, Ramesh, & Rao, 1988). To maintain consistency of these data structures,
mutual exclusion locks (mutexes) need to be used to ensure that a single thread accesses the data
structure at a time. We call this search parallel A*. Since each node that is expanded is taken
from the open list and each node that is generated is looked up in the closed list by every thread, this
approach requires a lot of synchronization overhead to ensure the consistency of its data structures.
As we see in Section 4.3, this naive approach performs worse than serial A*.
There has been much work on designing complex data structures that retain correctness under
concurrent access. The idea behind these special wait-free data structures is that many threads
can use portions of the data structure concurrently without interfering with one another. Most of
691

fiB URNS , L EMONS , RUML , & Z HOU

these approaches use a special compare-and-swap primitive to ensure that, while modifying the
structure, it does not get modified by another thread. We implemented a simple parallel A* search,
which we call lock-free parallel A*, in which all threads access a single shared, concurrent priority
queue and concurrent hash table for the open and closed lists, respectively. We implemented the
concurrent priority queue data structure of Sundell and Tsigas (2005). For the closed list, we used
a concurrent hash table which is implemented as an array of buckets, each of which is a concurrent
ordered list as developed by Harris (2001). These lock-free data structures used to implement LPA*
require a special lock-free memory manager that uses reference counting and a compare-and-swap
based stack to implement a free list (Valois, 1995). We will see that, even with these sophistocated
structures, a straightforward parallel implementation of A* does not give competitive performance.
One way of avoiding contention altogether is to allow one thread to handle synchronization of
the work done by the other threads. K -Best-First Search (Felner, Kraus, & Korf, 2003) expands the
best k nodes at once, each of which can be handled by a different thread. In our implementation, a
master thread takes the k best nodes from open and gives one to each worker. The workers expand
their nodes and the master checks the children for duplicates and inserts them into the open list.
This allows open and closed to be used without locking, however, in order to adhere to a strict
k -best-first ordering this approach requires the master thread to wait for all workers to finish their
expansions before handing out new nodes. In the domains used in this paper, where node expansion
is not particularly slow, we show that this method does not scale well.
One way to reduce contention during search is to access the closed list less frequently. A technique called delayed duplicate detection (DDD) (Korf, 2003), originally developed for externalmemory search, can be used to temporarily delay access to the a closed list. While several variations have been proposed, the basic principle behind DDD is that generated nodes are added to
a single list until a certain condition is met (a depth level is fully expanded, some maximum list
size is reached (Stern & Dill, 1998), etc.) Once this condition has been met, the list is sorted to
draw duplicate nodes together. All nodes in the list are then checked against the closed list, with
only the best version being kept and inserted onto the open list. The initial DDD algorithm used a
breadth-first frontier search and therefore only the previous depth-layer was required for duplicate
detection. A parallel version was later presented by Niewiadomski, Amaral, and Holte (2006a),
which split each depth layer into sections and maintained separate input and output lists for each.
These were later merged in order to perform the usual sorting and duplicate detection methods.
This large synchronization step, however, will incur costs similar to KBFS. It also depends upon
an expensive workload distribution scheme to ensure that all processors have work to do, decreasing the bottleneck effect of nodes being distributed unevenly, but further increasing the algorithms
overhead. A later parallel best-first frontier search based on DDD was presented (Niewiadomski,
Amaral, & Holte, 2006b), but incurs even further overhead by requiring synchronization between
all threads to maintain a strict best-first ordering.
Jabbar and Edelkamp (2006) present an algorithm called parallel external A* (PEA*) that uses
distributed computing nodes and external memory to perform a best-first search. PEA* splits the
search space into a set of buckets that each contain nodes with the same g and h values. The
algorithm performs a best-first search by exploring all the buckets with the lowest f value beginning
with the one with the lowest g. A master node manages requests to distribute portions of the current
bucket to various processing nodes so that expanding a single bucket can be performed in parallel.
To avoid contention, PEA* relies on the operating system to synchronize access to files that are
shared among all of the nodes. Jabbar and Edelkamp used the PEA* algorithm to parallelize a
692

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

model-checker and achieved almost linear speedup. While partitioning on g and h works on some
domains it is not general if few nodes have the same g and h values. This tends to be the case in
domains with real-valued edge costs. We now turn our attention to two algorithms that will reappear
throughout the rest of this paper: PRA* and PSDD.
2.3 Parallel Retracting A*
PRA* (Evett et al., 1995) attempts to avoid contention by assigning separate open and closed lists
to each thread. A hash of the state representation is used to assign nodes to the appropriate thread
when they are generated. (Full PRA* also includes a retraction scheme that reduces memory use
in exchange for increased computation time; we do not consider that feature in this paper.) The
choice of hash function influences the performance of the algorithm, since it determines the way
that work is distributed. Note that with standard PRA*, any thread may communicate with any of
its peers, so each thread needs a synchronized message queue to which peers can add nodes. In a
multicore setting, this is implemented by requiring a thread to take a lock on the message queue.
Typically, this requires a thread that is sending (or receiving) a message to wait until the operation
is complete before it can continue searching. While this is less of a bottleneck than having a single
global, shared open list, we will see below that it can still be expensive. It is also interesting to
note that PRA* and the variants mentioned below practice a type of delayed duplicate detection,
because they store duplicates temporarily before checking them against a thread-local closed list
and possibly inserting them into the open list.
2.3.1 I MPROVEMENTS
Kishimoto et al. (2009) note that the original PRA* implementation can be improved by removing the synchronization requirement on the message queues between nodes. Instead, they use the
asynchronous send and receive functionality from the MPI message passing library (Snir & Otto,
1998) to implement an asynchronous version of PRA* that they call Hash Distributed A* (HDA*).
HDA* distributes nodes using a hash function in the same way as PRA*, except the sending and
receiving of nodes happens asynchronously. This means that threads are free to continue searching
while nodes which are being communicated between peers are in transit.
In contact with the authors of HDA*, we have created an implementation of HDA* for multicore
machines that does not have the extra overhead of message passing for asynchronous communication between threads in a shared memory setting. Also, our implementation of HDA* allows us
to make a fair comparison between algorithms by sharing common data structures such as priority
queues and hash tables.
In our implementation, each HDA* thread is given a single queue for incoming nodes and one
outgoing queue for each peer thread. These queues are implemented as dynamically sized arrays
of pointers to search nodes. When generating nodes, a thread performs a non-blocking call to
acquire the lock2 for the appropriate peers incoming queue, acquiring the lock if it is available and
immediately returning failure if it is busy, rather than waiting. If the lock is acquired then a simple
pointer copy transfers the search node to the neighboring thread. If the non-blocking call fails the
nodes are placed in the outgoing queue for the peer. This operation does not require a lock because
the outgoing queue is local to the current thread. After a certain number of expansions, the thread
attempts to flush the outgoing queues, but it is never forced to wait on a lock to send nodes. It
2. One such non-blocking call is the pthread mutex trylock function of the POSIX standard.

693

fiB URNS , L EMONS , RUML , & Z HOU

Figure 1: A simple abstraction. Self-loops have been eliminated.
also attempts to consume its incoming queue and only waits on the lock if its open list is empty,
because in that case it has no other work to do. Using this simple and efficient implementation,
we confirmed the results of Kishimoto et al. (2009) that show that the asynchronous version of
PRA* (called HDA*) outperforms the standard synchronous version. Full results are presented in
Section 4.
PRA* and HDA* use a simple representation-based node hashing scheme that is the same one,
for example, used to look up nodes in closed lists. We present two new variants, APRA* and
AHDA*, that make use of state space abstraction to distribute search nodes among the processors.
Instead of assigning nodes to each thread, each thread is assigned a set of blocks of the search space
where each block corresponds to a state in the abstract space. The intuition behind this approach
is that the children of a single node will be assigned to a small subset of all of the remote threads
and, in fact, can often be assigned back to the expanding thread itself. This reduces the number of
edges in the communication graph among threads during search, reducing the chances for thread
contention. Abstract states are distributed evenly among all threads by using a modulus operator in
the hope that open nodes will always be available to each thread.
2.4 Parallel Structured Duplicate Detection
PSDD is the major previously-proposed alternative to PRA*. The intention of PSDD is to avoid
the need to lock on every node generation and to avoid explicitly passing individual nodes between
threads. It builds on the idea of structured duplicate detection (SDD), which was originally developed for external memory search (Zhou & Hansen, 2004). SDD uses an abstraction function, a
many-to-one mapping from states in the original search space to states in an abstract space. The
abstract node to which a state is mapped is called its image. An nblock is the set of nodes in the
state space that have the same image in the abstract space. The abstraction function creates an abstract graph of nodes that are images of the nodes in the state space. If two states are successors in
the state space, then their images are successors in the abstract graph. Figure 1 shows a state space
graph (left) consisting of 36 nodes and an abstract graph (right) which consists of nine nodes. Each
node in the abstract graph represents a grouping of four nodes, called an nblock, in the original state
space, shown by the dotted lines in the state space graph on the left.
694

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Figure 2: Two disjoint duplicate detection scopes.

Each nblock has an open and closed list. To avoid contention, a thread will acquire exclusive
access to an nblock. Additionally, the thread acquires exclusive access to the nblocks that correspond to the successors in the abstract graph of the nblock that it is searching. For each nblock we
call the set of nblocks that are its successors in the abstract graph the its duplicate detection scope.
This is because these are the only abstract nodes to which access is required in order to perform
perfect duplicate detection when expanding nodes from the given nblock. If a thread expands a
node n in nblock b the children of n must fall within b or one of the nblocks that are successors of
b in the abstract graph. Threads can determine whether or not new states generated from expanding
n are duplicates by simply checking the closed lists of nblocks in the duplicate detection scope.
This does not require synchronization because the thread has exclusive access to this set of nblocks.
In PSDD, the abstract graph is used to find nblocks whose duplicate detection scopes are disjoint. These nblocks can be searched in parallel without any locking during node expansions.
Figure 2 shows two disjoint duplicate detection scopes delineated by dashed lines with different
patterns. An nblock that is not in use by any thread and whose duplicate detection scope is also
not in use is considered to be free. A free nblock is available for a thread to acquire it for searching. Free nblocks are found by explicitly tracking, for each nblock b, (b), the number of nblocks
among bs successors that are in use by another thread. An nblock b can only be acquired when
(b) = 0.
The advantage of PSDD is that it only requires a single lock, the one controlling manipulation
of the abstract graph, and the lock only needs to be acquired by threads when finding a new free
nblock to search. This means that threads do not need to synchronize while expanding nodes, their
most common operation.
Zhou and Hansen (2007) used PSDD to parallelize breadth-first heuristic search (Zhou & Hansen,
2006). In this algorithm, each nblock has two lists of open nodes. One list contains open nodes
at the current search depth and the other contains nodes at the next search depth. In each thread,
only the nodes at the current search depth in an acquired nblock are expanded. The children that
are generated are put in the open list for the next depth in the nblock to which they map (which will
be in the duplicate detection scope of the nblock being searched) as long as they are not duplicates.
When the current nblock has no more nodes at the current depth, it is swapped for a free nblock
695

fiB URNS , L EMONS , RUML , & Z HOU

that does have open nodes at this depth. If no more nblocks have open nodes at the current depth,
all threads synchronize and then progress together to the next depth. An admissible heuristic is used
to prune nodes that fall on or above the current solution upper bound.
2.4.1 I MPROVEMENTS
While PSDD can be viewed as a general framework for parallel search, in our terminology, PSDD
refers to an instance of SDD in a parallel setting that uses layer-based synchronization and breadthfirst search. In this subsection, we present two algorithms that use the PSDD framework and attempt
to improve on the PSDD algorithm in specific ways.
As implemented by Zhou and Hansen (2007), the PSDD algorithm uses the heuristic estimate
of a node only for pruning; this is only effective if a tight upper bound is already available. To
cope with situations where a good bound is not available, we have implemented a novel algorithm
using the PSDD framework that uses iterative deepening (IDPSDD) to increase the bound. As we
report below, this approach is not effective in domains such as grid pathfinding that do not have a
geometrically increasing number of nodes within successive f bounds.
Another drawback of PSDD is that breadth-first search cannot guarantee optimality in domains
where operators have differing costs. In anticipation of these problems, Zhou and Hansen (2004)
suggest two possible extensions to their work, best-first search and a speculative best-first layering
approach that allows for larger layers in the cases where there are few nodes (or nblocks) with the
same f value. To our knowledge, we are the first to implement and test these algorithms.
Best-first PSDD (BFPSDD) uses f value layers instead of depth layers. This means that all
nodes that are expanded in a given layer have the same (lowest) f value. BFPSDD provides a bestfirst search order, but may incur excessive synchronization overhead if there are few nodes in each
f layer. To ameliorate this, we loosen the best-first ordering by enforcing that at least m nodes
are expanded before abandoning a non-empty nblock. (Zhou & Hansen, 2007 credit Edelkamp &
Schrodl, 2000 with this idea.) Also, when populating the list of free nblocks for each layer, all of
the nblocks that have nodes with the current layers f value are used or a minimum of k nblocks are
added where k is four times the number of threads. (This value for k gave better performance than
other values tested.) This allows us to add additional nblocks to small layers in order to amortize the
cost of synchronization. In addition, we tried an alternative implementation of BFPSDD that used
a range of f values for each layer. A parameter f was used to proscribe the width (in f values)
of each layer of search. This implementation did not perform as well and we do not present results
for it. With either of these enhancements, threads may expand nodes with f values greater than that
of the current layer. Because the first solution found may not be optimal, search continues until all
remaining nodes are pruned by the incumbent solution.
Having surveyed the existing approaches to parallel best-first search, we now present a new
approach which comprises the main algorithmic contribution of this paper.

3. Parallel Best-N Block-First (PBNF)
In an ideal scenario, all threads would be busy expanding nblocks that contain nodes with the lowest
f values. To approximate this, we combine PSDDs duplicate detection scopes with an idea from
the Localized A* algorithm of Edelkamp and Schrodl (2000). Localized A*, which was designed
to improve the locality of external memory search, maintains sets of nodes that reside on the same
memory page. The decision of which set to process next is made with the help of a heap of sets
696

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

1. while there is an nblock with open nodes
2. lock; b  best free nblock; unlock
3. while b is no worse than the best free nblock or weve done fewer than min expansions
4.
m  best open node in b
5.
if f (m)  f (incumbent), prune all open nodes in b
6.
else if m is a goal
7.
if f (m) < f (incumbent)
8.
lock; incumbent  m; unlock
9.
else for each child c of m
10.
if c is not on the closed list of its nblock
11.
insert c in the open list of the appropriate nblock
Figure 3: A sketch of basic PBNF search, showing locking.
ordered by the minimum f value in each set. By maintaining a heap of free nblocks ordered on each
nblocks best f value, we can approximate our ideal parallel search. We call this algorithm Parallel
Best-N Block-First (PBNF) search.
In PBNF, threads use the heap of free nblocks to acquire the free nblock with the best open
node. A thread will search its acquired nblock as long as it contains nodes that are better than those
of the nblock at the front of the heap. If the acquired nblock becomes worse than the best free
one, the thread will attempt to release its current nblock and acquire the better one which contains
open nodes with lower f values. There is no layer synchronization, so threads do not need to wait
unless no nblocks are free. The first solution found may be suboptimal, so search must continue
until all open nodes have f values worse than the incumbent solution. Figure 3 shows high-level
pseudo-code for the algorithm.
Because PBNF is designed to tolerate a search order that is only approximately best-first, we
have freedom to introduce optimizations that reduce overhead. It is possible that an nblock has only
a small number of nodes that are better than the best free nblock, so we avoid excessive switching
by requiring a minimum number of expansions. Due to the minimum expansion requirement it is
possible that the nodes expanded by a thread are arbitrarily worse than the frontier node with the
minimum f . We refer to these expansions as speculative. This can be viewed as trading off node
quality for reduced contention on the abstract graph. Section 4.1 shows the results of an experiment
that evaluates this trade off.
Our implementation also attempts to reduce the time a thread is forced to wait on a lock by
using non-blocking operations to acquire the lock whenever possible. Rather than sleeping if a lock
cannot be acquired, a non-blocking lock operation (such as pthread mutex trylock) will
immediately return failure. This allows a thread to continue expanding its current nblock if the lock
is busy. Both of these optimizations can introduce additional speculative expansions that would
not have been performed in a serial best-first search.
3.1 Livelock
The greedy free-for-all order in which PBNF threads acquire free nblocks can lead to livelock in
domains with infinite state spaces. Because threads can always acquire new nblocks without waiting
for all open nodes in a layer to be expanded, it is possible that the nblock containing the goal will
697

fiB URNS , L EMONS , RUML , & Z HOU

never become free. This is because we have no assurance that all nblocks in its duplicate detection
scope will ever be unused at the same time. For example, imagine a situation where threads are
constantly releasing and acquiring nblocks that prevent the goal nblock from becoming free. To
fix this, we have developed a method called hot nblocks where threads altruistically release their
nblock if they are interfering with a better nblock. We call this enhanced algorithm Safe PBNF.
We use the term the interference scope of b to refer to the set of nblocks that, if acquired,
would prevent b from being free. The interference scope includes not only bs successors in the
abstract graph, but their predecessors too. In Safe PBNF, whenever a thread checks the heap of
free nblocks to determine if it should release its current nblock, it also ensures that its acquired
nblock is better than any of those that it interferes with (nblocks whose interference scope the
acquired nblock is in). If it finds a better one, it flags that nblock as hot. Any thread that finds
itself blocking a hot nblock will release its nblock in an attempt to free the hot nblock. For each
nblock b we define h (b) to be the number of hot nblocks that b is in the interference scope of. If
h (b) 6= 0, b is removed from the heap of free nblocks. This ensures that a thread will not acquire
an nblock that is preventing a hot nblock from becoming free.
Consider, for example, an abstract graph containing four nblocks connected in a linear fashion:
A  B  C . A possible execution of PBNF can alternate between a thread expanding from
nblocks A and C . If this situation arrises then nblocks B will never be considered free. If the only
goals are located in nblock B then, in an infinite search space there may be a livelock. With the
Safe variant of PBNF, however, when expanding from either A or C a thread will make sure to
check the f value of the best open node in nblock B periodically. If the best node in B is seen to be
better than the nodes in A or C then B will be flagged as hot and both nblocks A and C will no
longer be eligable for expansion until after nblock B has been acquired.
More formally, let N be the set of all nblocks, Predecessors(x ) and Successors(x ) be the sets
of predecessors and successors in the abstract graph of nblock x , H be the set of all hot nblocks,
IntScope(b) = {l  N : x  Successors(b) : l  Predecessors(x )} be the interference scope
of an nblock b and x  y be a partial order over the nblocks where x  y iff the minimum f
value over all of the open nodes in x is lower than that of y. There are three cases to consider when
attempting to set an nblock b to hot with an undirected abstract graph:
1. H  IntScope(b) = {}  H  {x  N : b  IntScope(x )} = {}; none of the nblocks b
interferes with or that interfere with b are hot, so b can be set to hot.
2. x  H : x  IntScope(b)  x  b; b is interfered with by a better nblock that is already
hot, so b must not be set to hot.
3. x  H : x  IntScope(b)  b  x ; b is interfered with by an nblock x that is worse than
b and x is already hot. x must be un-flagged as hot (updating h values appropriately) and in
its place b is set to hot.
Directed abstract graphs have two additional cases:
4. x  H : b  IntScope(x )  b  x ; b is interfering with an nblock x and b is better than x
so un-flag x as hot and set b to hot.
5. x  H : b  IntScope(x )  x  b; b is interfering with an nblock x and x is better than b
so do not set b to hot.
698

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

This scheme ensures that there are never two hot nblocks interfering with one another and that
the nblock that is set to hot is the best nblock in its interference scope. As we verify below, this
approach guarantees the property that if an nblock is flagged as hot it will eventually become free.
Full pseudo-code for Safe PBNF is given in Appendix A.
3.2 Correctness of PBNF
Given the complexity of parallel shared-memory algorithms, it can be reassuring to have proofs of
correctness. In this subsection we will verify that PBNF exhibits various desirable properties:
3.2.1 S OUNDNESS
Soundness holds trivially because no solution is returned that does not pass the goal test.
3.2.2 D EADLOCK
There is only one lock in PBNF and the thread that currently holds it never attempts to acquire it a
second time, so deadlock cannot arise.
3.2.3 L IVELOCK
Because the interaction between the different threads of PBNF can be quite complex, we modeled
the system using the TLA+ (Lamport, 2002) specification language. Using the TLC model checker
(Yu, Manolios, & Lamport, 1999) we were able to demonstrate a sequence of states that can give rise
to a livelock in plain PBNF. Using a similar model we were unable to find an example of livelock
in Safe PBNF when using up to three threads and 12 nblocks in an undirected ring-shaped abstract
graph and up to three threads and eight nblocks in a directed graph.
In our model the state of the system is represented with four variables: state, acquired, isHot and
Succs. The state variable contains the current action that each thread is performing (either search
or nextblock). The acquired variable is a function from each thread to the ID of its acquired nblock
or the value None if it currently does not have an nblock. The variable isHot is a function from
nblocks to either TRUE or FALSE depending on whether or not the given nblock is flagged as hot.
Finally, the Succs variable gives the set of successor nblocks for each nblock in order to build the
nblock graph.
The model has two actions: doSearch and doNextBlock. The doSearch action models the search
stage performed by a PBNF thread. Since we were interested in determining if there is a livelock,
this action abstracts away most of the search procedure and merely models that the thread may
choose a valid nblock to flag as hot. After setting an nblock to hot, the thread changes its state
so that the next time it is selected to perform an action it will try to acquire a new nblock. The
doNextBlock simulates a thread choosing its next nblock if there is one available. After a thread
acquires an nblock (if one was free) it sets its state so that the next time it is selected to perform an
action it will search.
The TLA+ source of the model is located in Appendix B.
Formal proof: In addition to model checking, the TLA+ specification language is designed to
allow for formal proofs of properties. This allows properties to be proved for an unbounded space.
Using our model we have completed a formal proof that a hot nblock will eventually become free
699

fiB URNS , L EMONS , RUML , & Z HOU

regardless of the number of threads or the abstract graph. We present here an English summary.
First, we need a helpful lemma:
Lemma 1 If an nblock n is hot, there is at least one other nblock in its interference scope that is
in use. Also, n is not interfering with any other hot nblocks.
Proof: Initially no nblocks are hot. This can change only while a thread searches or when it releases
an nblock. During a search, a thread can only set n to hot if it has acquired an nblock m that is in
the interference scope of n. Additionally, a thread may only set n to hot if it does not create any
interference with another hot nblock. During a release, if n is hot, either the final acquired nblock
in its interference scope is released and n is no longer hot, or n still has at least one busy nblock in
its interference scope.
2
Now we are ready for the key theorem:
Theorem 1 If an nblock n becomes hot, it will eventually be added to the free list and will no
longer be hot.
Proof: We will show that the number of acquired nblocks in the interference scope of a hot nblock
n is strictly decreasing. Therefore, n will eventually become free.
Assume an nblock n is hot. By Lemma 1, there is a thread p that has an nblock in the interference scope of n, and n is not interfering with or interfered by any other hot nblocks. Assume that
a thread q does not have an nblock in the interference scope of n. There are four cases:
1. p searches its nblock. p does not acquire a new nblock and therefore the number of nblocks
preventing n from becoming free does not increase. If p sets an nblock m to hot, m is not in
the interference scope of n by Lemma 1. p will release its nblock after it sees that n is hot
(see case 2).
2. p releases its nblock and acquires a new nblock m from the free list. The number of acquired
nblocks in the interference scope of n decreases by one as p releases its nblock. Since m,
the new nblock acquired by p, was on the free list, it is not in the interference scope of n.
3. q searches its nblock. q does not acquire a new nblock and therefore the number of nblocks
preventing n from becoming free does not increase. If q sets an nblock m to hot, m is not in
the interference scope of n by Lemma 1.
4. q releases its nblock (if it had one) and acquires a new nblock m from the free list. Since m,
the new nblock acquired by q, was on the free list, it is not in the interference scope of n and
the number of nblocks preventing n from becoming free does not increase.
2
We can now prove the progress property that we really care about:
Theorem 2 A node n with minimum f value will eventually be expanded.
Proof: We consider ns nblock. There are three cases:
1. The nblock is being expanded. Because n has minimum f , it will be at the front of open and
will be expanded.
700

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

2. The nblock is free. Because it holds the node with minimum f value, it will be at the front of
the free list and selected next for expansion, reducing to case 1.
3. The nblock is not on the free list because it is in the interference scope of another nblock that
is currently being expanded. When the thread expanding that nblock checks its interference
scope, it will mark the better nblock as hot. By Theorem 1, we will eventually reach case 2.
2
3.2.4 C OMPLETENESS
This follows easily from liveness:
Corollary 1 If the heuristic is admissible or the search space is finite, a goal will be returned if one
is reachable.
Proof: If the heuristic is admissible, we inherit the completeness of serial A* (Nilsson, 1980) by
Theorem 2. Nodes are only re-expanded if their g value has improved, and this can happen only a
finite number of times, so a finite number of expansions will suffice to exhaust the search space. 2
3.2.5 O PTIMALITY
Because PBNFs expansion order is not strictly best-first, it operates like an anytime algorithm, and
its optimality follows the same argument as that for algorithms such as Anytime A* (Hansen &
Zhou, 2007).
Theorem 3 PBNF will only return optimal solutions.
Proof: After finding an incumbent solution, the search continues to expand nodes until the minimum
f value among all frontier nodes is greater than or equal to the incumbent solution cost. This means
that the search will only terminate with the optimal solution.
2
Before discussing how to adapt PBNF to suboptimal and anytime search, we first evaluate its
performance on optimal problem solving.

4. Empirical Evaluation: Optimal Search
We have implemented and tested the parallel heuristic search algorithms described above on three
different benchmark domains: grid pathfinding, the sliding tile puzzle, and STRIPS planning. We
will discuss each domain in turn. With the exception of the planning domain, the algorithms were
programmed in C++ using the POSIX threading library and run on dual quad-core Intel Xeon E5320
1.86GHz processors with 16Gb RAM. For the planning results the algorithms were written independently in C from the pseudo code in Appendix A. This gives us additional confidence in the
correctness of the pseudo code and our performance claims. The planning experiments were run
on dual quad-core Intel Xeon X5450 3.0GHz processors limited to roughly 2GB of RAM. All open
lists and free lists were implemented as binary heaps except in PSDD and IDPSDD which used a
queue giving them less overhead since they do not require access to minimum valued elements. All
closed lists were implemented as hash tables. PRA* and APRA* used queues for incoming nodes,
and a hash table was used to detect duplicates in both open and closed. For grids and sliding tiles,
701

fiB URNS , L EMONS , RUML , & Z HOU

we used the jemalloc library (Evans, 2006), a special multi-thread-aware malloc implementation,
instead of the standard glibc (version 2.7) malloc, because we found that the latter scales poorly
above 6 threads. We configured jemalloc to use 32 memory arenas per CPU. In planning, a custom
memory manager was used which is also thread-aware and uses a memory pool for each thread.
On grids and sliding tiles abstractions were hand-coded and, nblock data structures were created
lazily, so only the visited part of abstract graph was instantiated. The time taken to create the
abstraction is accounted for in all of the wall time measurements for these two domains. In STRIPS
planning the abstractions were created automatically and the creation times for the abstractions are
reported separately as described in Section 4.5.
4.1 Tuning PBNF
In this section we present results for a set of experiments that we designed to test the behavior of
PBNF as some of its parameters are changed. We study the effects of the two important parameters
of the PBNF algorithm: minimum expansions required before switching to search a new nblock
and the size of the abstraction. This study used twenty 5000x5000 four-connected grid pathfinding
instances with unit cost moves where each cell has a 0.35 probability of being an obstacle. The
heuristic used was the Manhattan distance to the goal location. Error bars in the plots show 95%
confidence intervals and the legends are sorted by the mean of the dependent variable in each plot.
In the PBNF algorithm, each thread must perform a minimum number of expansions before
it is able to acquire a new nblock for searching. Requiring more expansions between switches is
expected to reduce the contention on the nblock graphs lock but could increase the total number
of expanded nodes. We created an instrumented version of the PBNF algorithm that tracks the
time that the threads have spent trying to acquire the lock and the amount of time that threads
have spent waiting for a free nblock. We fixed the size of the abstraction to 62,500 nblocks and
varied the number of threads (from 1 to 8) and minimum expansions (1, 8, 16, 32 and 64 minimum
expansions).
The upper left panel in Figure 4 shows the average amount of CPU time in seconds that each
thread spent waiting to acquire the lock (y-axis) as the minimum expansions parameter was increased (x-axis). Each line in this plot represents a different number of threads. We can see that the
configuration which used the most amount of time trying to acquire the lock was with eight threads
and one minimum expansion. As the number of threads decreased, there was less contention on
the lock as there were fewer threads to take it. As the number of minimum required expansions
increased the contention was also reduced. Around eight minimum expansions the benefit of increasing the value further seemed to greatly diminish.
The upper right panel of Figure 4 shows the results for the CPU time spent waiting for a free
nblock (y-axis) as minimum expansions was increased (x-axis). This is different than the amount
of time waiting on the lock because, in this case, the thread successfully acquired the lock but
then found that there were no free nblocks available to search. We can see that the configuration
with eight threads and one for minimum expansions caused the longest amount of time waiting
for a free nblock. As the number of threads decreased and as the required number of minimum
expansions increased the wait time decreased. The amount of time spent waiting, however, seems
fairly insignificant because it is an order of magnitude smaller than the lock time. Again, we see
that around eight minimum expansions the benefit of increasing seemed to diminish.
702

fi8
7
6
5
4
3
2
1

0.3

average time waiting (seconds)

average time acquiring locks (seconds)

B EST-F IRST S EARCH FOR M ULTICORE M ACHINES

0.2

0.1

8
7
6
5
4
3
2
1

0.02

0.01

0.0
20

40

60

20

total nodes expanded (1K nodes)

minimum expansions

40

60

minimum expansions

2,600

2,500

8
7
6
5
4
3
2
1

2,400

20

40

60

minimum expansions

Figure 4: PBNF locking behavior vs minimum expansions on grid pathfinding with 62,500
nblocks. Each line represents a different number of threads.

The final panel, on the bottom in Figure 4, shows the total number of nodes expanded (y-axis,
which is in thousands of nodes) as minimum expansions was increased. Increasing the minimum
number of expansions that a thread must make before switching to an nblock with better nodes
caused the search algorithm to explore more of the space that may not have been covered by a strict
best-first search. As more of these speculative expansions were performed the total number of
nodes encountered during the search increased. We can also see that adding threads increased the
number of expanded nodes too.
From the results of this experiment it appears that requiring more than eight expansions before switching nblocks had a decreasing benefit with respect to locking and waiting time. In our
non-instrumented implementation of PBNF we found that slightly greater values for the minimum
expansion parameter lead to the best total wall times. For each domain below we use the value that
gave the best total wall time in the non-instrumented PBNF implementation.
703

fiB URNS , L EMONS , RUML , & Z HOU

0.9

8
7
6
5
4
3
2
1

8
7
6
5
4
3
2
1

0.08

average time waiting (seconds)

average time acquiring locks (seconds)

1.2

0.6

0.3

0.06

0.04

0.02

0.0
50

100

150

200

250

50

total nodes expanded (1K nodes)

abstraction size (1K nblocks)

100

150

200

250

abstraction size (1K nblocks)

2,600

2,500

8
7
6
5
4
3
2
1

2,400

50

100

150

200

250

abstraction size (1K nblocks)

Figure 5: PBNF abstraction size: 5000x5000 grid pathfinding, 32 minimum expansions.

Since PBNF uses abstraction to decompose a search space it is also important to understand the
effect of abstraction size on search performance. Our hypothesis was that using too few abstract
states would lead to only a small number of free nblocks therefore making threads spend a lot of
time waiting for an nblock to become free. On the other hand, if there are too many abstract states
then there will be too few nodes in each nblock. If this happens, threads will perform only a small
amount of work before exhausting the open nodes in their nblock and being forced to switch to
a new portion of the search space. Each time a thread must switch nblocks the contention on the
lock is increased. Figure 5 shows the results of an experiment that was performed to verify this
theory. In each plot we have fixed the minimum expansions parameter to 32 (which gave the best
total wall time on grid pathfinding) and varied the number of threads (from 1 to 8) and the size of
the abstraction (10,000, 62,500 and 250,000 nblocks).
The upper left panel of Figure 5 shows a plot of the amount of CPU seconds spent trying to acquire the lock (y-axis) versus the size of the abstraction (x-axis). As expected, when the abstraction
was very coarse there was little time spent waiting on the lock, but as the size of the abstraction grew
704

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

and the number of threads increased the amount of time spent locking increased. At eight threads
with 250,000 nblocks over 1 second of CPU time was spent waiting to acquire the lock. We suspect
that this is because threads were exhausting all open nodes in their nblocks and were, therefore,
being forced to take the lock to acquire a new portion of the search space.
The upper right panel of Figure 5 shows the amount of time that threads spent waiting for an
nblock to become free after having successfully acquired the lock only to find that no nblocks are
available. Again, as we suspected, the amount of time that threads wait for a free nblock decreases
as the abstraction size is increased. The more available nblocks, the more disjoint portions of the
search space will be available. As with our experiments for minimum expansions, the amount of
time spent waiting seems to be relatively insignificant compared to the time spent acquiring locks.
The bottom panel in Figure 5 shows that the number of nodes that were expanded increased
as the size of the abstraction was increased. For finer grained abstractions the algorithm expanded
more nodes. This is because each time a thread switches to a new nblock it is forced to perform at
least the minimum number of expansions, therefore the more switches, the more forced expansions.
4.2 Tuning PRA*
We now turn to looking at the performance impact on PRA* of abstraction and asynchronous communication. First, we compare PRA* with and without asynchronous communication. Results from
a set of experiments on twenty 5000x5000 grid pathfinding and a set of 250 random 15-puzzle instances that were solvable by A* in 3 million expansions are shown in Figure 6. The line labeled
sync. (PRA*) used synchronous communication, async. sends, used synchronous receives and asynchronous sends, async. receives, used synchronous sends and asynchronous receives and async.
(HDA*), used asynchronous communication for both sends and receives. As before, the legend is
sorted by the mean performance and the error bars represent the 95% confidence intervals on the
mean. The vertical lines in the plots for the life cost grid pathfinding domains show that these
configurations were unable to solve instances within the 180 second time limit.
The combination of both asynchronous sends and receives provided the best performance. We
can also see from these plots that making sends asynchronous provided more of a benefit than
making receives asynchronous. This is because, without asynchronous sends, each node that is generated will stop the generating thread in order to communicate. Even if communication is batched,
each send may be required to go to a separate neighbor and therefore a single send operation may be
required per-generation. For receives, the worst case is that the receiving thread must stop at each
expansion to receive the next batch nodes. Since the branching factor in a typical search space is
approximately constant there will be approximately a constant factor more send communications as
there are receive communications in the worst case. Therefore, making sends asynchronous reduces
the communication cost more than receives.
Figure 7 shows the results of an experiment that compares PRA* using abstraction to distribute
nodes among the threads versus PRA* with asynchronous communication. The lines are labeled
as follows: sync. (PRA*) used only synchronous communication, async. (HDA*) used only asynchronous communication and sync. with abst. (APRA*) used only synchronous communication and
used abstraction to distribute nodes among the threads and async. and abst. (AHDA*) used a combination of asynchronous communication and abstraction. Again, the vertical lines in the plots for
the life cost grid pathfinding domains show that these configurations were unable to solve instances
within the 180 second time limit.
705

fiB URNS , L EMONS , RUML , & Z HOU

Grid Unit Four-way
sync. (PRA*)
async. receives
async. sends
async. (HDA*)

sync. (PRA*)
async. receives
async. sends
async. (HDA*)

30

wall time (seconds)

30

wall time (seconds)

Grid Unit Eight-way

20

20

10

10

2

4

6

8

2

4

threads
Grid Life Four-way
200

6

8

threads
Grid Life Eight-way
200

sync. (PRA*)
async. receives
async. sends
async. (HDA*)

sync. (PRA*)
async. receives
async. sends
async. (HDA*)

wall time (seconds)

wall time (seconds)

160

120

80

100

40
0
2

4

6

8

threads

2

6

8

threads

sync. (PRA*)
async. receives
async. sends
async. (HDA*)

10

wall time (seconds)

4

15-Puzzles: 250 easy

8

6

4

2

4

6

8

threads

Figure 6: PRA* synchronization: 5000x5000 grids and easy sliding tile instances.

It is clear from these plots that the configurations of PRA* that used abstraction gave better
performance than PRA* without abstraction in the grid pathfinding domain. The reason for this is
706

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Grid Unit Four-way
sync. (PRA*)
async. (HDA*)
sync. and abst. (APRA*)
async. and abst. (AHDA*)

sync. (PRA*)
async. (HDA*)
sync. and abst. (APRA*)
async. and abst. (AHDA*)

30

wall time (seconds)

30

wall time (seconds)

Grid Unit Eight-way

20

20

10

10

2

4

6

8

2

4

threads
Grid Life Four-way
200

6

8

threads
Grid Life Eight-way
200

sync. (PRA*)
async. (HDA*)
sync. and abst. (APRA*)
async. and abst. (AHDA*)

sync. (PRA*)
async. (HDA*)
sync. and abst. (APRA*)
async. and abst. (AHDA*)

wall time (seconds)

wall time (seconds)

160

120

80

100

40

0
2

4

6

8

threads

4

6

8

threads

sync. (PRA*)
async. (HDA*)
sync. and abst. (APRA*)
async. and abst. (AHDA*)

10

wall time (seconds)

2

15 Puzzles: 250 easy

8

6

4

2

4

6

8

threads

Figure 7: PRA* abstraction: 5000x5000 grids and easy sliding tile instances.

because the abstraction in grid pathfinding will often assign successors of a node being expanded
back to the thread that generated them. When this happens no communication is required and the
707

fiB URNS , L EMONS , RUML , & Z HOU

nodes can simply be checked against the local closed list and placed on the local open list if they
are not duplicates. With abstraction, the only time that communication will be required is when a
node on the edge of an abstract state is expanded. In this case, some of the children will map into
a different abstract state and communication will be required. This experiment also shows that the
benefits of abstraction were greater than the benefits of asynchronous communication in the grid
pathfinding problems. We see the same trends on the sliding tile instances, however they are not
quite as pronounced; the confidence intervals often overlap.
Overall, it appears that the combination of PRA* with both abstraction for distributing nodes
among the different threads and using asynchronous communication gave the best performance. In
the following section we show the results of a comparison between this variant of PRA*, the Safe
PBNF algorithm and the best-first variant of PSDD.
4.3 Grid Pathfinding
In this section, we evaluate the parallel algorithms on the grid pathfinding domain. The goal of
this domain is to navigate through a grid from an initial location to a goal location while avoiding
obstacles. We used two cost models (discussed below) and both four-way and eight-way movement.
On the four-way grids, cells were blocked with a probability of 0.35 and on the eight-way grids
cells were blocked with a probability of 0.45. The abstraction function that was used maps blocks
of adjacent cells to the same abstract state, forming a coarser abstract grid overlaid on the original
space. The heuristic was the Manhattan distance to the goal location. The hash values for states
(which are used to distribute nodes in PRA* and HDA*) are computed as: x  ymax + y of the state
location. This gives a minimum perfect hash value for each state. For this domain we were able to
tune the size of the abstraction and our results show execution with the best abstraction size for each
algorithm where it is relevant.
4.3.1 F OUR -WAY U NIT C OST
In the unit-cost model, each move has the same cost: one.
Less Promising Algorithms Figure 8, shows a performance comparison between algorithms that,
on average, were slower than serial A*. These algorithms were tested on 20 unit-cost four-way
movement 1200x2000 grids with the start location in the bottom left corner and the goal location in
the bottom right. The x-axis shows the number of threads used to solve each instance and the y-axis
shows the mean wall clock time in seconds. The error bars give a 95% confidence interval on the
mean wall clock time and the legend is sorted by the mean performance.
From this figure we can see that PSDD gave the worst average solution times. We suspect that
this was because the lack of a tight upper bound which PSDD uses for pruning. We see that A* with
a shared lock-free open and closed list (LPA*) took, on average, the second longest amount of time
to solve these problems. LPA*s performance improved up to 5 threads and then started to drop off
as more threads were added. The overhead of the special lock-free memory manager along with
the fact that access to the lock-free data structures may require back-offs and retries could account
for the poor performance compared to serial A*. The next algorithm, going down from the top in
the legend, is KBFS which slowly increased in performance as more threads were added however
it was not able to beat serial A*. A simple parallel A* implementation (PA*) using locks on the
open and closed lists performed worse as threads were added until about four where it started to
give a very slow performance increase matching that of KBFS. The PRA* algorithm using a simple
708

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

15

PSDD
LPA*
KBFS
PA*
PRA*
Serial A*

wall time (seconds)

12

9

6

3

2

4

6

8

threads

Figure 8: Simple parallel algorithms on unit cost, four-way 2000x1200 grid pathfinding.

state representation based hashing function gave the best performance in this graph but it was fairly
erratic as the number of threads changed, sometimes increasing and sometimes decreasing. At 6
and 8 threads, PRA* was faster than serial A*.
We have also implemented the IDPSDD algorithm which tries to find the upper bound for a
PSDD search using iterative deepening, but the results are not shown on the grid pathfinding domains. The non-geometric growth in the number of states when increasing the cost bound leads to
very poor performance with iterative deepening on grid pathfinding. Due to the poor performance of
the above algorithms, we do not show their results in the remaining grid, tiles or planning domains
(with the exception of PSDD which makes a reappearance in the STRIPS planning evaluation of
Section 4.5, where we supply it with an upper bound).
More Promising Algorithms The upper left plot in Figure 9 shows the performance of algorithms
on unit-cost four-way grid pathfinding problems. The y-axis represents the speedup over serial A*
and the x-axis shows the number of threads in use for each data point. Error bars indicate 95%
confidence intervals on the mean over 20 different instances. Algorithms in the legend are ordered
by their average performance. The line labeled Perfect speedup shows a perfect linear speedup
where each additional thread increases the performance linearly.
A more practical reference point for speedup is shown by the Achievable speedup line. On
a perfect machine with n processors, running with n cores should take time that decreases linearly
with n. On a real machine, however, there are hardware considerations such as memory bus contention that prevent this n-fold speedup. To estimate this overhead for our machines, we ran sets
of n independent A* searches in parallel for 1  n  8 and calculated the total time for each set
to finish. On a perfect machine all of these sets would take the same time as the set with n = 1.
We compute the Achievable speedup with the ratio of the actual completion times to the time
709

fiB URNS , L EMONS , RUML , & Z HOU

Grid Unit Four-way

Grid Unit Eight-way
8

Perfect speedup
Achievable speedup
Safe PBNF
AHDA*
BFPSDD

6

speedup over serial A*

speedup over serial A*

8

4

Perfect speedup
Achievable speedup
Safe PBNF
AHDA*
BFPSDD

6

4

2

2

2

4

6

8

2

threads
Grid Life Four-way

6

8

6

8

threads
Grid Life Eight-way
8

Perfect speedup
Achievable speedup
Safe PBNF
AHDA*
BFPSDD

6

speedup over serial A*

speedup over serial A*

8

4

4

2

Perfect speedup
Achievable speedup
AHDA*
Safe PBNF
BFPSDD

6

4

2

2

4

6

8

threads

speedup over serial A*

8

2

4

threads

15 Puzzles: 250 easy
Perfect speedup
Achievable speedup
Safe PBNF
AHDA*

6

4

2

2

4

6

8

threads

Figure 9: Speedup results on grid pathfinding and the sliding tile puzzle.

for the set with n = 1. At t threads given the completion times for the sets, hC1 , C2 , ..., Cn i,
1
achievable speedup(t) = tC
Ct .
710

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

The upper left panel shows a comparison between AHDA* (PRA* with asynchronous communication and abstraction), BFPSDD and Safe PBNF algorithm on the larger (5000x5000) unit-cost
four-way problems. Safe PBNF was superior to any of the other algorithms, with steadily decreasing solution times as threads were added and an average speedup over serial A* of more than 6x
when using eight threads. AHDA* had less stable performance, sometimes giving a sharp speedup
increase and sometimes giving a decreased performance as more threads were added. At seven
threads where AHDA* gave its best performance, it was able to reach 6x speedup over serial A*
search. The BFPSDD algorithm solved problems faster as more threads were added however it was
not as competitive as PBNF and AHDA* giving no more than 3x speedup over serial A* with eight
threads.
4.3.2 F OUR -WAY L IFE C OST
Moves in the life cost model have a cost of the row number of the state where the move was
performedmoves at the top of the grid are free, moves at the bottom cost 4999 (Ruml & Do,
2007). This differentiates between the shortest and cheapest paths which has been shown to be a
very important distinction (Richter & Westphal, 2010; Cushing, Bentor, & Kambhampati, 2010).
The left center plot in Figure 9 shows these results in the same format as for the unit-cost variant 
number of threads on the x axis and speedup over serial A* on the y axis. On average, Safe PBNF
gave better speedup than AHDA*, however AHDA* outperformed PBNF at six and seven threads.
At eight threads, however, APRA* did not perform better than at seven threads. Both of these algorithms achieve speedups that are very close to the Achievable speedup for this domain. Again
BFPSDD gave the worst performance increase as more threads were added reaching just under 3x
speedup.
4.3.3 E IGHT-WAY U NIT C OST
In eight-way movement path
 planning problems, horizontal and vertical moves have cost 1, but
diagonal movements cost 2. These real-valued costs make the domain different from the previous
two path planning domains. The upper right panel of Figure 9 shows number of threads on the x
axis and speedup over serial A* on the y axis for the unit cost eight-way movement domain. We see
that Safe PBNF gave the best average performance reaching just under 6x speedup at eight threads.
AHDA* did not outperform Safe PBNF on average, however it was able to achieve a just over 6x
speedup over serial A* at seven threads. Again however, we see that AHDA* did not give very
stable performance increases with more threads. BFPSDD improved as threads were added out to
eight but it never reached more than 3x speedup.
4.3.4 E IGHT-WAY L IFE C OST
This model combines the eight-way movement and the life cost models; it tends to be the most difficult path planning domain presented in this paper. The right center panel of Figure 9 shows threads
on the x axis and speedup over serial A* on the y axis. AHDA* gave the best average speedup over
serial A* search, peaking just under 6x speedup at seven threads. Although it outperformed Safe
PBNF on average at eight threads AHDA* has a sharp decrease in performance reaching down to
almost 5x speedup where Safe PBNF had around 6x speedup over serial A*. BFPSDD again peaks
at just under 3x speedup at eight threads.
711

fiB URNS , L EMONS , RUML , & Z HOU

AHDA* minus Safe PBNF wall time (seconds)

15 puzzles 250 easy AHDA* vs Safe PBNF paired difference
(AHDA*) - (Safe PBNF)
zero

2

1

0
2

4

6

8

threads

Figure 10: Comparison of wall clock time for Safe PBNF versus AHDA* on the sliding tile puzzle.

4.4 Sliding Tile Puzzle
The sliding tile puzzle is a common domain for benchmarking heuristic search algorithms. For these
results, we use 250 randomly generated 15-puzzles that serial A* was able to solve within 3 million
expansions.
The abstraction used for the sliding tile puzzles ignores the numbers on a set of tiles. For
example, the results shown for Safe PBNF in the bottom panel of Figure 9 use an abstraction that
looks at the position of the blank, one and two tiles. This abstraction gives 3360 nblocks. In order
for AHDA* to get the maximum amount of expansions that map back to the expanding thread (as
described above for grids), its abstraction uses the one, two and three tile. Since the position of the
blank is ignored, any state generation that does not move the one, two or three tiles will generate a
child into the same nblock as the parent therefore requiring no communication. The heuristic that
was used in all algorithms was the Manhattan distance heuristic. The hash value used for tiles states
was a perfect hash value based on the techniques presented by Korf and Schultze (2005).
The bottom panel of Figure 9 shows the results for AHDA*, and Safe PBNF on these sliding
tiles puzzle instances. The plot has the number of threads on the x axis and the speedup over serial
A* on the y axis. Safe PBNF had the best mean performance but there was overlap in the confidence
intervals with AHDA*. BFPSDD was unable to show a speedup over serial A* and its performance
was not shown in this plot.
Because sliding tile puzzles vary so much in difficulty, in this domain we also did a paireddifference test, shown in Figure 10. The data used for Figure 10 was collected on the same set of
runs as shown in the bottom panel of Figure 9. The y-axis in this figure, however, is the average,
over all instances, of the time that AHDA* took on that instance minus the time that Safe PBNF
took. This paired test gives a more powerful view of the algorithms relative performance. Values
greater than 0.0 represent instances where Safe PBNF was faster than AHDA* and values lower than
712

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

0.0 represent those instances where AHDA* was faster. The error bars show the 95% confidence
interval on the mean. We can clearly see that the Safe PBNF algorithm was significantly faster than
AHDA* across all numbers of threads from 1 to 8.
4.5 STRIPS Planning
In addition to the path planning and sliding tiles domains, the algorithms were also embedded into
a domain-independent optimal sequential STRIPS planner. In contrast to the previous two domains
where node expansion is very quick and therefore it is difficult to achieve good parallel speedup,
node expansion in STRIPS planning is relatively slow. The planner used in these experiments uses
regression and the max-pair admissible heuristic of Haslum and Geffner (2000). The abstraction
function used in this domain is generated dynamically on a per-problem basis and, following Zhou
and Hansen (2007), this time was not taken into account in the solution times presented for these
algorithms. The abstraction function is generated by greedily searching in the space of all possible
abstraction functions (Zhou & Hansen, 2006). Because the algorithm needs to evaluate one candidate abstraction for each of the unselected state variables, it can be trivially parallelized by having
multiple threads work on different candidate abstractions.
Table 1 presents the results for A*, AHDA*, PBNF, Safe PBNF, PSDD (given an optimal upper
bound for pruning and using divide-and-conquer solution reconstruction), APRA* and BFPSDD.
The values of each cell are the total wall time in seconds taken to solve each instance. A value
of M indicates that the program ran out of memory. The best result on each problem and results
within 10% of the best are marked in bold. Generally, all of the parallel algorithms were able to
solve the instances faster as they were allowed more threads. All of the parallel algorithms were
able to solve instances much faster than serial A* at seven threads. The PBNF algorithm (either
PBNF or Safe PBNF) gave the best solution times in all but three domains. Interestingly, while
plain PBNF was often a little faster than the safe version, it failed to solve two of the problems. This
is most likely due to livelock, although it could also simply be because the hot nblocks fix forces
Safe PNBF to follow a different search order than PBNF. AHDA* tended to give the second-best
solution times, followed by PSDD which was given the optimal solution cost up-front for pruning.
BFPSDD was often better than APRA*,
The column, labeled Abst. shows the time that was taken by the parallel algorithms to serially
generate the abstraction function. Even with the abstraction generation time added on to the solution
times all of the parallel algorithms outperform A* at seven threads, except in the block-14 domain
where the time taken to generate the abstraction actually was longer than the time A* took to solve
the problem.
4.6 Understanding Search Performance
We have seen that the PBNF algorithm tends to have better performance than the AHDA* algorithm
for optimal search. In this section we show the results of a set of experiments that attempts to
determine which factors allow PBNF to perform better in these domains. We considered three
hypotheses. First, PBNF may achieve better performance because it expands fewer nodes with f
values greater than the optimal solution cost. Second, PBNF may achieve better search performance
because it tends to have many fewer nodes on each priority queue than AHDA*. Finally, PBNF
may achieve better search performance because it spends less time coordinating between threads.
In the following subsections we show the results of experiments that we performed to test our
713

fiB URNS , L EMONS , RUML , & Z HOU

threads
logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8

threads
logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8

threads
logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8

A*
1
2.30
5.19
117.78
130.85
335.74
199.06
M
M
M

1
1.17
6.21
39.58
77.02
150.39
127.07
156.36
154.15
235.46

1
1.44
7.37
62.61
95.11
215.19
153.71
319.48
334.28
569.26

AHDA*
1
3
5
7
1.44
0.70 0.48 0.40
7.13
5.07 2.25 2.13
59.51 33.95 15.97 12.69
95.50 33.59 24.11 18.24
206.16 96.82 67.68 57.10
147.96 93.55 38.24 27.37
299.66 126.34 50.97 39.10
315.51 85.17 51.28 48.91
532.51 239.22 97.61 76.34
SafePBNF
3
5
0.64
0.56
2.69
2.20
16.87
11.23
24.09
17.29
53.45
34.23
47.10
38.07
63.04
42.91
59.98
38.84
98.21
63.65

7
0.62
2.02
9.21
13.67
27.02
37.02
34.66
31.22
51.50

APRA*
3
5
7
0.75
1.09
0.81
5.30
3.26
2.92
43.13 37.62 26.78
42.85 67.38 52.82
243.24 211.45 169.92
122.00 63.47 37.94
138.30 67.24 49.58
99.37 89.73 104.87
351.87 236.93 166.19

1
1.20
6.36
65.74
61.53
162.76
126.31
159.98
155.93
387.81

1
1.27
6.28
39.66
68.14
156.64
185.68
M
M
229.88

PBNF
3
5
0.72 0.58
3.76 2.70
16.43 10.92
34.15 20.84
56.25 34.84
64.06 44.05
M
M
M
M
95.63 60.87

PSDD
3
5
0.78
0.68
3.57
2.96
29.37
21.88
23.56
16.71
62.68
43.34
53.76
45.47
73.00
57.65
63.20
41.85
172.01
120.79

BFPSDD
1
3
5
2.11
1.06 0.79
7.78
4.32 3.87
41.56 18.02 12.21
62.01 24.06 20.43
151.50 58.52 40.95
131.30 57.14 47.74
167.24 66.89 48.32
152.08 61.63 42.81
243.44 101.11 70.84

Table 1: Wall time on STRIPS planning problems.

714

7
0.53
2.63
8.57
16.57
26.72
36.08
M
M
48.32

7
0.64
2.87
19.19
13.26
36.66
43.71
54.70
34.02
105.54
Abst.

7
0.71
3.40
10.20
13.54
32.48
45.07
42.68
34.70
59.18

1
0.42
7.9
0.8
1
0.7
17
3.6
9.7
1.1

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

AHDA*
Safe PBNF

AHDA*
Safe PBNF

6e+08

4e+07

Cumulative expansions

Cumulative expansions

5e+07

3e+07

2e+07

4e+08

2e+08

1e+07

0

0
0.7

0.8

0.9

1

Factor of optimal cost

0.6

0.9

1.2

Factor of optimal cost

Figure 11: Cumulative normalized f value counts for nodes expanded with eight threads on unitcost four-way grid pathfinding (left) and the 15-puzzle (right).

three hypotheses. The results of these experiments agree with the first two hypotheses, however, it
appears that the third hypothesis does not hold and, in fact, PBNF occasionally spends more time
coordinating between threads than AHDA*.
4.6.1 N ODE Q UALITY
Because both PBNF and AHDA* merely approximate a best-first order, they may expand some
nodes that have f values greater than the optimal solution cost. When a thread expands a node
with an f value greater than the optimal solution cost its effort was a waste because the only nodes
that must be expanded when searching for an optimal solution are those with f values less than the
optimal cost. In addition to this, both search algorithms may re-expand nodes for which a lower
cost path has been found. If this happens work was wasted during the first sub-optimal expansion
of the node.
Threads in PBNF are able to choose which nblock to expand based on the quality of nodes in
the free nblocks. In AHDA*, however, a thread must expand only those nodes that are assigned
to it. We hypothesized that PBNF may expand fewer nodes with f values that are greater than the
optimal solution cost because the threads have more control over the quality of the nodes that they
choose to expand.
We collected the f value of each node expanded by both PBNF and AHDA*. Figure 11 shows
cumulative counts for the f values of nodes expanded by both PBNF and AHDA* on the same set
of unit-cost four-way 5000x5000 grid pathfinding instances as were used in Section 4.3 (right) and
on the 15-puzzle instances used in Section 4.4 (left). In both plots, the x axis shows the f value of
expanded nodes as a factor of the optimal solution cost for the given instance. The y axis shows
the cumulative count of nodes expanded up to the given normalized f over the set of instances.
715

fiMean CPU time (seconds)

B URNS , L EMONS , RUML , & Z HOU

3e-05

2e-05

1e-05

0

SafePBNF
AHDA*
Grid pathfinding

SafePBNF
AHDA*
15-puzzle

Figure 12: Mean CPU time per open list operation.
By looking at y-location of the right-most tip of each line we can find the total number of nodes
expanded by each algorithm summed over all instances.
On the left panel of Figure 11 we can see that both algorithms tended to expand only a very
small number of nodes with f values that were greater than the optimal solution cost on the grid
pathfinding domain. The AHDA* algorithm expanded more nodes in total on this set of instances.
Both PBNF and AHDA* must expand all of the nodes below the optimal solution cost. Because of
this, the only way that AHDA* can have a greater number of expansions for nodes below a factor
of 1 is if it re-expanded nodes. It appears that AHDA* re-expanded more nodes than PBNF and this
seems to account for the fact that AHDA* expanded more nodes in total.
The right half of Figure 11 shows the results on the 15-puzzle. We see that, again, AHDA*
expanded more nodes in total than PBNF. In this domain the algorithms expanded approximately
the same number of nodes with f values less than the optimal solution cost. We can also see from
this plot that AHDA* expanded many more nodes that had f values greater than or equal to the
optimal solution cost. In summary, PBNF expanded fewer nodes and better quality nodes than
AHDA* in both the grid pathfinding and sliding tiles domains. We speculate that this may happen
because in PBNF the threads are allowed to choose which portion of the space they search and they
choose it based on low f value. In AHDA* the threads must search the nodes that map to them and
these nodes may not be very good.
4.6.2 O PEN L IST S IZES
We have found that, since PBNF breaks up the search space into many different nblocks, it tends
to have data structures with many fewer entries than AHDA*, which breaks up the search space
based on the number of threads. Since we are interested general-purpose algorithms that can handle
domains with real-valued costs (like eight-way grid pathfinding) both PBNF and AHDA* use binary
heaps to implement their open lists. PBNF has one heap per nblock (that is one per abstract state)
whereas AHDA* has one heap per thread. Because the number of nblocks is greater than the
716

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

number of threads AHDA* will have many more nodes than PBNF in each of its heaps. This causes
the heap operations in AHDA* to take longer than the heap operations in PBNF.
The cost of operations on large heaps has been shown to greatly impact overall performance of
an algorithm (Dai & Hansen, 2007). In order to determine the extent to which large heaps effect the
performance of AHDA* we added timers to all of the heap operations for both algorithms. Figure 12
shows the mean CPU time for a single open list operation for unit-cost four-way grid pathfinding
domain and for the 15-puzzle. The boxes show the second and third quartiles with a line drawn
across at the median. The whiskers show the extremes of the data except that data points residing
beyond the first and third quartile by more than 1.5 times the inter-quartile range are signified by
a circle. The shaded rectangle shows the 95% confidence interval on the mean. We can see that,
in both cases, AHDA* tended to spend more time performing heap operations than PBNF which
typically spent nearly no time per heap operation. Heap operations must be performed once for each
node that is expanded and may be required on each node generation. Even though these times are in
the tens of microseconds the frequency of these operations can be very high during a single search.
Finally, as is described by Hansen and Zhou (2007), the reduction in open list sizes can also explain the good single thread performance that PBNF experiences on STRIPS planning (see Table 1).
Hansen and Zhou point out that, although A* is optimally efficient in terms of node expansions, it is
not necessarily optimal with respect to wall time. They found that the benefit of managing smaller
open lists enabled the Anytime weighted A* algorithm to outperform A* in wall time even though
it expanded more nodes when converging to the optimal solution. As we describe in Section 9, this
good single thread performance may also be caused by speculative expansions and pruning.
4.6.3 C OOORDINATION OVERHEAD
Our third hypothesis was that the amount of time that each algorithm spent on coordination overhead might differ. Both parallel algorithms must spend some of their time accessing data structures
shared among multiple threads. This can cause overhead in two places. The first place where coordination overhead can be seen is in the synchronization of access to shared data structures. PBNF
has two modes of locking the nblock graph. First, if a thread has ownership of an nblock with open
nodes that remain to be expanded then it will use try lock because there is work that could be
done if it fails to acquire the lock. Otherwise, if there are no nodes that the thread could expand
then it attempt to acquire the lock on the nblock graph using the normal operation that blocks on
failure. AHDA* will use a try lock on its receive queue at each expansion where it has nodes
on this queue and on its open list. In our implementation AHDA* will only use the blocking lock
operation when a thread has no nodes remaining to expand but has nodes remaining in its send or
receive buffers.
The second place where overhead may be incurred is when threads have no nodes to expand.
In PBNF this occurs when a thread exhausts its current nblock and there are no free nblocks to
acquire. The thread must wait until a new nblock becomes free. In AHDA* if no open nodes map
to a thread then it may have no nodes to expand. In this situation the thread will busy-wait until a
node arrives on its receive queue. In either situation, locking or waiting, there is time that is wasted
because threads are not actively searching the space.
When evaluating coordination overhead, we combine the amount of time spent waiting on a
lock and the amount of time waiting without any nodes to expand. Figure 13 shows the per-thread
coordination times for locks, waiting and the sum of the two normalized to the total wall time.
717

fipercentage of wall time

B URNS , L EMONS , RUML , & Z HOU

8

4

percentage of wall time

SafePBNF AHDA*
Locks

SafePBNF AHDA*
Wait

SafePBNF AHDA*
Sum

SafePBNF AHDA*
Wait

SafePBNF AHDA*
Sum

80

40

SafePBNF AHDA*
Locks

Figure 13: Per-thread ratio of coordination time to wall time on unit-cost four-way pathfinding (top)
and the 15-puzzle (bottom).

Unlike the previous set of boxplots, individual data points residing at the extremes are not signified
by circles in order to improve readability. The Locks column of this plot shows the distribution
of times spent by each thread waiting on a lock, the Wait column shows the distribution of times
that threads spent waiting without any nodes available to expand and the Sum column shows the
distribution of the sum of the mean lock and wait times.
The left side of Figure 13 shows the results for grid pathfinding. From Locks column we see
that threads in AHDA* spent almost no time acquiring locks. This is expected because AHDA*
uses asynchronous communication. It appears that the amount of time that threads in PBNF spent
acquiring locks was significantly greater than that of AHDA*. The Wait column of this plot
shows that both PBNF and AHDA* appeared to have threads spend nearly the same amount of time
waiting without any nodes to expand. Finally, the Sum column shows that the threads in PBNF
spent more time overall coordinating between threads.
The bottom half of Figure 13 shows the coordination overhead for the 15-puzzle domain. Again,
we see that threads in AHDA* spent almost no time acquiring a lock. Individual threads in PBNF,
however, tended to spend a larger fraction of their time waiting on locks in the sliding tiles domain
718

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

than in grid pathfinding. In the Wait column of this figure we can see that AHDA* spent more
time than PBNF without any nodes to expand. Finally, we see that, over all, PBNF spent more time
coordinating between threads than AHDA*.
Overall our experiments have verified that our first two hypotheses that PBNF expanded better
quality nodes than AHDA* and that it spent less time performing priority queue operations than
AHDA*. We also found that our third hypothesis did not hold and that threads in PBNF tended to
have more coordination overhead that AHDA* but this seems to be out-weighed by the other two
factors.
4.7 Summary
In this section we have shown the results of an empirical evaluation of optimal parallel best-first
search algorithms. We have shown that several simple parallel algorithms can actually be slower
than a serial A* search even when offered more computing power. Additionally we showed empirical results for a set of algorithms that make good use of parallelism and do outperform serial A*.
Overall the Safe PBNF algorithm gave the best and most consistent performance of this latter set of
algorithms. Our AHDA* variant of PRA* had the second fastest mean performance in all domains.
We have also shown that using abstraction in a PRA* style search to distribute nodes among
the different threads can give a significant boost in speed by reducing the amount of communication. This modification to PRA* appears to be a lot more helpful than simply using asynchronous
communication. Using both of these improvements in conjunction (AHDA*), yields a competitive
algorithm that has the additional feature of not relying on shared memory.
Finally, we performed a set of experiments in an attempt to explain why Safe PBNF tended to
give better search performance than AHDA*. Our experiments looked at three factors: node quality,
open list sizes and thread-coordination overhead. We concluded that PBNF is faster because it
expands fewer nodes with suboptimal f values and it takes less time to perform priority queue
operations.

5. Bounded Suboptimal Search
Sometimes it is acceptable or even preferable to search for a solution that is not optimal. Suboptimal
solutions can often be found much more quickly and with lower memory requirements than optimal
solutions. In this section we show how to create bounded-suboptimal variants of some of the best
optimal parallel search algorithms.
Weighted A* (Pohl, 1970), a variant of A* that orders its search on f  (n) = g(n) + w  h(n),
with w > 1, is probably the most popular suboptimal search. It guarantees that, for an admissible
heuristic h and a weight w , the solution returned will be w -admissible (within a w factor of the
optimal solution cost) (Davis, Bramanti-Gregor, & Wang, 1988).
It is possible to modify AHDA*, BFPSDD, and PBNF to use weights to find suboptimal solutions, we call these algorithms wAHDA*, wBFPSDD and wPBNF. Just as in optimal search,
parallelism implies that a strict f  search order will not be followed. The proof of weighted A*s
w -optimality depends crucially on following a strict f  order, and for our parallel variants we must
prove the quality of our solution by either exploring or pruning all nodes. Thus finding effective
pruning rules can be important for performance. We will assume throughout that h is admissible.
719

fiB URNS , L EMONS , RUML , & Z HOU

5.1 Pruning Poor Nodes
Let s be the current incumbent solution and w the suboptimality bound. A node n can clearly be
pruned if f (n)  g(s). But according to the following theorem, we only need to retain n if it is on
the optimal path to a solution that is a factor of w better than s. This is a much stronger rule.
Theorem 4 We can prune a node n if w  f (n)  g(s) without sacrificing w -admissibility.
Proof: If the incumbent is w -admissible, we can safely prune any node, so we consider the case
where g(s) > w g(opt), where opt is an optimal goal. Note that without pruning, there always exists
a node p in some open list (or being generated) that is on the best path to opt. Let f  be the cost of an
optimal solution. By the admissibility of h and the definition of p, w f (p)  w f  (p) = w g(opt).
If the pruning rule discards p, that would imply g(s)  w  f (p) and thus g(s)  w  g(opt), which
contradicts our premise. Therefore, an open node leading to an optimal solution will not be pruned
if the incumbent is not w -admissible. A search that does not terminate until open is empty will not
terminate until the incumbent is w -admissible or it is replaced by an optimal solution.
2
We make explicit a useful corollary:
Corollary 2 We can prune a node n if f  (n)  g(s) without sacrificing w -admissibility.

Proof: Clearly w  f (n)  f  (n), so Theorem 4 applies.
2
With this corollary, we can use a pruning shortcut: when the open list is sorted on increasing f  and
the node at the front has f   g(s), we can prune the entire open list.
5.2 Pruning Duplicate Nodes
When searching with an inconsistent heuristic, as in weighted A*, it is possible for the search to
find a better path to an already-expanded state. Likhachev, Gordon, and Thrun (2003) noted that,
provided that the underlying heuristic function h is consistent, weighted A* will still return a w admissible solution if these duplicate states are pruned during search. This ensures that each state
is expanded at most once during the search. Unfortunately, their proof depends on expanding in
exactly best-first order, which is violated by several of the parallel search algorithms we consider
here. However, we can still prove that some duplicates can be dropped. Consider the expansion
of a node n that re-generates a duplicate state d that has already been expanded. We propose the
following weak duplicate dropping criterion: the new copy of d can be pruned if the old g(d ) 
g(n) + w  c  (n, d ), where c  (n, d ) is the optimal cost from node n to node d .
Theorem 5 Even if the weak dropping rule is applied, there will always be a node p from an optimal
solution path on open such that g(p)  w  g  (p).
Proof: We proceed by induction over iterations of search. The theorem clearly holds after expansion
of the initial state. For the induction step, we note that node p is only removed from open when it
is expanded. If its child pi that lies along the optimal path is added to open, the theorem holds. The
only way it wont be added is if there exists a previous duplicate copy pi and the pruning rule holds,
i.e., g(pi )  g(pi1 ) + w  c  (pi1 , pi ). By the inductive hypothesis, g(pi1 )  w  g  (pi1 ), and
2
by definition g  (pi1 ) + c  (pi1 , pi ) = g  (pi ), so we have g(pi )  w  g  (pi ).
Note that the use of this technique prohibits using the global minimum f value as a lower bound on
the optimal solutions cost, because g values can now be inflated by up to a factor of w . However,
if s is the incumbent and we search until the global minimum f  value is  g(s), as in a serial
weighted A* search, then w -admissibility is assured:
720

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Corollary 3 If the minimum f  value is  g(s), where s is the incumbent, then we have g(s) 
w  g  (opt)
Proof: Recall node p from Theorem 5. g(s)  f  (p) = g(p) + w  h(p)  w  (g  (p) + h(p)) 
w  g  (opt).
2
It remains an empirical question whether pruning on this rather weak criterion will lead to better
performance in practice. Our results indicate that it does provide an advantage in the grid pathfinding
domain. Results are presented in Section 6.1. It should be noted that, while extra pruning can
preserve w -admissibility, it may result in solutions of lower quality than those resulting from search
without pruning.
5.3 Optimistic Search
Korf (1993) showed that weighted A* typically returns solutions that are better than the bound, w ,
would suggest. To take advantage of this, Thayer and Ruml (2008) use an optimistic approach to
bounded suboptimal search that works in two stages: aggressive search using a weight that is greater
than the desired optimality bound to find an incumbent solution and then a cleanup phase to prove
that the incumbent is indeed within the bound. The intuition behind this approach is that wA* can
find a solution within a very tight bound (much tighter than w  g(opt)), then the search can continue
looking at nodes in f order until the bound can be proved. Thayer and Ruml show that, indeed,
this approach can surpass the speed of wA* for a given optimality bound. We have implemented an
optimistic version of PBNF (oPBNF).
One of the requirements of oPBNF is that it must have access to the minimum f value over all
nodes in order to prove the bound on the incumbent solution. For the aggressive search stage, the
open lists and the heap of free nblocks are sorted on f  instead of f so a couple of additions need to
be made. First, each nblock has an additional priority queue containing the open search nodes sorted
on f . We call this queue openf . The openf queue is simply maintained by adding and removing
nodes as nodes are added and removed from the f  ordered open list of each nblock. Second, a
priority queue, called minf , of all of the nblocks is maintained, sorted on the lowest f value in each
nblock at the time of its last release. minf is used to track a lower bound on the minimum f value
over all nodes. This is accomplished by lazily updating minf only when an nblock is released by
a thread. When a thread releases an nblock, it sifts the released nblock and its successors to their
new positions in the minf queue. These are the only nblocks whose minimum f values could have
been changed by the releasing thread. Since the global minimum f value over all nodes is strictly
increasing (assuming a consistent heuristic) we have the guarantee that the f value at the front of
the minf queue is strictly increasing and is a lower bound on the global minimum f value at any
given time. Using this lower bound, we are able to prove whether or not an incumbent solution is
properly bounded.
oPBNF needs to decide when to switch between the aggressive search phase and the cleanup
phase of optimistic search. As originally proposed, optimistic search performs aggressive search
until the first incumbent is found then it switches between cleanup (when f  (n)  g(s), where n
is the best node based on f  and s is the incumbent solution) and aggressive search (when f  (n) <
g(s)) to hedge against the case when the current incumbent is not within the bound. In oPBNF,
we were left with a choice: switch between aggressive search and cleanup on a global basis or on
a per-nblock basis. We choose to switch on a per-nblock basis under the assumption that some
threads could be cleaning up areas of the search space with low f values while other threads look
721

fiB URNS , L EMONS , RUML , & Z HOU

for better solutions in areas of the search space with low f  values. In oPBNF, when deciding if
one nblock is better than another (when deciding to switch or to set an nblock to hot), the choice
is no longer based solely on the best f  value of the given nblock, but instead it is based on the f 
value first, then the f value to break ties of if the best f  value is out of the bound of the incumbent.
When acquiring a new nblock, a thread takes either the free nblock with the best f  value or best f
value depending on which nblock is better (where the notion of better is described in the previous
sentence). Finally, when expanding nodes, a thread selects aggressive search or cleanup based on
the same criteria as standard optimistic search for the nodes within the acquired nblock.

6. Empirical Evaluation: Bounded Suboptimal Search
We implemented and tested weighted versions of the parallel search algorithms discussed above:
wAHDA*, wAPRA*, wBFPSDD, wPBNF and oPBNF. All algorithms prune nodes based on the
w  f criterion presented in Theorem 4 and prune entire open lists on f  as in Corollary 2. Search
terminates when all nodes have been pruned by the incumbent solution. Our experiments were
run on the same three benchmark domains as for optimal search: grid pathfinding, the sliding tile
puzzle, and STRIPS planning.
6.1 Grid Pathfinding
Results presented in Table 2 show the performance of the parallel search algorithms in terms of
speedup over serial weighted A* on grid pathfinding problems. Duplicate states that have already
been expanded are dropped in the serial wA* algorithm, as discussed by Likhachev et al. (2003).
The rows of this table show the number of threads and different algorithms whereas the columns
are the weights used for various domains. Each entry shows the mean speedup over serial weighted
A*. We performed a Wilcoxon signed-rank test to determine which mean values were significantly
different; elements that are in bold represent values that were not significantly different (p < 0.05)
from the best mean value in the given column. In general, the parallel algorithms show increased
speedup as threads are added for low weights, and decreased speedup as the weight is increased.
In unit-cost four-way movement grids, for weights of 1.1, and 1.2 the wPBNF algorithm was
the fastest of all of the algorithms tested reaching over five times the speed of wA* at a weight of
1.1 at and over 4.5x at a weight of 1.2 . At a weight of 1.4 wPBNF, wBFPSDD and wAHDA* did
not show a significant difference in performance at 8 threads. wAHDA* had the best speed up of
all algorithms at a weight of 1.8. wAPRA* never gave the best performance in this domain.
In eight-way movement grids wPBNF gave the best performance for a weight of 1.1 and 1.4,
although in the latter case this best performance was a decrease over the speed of wA* and it was
achieved at 1 thread. wAHDA* was the fastest when the weight was 1.2, however, this did not scale
as expected when the number of threads was increased. Finally wAPRA* gave the least performance
decrease over weighted A* at a weight of 1.8 with 1 thread. In this case, all algorithms were slower
than serial weighted A* but wAPRA* gave the closest performance to the serial search. wBFPSDD
never gave the best performance in this domain.
In the life-cost domain wPBNF outperformed all other algorithms for weights 1.1, 1.2 and 1.4.
At weight 1.8, wPBNFs performance quickly dropped, however and wAHDA* had the best results
with more than a 4x speedup over wA*, although the performance appears to have been very inconsistent as it is not significantly different from much lower speedup values for the same weight.
wAPRA* never gave the best performance in this domain.
722

fiwAPRA*

wAHDA*

threads

wBFPSDD

wPBNF

B EST-F IRST S EARCH FOR M ULTICORE M ACHINES

1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8

Unit Four-way Grids
1.1
1.2
1.4
1.8
0.98 0.91 0.51 0.73
1.74 1.65 1.07 0.87
2.47 2.33 1.62 0.89
3.12 2.92 2.13 0.90
3.76 3.52 2.48 0.91
4.30 3.99 2.80 0.89
4.78 4.40 3.01 0.88
5.09 4.66 3.11 0.87
0.82 0.84 0.96 0.94
1.26 1.26 1.45 0.91
1.65 1.65 1.90 0.84
1.93 1.92 2.09 0.79
2.24 2.24 2.36 0.75
2.51 2.51 2.58 0.71
2.73 2.69 2.63 0.67
2.91 2.84 2.68 0.63
0.87 0.79 0.32 0.56
1.35 1.17 0.63 0.84
1.90 1.69 1.30 1.30
2.04 2.10 1.57 1.30
1.77 2.08 1.79 0.97
3.23 3.03 2.18 1.33
3.91 3.78 2.56 1.30
3.79 3.64 3.02 1.13
0.88 0.81 0.32 0.56
0.51 0.44 0.22 0.36
0.36 0.32 0.20 0.26
0.50 0.44 0.30 0.41
0.55 0.56 0.39 0.48
0.52 0.49 0.31 0.30
0.73 0.67 0.40 0.36
1.09 1.07 0.82 0.77

weight
Unit Eight-way Grids
1.1
1.2
1.4
1.8
0.93 1.37 0.73 0.74
1.65 1.82 0.57 0.66
2.36 1.77 0.55 0.61
2.97 1.72 0.53 0.58
3.55 1.67 0.52 0.56
4.04 1.61 0.50 0.54
4.40 1.55 0.49 0.51
4.70 1.49 0.45 0.46
0.87 0.79 0.43 0.33
1.37 1.10 0.43 0.35
1.80 1.22 0.41 0.33
2.13 1.25 0.42 0.33
2.47 1.31 0.39 0.32
2.74 1.21 0.36 0.30
2.94 1.26 0.34 0.29
3.10 1.23 0.32 0.26
0.79 1.10 0.66 0.76
1.04 1.99 0.62 0.61
2.08 2.93 0.64 0.62
2.48 2.84 0.56 0.54
2.49 2.52 0.42 0.41
3.73 2.83 0.49 0.45
4.45 2.89 0.45 0.41
4.39 2.58 0.37 0.38
0.80 1.11 0.67 0.77
0.35 0.69 0.31 0.28
0.41 0.65 0.23 0.22
0.43 0.73 0.22 0.19
0.49 0.87 0.23 0.19
0.50 0.65 0.16 0.14
0.62 0.73 0.17 0.14
0.89 1.38 0.28 0.22

Life Four-way Grids
1.1
1.2
1.4
1.8
0.65 0.66 0.84 0.67
1.15 1.17 1.59 0.39
1.65 1.67 2.32 0.39
2.08 2.10 2.96 0.49
2.53 2.55 3.63 1.49
2.94 2.95 4.20 1.64
3.31 3.33 4.63 2.12
3.61 3.64 5.11 1.06
0.52 0.53 0.58 0.60
0.83 0.83 0.92 0.76
1.10 1.09 1.26 0.84
1.29 1.29 1.48 0.89
1.53 1.51 1.61 0.93
1.73 1.72 1.78 0.93
1.91 1.89 1.94 0.91
2.06 2.03 2.10 0.85
0.56 0.55 0.71 0.22
0.88 0.86 1.29 0.32
1.09 1.39 1.86 0.56
1.60 1.64 2.24 0.56
1.88 1.92 2.58 0.41
2.15 2.17 3.02 1.50
2.39 2.41 3.50 1.07
2.38 2.42 3.55 4.16
0.56 0.56 0.72 0.23
0.35 0.34 0.46 0.12
0.23 0.26 0.32 0.10
0.42 0.43 0.55 0.16
0.54 0.56 0.67 0.20
0.39 0.39 0.49 0.13
0.49 0.49 0.65 0.18
1.00 0.98 1.22 0.42

Table 2: Grid Pathfinding: Average speedup over serial weighted A* for various numbers of
threads.

723

fiB URNS , L EMONS , RUML , & Z HOU

threads
1
2
3
4
5
6
7
8

1.4
0.68
1.35
1.48
1.70
2.04
2.16
2.55
2.71

wPBNF
1.7
2.0
0.44 0.38
0.81 1.00
0.97 0.85
1.20 0.93
1.38 0.97
1.30 1.19
1.46 1.04
1.71 1.10

3.0
0.69
0.63
0.56
0.60
0.74
0.67
0.62
0.60

1.4
0.61
1.18
1.53
1.91
2.33
2.28
2.71
2.70

wAHDA*
1.7
2.0
0.60 0.59
1.11 1.32
1.30 1.40
1.57 1.55
1.70 1.27
1.72 1.24
1.50 1.03
1.51 1.24

3.0
0.54
0.78
0.73
0.74
0.66
0.52
0.44
0.44

threads
1
2
3
4
5
6
7
8

1.4
0.65
0.87
1.05
1.09
1.27
1.33
1.49
1.53

wBFPSDD
1.7
2.0
0.61 0.44
0.74 0.49
0.72 0.63
1.00 0.57
0.97 0.65
1.17 0.61
1.10 0.59
1.08 0.62

3.0
0.35
0.43
0.46
0.45
0.40
0.39
0.34
0.33

1.4
0.61
1.18
1.45
1.77
2.32
2.18
2.63
2.34

wAPRA*
1.7
2.0
0.59 0.59
1.08 1.36
1.25 1.32
1.50 1.36
1.62 1.26
1.54 1.83
1.40 1.09
1.61 1.22

3.0
0.54
0.78
0.78
0.62
0.64
0.47
0.43
0.41

Table 3: 15-puzzle: Average speedup over serial weighted A* for various numbers of threads.

oPBNF

threads
1
2
3
4
5
6
7
8

Unit Four-way Grids
1.1
1.2
1.4
1.8
0.54 0.99 0.74 0.47
0.99 2.00 1.05 0.45
1.40 2.89 1.19 0.45
1.76 3.62 1.26 0.44
2.11 4.29 1.33 0.43
2.43 4.84 1.35 0.44
2.70 5.44 1.37 0.43
2.97 6.01 1.39 0.42

Unit Eight-way Grids
1.1
1.2
1.4
1.8
0.74 0.76 0.09 0.05
1.26 0.71 0.09 0.05
1.64 0.70 0.09 0.05
1.90 0.69 0.09 0.05
2.09 0.68 0.08 0.05
2.21 0.68 0.08 0.05
2.29 0.67 0.08 0.04
2.30 0.67 0.08 0.04

250 easy 15-puzzles
1.4
1.7
2.0
3.0
0.56 0.58 0.77 0.60
0.85 1.07 0.83 0.72
1.06 0.94 0.79 0.80
1.01 0.82 0.93 0.69
1.20 1.21 0.97 0.74
1.32 0.83 0.99 0.67
1.14 0.93 0.88 0.71
1.33 0.87 0.81 0.64

Table 4: Average speedup over serial optimistic search for various numbers of threads.

724

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Overall, we see that wPBNF often had the best speedup results at eight threads and for weights
less than 1.8. wAHDA*, however, gave the best performance at a weight of 1.8 across all grid
pathfinding domains. wBFPSDD often gave speedup over serial weighted A*, however it was not
quite as competitive as wPBNF or wAHDA*. wAPRA* was only very rarely able to outperform
the serial search.
Table 4 shows the results for the optimistic variant of the PBNF algorithm (oPBNF). Each cell
in this table shows the mean speedup of oPBNF over serial optimistic search. Once again, the bold
cells entries that are not significantly different from the best value in the column. For unit-cost
four-way pathfinding problems oPBNF gave a performance increase over optimistic search for two
or more threads and for all weights less than 1.8. At a weight of 1.2, oPBNF tended to give the
best speedup, this may be because optimistic search performed poorly at this particular weight. In
unit-cost eight-way pathfinding, we see that oPBNF performs comparably to the unit-cost domain
for a weight of 1.1, however, at all higher weights the algorithm is slower than serial optimistic
search.
6.2 Sliding Tile Puzzles
For the sliding tiles domain, we used the standard Korf 100 15-puzzles (Korf, 1985). Results are
presented in Table 3. wPBNF, wAHDA* and wAPRA* tended to give comparable performance in
the sliding tile puzzle domain each having values that are not significantly different for weights of
1.4 and 1.7. At a weight of 3.0, wAHDA* gave the least performance decrease over weighted A* at
2 threads.
The right-most column of Table 4 shows the results for optimistic PBNF on 250 15-puzzle
instances that were solvable by A* in fewer than 3 million expansions. oPBNF gave its best performance at a weight of 1.4. For weights greater than 1.4 oPBNF was unable to outperform its serial
counterpart. For greater weights oPBNF tended to perform better with smaller numbers of threads.
One trend that can be seen in both the sliding tiles domain and the grid pathfinding domain is
that the speedup of the parallel algorithms over serial suboptimal search decreases as the weight is
increased. We suspect that the decrease in relative performance is due to the problems becoming
sufficiently easy (in terms of node expansions) that the overhead for parallelism becomes harmful
to overall search. In problems that require many node expansions the cost of parallelism (additional
expansions, spawning threads, synchronization  albeit small, waiting for threads to complete, etc.)
is amortized by the search effort. In problems that require only a small number of expansions,
however, this overhead accounts for more of the total search time and a serial algorithm could
potentially be faster.
To confirm our understanding of the effect of problem size on speedup, Figure 14 shows a comparison of wPBNF to weighted A* on all of the 100 Korf 15-puzzle instances using eight threads.
Each point represents a run on one instance at a particular weight, the y-axis represents wPBNF
speedup relative to serial wA*, and the x-axis represents the number of nodes expanded by wA*.
Different glyphs represents different weight values used for both wPBNF and wA*. The figure
shows that, while wPBNF did not outperform wA* on easier problems, the benefits of wPBNF over
wA* increased as problem difficulty increased. The speed gain for the instances that were run at
a weight of 1.4 (the lowest weight tested) leveled off just under 10 times faster than wA*. This is
because the machine has eight cores. There are a few instances that seem to have speedup greater
than 10x. These can be explained by the speculative expansions that wPBNF performs which may
725

fiB URNS , L EMONS , RUML , & Z HOU

Sliding Tiles wPBNF v.s. wA*
W

log10(Times faster than wA*)

1
W

S

WW W
W
W
W
S
WW
W
W
WS S
S W WW
S WSS W WW
W W W WW
SS S S
W
W
W
W
W
W W
W W WW
SS S
S S
W
W
WSW SWWW
W
SS
WS SSW
W
S
SW W
WWW
WW
S SW
W WW WWW
S
W
S
S
W WW WW
SS
SS
S S W W
SS SSSSSW
W
S
S
S W
W
SSS SW
WS
W
WW
S
SS SSS
S
S
SS S W
W
W
S
S
S
S
W
W
S
S
WS
WS
W
S
S S SS S
SS SS
W
S
S
S SS
SSS SS
SS S
SW
S
W
W

0

-1

W
S
S

3

4

5

W
W

W

wPBNF-1.4
wPBNF-1.7
wPBNF-2.0
wPBNF-3.0
wPBNF-5.0

W

S

6

log10(Nodes expanded by wA*)

Figure 14: wPBNF speedup over wA* as a function of problem difficulty.
find a bounded solution faster than weighted A* due to the pruning of more nodes with f  values
equal to that of the resulting solution. The poor behavior of wPBNF for easy problems is most
likely due to the overhead described above. This effect of problem difficulty means that wPBNF
outperformed wA* more often at low weights, where the problems required more expansions, and
less often at higher weights, where the problems were completed more quickly.
6.3 STRIPS Planning
Table 5 shows the performance of the parallel search algorithms on STRIPS planning problems,
again in terms of speedup versus serial weighted A*. In this table columns represent various weights
and the rows represent different planning problems with two and seven threads. Bold values represent table entries that are within 10% of the the best performance for the given domain. All
algorithms had better speedup at seven threads than at two. wPBNF gave the best speedup for the
most number of domains followed by wAHDA* which was the fastest for three of the domains at
seven threads. At two threads there were a couple of domains (satellite-6 and freecell-3) where
wBFPSDD gave the most speedup, however it never did at seven threads. wAPRA* was always
slower than the three remaining algorithms. On one problem, freecell-3, serial weighted A* performs much worse as the weight increases. Interestingly, wPBNF and wBFPSDD do not show this
pathology, and thus record speedups of up to 1,700 times.
6.4 Summary
In this section, we have seen that bounded suboptimal variants of the parallel searches can give
better performance than their serial progenitors. We have also shown that, on the sliding tile puzzle,
parallel search gives more of an advantage over serial search as problem difficulty increases and we
suspect that this result holds for other domains too. We suspect that this is because the overhead of
using parallelism is not amortized by search time for very easy problems.
726

fi7 threads

2 threads

7 threads

2 threads

B EST-F IRST S EARCH FOR M ULTICORE M ACHINES

logistics-8
blocks-16
gripper-7
satellite-6
elevator-12
freecell-3
depots-13
driverlog-11
gripper-8
logistics-8
blocks-16
gripper-7
satellite-6
elevator-12
freecell-3
depots-13
driverlog-11
gripper-8

logistics-8
blocks-16
gripper-7
satellite-6
elevator-12
freecell-3
depots-13
driverlog-11
gripper-8
logistics-8
blocks-16
gripper-7
satellite-6
elevator-12
freecell-3
depots-13
driverlog-11
gripper-8

1.5
0.99
1.29
0.76
0.68
0.65
1.03
0.73
0.91
0.63
3.19
3.04
1.71
1.11
0.94
3.09
2.38
1.90
1.70

1.5
2.68
0.93
2.01
2.02
2.02
2.06
2.70
0.85
2.06
7.10
2.87
5.67
4.42
6.32
7.01
3.12
1.72
5.85

wAPRA*
2
3
1.02
0.59
0.88
4.12
0.76
0.77
0.93
0.70
0.72
0.71
1.00
1.78
1.25
0.97
0.79
0.94
0.61
0.62
3.10
3.26
1.37
1.08
1.74
1.73
1.01
1.29
0.97
1.04
7.99
2.67
5.36
1.13
1.25
0.93
1.68
1.68

5
1.37
0.30
0.77
0.75
0.77
1.61
1.08
0.93
0.62
2.58
0.37
1.82
1.44
1.02
2.93
1.17
0.92
1.74

1.5
1.25
1.52
1.36
1.15
1.16
1.49
0.92
1.30
1.14
4.59
3.60
3.71
3.22
2.77
4.77
2.98
3.52
3.71

wAHDA*
2
3
1.11
0.80
1.09
4.86
1.35
1.33
1.09
1.28
1.20
1.27
1.20
7.56
1.29
0.96
0.97
0.96
1.16
1.15
4.60
3.61
1.62
0.56
3.66
3.74
3.57
3.05
2.88
2.98
2.71
48.66
6.09
1.22
1.48
0.95
3.63
3.67

wPBNF
3
4.06
0.48
1.99
5.90
2.21
8.11
0.82
0.69
2.08
1.91
0.37
5.07
2.68
6.60
131.12
0.87
0.67
5.40

5
1.00
1.32
2.02
3.04
2.15
10.69
0.81
0.62
2.07
0.46
1.26
5.18
5.89
7.10
1,721.33
0.88
0.42
5.44

1.5
1.86
0.34
1.91
1.71
1.76
1.42
1.48
0.85
2.00
3.17
0.49
4.33
3.13
3.68
2.12
1.88
1.26
4.62

wBFPSDD
2
3
5
2.12
1.14
0.15
0.19
0.16
0.32
1.89
1.86
1.84
2.22
7.50
2.80
1.76
1.81
2.18
0.54 16.88
55.75
1.58
0.18
0.14
0.11
0.19
0.21
1.96
1.97
1.98
3.59
0.62
0.10
0.22
0.11
0.32
4.28
4.14
4.05
2.31
3.01
1.05
3.78
4.04
3.95
0.70 44.49 137.19
1.87
0.15
0.12
0.21
0.30
0.23
4.55
4.55
4.51

2
2.27
0.54
1.99
1.53
2.08
0.84
4.49
0.19
2.04
6.88
0.70
5.09
2.85
6.31
2.31
1.80
0.43
5.31

5
1.51
0.38
1.30
1.44
1.22
1.40
1.09
0.93
1.16
2.58
0.32
3.83
3.60
3.03
4.77
1.17
0.92
4.00

Table 5: Speed-up over serial weighted A* on STRIPS planning problems for various weights.

727

fiB URNS , L EMONS , RUML , & Z HOU

7. Anytime Search
A popular alternative to bounded suboptimal search is anytime search, in which a highly suboptimal
solution is returned quickly and then improved solutions are returned over time until the algorithm
is terminated (or the incumbent solution is proved to be optimal). The two most popular anytime
heuristic search algorithms are Anytime weighted A* (AwA*) (Hansen & Zhou, 2007) and anytime
repairing A* (ARA*) (Likhachev, Gordon, & Thrun, 2003). In AwA* a weighted A* search is
allowed to continue after finding its first solution, pruning when the unweighted f (n)  g(s) where
s is an incumbent solution and n is a node being considered for expansion. ARA* uses a weighted
search where the weight is lowered when a solution meeting the current suboptimality bound has
been found and a special INCONS list is kept that allows the search to expand a node at most once
during the search at each weight.
In this section we present anytime versions of the best performing parallel searches from our
previous sections. We used the PBNF framework to implement Anytime weighted PBNF (AwPBNF) and Anytime Repairing PBNF (ARPBNF). We use the PRA* framework to create anytime
weighted AHDA* (AwAHDA*). We also show the performance of a very simple algorithm that
runs parallel weighted A* searches with differing weights. In the planning domain, we have implemented anytime weighted BFPSDD (AwBFPSDD) for comparison as well.
Because our parallel searches inherently continue searching after their first solutions are found,
they serve very naturally as anytime algorithms in the style of Anytime weighted A*. The main
difference between the standard, optimal versions of these algorithms and their anytime variants is
that the anytime versions will sort all open lists and the heap of free nblocks on f  (n) = g(n) +
w  h(n). In fact, in both cases the optimal search is a degenerate case of the anytime search
where w = 1. This approach (simply using w > 1) is used to implement all algorithms except for
ARPBNF and multi-weighted A*.
Next, we will discuss the details of the ARPBNF algorithm. Following that, we introduce a
new parallel anytime algorithm called multi-weighted A*. Finally, we show the results of a set of
comparisons that we performed on the anytime algorithms discussed in these sections.
7.1 Anytime Repairing PBNF
ARPBNF is a parallel anytime search algorithm based on ARA* (Likhachev et al., 2003). In
ARPBNF, open lists and the heap of nblocks are sorted on f  as in AwPBNF, but instead of merely
continuing the search until the incumbent is proved optimal, ARPBNF uses a weight schedule. Each
time an incumbent is found, the weight on the heuristic value is lowered by a specified amount, all
open lists are resorted and the search continues. On the final iteration, the weight will be 1.0 and
the optimal solution will be found.
The following procedure is used to resort the nblocks in parallel between incumbent solutions:
1. The thread calling for a resort (the one that found a goal) becomes the leader by taking the
lock on the nblock graph and setting the resort flag. (If the flag has already been set, then
another thread is already the leader and the current thread becomes a worker). After the flag
is set the leader thread releases the lock on the nblock graph and waits for all nblocks to have
 values of zero (no nblocks are acquired).
2. Threads check the resort flag each expansion, if it is set then threads release their nblocks and
become worker threads and wait for the leader to set the start flag.
728

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

3. Once all nblocks have  = 0, the leader re-takes the lock on the nblock graph and ensures
that all  values are still zero (if not, then it releases the lock and retries). The leader sets
the global weight value to the next weight on the weight schedule and populates a lock-free
queue with all nblocks. Once the queue has been populated, the leader sets the start flag.
4. All threads greedily dequeue nblocks and resort them until the queue is empty.
5. When all nblocks have been resorted, the leader thread clears the resort flag and the start flag
and releases the lock on the nblock graph. All threads will now acquire new nblocks and the
search will continue.
We modeled this procedure in TLA+ and showed it to be live-lock and dead-lock free for up to
4 threads and 5 nblocks by the use of the TLC model checker (Yu et al., 1999). This model is very
simple so we do not include it in an appendix.
7.2 Multi-weighted A*
In this section we introduce a new and simple parallel anytime algorithm called multi-weighted A*.
The PBNF and PRA* frameworks for parallelizing anytime algorithms can be thought of as one
end on a spectrum of parallel anytime algorithms. In PBNF and PRA* all threads are working on
finding a single solution of a given quality; on the opposite end of the spectrum each thread would
be working to find its own solution. To compare to an algorithm at that end of the spectrum we
implemented an algorithm we call multi-weighted A* that allocates its available threads to their own
weighted A* searches. The thread that finishes first will generally be the thread that was searching
at the greatest weight and therefore the solution will be of the worst quality. The next thread to
finish will have the next greatest weight, and so on. The final thread to complete will generally be
searching at a weight of 1.0, performing a standard A* search, and will return the optimal solution.
The algorithm is given a schedule of weighs in decreasing order. The largest weights in the
schedule are distributed among the available threads. The threads begin searching using wA* with
their given weight values. When a thread finds a new solution that is better than the current one,
it updates the incumbent that is shared between all threads to allow for pruning. When a thread
finds a better incumbent solution, it will be w -admissible with respect to the weight the thread was
searching with. If a thread finishes (either finding a solution or pruning its entire open list), it takes
the highest unclaimed weight from the schedule and starts a fresh search using that weight. If there
are no weights left in the schedule, the thread terminates. When all threads have terminated, the
search is complete. If the final weight in the schedule is 1.0, then the last solution found will be
optimal.
One of the benefits of multi-weighted A* is that it is a very simple algorithm to implement.
However, as we will see below, it doesnt benefit much from added parallelism. A reason for
this may be because, when the weight schedule is exhausted (a thread is searching with the lowest
weight, 1.0) threads that complete their searches will sit idle until the entire search terminates. Since
the final weight will take the longest, this may be a majority of the search time. A more dynamic
schedule could be used to keep threads busy until the optimal solution is found. One could also
attempt to use more threads at once by using some multi-threaded search at each weight, such as
wPBNF or wAHDA*. We leave these extensions for future work.
729

fiB URNS , L EMONS , RUML , & Z HOU

Solution Cost (factor over optimal)

1.1

1.0

1.1

1.0
0.2

0.4

0.6

0.8

1.0

wt sched 1
wt sched 2
wt sched 3
wt sched 4

1.1

1.0
0.2

0.4

0.6

0.8

1.0

0.2

0.4

0.6

0.8

Wall time relative to serial A*

Wall time relative to serial A*

Wall time relative to serial A*

Grid Unit Four-way AwA* lower hull

Grid Unit Four-way AwPBNF (8 threads) lower hull

Grid Unit Four-way ARA* lower hull

1.2

1.2

AwA*
Solution Cost (factor over optimal)

Solution Cost (factor over optimal)

Grid Unit Four-way ARA* raw data
1.2

3.4
1.8
1.4
1.1
1.2

1.1

1.0

1.2

AwPBNF 8 threads
Solution Cost (factor over optimal)

Solution Cost (factor over optimal)

Grid Unit Four-way AwPBNF (8 threads) raw data
1.2

3.4
1.8
1.4
1.2
1.1

Solution Cost (factor over optimal)

Grid Unit Four-way AwA* raw data
1.2

1.1

1.0
0.2

0.4

0.6

0.8

Wall time relative to serial A*

1.0

1.0

ARA*

1.1

1.0
0.2

0.4

0.6

0.8

Wall time relative to serial A*

1.0

0.2

0.4

0.6

0.8

Wall time relative to serial A*

Figure 15: Raw data profiles (top) and lower hull profiles (bottom) for AwA* (left), AwPBNF (center), and ARA* (right). Grid unit-cost four-way pathfinding.

8. Empirical Evaluation: Anytime Search
The implementation and empirical setup was similar to that used for suboptimal search. For ARA*,
ARPBNF and Multi-wA* we considered four different weight schedules: {7.4, 4.2, 2.6, 1.9, 1.5,
1.3, 1.1, 1}, {4.2, 2.6, 1.9, 1.5, 1.3, 1.1, 1.05, 1}, {3, 2.8, . . . , 1.2, 1}, {5, 4.8, . . . , 1.2, 1}. For AwA*
and the other anytime parallel algorithms we consider weights of: 1.1, 1.2, 1.4, 1.8 and 3.4 for grid
pathfinding and 1.4, 1.7, 2.0, 3.0 and 5.0 for the sliding tiles domain. To fully evaluate anytime
algorithms, it is necessary to consider their performance profile, i.e., the expected solution quality
as a function of time. While this can be easily plotted, it ignores the fact that the anytime algorithms
considered in this paper all have a free parameter, namely the weight or schedule of weights used
to accelerate the search. In order to compare algorithms, we make the assumption that, in any
particular application, the user will attempt to find the parameter setting giving good performance
for the timescale they are interested in. Under this assumption, we can plot the performance of each
anytime algorithm by computing, at each time point, the best performance that was achieved by any
of the parameter settings tried for that algorithm  that is minimum solution cost over all parameter
settings for a given algorithm up to the given time point. We refer to this concept as the lower hull
of the profiles, because it takes the minimum over the profiles for each parameter setting.
730

1.0

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Grid Unit Four-way 2 threads

Grid Unit Four-way 8 threads
1.2

ARA*
ARPBNF 2 threads
AwA*
Multi wA* 2 threads
AwAHDA* 2 threads
AwPBNF 2 threads

Solution Cost (factor over optimal)

Solution Cost (factor over optimal)

1.2

1.1

1.0

ARA*
AwA*
Multi wA* 8 threads
ARPBNF 8 threads
AwAHDA* 8 threads
AwPBNF 8 threads

1.1

1.0
0.4

0.8

1.2

1.6

2.0

Wall time relative to serial A*

0.4

0.8

1.2

1.6

2.0

Wall time relative to serial A*

Figure 16: Grid unit-cost four-way pathfinding lower hull anytime profiles.
The top row of Figure 15 shows an example of the raw data for three algorithms on our
5000x5000 unit-cost four-way grid pathfinding problems. The y-axis of these plots is the solution quality as a factor of optimal and the x-axis is the wall clock time relative to the amount of
time A* took to find an optimal solution. The bottom row of this figure shows the lower hull for the
respective data displayed above. By comparing the two images on the left that display the data for
the AwA* algorithm, one can see that the three big steps in the lower hull plot is where a different weight is used in the hull because it has found a better solution for the same time bound. The
center panel in Figure 15 shows that the AwPBNF algorithm gives a similar performance to AwA*,
however it is often faster. This is not surprising since AwPBNF is based on the AwA* approach and
it is running at eight threads instead of one. The final panel in Figure 15 shows ARA*, which uses
weight schedules instead of a single weight.
Figures 16-17 present the lower hulls of both serial and parallel algorithms on grid pathfinding
and the sliding tile puzzle. In each panel, the y-axis represents solution cost as a factor of the optimal
cost. In Figure 16 the x-axis represents wall time relative to the amount of time that serial A* took to
find an optimal solution. This allows for a comparison between the anytime algorithms and standard
serial A*. Since A* is not able to solve all of Korfs 100 15-puzzle instances on this machine, the
x-axis in Figure 17 is the absolute wall time in seconds. Both serial and parallel algorithms are
plotted. The profiles start when the algorithm first returns a solution and ends when the algorithm
has proved optimality or after a 180 second cutoff (since Multi-wA* can consume memory more
quickly than the other algorithms, we gave it a 120 second cutoff on the sliding tile puzzle to prevent
thrashing).
8.1 Four-Way Unit Cost Grids
Figure 16 shows the anytime performance for unit cost four-way movement grid pathfinding problems. AwAHDA* and AwPBNF found the best solutions quicker than the other algorithms. Both
731

fiB URNS , L EMONS , RUML , & Z HOU

Korfs 100 15-puzzles 2 threads

1.016

1.012

1.008

1.004

40

80

120

ARA*
Multi wA* 8 threads
ARPBNF 8 threads
AwA*
AwPBNF 8 threads
AwAHDA* 8 threads

1.02

Solution Cost (factor over optimal)

ARA*
Multi wA* 2 threads
ARPBNF 2 threads
AwAHDA* 2 threads
AwA*
AwPBNF 2 threads

1.02

Solution Cost (factor over optimal)

Korfs 100 15-puzzels 8 threads

160

1.016

1.012

1.008

1.004

40

Wall time (seconds)

80

120

160

Wall time (seconds)

Figure 17: Korfs 100 15-puzzles lower hull anytime profiles.
of these algorithms improved in the amount of time taken to find better solutions as more threads
were added. AwPBNF converged more quickly as more threads were added. Even at two threads
AwPBNF was the first algorithm to converge on the optimal solution in 60% of the time of serial A*.
The next two algorithms are Multi-wA* and anytime repairing PBNF (ARPBNF). Multi-wA* converged more quickly as threads were added, but its performance on finding intermediate solutions
did not change too much for different numbers of threads. ARPBNF, on the other hand, took longer
to find good solutions for low thread counts, but as threads were added it started to perform better,
eventually matching Multi wA* at eight threads. Both of these algorithms improved the solution
quality more steadily than AwPBNF and AwAHDA* which had large jumps in their lower hulls.
Each of these jumps corresponds to the hull switching to a different weight value (compare with the
raw data for AwPBNF in Figure 15). All of the parallel algorithms found good solutions faster than
serial AwA* and serial ARA*. Some parallel algorithms, however, took longer to prove optimality
than AwA* in this domain.
8.2 Sliding Tile Puzzles
Figure 17 presents lower hulls for the anytime algorithms on Korfs 100 instances of the 15-puzzle.
In this figure, the x-axes show the total wall clock time in seconds. These times are not normalized to
A* because it is not able to solve all of the instances. In these panels, we see that AwAHDA* tended
to find good solutions faster than all other algorithms. AwA* and AwPBNF performed very similarly
at two threads and as the number of threads increased AwPBNF begun to find better solutions faster
than AwA*. ARPBNF took longer to find good solutions than AwPBNF and AwAHDA* but it
was able to find better solutions faster than its serial counterpart. The simple Multi wA* algorithm
performed the worst of the parallel algorithms. Increasing the number of threads used in Multi-wA*
did not seem to increase the solution quality. ARA* gave the worst performance in this domain; its
profile curve can be seen at the very top of these three panels.
732

fi7 threads

2 threads

7 threads

2 threads

B EST-F IRST S EARCH FOR M ULTICORE M ACHINES

logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8
logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11

1.5
1.09
1.36
0.78
0.77
0.64
1.37
1.24
1.15
0.61
1.45
2.54
1.77
1.22
0.93
3.64
3.60
3.04

logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8
logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11

1.5
1.06
1.91
2.05
1.58
2.01
1.93
1.94
1.95
2.04
2.04
3.72
5.61
5.96
6.18
3.54
5.74
5.78

AwAPRA*
2
3
1.06
1.40
7.76 56.41
0.77
0.76
0.78
0.78
0.67
0.69
1.43
4.61
1.30
1.30
1.19
1.11
0.62
0.62
1.43
1.81
15.63 98.52
1.68
1.71
1.22
1.26
0.93
0.95
3.75 11.59
3.64
3.65
3.20
3.05
AwPBNF
2
3
1.35
1.94
1.99
13.22
1.96
1.99
1.96
1.98
2.07
2.13
1.06
2.78
2.00
2.01
2.10
1.99
2.05
2.09
2.46
4.19
22.37
25.69
5.05
5.03
4.66
5.74
6.03
6.20
1.50
15.32
5.52
5.48
5.83
5.73

5
1.40
>90.16
0.75
0.76
0.70
1.37
2.68
1.20
0.62
1.81
>177.08
1.73
1.26
0.94
4.44
7.60
3.17

5
1.98
>22.36
1.95
1.91
2.07
6.23
4.10
0.77
2.06
4.21
>7.20
5.06
4.70
6.05
11.46
10.84
2.18

1.5
1.23
1.62
1.35
1.26
1.20
1.66
1.51
1.50
1.16
2.87
3.30
3.75
3.56
2.77
5.00
4.41
4.74

1.5
0.68
1.02
1.94
1.85
1.74
1.45
1.44
1.73
2.01
1.02
1.60
4.30
4.10
3.71
1.78
2.02
2.58

AwAHDA*
2
3
1.21
1.59
9.90
63.60
1.33
1.32
1.23
1.24
1.19
1.16
1.68
5.65
1.51
1.50
1.55
1.46
1.11
1.14
2.81
3.65
19.91 132.97
3.69
3.61
3.46
3.51
2.75
2.79
4.97
16.36
4.42
4.40
4.82
4.66
AwBFPSDD
2
3
0.91
0.91
1.18
7.71
1.89
1.94
1.87
1.49
1.74
1.75
1.46
1.97
1.45
1.32
1.78
1.59
2.00
1.98
1.35
1.37
1.96
12.10
4.24
4.16
3.54
4.16
3.74
3.73
1.82
2.59
1.96
1.92
2.86
2.57

5
1.66
>110.16
1.33
1.23
1.17
1.95
3.18
1.54
1.11
3.74
>231.45
3.67
3.50
2.77
21.57
9.25
4.87

5
0.56
>11.92
1.82
1.80
1.69
3.08
2.40
1.41
1.96
0.92
>19.94
3.96
3.88
3.38
4.14
3.68
2.34

Table 6: Speed-up of anytime search to optimality over serial AwA* on STRIPS planning using
various weights.

8.3 STRIPS Planning
Table 6 shows the speedup of the parallel anytime algorithms over serial anytime A*. All algorithms
were run until an optimal solution was proved. (For a weight of 5, AwA* ran out of memory on
blocks-14, so our speedup values at that weight for that instance are lower bounds.) The bold entries
733

fi7 Threads

B URNS , L EMONS , RUML , & Z HOU

logistics-6
blocks-14
gripper-7
satellite-6
elevator-12
freecell-3
depots-7
driverlog-11
gripper-8

1.5
1.48
1.24
1.07
1.10
1.06
1.05
1.20
1.16
1.06

AwPBNF
2
3
1.84 2.36
1.22 0.21
0.99 0.99
0.87 1.08
1.04 1.04
0.44 0.99
1.15 1.15
1.15 1.19
0.99 0.99

5
2.27
0.03
1.00
0.88
1.03
0.29
1.08
0.43
1.00

1.5
0.68
0.87
0.93
0.88
0.77
0.64
0.54
0.53
0.99

AwBFPSDD
2
3
0.93 0.71
0.18 0.16
0.95 0.93
0.77 0.91
0.78 0.76
0.64 0.20
0.53 0.52
0.58 0.54
0.98 0.99

5
0.54
0.16
0.92
0.90
0.73
0.14
0.49
0.50
0.97

1.5
1.12
1.46
0.99
0.99
1.02
1.13
M
M
M

AwAPRA*
2
3
1.08 1.08
1.46 1.42
1.03 1.01
1.00 1.01
1.00 1.00
1.16 0.82
M
M
M
M
M
M

5
0.98
0.94
0.99
1.02
1.00
0.10
M
M
M

Table 7: Speed-up of anytime search to optimality over PBNF on STRIPS planning problems using
various weights.

in the table represent values that are within 10% of the best performance for the given domain.
For all algorithms, speedup over serial generally increased with more threads and a higher weight.
PBNF gave the fastest performance for all except two domains (blocks-14 and freecell-3). In these
two domains the AwAHDA* gave the best performance by at least a factor of 10x over AwPBNF.
Hansen and Zhou (2007) show that AwA* can lead to speedup over A* for some weight values
in certain domains. Finding a suboptimal solution quickly allows f pruning that keeps the open list
short and quick to manipulate, resulting in faster performance even though AwA* expands more
nodes than A*. We found a similar phenomenon in the corresponding parallel case. Table 7 shows
speedup over unweighted optimal PBNF when using various weights for the anytime algorithms. A
significant fraction of the values are greater than 1, representing a speedup when using the anytime
algorithm instead of the standard optimal parallel search. In general, speedup seems more variable
as the weight increases. For a weight of 1.5, AwPBNF always provides a speedup.
8.4 Summary
In this part of the paper we have shown how to create some new parallel anytime search algorithms
based on the frameworks introduced in the previous sections. We have also created a new parallel
anytime algorithm that simply runs many weighted A* searches with differing weights. In our
experiments, we have seen that AwPBNF and AwAHDA* found higher quality solutions faster than
other algorithms and that they both showed improved performance as more threads were added.
Additionally, ARPBNF, a parallel algorithm that is based on ARA*, improved with more threads
and tended to give a smoother increase in solution quality than the former two algorithms, although
it did not find solutions quite as quickly and it was unable to converge on the optimal solution in
the sliding tiles domain within the given time limit. Running multiple weighted A* searches did
not give solutions faster as the number of threads increased, and its convergence performance was
mixed.

9. Discussion
We have explored a set of best-first search algorithms that exploit the parallel capabilities of modern
CPUs. First we looked at parallel optimal search with (Safe) PBNF, several variants of PRA* and a
734

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

set of simpler previously proposed algorithms. Overall, Safe PBNF gave the best performance for
optimal search. Next we created a set of bounded-suboptimal search algorithms based on PBNF,
the successful variants of PRA*, and the BFPSDD algorithm. PBNF and PRA* with asynchronous
communication and abstraction (AHDA*) gave the best performance over all, with PBNF doing
slightly better on the average. In addition, we showed some results that suggest that boundedsuboptimal PBNF has more of an advantage over serial weighted A* search as problem difficulty
increases. Finally we converted PBNF and PRA* into anytime algorithms and compared them with
some serial anytime algorithms and a new algorithm called multi-weighted A*. We found that
anytime weighted PBNF and the anytime variant of AHDA* gave the best anytime performance
and were occasionally able to find solutions faster than their non-anytime counterparts.
Our results show that PBNF outperforms PSDD. We believe that this is because of the lack of
layer-based synchronization and a better utilization of heuristic cost-to-go information. The fact
that BFPSDD got better as its f layers were widened is suggestive evidence. Another less obvious
reason why PBNF may perform better is because a best-first search can have a larger frontier size
than the breadth-first heuristic search used by PSDD. This larger frontier size will tend to create
more nblocks containing open search nodes. There will be more disjoint duplicate detection scopes
with nodes in their open lists and, therefore, more potential for increased parallelism.
Some of our results show that, even for a single thread, PBNF can outperform a serial A* search
(see Table 1). This may be attributed in part to the speculative behavior of the PBNF algorithm.
Since PBNF uses a minimum number of expansions before testing if it should switch to an nblock
with better f values, it will search some sub-optimal nodes that A* would not search. In order to
get optimal solutions, PBNF acts as an anytime algorithm; it stores incumbent solutions and prunes
until it can prove that it has an optimal solution. Zhou and Hansen show that this approach has the
ability to perform better than A* (Hansen & Zhou, 2007) because of upper bound pruning, which
reduces the number of expansions of nodes with an f value that is equal to the optimal solution
cost and can reduce the number of open nodes, increasing the speed of operations on the open list.
PBNF may also give good single thread performance because it breaks up the search frontier into
many small open lists (one for each nblock). Because of this, each of the priority queue operations
that PBNF performs can be on much smaller queues than A*, which uses one big single queue (see
Section 4.6.2).
9.1 Possible Extensions
While the basic guideline for creating a good abstractions in SDD (and PBNF) is to minimize the
connectivity between abstract states, there are other aspects of abstraction that could be explored.
For instance, discovering which features are good to include or abstract away may be helpful to
users of PBNF. Too much focus on one feature could cause good nodes to be too focused in a small
subset of nblocks (Zhou & Hansen, 2009). Likewise, size of the abstraction could be examined in
more detail. Although we always use a constant abstraction size in our current work for simplicity
it seems likely that abstraction size should change when number of threads changes or perhaps even
based on features of the domain or problem instance. If a guideline could be devised, such as a ratio
between number of nblocks to threads or h value of the start state, a problem-adaptive abstraction
size would be much simpler in real world use. Additionally, edge partitioning (Zhou & Hansen,
2007) could allow us to reduce connectivity of the abstraction used by PBNF, but further study will
be necessary to discover the full impact of this technique on PBNFs behavior.
735

fiB URNS , L EMONS , RUML , & Z HOU

Some possible future extensions to PBNF include adaptive minimum expansion values, use of
external memory, and extension to a distributed setting. Our preliminary work on adapting minimum expansion values indicated that simply increasing or decreasing based on lock failures and
successes had either neutral or negative effect on performance. One reason for this may be because
the minimum expansions parameter adds speculation.
It may be possible to combine PBNF with PRA* in a distributed memory setting. This algorithm
may use a technique based on PRA* to distribute portions of the search space among different nodes
on a cluster of work stations while using a multicore search such as PBNF on each node.
An additional technique that was not explored in this paper is running multicore search algorithms with more threads than there are available cores. This technique has been used to improve
the performance of parallel delayed duplicate detection (Korf, 1993; Korf & Schultze, 2005) which
is heavily I/O intensive. Using this approach, when one thread is blocked on I/O another thread
can make use of the newly available processing core. Even without disk I/O this technique may be
useful if threads spend a lot of time waiting to acquire locks.

10. Conclusions
In this paper we have investigated algorithms for best-first search on multicore machines. We have
shown that a set of previously proposed algorithms for parallel best-first search can be much slower
than running A* serially. We have presented a novel hashing function for PRA* that takes advantage
of the locality of a search space and gives superior performance. Additionally, we have verified results presented by Kishimoto et al. (2009) that using asynchronous communication in PRA* allows
it to perform better than using synchronous communication. We present a new algorithm, PBNF,
that approximates a best-first search ordering while trying to keep all threads busy. We proved
the correctness of the PBNF search framework and used it to derive new suboptimal and anytime
algorithms.
We have performed a comprehensive empirical comparison with optimal, suboptimal and anytime variations of parallel best-first search algorithms. Our results demonstrate that using a good
abstraction to distribute nodes in PRA* can be more beneficial than asynchronous communication,
but that these two techniques can be used together (yielding AHDA*). We also found that the original breadth-first PSDD algorithm does not give competitive behavior without a tight upper bound
for pruning. We implemented a novel extension to PSDD, BFPSDD, that gives reasonable performance on all domains we tested. Our experiments, however, demonstrate that the new PBNF and
AHDA* algorithms outperformed all of the other algorithms. PBNF performs best for optimal and
bounded-suboptimal search and both PBNF and AHDA* gave competitive anytime performance.

Acknowledgments
We gratefully acknowledge support from NSF (grant IIS-0812141), the DARPA CSSG program
(grant HR0011-09-1-0021) and helpful suggestions from Jordan Thayer. Some of these results
were previously reported by Burns, Lemons, Zhou, and Ruml (2009b) and Burns, Lemons, Ruml,
and Zhou (2009a).
736

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Appendix A. Pseudo-code for Safe PBNF
In the following pseudo code there are three global structures. The first is a pointer to the current
incumbent solution, incumbent, the second is a done flag that is set to true when a thread recognizes
that the search is complete and the third is the nblock graph. The nblock graph structure contains
the list of free nblocks, freelist along with the  and h values for each nblock. For simplicity, this
code uses a single lock to access either structure. Each thread also has a local exp count. The best
function on a set of nblocks results in the nblock containing the open node with the lowest f value.
S EARCH ( INITIAL NODE )
1. insert initial node into open
2. for each p  processors, T HREAD S EARCH()
3. while threads are still running, wait()
4. return incumbent
T HREAD S EARCH ()
1. b  NULL
2. while done
3.
b  N EXT N BLOCK(b)
4.
exp  0
5.
while S HOULD S WITCH(b, exp)
6.
m  best open node in b
7.
if m > incumbent then prune m
8.
if m is a goal then
9.
if m < incumbent then
10.
lock; incumbent  m; unlock
11.
else if m is not a duplicate then
12.
children  expand(m)
13.
for each child  children
14.
insert child into open of appropriate nblock
15.
exp  exp + 1
S HOULD S WITCH ( B , EXP )
1. if b is empty then return true
2. if exp < min-expansions then return false
3. exp  0
4. if best(freelist) < b or best(interferenceScope(b)) < b then
5.
if best(interferenceScope(b)) < best(freelist) then
6.
S ET H OT(best(interferenceScope(b)))
7.
return true
8. lock
9. for each b   interferenceScope(b)
10.
if hot(b  ) then S ET C OLD(b  )
11. unlock
12. return false

737

fiB URNS , L EMONS , RUML , & Z HOU

S ET H OT ( B )
1. lock
2. if hot(b) and (b) > 0
3.
and i  interferenceScope(b) : i < b  hot(i ) then
4.
hot(b)  true
5.
for each m   interferenceScope(b)
6.
if hot(m  ) then S ET C OLD(m  )
7.
if (m  ) = 0 and h (m  ) = 0
8.
and m  is not empty then
9.
freelist  freelist \ {m  }
10.
h (m  )  h (m  ) + 1
11. unlock
S ET C OLD ( B )
1. hot(b)  false
2. for each m   interferenceScope(b)
3.
h (m  )  h (m  )  1
4.
if (m  ) = 0 and h (m  ) = 0 and m  is not empty then
5.
if hot(m  ) then
6.
S ET C OLD(m  )
7.
freelist  freelist  {m  }
8.
wake all sleeping threads
R ELEASE ( B )
1. for each b   interferenceScope(b)
2.
(b  )  (b  )  1
3.
if (b  ) = 0 and h (b  ) = 0 and b  is not empty then
4.
if hot(b  ) then
5.
S ET C OLD(b  )
6.
freelist  freelist  {b  }
7.
wake all sleeping threads
N EXT N BLOCK ( B )
1. if b has no open nodes or b was just set to hot then lock
2. else if trylock() fails then return b
3. if b 6= NULL then
4.
bestScope  best(interferenceScope(b))
5.
if b < bestScope and b < best(freelist) then
6.
unlock; return b
7.
R ELEASE(b)
8. if (l  nblocks : (l ) = 0) and freelist is empty then
9.
done  true
10.
wake all sleeping threads
11. while freelist is empty and done, sleep
12. if done then n  NULL
738

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

13. else
14.
m  best(freelist)
15.
for each b   interferenceScope(m)
16.
(b  )  (b  ) + 1
17. unlock
18. return m

739

fiB URNS , L EMONS , RUML , & Z HOU

Appendix B. TLA+ Model: Hot N blocks
Here we present the model used to show that Safe PBNF is live-lock free. Refer to Section 3.2.3.
MODULE HotNblocks
FiniteSets, Naturals
CONSTANTS nnblocks, nprocs, search, nextblock , none
VARIABLES state, acquired , isHot, Succs

Vars = hstate, acquired , isHot, Succsi

States = {search, nextblock }

Nblocks = 0 . . nnblocks  1

Procs = 0 . . nprocs  1
ASSUME nnblocks  nprocs  nprocs > 0  nnblocks > 1  none 
/ Nblocks  Cardinality(States) = 2

Preds(x ) = {y  Nblocks : x  Succs[y]} Set of predecessors to Nblock x

IntScope(x ) = Preds(x )  UNION {Preds(y) : y  Succs[x ]} The interference scope of x

IntBy(x ) = {y  Nblocks : x  IntScope(y)} Set of Nblocks which x interferes.

Busy(A) = A  UNION {Succs[x ] : x  A} Set of Nblocks which are busy given the set of acquired nblocks

Overlap(x , A) = A  IntScope(x ) Set of Busy Nblocks overlapping the successors of x

Hot(A) = {x  Nblocks : isHot[x ]  Overlap(x , A) 6= {}} Set of all hot nblocks given the set of acquired nblocks

HotInterference(A) = UNION {IntScope(x ) : x  Hot(A)} Set of Nblocks in interference scopes of hot nblocks

Free(A) = {x  Nblocks : Overlap(x , A) = {}  x 
/ HotInterference(A)} Free Nblocks

Acquired = {acquired [x ] : x  Procs} \ {none} Set of Nblocks which are currently acquired

OverlapAmt(x ) = Cardinality(Overlap(x , Acquired )) The number of nblocks overlapping x .

doNextBlock (x ) =  UNCHANGED hSuccsi
 state[x ] = nextblock  acquired [x ] = none  Free(Acquired ) 6= {}
 IF Free(Acquired \ {acquired [x ]}) 6= {} THEN
  y  Free(Acquired \ {acquired [x ]}) : acquired  = [acquired EXCEPT ! [x ] = y]
 state  = [state EXCEPT ! [x ] = search]
 isHot  = [y  Nblocks 7 IF y  Free(Acquired \ {acquired [x ]})
THEN FALSE ELSE isHot[y]]
ELSE  acquired  = [acquired EXCEPT ![x ] = none]
 isHot  = [y  Nblocks 7 IF y  Free(Acquired  )
THEN FALSE ELSE isHot[y]]
 UNCHANGED hstatei

doSearch(x ) =  UNCHANGED hacquired , Succsi
 state[x ] = search  state  = [state EXCEPT ![x ] = nextblock ]
  UNCHANGED hisHoti
  y  IntBy(acquired [x ]) :  isHot[y]
 IntScope(y)  Hot(Acquired ) = {}
y 
/ HotInterference(Acquired )
 isHot  = [isHot EXCEPT ![y] = TRUE]

Init =  state = [x  Procs 7 nextblock ]  acquired = [x  Procs 7 none]
 isHot = [x  Nblocks 7 FALSE]
EXTENDS

This is a basic graph where each nblock is connected to its neighbors forming a loop.

 Succs = [x  Nblocks 7

x = 0 THEN {nnblocks  1, x + 1}
x = nnblocks  1 THEN {0, x  1} ELSE {x  1, x + 1}]

Next =  x  Procs : (doNextBlock (x )  doSearch(x ))

Fairness =  x  Procs : WFVars (doNextBlock (x )  doSearch(x ))

Prog = Init  2[Next]Vars  Fairness

HotNblocks =  x  Nblocks : isHot[x ] ; isHot[x ] The property to prove
IF

ELSE IF

740

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

References
Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2009a). Suboptimal and anytime heuristic search on
multi-core machines. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS-09).
Burns, E., Lemons, S., Zhou, R., & Ruml, W. (2009b). Best-first heuristic search for multi-core
machines. In Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-09).
Cushing, W., Bentor, J., & Kambhampati, S. (2010). Cost based search considered harmful. In The
2010 International Symposium on Combinatorial Search (SOCS-10).
Dai, P., & Hansen, E. A. (2007). Prioritizing bellman backups without a priority queue. In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling
(ICAPS-09).
Davis, H. W., Bramanti-Gregor, A., & Wang, J. (1988). The advantages of using depth and breadth
components in heuristic search. In Methodologies for Intelligent Systems 3, pp. 1928.
Edelkamp, S., & Schrodl, S. (2000). Localizing A*. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence (AAAI-00), pp. 885890. AAAI Press.
Edelkamp, S., & Sulewski, D. (2010). GPU exploration of two-player games with perfect hash
functions. In The 2010 International Symposium on Combinatorial Search (SOCS-10).
Evans, J. (2006). A scalable concurrent malloc(3) implementation for FreeBSD. In Proceedings of
BSDCan 2006.
Evett, M., Hendler, J., Mahanti, A., & Nau, D. (1995). PRA* - massively-parallel heuristic-search.
Journal of Parallel and Distributed Computing, 25(2), 133143.
Felner, A., Kraus, S., & Korf, R. (2003). KBFS: K-best-first search. Annals of Mathematics and
Artificial Intelligence, 39(1-2), 1939.
Ferguson, C., & Korf, R. E. (1988). Distributed tree search and its applications to alpha-beta pruning. In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88).
Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. Journal of Artificial Intelligence
Research, 28, 267297.
Harris, T. L. (2001). A pragmatic implementation of non-blocking linked-lists. In Lecture Notes in
Computer Science, Vol. 2180/2001, pp. 300314. Springer Berlin / Heidelberg.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. IEEE Transactions of Systems Science and Cybernetics, SSC-4(2),
100107.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings of
the Fifth Internationas Conference on Artificial Intelligence Planning and Scheduling Systems
(AIPS-00), pp. 140149.
Holzmann, G. J., & Bosnacki, D. (2007). The design of a multicore extension of the SPIN model
checker. IEEE Transactions on Software Engineering, 33(10), 659674.
741

fiB URNS , L EMONS , RUML , & Z HOU

Jabbar, S., & Edelkamp, S. (2006). Parallel external directed model checking with linear I/O. In
Emerson, E., & Namjoshi, K. (Eds.), Verification, Model Checking, and Abstract Interpretation, Vol. 3855 of Lecture Notes in Computer Science, pp. 237251. Springer Berlin / Heidelberg.
Kishimoto, A., Fukunaga, A., & Botea, A. (2009). Scalable, parallel best-first search for optimal
sequential planning. In Proceedings of the Nineteenth International Conference on Automated
Planning and Scheduling (ICAPS-09).
Korf, R. E. (1985). Iterative-deepening-A*: An optimal admissible tree search. In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI-85), pp. 10341036.
Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62(1), 4178.
Korf, R. E. (2003). Delayed duplicate detection: extended abstract. In Proceedings of the Eighteenth
International Joint Conference on Articial Intelligence (IJCAI-03), pp. 15391541.
Korf, R. E., & Schultze, P. (2005). Large-scale parallel breadth-first search. In Proceedings of the
Twentieth National Conference on Articial Intelligence (AAAI-05), pp. 13801385.
Kumar, V., Ramesh, K., & Rao, V. N. (1988). Parallel best-first search of state-space graphs: A summary of results. In Proceedings of the Seventh National Conference on Artificial Intelligence
(AAAI-88), pp. 122127.
Lamport, L. (2002). Specifying Systems: The TLA+ Language and Tools for Hardware and Software
Engineers. Addison-Wesley.
Likhachev, M., Gordon, G., & Thrun, S. (2003). ARA*: Anytime A* with provable bounds on
sub-optimality. In Proceedings of the Seventeenth Annual Conference on Neural Information
Porcessing Systems (NIPS-03).
Likhachev, M., Gordon, G., & Thrun, S. (2003). ARA*: Formal analysis. Tech. rep. CMU-CS-03148, Carnegie Mellon University School of Computer Science.
Niewiadomski, R., Amaral, J., & Holte, R. (2006a). A parallel external-memory frontier breadthfirst traversal algorithm for clusters of workstations. In Proceedings of the 2006 International
Conference on Parallel Processing (ICPP-06), pp. 531538.
Niewiadomski, R., Amaral, J. N., & Holte, R. C. (2006b). Sequential and parallel algorithms for
frontier A* with delayed duplicate detection. In Proceedings of the 21st national conference
on Artificial intelligence (AAAI-06), pp. 10391044. AAAI Press.
Nilsson, N. J. (1980). Principles of Artificial Intelligence. Tioga Publishing Co.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1, 193
204.
Powley, C., & Korf, R. E. (1991). Single-agent parallel window search. IEEE Transactions Pattern
Analysis Machine Intelligence, 13(5), 466477.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime planning with
landmarks. Journal of Artificial Intelligence Research, 39.
Ruml, W., & Do, M. B. (2007). Best-first utility-guided search. In Proceedings of IJCAI-07, pp.
23782384.
742

fiB EST-F IRST S EARCH FOR M ULTICORE M ACHINES

Snir, M., & Otto, S. (1998). MPI-The Complete Reference: The MPI Core. MIT Press, Cambridge,
MA, USA.
Stern, U., & Dill, D. L. (1998). Using magnetic disk instead of main memory in the mur  verifier.
In Computer Aided Verification, pp. 172183. Springer.
Sundell, H., & Tsigas, P. (2005). Fast and lock-free concurrent priority queues for multi-thread
systems. Parallel and Distributed Processing Symposium, International, 65(5), 609627.
Thayer, J. T., & Ruml, W. (2008). Faster than weighted A*: An optimistic approach to bounded
suboptimal search. In Proceedings of the Eighteenth International Conference on Automated
Planning and Scheduling (ICAPS-08).
Valois, J. D. (1995). Lock-Free Data Structures. Ph.D. thesis, Rensselaer Polytechnic Institute.
Yu, Y., Manolios, P., & Lamport, L. (1999). Model checking TLA+ specifications. In Correct
Hardware Design and Verification Methods, pp. 5466. Springer Berlin / Heidlberg.
Zhou, R., & Hansen, E. (2006). Domain-independent structured duplicate detection. In Proceedings
of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), pp. 10821087.
Zhou, R., & Hansen, E. (2007). Edge partitioning in external-memory graph search. In Proceedings
of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-07).
Zhou, R., & Hansen, E. (2009). Dynamic state-space partitioning in external-memory graph search.
In The 2009 International Symposium on Combinatorial Search (SOCS-09).
Zhou, R., & Hansen, E. A. (2004). Structured duplicate detection in external-memory graph search.
In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).
Zhou, R., & Hansen, E. A. (2006). Breadth-first heuristic search. Artificial Intelligence, 170(45),
385408.
Zhou, R., & Hansen, E. A. (2007). Parallel structured duplicate detection. In Proceedings of the
Twenty-Second Conference on Artificial Intelligence (AAAI-07).

743

fiJournal of Artificial Intelligence Research 39 (2010) 373-427

Submitted 4/10; published 10/10

A Constraint Satisfaction Framework for Executing
Perceptions and Actions in Diagrammatic Reasoning
Bonny Banerjee
B. Chandrasekaran

banerjee.28@osu.edu
chandra@cse.ohio-state.edu

Laboratory for Artificial Intelligence Research
Department of Computer Science & Engineering
The Ohio State University, Columbus, OH 43210, USA

Abstract
Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported
in this paper is a contribution to building a general purpose DR system as an extension
to a soar-like problem solving architecture. The work is in a framework in which DR is
modeled as a process where subtasks are solved, as appropriate, either by inference from
symbolic representations or by interaction with a diagram, i.e., perceiving specied information from a diagram or modifying/creating objects in a diagram in specied ways
according to problem solving needs. The perceptions and actions in most DR systems built
so far are hand-coded for the specic application, even when the rest of the system is built
using the general architecture. The absence of a general framework for executing perceptions/actions poses as a major hindrance to using them opportunistically  the essence of
open-ended search in problem solving.
Our goal is to develop a framework for executing a wide variety of specied perceptions and actions across tasks/domains without human intervention. We observe that
the domain/task-specic visual perceptions/actions can be transformed into domain/taskindependent spatial problems. We specify a spatial problem as a quantied constraint
satisfaction problem in the real domain using an open-ended vocabulary of properties, relations and actions involving three kinds of diagrammatic objects  points, curves, regions.
Solving a spatial problem from this specication requires computing the equivalent simplied quantier-free expression, the complexity of which is inherently doubly exponential.
We represent objects as conguration of simple elements to facilitate decomposition of
complex problems into simpler and similar subproblems. We show that, if the symbolic
solution to a subproblem can be expressed concisely, quantiers can be eliminated from
spatial problems in low-order polynomial time using similar previously solved subproblems. This requires determining the similarity of two problems, the existence of a mapping
between them computable in polynomial time, and designing a memory for storing previously solved problems so as to facilitate search. The ecacy of the idea is shown by time
complexity analysis. We demonstrate the proposed approach by executing perceptions and
actions involved in DR tasks in two army applications.

1. Introduction
The research reported in this paper is a contribution to building problem solving agents
in articial intelligence (AI) that use diagrams, much as people do, but most of AI does
not, given the almost exclusive emphasis in AI on language-like or predicate-symbolic representations. Diagrammatic reasoning (DR) is an emerging area of research in a number
c
2010
AI Access Foundation. All rights reserved.

fiBanerjee & Chandrasekaran

of elds, including AI (Glasgow, Narayanan, & Chandrasekaran, 1995; Chandrasekaran,
Kurup, & Banerjee, 2005), logic (Barwise & Etchemendy, 1998; Allwein & Barwise, 1999),
and psychology (Tversky, 2000; Tricket & Trafton, 2006). While all research in DR is in
one way or other dealing with diagrams, dierent research issues are addressed by dierent
researchers. The research reported in this paper considers DR as a problem solving activity
in which an agent (human or articial) makes use of two forms of representation  a spatial
representation in the form of 2D diagrams and a symbolic representation that contains information in a predicate-symbolic form similar to logic and natural language. A schematic
DR architecture, as proposed by Chandrasekaran et al. (2002, 2004, 2005), is illustrated in
Figure 1.
Problem
Spatial problem in
specification language

Spatial
Problem
Solver

Problem
Solver
Solution to the
spatial problem

Diagram

Solution

Inference
Rules
Symbolic
Information
Traditional AI
problem solver

Figure 1: The diagrammatic reasoning architecture.

1.1 Diagrammatic Reasoning as a Problem Solving Activity
The DR architecture shares the idea of problem solving as search in problem state space
(Laird, Rosenbloom, & Newell, 1986; Newell, 1990). In this approach, starting from an
initial state, the agent applies operators to bring about state transitions to reach the goal
state. A goal is either reached or decomposed into subgoals by the use of general and
domain knowledge. Reaching a goal or subgoal requires information which is generated in
the traditional problem solving architectures (e.g., soar in Laird, Newell, & Rosenbloom,
1987, act-r in Anderson, 1993) by inference using predicate-symbolic representation. In the
DR architecture, the agent can extract information from diagrams by applying perceptionlike operations in addition to inference using predicate-symbolic representation to reach the
goal/subgoal. The agent can also create or modify objects in a diagram that propose new
states from which the goal might be reached with subsequent perceptions and inferences.
To illustrate our conceptualization of DR, let us consider a real-world problem. An
army commander, planning strategic operations, uses a terrain map to chalk out a path for
his troops to safely travel from one base camp location L1 to another L2 within a given
time. The information he has is regarding the nature of the terrain (e.g., slow-go or no-go
regions, altitude of dierent parts of the terrain, the speed at which his troops can travel in
dierent kinds of terrain) and an estimate of the maximum repower range of the enemy.
The commander, being a veteran in the eld, is well aware of the possibility that his troops
might be ambushed along any path by the enemy who might be hiding in the neighboring
regions. His problem solving might proceed as follows. A diagram consisting of the part of
374

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

the terrain map of interest for this particular problem is given, along with the peripheries
of the no-go regions and the two points, L1 and L2 (see Figure 2(a)). The commander
draws one of the shorter paths from L1 to L2 maintaining a maximum distance from the
neighboring no-go regions (see Figure 2(b)). He knows what kinds of spatial relations
between points on the route and the points where the enemy could be hiding correspond to
ambush potential. He then uses that knowledge to perceive (and mark) the portions of the
path that are prone to ambush due to enemies hiding behind the neighboring no-go regions
(see Figure 2(c)). If no such portion is found, the path is inferred to be safe. If the length
of a safe path can be traversed in the given time, it is considered a suitable path for the
operation. If the path drawn is not safe or does not satisfy the time constraint, another path
is drawn (see Figure 2(d)) and analyzed. This procedure continues until all paths have been
exhausted. If a suitable path is still not found, the least risky path might be considered
for the operation. In the worst case, the commander might infer that this operation is not
possible. A problem in a similar vein, as described above, has been considered by Forbus,
Usher, and Chapman (2003).
In the above example, it is noteworthy how the problem solver (the commander) opportunistically brings together symbolic knowledge (such as, the repower range of enemies)
and perception and action on a diagram to solve a real-world problem. Such a phenomenon
is characteristic of DR whenever it is used to solve problems in dierent domains, such as,
economics, geometry, engineering, computer-aided design, military, and so on. We observe
that executing the perceptions and actions require solving purely spatial problems with
no involvement of domain knowledge. These spatial problems can be described in terms
of diagrammatic objects, such as, points, curves, and regions, and spatial properties (e.g.,
length of a curve) and relations (e.g., point on a curve) involving them. For example, perceiving the portions of a path prone to ambush due to enemies hiding behind a mountain
range requires computing the set of points q on a curve (the path) c1 such that q is within
a specied distance (the repower range) d from some point p on a curve (the mountain
range) c2 (see Figure 22(b)). Formally, this can be written as
RiskyP ortionsof P ath(q, c1 , c2 , d)  On(q, c1 )p, On(p, c2 )DistanceLessT han(p, q, d)
DistanceLessT han(p, q, d)  Distance(p, q)  d
where p is a point. In this paper, we propose a general and ecient framework for spatial
problem solving to autonomously execute perceptions and actions in DR.
1.2 What Do We Mean by a Diagram?
Definition 1. Diagram. A diagram D is a set of labeled 2D objects {O1 , O2 , ...On } all
located clearly inside (i.e., no intersection or touching) a common region (or bounding box)
B. The objects are of three types  points, curves, regions.
Definition 2. Diagrammatic Object. A diagrammatic object O is a 3-tuple < L, T , E >
where L is a label, T is a type (point, curve or region), and E is its spatial extent. The
spatial extent of a diagrammatic object is the set of points constituent of the object.
375

fiBanerjee & Chandrasekaran

(a) The given diagram consisting of two points, L1
and L2 , and three region obstacles.

(b) One of the shorter paths between L1 and L2
avoiding the obstacles is drawn.

(c) Portions of the path prone to ambush are perceived and marked.

(d) Another path is drawn and will be analyzed
for risk.

Figure 2: Diagrammatic reasoning by an army commander for nding a safe path for transporting his troops from L1 to L2 within a given time.

Definition 3. Diagrammatic Image. The diagrammatic image, I, of a diagram is the
set of points constituent of all objects in the diagram. Thus, if D= {O1 , O2 , ...On } is a
diagram where Oi =<L(Oi ),T (Oi ),E(Oi ) >, then its diagrammatic image I(D) is given by
n

I(D) =
E(Oi )
i1

This denition of a diagram, due to Chandrasekaran et al. (2002, 2004, 2005), supports
the functional representation of a diagram in an articial agent. A diagram on an external
medium (e.g., piece of paper, computer screen) is, at one level, an image consisting of pixels
with dierent intensities. At another level, it is an interpreted representation consisting of
spatial objects in some domain of interest. The abstract diagram is ideal, i.e., the points
are dimensionless, curves have no thickness, etc. In an external diagram, points and curves
consist of at least one pixel with nite dimensions. We will need to interchange between
the two forms of diagrams for reasoning and interaction purposes. In the rest of the paper,
376

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

the term diagram will refer to an abstract diagram only, unless otherwise stated. We are
interested only in diagrams that are line drawings with no color or intensity variation. Such
diagrams form a substantial class of diagrams in everyday use.
1.3 Perceptions and Actions in Diagrammatic Reasoning
Definition 4. Perception. A perception is an act of extracting a new piece of information from a diagram. The new piece of information satisfies constraints specified in terms
of properties and relations among existing objects in the diagram and is a boolean or a real
number or a diagrammatic object(s). Thus, a perception P is a mapping from a diagram D
to a set of booleans {T rue, F alse} or real numbers  or a set of diagrammatic objects D
satisfying constraints C.
C

P : D {T rue, F alse}    D ,

I(D )  I(D)

Definition 5. Action. An action is an act of introducing a new object(s), or modifying
or deleting an existing object(s) in a diagram satisfying constraints specified in terms of
properties and relations among existing objects. Thus, an action A is a mapping from a
diagram D to a new set of diagrammatic objects D satisfying constraints C.
C

A : DD ,

I(D) = I(D )

In the last couple of decades, numerous DR systems have been built for dierent applications in dierent domains. In the following we review some well-known DR systems
where a problem solving agent reasons using diagrams. This review will help realize the
role of perception and action in DR, and the spatial problems implicit in such perceptions
and actions.
Sketchy (Pisan, 1995) is a computer implementation of a model of graph understanding. It recognizes the diagrammatic objects - points, lines, regions, and a vocabulary of
properties and relations that includes coordinate at point, right of, above, inside, steeper,
bigger, vertical, change in slope, touches, intersects, on line, on border, forms border, etc. for
representing conceptual relationships in domains, such as, thermodynamics and economics.
A domain translator is responsible for converting domain-specic conceptual questions into
domain-independent graphical relations. Examples of perception from a supply-demand
graph in economics include how price eects the supply, demand, and market price of the
product, which requires solving visual problems, such as, At what point is supply equal to
demand? (corresponding spatial problem: compute the intersection of two curves), What
is the price for the supply line when the quantity is 350? (corresponding spatial problem:
compute a point on a curve whose one coordinate is given), Are the quantity and price
directly proportional? (corresponding spatial problem: check whether the slope of a curve
between two points is a positive constant or not), Are the quantity and price inversely
proportional? (corresponding spatial problem: check whether the slope of a curve between
two points is a negative constant or not), etc. Actions in this model are not required due
to the nature of its task. Examples of graphs understood by sketchy are shown in Figure
3.
377

fiBanerjee & Chandrasekaran

(a) Graph from economics

(b) Graph from thermodynamics

Figure 3: Examples of graphs understood by sketchy. Reproduced with permission from
Pisan (1994).

Figure 4: An example of a deected frame analysis (from civil engineering) by redraw.
Reproduced with permission from Tessler et al. (1995).

378

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

The redraw system (Tessler et al., 1995) combines diagrammatic and symbolic reasoning to qualitatively determine the deected shape of a frame structure under a load, a
structural analysis problem in civil engineering. It uses a vocabulary of properties and relations including get-angular-displacement, get-displacement, symmetrical-p, connected-to,
near, left, above, rotate, bend, translate, smooth, etc. on three kinds of diagrammatic objects  lines, splines, circles. Though most properties and relations are domain-independent,
some, such as, bend reect the assumptions implicit in the domain and the task and can
be dened accordingly. Perceptions and actions are called inspection and manipulation operators in the system. The underlying representation is a combination of a grid-based and
Cartesian coordinates  shapes are represented using the grid where each element in the
grid corresponds to a point in the diagram while lines are represented by a set of coordinate
points. Examples of perception and action include deecting a beam in the same direction
as the load, checking whether a beam and column are perpendicular at a particular rigid
joint, etc. which require solving visual problems, such as, Bend Beam3 in the negative
direction of the y-axis (corresponding spatial problem: compute a curve with a given slope
at a given point), Make the angle between Beam3 and Column3 at Joint3 90 degrees without modifying Beam3 (corresponding spatial problem: compute a curve such that it makes
a particular angle at a given point with a given curve), Get the angle between Beam3 and
Column3 at the ends connected by Joint3 (corresponding spatial problem: compute the
angle between two curves at a given point), etc. An example of a deected frame analysis
by redraw is shown in Figure 4.
The archimedes system (Lindsay, 1998) assists a human in demonstrating theorems
in Euclidean geometry by modifying/creating diagrams according to his instructions and
thereafter perceiving/inferencing from the diagram. It operates on two diagrammatic objects - points and line segments, and recognizes shapes, such as, square, triangle, path,
etc. The underlying representation is array- or grid-based. The perceptions, called retrieval
processes, are of dierent classes, such as, verify relationship, test for a condition, etc. The
actions, called construction processes, are also of dierent classes, such as, create an object
with certain properties, transform an object, etc. Executing the perceptions and actions require solving spatial problems, such as, create a segment parallel to a given segment through
a given point, rotate an object and check whether it coincides with another object, etc. An
example of a geometry theorem demonstrated by archimedes is shown in Figure 5.
The diamond (Jamnik, 2001), a system for proving mathematical theorems, uses a
sequence of actions on diagrams assisted by a human to prove specic ground instances and
then generalizes by induction. It uses a mixture of Cartesian and topological representations
to represent a dot (equivalent to a point in Cartesian representation) as a diagrammatic
object in the discrete space, and a line and an area (or region) as diagrammatic objects in the
continuous space. Elementary shapes, such as, row, column, ell, and frame, are constructed
from dots, while derived shapes, such as, square, triangle, rectangle, etc. are constructed
from the elementary or other derived shapes. The vocabulary consists of atomic or onestep operations (e.g., rotate, translate, cut, join, project from 3D to 2D, remove, insert a
segment, etc.). Spatial problems in this system are composite operations composed from the
atomic ones, such as, draw a right-angled triangle, translate and rotate a triangle, etc. The
system does not need to execute perceptions as information from a diagram is perceived by
379

fiBanerjee & Chandrasekaran

Figure 5: An example of a geometry theorem demonstrated by archimedes.

Figure 6: An example of a mathematical theorem proven by diamond. The theorem is
after Nelson (1993).

380

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

a human who decides what actions to be applied during the proof search. An example of a
mathematical theorem proven by diamond is shown in Figure 6.
Georep (Ferguson & Forbus, 2000) takes as input a line drawing in vector graphics
representation and creates a predicate calculus representation of the drawings spatial relations. Five primitive shape types are recognized, namely line segments, circular arcs, circles
and ellipses, splines (open and closed), and positioned text. Properties and relations, such
as, proximity detection, orientation detection (e.g., horizontal, vertical, above, beside), parallelism, connectivity (e.g., detecting corner, intersection, mid-connection, touch), etc. are
deployed to accomplish its task. The underlying representation is vector graphics or line
drawings. Systems, such as, magi (Ferguson, 1994), juxta (Ferguson & Forbus, 1998),
and coadd are built using georep for symmetry detection, critiquing diagrams based on
their captions, and producing a description of the units, areas, and tasks from a course of
action diagram, respectively. georep, due to the limitation of its task, does not need to
execute any action. Examples of visual problems in georep include guring out which cup
contains more liquid (corresponding spatial problem: compare the areas of polygons representing the cups), determine whether a gure is symmetric or not (corresponding spatial
problem: check whether one polygon is congruent to the reection of the other polygon),
etc. An example of ambush analysis by georep is shown in Figure 7.

Figure 7: An example of ambush analysis by georep. Reproduced with permission from
Forbus et al. (2003).

The preceding discussion leads to the observation that all DR systems require perceiving from and/or acting on diagrams, and that every perception/action requires solving
a domain-independent spatial problem. Thus, a general-purpose DR system for solving
problems for applications across multiple domains would require solving a large variety of
non-trivial domain-independent spatial problems. These spatial problems can be described
381

fiBanerjee & Chandrasekaran

in terms of three diagrammatic objects  points, curves, regions, and spatial properties and
relations involving them.
1.4 The Problem
How are the perceptions and actions solved in a DR system? Typically, the human developing a DR system identies a priori the problem solving steps including a set of perceptions
and actions, and hand-codes ecient algorithms for solving each of them. If the problem
solving steps need to be altered in future and as a result, a new perception arises, the
developer has to write another algorithm for obtaining its solution. Thus, algorithms need
to be hand-coded for each perception/action. Clearly, this is inconvenient and time consuming in developing a DR system, and does not allow fast and easy experimentation with
dierent problem solving strategies for the same problem. These drawbacks are further
magnied when the goal is to build a general-purpose DR system where a very large variety of perceptions and actions are possible which is not feasible to ascertain a priori, and
develop and store algorithms for. Hence, our goal is to investigate a spatial problem solver
(SPS) for eciently solving spatial problems implicit in perceptions/actions without human
intervention.1
1.5 Contributions
In this paper, we make the following contributions:
1. We observe that the wide variety of visual perceptions/actions for DR applications can
be transformed into domain/task-independent spatial problems. We developed a language
for specifying spatial problems (i.e., spatial relations or actions) as quantied constraint
satisfaction problems (QCSPs) in rst-order logic using a xed set of mathematical/logical
operators in the real domain and an open-ended vocabulary of properties, relations and
actions. Any spatial relation or action involving only points that can be expressed using
those operators and real variables in rst-order logic can be included in the vocabulary.
Further, any spatial relation or action involving curves and/or regions that can be expressed
using the relations On(p, c) and/or Inside(p, r) where p is a point, c is a curve, r is a region,
and any relation/action involving only points in rst-order logic can be included in our
vocabulary. The vocabulary grows richer as more spatial relations and actions are specied.
2. Any spatial relation or action that can be included in the vocabulary is solvable by our
SPS. Real QCSPs are known to be computationally intractable, so a substantial part of the
spatial problem solving literature concentrates on constraint satisfaction problems (CSPs).
We developed a general framework for solving spatial problems specied as QCSPs. The
framework bypasses the process of quantier elimination (QE)  the computational bottleneck and a doubly exponential problem  by taking the help of previously solved similar
spatial problems. We show that, if the symbolic solution to a problem can be expressed
1. The reader should keep the DR architecture in mind. As shown in Figure 1, there are two problem
solvers  the main problem solver which will be always referred to as the problem solver (this might
be a human) and the spatial problem solver which will be referred to as the SPS (this strictly has no
human intervention). The problem solver is responsible for the entire problem solving strategy including
converting domain-specific perceptions and actions into domain-independent spatial problems. The SPS
is responsible only for solving the domain-independent spatial problems that it receives from the problem
solver. It is important to not get confused between the roles played by the two.

382

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

concisely, quantiers can be eliminated from spatial problems in low-order polynomial time
using similar previously solved problems. The framework leaves room to be more ecient
and convenient by incorporating future results in at least two possible directions  learning
constraints from examples (automatic constraint acquisition) and carefully exploiting a rich
portfolio of QE algorithms.
The rest of the paper is organized as follows. In the next section, we discuss the language
for specifying a spatial problem to the SPS. Section 3 describes the SPS. Section 4 analyzes
the computational complexity of the SPS. Section 5 shows how the proposed SPS can be
augmented to a traditional AI problem solver (soar) for reasoning with diagrams in two
real-world applications. Finally, we end with discussion and conclusion.

2. Specification Language
In this section, we discuss a high-level language that is nite, extensible, human-usable,
and expressive enough to describe a wide variety of 2D spatial problems relevant to DR.
The problems specied in this language will be accepted as input by the SPS and solved
without human intervention. The specication language is independent of the SPS, i.e.,
the problem specication remains unchanged even if the underlying representation and
reasoning strategy of the SPS change.
2.1 Diagrammatic Objects
The specication language recognizes three kinds of diagrammatic objects  points, curves,
regions.
Point. A point is the basic diagrammatic object. The other objects are dened in terms
of a set of points.
Curve. A curve is the set of points on it. We approximate a curve piecewise-linearly.
Thus, if curve c is approximated by the sequence of n points {p1 , p2 , ...pn }, then c is the set
of points that lies on its constituent line segments, i.e.
c  {p : On(p, {p1 , p2 })  On(p, {p2 , p3 })  ...On(p, {pn1 , pn })}
where p  (x, y), x, y  , and {pi , pi+1 } is the line segment from pi to pi+1 . We call the
points {p1 , p2 , ...pn } vertex points. For the sake of simplicity of specication, the problem
solver will write the sequence of vertex points {p1 , p2 , ...pn } to specify a curve c.
Region. A region is the set of points inside its boundary. The boundary of a region is a
closed curve which is approximated piecewise-linearly. Thus, a region is a simple (convex
or concave) polygon. Any simple polygon can be triangulated such that a point inside the
region is inside one of the triangles. Thus, if the boundary of region r is approximated by
the sequence of n points {p1 , p2 , ...pn }, then
r  {p : Inside(p, (r)[1])  Inside(p, (r)[2])  ...Inside(p, (r)[m])}
where m is the number of triangles in region r after triangulation, (r)[i] is the ith triangle
of r, and p  (x, y), x, y  . For the sake of simplicity of specication, the problem solver
will write the sequence of vertex points {p1 , p2 , ...pn } of the boundary curve to specify a
383

fiBanerjee & Chandrasekaran

region r. Whether a sequence of vertex points corresponds to a curve or a region will be
determined automatically by the system from the context of the property/relation predicate.
More on how we dene On and Inside in section 3.1.
Further, the SPS can be asked to recognize the kind of diagrammatic object(s) obtained
as the solution to a spatial problem. This is achieved by the function Recognize(Dext )
where Dext is an external diagram (i.e., constituted of pixels unlike an abstract diagram).
For example, the set of all points behind a curve c with respect to a given point p can be a
region object or a curve object depending on the nature of c and its location with respect
to p. In order to recognize the output, the SPS colors the corresponding set of pixels on an
external diagram where each pixel at some predetermined resolution corresponds to a point.
The set of colored pixels are grouped such that two adjacent pixels always belong to the
same group. Each group of pixels constitutes a diagrammatic object. The boundary pixels
of each group is determined. If a group consists of less than three pixels, we consider it as
a point object. If a group consists of more than two pixels and its width (both horizontal
and vertical) is always less than three pixels, we consider it as a curve object. Otherwise,
the group constitutes a region object.
2.2 Vocabulary
Unlike certain well-known qualitative spatial reasoning calculi (e.g., intersection calculus
in Egenhofer, 1991, cardinal direction calculus in Frank, 1991, region connection calculus
in Randell, Cui, & Cohn, 1992), we are not interested in nding a minimal set of spatial
relations nor is our vocabulary based on a closed set of predicates. Rather, our vocabulary
is based on a closed set of operators (to be discussed shortly in section 2.3). The spatial
relations and actions that can be included in our vocabulary are as follows:
1. Any spatial relation or action involving only points that can be expressed using the
xed set of operators and real variables in rst-order logic.
2. Any spatial relation or action involving points, curves or regions that can be expressed
in rst-order logic using the xed set of operators, real variables, any relation/action from
#1, and the relations On(p, c) and/or Inside(p, r) where p is a point, c is a curve, r is a
region.
3. Any spatial relation or action involving points, curves or regions that can be expressed
in rst-order logic using the xed set of operators, real variables, and any relation/action
from #1 and #2.
Thus, our vocabulary is open-ended and addition of new properties and relations is
encouraged when a problem cannot be easily expressed using the existing ones. The observation is that, a human often encounters new perceptions/actions but most of them can be
specied using the already known ones. However, having a large vocabulary helps specify
new ones more conveniently. From the DR literature (Pisan, 1995; Tessler et al., 1995; Lindsay, 1998; Jamnik, 2001; Ferguson & Forbus, 2000; Chandrasekaran et al., 2004; Banerjee &
Chandrasekaran, 2004), we have identied a vocabulary of properties, relations and actions
based on their wide usage for expressing a variety of real-world spatial problems in dierent
domains. The same vocabulary will be used in this paper as a starting point for specifying
spatial problems. In what follows are a few examples of properties, relations and actions in
our vocabulary.
384

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Properties. Associated with each kind of object are a few properties  location of a
point; location, closedness and length of a curve; and location, area and periphery of a
region, where the periphery of a region refers to its boundary curve. The user can also
dene particular shapes (e.g., circle, triangle, annulus, etc.) for curves and regions as
appropriate for reasoning in his domain. Dierent shapes might have their own specic
properties, such as, radius of a circle, height of a triangle, etc. which can be easily associated
with the objects in our vocabulary by the user. DR also requires solving spatial problems
concerning a discrete set of points. For such problems, properties, such as, Centroid(S)
and V ariance(S), where S is a set of points, are included in the vocabulary.
Relations. The vocabulary also contains a few widely used relations (or relational
predicates) involving points, such as, Lef tof (p1 , p2 ), T opof (p1 , p2 ), Collinear(p1 , p2 , p3 ),
Between(p1 , p2 , p3 ) where p1 , p2 , p3 are points. Any other relation involving points can be
included in the vocabulary as needed. On(p, c), where p is a point and c a curve, is the
fundamental relation involving a curve while Inside(p, r), where p is a point and r a region, is the fundamental relation involving a region in our vocabulary as any other relation
involving curves or regions uses On and/or Inside. Some of the relational predicates involving curves or regions in our vocabulary are Intersect(c1 , c2 ), IntersectionP oints(q, c1 , c2 ),
T ouches(c1 , c2 ), Subcurveof (c1 , c2 ) where c1 , c2 are curves, and Subregionof (r1 , r2 ) where
r1 , r2 are regions.
Actions. Further, there is a set of predicates for identifying emergent objects or modications of existing objects. For example, T ranslate(q, O, tx , ty ) returns a translation of object
O for tx units along x-axis and ty units along y-axis, Rotate(q, O, c, ) returns a rotation of
the object O with respect to point c as center for  degrees in the anti-clockwise direction,
Ref lect(q, O, {a, b}) returns a reection of object O with respect to the line segment {a, b}
(i.e., from point a to point b), Scale(q, O, c, sx , sy ) returns a scaling of the object O with
respect to point c for sx units along x-axis and sy units along y-axis. When O is a curve or
region, each of these predicates is dened using the corresponding action involving a point
and the predicates On and/or Inside.

2.3 The Language
This is the language in which the problem solver (human or articial) species a spatial
problem to the SPS. The internal representations of objects, properties, relations, and the
problem-solving strategies are hidden from the problem solver. The specication language
remains unchanged even if the underlying representation or problem-solving strategy is
changed. We use rst-order predicate logic as the specication language, previously reported
by Banerjee and Chandrasekaran (2007).
Operators. The language recognizes a set of boolean operators {, , }, a set of arithmetic operators {+, , , }, a set of relational operators {<, >, =, =}, and the quantiers
{, }. The brackets () are used to express precedence while the brackets {} are used to
express a set. In this paper, we will often use certain combination of operators, such as, ,
, , etc. for the sake of brevity.
385

fiBanerjee & Chandrasekaran

Domain. The language allows the problem solver to specify the domain as a set from
which the variables can assume values. Unless otherwise stated, the domain is the real plane
2 for a point variable and the real line  for a non-diagrammatic variable.
Functions. Further, the language recognizes two functions  M aximize(f, {x, y, ...}, C)
and M inimize(f, {x, y, ...}, C), which maximizes and minimizes the function f with respect
to the variables {x, y, ...} satisfying the boolean combination of constraints C (which might
involve quantiers) and returns the maximum and minimum value of f respectively along
with the conditions on the variables.
Quantified Constraint Satisfaction Problem. An instance of a constraint satisfaction problem (CSP) consists of a tuple < V, D, C > where V is a nite set of variables,
D is a domain, and C= {C 1 , ...C k } is a set of constraints. A constraint C i consists of a pair
< S i , Ri > where S i is a list of mi variables and Ri is a mi -ary relation over the domain D.
The question is to decide whether or not there is an assignment mapping each variable to
a domain element such that all the constraints are satised. All of the variables in a CSP
can be thought of as being implicitly existentially quantied.
A useful generalization of the CSP is the quantied constraint satisfaction problem,
where variables may be both existentially and universally quantied. An instance of the
QCSP consists of a quantied formula in rst-order logic, which consists of an ordered list
of variables with associated quantiers along with a set of constraints. A QCSP can be
expressed as follows:
(v1 , ...vm )  Q(xn , ...x1 ) (v1 , ...vm , x1 , ...xn )
Q(xn , ...x1 )  Qn xn , ...Q1 x1
where Qi  {, }, {x1 , ...xn } is the set of quantied variables, {v1 , ...vm } is the set of free
variables, V= {v1 , ...vm , x1 , ...xn }, and  is a quantier-free expression called the matrix.
Such representation of a quantied expression , where it is written as a sequence of quantiers followed by the matrix, is referred to as prenex form. Example of a QCSP is as follows:
Subcurveof (c1 , c)  p, On(p, c1 )  On(p, c)
where c1 , c are curves in 2 . In this example, there are two constraints:
< {p, c1 }, On >
< {p, c}, On >
Further, V= {p} and D = 2 . The variables c, c1 are given. The question here is to
decide whether there is an assignment mapping p to an element in 2 such that the logical
combination of constraints is not satised. If such an assignment exists, then c1 is not a
subcurve of c; otherwise it is.
Decision, Function and Optimization problems. In the proposed specication
language, a spatial problem  is expressed as a QCSP where V consists of variables of type
point, curve or region and D = 2 . Solving a spatial problem involves:
1. When there are no free variables in V (i.e., all variables in V are quantied), deciding
whether or not there exists a mapping from V to D satisfying C.
386

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

2. When there are free variables in V, computing the conditions on the free variables
such that a mapping from V to D satisfying C exists.
Thus, a spatial problem can be classied as a decision or a function or an optimization
problem in the real domain. The rst case constitutes a decision problem and yields a True
or False solution. The second case constitutes a function problem which involves computing
the diagrammatic object(s) described by the conditions on the free variables. If a spatial
problem requires computing the best mapping from V to D satisfying C, it is called an
optimization problem.
Let us consider an example. Given a curve c and two points p, q, the spatial problem
BehindCurve(q, c, p) is dened as deciding whether or not q is behind c with respect to
p. This might be specied as deciding whether or not the curve c and line segment {p, q}
intersect. Thus,
BehindCurve(q, c, p)  Intersect(c, {p, q})
For particular instances of q, p, c, the solution to this problem is T rue or F alse, hence
it is a decision problem (see Figure 8). For particular instances of p, c, and generalized
coordinates of q i.e., q  (x, y), the solution to the same problem is a logical combination
of conditions involving x and y, which when plotted constitutes a region object (see Figure
9). Hence, it is a function problem. While a decision problem merely requires checking
whether or not a given instance of an object satises the constraints or not, a function
problem requires computing all conditions for a general object to satisfy the constraints.

Figure 8: The BehindCurve as a decision problem. One of the points q is behind c with
respect to p while the other one is not.

Again, given a curve c and two points p, q, the spatial problem F urthestBehindCurve(q,
c, p) is dened as deciding whether or not q is the furthest point behind c with respect to
p. This might be specied as deciding whether or not q lies behind c with respect to p and
387

fiBanerjee & Chandrasekaran

Figure 9: The BehindCurve as a function problem. The shaded region r is behind c with
respect to p.

distance between p and q is maximum. Thus,
F urthestBehindCurve(q, c, p)  BehindCurve(q, c, p)  b, BehindCurve(b, c, p) 
CompareDistance(b, p, q, p)
CompareDistance(a, b, c, d)  Distance(a, b)  Distance(c, d)
For particular instances of q, p, c, the solution to this problem is T rue or F alse, hence it
is a decision problem. For particular instances of p, c, and generalized coordinates of q i.e.,
q  (x, y), the solution to the same problem is a logical combination of conditions involving
x and y, which when plotted constitutes a single point object, assuming there is only one
furthest point behind c with respect to p, which is dependent on the nature of c and how
the Distance function is dened (see Figure 10).
An alternative way of specifying the same problem F urthestBehindCurve(q, c, p) is by
explicitly asking to maximize the distance between p and q where q satises the constraint
BehindCurve(q, c, p), written as:
F urthestBehindCurve(q, c, p)  M aximize(Distance(q, p), {q}, BehindCurve(q, c, p))
This outputs the conditions involving x and y, which constitutes a single point object. The
M aximize (or M inimize) function assumes the pool of candidates from which to choose the
best are those that satisfy the set of constraints. This fact has to be stated explicitly if not
using the M aximize (or M inimize) function which makes the specication more dicult
to come up with and also cumbersome. On the ip side, the specication of a problem
using the M aximize (or M inimize) function cannot be used as a decision problem. That
is, whether or not a particular instance of an object is the best candidate that satises the
388

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

q

Figure 10: The F urthestBehindCurve as an optimization problem. The point q is the
furthest point behind c with respect to p.

constraints cannot be computed from this specication, unlike the former specication. A
problem of this type, which computes the best candidate out of a pool of candidates, is
called an optimization problem.
Definition 6. Spatial Problem. A spatial problem (or problem) is a QCSP where a
variable (quantified or free) can only be of type point, and the domain is 2 .
Thus, a spatial problem  is a mapping from a diagram D satisfying a logical combination of constraints C to a set of booleans {T rue, F alse} or real numbers  or diagrammatic
objects D , i.e.,
C

 : D{T rue, F alse}  D
Solving a spatial problem requires eliminating the quantiers and solving algebraic equations/inequalities to arrive at the most simplied expression. The computational bottleneck
in solving a spatial problem is quantier elimination (QE) which is inherently doubly exponential (Davenport & Heintz, 1988). More recently, Brown and Davenport (2007) have
shown that real QE is doubly-exponential even when there is only one free variable and all
polynomials in the quantied input are linear. In this paper, we will concentrate primarily on QE as part of spatial problem solving and hence, our solution will be an equivalent
quantifier-free expression but not necessarily the most simplified one. Theoretically, the
best complexity for QE achieved so far is O(s(l+1)(ki +1) d(l+1)ki ) where s is the number
of polynomials, their maximum degree is d and coecients are real, l is the number
 of free
variables, ki is the number of variables in the ith quantier block while k =
ki is the
number of quantied variables (Basu, Pollack, & Roy, 2003). However, this algorithm is
too complicated to yet have a practical implementation. The most general and elaborately
implemented method for real QE is the cylindrical algebraic decomposition or CAD (Collins
389

fiBanerjee & Chandrasekaran

k1

& Hong, 1991), complexity of which is (sd)O(1) . Another implemented method, QE by
virtual substitution (Weispfenning, 1988), is restricted to formulas in which the quantied
variables occur at most quadratically. The complexity of this method is doubly exponential
in the number of blocks of variables delimited by alternations of the existential and universal quantiers. Thus, while there exist general algorithms for QE, for large real-world
problems, it soon becomes too time consuming.

3. Spatial Problem Solver
In this paper, we concentrate on developing an ecient SPS without sacricing its generality.
The goal of our design of the SPS is to bypass the general QE algorithms as much as possible,
either by taking the help of previously solved similar problems in memory to obtain the
solution or by using a set of more practical algorithms each of which is developed for a
limited class of problems. Here we describe the overall control mechanism of the SPS (see
Figure 11).
In many domains, such as, military, spatial problems involve diagrammatic objects that
are arbitrary shaped (e.g., mountainous regions) and often cannot be approximated enough
by well-dened shapes so that the solution can reliably depend on the specics of the shape.
For example, the solution to the problem of nding all places behind a mountain where one
can hide from the enemy depends critically on the particular shape of the mountain. Due
to such nature of domains, we choose to represent curves piecewise-linearly and regions
as polygons. Piecewise-linear curves and polygonal regions are unions of line segments
and triangular regions respectively, which facilitate decomposition of complex problems
into simpler and similar subproblems. We observe that similar subproblems involving
both existential and universal quantiers occur regularly in the spatial problem solving
process which are solved by one of the QE algorithms (e.g., CAD), thereby incurring doubly
exponential time. We minimize this enormous computational cost by reusing the solutions
of subproblems previously solved.
Given a spatial problem  in the specication language, the SPS replaces numerical values in the problem by symbolic variables, and then transforms the symbolic problem from
specication to a modeling language (to be described shortly) by progressively replacing
objects/predicates by base objects/predicates in their internal denitions. If a denition
cannot be found, it ags an error and halts till provided. As a rst step, the SPS decomposes  into disjunctions and/or conjunctions of subproblems i in prenex form. As we will
see later, all of these subproblems i are similar to each other in that if one of them can be
solved, solution to any of the others can be computed from it. Next, it searches the memory
for problems similar to i . The memory contains symbolic problems and their corresponding quantier-free symbolic solutions. If i can be mapped to one of these problems, its
solution is readily obtained by reverse-mapping from the corresponding symbolic solution
in memory. Obtaining a solution in such a way completely bypasses the QE process, which
is the computational bottleneck of the SPS, thereby reducing the computational costs considerably. If SPS cannot map i to any problem in memory, it sends i to the problem
classier that classies and sends it to the appropriate QE algorithm. The problem classier and combination of QE algorithms have been borrowed from Mathematica (Wolfram,
2003). Once the SPS solves a new subproblem, the subproblem and its solution are stored
390

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Problem I in
specification language

Convert problem to modeling language: Search
vocabulary and replace terms in specification by their
definitions, if exists; otherwise request their definition

Problem I in
modeling language
Decompose problem I into conjunctions and/or
disjunctions of subproblems in prenex form

For the first subproblem I1 ,
search memory for a
similar subproblem

Match found

Memory

Match not found

Compute the solution of
subproblem I1 with the
help of the solution to
the matched problem

Problem classifier and
combination of constraint
solvers and quantifier
elimination algorithms

Subproblem I1
and its
solution

Compute solutions of other subproblems
from the solution of I1 and combine them

Solution

Figure 11: Flow diagram of our spatial problem solver.
in memory so that the solution can be used when a similar problem is encountered in future.
Thus, the SPS grows more ecient as it solves more problems. Finally, the SPS computes
the solution to the given problem  by combining the solutions of all its subproblems.
Unfortunately, for some problems, quantiers cannot be eliminated symbolically in reasonable time. The SPS tries for a prescribed time, after which it resorts to more practical
391

fiBanerjee & Chandrasekaran

methods, such as, techniques especially suited for low degree polynomials (e.g., Dolzmann,
Sturm, & Weispfenning, 1998) and approximate methods for obtaining a subset of the solution sucient for immediate purposes (e.g., Ratschan, 2006; Lasaruk & Sturm, 2006). It
has been shown, in integer linear programming (e.g., Leyton-Brown, Nudelman, & Shoham,
2002) and satisability testing (e.g., Xu, Hutter, Hoos, & Leyton-Brown, 2008), that the
best on-average solver can be out-performed by carefully exploiting a portfolio of possibly
poorer on-average solvers, and accordingly, researchers have experimented with dierent
ways of selecting a portfolio of solvers (see for example, Xu et al., 2008; Pulina & Tacchella,
2007; Sayag, Fine, & Mansour, 2006; Streeter, Golovin, & Smith, 2007; Gebruers, Hnich,
Bridge, & Freuder, 2005; OMahony, Hebrard, Holland, Nugent, & OSullivan, 2008). As
none of these work involve solving QCSPs over the real domain, they are not directly usable
for our purposes and will not be further discussed in this paper. However, we do expect
the same result to extend to QCSP solvers over the real domain, and building and smartly
selecting from a portfolio of QCSP solvers is a promising line of future research. In our
approach, once a subproblem is deemed symbolically unsolvable in the prescribed time,
its specication is stored in memory so that in future, a similar problem can be directly
subjected to practical methods, thereby saving the prescribed time.
3.1 Modeling Language
This is the language in which a problem is described in terms of the underlying representations of objects/properties/relations in a form that can be readily subjected to algebraic
manipulation. The location of a point p is represented as a pair (x, y), x, y  , as its
coordinates.
Notation. The x- and y-coordinates of a point p are denoted by p.x and p.y respectively.
The distance between two points, p and q, is given by
Distance(p, q) 



(p.x  q.x)2 + (p.y  q.y)2

The location of a curve c is represented by the sequence of vertex points {p1 , p2 , ...pn }.
Notation. The number of vertex points in a curve c is denoted by #(c), the ith vertex
point is denoted by c[i], and the ith line segment is denoted by {c[i], c[i + 1]}.
A line segment ls is specied by its pair of vertex (or terminal) points. The x- and
y-coordinates of ls are represented parametrically as
fx (ls, t)  ls[1].x + t  (ls[2].x  ls[1].x)
fy (ls, t)  ls[1].y + t  (ls[2].y  ls[1].y)
where t is a parameter, 0  t  1. The relation On(p, ls), where p is a point, is given by
On(p, ls)  t, 0  t  1  fx (ls, t) = p.x  fy (ls, t) = p.y
Length of a line segment ls is given by

392

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Length(ls)  Distance(ls[1], ls[2])
Length of a curve c is given by


#(c)1

Length(c) 

Length({c[i], c[i + 1]})

i1

The location of a region r is represented by the location of its periphery which is a
piecewise linear closed curve. As discussed in section 2.1, internally a region is triangulated
(computable in linear time as shown in Chazelle, 1991; Seidel, 19912 ) with the aim of
reducing and simplifying computations (more on this in section 3.2).
Notation. On triangulation, the number of triangles in a region r is denoted by # (r)
while the ith triangle in r is denoted by (r)[i].
The area of a triangle  is given by
Area() 

1
2

3


[i].x  [i\3 + 1].y  [i\3 + 1].x  [i].y

i1

Note that the area of a triangle is positive if the sequence of vertex points on its periphery are
given in a counter-clockwise direction, otherwise it is negative. Area of a region r is given by
# (r)

Area(r) 



Area((r)[i])

i1

The relation Inside(p, ), where p is a point and  is a triangle, is given by
Inside(p, )  3i1 Lef tof (p, {[i], [i\3 + 1]})
Lef tof (p, ls)  Area({ls[1], ls[2], p}) > 0
where ls is a line segment.
The action T ranslate(q, c, tx , ty ) where q  (x, y), c is a curve, and tx , ty are real numbers, is given by
T ranslate(q, p, tx , ty )  q.x = p.x + tx  q.y = p.y + ty
T ranslate(q, c, tx , ty )  a, On(a, c)  T ranslate(q, a, tx , ty )
Definition 7. Base Object. A base object is the simplest form of a diagrammatic object.
A point is its own simplest form. A line segment is the simplest form of a curve. A
triangular region is the simplest form of a region. Thus, internally, there are three base
objects  point, line segment, triangle.
Definition 8. Base Predicate. A base predicate is a predicate which accepts only the base
objects as arguments.
2. Vik (2001) discusses an implementation in Mathematica.

393

fiBanerjee & Chandrasekaran

Examples of base predicates include Lef tof (p1 , p2 ), Between(p1 , p2 , p3 ), On(p, ls),
Inside(p, ), where p, p1 , p2 , p3 are points, ls is a line segment,  is a triangular region.
#(c)1

Lemma 1. On(p, c)  i1

On(p, {c[i], c[i + 1]})

Proof. The proof follows from our representation of a curve, as described in section 2.1.
# (r)


Lemma 2. Inside(p, r)  i1
Inside(p, (r)[i])

Proof. The proof follows from our representation of a region, as described in section 2.1.
The relations included in the vocabulary are internally dened in terms of base predicates. For example, the predicate, Intersect(c1 , c2 ) where c1 , c2 are curves, is dened in
terms of base predicates as
Intersect(c1 , c2 )
 a, On(a, c1 )  On(a, c2 )
#(c )1

 a, i11

#(c )1

On(a, {c1 [i], c1 [i + 1]})  j12

On(a, {c2 [j], c2 [j + 1]})

3.2 Decomposing a Problem
Definition 9. Decomposition. Decomposition is the process of replacing the relational
predicates, involving free variables of types curve and region, in a spatial problem (quantified expression) by conjunctions/disjunctions of base predicates and taking those conjunctions/disjunctions to the front of the expression. The expression following those conjunctions/disjunctions is a subproblem.
Example. Decomposition of the problem Intersect(c1 , c2 )  a, On(a, c1 )  On(a, c2 )
occurs as follows:
Intersect(c1 , c2 )  a, On(a, c1 )  On(a, c2 )
#(c )1

 a, i11

#(c )1

On(a, {c1 [i], c1 [i+1]})j12

On(a, {c2 [j], c2 [j+1]}) (bef ore decomposition)

#(c )1 #(c2 )1
j1 a, On(a, {c1 [i], c1 [i+1]})On(a, {c2 [j], c2 [j+1]})

 i11

#(c )1

 i11

#(c )1

j12

(af ter decomposition)

Intersect ({c1 [i], c1 [i + 1]}, {c2 [j], c2 [j + 1]})

where Intersect ({c1 [i], c1 [i + 1]}, {c2 [j], c2 [j + 1]}) is a subproblem. However, the question
#(c )1 #(c )1
arises  is Intersect(c1 , c2 )  i11 j12 Intersect ({c1 [i], c1 [i+1]}, {c2 [j], c2 [j +1]})?
That is, can we replace the  by a ?
Theorem 1. A problem is equivalent before and after decomposition if and only if it does
not contain the following forms:
394

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

F1:

p, On(p, c)  Inside(p, r)

F2:

p, On(p, c)  Inside(p, r)

F3:

p, Inside(p, r)  Inside(p, r1 )

F4:

p, Inside(p, r)  Inside(p, r1 )

(complement of F1)

(complement of F3)

where c is a curve, r, r1 are regions, and c, r, r1 are free variables.
Proof. As discussed in section 2.2, in our framework, On(p, c) and Inside(p, r) are the two
fundamental relations using which any other relation involving a curve or region is specied.
Also, in our framework, point is the only quantiable variable, {, } are the only quantiers, and {, , } are the boolean operators sucient to express any boolean expression.
Thus, any spatial problem involving only curves (and points but no regions) is a logical
combination of smaller problems of the following form:
Qp, Rel(p, c) where Q  {, }, Rel  {On, On}
Any spatial problem involving only regions (and points but no curves) is a logical combination of smaller problems of the following form:
Qp, Rel(p, c) where Q  {, }, Rel  {Inside, Inside}
Any spatial problem involving both curves and regions (and points) is a logical combination
of smaller problems of the following form:
Qp, Rel1 (p, c)  Rel2 (p, r)
where Q  {, }, Rel1  {On, On}, Rel2  {Inside, Inside},   {, }
Any spatial problem involving two curves, c and c1 , (and points) is a logical combination
of smaller problems of the following form:
Qp, Rel1 (p, c)  Rel2 (p, c1 )
where Q  {, }, Rel1  {On, On}, Rel2  {On, On},   {, }
Again, any spatial problem involving two regions, r and r1 , (and points) is a logical combination of smaller problems of the following form:
Qp, Rel1 (p, r)  Rel2 (p, r1 )
where Q  {, }, Rel1  {Inside, Inside}, Rel2  {Inside, Inside},   {, }
We symbolically solved each of the above problems (56 total) in two ways  directly and by
decomposing  for p  (x, y), c  {p1 , p2 , ...pn } (n  2), pi  (xpi , ypi ), c1  {a1 , a2 , ...au }
395

fiBanerjee & Chandrasekaran

(u  2), ai  (xai , yai ), r  {q1 , q2 , ...qm } (m  3), qi  (xqi , yqi ), and r1  {b1 , b2 , ...bv }
(v  3), bi  (xbi , ybi ). It turned out that the solutions from the two ways were equivalent
for all problems, except the four cases stated in the theorem statement. Note that F 2 is
the specication for computing whether a curve c is entirely inside a region r or not. Let
lsi be the ith line segment of c (1  i  n  1) and j be the j th triangular region of r
(1  j  m  2). We found
p, On(p, c)  Inside(p, r)
m2
 p, (n1
i1 On(p, lsi ))  (j1 Inside(p, j ))
m2
 p, n1
i1 j1 (On(p, lsi )  Inside(p, j ))
m2
 n1
i1 j1 (p, On(p, lsi )  Inside(p, j ))

That is because, for c to be entirely inside r, it is not necessary for all line segments of c to
be inside a triangle of r; a line segment of c can span across multiple triangles of r and c
can still be inside r. Figure 12(a) shows an example where c is inside r but a line segment
of c spans across two triangles of r. In such a case, the solution to the problem will be
T rue when computed directly but will be F alse when computed via decomposition. The
rst case F 1 in the theorem statement can be explained similarly. The forms F 1 and F 2
can be rewritten as follows:
F1 :

p, On(p, c)  Inside(p, r)
 p, On(p, c)  Inside(p, r) where r  B  r

F2 :

p, On(p, c)  Inside(p, r)
 p, On(p, c)  Inside(p, r) where r  B  r

where B is the rectangular region (boundary) containing the diagram as discussed in section
1.2. Note that each of the rewritten forms is equivalent before and after decomposition.
Again, F 4 is the specication for computing whether a region r is entirely inside a region
r1 or not. Let 1,i be the ith triangle of r1 (1  i  v  2) and j be the j th triangular
region of r (1  j  m  2). We found
p, Inside(p, r)  Inside(p, r1 )
v2
 p, (m2
j1 Inside(p, j ))  (i1 Inside(p, 1,i ))
v2
 p, m2
j1 i1 (Inside(p, j )  Inside(p, 1,i ))
v2
 m2
j1 i1 (p, Inside(p, j )  Inside(p, 1,i ))

396

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

r

r
c

(a) Curve c is inside region r but each line
segment of c is not inside one triangle of r.

r1

(b) Region r is inside region r1 but each triangle of r is not inside one triangle of r1 .

Figure 12: Examples to show the decomposition of curves and regions for problems containing the forms F 1, F 2, F 3, F 4.

That is because, for r to be entirely inside r1 , it is not necessary for all triangles of r to
be inside a triangle of r1 ; a triangle of r can span across multiple triangles of r1 and r can
still be inside r1 . Figure 12(b) shows an example where r is inside r1 but a triangle of r
spans across two triangles of r1 . In such a case, the solution to problem will be T rue when
computed directly but will be F alse when computed via decomposition. The third case
F 3 in the theorem statement can be explained similarly. The forms F 3 and F 4 can be
rewritten as follows:
F3 :

p, Inside(p, r)  Inside(p, r1 )
 p, Inside(p, r)  Inside(p, r1 ) where r1  B  r1

F4 :

p, Inside(p, r)  Inside(p, r1 )
 p, Inside(p, r)  Inside(p, r1 ) where r1  B  r1

Again, each of the rewritten forms is equivalent before and after decomposition.

Theorem 2. Any subproblem resulting from decomposing a problem contains base predicates
only.
Proof. A problem  is decomposable due to the presence of relational predicates, involving
free variables of types curve and region, in its specication. As stated in section 2.2, a
problem involving a curve or region can be specied in our framework using a relation(s)
involving only points and the relation On or Inside. Thus, a relation Rel(q, c) involving a
point q and a curve c can be rewritten as:
Rel(q, c)  a, On(a, c)  Rel (q, a)
or
397

fiBanerjee & Chandrasekaran

Rel(q, c)  a, On(a, c)  Rel (q, a)
where Rel is a base predicate involving the points q and a. In both cases, the expression
on the right-hand side of  contains base predicates only. Let  be a problem involving
points and curves but no regions. Let us replace in  each occurrence of non-base predicates
involving a curve, such as, Rel(q, c), by their equivalent expression consisting of On and
base predicates involving points and line segments only. Then the resulting expression for 
consists of base predicates involving points only and On. By lemma 1, a non-base On can
be rewritten as disjunctions of base On. Therefore, the resulting expression for  consists
of base predicates involving points only.
Similarly, a relation Rel(q, r) involving a point q and a region r can be rewritten as:
Rel(q, r)  a, Inside(a, r)  Rel (q, a)
or
Rel(q, r)  a, Inside(a, r)  Rel (q, a)
where Rel is a base predicate involving the points q and a. If  is a problem involving
points and regions but no curves, replacing each occurrence of non-base predicates involving
a region, such as, Rel(q, r), by their equivalent expression consisting of Inside and base
predicates involving points only, and then using lemma 2, results in an expression for 
consisting of base predicates involving points and triangular regions only. Both the above
processes can be employed when  involves both curves and regions. Thus, any subproblem
resulting from decomposing a problem will contain base predicates only.
3.3 Mapping to a Similar Problem
Definition 10. Similarity. We define two spatial problems (quantified expressions) to be
similar if there exists a one-to-one correspondence between their variables (free and quantified).
Given two similar problems, 1 and 2 , and the solution 1 of 1 , the goal is to construct
a one-to-one mapping  between the variables of 1 and 2 such that the solution of 2
can be obtained by replacing the variables in 1 by the corresponding variables, thereby
completely bypassing the QE process  the computational bottleneck of SPS. The one-toone mapping exists if 1 and 2 are logically equivalent. However, equivalence checking
for logical expressions is NP-hard (Dershowitz & Jouannaud, 1990; Goldberg & Novikov,
2003). Thus, equivalence checking cannot be used to determine similarity eciently.
Problem features. Let  be the quantier free expression when  is expressed in
prenex form, i.e.,
(v1 , ...vm )  Q(xn , ...x1 ) (v1 , ...vm , x1 , ...xn )
where no variable xi appears more than once in Q and Q contains no redundant variables.
A quantier block qb of Q is a maximal contiguous subsequence of Q where every variable
in qb has the same quantier type. The quantier blocks are ordered by their sequence of
appearance in Q; qb1  qb2 i qb1 is equal to or appears before qb2 in Q. Each quantied
398

fiExecuting Perceptions and Actions in Diagrammatic Reasoning



...
P1

V

P2

P3

V

P4



P6

P5
Figure 13: Parse tree for the matrix of a problem in conjunctive normal form.

variable xi in  appears in some quantier block qb(xi ), and the ordering of the quantier blocks imposes a partial order on the quantied variables. The variables in the same
quantier block are unordered.
Let 1  Q1 1 and 2  Q2 2 while 1 and 2 be the parse trees for 1 and 2 respectively. For example, the matrix  of a problem  in conjunctive normal form might look like:
  P1  (P2  P3 )  (P4  P5  P6 )  ...
where each Pi is a predicate. If  is a subproblem, each Pi is a base predicate. The parse
tree of the above sentence is shown in Figure 13.
Two trees, 1 and 2 , are isomorphic if there exists a bijection  : 1  2 that preserves adjacency and root vertex, i.e., (u) is adjacent to (w)  u is adjacent to w, and
(root(1 )) = root(2 ). It follows that two isomorphic trees have the same maximum height
and the same number of vertices at any height. Let l be the maximum height of  and i
be the number of vertices at height i. The function , dened as

(< 1 , ... >) =




i i

i1

where i is an integer, <> denotes a sequence, and i is the ith smallest prime number,
maps a sequence of integers to an unique integer. From a problem , a tuple () can be
constructed as follows:

399

fiBanerjee & Chandrasekaran

() =

< l,
(# vertices at dierent heights of parse tree),
# quantier blocks,
order of quantier blocks,
(# variables in dierent quantier blocks) >

Definition 11. Structural Equivalence. Two spatial problems (quantified expressions),
1 and 2 , are structurally equivalent if they satisfy all of the following conditions:
1. (1 ) = (2 )
2. 1 and 2 are isomorphic to each other, where 1 and 2 are the parse trees of the matrices
of 1 and 2 respectively.
3. The contents (predicate or boolean operator {, , }) of each pair of corresponding nodes
of 1 and 2 are identical.
4. There exists a one-to-one correspondence between the variables in the arguments of
predicates contained in each pair of corresponding nodes of 1 and 2 . Moreover, any two
mappings obtained from two pairs of corresponding nodes of 1 and 2 do not contradict
each other.
As we will see in section 4, structural equivalence of two problems can be computed
in time linear in the size of their parse trees. Note that if two problems are structurally
equivalent, they will be logically equivalent but not vice versa. For example, the expressions (P  P )  Q and Q, where P and Q are base predicates, are logically equivalent, but
not structurally equivalent since their parse trees are not isomorphic. In general, logical
equivalence does not imply structural equivalence when there are redundancies (redundant
variables and/or predicates) in one or both problems. For the sake of computational eciency, we will use structural equivalence to determine the similarity of two problems.
Theorem 3. All subproblems obtained by decomposing a problem are always similar.
Proof. Let us assume, for a contradiction, there exists a problem  that decomposes into
subproblems such that two of them, j and k , are dissimilar. Without loss of generality,
assume that all subproblems of  except k are similar to j . Then,

n

 (ni11=1 ni22=1 ... ipp=1 Qi1 i2 ...ip i1 i2 ...ip )  Qk k ,   {, },
j  {i1 i2 ...ip |0  i1  n1 , 0  i2  n2 , ...0  ip  np },
Qj  {Qi1 i2 ...ip |0  i1  n1 , 0  i2  n2 , ...0  ip  np }
n

 ni11=1 ni22=1 ... ipp=1 (Qi1 i2 ...ip Qk )(i1 i2 ...ip  k )
 ni11=1 ni22=1 ... ipp=1 Qi1 i2 ...ip i1 i2 ...ip
n

Thus, all subproblems of  are similar which contradicts our assumption. Hence the proof
follows.
Intuitively, in the proposed framework, while a curve is represented by an arbitrary number of vertices, a line segment is always represented by its two end points. Similarly, while
400

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

the periphery of a region is represented by an arbitrary number of vertices, the periphery
of a triangular region is always represented by three vertices. Hence, two line segments or
triangular regions are always represented similarly and dier only in the coordinates of their
constituent vertices, unlike two curves or regions. The base predicates are dened in terms
of base objects  points, line segments, and triangular regions. Thus, when a predicate is
dened as conjunctions or disjunctions of base predicates, the base predicates are always
similar. Decomposition of a problem into subproblems merely replaces one or more of its
predicates by similar base predicates. Hence, all the subproblems have to be similar.
3.4 Memory Organization
Memory in the SPS is hierarchically organized and stores problems in disjoint classes based
progressively on a problems features in  (see Figure 14). After decomposing a problem
into subproblems and computing their , if the subproblems have the same value for ,
SPS checks whether or not their parse trees are isomorphic and a mapping exists between
their variables. Since the memory hierarchy has a constant height, insertion of a problem or
searching for a potential class of similar problems can be executed in constant time. Also,
the features that classify the problems are discriminative enough to create a large number
of classes (leaf nodes), each class containing only a few problems, thereby reducing search
to a few problems belonging to a class.

4. Computational Complexity
We will now analyze the time complexity of the algorithms used in our framework. In our
implementation, a problem  is a data structure consisting of two elds  P arseT ree and
Solution. The P arseT ree stores the lexicographically sorted parse tree of the matrix of
 while the Solution stores the symbolic solution to  in a concise form. A parse tree
can be constructed in time O(t) where t is the number of base predicates and boolean
operators {, , } in . The boolean operators occupy the non-leaf nodes in the parse tree
while the base predicates occupy the leaf nodes. Lexicographically sorting a tree requires
lexicographically sorting the contents of the children of each non-leaf node in the tree. Let
t be the number of boolean operators and ti be the number of children of the ith boolean
t

operator. Thus,
ti = t  1. Note that since each base predicate is always followed by
i1

a boolean operator, t = t where  is a constant. Lexicographically sorting a list of the
contents of the children of a node requires O(ti logti ) time. Thus, the total time required
t

for repeating this process for all non-leaf nodes is
O(ti logti ). Since the average number
i1



of children per node is

i1



tree is

t

i1

1
t

t


O(

ti =

t1
, the total time required to lexicographically sort a
t

t1
t1
log  ) = O(t).

t
t
401

fiBanerjee & Chandrasekaran

l
l1

...

lk

[ (# vertices at different

[ (# vertices at different

levels of parse tree)

levels of parse tree)

D

1

...

Dm

#(qb)

q1

...

#(qb)

qr

...

order of qbs

o1

...

...

order of qbs

os

...

[ (# variables in

[ (# variables in

different qb's)

different qb's)

E

1

...

Et

...

Problems

Problems

Figure 14: Hierarchical problem classication in memory. At each height in the hierarchy,
the branches correspond to the dierent values of the features captured by .
For example, l1 through lk correspond to the k dierent maximum heights of
parse trees for the matrices of spatial problems.

Given two problems  1 , 2 , the algorithm Similar(1 , 2 ) computes whether 1 and
2 are similar to each other or not (see Figure 15). Since computing  requires O(t) time,
line 1 requires O(t) time. Since checking whether two trees are isomorphic or not requires
O(t) time (as shown in Aho, Hopcroft, & Ullman, 1974), line 6 requires O(t) time. Lines 9
through 11 requires O(t) time. Thus, the algorithm runs in O(t) time.
Given an unsolved problem  and a similar solved problem similar , the algorithm
ComputeSolutionF romSimilarP roblem(, similar ) computes the solution to  by variable mapping from similar (see Figure 16). V ariableM ap is a list where each entry is
a pair < v, vsimilar >, v being a free variable in  and vsimilar is the corresponding free
variable in similar . Let the size of V ariableM ap be k  . The lines 5 through 11 requires
O(tk  ) time since the number of nodes in similar is t and the number of arguments in any
predicate is small. Lines 12 through 13 requires O(k  ) time where  is the size of the
solution to similar . Thus, the algorithm runs in O(tk  + k  ) time.
Finally, given an unsolved problem  and a memory M emory that stores problems hierarchically (as described in section 3.4), the algorithm EliminateQuantif iers(, M emory)
computes the solution to  by variable mapping from a similar problem in M emory, if such
a problem exists; otherwise solves  using a problem classier and combination of constraint
402

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Similar(1 , 2 )
1. if (1 ) = (2 ),
2.
return F alse
3. else
4.
1  1 .P arseT ree
5.
2  2 .P arseT ree
6.
if Isomorphic(1 , 2 ) = F alse,
7.
return F alse
8.
else
9.
for each node in 1
10.
if the predicate or boolean operator at the corresponding node in 2 does
not match,
11.
return F alse
12. return T rue

Figure 15: Algorithm for deciding whether two problems  1 , 2  are similar or not by
computing structural equivalence. A problem is a quantied expression.

ComputeSolutionF romSimilarP roblem(, similar )
1.   .P arseT ree
2. similar  similar .P arseT ree
3. similar  similar .Solution
4.   similar
5. for each node in similar
6.
if the node contains a predicate (say P ),
7.
for j  1 to # arguments in P
8.
v  variable occupying j th argument of P in 
9.
vsimilar  variable occupying j th argument of P in similar
10.
if V ariableM ap.Contains(v) = F alse,
11.
V ariableM ap.Add(< v, vsimilar >)
12. for i  1 to |V ariableM ap|
13.
Replace all occurrences of V ariableM ap[i, 2] in  by V ariableM ap[i, 1]
14. return 

Figure 16: Algorithm for computing the solution to a problem  by mapping variables
from a similar problem similar . The problem is a quantied expression and the
solution is the equivalent quantier-free expression.

solvers and QE algorithms (as described in section 3). The algorithm is shown in Figure
17.
403

fiBanerjee & Chandrasekaran

Let there be n subproblems to a problem. In a problem, some of the predicates are already base predicates while the rest are not which can be written as conjunctions/disjunctions
of base predicates thereby leading to decomposition of a problem into subproblems. For
example, in section 1.1, the problem RiskyP ortionsof P ath(q, c1 , c2 , d) is dened in terms
of the base predicate DistanceLessT han(p, q, d) (i.e., Distance(p, q)  d) and the nonbase predicates On(q, c1 ) and On(p, c2 ). Each of these non-base predicates can be written
as disjunctions of base predicates, such as, On(q, {c1 [i], c1 [i + 1]}) and On(q, {c2 [j], c2 [j +
1]}), respectively, thereby leading to decomposition of RiskyP ortionsof P ath into subproblems. Each of the subproblems inherits the base predicates from the problem (e.g.,
DistanceLessT han(p, q, d)) and also includes the new base predicates (e.g., On(q, {c1 [i], c1 [i+
1]}), On(q, {c2 [j], c2 [j + 1]})) obtained from the non-base predicates. Let  be the number
of polynomials in the base predicates of a problem and  be the number of polynomials due
to the newly obtained base predicates in a subproblem. Since all subproblems are similar,
each of them will have  +  polynomials. The total number of polynomials s in a problem
is O( + n).
Let d be the maximum degree of any polynomial in a subproblem. Since all subproblems
are similar, each of them will have maximum degree d. The maximum degree of polynomials
in a problem will also be d if objects are represented piecewise-linearly, in which case d  2.
If the objects are not represented piecewise-linearly, the degree will be much larger than
two which might lead to a situation where the problem might not be solvable in reasonable
time.
Let k be the number of quantied variables in a problem. Then each subproblem
also has k quantied variables. Let the computational complexity of using a general QE
algorithm for solving a problem be T (n) while that for solving a subproblem is T (1), where
k1
T is a doubly exponential function, such as, when using CAD, T (n) = (sd)O(1) . Note
that T (n)  nT (1), i.e., it is more ecient to solve each subproblem using a general QE
algorithm than to solve the whole problem using the same algorithm.
In algorithm EliminateQuantif iers(, M emory), lines 4 through 7 require O(n) time.
Lines 8 and 9 require O(t) time each. Since line 13 requires O(t) time, lines 11 through
16 require O(mt) time. Line 18 requires time T (1) while lines 20 through 23 require
O((n1)(tk  +k  )) time. Thus, the entire algorithm runs in O(T (1)+mt+(n1)(tk  +k  ))
time. Note that  is the size of the symbolic solution, and if the symbolic solution
can be expressed concisely,  is small. Since the number of boolean operators is of the
order of number of base predicates and each base predicate is dened in terms of at
least one polynomial, t = O(s) = O( + n). Thus, the complexity of the algorithm is
O(T (1) + (m + (n  1)k  )s + (n  1)k  ). It can be seen that
nT (1) > T (1) + (m + (n  1)k  )s + (n  1)k 
or,

(( + )d)O(1)

k1

m
> ( n1
+ k  )( + n) + k 

is true provided  is not large. That is, it is more ecient to solve a problem by variable
mapping than to solve each subproblem using a general QE algorithm provided the size of
the stored symbolic solution is not large. For every decomposable problem, the complexity
of QE can be reduced as above.
404

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

EliminateQuantif iers(, M emory)
1. .Solution  
2. Decompose  into subproblems i  Qi i , 1  i  n
p

np
n1
n2

nk
where   i1 =1 i2 =1 ... ip =1 Qi i , n =
k1

3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.

i1
for k  p to 1
for j  1 to nk
i  the ith operator ( or ) in decomposed  from right to left
ii+1
Construct the parse tree for the matrix 1
Compute ()
f lag  0
for j  1 to m
M emory,j  j th problem in M emory
if Similar(M emory,j , 1 ) = T rue,
similar  M emory,j
f lag  1
break loop
if f lag = 0,
1 .Solution  ComputeSolutionF romQEAlgorithms(1 )
similar  1
for i  (2  f lag + 2)\3 to n
i .Solution  ComputeSolutionF romSimilarP roblem(i , similar )
.Solution  .Solution i i .Solution
return .Solution

Figure 17: Algorithm for computing the solution to a spatial problem  taking the help
of previously solved similar problems in M emory, thereby bypassing quantier
elimination whenever possible. The problem is a quantied expression and the
solution is the equivalent quantier-free expression.

When a problem is encountered by the SPS for the rst time, it is solved by decomposing into subproblems, solving the rst subproblem using a general QE algorithm and
then obtaining the solution of the rest of the subproblems by mapping their variables to
the rst subproblem. Since a subproblem and its solution are stored in memory, if a similar
subproblem is encountered in future, the SPS bypasses the QE algorithm completely and
solves it by variable mapping. In such a case, line 18 of the algorithm is never executed,
and the time complexity of solving the problem is
(m + nk  )s + nk 

405

fiBanerjee & Chandrasekaran

which is a considerable savings compared to the complexity of solving the entire problem
k1
using a general QE algorithm (e.g., complexity of CAD is (sd)O(1) ), provided  is not
large. As the SPS solves more problems, the probability to encounter a similar problem in
memory increases thereby leading to the above scenario which incurs a complexity of low
order polynomial as compared to doubly exponential.3
Example. To illustrate the problem solving process, let us consider the spatial problem BehindCurve(q, c, p) (described in section 2.3). For a point p  (px , py ) and a curve
c  {p1 , p2 , ...pn } where pi  (xi , yi ) is a point, decomposition of the problem occurs as
follows:

 BehindCurve(q, c, p)
 Intersect(c, {p, q})
 a, On(a, c)  On(a, {p, q})
 a, (n1
i=1 On(a, {pi , pi+1 }))  On(a, {p, q})
 n1
i=1 (a, On(a, {pi , pi+1 })  On(a, {p, q}))

 n1
i=1 (Qi i )

 n1
i=1 i
Thus there are (n  1) subproblems i , where
i  On(a, {pi , pi+1 })  On(a, {p, q})
Qi  a
i  a, On(a, {pi , pi+1 })  On(a, {p, q})
From Figure 18, (i ) =< 2, 21 33 , 1, <  >, 21 > for i = 1, 2, ...n  1. By theorem 3,
all i s are similar since they are the subproblems of the same problem. If the SPS does
3. It should be noted that approximating a continuous curve by a sequence of line segments has its drawbacks. For example, a point p that is on a continuous curve c might not be on the piecewise-linear
approximation of c. The SPS can accept a parameter that specifies the maximum length of a line
segment to be used in the approximation. As of our current implementation, we leave the onus of determining this maximum length on the problem solver. In this context, it deserves mention that loss
of information is inevitable in almost any kind of approximation. For example, when the space in a
diagram is approximated by a finite number of pixels, as shown by Banerjee and Chandrasekaran (2010),
the diagrammatic objects lose certain spatial information that might be detrimental to spatial problem
solving which can be avoided by knowing the minimum allowable resolution (or maximum length of one
side of a square pixel).

406

fiExecuting Perceptions and Actions in Diagrammatic Reasoning



On(a,{p1,p2})

On(a,{p,q})

Figure 18: Parse tree for the matrix of the rst subproblem of the BehindCurve problem.

not nd a problem in memory similar to the rst subproblem 1 , it is sent to the problem
classier who sends it to the appropriate QE algorithm. The problem denition, its tuple
, parse tree, and solution are then stored in memory as follows:
1 (q, {p1 , p2 }, p)  a, On(a, {p1 , p2 })  On(a, {p, q})
1 ((x, y), {(x1 , y1 ), (x2 , y2 )}, (px , py ))
 (px  x < 0  px  x1  0  x1  x  0  py x1  py x + px y  x1 y  px y1 + xy1 = 0)  (x  px <
0  x1  px  0  x  x1  0  py x1  py x + px y  x1 y  px y1 + xy1 = 0)  ...
where the arguments of 1 are the free variables. The other subproblems are solved by
replacing the variables in 1 by the mapped variables. If a problem similar to 1 is found
in memory, 1 will also be solved by replacing the mapped variables, just as the other
subproblems.
Note that, for example, the BehindCurve problem, in the absence of an appropriate
vocabulary of properties/relations, would have been specied as (see redlog in Weispfenning, 2001):
BehindCurve((x, y), {(p1,x , p1,y ), (p2,x , p2,y ), ...(pn,x , pn,y )}, (px , py ))
 ax , ay , t, 0  t  1  px + t(x  px ) = ax  py + t(y  py ) = ay  n1
i1 (ti , 0  ti 
1  pi,x + ti (pi+1,x  pi,x ) = ax  py + ti (pi+1,y  pi,y ) = ay )
Here the total number of quantiers is n + 3, dependent on the number of line segments
forming the curve which can be huge for complicated curves as in many real-world applications. In our SPS, due to appropriate decomposition of problems into subproblems, the
number of quantiers in any subproblem is always xed (4 in this case) irrespective of the
spatial complexity of the object(s) (curve in this case). The symbolic solutions of these
simple subproblems can be stored for future use which is not possible in systems like redlog. Needless to say, though solving the problem using both the systems produce the same
solution, ours is much faster.
407

fiBanerjee & Chandrasekaran

5. Applications
In this section, we illustrate how the SPS can be deployed in conjunction with a problem
solver, human or articial (such as, soar), for solving spatial problems without human intervention as needed for DR. Two applications will be considered  entity re-identication
and ambush analysis  that are deemed very important in the military domain. The subproblems that the SPS autonomously decomposes each spatial problem into will be shown.
Problems in military domain involve a wide variety of objects with arbitrary properties and
relations, and hence, help to illustrate the expressiveness of the specication language and
the eciency and generality of the SPS.
For the implementation, we used bisoar, due to Kurup and Chandrasekaran (2007), a
bimodal version of soar (Laird et al., 1987), where the problem solver uses two kinds of operators  predicate-symbolic operators that are applied on information in predicate-symbolic
form and perception-like operators that are applied on a diagram  to bring about state
transitions to reach the goal state from an initial state. A human is responsible for providing
the broad problem solving strategy for a class of problems; given a specic problem from
that class, bisoar uses the predicate-symbolic and perception-like operators accordingly.
Since we have used bisoar in a number of dierent domains (e.g., military, Euclidean geometry, physics, civil engineering; Banerjee & Chandrasekaran, 2007 provide some examples)
and still continue to do so, it knows several dierent problem solving strategies and operators, both predicate-symbolic and perception-like. The emphasis of this section is not on
how eciently bisoar solves problems but on how eciently the perception-like operators
can be executed without incorporating any knowledge that jeopardizes the generality of a
general-purpose problem solver. For each spatial problem, we will compare the performance
of our proposed SPS with the CAD algorithm in terms of actual computation time which
is determined by taking the average of at least 10 runs. As we will see, the SPS excels by
a signicant margin in most cases.
5.1 Entity Re-identification
The entity re-identication problem is a core task in the US Armys All-Source Analysis
System (ASAS). ASAS receives a new report about sighting of an entity T3 of type T (e.g.
tanks). The task is to decide if the new sighting is the same as any of the entities in
its database of earlier sightings, or an entirely new entity. Reasoning has to dynamically
integrate information from dierent sources  database of sightings, mobility of vehicles,
sensor reports, terrain and map information  to make the decision. We will follow a novel
capability using failure of expectation: If H were true, O should have been observed, but
since it was not, H is likely not the case, where H and O are hypotheses and observations
respectively (Josephson & Josephson, 1996; Chandrasekaran et al., 2004). In the following,
we consider a simple version of the problem to illustrate how the task is solved using DR
and the spatial problems involved therein.
Figure 19(a) shows the terrain of interest  mountainous with the closed regions marking
impassable areas for entities of type T (e.g., tanks). Let T3 be an entity newly sighted at
time t3 located at point p3 while T1 , T2 are the two entities that were located at points p1 ,
p2 when last sighted at times t1 , t2 respectively. T1 and T2 were retrieved from the database
as having the potential to be T3 based on their partial identity information. Also, in the
408

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

T2

T1

T3

(a) Terrain, impassable regions, and sighted tanks.

(c) A short path between T1 and T3 .

(b) A short path between T2 and T3 .

(d) A path from the only plausible homotopy
class.

Figure 19: Reasoning steps for entity re-identication
area of interest, there are three enemy regions or obstacles {r1 , r2 , r3 } (as shown in Figure
19(a)) with a given repower/sight range d of the enemy. Reasoning proceeds as follows. If
T1 can reach p3 within the time t3  t1 , then T3 might be T1 . Similarly for T2 . Since each
mountainous region (or obstacle) is a hiding place for enemies with a repower range d,
the existence of an entity shows that it most probably did not traverse through a territory
within the repower range. Further, there might be sensor elds that report to the database
when they sense entities. If no entity was sensed by a sensor eld between the times t1 and
t3 , then T1 could not have followed a path that passed through that sensor eld. Such
constraints have to be taken into account while reasoning. All information might not be
available in the database at once. In what follows is a simple scenario and a discussion of
the spatial problems as they occur.
The problem solver (e.g., a commander) wants to know whether there exists a contiguous
safe region containing the points p1 and p3 . He species the problem Saf eRegion as follows:
Saf eRegion(q, {r1 , r2 , ...rn }, d)

409

fiBanerjee & Chandrasekaran

 a, (ni1 Inside(a, ri ))  Distance(q, a)  d
# (ri )


 a, (ni1 j1

# (ri )


 a, (ni1 j1

Inside(a, (ri )[j]))  Distance(q, a)  d

Inside(a, (ri )[j]))  Distance(q, a)  d

# (ri )

a, Inside(a, (ri )[j])  Distance(q, a)  d

# (ri )

a, Inside(a, (ri )[j])  Distance(q, a)  d

# (ri )

Saf eRegion (q, {(ri )[j]}, d)


 ni1 j1

 ni1 j1

 ni1 j1

where q  (x, y). Decomposition of the problem by the SPS is shown above. The subproblem is symbolically solved and the solution stored in memory along with the subproblem
specication. In order to compare the actual times required to solve the problem, we constructed a very simple diagram consisting of four polygonal regions depicting obstacles (see
Figure 20(a)). The four regions are
r1  {(10, 10), (30, 10), (30, 30), (10, 30)},
r2  {(20, 0), (0, 0), (10, 20)},
r3  {(0, 20), (10, 40), (10, 40)},
r4  {(50, 20), (70, 20), (80, 40), (60, 50), (40, 40)},
while d  2. Triangulation of the regions produced seven triangles. Once a subproblem is
symbolically solved and stored, solving the problem required 0.25 seconds while solving the
same using the CAD algorithm required 5.5 seconds.
The diagram with the shaded safe region is input to the Recognize function which computes the vertices and boundaries of the shaded region, as shown in Figure 20(b). Next the
problem solver wants to know whether there exists a path between points p1 and p3 safely
avoiding the obstacles and enemy repower range, and whether that path can be traversed
in time t3  t1 . Let v be the velocity of the sighted entity  a piece of symbolic knowledge
available from the database. Then, the maximum length of path traversable in the given
time is L = v  (t3  t1 ). Let l  L be a rational number. Then, the problem of path
existence between two points s and t such that the path lies inside a region r and is less
than a given length l can be specied as:
P athExists(s, t, r, l)
 q, Inside(q, r)  Distance(s, q) + Distance(q, t)  l
# (r)


 q, (i1
Inside(q, (r)[i]))  Distance(s, q) + Distance(q, t)  l

410

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

r3

r3

r4

r4

r1

r1

r2

r2



(a) The unshaded polygons are obstacles. The
shaded region is the safe region, as computed by
the SPS.

(b) The points shown are vertices of the boundaries of the safe region as computed by the
Recognize function.

s

s

r3

r3

r4

r1

r2

r4

r1

r2

t

t



(c) Paths lying in the safe region and less than a
given length between two points, as computed by
the SPS.

(d) Paths lying in the safe region and less than a
given length between two points, as computed by
the CAD algorithm.

Figure 20: A simplied scenario to illustrate the performance of the proposed SPS as compared to the CAD algorithm for entity re-identication.

# (r)


 i1
q, Inside(q, (r)[i])  Distance(s, q) + Distance(q, t)  l

# (r)


 i1
P athExists (s, t, (r)[i], l)

411

fiBanerjee & Chandrasekaran

Decomposition of the problem by the SPS is shown above. The subproblem is symbolically
solved and stored. Again, we resort to the simple diagram in Figure 20 to compare the actual
computation times for the P athExists(s, t, r, l) problem, where s  (0, 45), t  (20, 5),
r  Recognize(Saf eRegion((x, y), {r1 , r2 , ...rn }, 2)), and dierent sets of regions ri and
dierent values of l. Triangulation of r produced 8, 7, 7, 9 and 24 triangles for {r1 }, {r2 },
{r3 }, {r4 } and {r1 , r2 , r3 , r4 } respectively. Once a subproblem is symbolically solved and
stored, the computation times required for solving the problem using the proposed SPS is
signicantly less than that using the CAD algorithm (see Table 1).

Table 1: Comparison of computation times (in seconds) between the CAD algorithm and
our proposed SPS for the P athExists(s, t, r, l) problem, where s  (0, 45), t 
(20, 5), r  Recognize(Saf eRegion((x, y), {r1 , r2 , ...rn }, 2)). A 2.8 GHz PC with
4 GB RAM, 5356 MB virtual memory and 32-bit operating system was used. The
implementation was done in M athematica. Below, res refers to result, T 
refers to T rue, F  refers to F alse, and OOM  refers to out of memory.

l
100
 500
1000
1009
1010
2000
4000
 8000
16000

{r1 }
CAD,SPS,res
2.78, 0.41, F
2.77, 0.42, F
2.66, 0.39, F
2.28, 0.42, T
2.28, 0.41, T
1.88, 0.39, T
1.88, 0.34, T
1.88, 0.33, T
1.88, 0.33, T

{r2 }
CAD,SPS,res
498.22, 0.53, F
482.77, 0.44, F
118.97, 0.55, T
119.28, 0.52, T
120.06, 0.53, T
120.38, 0.42, T
120.73, 0.39, T
121.58, 0.34, T
121.45, 0.34, T

{r3 }
CAD,SPS,res
470.74, 0.5, F
476.97, 0.5, F
135.03, 0.49, T
134.75, 0.5, T
135.02, 0.52, T
135.3, 0.38, T
135.08, 0.34, T
135.03, 0.33, T
135.13, 0.36, T

{r4 }
CAD,SPS,res
OOM, 0.47, F
OOM, 0.48, F
OOM, 0.49, T
OOM, 0.52, T
OOM, 0.47, T
OOM, 0.44, T
OOM, 0.31, T
OOM, 0.39, T
OOM, 0.36, T

{r1 , r2 , r3 , r4 }
CAD,SPS,res
OOM, 1.42, F
OOM, 1.42, F
OOM, 1.42, F
OOM, 1.42, F
OOM, 1.41, T
OOM, 1.19, T
OOM, 1.0, T
OOM, 0.98, T
OOM, 0.92, T

In general, the P athExists(s, t, r, l) problem can be specied as:
P athExists(s, t, r, l)
 q1 , q2 , ...qn , (a, On(a, {s, q1 , q2 , ...qn , t})  Inside(a, r))  Length({s, q1 , q2 , ...qn , t})  l
 q1 , q2 , ...qn , (a, On(a, c)  Inside(a, r))  Length(c)  l
 q1 , q2 , ...qn , (a, On(a, c)  Inside(a, r))  Length(c)  l
# (r)


 q1 , q2 , ...qn , (a, On(a, c)  (k1
Inside(a, (r)[k])))  Length(c)  l

# (r)


 q1 , q2 , ...qn , (a, On(a, c)  (k1
Inside(a, (r)[k])))  Length(c)  l

412

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

# (r)


 k1
q1 , q2 , ...qn , (a, On(a, c)  Inside(a, (r)[k]))  Length(c)  l

# (r)


 k1
P athExists (s, t, (r)[k], l)

where c  {s, q1 , q2 , ...qn , t} and r  Br. Note that even though c is a curve, On(a, c)
cannot be decomposed since c is not a free variable (see denition of Decomposition in
section 3.2). Also, note that the above problem contains the form F 2 discussed in T heorem
1, so r has been used.
If there exists a path between points p1 and p3 safely avoiding the obstacles and enemy
repower range such that it can be traversed in time t3  t1 , then the problem solver wants
to compute the path(s). The problem can be specied as:
F indP ath(q, s, t, r, l)
 Inside(q, r)  Distance(s, q) + Distance(q, t)  l
# (r)


 (i1
Inside(q, (r)[i]))  Distance(s, q) + Distance(q, t)  l

# (r)


 i1
Inside(q, (r)[i])  Distance(s, q) + Distance(q, t)  l

# (r)


 i1
F indP ath (q, s, t, (r)[i], l)

where q  (x, y). Since there are no quantiers, solving the problem by decomposition and
variable mapping does not achieve reduction in computation time by anysignicant amount.
The region consisting of all paths that satisfy the constraints (l  1010) is shown in
Figure 20(c). The quality of the solution depends on the Recognize function. For example,
the solution shown in Figure 20(d) is more accurate than in Figure 20(c) as the Recognize
function failed to determine the vertices of the safe region accurately. An alternate denition
of the semi-linear motion planning problem can be found in Weispfenning (2001), where a
semi-linear path consists of n translations along straight lines each of which is parallel to
one of the given k vectors.
From the results, the problem solver infers that T3 might be T1 . Next he repeats the
same for entities T3 and T2 , and nds that there exists a path between points p2 and p3
safely avoiding the obstacles and enemy repower range such that it can be traversed in
time t3  t2 . So T3 might be T2 as well. The sensor database informs that there are two
sensor elds  SENSOR1, SENSOR2  in the area of interest but there has been no report from
them of any passing vehicle. Problem solver wants to verify whether any of the paths passes
through any of the sensor elds. He species the problem Intersect(r1 , r2 ) to compute the
intersection of two regions r1 and r2 .
IntersectRegions(r1 , r2 )
 q, Inside(q, r1 )  Inside(q, r2 )
# (r1 )


 q, (i1

# (r2 )


Inside(q, (r1 )[i]))  (i1

Inside(q, (r2 )[j]))

413

fiBanerjee & Chandrasekaran

# (r1 )


 i1

# (r2 )


j1

IntersectRegions ((r1 )[i], (r2 )[j])

He
computes
the
problem
IntersectRegions(paths13 , s1 ) where paths13  Recognize(F indP ath(q, p1 , p3 , r, l)) and
s1 is the region covered by SENSOR1. In our scenario in Figure 19(c), the solution is T rue.
Next the problem solver wants to know whether there exists a path between points p1 and
p3 safely avoiding the obstacles and enemy repower range such that it can be traversed in
time t3  t1 . He computes P athExists(p1 , p3 , r13 , l), where r13  Recognize(paths13  s1 ),
which returns T rue. The inference follows that T3 might be T2 . The same reasoning is
repeated for T3 and T2 ; Intersect(paths23 , s2 ) returns T rue while P athExists(p2 , p3 , r23 , l)
returns F alse (see Figure 19(b)). The inference follows that T3 cannot be T1 . Hence, the
problem solver identies T3 as T2 .
The entity reidentication problem could also have been solved by computing the shortest paths between the pairs p1 , p3 and p2 , p3 avoiding the sensors and checking whether
their lengths satisfy the time constraints. That requires computing the shortest path between two points p1 and p3 safely avoiding the obstacles and enemy repower range (i.e.,
lying entirely within the safe region r). Since such a path will not have any loop and will
share its intermediate vertices, if any, with the vertices of r, the path can have at most
#(r) intermediate vertices. Let S  r  {p1 , p3 }, m  #(S), and c  {q1 , q2 , ...qm } be
the shortest path, where q1  p1 , qm  p3 , qi  r (2  i  m  1). Then, the problem of
computing the shortest path can be specied as
F indShortestP ath(r, c)
 M inimize(Length(c), {c[2], c[3], ...c[m  1]}, CurveInsideRegion(c, r))
where CurveInsideRegion(c, r) is the constraint that can be specied and decomposed as
follows.
CurveInsideRegion(c, r)
 a, On(a, c)  Inside(a, r)
 a, On(a, c)  Inside(a, r)
 a, On(a, c)  Inside(a, r)
#(c)1

 a, (i1

# (r)

a, On(a, {c[i], c[i + 1]})  Inside(a, (r)[j])

# (r)

CurveInsideRegion ({c[i], c[i + 1]}, (r)[j])

#(c)1


j1

#(c)1


j1

 i1
 i1

# (r)


On(a, {c[i], c[i + 1]}))  (j1
Inside(a, (r)[j])))

414

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

where r  Br. Since the above problem is of the form F 2, r had to be used. Once a subproblem is symbolically solved and stored, solving the problem CurveInsideRegion(c, r),
where
c  {(0, 45), (14, 42), (35, 42), (15, 35), (34, 32), (36, 19), (47, 15), (87, 15), (30, 7), (20, 5)},
r  Recognize(Saf eRegion((x, y), {r1 , r2 , r3 , r4 }, 2)),
by SPS required 3.11 seconds while solving the same using the CAD algorithm required 175.01 seconds (see Figure 21(a)). The shortest path obtained by solving the
F indShortestP ath(r, c) problem is shown in Figure 21(b).

(a) A path c between two points lying inside the
shaded region r.

(b) Shortest path between two points as computed
from the F indShortestP ath(r, c) problem.

Figure 21: Paths between two points lying inside the safe (shaded) region.

5.2 Ambush Analysis
There are two main factors  range of repower and sight  that determine the area covered
by a military unit. Presence of terrain features, such as, mountains, limit these factors
and allow units to hide from opponents. These hidden units not only enjoy the advantage
of concealing their resources and intentions from the opponents but can also attack the
opponents catching them unawares if they are traveling along a path that is within the
sight and repower range of the hidden units, thereby ambushing them. Thus, it is of
utmost importance for any military unit to a priori determine the areas or portions of a
path prone to ambush before traversing them. We had already described in section 1.1 how
a problem solver (e.g., an army commander) reasons using diagrams to gure out the safest
path to transport his troops from one base camp to another in a given time. In this section,
given a curve or region as a hiding place and the repower and sight ranges, we show how
415

fiBanerjee & Chandrasekaran

the regions and portions of path prone to ambush is eciently computed by the proposed
SPS.
Given a curve c and the repower and sight range d, the spatial problem
RiskyRegion(q, c, d) is dened as the set of all points covered by that range from c. Thus,
the problem specication is:
RiskyRegion(q, c, d)
 a, On(a, c)  Distance(a, q)  d
#(c)1

 a, (i1

On(a, {c[i], c[i + 1]}))  Distance(a, q)  d

#(c)1

a, On(a, {c[i], c[i + 1]})  Distance(a, q)  d

#(c)1

RiskyRegion (q, {c[i], c[i + 1]}, d)

 i1
 i1

where q  (x, y). In order to compare the actual computation times required to solve the
problem, we constructed a very simple diagram consisting of two curves, path and mntn,
where
path  {(25, 10), (5, 10), (3, 15), (7, 17), (2, 18), (2, 18), (7, 15),
(3, 12), (5, 10), (40, 10)}
mntn  {(5, 5), (7, 2), (9, 9), (6, 12), (0, 4), (2, 3), (15, 5), (25, 12), (30, 20)}
The solution to the problem RiskyRegion(q, mntn, d) is the shaded region shown in Figure
22(a) where mntn is an obstacle for hiding (e.g., mountain range) and d  15. The problem
RiskyRegion(q, r, d) for a region r can be specied by replacing the predicate On(p, c) by
Inside(p, r).
Again, given a curve c1 as a path, a curve c2 for hiding, and a repower range d, the
problem RiskyP ortionsof P ath(q, c1 , c2 , d) is dened as parts of c1 covered by that range
from c2 . Thus,
RiskyP ortionsof P ath(q, c1 , c2 , d)
 On(q, c1 )  p, On(p, c2 )  Distance(p, q)  d
#(c )1

 p, (i11
d
#(c )1

j12

#(c )1

j12

 i11
 i11

#(c )1

On(q, {c1 [i], c1 [i+1]}))(j12

#(c )1

#(c )1

On(p, {c2 [j], c2 [j +1]}))Distance(p, q) 

p, On(q, {c1 [i], c1 [i+1]}))On(p, {c2 [j], c2 [j +1]})Distance(p, q)  d
RiskyP ortionsof P ath (q, {c1 [i], c1 [i + 1]}, {c2 [j], c2 [j + 1]}, d)

416

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

(a) The shaded region, as computed from the
problem RiskyRegion(q, mntn, 15), is the risky
region prone to ambush due to enemies hiding at
mntn. The portions of path inside the risky region are the risky portions of the path.

(b) The bold parts of path, as computed
from the problem RiskyP ortionsof P ath(q, path,
mntn, 15), are the risky portions of the path.
The shaded region, as computed from the problem BehindCurvewrtRiskyP ath(q, mntn, path),
is where enemies could be hiding from troops traveling on path.

(c) Troops traveling on rskyprtn1 , a risky portion (in bold) of path, should be careful of being
ambushed by enemies hiding in the shaded region because rskyprtn1 is within firepower range
from that region, computed from the problem
BehindCurvewrtRiskyP athDistance(q, mntn,
rskyprtn1 , 20).

(d) Troops traveling on rskyprtn2 , a risky portion (in bold) of path, should be careful of being
ambushed by enemies hiding in the shaded region because rskyprtn2 is within firepower range
from that region, computed from the problem
BehindCurvewrtRiskyP athDistance(q, mntn,
rskyprtn2 , 20).

Figure 22: A simplied scenario to illustrate the performance of the proposed SPS for ambush analysis.

417

fiBanerjee & Chandrasekaran

where q  (x, y). Alternatively, the same problem can be specied as
RiskyP ortionsof P ath(q, c1 , r2 , d)
 On(q, c1 )  Inside(q, r2 )
#(c )1

 (i11

# (r2 )


On(q, {c1 [i], c1 [i + 1]}))  (j1

# (r2 )

On(q, {c1 [i], c1 [i + 1]})  Inside(q, (r2 )[j])

# (r2 )

RiskyP ortionsof P ath (q, {c1 [i], c1 [i + 1]}, (r2 )[j], d)

#(c )1


j1

#(c )1


j1

 i11

 i11

Inside(q, (r2 )[j]))

where r2  Recognize(RiskyRegion((x, y), c2 , d)) and q  (x, y).
The solution
to the problem RiskyP ortionsof P ath(q, path, mntn, d), where d  15, is the parts
of path inside the shaded region shown in Figure 22(a). Figure 22(b) shows the
risky portions of the path  rskyprtn1 , rskyprtn2  in bold as obtained from
Recognize(RiskyP ortionsof P ath(q, c1 , c2 , d)).
rskyprtn1  {(16, 10), (5, 10), (3.7, 12.6)}
rskyprtn2  {(3, 12), (5, 10), (16.1, 10)}
Note that the latter specication is free from quantiers while the former is not. However,
the solution computed from the latter specication might have less accuracy than the same
from the former due to the use of Recognize function. If the hiding place is a region r
instead of the curve c2 , the problem RiskyP ortionsof P ath(q, c1 , r, d) can be specied by
replacing the predicate On(p, c2 ) by Inside(p, r). The portions of the path marked in Figure
2(c) is computed from this specication.
The region behind c2 where the enemies might be hiding is the set of all points that are
behind c2 with respect to each point on the risky portions of curve c1 . Thus, if c is a risky
portion of a path, we have
BehindCurvewrtRiskyP ath(q, c2 , c)
 a, On(a, c)  BehindCurve(q, c2 , a)
 a, On(a, c)  Intersect(c2 , {a, q})
 a, On(a, c)  (b, On(b, c2 )  On(b, {a, q}))
#(c)1

 a, b, (i1
#(c)1

 i1

#(c )1

j12

#(c )1

On(a, {c[i], c[i + 1]}))  (j12

On(b, {c2 [j], c2 [j + 1]}))  On(b, {a, q})

(a, b, On(a, {c[i], c[i + 1]})  On(b, {c2 [j], c2 [j + 1]})  On(b, {a, q}))

418

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

#(c)1

 i1

#(c )1

j12

BehindCurvewrtRiskyP ath (q, {c2 [j], c2 [j + 1]}, {c[i], c[i + 1]})

where
q  (x, y). The solution to the problem BehindCurvewrtRiskyP ath(q, mntn, rskyprtn1 )
BehindCurvewrtRiskyP ath(q, mntn, rskyprtn2 ) is the shaded region shown in Figure 22(b). If the hiding place is a region r instead of the curve c2 , the problem
BehindCurvewrtRiskyP ath(q, r, c) can be specied by replacing the predicate On(p, c2 )
by Inside(p, r).
However, the enemies might be hiding not anywhere behind a mountain but within
a distance from where they can ambush the friendly units. Hence, a more reasonable problem for the commander from the friendly side to compute would be
BehindCurvewrtRiskyP athDistance(q, c2 , c, d) where d is the distance from where the
enemies can ambush them. The problem is specied as:
BehindCurvewrtRiskyP athDistance(q, c2 , c, d)
 a, On(a, c)  BehindCurve(q, c2 , a)  Distance(a, q)  d
 a, On(a, c)  Intersect(c2 , {a, q})  Distance(a, q)  d
 a, On(a, c)  (b, On(b, c2 )  On(b, {a, q}))  Distance(a, q)  d
#(c)1

#(c )1

 a, b, (i1 On(a, {c[i], c[i + 1]}))  (j12
Distance(a, q)  d
#(c)1

On(b, {c2 [j], c2 [j + 1]}))  On(b, {a, q}) 

#(c )1

 i1 j12
(a, b, On(a, {c[i], c[i + 1]})  On(b, {c2 [j], c2 [j + 1]})  On(b, {a, q})) 
Distance(a, q)  d
#(c)1

 i1
1]}, d)

#(c )1

j12

BehindCurvewrtRiskyP athDistance (q, {c2 [j], c2 [j + 1]}, {c[i], c[i +

where
q

(x, y).
The solutions to the problems BehindCurvewrtRiskyP athDistance(q, mntn, rskyprtn1 , d)
and BehindCurvewrtRiskyP athDistance(q, mntn, rskyprtn2 , d), where d  20, are the
shaded regions shown in Figure 22(c), 22(d) respectively. If the hiding place is a region r
instead of the curve c2 , the problem BehindCurvewrtRiskyP athDistance(q, r, c, d) can be
specied by replacing the predicate On(p, c2 ) by Inside(p, r). A comparison between the
CAD algorithm and our proposed SPS of actual times required to compute the problems
relevant to ambush analysis as discussed above is shown in Table 2.

419

fiBanerjee & Chandrasekaran

Table 2: Comparison of computation times (in seconds) between the CAD algorithm and
our SPS for the dierent problems relevant to ambush analysis. A 2.8 GHz PC
with 4 GB RAM, 5356 MB virtual memory and 32-bit operating system was used.
The implementation was done in M athematica. All of the following are function
problems where q  (x, y).
P roblem
RiskyRegion(q, mntn, 15)
RiskyP ortionsof P ath(q, path, mntn, 15)
BehindCurve(q, mntn, (5, 10))
BehindCurvewrtRiskyP ath(q, mntn, path)
BehindCurvewrtRiskyP ath(q, mntn, rskyprtn1 )
BehindCurvewrtRiskyP ath(q, mntn, rskyprtn2 )
BehindCurvewrtRiskyP athDistance(q, mntn, rskyprtn1 , 20)
BehindCurvewrtRiskyP athDistance(q, mntn, rskyprtn2 , 20)

SPS
0.11
0.3
0.27
50.2
8.04
10.92
6.16
6.3

CAD
0.11
0.48
0.71
102.88
11.89
17.48
16.08
15.33

6. Discussion
Spatial problem solving has been an area of active research since Sutherlands sketchpad
(1963). The need to access, communicate and manipulate spatial information precisely
(much as engineers and scientists do) using a high-level language (much as common people
use) has been one of the frontiers in AI. It has been well-known that such capabilities
are oered by rst-order predicate logic and that, rst-order logic is generally intractable
except for limited domains. Under the umbrella of Qualitative Spatial Reasoning (QSR),
researchers have investigated a plethora of spatial calculi, the most prominent of which are
mereotopological calculi (Clarke, 1981; Bennett, 1997), cardinal direction calculus (Frank,
1991, 1992; Skiadopoulos & Koubarakis, 2004), double cross calculus (Freksa, 1992), 4and 9-intersection calculi (Egenhofer, 1991; Egenhofer & Franzosa, 1991), ip-op calculus
(Ligozat, 1993), dipole calculus (Moratz, Renz, & Wolter, 2000; Schlieder, 1995; Dylla &
Moratz, 2005), and the various region connection calculi (Randell et al., 1992; Bennett, Isli,
& Cohn, 1997; Gerevini & Nebel, 2002; Cohn, Bennett, Gooday, & Gotts, 1997; Duntsch,
Wang, & McCloskey, 1999; Gerevini & Renz, 1998). There are two main points of distinction
between QSR and our approach to spatial problem solving as reported in this paper.
1. The dierent QSR calculi emphasize dierent aspects of space, such as, ontological issues, topology, distance, orientation, shape, etc. Depending on the spatial aspect of interest, the calculus is based on a minimal set of spatial relations.
For example, the 9-intersection calculus (Egenhofer & Franzosa, 1991) is
based on nine spatial relations {r0 , r1 , r3 , r6 , r7 , r10 , r11 , r14 , r15 } between two spatial regions, the double cross calculus (Freksa, 1992) is based on fteen spatial relations
{lf, lp, lc, ll, lb, sf, sp, sc, sl, sb, rf, rp, rc, rl, rb} among three points, etc. Our framework
is not based on a minimal set of spatial relations; it is based on a xed set of mathematical/logical operators (see section 2.3). Any spatial relation among points that can be
expressed using real variables and the xed set of operators in rst-order logic is included in
our vocabulary. Any spatial relation involving curves and/or regions that can be expressed
420

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

in rst-order logic using spatial relations among points and the relations On and Inside is
included in our vocabulary.
2. The spatial problems of interest to the QSR community are CSPs involving either
points (e.g., double cross calculus) or regions (e.g., 4- and 9-intersection calculi, region
connection calculi) and a closed set of their properties/relations often limited to the binary
domain. A general-purpose SPS for helping a human perceive from and act on diagrams
in dierent real-world applications will need to solve QCSPs involving points, curves and
regions and an open-ended vocabulary of their properties/relations/actions over the entire
real domain, which is what our framework oers. Since QE is the computational bottleneck
of our SPS, we concentrate our eorts on the real QE algorithms, as discussed towards the
end of section 2.3.
Naturally, the question arises  how convenient is it for a human to specify a spatial
problem as a QCSP? While we acknowledge that the process of specifying a spatial problem
as a QCSP is not as eortless as explaining it to another human in a natural language, we
have taken the rst step in making the process less strenuous by oering a vocabulary of
predicates that is open-ended. Not all QCSP-solving systems, such as, redlog (Dolzmann
& Sturm, 1999) and qepcad (Brown, 2003), oer such a vocabulary for spatial problem
solving which makes it dicult for a user to specify a problem as he has to dig deep into an
ocean of equations and inequalities and cannot communicate naturally in terms of high-level
predicates.4 We are still far from building systems that can understand communication in
natural language. However, research in automatic constraint acquisition from examples
is already underway. Vu and OSullivan (2008) discuss recent advances in that direction.
While we did not use any of those ideas or results in our work, it is not dicult to see how
those ideas in conjunction with the work reported in this paper will be able to build a more
convenient and ecient spatial problem solving framework.

7. Conclusion
DR requires perceiving specied information from a diagram or modifying/creating objects
in a diagram in specied ways according to problem solving needs. A number of DR systems
have been built in the last couple of decades, in each of which the developers have ascertained
a priori and hand-coded the required perceptions and actions. This approach of building
DR systems defeats the very purpose of open-ended exploration  the essence of human-like
problem solving. Our goal, in this paper, was to develop a general and ecient framework
for executing perceptions and actions as relevant to reasoning with 2D diagrams across a
wide variety of domains and tasks. We make two important contributions:
1. We observe that the wide variety of visual perceptions/actions for DR applications can
be transformed into domain/task-independent spatial problems. This observation makes it
possible to use the well-established constraint satisfaction framework for spatial problem
solving. We developed a language in which to specify spatial problems as QCSPs in the
real domain using an open-ended vocabulary of properties, relations and actions involving
three kinds of diagrammatic objects  points, curves, regions. Solution to a spatial problem
is the equivalent simplied quantier-free expression. That reduces the goal to developing
a general and ecient SPS for solving 2D spatial problems without human intervention.
4. To be fair, redlog and qepcad were not developed for solving only spatial problems but any QCSP.

421

fiBanerjee & Chandrasekaran

2. The spatial problems were specied as QCSPs in rst-order logic. QE, an inherently
doubly exponential problem, was the computational bottleneck of the SPS. We represented
the objects (points, curves, regions) as conguration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems. We showed that, if
the symbolic solution to a subproblem can be expressed concisely, QE can be achieved in
low-order polynomial time by storing problems and their solutions in memory so that when
a similar problem is encountered in future, it can be solved by mapping its solution from a
similar previously solved problem. The SPS grows more ecient as it solves more problems.
Even though we used the CAD algorithm for QE and compared the complexity results with
that of CADs, this approach is by no means limited to any particular algorithm. The complexity of any QE algorithm can be signicantly improved for spatial problem solving by
using the idea of problem decomposition and variable mapping, as discussed in this paper.
The framework leaves room to be more ecient and convenient by incorporating future
results in at least two possible directions  learning constraints from examples (automatic
constraint acquisition) and carefully exploiting a rich portfolio of QE algorithms for solving
new problems.

Acknowledgments
This research was partially supported by participation in the Advanced Decision Architectures Collaborative Technology Alliance sponsored by the U.S. Army Research Laboratory
under Cooperative Agreement DAAD19-01-2-0009. We thank the anonymous reviewers for
their constructive comments.

References
Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). The Design and Analysis of Computer
Algorithms. Addison-Wesley.
Allwein, G., & Barwise, J. (1999). Logical reasoning with diagrams. Journal of Logic,
Language and Information, 8 (3), 387390.
Anderson, J. R. (1993). Rules of the Mind. Lawrence Erlbaum Associates, Hillsdale, NJ.
Banerjee, B., & Chandrasekaran, B. (2004). Perceptual and action routines in diagrammatic
reasoning for entity re-identication. In Proc. 24th Army Science Conf., Orlando, FL.
Banerjee, B., & Chandrasekaran, B. (2007). A constraint satisfaction framework for visual
problem solving. In Benhamou, F., Jussien, N., & OSullivan, B. (Eds.), Trends in
Constraint Programming, chap. 26, pp. 383393. ISTE, London.
Banerjee, B., & Chandrasekaran, B. (2010). A spatial search framework for executing
perceptions and actions in diagrammatic reasoning. In Goel, A. K., Jamnik, M., &
Narayanan, N. H. (Eds.), Diagrammatic Representation and Inference, Lecture Notes
in AI, Vol. 6170, pp. 144159. Springer, Heidelberg.
Barwise, J., & Etchemendy, J. (1998). A computational architecture for heterogeneous
reasoning. In Gilboa, I. (Ed.), Proc. 7th Conf. Theoretical Aspects of Rationality and
Knowledge, pp. 127. Morgan Kaufmann.
422

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Basu, S., Pollack, R., & Roy, M.-F. (2003). Algorithms in real algebraic geometry. SpringerVerlag.
Bennett, B. (1997). Logical Representations for Automated Reasoning about Spatial Relationships. Ph.D. thesis, School of Computer Studies, The University of Leeds.
Bennett, B., Isli, A., & Cohn, A. G. (1997). When does a composition table provide a
complete and tractable proof procedure for a relational constraint language?. In Proc.
IJCAI Workshop Spatial and Temporal Reasoning, Nagoya, Japan.
Brown, C. W. (2003). QEPCAD B: A program for computing with semi-algebraic sets using
cylindrical algebraic decomposition. ACM SIGSAM Bulletin, 37 (4), 97108.
Brown, C. W., & Davenport, J. H. (2007). The complexity of quantier elimination and
cylindrical algebraic decomposition. In Proc. Intl. Symp. Symbolic and Algebraic Computation, pp. 5460. ACM, NY.
Chandrasekaran, B., Josephson, J. R., Banerjee, B., Kurup, U., & Winkler, R. (2002).
Diagrammatic reasoning in support of situation understanding and planning. In Proc.
23rd Army Science Conf., Orlando, FL.
Chandrasekaran, B., Kurup, U., & Banerjee, B. (2005). A diagrammatic reasoning architecture: Design, implementation and experiments. In Proc. AAAI Spring Symp.,
Reasoning with Mental and External Diagrams: Computational Modeling and Spatial
Assistance, pp. 108113, Stanford University, CA.
Chandrasekaran, B., Kurup, U., Banerjee, B., Josephson, J. R., & Winkler, R. (2004). An
architecture for problem solving with diagrams. In Blackwell, A., Marriott, K., &
Shimojima, A. (Eds.), Lecture Notes in AI, Vol. 2980, pp. 151165. Springer-Verlag.
Chazelle, B. (1991). Triangulating a simple polygon in linear time. Discrete and Computational Geometry, 6, 485524.
Clarke, B. L. (1981). A calculus of individuals based on connection. Notre Dame Journal
of Formal Logic, 22, 204218.
Cohn, A. G., Bennett, B., Gooday, J. M., & Gotts, N. (1997). RCC: A calculus for region
based qualitative spatial reasoning. GeoInformatica, 1, 275316.
Collins, G. E., & Hong, H. (1991). Partial cylindrical algebraic decomposition for quantier
elimination. Journal of Symbolic Computation, 12 (3), 299328.
Davenport, J. H., & Heintz, J. (1988). Real quantier elimination is doubly exponential.
Journal of Symbolic Computation, 5 (1-2), 2935.
Dershowitz, N., & Jouannaud, J. P. (1990). Rewrite systems. In Handbook of Theoretical
Computer Science, Vol. B, chap. 6, pp. 243320. Elsevier, North Holland: Amsterdam.
Dolzmann, A., & Sturm, T. (1999). REDLOG user manual, edition 2.0 for version 2.0.
Tech. rep. MIP-9905, FMI, Universitt Passau, Passau, Germany.
Dolzmann, A., Sturm, T., & Weispfenning, V. (1998). Real quantier elimination in practice.
In Matzat, B. H., Greuel, G.-M., & Hiss, G. (Eds.), Algorithmic Algebra and Number
Theory, pp. 221247. Springer, Berlin.
423

fiBanerjee & Chandrasekaran

Duntsch, I., Wang, H., & McCloskey, S. (1999). Relation algebras in qualitative spatial
reasoning. Fundamenta Informaticae, 39 (3), 229249.
Dylla, F., & Moratz, R. (2005). Exploiting qualitative spatial neighborhoods in the situation
calculus. In Freksa, C., Knau, M., Krieg-Brckner, B., Nebel, B., & Barkowsky, T.
(Eds.), Spatial Cognition IV. Reasoning, Action, and Interaction, Vol. 3343 of Lecture
Notes in Computer Science, pp. 304322. Springer.
Egenhofer, M. J. (1991). Reasoning about binary topological relations. In Gunther, O.,
& Schek, H.-J. (Eds.), Proc. 2nd Symp. Large Spatial Databases, Vol. 525 of Lecture
Notes in Computer Science, pp. 143160. Springer.
Egenhofer, M. J., & Franzosa, R. D. (1991). Point set topological relations. Intl. Journal
of Geographical Information Systems, 5, 161174.
Ferguson, R. W. (1994). MAGI: Analogy-based encoding using symmetry and regularity.
In Proc. 16th Annual Conf. Cognitive Science Society, pp. 283288, Atlanta, GA.
Ferguson, R. W., & Forbus, K. D. (1998). Telling juxtapositions: Using repetition and
alignable dierence in diagram understanding. In Holyoak, K., Gentner, D., & Kokinov, B. (Eds.), Advances in Analogy Research, pp. 109117. Soa, New Bulgarian
University.
Ferguson, R. W., & Forbus, K. D. (2000). GEOREP: A exible tool for spatial representation
of line drawings. In Proc. 18th Natl. Conf. AI, pp. 510516, Austin, TX.
Forbus, K. D., Usher, J., & Chapman, V. (2003). Qualitative spatial reasoning about sketch
maps. In Riedl, J., & Hill, R. (Eds.), Proc. 15th Annual Conf. Innovative Applications
of AI, pp. 8592, Acapulco, Mexico. AAAI Press, Menlo Park, CA. ISBN 978-1-57735188-7.
Frank, A. U. (1991). Qualitative spatial reasoning with cardinal directions. In Kaindl, H.
(Ed.), Proc. 7th Austrian Conf. AI, Vol. 287 of Informatik-Fachberichte, pp. 157167.
Springer.
Frank, A. U. (1992). Qualitative spatial reasoning about distances and directions in geographic space. Journal of Visual Languages and Computing, 3, 343371.
Freksa, C. (1992). Using orientation information for qualitative spatial reasoning. In Frank,
A. U., Campari, I., & Formentini, U. (Eds.), Spatio-Temporal Reasoning, Vol. 639 of
Lecture Notes in Computer Science, pp. 162178. Springer.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR to select solution
strategies in constraint programming. In Proc. 6th Intl. Conf. Case-based Reasoning,
pp. 222236. Springer.
Gerevini, A., & Nebel, B. (2002). Qualitative spatio-temporal reasoning with rcc-8 and
allens interval calculus: Computational complexity. In Proc. 15th European Conf. AI,
pp. 312316. IOS Press.
Gerevini, A., & Renz, J. (1998). Combining topological and qualitative size constraints
for spatial reasoning. In Proc. 4th Intl. Conf. Principles and Practice of Constraint
Programming, pp. 220234. Springer.
424

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Glasgow, J., Narayanan, N. H., & Chandrasekaran, B. (1995). Diagrammatic Reasoning:
Cognitive and Computational Perspectives. AAAI Press.
Goldberg, E., & Novikov, Y. (2003). On complexity of equivalence checking. Tech. rep.
CDNL-TR-2003-0826, Cadence Berkeley Labs, CA.
Jamnik, M. (2001). Mathematical Reasoning with Diagrams: From Intuition to Automation.
CSLI Press, Stanford University, CA.
Josephson, J. R., & Josephson, S. G. (1996). Abductive Inference: Computation, Philosophy,
Technology. Cambridge University Press, Cambridge, MA.
Kurup, U., & Chandrasekaran, B. (2007). A bimodal cognitive architecture: Explorations in
architectural explanation of spatial reasoning. In AAAI Spring Symp. Control Mechanisms for Spatial Knowledge Processing in Cognitive/Intelligent Systems, Stanford
University, CA.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). SOAR: An architecture for general
intelligence. Artificial Intelligence, 33, 164.
Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986). Universal Subgoaling and Chunking.
Kluwer Academic Publishers.
Lasaruk, A., & Sturm, T. (2006). Weak quantier elimination for the full linear theory
of the integers. A uniform generalization of Presburger arithmetic. Technical report
MIP-0604, FMI, Universitt Passau, Germany.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning the empirical hardness
of optimization problems: The case of combinatorial auctions. In Proc. 8th Intl. Conf.
Principles and Practice of Constraint Programming, pp. 556572.
Ligozat, G. (1993). Qualitative triangulation for spatial reasoning. In Frank, A. U., &
Campari, I. (Eds.), Spatial Information Theory: A Theoretical Basis for GIS, Vol. 716
of Lecture Notes in Computer Science, pp. 5468. Springer.
Lindsay, R. K. (1998). Using diagrams to understand geometry. Computational Intelligence,
14 (2), 238272.
Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative spatial reasoning about line segments.
In Proc. 14th European Conf. AI, pp. 234238. IOS Press.
Nelson, R. B. (1993). Proofs without Words: Exercises in Visual Thinking. The Mathematical Association of America, Washington, DC.
Newell, A. (1990). Unified Theories of Cognition. Harvard University Press, Cambridge,
MA.
OMahony, E., Hebrard, E., Holland, A., Nugent, C., & OSullivan, B. (2008). Using casebased reasoning in an algorithm portfolio for constraint solving. In van Dongen, M.
R. C., Lecoutre, C., & Roussel, O. (Eds.), Proc. 3rd Intl. CSP Solver Competition,
pp. 5362.
Pisan, Y. (1994). Visual reasoning with graphs. In 8th Intl. Workshop Qualitative Reasoning
about Physical Systems, Nara, Japan.
425

fiBanerjee & Chandrasekaran

Pisan, Y. (1995). A visual routines based model of graph understanding. In Proc. 17th
Annual Conf. Cognitive Science Society, pp. 692697, Pittsburgh. Lawrence Erlbaum
Associates. ISBN: 0-8058-2159-7.
Pulina, L., & Tacchella, A. (2007). A multi-engine solver for quantied boolean formulas.
In Proc. 13th Intl. Conf. Principles and Practice of Constraint Programming, pp.
574589.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). A spatial logic based on regions and connection. In Nebel, B., Swartout, W., & Rich, C. (Eds.), Proc. 3rd Intl. Conf. Principles
of Knowledge Representation and Reasoning, pp. 165176. Morgan Kaufmann.
Ratschan, S. (2006). Ecient solving of quantied inequality constraints over the real
numbers. ACM Trans. Computational Logic, 7 (4), 723748.
Sayag, T., Fine, S., & Mansour, Y. (2006). Combining multiple heuristics. In Proc. 23rd
Intl. Symp. Theoretical Aspects of Computer Science, Vol. 2884 of Lecture Notes in
Computer Science, pp. 242253. Springer.
Schlieder, C. (1995). Reasoning about ordering. In Frank, A. U., & Kuhn, W. (Eds.),
Spatial Information Theory: A Theoretical Basis for GIS, Vol. 988 of Lecture Notes
in Computer Science, pp. 341349. Springer.
Seidel, R. (1991). A simple and fast incremental randomized algorithm for computing
trapezoidal decompositions and for triangulating polygons. Computational Geometry:
Theory and Applications, 1 (1), 5164.
Skiadopoulos, S., & Koubarakis, M. (2004). Composing cardinal direction relations. Artificial Intelligence, 152 (2), 143171.
Streeter, M. J., Golovin, D., & Smith, S. F. (2007). Combining multiple heuristics online.
In Proc. 22nd Conf. AI, pp. 11971203. AAAI Press.
Sutherland, I. E. (1963). Sketchpad: A man-machine graphical communication system. In
Proc. Spring Joint Computer Conf., pp. 329346.
Tessler, S., Iwasaki, Y., & Law, K. (1995). Qualitative structural analysis using diagrammatic reasoning. In Glasgow, J., Narayanan, N. H., & Chandrasekaran, B. (Eds.),
Diagrammatic Reasoning: Cognitive and Computational Perspectives, chap. 21, pp.
711730. AAAI Press, Menlo Park, CA. ISBN 0-262-57112-9.
Tricket, S. B., & Trafton, J. G. (2006). Toward a comprehensive model of graph comprehension: Making the case for spatial cognition. In Barker-Plummer, D., Cox, R., &
Swoboda, N. (Eds.), Lecture Notes in AI, Vol. 4045, pp. 286300. Berlin: SpringerVerlag.
Tversky, B. (2000). Some ways that maps and diagrams communicate. In Freksa, C.,
Brauer, W., Habel, C., & Wender, K. F. (Eds.), Spatial Cognition II: Integrating
Abstract Theories, Empirical Studies, Formal Methods, and Practical Applications,
Vol. 1849 of Lecture Notes in Computer Science, pp. 7279. Berlin: Springer-Verlag.
Vik, S. (2001). An implementation of a near-linear polygon triangulation algorithm for
general polygons. Senior thesis at Macalester College, St. Paul, Minnesota. Available
online at http://sigbjorn.vik.name/projects/Triangulation.pdf.
426

fiExecuting Perceptions and Actions in Diagrammatic Reasoning

Vu, X. H., & OSullivan, B. (2008). A unifying framework for generalized constraint acquisition. Intl. Journal on AI Tools, 17 (5), 803833.
Weispfenning, V. (1988). The complexity of linear problems in elds. Journal of Symbolic
Computation, 5 (12), 327.
Weispfenning, V. (2001). Semilinear motion planning in REDLOG. Applicable Algebra in
Engineering, Communication and Computing, 12, 455475.
Wolfram, S. (2003).
The Mathematica Book (5th edition).
http://documents.wolfram.com/.

Available online at

Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based
algorithm selection for SAT. Journal of Artificial Intelligence Research, 32, 565606.

427

fiJournal of Artificial Intelligence Research 39 (2010) 127177

Submitted 11/09; published 09/10

The LAMA Planner:
Guiding Cost-Based Anytime Planning with Landmarks
Silvia Richter

silvia.richter@nicta.com.au

IIIS, Griffith University, Australia
and NICTA QRL, Australia

Matthias Westphal

westpham@informatik.uni-freiburg.de

Albert-Ludwigs-Universitat Freiburg
Institut fur Informatik
Freiburg, Germany

Abstract
LAMA is a classical planning system based on heuristic forward search. Its core feature is
the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true
in every solution of a planning task. LAMA builds on the Fast Downward planning system, using
finite-domain rather than binary state variables and multi-heuristic search. The latter is employed to
combine the landmark heuristic with a variant of the well-known FF heuristic. Both heuristics are
cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost.
A weighted A search is used with iteratively decreasing weights, so that the planner continues to
search for plans of better quality until the search is terminated.
LAMA showed best performance among all planners in the sequential satisficing track of the
International Planning Competition 2008. In this paper we present the system in detail and investigate which features of LAMA are crucial for its performance. We present individual results for
some of the domains used at the competition, demonstrating good and bad cases for the techniques
implemented in LAMA. Overall, we find that using landmarks improves performance, whereas the
incorporation of action costs into the heuristic estimators proves not to be beneficial. We show that
in some domains a search that ignores cost solves far more problems, raising the question of how
to deal with action costs more effectively in the future. The iterated weighted A search greatly
improves results, and shows synergy effects with the use of landmarks.

1. Introduction
In the last decade, heuristic search has become the dominant approach to domain-independent satisficing planning. Starting with the additive heuristic by Bonet and Geffner (2001), implemented
in the HSP planning system, much research has been conducted in search of heuristic estimators
that are efficient to calculate yet powerful in guiding the search towards a goal state. The FF planning system by Hoffmann and Nebel (2001), using a heuristic estimator based on relaxed planning
graphs, broke ground by showing best performance among all fully automated systems at the International Planning Competition in 2000, and continues to be state of the art today. Ever since,
heuristic-search approaches have played a prominent role in the classical or sequential satisficing
tracks of the biennial competition, with Fast Downward (Helmert, 2006) winning in 2004 and SGPlan (Chen, Wah, & Hsu, 2006) placing first in 2006.
The LAMA planning system is the youngest member in this line, winning the sequential satisficing track at the International Planning Competition (IPC) in 2008. LAMA is a classical planning
c
2010
AI Access Foundation. All rights reserved.

fiRichter & Westphal

system based on heuristic search. It follows in the footsteps of HSP, FF, and Fast Downward and
uses their earlier work in many respects. In particular, it builds on Fast Downward by extending it
in three major ways:
1. Landmarks. In LAMA, Fast Downwards causal graph heuristic is replaced with a variant of
the FF heuristic (Hoffmann & Nebel, 2001) and heuristic estimates derived from landmarks.
Landmarks are propositional formulas that have to become true at some point in every plan
for the task at hand (Porteous, Sebastia, & Hoffmann, 2001). LAMA uses landmarks to
direct search towards those states where many landmarks have already been achieved. Via
preferred operators, landmarks are also used as an additional source of search control which
complements the heuristic estimates. In recent work, we have shown this use of landmarks
in addition to the FF heuristic to improve performance, by leading to more problems being
solved and shorter solution paths (Richter, Helmert, & Westphal, 2008).
2. Action costs. Both the landmark heuristic we proposed earlier (Richter et al., 2008) and the
FF heuristic have been adapted to use action costs. However, LAMA does not focus purely on
the cost-to-go, i. e., the estimated cost of reaching the goal from a given search node. There
is a danger that a cost-sensitive planner may concentrate too much on finding a cheap plan,
at the expense of finding a plan at all within a given time limit. LAMA weighs the estimated
cost-to-go (as a measure of plan quality) against the estimated goal distance (as a measure of
remaining search effort) by combining the values for the two estimates.
3. Anytime search. LAMA continues to search for better solutions until it has exhausted the
search space or is interrupted. After finding an initial solution with a greedy best-first search,
it conducts a series of weighted A searches with decreasing weights, restarting the search
each time from the initial state when an improved solution is found. In recent work, we have
shown this approach to be very efficient on planning benchmarks compared to other anytime
methods (Richter, Thayer, & Ruml, 2010).
At the International Planning Competition 2008, LAMA outperformed its competitors by a
substantial margin. This result was not expected by its authors, as their previous work concerning
LAMAs putative core feature, the landmark heuristic (Richter et al., 2008), showed some, but
not tremendous improvement over the base configuration without landmarks. This paper aims to
provide a reference description of LAMA as well as an extensive evaluation of its performance in
the competition.
 Detailed description of LAMA. We present all distinguishing components of the planner
in detail, describing how landmarks are generated and used in LAMA, how action costs are
incorporated into the heuristic estimators and how the anytime search proceeds. Some aspects of LAMA have been presented in previous publications (Richter et al., 2008, 2010;
Helmert, 2006). However, aspects that have not been adequately covered in those publications, in particular the procedure for finding landmarks, are described here in detail. Other
relevant aspects described in previous work, like the landmark heuristic, are summarised for
the convenience of the reader. Our aim is that this paper, together with previous ones, form a
comprehensive picture of the LAMA system.
 Experimental evaluation of LAMA. Building on this, we conduct an experimental evaluation focusing on the aspects that differentiate LAMA from predecessor systems like FF and
128

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Fast Downward. We do not repeat comparisons published in earlier work, like the comparison
between LAMAs anytime method and other anytime algorithms (Richter et al., 2010), or the
comparison of LAMAs methods for handling landmarks to alternative landmark approaches
(Richter et al., 2008). Instead, we aim to elicit how much the performance of the LAMA
system as a whole is enhanced by each of the three distinguishing features described above
(landmarks, action costs and anytime search). To answer this question, we contrast several
variations of our planner using various subsets of these features.
We find that using cost-sensitive heuristics did not pay off on the IPC 2008 benchmark tasks.
Our results show that the cost-sensitive variant of the FF heuristic used in LAMA performs significantly worse than the traditional unit-cost version of the same heuristic. Similarly, all other
cost-sensitive planners in the competition fared worse than the baseline planner FF that ignored action costs, demonstrating that cost-based planning presents a considerable challenge. While we do
not conduct a full analysis of the reasons for this, we showcase the problems of the cost-sensitive FF
heuristic in some example domains and provide informed hypotheses for the encountered effects.
Landmarks prove to be particularly helpful in this context. While in the unit-cost case landmarks
only lead to a moderate increase in performance, in the case of planning with action costs they
substantially improve coverage (the number of problems solved), thus effectively mitigating the
problems of the cost-sensitive FF heuristic in LAMA. The anytime search significantly improves
the quality of solutions throughout and even acts in synergy with landmarks in one domain.

2. Preliminaries
We use a planning formalism with state variables of finite (rather than binary) range, similar to the
one employed by Helmert (2009). It is based on the SAS+ planning model (Backstrom & Nebel,
1995), but extends it with conditional effects. While LAMA also handles axioms in the same way
as Fast Downward (Helmert, 2006), we do not formalise axioms here, since they are not important
for our purposes.
Definition 1. Planning tasks in finite-domain representation (FDR tasks)
A planning task in finite-domain representation (FDR task) is given by a 5-tuple hV, s0 , s? , O, Ci
with the following components:
 V is a finite set of state variables, each with an associated finite domain Dv .
A fact is a pair hv, di (also written v 7 d), where v  V and d  Dv . A partial variable
assignment s is a set of facts, each with a different variable. (We use set notation such as
hv, di  s and function notation such as s(v) = d interchangeably.) A state is a variable
assignment defined on all variables V.
 s0 is a state called the initial state.
 s? is a partial variable assignment called the goal.
 O is a finite set of operators. An operator hpre, effi consists of a partial variable assignment
pre called its precondition, and a finite set of effects eff. Effects are triplets hcond, v, di,
where cond is a (possibly empty) partial variable assignment called the effect condition, v is
the affected variable and d  Dv is called the new value for v.
129

fiRichter & Westphal

 C : O  N+0 is an integer-valued non-negative action cost function.
An operator o = hpre, effi  O is applicable in a state s if pre  s, and its effects are consistent,
i. e., there is a state s0 such that s0 (v) = d for all hcond, v, di  eff where cond  s, and s0 (v) = s(v)
otherwise. In this case, we say that the operator o can be applied to s resulting in the state s0 and
write s[o] for s0 .
For operator sequences  = ho1 , . . . , on i, we write s[] for s[o1 ] . . . [on ] (only defined if each operator is applicable in the respective state). The operator sequence  is called a plan if s?  s0 [].
P
The cost of  is the sum of the action costs of its operators, ni=1 C(oi ).
Each state variable v of a planning task in finite-domain representation has an associated directed
graph called the domain transition graph, which captures the ways in which the value of v may
change (Jonsson & Backstrom, 1998; Helmert, 2006). The vertex set of this graph is Dv , and it
contains an arc between two nodes d and d0 if there exists an operator that can change the value of
v from d to d0 . Formally:
Definition 2. Domain transition graph
The domain transition graph (DTG) of a state variable v  V of an FDR task hV, s0 , s? , O, Ci is
the digraph hDv , Ai which includes an arc hd, d0 i iff d , d0 , there is an operator hpre, effi  O with
hcond, v, d0 i  eff, and for the union of conditions pre  cond it holds that either it contains v = d or
it does not contain v = d for any d  Dv .

3. System Architecture
LAMA builds on the Fast Downward system (Helmert, 2006), inheriting the overall structure and
large parts of the functionality from that planner. Like Fast Downward, LAMA accepts input in
the PDDL2.2 Level 1 format (Fox & Long, 2003; Edelkamp & Hoffmann, 2004), including ADL
conditions and effects and derived predicates (axioms). Furthermore, LAMA has been extended
to handle the action costs introduced for IPC 2008 (Helmert, Do, & Refanidis, 2008). Like Fast
Downward, LAMA consists of three separate components:
 The translation module
 The knowledge compilation module
 The search module
These components are implemented as separate programs that are invoked in sequence. In the
following, we provide a brief description of the translation and knowledge compilation modules.
The main changes in LAMA, compared to Fast Downward, are implemented in the search module,
which we discuss in detail.
3.1 Translation
The translation module, short translator, transforms the PDDL input into a planning task in finitedomain representation as specified in Definition 1. The main components of the translator are an
efficient grounding algorithm for instantiating schematic operators and axioms, and an invariant
130

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

synthesis algorithm for determining groups of mutually exclusive facts. Such fact groups are consequently replaced by a single state variable, encoding which fact (if any) from the group is satisfied
in a given world state. Details on this component can be found in a recent article by Helmert (2009).
The groups of mutually exclusive facts (mutexes) found during translation are later used to
determine orderings between landmarks. For this reason, LAMA does not use the finite-domain
representations offered at IPC 2008 (object fluents), but instead performs its own translation from
binary to finite-domain variables. While not all mutexes computed by the translation module are
needed for the new encoding of the planning task, the module has been extended in LAMA to retain
all found mutexes for their later use with landmarks.
Further changes we made, compared to the translation module described by Helmert, were
to add the capability of handling action costs, implement an extension concerning the parsing of
complex operator effect formulas, and limit the runtime of the invariant synthesis algorithm. As
invariant synthesis may be time critical, in particular on large (grounded) PDDL input, we limit the
maximum number of considered mutex candidates in the algorithm, and abort it, if necessary, after
five minutes. Note that finding few or no mutexes does not change the way the translation module
works; if no mutexes are found, the resulting encoding of the planning task contains simply the
same (binary-domain) state variables as the PDDL input. When analysing the competition results,
we found that the synthesis algorithm had aborted only in some of the tasks of one domain (Cyber
Security).
3.2 Knowledge Compilation
Using the finite-domain representation generated by the translator, the knowledge compilation module is responsible for building a number of data structures that play a central role in the subsequent
landmark generation and search. Firstly, domain transition graphs (see Definition 2) are produced
which encode the ways in which each state variable may change its value through operator applications and axioms. Furthermore, data structures are constructed for efficiently determining the set of
applicable operators in a state and for evaluating the values of derived state variables. We refer to
Helmert (2006) for more detail on the knowledge compilation component, which LAMA inherits
unchanged from Fast Downward.
3.3 Search
The search module is responsible for the actual planning. Two algorithms for heuristic search are
implemented in LAMA: (a) a greedy best-first search, aimed at finding a solution as quickly as
possible, and (b) a weighted A search that allows balancing speed against solution quality. Both
algorithms are variations of the standard textbook methods, using open and closed lists. The greedy
best-first search always expands a state with minimal heuristic value h among all open states and
never expands a state more than once. In order to encourage cost-efficient plans without incurring
much overhead, it breaks ties between equally promising states by preferring those states that are
reached by cheaper operators, i. e., taking into account the last operator on the path to the considered
state in the search space. (The cost of the entire path could only be used at the expense of increased
time or space requirements, so that we do not consider this.) Weighted A search (Pohl, 1970)
associates costs with states and expands a state with minimal f 0 -value, where f 0 = w  h + g, the
weight w is an integer  1, and g is the best known cost of reaching the considered state from the
131

fiRichter & Westphal

initial state. In contrast to the greedy search, weighted A search re-expands states whenever it finds
cheaper paths to them.
In addition, both search algorithms use three types of search enhancements inherited from Fast
Downward (Helmert, 2006; Richter & Helmert, 2009). Firstly, multiple heuristics are employed
within a multi-queue approach to guide the search. Secondly, preferred operators  similar to the
helpful actions in FF  allow giving precedence to operators that are deemed more helpful than
others in a state. Thirdly, deferred heuristic evaluation mitigates the impact of large branching
factors assuming that heuristic estimates are fairly accurate. In the following, we discuss these
techniques and the resulting algorithms in more detail and give pseudo code for the greedy best-first
search. The weighted A search is very similar, so we point out the differences between the two
algorithms along the way.
Multi-queue heuristic search. LAMA uses two heuristic functions to guide its search: the namegiving landmark heuristic (see Section 5), and a variant of the well-known FF heuristic (see Section 6). The two heuristics are used with separate queues, thus exploiting strengths of the utilised
heuristics in an orthogonal way (Helmert, 2006; Roger & Helmert, 2010). To this end, separate
open lists are maintained for each of the two heuristics. States are always evaluated with respect to
both heuristics, and their successors are added to all open lists (in each case with the value corresponding to the heuristic of that open list). When choosing which state to evaluate and expand next,
the search algorithm alternates between the different queues based on numerical priorities assigned
to each queue. These priorities are discussed later.
Deferred heuristic evaluation. The use of deferred heuristic evaluation means that states are
not heuristically evaluated upon generation, but upon expansion, i. e., when states are generated in
greedy best-first search, they are put into the open list not with their own heuristic value, but with
that of their parent. Only after being removed from the open list are they evaluated heuristically,
and their heuristic estimate is in turn used for their successors. The use of deferred evaluation in
weighted A search is analogous, using f 0 instead of h as the sorting criterion of the open lists.
If many more states are generated than expanded, deferred evaluation leads to a substantial reduction in the number of heuristic estimates computed. However, deferred evaluation incurs a loss of
heuristic accuracy, as the search can no longer use h-values or f 0 -values to differentiate between the
successors of a state (all successors are associated with the parents value in the open list). Preferred
operators are very helpful in this context as they provide an alternative way to determine promising
successors.
Preferred operators. Operators that are deemed particularly useful in a given state are marked
as preferred. They are computed by the heuristic estimators along with the heuristic value of a
state (see Sections 6 and 5). To use preferred operators, in the greedy best-first search as well as
in the weighted A search, the planner maintains an additional preferred-operator queue for each
heuristic. When a state is evaluated and expanded, those successor states that are reached via a
preferred operator (the preferred states) are put into the preferred-operator queues, in addition to
being put into the regular queues like the non-preferred states. (Analogously to regular states, any
state preferred by at least one heuristic is added to all preferred-operator queues. This allows for
cross-fertilisation through information exchange between the different heuristics.) States in the
preferred-operator queues are evaluated earlier on average, as they form part of more queues and
have a higher chance of being selected at any point in time than the non-preferred states. In addition,
132

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

LAMA (like the IPC 2004 version of Fast Downward) gives even higher precedence to preferred
successors via the following mechanism. The planner keeps a priority counter for each queue,
initialised to 0. At each iteration, the next state is removed from the queue that has the highest
priority. Whenever a state is removed from a queue, the priority of that queue is decreased by 1. If
the priorities are not changed outside of this routine, this method will alternate between all queues,
thus expanding states from preferred queues and regular queues equally often. To increase the use
of preferred operators, LAMA increases the priorities of the preferred-operator queues by a large
number boost of value 1000 whenever progress is made, i. e., whenever a state is discovered that has
a better heuristic estimate than previously expanded states. Subsequently, the next 1000 states will
be removed from preferred-operator queues. If another improving state is found within the 1000
states, the boosts accumulate and, accordingly, it takes longer until states from the regular queues
are expanded again.
Alternative methods for using preferred operators include the one employed in the YAHSP
system (Vidal, 2004), where preferred operators are always used over non-preferred ones. By contrast, our scheme does not necessarily empty the preferred queues before switching back to regular
queues. In the FF planner (Hoffmann & Nebel, 2001), the emphasis on preferred operators is even
stronger than in YAHPS: the search in FF is restricted to preferred operators until either a goal is
found or the restricted search space has been exhausted (in which case a new search is started without preferred operators). Compared to these approaches, the method for using preferred operators
in LAMA, in conjunction with deferred heuristic evaluation, has been shown to result in substantial
performance improvement and deliver best results in the classical setting of operators with unit costs
(Richter & Helmert, 2009). The choice of 1000 as the boost value is not critical here, as we found
various values between 100 and 50000 to give similarly good results. Only outside this range does
performance drop noticeably.
Note that when using action costs, the use of preferred operators may be even more helpful
than in the classical setting. For example, if all operators have a cost of 0, a heuristic using pure
cost estimates might assign the same heuristic value of 0 to all states in the state space, giving no
guidance to search at all. Preferred operators, however, still provide the same heuristic guidance
in this case as in the case with unit action costs. While this is an extreme example, similar cases
appear in practice, e. g. in the IPC 2008 domain Openstacks, where all operators except for the one
opening a new stack have an associated cost of 0.
Pseudo code. Algorithm 1 shows pseudo code for the greedy best-first search. The main loop
(lines 2536) runs until either a goal has been found (lines 2729) or the search space has been
exhausted (lines 3233). The closed list contains all seen states and also keeps track of the links
between states and their parents, so that a plan can be efficiently extracted once a goal state has
been found (line 28). In each iteration of the loop, the search adds the current state (initially the
start state) to the closed list and processes it (lines 3031), unless the state has been processed
before, in which case it is ignored (line 26). By contrast, weighted A search processes states again
whenever they are reached via a path with lower cost than before, and updates their parent links
in the closed list accordingly. Then the search selects the next open list to be used (the one with
highest priority, line 34), decreases its priority and extracts the next state to be processed (lines
3536). The processing of a state includes calculating its heuristic values and preferred operators
with both heuristics (lines 34), expanding it, and inserting the successors into the appropriate open
133

fiRichter & Westphal

Global variables:
 = hV, s0 , s? , O, Ci
regFF , pref FF , regLM , pref LM
best seen value
priority
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:

. Planning task to solve
. Regular and preferred open lists for each heuristic
. Best heuristic value seen so far for each heuristic
. Numerical priority for each queue

function expand state(s)
progress  False
for h  {FF, LM} do
h(s), preferred ops(h, s)  heuristic value of s and preferred operators given h
if h(s) < best seen value[h] then
progress  True
best seen value[h]  h(s)
if progress then
. Boost preferred-operator queues
priority[pref FF ]  priority[pref FF ] + 1000
priority[pref LM ]  priority[pref LM ] + 1000
succesor states  { s[o] | o  O and o applicable in s }
for s0  succesor states do
for h  {FF, LM} do
add s0 to queue regh with value h(s)
. Deferred evaluation
0
if s reached by operator o  preferred ops(h, s) then
add s0 to queue pref FF with value FF(s), and to queue pref LM with value LM(s)
function greedy bfs lama
closed list  
for h  {FF, LM} do
. Initialize FF and landmark heuristics
best seen value[h]  
for l  {reg, pref } do
. Regular and preferred open lists for each heuristic
lh  
priority[lh ]  0
current state  s0
loop
if current state < closed list then
if s = s? then
extract plan  by tracing current state back to initial state in closed list
return 
closed list  closed list  {current state}
expand state(current state)
if all queues are empty then
return failure
. No plan exists
q  non-empty queue with highest priority
priority[q]  priority[q]  1
. Get lowest-valued state from queue q
current state  pop state(q)
Algorithm 1: The greedy best-first-search with search enhancements used in LAMA.
134

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

lists (lines 1116). If it is determined that a new best state has been found (lines 5-7), the preferredoperator queues are boosted by 1000 (lines 8-10).
3.3.1 Restarting Anytime Search
LAMA was developed for the International Planning Competition 2008 and is tailored to the conditions of this competition in several ways. In detail, those conditions were as follows. While in
previous competitions coverage, plan quality and runtime were all used to varying degrees in order
to determine the effectiveness of a classical planning system, IPC 2008 introduced a new integrated
performance criterion. Each operator in the PDDL input had an associated non-negative integer
action cost, and the aim was to find a plan of lowest-possible total cost within a given time limit
of 30 minutes per task. Given that a planner solves a task at all within the time limit, this new
performance measure depends only on plan quality, not on runtime, and thus suggests guiding the
search towards a cheapest goal rather than a closest goal as well as using all of the available time to
find the best plan possible.
Guiding the search towards cheap goals may be achieved in two ways, both of which LAMA
implements: firstly, the heuristics should estimate the cost-to-go, i. e., the cost of reaching the goal
from a given state, rather than the distance-to-go, i. e., the number of operators required to reach the
goal. Both the landmark heuristic and the FF heuristic employed in LAMA are therefore capable of
using action costs. Secondly, the search algorithm should not only take the cost-to-go from a given
state into account, but also the cost necessary for reaching that state. This is the case for weighted
A search as used in LAMA. To make the most of the available time, LAMA employs an anytime
approach: it first runs a greedy best-first search, aimed at finding a solution as quickly as possible.
Once a plan is found, it searches for progressively better solutions by running a series of weighted
A searches with decreasing weight. The cost of the best known solution is used for pruning the
search, while decreasing the weight over time makes the search progressively less greedy, trading
speed for solution quality.
Several anytime algorithms based on weighted A have been proposed (Hansen & Zhou, 2007;
Likhachev, Ferguson, Gordon, Stentz, & Thrun, 2008). Their underlying idea is to continue the
weighted A search past the first solution, possibly adjusting search parameters like the weight or
pruning bound, and thus progressively find better solutions. The anytime approach used in LAMA
differs from these existing algorithms in that we do not continue the weighted A search once it
finds a solution. Instead, we start a new weighted A search, i. e., we discard the open lists of
the previous search and re-start from the initial state. While resulting in some duplicate effort, these
restarts can help overcome bad decisions made by the early (comparatively greedy) search iterations
with high weight (Richter et al., 2010). This can be explained as follows: After finding a goal state
sg , the open lists will usually contain many states that are close to sg in the search space, because the
ancestors of sg have been expanded; furthermore, those states are likely to have low heuristic values
because of their proximity to sg . Hence, if the search is continued (even after updating the open
lists with lower weights), it is likely to expand most of the states around sg before considering states
that are close to the initial state. This can be critical, as it means that the search is concentrating
on improving the end of the current plan, as opposed to its beginning. A bad beginning of a plan,
however, may have severe negative influence on its quality, as it may be impossible to improve the
quality of the plan substantially without changing its early operators.
135

fiRichter & Westphal

3.8
10.6
3.4
9.8
2.6
8.2
1.8
7.6
1.0
7.0
1.0
8.0

3.8
9.6
3.4
8.8
2.6
8.2
1.8
7.6
1.0
7.0

g1

3.8
8.6

s

4.0
9.0

2.6
8.2
1.8
7.6
1.0
7.0
1.0
8.0

g2

(a) initial search, w = 2

2.6
8.9
2.6
8.9
2.6
8.9

2.6
7.9
1.8
6.7
1.8
7.7
1.8
8.7

3.8
8.7
3.4
8.1
2.6
6.9
1.8
6.7
1.0
6.5
1.0
7.5

3.8
7.7

X

s

4.0
7.0

X
X
X
X

g1

2.6 1.9 2.0
6.9 6.85 8.0
1.8 1.9 1.0
6.7 6.85 6.5
1.0 1.9 1.0
6.5 7.85 6.5
1.0 1.9
7.5 8.85

2.0
9.0
1.0
7.5

g2

(b) continued search, w = 1.5

3.8
7.7
3.4
7.1

3.8
6.7

s

4.0
7.0

2.0
6.0
1.0
5.5
1.0
6.5

4.0
8.0
3.0
6.5
2.0
6.0
1.0
5.5

g2

3.0
7.5
2.0
6.0
1.0
5.5
1.0
6.5

g1
(c) restarted search, w = 1.5

Figure 1: The effect of low-h bias. For all grid states generated by the search, h-values are shown
above f 0 -values. (a) Initial weighted A search finds a solution of cost 6. (b) Continued search
expands many states around the previous Open list (grey cells), finding another sub-optimal solution
of cost 6. (c) Restarted search quickly finds the optimal solution of cost 5.

136

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Consider the example of a search problem shown in Figure 1. The task is to reach a goal state
(g1 or g2) from the start state s in a gridworld, where the agent can move with cost 1 to each of
the 8 neighbours of a cell if they are not blocked. The heuristic values are inaccurate estimates of
the straight-line goal distances of cells. In particular, the heuristic values underestimate distances
in the left half of the grid. We conduct a weighted A search with weight 2 in Figure 1a (assuming
for simplicity a standard textbook search, i. e., no preferred operators and no deferred evaluation).
Because the heuristic values to the left of s happen to be lower than to the right of s, the search
expands states to the left and finds goal g1 with cost 6. The grey cells are generated, but not
expanded in this search phase, i. e., they are in the open list. In Figure 1b, the search continues with
a reduced weight of 1.5. A solution with cost 5 consists in turning right from s and going to g2.
However, the search will first expand all states in the open list that have an f 0 -value smaller than 7.
After expanding a substantial number of states, the second solution it finds is a path which starts off
left of s and takes the long way around the obstacle to g2, again with cost 6. If we instead restart
with an empty open list after the first solution (Figure 1c), fewer states are expanded. The critical
state to the right of s is expanded quickly and the optimal path is found.
Note that in the above example, it is in particular the systematic errors of the heuristic values
that leads the greedy search astray and makes restarts useful. In planning, especially when using
deferred evaluation, heuristic values may also be fairly inaccurate, and restarts can be useful. In
an experimental comparison on all tasks from IPC 1998 to IPC 2006 (Richter et al., 2010) this
restarting approach performed notably better than all other tested methods, dominating similar algorithms based on weighted A (Hansen, Zilberstein, & Danilchenko, 1997; Hansen & Zhou, 2007;
Likhachev, Gordon, & Thrun, 2004; Likhachev et al., 2008), as well as other anytime approaches
(Zhou & Hansen, 2005; Aine, Chakrabarti, & Kumar, 2007).
3.3.2 Using cost and distance estimates
Both heuristic estimators used in LAMA are cost-sensitive, aiming to guide the search towards
high-quality solutions. Focusing a planner purely on action costs, however, may be dangerous, as
cheap plans may be longer and more difficult to find, which in the worst case could mean that the
planner fails to find a plan at all within the given time limit. Zero-cost operators present a particular
challenge: since zero-cost operators can always be added to a search path for free, even a costsensitive search algorithm like weighted A may explore very long search paths without getting
closer to a goal. Methods have been suggested that allow a trade-off between the putative cost-to-go
and the estimated goal distance (Gerevini & Serina, 2002; Ruml & Do, 2007). However, they require the user to specify the relative importance of cost versus distance up-front, a choice that was
not obvious in the context of IPC 2008. LAMA gives equal weight to the cost and distance estimates by adding the two values during the computation of its heuristic functions (for more details,
see Sections 5 and 6). This measure is a very simple one, and its effect changes depending on the
magnitude and variation of action costs in a problem: the smaller action costs are, the more this
method favours short plans over cheap plans. For example, 5 zero-cost operators result in an estimated cost of 5, whereas 2 operators of cost 1 result in an estimated cost of 4. LAMA would thus
prefer the 2 operators of cost 1 over the 5 zero-cost operators. By contrast, when the action costs
in a planning task are larger than the length of typical plans, the cost estimates dominate the distance estimates and LAMA is completely guided by costs. Nevertheless this simple measure proves
useful on the IPC 2008 benchmarks, outperforming pure cost search in our experiments. More so137

fiRichter & Westphal

A
C

B

E

plane

box
D
truck

Figure 2: A simple Logistics task: transport the box from location B to location E.

phisticated methods for automatically balancing cost against distance (for example by normalising
the action costs in a given task with respect to their mean or median) are a topic of future work.

4. Landmarks
Landmarks are subgoals that must be achieved in every plan. They were first introduced by Porteous,
Sebastia and Hoffmann (2001) and were later studied in more depth by the same authors (Hoffmann,
Porteous, & Sebastia, 2004). Using landmarks to guide the search for a solution in planning is an
intuitive approach that humans might use. Consider the well-known benchmark domain Logistics,
where the goal is to deliver objects (e. g. boxes) between various locations using a fleet of vehicles.
Cities consist of sets of locations, where trucks may transport boxes within the city, whereas planes
have to be used between cities. An example Logistics task is shown in Figure 2. Arguably the first
mental step a human would perform, when trying to solve the task in Figure 2, is to realise that the
box must be transported between the two cities, from the left city (locations AD) to the right city
(location E), and that therefore, the box will have to be transported in the plane. This in turn means
that the box will have to be at the airport location C, so it can be loaded into a plane. This partitions
the task into two subproblems, one of transporting the box to the airport at location C, and one of
delivering it from there to the other city. Both subproblems are smaller and easier to solve than the
original task.
Landmarks capture precisely these intermediate conditions that can be used to direct search: the
facts L1 = box is at C and L2 = box is in plane are landmarks in the task shown in Figure 2.
This knowledge, as well as the knowledge that L1 must become true before L2 , can be automatically
extracted from the task in a preprocessing step (Hoffmann et al., 2004).
LAMA uses landmarks to derive goal-distance estimates for a heuristic search. It measures
the goal distance of a state by the number of landmarks that still need to be achieved on the path
from this state to a goal. Orderings between landmarks are used to infer which landmarks should
be achieved next, and whether certain landmarks have to be achieved more than once. In addition,
preferred operators (Helmert, 2006) are used to suggest operators that achieve those landmarks that
need to become true next. As we have recently shown, this method for using landmarks leads to
substantially better performance than the previous use of landmarks by Hoffmann et al., both in
terms of coverage and in terms of plan quality (Richter et al., 2008). We discuss the differences
between their approach and ours in more detail in Section 4.3. In the following section we define
138

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

A
plane1
E

C

B
box

plane2
F

truck2

D
truck1

Figure 3: Extended logistics task: transport the box from location B to location F.
landmarks and their orderings formally, including some useful special cases that can be detected
efficiently.
4.1 Definitions
Hoffmann et al. (2004) define landmarks as facts that are true at some point in every plan for a
given planning task. They also introduce disjunctive landmarks, defined as sets of facts of which
at least one needs to be true at some point. We subsume their landmark definitions into a more
general definition based on propositional formulas, as we believe this to be useful for future work
on the topic of landmarks. It should be noted, however, that LAMA currently only supports fact
landmarks and disjunctions of facts (for more details, see Section 4.2). Hoffmann et al. show that
it is PSPACE-hard to determine whether a given fact is a landmark, and whether an ordering holds
between two landmarks. Their complexity results carry over in a straight-forward way to the more
general case of propositional formulas, so we do not repeat the proofs.
Definition 3. Landmark
Let  = hV, s0 , s? , O, Ci be a planning task in finite-domain representation, let  = ho1 , . . . , on i be
an operator sequence applicable in s0 , and let i, j  {0, . . . , n}.
 A propositional formula  over the facts of  is called a fact formula.
 A fact F is true at time i in  iff F  s0 [ho1 , . . . , oi i].
 A fact formula  is true at time i in  iff  holds given the truth value of all facts of  at time
i. At any time i < 0,  is not considered true.
 A fact formula  is a landmark of  iff in each plan for ,  is true at some time.
 A propositional formula  over the facts of  is added at time i in  iff  is true at time i in
, but not at time i  1 (it is considered added at time 0 if it is true in s0 ).
 A fact formula  is first added at time i in  iff  is true at time i in , but not at any time j < i.
Note that facts in the initial state and facts in the goal are always landmarks by definition.
The landmarks we discussed earlier for the example task in Figure 2 were all facts. However,
more complex landmarks may be required in larger tasks. Consider an extended version of the
139

fiRichter & Westphal

example, where the city on the right has two airports, and there are multiple planes and trucks,
as depicted in Figure 3. The previous landmark L1 = box is at C is still a landmark in our
extended example. However, L2 = box is in plane has no corresponding fact landmark in this
task, since neither box is in plane1  nor box is in plane2  is a landmark. The disjunction box
is in plane1  box is in plane2 , however, is a landmark. In the following we refer to landmarks
that are facts as fact landmarks, and to disjunctions of facts as disjunctive landmarks. While the
use of disjunctive landmarks has been shown to improve performance, compared to using only fact
landmarks (Richter et al., 2008), more complex landmarks introduce additional difficulty both with
regard to their detection and their handling during planning. As mentioned before, LAMA currently
only uses fact landmarks and disjunctive landmarks, rather than general propositional formulas. The
extension to more complex types of landmarks is an interesting topic of future work. (See Keyder,
Richter and Helmert, 2010, for a discussion of conjunctive landmarks).
Various kinds of orderings between landmarks can be defined and exploited during the planning
phase. We define three types of orderings for landmarks, which are equivalent formulations of the
definitions by Hoffmann et al. (2004) adapted to the FDR setting:
Definition 4. Orderings between landmarks
Let  and  be landmarks in an FDR planning task .
 We say that there is a natural ordering between  and , written   , if in each plan
where  is true at time i,  is true at some time j < i.
 We say that there is a necessary ordering between  and , written  n , if in each plan
where  is added at time i,  is true at time i  1.
 We say that there is a greedy-necessary ordering between  and , written  gn , if in
each plan where  is first added at time i,  is true at time i  1.
Natural orderings are the most general; every necessary or greedy-necessary ordering is natural,
but not vice versa. Similarly, every necessary ordering is greedy-necessary, but not vice versa.
Knowing that a natural ordering is also necessary or greedy-necessary allows deducing additional
information about plausible temporal relationships between landmarks, as described later in this
section. Also, the landmark heuristic in LAMA uses this knowledge to deduce whether a landmark
needs to be achieved more than once. As a theoretical concept, necessary orderings ( is always true
in the step before ) are more straightforward and appealing than greedy-necessary orderings ( is
true in the step before  becomes true for the first time). However, methods that find landmarks
in conjunction with orderings can often find many more landmarks when using the more general
concept of greedy-necessary orderings (Hoffmann et al., 2004). LAMA follows this paradigm and
finds greedy-necessary (as well as natural) orderings, but not necessary orderings. In our example in
Figure 3, box is in truck1  must be true before box is at C and also before box is at F. The first
of these orderings is greedy-necessary, but not necessary, and the second is neither greedy-necessary
nor necessary, but natural.
Hoffmann et al. (2004) propose further kinds of orderings between landmarks that can be usefully exploited. For example, reasonable orderings, which were first introduced in the context of
top-level goals (Koehler & Hoffmann, 2000), are orderings that do not necessarily hold in a given
planning task. However, adhering to these orderings may save effort when solving the task. In our
example task, it is reasonable to load the box onto truck1 before driving the truck to the airport at
140

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

C. However, this order is not guaranteed to hold in every plan, as it is possible, though not reasonable, to drive the truck to C first, then drive to B and collect the box, and then return to C. The idea
is that if a landmark  must become false in order to achieve a landmark , but  is needed after ,
then it is reasonable to achieve  before  (as otherwise, we would have to achieve  twice). The
idea may be applied iteratively, as we are sometimes able to find new, induced reasonable orderings
if we restrict our focus to plans that obey a first set of reasonable orderings. Hoffmann et al. call
the reasonable orderings found in such a second pass obedient-reasonable orderings. The authors
note that conducting more than two iterations of this process is not worthwhile, as it typically does
not result in notable additional benefit. The following definition characterises these two types of
orderings formally.
Definition 5. Reasonable orderings between landmarks
Let  and  be landmarks in an FDR planning task .
 We say that there is a reasonable ordering between  and , written  r , if for every plan
 where  is added at time i and  is first added at time j with i < j, it holds that  is not true
at time m with m  {i + 1, . . . , j} and  is true at some time k with j  k.
 We say that a plan  obeys a set of orderings O, if for all orderings  x   O, regardless
of their type, it holds that  is first added at time i in  and  is not true at any time j  i.
 We say that there is an obedient-reasonable ordering between  and  with regard to a set of
orderings O, written  O
r , if for every plan  obeying O where  is added at time i and 
is first added at time j with i < j, it holds that  is not true at time m with m  {i + 1, . . . , j}
and  is true at some time k with j  k.
Our definitions are equivalent to those of Hoffmann et al. (2004), except that we care only
about plans rather than arbitrary operator sequences, allowing us to (theoretically) identify more
reasonable orderings. In practice, we use the same approximation techniques as Hoffmann et al.,
thus generating the same orderings.
A problem with reasonable and obedient-reasonable orderings is that they may be cyclic, i. e.,
chains of orderings  r  x . . . r  for landmarks  and  may exist (Hoffmann et al., 2004).
This is not the case for natural orderings, as their definition implies that they cannot be cyclic in
solvable tasks.
In addition, the definitions as given above are problematic in special cases. Note that the definition of a reasonable ordering  r  includes the case where there exist no i < j such that  is
added at time i and  is first added at time j, i. e., the case where it holds that in all plans  is first
added (a) before or (b) at the same time as .1 While (a) implies that reasonable orderings are a
generalisation of natural orderings, which might be regarded as a desirable property, (b) may lead
to undesirable orderings. For example, it holds that  r  and  r  for all pairs ,  that are
first added at the same time in all plans, for instance if  and  are both true in the initial state.
Similarly, it holds that  r  for all . We use these definitions despite their weaknesses here,
and simply note that our planner does not create all such contentious orderings. LAMA does not
create reflexive orderings  r ; and  r  with ,  true in the initial state is only created if it
is assumed or proven that  must be true strictly after  at some point in any plan (see also Section
1. According to personal communication with the authors, this case was overlooked by Hoffmann et al.

141

fiRichter & Westphal

truck1 at D
truck1 at B

box at B

box in truck1
truck1 at C
plane1 at C  plane2 at C

box at C

box in plane1  box in plane2
box at F
Figure 4: Partial landmark graph for the example task shown in Figure 3. Bold arcs represent natural
orderings, dashed arcs represent reasonable orderings.

4.2.5). A re-definition of reasonable orderings, addressing the problems of the definition by Hoffmann et al. and identifying precisely the wanted/unwanted cases, is a topic of future work. Closely
connected is the question whether reasonable orderings should be interpreted as strict orderings,
where  should be achieved before  (as in the definition of obedience above), or whether we allow
achieving  and  simultaneously. We use the strict sense of obedience for reasons of consistency
with the previous work by Hoffmann et al., and because it aligns better with our intended meaning of
reasonable orderings, even though this strict interpretation of obedience does not fit the contentious
cases discussed above.
Landmarks and orderings may be represented using a directed graph called the landmark graph.
A partial landmark graph for our extended example is depicted in Figure 4. The following section
4.2 contains an extensive description of how landmarks and their orderings are discovered in LAMA.
Readers not interested in the exact details of this process may skip this description, as it is not central
to the rest of this paper. Section 4.3 discusses how our approach for finding and using landmarks
relates to previous work. Section 5 describes how landmarks are used as a heuristic estimator in
LAMA.
4.2 Extracting Landmarks and Orderings
As mentioned before, deciding whether a given formula is a landmark and deciding orderings between landmarks are PSPACE-hard problems. Thus, practical methods for finding landmarks are
incomplete (they may fail to find a given landmark or ordering) or unsound (they may falsely declare a formula to be a landmark, or determine a false ordering). Several polynomial methods have
been proposed for finding fact landmarks and disjunctive landmarks, such as back-chaining from
the goals of the task, using criteria based on the relaxed planning graph (Porteous et al., 2001; Hoffmann et al., 2004; Porteous & Cresswell, 2002), and forward propagation in the planning graph
(Zhu & Givan, 2003).
142

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

The algorithm used in LAMA for finding landmarks and orderings between them is partly based
on the previous back-chaining methods mentioned above, adapting them to the finite-domain representation including conditional effects. In addition, our algorithm exploits the finite-domain representation by using domain transition graphs to find further landmarks. We discuss the differences
between our method and the previous ones in detail in Section 4.3. The idea of back-chaining is to
start from a set of known landmarks and to find new fact landmarks or disjunctive landmarks that
must be true in any plan before an already known landmark may become true. This procedure starts
from the set of all goal facts, and stops when no more new landmarks can be found. Our method
identifies new landmarks and orderings by considering, for any given fact landmark or disjunctive
landmark  that is not true in the initial state:
 The shared preconditions of its possible first achievers. These are the operator preconditions
and effect conditions shared by all effects that can potentially first achieve . This method
has been adapted from previous work (see Section 4.3).
 For fact landmarks v 7 d, the domain transition graph (DTG) of v. Here, we identify nodes
in the DTG (i. e., values d0 of v) that must necessarily be traversed in order to reach d.
 A restricted relaxed planning graph lacking all operators that could possibly achieve . (There
are some subtleties involving conditional effects that will be explained later.) Every landmark
which does not occur in the last level of this graph can only be achieved after .
As in previous work (Porteous et al., 2001; Hoffmann et al., 2004), we subsequently use the discovered landmarks and orderings to derive reasonable and obedient-reasonable orderings in a postprocessing step. In the following, we give a detailed description of each step of the procedure for
finding landmarks and orderings in LAMA. High-level pseudo code for our algorithm, containing
the steps described in the following sections 4.2.14.2.4, is shown in Algorithm 2.
4.2.1 Back-Chaining: Landmarks via Shared Preconditions of Possible First Achievers
First achievers of a fact landmark or disjunctive landmark  are those operators that potentially
make  true and can be applied at the end of a partial plan that has never made  true before. We
call any fact A that is a precondition for each of the first achievers a shared precondition. As at least
one of the first achievers must be applied to make  true, A must be true before  can be achieved,
and A is thus a landmark, with the ordering A gn . Any effect condition for  in an operator
can be treated like a precondition in this context, as we are interested in finding the conditions that
must hold for  to become true. We will in the following use the term extended preconditions of an
operator o for  to denote the union of the preconditions of o and its effect conditions for . The
extended preconditions shared by all achievers of a fact are calculated in line 19 of Algorithm 2. In
addition, we can create disjunctive landmarks  by selecting, from the precondition facts of the first
achievers, sets of facts such that each set contains one extended precondition fact from each first
achiever (line 22). As one of the first achievers must be applied to make  true, one of the facts in
 must be true before , and the disjunction  is thus a landmark, with the ordering  gn . Since
the number of such disjunctive landmarks is exponential in the number of achievers of , we restrict
ourselves to disjunctions where all facts stem from the same predicate symbol, which are deemed
most helpful (Hoffmann et al., 2004). Furthermore, we discard any fact sets of size greater than
four, though we found this restriction to have little impact compared to the predicate restriction.
143

fiRichter & Westphal

Global variables:
 = hV, s0 , s? , O, Ci
LG = hL, Oi
queue
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:

. Planning task to solve
. Landmark graph of 
. Landmarks to be back-chained from

function add landmark and ordering(,  x )
if  is a fact and   L :  .  and  |=  then
. Prefer fact landmarks
L  L \ {}
. Remove disjunctive landmark
O  O \ { ( x ), ( x ) |   L }
. Remove obsolete orderings
if   L :  .  and var()  var() ,  then . Abort on overlap with existing landmark
return
if  < L then
. Add new landmark to graph
L  L  {}
queue  queue  {}
O  O  { x }
. Add new ordering to graph
function identify landmarks
LG  hs? , i
. Landmark graph starts with all goals, no orderings
queue  s?
further orderings  
. Additional orderings (see Section 4.2.3)
while queue ,  do
  pop(queue)
if s0 6|=  then
RRPG  the restricted relaxed plan graph for 
preshared  shared extended preconditions for  extracted from RRPG
for   preshared do
add landmark and ordering(,  gn )
predisj  sets of facts covering shared extended preconditions for  given RRPG
for   predisj do
if s0 6|=  then
add landmark and ordering(,  gn )
if  is a fact then
prelookahead  extract landmarks from DTG of the variable in  using RRPG
for   prelookahead do
add landmark and ordering(,   )
potential orderings  potential orderings  {   F | F never true in RRPG }
add further orderings between landmarks from potential orderings

Algorithm 2: Identifying landmarks and orderings via back-chaining, domain transition graphs and
restricted relaxed planning graphs.

144

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

p1

A

B

t1

E

t2

C

p2

D

F

Figure 5: Domain transition graph for the location of the box in our extended example (Figure 3).

Since it is PSPACE-hard to determine the set of first achievers of a landmark  (Hoffmann et al.,
2004), we use an over-approximation containing every operator that can possibly be a first achiever
(Porteous & Cresswell, 2002). By intersecting over the extended preconditions of (possibly) more
operators we do not lose correctness, though we may miss out on some landmarks. The approximation of first achievers of  is done with the help of a restricted relaxed planning graph. During
construction of the graph we leave out any operators that would add  unconditionally, and we also
ignore any conditional effects which could potentially add . When the relaxed planning graph
levels out, its last set of facts is an over-approximation of the facts that can be achieved before  in
the planning task. Any operator that is applicable given this over-approximating set and achieves 
is a possible first achiever of .
4.2.2 Landmarks via Domain Transition Graphs
Given a fact landmark L = {v 7 l}, we can use the domain transition graph of v to find further fact
landmarks v 7 l0 (line 27) as follows. If the DTG contains a node that occurs on every path from
the initial state value s0 (v) of a variable to the landmark value l, then that node corresponds to a
landmark value l0 of v: We know that every plan achieving L requires that v takes on the value l0 ,
hence the fact L0 = {v 7 l0 } can be introduced as a new landmark and ordered naturally before L. To
find these kinds of landmarks, we iteratively remove one node from the DTG and test with a simple
graph algorithm whether s0 (v) and l are still connected  if not, the removed node corresponds to
a landmark. We further improve this procedure by removing, as a preprocessing step, all nodes for
which we know that they cannot be true before achieving L. These are the nodes that correspond to
facts other than L and do not appear in the restricted RPG that never adds L. Removing these nodes
may decrease the number of paths reaching L and may thus allow us to find more landmarks.
Consider again the landmark graph of our extended example, shown in Figure 4. Most of its
landmarks and orderings can be found via the back-chaining procedure described in the previous
section, because the landmarks are direct preconditions for achieving their successors in the graph.
There are two exceptions: box in truck1  and box at C. These two landmarks are however found
with the DTG method. The DTG in Figure 5 immediately shows that the box location must take on
both the value t1 and the value C on any path from its initial value B to its goal value F.
145

fiRichter & Westphal

4.2.3 Additional Orderings from Restricted Relaxed Planning Graphs
The restricted relaxed planning graph (RRPG) described in Section 4.2.1, which for a given landmark  leaves out all operators that could possibly achieve , can be used to extract additional
orderings between landmarks. Any landmark  that does not appear in this graph cannot be reached
before , and we can thus introduce a natural ordering   . For efficiency reasons, we construct
the RRPG for  only once (line 18), i. e., when needed to find possible first achievers of  during
the back-chaining procedure. We then extract all orderings between  and facts that can only be
reached after  (line 30). For all such facts F that are later recognised to be landmarks, we then
introduce the ordering   F (line 31).
4.2.4 Overlapping Landmarks
Due to the iterative nature of the algorithm it is possible that we find disjunctive landmarks for
which at least one of the facts is already known to be a fact landmark. In such cases, we let fact
landmarks take precedence over disjunctive ones, i. e., when a disjunctive landmark is discovered
that includes an already known fact landmark, we do not add the disjunctive landmark. Conversely,
as soon as a fact landmark is found that is part of an already known disjunctive landmark, we discard
the disjunctive landmark including its orderings2 , and add the fact landmark instead. To keep the
procedure and the resulting landmark graph simple, we furthermore do not allow landmarks to
overlap. Whenever some fact from a newly discovered disjunctive landmark is also part of some
already known landmark, we do not add the newly discovered landmark. All these cases are handled
in the function add landmark and ordering (lines 1 10).
4.2.5 Generating Reasonable and Obedient-Reasonable Orderings
We want to introduce a reasonable ordering L r L0 between two (distinct) fact landmarks L and
L0 if it holds that (a) L0 must be true at the same time or after first achieving L, and (b) achieving
L0 before L would require making L0 false again to achieve L. We approximate both (a) and (b) as
proposed by Hoffmann et al. (2004) with sufficient conditions. In the case of (a), we test if L0  s? or
if we have a chain of natural or greedy-necessary orderings between landmarks L = L1  . . .  Ln ,
with n > 1, Ln1 , L0 and a greedy-necessary ordering L0 gn Ln . For (b) we check whether (i) L
and L0 are inconsistent, i. e., mutually exclusive, or (ii) all operators achieving L have an effect
that is inconsistent with L0 , or (iii) there is a landmark L00 inconsistent with L0 with the ordering
L00 gn L.
Inconsistencies between facts can be easily identified in the finite-domain representation if the
facts are of the form v 7 d and v 7 d0 , i. e., if they map the same variable to different values. In
addition, LAMA uses the groups of inconsistent facts computed by its translator component.
In a second pass, obedient-reasonable orderings are added. This is done with the same method
as above, except that now reasonable orderings are used in addition to natural and greedy-necessary
orderings to derive the fact that a landmark L0 must be true after a landmark L. Finally, we use a
simple greedy algorithm to break possible cycles due to reasonable and obedient-reasonable orderings in the landmark graph, where every time a cycle is identified, one of the involved reasonable or
2. Note that an ordering {F, G}   neither implies F   nor G   in general. Conversely,   {F, G} neither
implies   F nor   G.

146

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

obedient-reasonable orderings is removed. The algorithm removes obedient-reasonable orderings
rather than reasonable orderings whenever possible.
4.3 Related Work
Orderings between landmarks are a generalisation of goal orderings, which have been frequently
exploited in planning and search in the past. In particular, the approach by Irani and Cheng (Irani &
Cheng, 1987; Cheng & Irani, 1989) is a preprocessing procedure like ours that analyses the planning
task to extract necessary orderings between goals, which are then imposed on the search algorithm.
A goal A is ordered before a goal B in this approach if in any plan A is necessarily true before B.
Koehler and Hoffmann (2000) introduce reasonable orderings for goals.
Hoffmann et al. (2004), in an article detailing earlier work by Porteous et al. (2001), introduce
the idea of landmarks, generalise necessary and reasonable orderings from goals to landmarks, and
propose methods for finding and using landmarks for planning. The proposed method for finding
landmarks, which was subsequently extended by Porteous and Cresswell (2002), is very closely
related to ours. Hoffmann et al. propose a method for finding fact landmarks that proceeds in three
stages. First, potential landmarks and orderings are suggested by a fast candidate generation procedure. Second, a filtering procedure evaluates a sufficient condition for landmarks on each candidate
fact, removing those which fail the test. Third, reasonable and obedient-reasonable orderings between the landmarks are approximated. This step is largely identical in their approach and ours,
except that we use different methods to recognise inconsistencies between facts.
The generation of landmark candidates is done via back-chaining from the goal much like in
our approach, and intersecting preconditions over all operators which can first achieve a fact F
and appear before F in the relaxed planning graph. Note that even if all these operators share a
common precondition L, there might be other first achievers of F (appearing after F in the relaxed
planning graph) that do not have L as a precondition, and hence L is not a landmark. To test whether
a landmark candidate L found via back-chaining is indeed a landmark, Hoffmann et al. (2004)
build a restricted relaxed planning task leaving out all operators which could add L. If this task is
unsolvable, then L is a landmark. This is a sufficient, but not necessary condition: if L is necessary
for solving the relaxed task it is also necessary for solving the original task, while the converse is not
true. This verification procedure guarantees that the method by Hoffmann et al. only generates true
landmarks; however, unsound orderings may be established due to unsound landmark candidates.
While the unsound landmarks are pruned after failing the verification test, unsound orderings may
remain.
Porteous and Cresswell (2002) propose the alternative approximation for first achievers of a
fact F that we use. They consider all first achievers that are possibly applicable before F and
thus guarantee the correctness of the found landmarks and orderings. They also find disjunctive
landmarks. Our method for landmark detection differs from theirs by adding detection of landmarks
via domain transition graphs, and detection of additional orderings via restricted relaxed planning
graphs. Porteous and Cresswell additionally reason about multiple occurrences of landmarks (if the
same landmark has to be achieved, made false again and re-achieved several times during all plans),
which we do not.
The approach by Hoffmann et al. (2004) exploits landmarks by decomposing the planning task
into smaller subtasks, making the landmarks intermediary goals. Instead of searching for the goal
of the task, it iteratively aims to achieve a landmark that is minimal with respect to the orderings. In
147

fiRichter & Westphal

detail, it first builds a landmark graph (with landmarks as vertices and orderings as arcs). Possible
cycles are broken by removing some arcs. The sources S of the resulting directed acyclic graph are
handed over to a base planner as a disjunctive goal, and a plan is generated to achieve one of the
landmarks in S . This landmark, along with its incident arcs, is then removed from the landmark
graph, and the process repeats from the end state of the generated plan. Once the landmark graph
becomes empty, the base planner is asked to generate a plan to the original goal. (Note that even
though all goal facts are landmarks and were thus achieved previously, they may have been violated
again.)
As a base planner for solving the subtasks any planner can be used; Hoffmann et al. (2004)
experimented with FF. They found that the decomposition into subtasks can lead to a more directed search, solving larger instances than plain FF in many domains. However, we found that
their method leads to worse average performance on the IPC benchmarks from 1998 to 2006 when
using Fast Downward as a base planner (Richter et al., 2008). Furthermore, the method by Hoffmann et al. often produces solutions that are longer than those produced by the base planner, as
the disjunctive search control frequently switches between different parts of the task which may
have destructive interactions. Sometimes this even leads to dead ends, so that this approach fails on
solvable tasks. By contrast, our approach incorporates landmark information while searching for
the original goal of the planning task via a heuristic function derived from the landmarks (see next
section). As we have recently shown, this avoids the possibility of dead-ends and usually generates
better-quality solutions (Richter et al., 2008).
Sebastia et al. (2006) extend the work by Hoffmann et al. by employing a refined preprocessing technique that groups landmarks into consistent sets, minimising the destructive interactions
between the sets. Taking these sets as intermediary goals, they avoid the increased plan length
experienced by Hoffmann et al. (2004). However, according to the authors this preprocessing is
computationally expensive and may take longer than solving the original problem.
Zhu and Givan (2003) propose a technique for finding landmarks by propagating necessary
predecessor information in a planning graph. Their definition of landmarks encompasses operators
that are necessary in any plan (called action landmarks), and they furthermore introduce the notion
of a causal landmark for fact landmarks that are required as a precondition for some operators
in every plan. They argue that fact landmarks which are not causal are accidental effects and
do not warrant being sought explicitly. Their algorithm computes action landmarks and causal
fact landmarks at the same time by propagating information during the construction of a relaxed
planning graph. An extended variant of their algorithm is also able to infer multiple occurrences
of landmarks. Gregory et al. (2004) build on their work to find disjunctive landmarks through
symmetry breaking.
Similar to our work, Zhu and Givan (2003) use the causal fact landmarks and action landmarks
to estimate the goal distance of a given state. To this end, they treat each fact landmark as a virtual
action (sets of operators that can achieve the fact landmark) and obtain a distance estimate by bin
packing. The items to be packed into bins are the real landmark actions (singletons) and virtual
actions, where each bin may only contain elements such that a pairwise intersection of the elements
is non-empty. Zhu and Givan employ a greedy algorithm to estimate the minimum number of bins
and use this value as distance estimate. Their experimental results are preliminary, however, and do
not demonstrate a significant advantage of their method over the FF planner.
148

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

5. The Landmark Heuristic
The LAMA planning system uses landmarks to calculate heuristic estimates. Since we know that
all landmarks must be achieved in order to reach a goal, we can approximate the goal distance of
a state s reached by a path (i. e., a sequence of states)  as the estimated number of landmarks that
still need to be achieved from s onwards. These landmarks are given by

L(s, ) B L \ Accepted(s, )  ReqAgain(s, )
where L is the set of all discovered landmarks, Accepted(s, ) is the set of accepted landmarks,
and ReqAgain(s, ) is the set of accepted landmarks that are required again, with the following
definitions based on a given landmarks graph (L, O) :

	


  L | s |=  and @( x )  O
 = hi





0
0
Accepted(s, ) B 
 = 0 ; hoi
Accepted(s0 [ ],  )    L | s |= 



	

 and ( x )  O :   Accepted(s0 [0 ], 0 )

ReqAgain(s, ) B   Accepted(s, ) | s 6|= 
	
and s? |=  or ( gn )  O :  < Accepted(s, )
A landmark  is first accepted in a state s if it is true in that state, and all landmarks ordered
before  are accepted in the predecessor state from which s was generated. Once a landmark has
been accepted, it remains accepted in all successor states. For the initial state, accepted landmarks
are all those that are true in the initial state and do not have any predecessors in the landmark graph.
An accepted landmark  is required again if it is not true in s and (a) it forms part of the goal or
(b) it must be true directly before some landmark  (i. e.,  gn ) where  is not accepted in s.
In the latter case, since we know that  must still be achieved and  must be true in the time step
before , it holds that  must be achieved again. The number |L(s, )| is then the heuristic value
assigned to state s. Pseudo code for the heuristic is given in Algorithm 3.
The landmark heuristic will assign a non-zero value to any state that is not a goal state, since
goals are landmarks that are always counted as required again per condition (a) above. However,
the heuristic may also assign a non-zero value to a goal state. This happens if plans are found that
do not obey the reasonable orderings in the landmark graph, in which case a goal state may be
reached without all landmarks being accepted.3 Hence, we need to explicitly test states for the goal
condition in order to identify goal states during search.
Note that this heuristic is path-dependent, i. e., it depends on the sequence of states by which s
is reached from the initial state. This raises the question of what happens if a state s can be reached
via several paths. In LAMA, the heuristic for a state is calculated only once, when it is first reached.
An alternative option would be to re-evaluate s each time a new path to s is discovered, taking into
account the information of all paths to s known at the time. As Karpas and Domshlak (2009) note,
we can calculate the landmarks that are accepted in s given a set of paths P to s as Accepted(s, P) B
T
P Accepted(s, ), since it holds that any landmark that is not achieved along all paths   P must
3. In the special case where  r  and  and  can become true simultaneously, we could avoid this by accepting
both  and  at once (Buffet & Hoffmann, 2010), or we could modify our definition of reasonable orderings such
that  r  does not hold unless  must become true strictly after . The general problem that goal states may be
assigned a non-zero value, however, still persists even with these modifications.

149

fiRichter & Westphal

Global variables:
 = hV, s0 , s? , O, Ci
LG = hL, Oi
Accepted

. Planning task to solve
. Landmark graph of 
. Landmarks accepted in states evaluated so far

function lm count heuristic(s, )
if  = hi then
. Initial state

	
Accepted(s, )    L | s0 |=  and @( x )  O
else
0  ho1 , . . . , on1 i for  = ho1 , . . . , on i
parent  s0 [0 ]
. Accepted(parent, 0 ) has been calculated before
Reached  {   L | s |=  and ( x )  O :   Accepted(parent, 0 ) }
Accepted(s, )  Accepted(parent, 0 )  Reached
NotAccepted  L \ Accepted(s, )
ReqGoal  { n  Accepted(s, ) | s 6|=  and s? |=  }
o
ReqPrecon    Accepted(s, ) | s 6|=  and  : ( gn )  O   < Accepted(s, )
return |NotAccepted  ReqGoal  ReqPrecon|
Algorithm 3: The landmark count heuristic.

be achieved from s onwards. The heuristic value of s can then be derived from this in an analogous
way as before.
The landmark heuristic as outlined above estimates the goal distance of states, i. e., the number
of operator applications needed to reach the goal state from a given state. To participate in IPC 2008,
we made this function cost-sensitive by weighting landmarks with an estimate of their minimum
cost. Apart from estimating goal distance by counting the number of landmarks that still need to
be achieved from a state, we estimate the cost-to-go from a state by the sum of all minimum costs
of those landmarks. The cost counted for each landmark is the minimum action cost of any of
its first achievers. (Alternative, more sophisticated methods for computing the costs of landmarks
are conceivable and are a potential topic of future work.) The heuristic value LAMA assigns to
a state is however not its pure cost-to-go estimate, but rather the sum of its cost estimate and its
distance estimate. By thus accounting for both the costs-to-go and the goal distances of states, this
measure aims to balance speed of the search and quality of the plans, and in particular counter-act
the problems that may arise from zero-cost operators (see Section 3.3).
We also generate preferred operators along with the landmark heuristic. An operator is preferred
in a state if applying it achieves an acceptable landmark in the next step, i. e., a landmark whose predecessors have already been accepted. If no acceptable landmark can be achieved within one step,
the preferred operators are those which occur in a relaxed plan to a nearest acceptable landmark. A
nearest landmark in the cost-unaware setting is one that is relaxed reachable with a minimal number
of operators, while in the cost-sensitive setting it is a landmark reachable with the cheapest hadd cost
(see Section 6), where again both cost and distance estimates are taken into account. This nearest
landmark can be computed by building a relaxed planning graph or, equivalently, performing a relaxed exploration (which is what LAMA does, see Section 6), and determining the earliest or least
costly occurrence of an acceptable landmark in this structure. A relaxed plan to this landmark is
150

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

then extracted, and the operators in this plan form preferred operators if they are applicable in the
current state.

6. The Cost-Sensitive FF/add Heuristic
When we first introduced the landmark heuristic (Richter et al., 2008), it proved not to be competitive on its own, compared to established heuristics like the FF heuristic (Hoffmann & Nebel,
2001). However, the joint use of the FF heuristic and the landmark heuristic in a multi-heuristic
search improved the performance of a planning system, compared with only using the FF heuristic.
This is thus the path LAMA follows. The FF heuristic is based on a relaxation of the planning task
that ignores delete effects, which in FDR tasks translates to allowing state variables to hold several
values simultaneously.
The FF heuristic for a state s is computed in two phases: the first phase, or forward phase,
calculates an estimate for each fact in the planning task of how costly it is to achieve the fact from
s in a relaxed task. Concurrently, it selects an operator called best support for each fact F, which
is a greedy approximation of a cheapest achiever (an achiever a of F where the costs of making a
applicable and applying it are minimal among all achievers of F, when starting in s). In the second
phase, a plan for the relaxed task is constructed based on the best supports for each fact. This is done
by chaining backwards from the goals, selecting the best supports of the goals, and then recursively
selecting the best supports for the preconditions of already selected operators. The union of these
best supports then constitutes the relaxed plan (i. e., for each fact its best support is only added to
the relaxed plan once, even if the fact is needed several times as a precondition). The length of the
resulting relaxed plan is the heuristic estimate reported for s.
The forward phase can be viewed as propagating cost information for operators and facts in
a relaxed planning graph (Hoffmann & Nebel, 2001). However, this graph does not need to be
explicitly constructed to compute the heuristic. Instead, a form of generalised Dijkstra cheapestpath algorithm as described by Liu, Koenig and Furcy (2002) is used in LAMA, which propagates
costs from preconditions to applicable operators and from operators to effects. In this method, each
operator and fact is represented only once, reducing the time and space requirements from O(NK),
where N is the size of the relaxed planning task and K the depth of the relaxed planning graph, to
O(N). In order to deal with conditional effects, operators with n effects are split into n operators
with one effect each, and the corresponding effect conditions are moved into the preconditions of
those operators. If any of those n operators is selected for inclusion in the relaxed plan, the original
operator is included instead (again, each operator is included in the relaxed plan only once).
The cost estimate for an operator in the original FF heuristic is its depth in the relaxed planning
graph, which in the case of planning with unit-cost operators is equivalent (Fuentetaja, Borrajo, &
Linares Lopez, 2009) to propagating costs via the hmax criterion (Bonet & Geffner, 2001). The hmax
criterion estimates the cost of an operator by the maximum over the costs of its preconditions, plus
the action cost of the operator itself (1 when planning without action costs). The cost of a fact is
estimated as the cost of its cheapest achiever, or zero if the fact is true in the current state s. While
originally proposed for unit-cost planning, this heuristic can be adapted to cost-based planning in a
straightforward way by using action costs in the cost propagation phase, and reporting the total cost
of the resulting relaxed plan, rather than its length, as the heuristic estimate.
Using other criteria for cost propagation results in variations of the FF heuristic (Bryce & Kambhampati, 2007; Fuentetaja et al., 2009). One variant that has been previously proposed in the litera151

fiRichter & Westphal

ture (Do & Kambhampati, 2003) is to use the hadd criterion (Bonet & Geffner, 2001). It is similar to
the hmax criterion except for estimating the cost of operators via the sum, rather than the maximum,
of the costs for their preconditions. We will in the following use the term FF/add for this variant of
the FF heuristic. Independently of us, Keyder and Geffner (2008) implemented the FF/add heuristic
which they call ha in their planner FF(ha ) at IPC 2008. A formal specification of the FF/add heuristic
can be found in their paper. The heuristic function in LAMA is similar to this cost-sensitive FF/add
heuristic. However, as with the landmark heuristic, LAMA is not purely guided by action costs,
but rather uses both cost and distance estimates equally. This means that during cost propagation,
each operator contributes its action cost plus 1 for its distance, rather than just its action cost, to the
propagated cost estimates.

7. Experiments
To evaluate how much each of the central features of LAMA contributes to its performance, we
have conducted a number of experiments comparing different configurations of these features. We
focus our detailed evaluation on the benchmark tasks from the International Planning Competition
(IPC) 2008, as we are interested in the setting of planning with action costs. The effect of landmarks
in classical planning tasks without actions costs has been studied in previous work (Richter et al.,
2008), but we provide summarising results for this case, using the domains of IPCs 19982006, in
Section 7.6. The benchmark set of IPC 2008 comprises 9 domains with 30 tasks each, resulting in a
total of 270 tasks. For one of the domains (Openstacks), two different formulations were available
(STRIPS and ADL). As in the competition, we report the better result of those two formulations for
each planner.
As described in Section 1, LAMA builds on the platform provided by Fast Downward in three
major ways: (1) through the use of landmarks, (2) by using cost-sensitive heuristics to guide search
for cheap plans, and (3) by employing anytime search to continue to search for better solutions while
time remains. To examine the usefulness of landmarks, we conduct experiments with and without
them, while keeping all other planner features fixed. The use of action costs in LAMA is the result of
a number of design decisions. Both the landmark heuristic and the FF/add heuristic have been made
cost-sensitive. However, rather than focusing purely on action costs, LAMA uses both distance
estimates and cost estimates in combination (see Section 3.3) to balance speed and quality of the
search. To measure the benefit of this combining approach, we test three different approaches to
dealing with costs: (a) using the traditional cost-unaware heuristics (distance estimates), (b) using
purely cost-sensitive heuristics (though using distance estimates for tie-breaking), and (c) using
the combination of the distance and cost estimates, as in LAMA. The different choices regarding
landmarks and approaches to action costs thus result in the following six planner configurations:
 F: Use the cost-unaware FF/add heuristic (estimating goal distance).
 Fc : Use the purely cost-sensitive FF/add heuristic (estimating cost-to-go).
 F+c : Use the FF/add heuristic that combines action costs and distances.
 FL: Use the cost-unaware variants of both the FF/add heuristic and the landmark heuristic.
 FLc : Use the purely cost-sensitive variants of both heuristics.
 FL+c : Use the variants that combine action costs and distances for both heuristics.
152

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Note that in contrast to the setting of optimal planning (Karpas & Domshlak, 2009), the landmark
heuristic by itself is not competitive in our case, and landmarks in LAMA are used only to provide
additional information to an already guided search. As such, we do not include any configurations using only landmarks as heuristic estimators in our detailed results. However, we provide
summarising results supporting our claim that they are not competitive.
Each configuration is run with iterated (anytime) search. When highlighting the contribution
of the iterated search, we report first solutions vs. final solutions, where the final solution of a
configuration is the last, and best, solution it finds within the 30-minute timeout. (Note that the
quality of a solution is always determined by its cost, irrespective of whether the heuristic used to
calculate it is cost-sensitive or not.) When discussing the three possible approaches to costs (costunaware search, purely cost-sensitive search, or LAMAs combination of distances and costs) we
write X, Xc , and X+c to denote the three cost approaches independently of the heuristics used.
We measure performance using the same criterion that was employed at IPC 2008 (Helmert
et al., 2008). Each planner configuration is run for 30 minutes per task. After the timeout, a planner
aggregates the ratio c /c to its total score if c is the cost of the plan it has found, and c is the cost of
the best known solution (e. g., a reference solution calculated by the competition organisers, or the
best solution found by any of the participating planners).
Experiments were run on the hardware used in the competition, a cluster of machines with Intel
Xeon CPUs of 2.66GHz clock speed. The time and memory limits were set to the same values as
in the competition, using a timeout of 30 minutes and a memory limit of 2 GB. In the following,
we first provide a general overview of the results. Then we discuss special cases, i. e., domains
where the results for certain configurations deviate from the overall trend, and try to give plausible
explanations for why this may happen.
7.1 Overview of Results
In this section, we show that the purely cost-based FF/add configuration Fc solves significantly
fewer tasks than its cost-unaware counterpart F. While Fc finds higher-quality solutions, this does
not make up for its low coverage (number of solved tasks) when measuring performance with the
IPC criterion. Using landmarks improves quality slightly, so that cost-unaware search using landmarks (FL) achieves the highest IPC performance score amongst our configurations. When using
the cost-sensitive FF/add heuristic, adding landmarks (resulting in the configurations FLc and FL+c )
increases coverage substantially, while incurring only a small loss in quality. Iterated search improves the scores of all configurations significantly. Lastly, using the combination of cost and
distance estimates in the heuristics (X+c ) is superior to pure cost-based search when using iterated search. Together, using landmarks and the combination of cost and distance estimates (FL+c )
achieves nearly the same performance as the FL configuration.
In the following, we support these findings with experimental data. In Section 7.1.1 (Performance in Terms of the IPC Score), we show that the cost-sensitive FF/add heuristic by itself scores
lowly in terms of the IPC criterion, but that landmarks and the combination of cost and distance estimates together make up for this bad performance. Furthermore, our results demonstrate the magnitude of the impact that iterated search has on the performance scores. In Section 7.1.2 (Coverage),
we show that the bad performance of the cost-sensitive FF/add heuristic is due to solving fewer
tasks, and that the use of landmarks mitigates this problem. In Section 7.1.3 (Quality), we present
data showing that the purely cost-sensitive FF/add heuristic finds higher-quality plans than the cost153

fiRichter & Westphal

Domain

Base

C3

Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total
(Total IPC 2008)

4
21
21
27
20
24
21
18
14
169
(176)

9
16
10
18
20
23
18
6
24
143
(151)

Domain

F

Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total

20
22
20
20
20
19
18
18
22
180

IPC Planners
FF(ha ) FF(has )
20
9
8
16
21
24
15
15
22
150
(157)

20
10
8
23
23
24
18
14
22
162
(169)

First solutions
FL FLc
Fc
F+c

FL+c

24
9
23
16
23
21
20
15
20
171

27
16
14
21
21
21
19
23
20
182

24
9
20
16
20
20
19
15
20
162

20
23
13
23
20
22
18
24
20
182

28
14
13
21
22
21
19
24
20
183

LAMA
28
20
27
21
29
26
24
27
25
227
(236)

Slowed LAMA
10
100

FL+c

27
20
27
19
29
25
22
25
24
218
()

28
22
27
22
29
26
23
26
24
227
()

26
17
26
12
26
22
15
21
17
183
()

Final solutions (iterated search)
F
Fc
F+c
FL FLc FL+c
23
29
27
23
29
24
24
19
23
220

24
10
29
16
29
24
24
17
21
194

25
15
28
16
29
25
24
17
22
201

26
27
27
24
29
29
22
24
20
229

28
16
28
22
29
24
23
26
23
217

28
22
27
22
29
26
23
26
24
227

Table 1: Performance scores (rounded to whole numbers) for planners scoring  100 points at
IPC 2008 (top) and our 6 experimental configurations (bottom). Scores for IPC planners were recalculated (see text). LAMA 10 and 100 refer to the results achieved by LAMA when slowed
down by factors of 10 and 100, respectively. FL+c is essentially the same as the IPC planner LAMA.

unaware FF/add heuristic in the first search, but that with iterated search, this difference all but
disappears. Furthermore, after iterated search the intermediate approach of using cost and distance
estimates scores higher than the purely cost-based search. LAMAs approach of using landmarks
and the combination of cost and distance estimates (FL+c ) thus effectively mitigates the bad performance of the cost-sensitive FF/add heuristic.
7.1.1 Performance in Terms of the IPC Score
The scores of all planners scoring more than 100 points at IPC 2008 are shown in the top part of
Table 1. Apart from LAMA, this includes a base planner run by the competition organisers (FF with
a preprocessing step that compiles away action costs), the FF(ha ) and FF(has ) planners by Keyder
154

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

and Geffner (2008) and the C3 planner by Lipovetzky and Geffner (2009). The plans found by
these planners have been obtained from the competition website (Helmert et al., 2008). However,
the scores for those plans depend on the best known solutions for the tasks. The scores we show
here thus differ from the ones published at IPC 2008, as we have re-calculated them to reflect new
best solutions found in our experiments. To illustrate the magnitude of the change, the original total
scores of the IPC planners are shown in parentheses in the last table row.
While the configuration FL+c results in essentially the same planner as (the IPC version of)
LAMA, we report its results again, as some minor corrections have been implemented in LAMA
since the competition. In addition, the planner makes arbitrary decisions at some points during
execution due to underlying programming library methods, leading to varying results. However,
as Table 1 shows these differences between FL+c and LAMA are very small. We have furthermore
added columns to the table showing the hypothetical results for LAMA that would be obtained if its
search were slowed down by the constant factors 10 and 100, respectively (i. e., the results obtained
when cutting of the search after 3 minutes, or 18 seconds, respectively). The numbers show that
LAMA still outperforms the other IPC planners even with a severe handicap, demonstrating that the
good performance of LAMA is not mainly due to an efficient implementation.
The bottom part of Table 1 contains the results for our six experimental configurations after the
first search iteration (left) and after the 30-minute timeout (right). As can be seen, both the use of
landmarks and iterated search lead to significant improvements in performance. Even with just one
of those two features our planner performs notably better than any of its competitors from IPC 2008.
(Note however that the baseline planner performed very badly in Cyber Security due to problems
with reading very large task descriptions.) In combination, the benefits of landmarks and iterated
search grow further: in cost-unaware search the use of landmarks results in 2 additional score points
for the first solutions, but in 9 additional points for the final solutions. Similar results hold for the
cost-sensitive configurations. This is mainly due to the Openstacks domain, where using landmarks
is highly detrimental to solution quality for the first solutions. Iterated search mitigates the problem
by improving quality to similar levels with and without landmarks. Overall, there is thus a slight
synergy effect between landmarks and iterated search, making the joint benefit of the two features
larger than the sum of their individual contributions. The effect of landmarks in the Openstacks
domain is discussed in more detail later.
The use of cost-sensitive search did not pay off in our experiments. Cost-unaware search is
always at least roughly equal, and often substantially better than the cost-sensitive configurations.
Cost-sensitive planning seems to be not only a problem for LAMA, but also for the other participating planners at IPC 2008: notably, all cost-sensitive competitors of LAMA fare worse than the
cost-ignoring baseline. In LAMA, best performance is achieved by using cost-unaware search with
landmarks and iterated search. However, using the combination of cost and distance estimates instead (FL+c ) leads to performance that is almost equally good. In particular, FL+c is substantially
better than the pure cost search FLc if iterated search is used.
A more detailed view on the same data is provided in Figure 6, where we show the performance
over time for our six experimental configurations. A data point at 100 seconds, for example, shows
the score the corresponding planner would have achieved if the timeout had been 100 seconds. As
the top panel shows, cost-sensitive search is consistently worse than cost-unaware search when using
only the FF/add heuristic. Using landmarks (see centre panel), the two settings FL and FL+c achieve
better performance than F, though FL+c needs 2 minutes to surpass F, while FL does so within 5
seconds. Pure cost search, even with landmarks (FLc ), performs worse than F at all times. The
155

fiRichter & Westphal

240
220

Score

200
180
160
F
Fc
F+c

140
120
1

10

100
Time (seconds)

1000

240
220

Score

200
180
160
F
FL
FLc
FL+c

140
120
1

10

100
Time (seconds)

1000

240
220

Score

200
180
160
F
FL
FLc
FL+c

140
120
1

10

100
Time (seconds)

1000

Figure 6: Score over time using iterated search (top and centre panel) and without iterated search,
i. e., showing first solutions only (bottom panel).

156

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Domain

Base

C3

FF(ha )

FF(has )

LAMA

FL+c

Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total

4
30
30
30
30
30
27
29
17
227

15
30
30
18
30
27
22
12
28
212

23
23
25
16
29
28
17
23
29
213

22
26
26
23
29
28
20
22
29
225

30
24
30
22
30
30
25
30
30
251

30
25
30
23
30
30
24
30
30
252

Domain

F

Fc

F+c

FL

FLc

FL+c

Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total

30
30
30
25
30
28
25
26
30
254

28
15
30
16
30
30
25
22
28
224

29
16
30
16
30
29
24
21
28
223

30
30
30
24
30
30
23
29
28
254

30
22
30
23
30
30
24
30
30
249

30
25
30
23
30
30
24
30
30
252

Table 2: Coverage for planners scoring  100 points at IPC 2008 (top) and our 6 experimental
configurations (bottom). Results of the IPC planners have been taken from the competition. FL+c is
essentially the same as the IPC planner LAMA.

bottom panel of Figure 6 shows that when not using iterated search, the performance of the 4 best
configurations FL, F, FL+c , and FLc is fairly similar eventually, but the cost-sensitive approaches
need more time than the cost-unaware configurations to reach the same performance levels.
7.1.2 Coverage
The bad performance of cost-sensitive search is surprising, given that our performance criterion
awards higher scores to cheaper plans. One explanation could be that this is mainly due to different coverage. If finding plans of high quality is substantially harder than finding plans of low
quality, then focusing on nearest goals rather than cheapest goals may solve more tasks within a
given time limit. In Table 2 we show the coverage for all considered planners and configurations.
The numbers confirm that when not using landmarks, the coverage of cost-unaware search is indeed
substantially higher than the coverage of cost-sensitive search. However, with landmarks, the differences in coverage between the various cost approaches are small. In particular, landmarks do not
improve coverage further for the cost-unaware search, but bring the cost-sensitive configurations up
157

fiRichter & Westphal

Domain
Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total

Domain
Cyber Security
Elevators
Openstacks
PARC Printer
Peg Solitaire
Scanalyzer
Sokoban
Transport
Woodworking
Total

Fc / F
Tasks C. Ratio
28
15
30
16
30
28
23
21
28
219

0.64
1.16
0.83
0.79
0.87
0.94
0.94
1.01
1.02
0.88

Fc / F
Tasks C. Ratio
28
15
30
16
30
28
23
21
28
219

0.81
1.51
0.95
0.97
0.99
1.01
1.01
0.98
0.99
0.99

F+c / F
Tasks C. Ratio
29
16
30
16
30
28
22
21
28
220

0.69
1.15
1.00
0.79
1.02
0.93
0.98
1.00
1.02
0.94

F+c / F
Tasks C. Ratio
29
16
30
16
30
28
22
21
28
220

0.82
1.05
0.96
0.97
1.00
0.93
1.00
0.89
0.94
0.94

FLc / Fc
Tasks C. Ratio
28
14
30
15
30
30
24
22
28
221

0.81
0.89
1.98
1.05
1.04
0.98
0.98
0.89
1.07
1.06

FLc / Fc
Tasks C. Ratio
28
14
30
15
30
30
24
22
28
221

0.81
0.89
1.04
1.01
1.02
1.02
1.02
0.89
1.00
0.97

FL+c / F+c
Tasks C. Ratio
29
16
30
15
30
29
23
21
28
221

0.83
0.92
1.46
1.05
0.95
1.00
1.00
0.89
1.06
1.01

FL+c / F+c
Tasks C. Ratio
29
16
30
15
30
29
23
21
28
221

0.82
0.99
1.04
1.01
1.01
1.01
1.00
0.89
1.01
0.97

Table 3: Average ratio of the first solution costs (top) and best solution costs after iterative search
(bottom) for various pairs of configurations on their commonly solved tasks.
to the same coverage level as the cost-unaware search. Landmarks thus seem to be very helpful in
overcoming the coverage problems of cost-sensitive search.
As mentioned before, the landmark heuristic by itself is however not competitive. Using only
the landmark heuristic and not the FF/add heuristic results in IPC 2008 performance scores between
164 and 167 with iterated search, and coverage points between 185 and 189 for the three possible
cost settings. This is substantially worse then the performance scores greater than 194 and coverage
points greater than 223 achieved by any of the other LAMA configurations.
7.1.3 Quality
As a next step, we look purely at solution quality. Firstly, we want to answer the question whether
the improvement in coverage achieved by landmarks in the cost-sensitive search comes at a price
in solution quality, i. e., whether using landmarks directs the search to close goals rather than cheap
goals. Secondly, we would like to know how the solution quality differs between the cost-sensitive
and the cost-unaware configurations. In particular, how much quality do we lose by combining
158

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

distance and cost estimates (X+c ) as opposed to using pure cost search (Xc )? The score used at IPC
2008 and in Table 1 incorporates both coverage and quality information by counting unsolved tasks
as 0  a method that allows ranking several planners solving different subsets of the total benchmark
set. When we are interested in examining quality independent of coverage, we must restrict our focus on those tasks solved by all compared planners. Table 3 contains quality information comparing
the solution costs of several configurations, where we compare configurations pair-wise in order
to maximise the number of commonly solved tasks. The top part of Table 3 contains comparisons
involving the first solutions found by each configuration, while the bottom part of the table concerns
the best solutions found after iterative search. For each pair of configurations we show the number
of tasks solved by both, and the geometric mean of the cost ratio for the plans they find.
As expected, the cost-sensitive configurations Fc and F+c find cheaper plans than the costunaware configuration F on average, where in particular the pure cost search Fc finds high-quality
first plans (see the first column in the top part of the table). For both Fc and F+c , however, the difference to F is not very large. In some domains, most notably in Elevators, the plans found by the
cost-sensitive heuristics are actually worse than the plans found by cost-unaware search.
Landmarks deteriorate quality for the first plans of Fc ; but F+c , which starts out with a worse
quality than Fc , is not noticeably further deteriorated by landmarks. For both configurations, however, the main negative impact through landmarks is in the Openstacks domain, where plans become
nearly twice as expensive for Fc , and 50% more expensive for F+c . By contrast, in the remaining
8 domains average plan quality for both configurations with landmarks is even slightly better on
average than without landmarks.
We note that iterative search has a remarkable impact on the relative performance of the different configurations. When looking at the solutions found after iterative search, Fc actually performs
worse than F+c , whereas it is the other way round for the first solutions (compare the first two
columns in the top row versus the bottom row of the table). This can be explained to some extent
by the fact that the same reasons that cause Fc to have low coverage also prevent it from improving much over time. As we will show in selected domains later, the cost-sensitive heuristic often
expands many more nodes than the cost-unaware search, leading to the observed behaviour. This is
most likely due to the fact that finding plans of high quality is hard and thus unsuccessful in many
of the benchmark tasks. For example, in some domains cost-sensitive search leads to large local
minima that do not exist for cost-unaware search. More generally, good plans are often longer than
bad plans, which may lead to increased complexity in particular in domains where the heuristic
values are inaccurate. We will showcase the problems of cost-sensitive search in more detail in the
Elevators and PARC Printer domains later on.
With iterative search, landmarks do not deteriorate quality for either Fc nor F+c on average, as
the negative impact of the Openstacks domain is no longer present. (This effect in the Openstacks
domain will be discussed in more detail later.)
Summarising our findings, we can say that landmarks effectively support the cost-sensitive
FF/add heuristic in finding solutions, without steering the search away from good solutions. Similarly, combining distance and cost estimates as in X+c leads the search to finding solutions quickly
without overly sacrificing quality, as is demonstrated by its superior anytime performance compared
to pure cost search.
By way of example, we now present detailed results for four of the nine competition domains.
We choose domains that we deem to be of particular interest because the results in them either exaggerate or contradict the general trends discussed so far. The domains Elevators and PARC Printer
159

fiRichter & Westphal

8
7
6
5
4
3
2
1
0
Figure 7: An example elevators task.

highlight the problems of cost-sensitive search; in Cyber Security cost-sensitive search performs
uncharacteristically well; and Openstacks is a domain where landmarks do not lead to the usual
improvement, but rather to a deterioration of performance.
7.2 Elevators
The Elevators domain models the transportation of passengers in a building via fast and slow elevators, where each elevator has a certain passenger capacity and can access certain floors. Passengers
may have to change elevators to get to their final destination, and furthermore the two different types
of elevators have different associated cost functions. This is in contrast to the Miconic domain, used
in an earlier international planning competition (Bacchus, 2001), which also models the transporting of passengers via elevators, but where there is only one elevator that can access all floors with
just one (unit-cost) operator. In Elevators, the floors in the building are grouped into blocks, overlapping by one floor. Slow elevators only operate within a block and can access all floors within
their block. Fast elevators can access all blocks, but only certain floors within each block (in the
first 10 IPC tasks every second floor, and in the other 20 tasks every fourth floor). Fast elevators are
usually more expensive than slow elevators except for a distance of two floors, where both elevator
types cost the same. However, fast elevators may sometimes be advantageous when transporting
passengers between blocks (as they avoid the need for passengers to switch elevators on the shared
floor between blocks), and they usually have a higher capacity.
An example task with eight floors, grouped into two blocks, is shown in Figure 7. There are a
total of four elevators, two slow ones and two fast ones. The cost function used in the 30 IPC tasks
for moving an elevator from its current location to a target floor is 6 + n for slow elevators and 2 + 3n
for fast elevators, where n is the distance travelled (the number of floors between the current location
of the elevator and its target). Operators concerning passengers boarding and leaving elevators are
free of cost. Assuming this cost function, it is cheaper in this example to transport the passenger
located at floor 0 using the two slow elevators (changing at floor 4) than using a direct fast elevator.
Elevators is one of the domains where configurations using the cost-sensitive FF/add heuristic
solve far fewer problems than their cost-unaware counterparts. Using landmarks increases coverage,
but does not solve the problem completely. Furthermore, it is notable that on the problems that the
cost-sensitive configurations do solve, their solutions often have worse quality than the solutions of
the cost-unaware configurations. Table 4 illustrates this fact for the first solutions found when using
160

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Task
01
02
03
04
05
06
07
08
09
11
12
13
14
20
21
Avg.

Quality (IPC Score)
F
Fc
F+c
0.57 0.59
0.53
0.69 0.72
0.72
0.88 0.58
0.51
0.71 0.70
0.72
0.68 0.54
0.54
0.60 0.60
0.61
0.38 0.46
0.40
0.84 0.54
0.51
0.71 0.54
0.57
0.66 0.52
0.52
0.70 0.54
0.58
0.58 0.51
0.54
0.70 0.70
0.70
0.67 0.47
0.58
0.70 0.63
0.71
0.67 0.58
0.58

F
26
27
21
34
33
56
71
47
54
39
55
60
81
132
84
55

Length
Fc
24
25
41
45
50
64
81
62
81
51
79
84
101
173
83
67

F+c
27
25
42
47
50
53
83
65
59
47
79
72
95
154
82
65

Table 4: Comparison of plan qualities (measured via the IPC scores) and plan lengths for the first
solutions of F, Fc , and F+c in Elevators. Shown are all tasks solved by all three configurations, with
bold print indicating the best solution.

only the FF/add heuristic. With iterative search (not shown), the solution quality for F+c improves to
a similar level as that of F, whereas Fc remains substantially worse.
While we do not have a full explanation for why the configurations involving the cost-sensitive
FF/add heuristic perform so badly in this domain, several factors seem to play a role. Firstly, in its
attempt to optimise costs, the cost-sensitive FF/add heuristic focuses on relatively complex solutions
involving mainly slow elevators and many transfers of passengers between elevators, where the
relaxed plans are less accurate (i. e., translate less well to actual plans), than in the case for the
cost-unaware heuristic. Secondly, the costs associated with the movements of elevators dominate
the heuristic values, causing local minima for the cost-sensitive heuristic. Thirdly, the capacity
constraints associated with elevators may lead to plateaus and bad-quality plans in particular for the
cost-sensitive heuristic. In the following sections, we describe each of these factors in some detail.
Lastly, we found that the deferred heuristic evaluation technique used in LAMA (see Section 3.3) did not perform well in this domain. When not using deferred evaluation, the Fc configuration solves 3 additional tasks (though the quality of solutions remains worse than with the F
configuration). This partly explains why the FF(ha ) planner by Keyder and Geffner (2008) has a
substantially higher coverage than our Fc configuration in this domain. While the two planners use
the same heuristic, they differ in several aspects. Apart from deferred evaluation these aspects include the search algorithm used (greedy best-first search vs. enhanced hill-climbing) and the method
for using preferred operators (maintaining additional queues for preferred states vs. pruning all nonpreferred successor states).
161

fiRichter & Westphal

F
Fc
F+c

Slow moves
275
405
404

Fast moves
45
21
12

Ratio fast/slow
6.11
19.29
33.67

Table 5: Total elevator moves and ratio of fast/slow moves in the first solutions found by the F, Fc ,
and F+c configurations, on the 15 Elevators instances solved by all three configurations.
7.2.1 Slow vs. Fast Elevators
When examining the results, we found that the Fc and F+c configurations tend to produce plans
where slow elevators are used for most or all of the passengers, while the F configuration uses
fast elevators more often (cf. Table 5). This is not surprising, as for each individual passenger,
travelling from their starting point to their destination tends to be cheaper in a slow elevator (unless
the distance is very short), whereas fewer operators are typically required when travelling in a fast
elevator. The independence assumptions inherent in the FF/add heuristic (see Section 6) lead to
constructing relaxed plans that aim to optimise the transportation of each passenger individually,
rather than taking synergy effects into account.
The plans produced by Fc and F+c are also longer, on average, than the plans produced by F (see
Table 4), one reason for this being that the predominant use of slow elevators requires passengers
to change between elevators more often. As plans become longer and involve more passengers
travelling in each of the slow elevators, heuristic estimates may become worse. For example, the
relaxed plans extracted for computation of the heuristic are likely to abstract away more details if
more passengers travel in the same elevator (e. g., since once a passenger has been picked up from
or delivered to a certain location, the elevator may teleport back to this location with no extra
cost in a relaxed plan to pick up or deliver subsequent passengers). Generally, we found that the
relaxed plans for the initial state produced by Fc and F+c tend to be similar in length and cost to those
produced by F, but the final solutions produced by Fc and F+c are worse than those of F. One reason
for this is probably that the increased complexity of planning for more passenger change-overs
between elevators in combination with worse relaxed plans poses a problem to the cost-sensitive
FF/add heuristic.
7.2.2 Local Minima Due to Elevator-Movement Costs
Since action costs model distances, the total cost of a relaxed plan depends on the target floors
relative to the current position of an elevator. For both Fc and F+c , the action costs of moving the
elevator usually dominate the estimates of the FF/add heuristic. Consider the two example tasks in
Figure 8, which differ only in the initial state of the elevators. The elevators need to travel to all
three floors in a solution plan, but due to abstracted delete effects a relaxed plan for the initial state
will only include operators that travel to the two floors other than the starting floor of the elevator
(i. e., the elevator can be teleported back to its starting floor without cost). In the left task, the
relaxed cost of visiting all three floors is lower than in the right task, as the cost is in the left task is
the sum of going from floor 4 to floor 8, and going from floor 4 to floor 0, resulting in a total cost
of 10 + 10 = 20. In the right task, the relaxed cost for visiting all floors is the cost of going from
floor 0 to floor 4, and from floor 0 to floor 8, resulting in a total cost of 10 + 14 = 24. In the left
task, once the passenger has boarded the elevator on floor 4, all immediate successor states have a
162

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

action cost

action cost

8
7
6
5
4
3
2
1
0

8
7
6
5
4
3
2
1
0

Figure 8: Action cost effects in Elevators in a relaxed setting. Travelling 4 floors costs 10, while
travelling 8 floors costs 14. Both tasks have the same solution cost (34), but the left task has a lower
relaxed cost (20) than the right task (24).

worse heuristic estimate due to the movement costs of the elevator. In particular, the correct action
of moving the elevator up to floor 8 (to deliver the passenger) results in a state of worse heuristic
value. If we increased the number of waiting passengers at floor 4, the planning system would
therefore try boarding all possible subsets of passengers before moving the elevator. And even once
the elevator is moved up to floor 8, the heuristic estimate will only improve after the passenger has
been dropped off and either (a) the elevator has moved back to floor 4, or (b) the second passenger
has boarded and the elevator has moved down to floor 0.
Consequently, movement costs may dominate any progress obtained by transporting passengers
for a number of successive states. In other words, the planner often has to blindly achieve some
progress and move the elevators towards a middle position given the remaining target floors, in
order for the cost-sensitive heuristic to report progress. For the cost-unaware heuristic, the situation
is less severe, as the number of elevator movements in the relaxed plan does not increase, and
hence the planner encounters a plateau in the search space rather than a local minimum. The use of
preferred operators may help to escape from the plateau relatively quickly, whereas a local minimum
is much harder to escape from. Two approaches exist that may circumvent this problem. Firstly,
the use of enforced hill-climbing (Hoffmann & Nebel, 2001) rather than greedy best-first search
is likely to avoid exploration of the entire local minima: in this approach, a breadth-first search
is conducted from the first state of a minima/plateau until an improving state is found. Secondly,
an improved heuristic could be used that approximates the optimal relaxed cost h+ more exactly.
The cost minima shown in Figure 8 is brought about by the independence assumptions inherent in
the FF/add heuristic, which estimate the relaxed cost for each goal fact individually in the cheapest
possible way. An optimal relaxed plan, however, costs the same in the left task as in the right task. A
more accurate approximation of the optimal relaxed cost h+ could therefore mitigate the described
cost minima. Keyder and Geffner (2009) have recently proposed such an improvement of the FF/add
heuristic4 and shown it to be particularly useful in the Elevators and PARC Printer domains.
4. In Keyder & Geffners approach, the relaxed plan extracted by the FF/add heuristic is improved by iteratively (1)
selecting a fact F, (2) fixing all operators that are not related to F (because they do not contribute to achieving F nor
rely on its achievement), and (3) computing a cheaper way of achieving F given the operators that were fixed in the
previous step.

163

fiRichter & Westphal

7.2.3 Plateaus Due to Capacity Constraints
In general, the relaxed plans in the Elevators domain are often of bad quality. One of the reasons is
the way the capacity of elevators is encoded in the operators for passengers boarding and leaving
elevators. For any passenger p transported in an elevator e, one of the preconditions for p leaving e
is that n passengers be boarded in e, where n is a number greater than 0. When constructing a relaxed
plan, the FF/add heuristic recursively selects operators that achieve each necessary precondition in
the cheapest way. This results in boarding the passenger that is closest to e in the initial state, even
if this passenger p0 is different from p, to achieve the condition that some passenger is boarded.
The relaxed plan will then contain operators for both boarding p and p0 into e, and may furthermore
contain other operators for boarding p0 into whatever elevator e0 is deemed best for transporting p0 .
Hence, the relaxed plans often contain many unnecessary boarding operators.
As mentioned in Section 3.3, the greedy best-first search in LAMA breaks ties between equally
promising operators by trying the cheaper operator first. Consequently, the zero-cost operators for
passengers boarding and leaving elevators are tried first in any state. We found that as soon as one
passenger is boarded into a certain elevator, the relaxed plans in the next state are often substantially
different, in that more passengers are being assigned to that same elevator. This can be explained
by the fact that as soon as one passenger is in an elevator, the precondition for leaving that elevator
which is having at least one person boarded, is fulfilled (rather than incurring additional cost). In
some example tasks we examined, we found that this effect results in committing to bad boarding
operators: LAMA may initially try some bad boarding operator, e. g. boarding the nearest passenger
into an elevator to satisfy a capacity precondition for another passenger, as described above. The
relaxed plan in the successor state then assigns more passengers to this elevator, at lower cost. Due
to the improved heuristic value of the successor state, LAMA retains this plan prefix, even though
the first operator was a bad one. It is plausible (though we did not explore it experimentally) that
this effect is stronger for the configurations involving the cost-sensitive heuristic, as the costs of
their relaxed plans vary more strongly from one state to the next.
More importantly, the capacity constraints lead to plateaus in the search space, as correct boarding and leaving operators are often not recognised as good operators. For example, if the capacity of
an elevator is c, then boarding the first c  1 passengers that need to be transported with this elevator
usually leads to improved heuristic values. However, boarding the c-th passenger does not result in
a state of better heuristic value if there are any further passengers that need to be transported via
the same elevator, because the c-th passenger boarding destroys the precondition that there must be
room in the elevator for other passengers to board. Similarly, the correct leaving of a passenger may
not lead to an improved heuristic value if it makes the elevator empty and other passengers need to
be transported with that elevator later (because the last passenger leaving destroys the precondition
for leaving that there must be at least one passenger boarded).
These effects exist for both the cost-sensitive and the cost-unaware heuristic. However, they
typically occur within the plateaus (F) or local minima (Fc , F+c ) created by the elevator positions,
as described in the previous section, which means they affect the cost-sensitive configurations more
severely. The plateaus become particularly large when several passengers are waiting on the same
floor, e. g. when passengers are accumulating on the floor shared by two blocks in order to switch
elevators. The planner then tries to board all possible subsets of people into all available elevators (as
the zero-cost boarding and leaving operators are always tried first), moving the elevators and even
dropping off passengers at other floors, and may still fail to find a state of better heuristic value.
164

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

First plans
Final plans

Fc
F+c
Fc
F+c

Solved
15
16
15
16

Original tasks
qual. > F qual. < F
3
10
5
10
0
15
5
6

No capacity constraints
Solved qual. > F qual. < F
29
17
6
30
22
3
29
9
20
30
15
11

Table 6: Relative qualities of solutions in the original Elevators domain and in a modified variant of
the domain where elevators have unlimited capacity. Shown is the total number of tasks solved by
the cost-sensitive configurations Fc and F+c , as well as the number of tasks where these configurations find a better/worse plan than the cost-unaware configuration F.
When examining the number of states in local minima for each of the configurations, we found that
Fc and F+c indeed encounter many more such states than F. For example, the percentage of cases in
which a state is worse than the best known state is typically around 10% (in rare cases 25%) for F.
For Fc and F+c , on the other hand, the numbers are usually more than 35%, often more than 50%,
and in large problems even up to 80%.
To verify that the capacity constraints indeed contribute to the bad performance of the costsensitive heuristic in this domain, we removed these constraints from the IPC tasks and ran the
resulting problems with the F, Fc and F+c configurations. Not surprisingly, the tasks become much
easier to solve, as elevators can now transport all passengers at once. More interestingly though,
the bad plan qualities produced by the cost-sensitive configurations (relative to the cost-unaware
configuration) indeed become much less frequent, as Table 6 shows.
In summary, our findings suggest that the bad performance of the cost-sensitive FF/add heuristic
in the Elevators domain is due to bad-quality relaxed plans (brought about by the focus on slow
elevators and the capacity constraints) and plateaus and local minima in the search space (resulting
from the movement costs of elevators and the capacity constraints).
7.3 PARC Printer
The PARC Printer domain (Do, Ruml, & Zhou, 2008) models the operation of a multi-engine printer
capable of processing several printing jobs at a time. Each sheet that must be printed needs to pass
through several printer components starting in a feeder and then travelling through transporters,
printing engines and possibly inverters before ending up in a finishing tray. The various sheets
belonging to a print job must arrive in the correct order at the same finisher tray, but may travel
along different paths using various printing engines. There are colour printing engines and ones that
print in black and white, where colour printing is more expensive. The action costs of operators
are comparatively large, ranging from 2000 to more than 200,000. Colour-printing is the most
expensive operator, while operators for printing in black and white cost roughly half as much, and
operators for transporting sheets are relatively cheap.
Like in the Elevators domain, the cost-sensitive FF/add heuristic did not perform well here,
with Fc and F+c failing to solve many of the tasks that the cost-unaware configuration F is able to
solve. (Note that Fc and F+c perform very similarly in this domain, as the large action costs outweigh the distance estimates in F+c .) However, in contrast to the Elevators domain, the Fc and F+c
configurations result in notably improved plan quality compared to F. An overview of the number
165

fiRichter & Westphal

Tasks solved out of 30
Avg. quality of first solution
Avg. quality of final solution

F
25
0.79
0.96

F+c
16
1.00
1.00

FL
24
0.93
1.00

FL+c
23
0.95
0.99

Table 7: Coverage vs. quality in the PARC Printer domain. Average qualities are average IPC scores
calculated only on those tasks solved by all configurations.

of problems solved and the average quality of first solutions is shown in Table 7. When using landmarks, the differences between cost-sensitive and cost-unaware configurations are strongly reduced,
with all three landmark configurations achieving a better performance than the F configuration.
Like in Elevators, we found the quality of relaxed plans to be poor. In the cost-unaware case,
a relaxed plan transports sheets from a feeder to the finishing tray via a shortest path, irrespective
of whether a suitable printing engine lies on this path. As any path from feeder to finishing tray
passes through some printing engine, this frequently involves printing a wrong image on a paper,
while additional operators in the relaxed plan handle the transportation from a feeder to a suitable
printing engine to print the correct image on the sheet as well. When the cost-sensitive heuristic is
used, relaxed plans furthermore become substantially longer, using many transportation operators
to reach a cheap printing engine. Analogously to the Elevators domain, the increased complexity
associated with longer plans (in combination with the bad quality of the relaxed plans) is thus
likely to be the reason for the bad performance of the cost-sensitive heuristic. However, landmarks
mitigate the problem, as the numbers of solved tasks in Table 7 clearly show. Landmarks found
in this domain encompass those for printing a correct image on each sheet, where a disjunctive
landmark denotes the possible printers for each sheet. This helps to counteract the tendencies of the
cost-sensitive FF/add heuristic to transport sheets to the wrong printers.
In summary, PARC Printer is like Elevators a domain where the cost-sensitive FF/add heuristic
performs badly, though in contrast to Elevators the problem here is purely one of coverage, not of solution quality. Even more than in Elevators, landmarks overcome the problems of the cost-sensitive
configurations, improving them to a similar performance levels as the cost-unaware configurations.
7.4 Cyber Security
The Cyber Security domain stands out as a domain where the cost-sensitive configurations perform
significantly better than their cost-unaware counterparts, especially when looking at first solutions.
(Iterative search reduces the gap, but does not close it completely.) The domain models the vulnerabilities of computer networks to insider attacks (Boddy, Gohde, Haigh, & Harp, 2005). The task
consists in gaining access to sensitive information by using various malware programs or physically
accessing computers in offices. Action costs model the likelihood of the attack to fail, i. e., the risk
of being exposed. For example, many actions in the office of the attacker, like using the computer,
do not involve any cost, whereas entering other offices is moderately costly, and directly instructing
people to install specific software has a very high associated cost. In particular, action costs are used
to model the desire of finding different methods of attack for the same setting. For example, several
tasks in the domain differ only in the costs they associate with certain operators.
In the Cyber Security domain, taking action costs into account pays off notably: while the Fc and
F+c configurations solve 2 and 1 problems less, respectively, than the F configuration (see Table 2),
166

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

IPC score for first solutions
IPC score for final solutions

F
20.44
23.12

FL
20.43
25.93

F+c
23.67
24.69

FL+c
26.60
27.53

Table 8: IPC scores in the Cyber security domain.

they nevertheless result in a better total score. Using landmarks, both cost-sensitive configurations
are improved such that they solve all problems while maintaining the high quality of solutions,
resulting in an even larger performance gap between FLc (27.59 points) and FL+c (26.60 points) on
the one side, and FL (20.43 points) on the other side.
The plans found by the cost-unaware search often involve physically accessing computers in
other offices or sending viruses by email, and as such result in large cost. Lower costs can be
achieved by more complex plans making sophisticated use of software. As opposed to the Elevators
and PARC Printer domains, the relaxed plans in Cyber Security are of very good quality. This
explains why the performance of the cost-sensitive heuristic is not negatively impacted by longer
plans. Using iterative search improves the performance of FL and F to nearly the same levels as
their cost-sensitive counterparts (see Table 8).
7.5 Openstacks
The Openstacks domain models the combinatorial optimisation problem minimum maximum simultaneous open stacks (Fink & Vo, 1999; Gerevini, Haslum, Long, Saetti, & Dimopoulos, 2009),
where the task is to minimise the storage space needed in a manufacturing facility. The manufacturer receives a number of orders, each comprising a number of products. Only one product can be
made at a time, and the manufacturer will always produce the total required quantity of a product
(over all orders) before beginning the production of a different product. From the time the first product in an order has been produced to the time when all products in the order have been produced,
the order is said to be open and requires a stack (a temporary storage space). The problem consists
in ordering the products such that the maximum number of stacks open at any time is minimised.
While it is easy to find a solution for this problem (any product order is a solution, requiring n
stacks in the worst case where n is the number of orders), finding an optimal solution is NP-hard.
The minimisation aspect is modelled in the planning tasks via action costs, in that only the operator
for opening new stacks has a cost of 1, while all other operators have zero cost. This domain was
previously used at IPC 2006 (Gerevini et al., 2009). While that earlier formulation of the domain
has unit costs, it is equivalent to the cost formulation described above. Since the number of operators that do not open stacks is the same in every plan for a given task, minimising plan length is
equivalent to minimising action costs.
We noticed that in this domain using landmarks resulted in plans of substantially worse quality,
compared to not using landmarks. In particular, this is true for the first plans found, whereas the
use of anytime search improves the results for both configurations to similar levels. Across all cost
settings, using the landmark heuristic in combination with the FF/add heuristic typically produces
plans where the majority of orders is started very early, resulting in a large number of simultaneously open stacks, whereas using only the FF/add heuristic leads to plans in which the products
corresponding to open orders are manufactured earlier, and the starting of new orders is delayed
until earlier orders have been shipped. This is mainly due to the fact that no landmarks are found by
167

fiRichter & Westphal

5000
4500

F+c
FL+c

Expanded Nodes

4000
3500
3000
2500
2000
1500
1000
500
0
5

10

15
Tasks

20

25

30

Figure 9: Number of expanded search nodes with and without landmarks in the first search iteration
(best-first search) in the Openstacks domain.

LAMA regarding the opening of stacks, which means that due to the choice of action costs in this
domain, all landmarks have cost zero and the landmark heuristic is not able to distinguish between
plans of different cost. The landmarks found by LAMA relate to the starting and shipping of orders
as well as the making of products.5 However, even if landmarks regarding the opening of stacks
were found, they would not be helpful: landmarks state that certain things must be achieved, not
that certain things need not be achieved. Landmarks can thus not be used to limit the number of
open stacks. The landmark orderings are furthermore not helpful for deciding an order between
products, as all product orders are possiblewhich means that no natural orderings exist between
the corresponding landmarksand no product order results in the form of wasted effort captured
by reasonable landmark orderings.
As mentioned above, all landmarks found by LAMA have a minimal cost of zero. Therefore,
the landmark heuristic fails to estimate the cost to the goal, and distinguishes states only via the
number of missing started or shipped orders and products. (These goal distance estimates are used
directly in FL, combined with the all-zero landmark heuristic cost estimates in FL+c , and as tiebreakers amongst the zero-cost estimates in FLc , resulting in the same relative ranking of states by
the landmark heuristic in all three cases.) As soon as one stack is open, for each order o the operator
that starts o achieves a landmark that is minimal with respect to landmark orderings (namely the
landmark stating that o must be started), and the planner thus tends to start orders as soon as possible.
The landmark heuristic is not able to take into account future costs that arise through bad product
orderings. This is also a problem for the FF/add heuristic, albeit a less severe one: the FF/add
heuristic accounts for the cost of opening (exactly) one new stack whenever at least one more stack
is needed, and the heuristic will thus prefer states that do not require any further stacks.
The landmark heuristic does, however, provide a good estimate of the goal distance. Since
the landmark heuristic prefers states closer to a goal state with no regard for costs, its use results
5. If the size of disjunctions were not limited in LAMA, it would always find a landmark stacks avail(1) 
stacks avail(2)      stacks avail(n) stating that at least one of the n stacks must be open at some point. However, any landmark stating that two or more stacks need to be open would require a more complex form of landmarks
involving conjunction, which LAMA cannot handle.

168

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

1

Plan Quality

0.8

0.6

0.4

0.2

0

F+c
FL+c
5

10

15
Tasks

20

25

30

Figure 10: Plan quality (measured via the IPC scores) with and without landmarks in the first search
iteration (best-first search) in the Openstacks domain.

1
50
45
40
35
Cost 30
25
20
15
10
5
0 10

Plan Quality

0.8
0.6
0.4
0.2
0

F+c
FL+c

100
Time (seconds)
5

10

15
Tasks

20

25

1000

5

30
25
20
15 Tasks
10

30

Figure 11: Effect of iterative search in the Openstacks domain. Left: plan quality (IPC score) of the
best plan found within 30 minutes with and without landmarks. Right: evolution of plan costs with
landmarks (FL+c ) over time.
in plans where stacks are opened as needed. This is reflected in our empirical results, where the
additional use of the landmark heuristic drastically reduces the number of expanded search nodes
(see Figure 9), but leads to higher-cost plans (see Figure 10). Without iterative search, the LAMA
configuration FL+c only achieves 13.85 points for this domain, compared to 19.77 points when not
using landmarks (configuration F+c ).
Using iterative search, the negative effect of the landmarks on quality is mitigated, as can be
seen in Figure 11. FL+c generates up to 21 distinct, and each time improved, plans per problem. In
the end, the difference in points is merely 27.40 for FL+c vs. 28.30 for F+c . This score is reached
after less than 5 minutes of iterated search per task.
Thus, Openstacks is an example for a domain where landmarks are detrimental to solution
quality. However, using landmarks provides the benefit of speeding up planning by reducing the
169

fiRichter & Westphal

number of expanded nodes. This allows iterative search to effectively improve solution quality in
the given time limit such that the final results of using landmarks are similar to those of not using
landmarks.
7.6 Domains from Previous Competitions
Tables 9 and 10 show results on the IPC domains from previous years (19982006). As these domains do not contain action costs, the cost-sensitive configurations of LAMA are not applicable and
LAMA runs with the FL configuration. The configurations examined for LAMA are thus FL and
F, both with iterated search and without, where FL with iterated search is shown as LAMA. Also
given are the results for two IPC-winning systems of previous years, FF and Fast Downward. For
both FF and Fast Downward, we ran current versions. In particular Fast Downward has evolved
substantially since its 2004 competition version, the original causal graph heuristic having been
replaced with the better context-enhanced additive heuristic (Helmert & Geffner, 2008). After correspondence with the authors, the version of Fast Downward used here is the one featuring in recent
work by Richter and Helmert (2009).
As Table 9 shows, LAMA performs better than both FF and Fast Downward in terms of the
IPC 2008 criterion. This is true even if we turn off landmarks or iterated search in LAMA, but
not if we turn off both options simultaneously. When viewing the large difference between the
scores of iterated versus non-iterated search in LAMA, note that on these domains no best known
reference results were used in the score calculation (in contrast to the 2008 tasks, for most of which
such reference results were generated manually or with domain-specific solvers by the competition
organisers). This means that the planner producing the best solution for a task is awarded the
highest-possible score of 1, even though better solutions might exist. This may skew results in favour
of the planner that delivers cheaper solutions, i. e., exaggerate the differences between planners.
Table 10 shows that LAMAs edge over Fast Downward is due to higher-quality solutions rather
than coverage, as Fast Downward solves more problems. Compared to FF, LAMA has better coverage, with the gap between LAMA and FF being substantially larger than the gap between LAMA
and Fast Downward. Note that the F and LAMA configurations roughly correspond to the results
published as base and heur in earlier work (Richter et al., 2008). However, subsequent changes
to the code to support action costs negatively affect in particular the Philosophers domain, where
we observe a significant decrease in coverage. This is also one of the reasons for the difference in
coverage between LAMA and the closely related Fast Downward system.
Comparing the various experimental configurations for LAMA, we note that the use of landmarks leads to moderate improvements in both coverage and solution quality. As mentioned above,
iterative search significantly improves performance in terms of the IPC 2008 score.

8. Conclusion and Outlook
In this article, we have given a detailed account of the LAMA planning system. The system uses
two heuristic functions in a multi-heuristic state-space search: a cost-sensitive version of the FF
heuristic, and a landmark heuristic guiding the search towards states where many subgoals have
already been achieved. Action costs are employed by the heuristic functions to guide the search
to cheap goals rather than close goals, and iterative search improves solution quality while time
remains.
170

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Domain

FF

F. Downw.

LAMA

F

FLfirst

Ffirst

Airport (50)
Assembly (30)
Blocks (35)
Depot (22)
Driverlog (20)
Freecell (80)
Grid (5)
Gripper (20)
Logistics 1998 (35)
Logistics 2000 (28)
Miconic (150)
Miconic Full ADL (150)
Miconic Simple ADL (150)
Movie (30)
MPrime (35)
Mystery (30)
Openstacks (30)
Optical Telegraphs (48)
Pathways (30)
Philosophers (48)
Pipesworld Notank. (50)
Pipesworld Tank. (50)
PSR Small (50)
Rovers (40)
Satellite (36)
Schedule (150)
Storage (30)
TPP (30)
Trucks (30)
Zenotravel (20)
Total (1512)
PSR Large (50)
PSR Middle (50)

35
29
30
20
13
69
4
20
35
28
150
124
140
30
28
14
29
12
19
11
25
16
41
38
35
99
16
23
10
19
1162



39
28
17
13
14
66
4
15
33
25
118
95
105
30
34
18
29
4
28
48
31
28
49
35
30
132
16
26
13
17
1143
26
40

35
30
33
16
19
73
5
20
34
28
150
136
148
30
35
19
29
2
28
29
43
36
50
39
33
147
19
30
13
19
1330
28
50

33
30
34
15
20
75
5
20
33
28
143
136
150
30
33
16
30
2
27
34
42
38
50
39
31
139
20
29
16
20
1318
16
41

35
29
22
13
16
62
4
20
33
28
150
107
117
30
31
18
29
2
28
29
26
27
49
37
32
137
16
28
12
18
1185
22
37

33
29
17
12
15
65
4
18
32
28
117
107
113
30
29
14
30
2
27
34
27
28
49
37
27
129
18
27
15
18
1129
14
35

Table 9: Performance scores (rounded to whole numbers) for FF, Fast Downward and LAMA as
well as experimental alternative configurations of LAMA (F: without landmarks, FLfirst : without
iterated search, Ffirst : without landmarks and without iterated search).

171

fiRichter & Westphal

Domain

FF

F. Downw.

LAMA

F

Airport (50)
Assembly (30)
Blocks (35)
Depot (22)
Driverlog (20)
Freecell (80)
Grid (5)
Gripper (20)
Logistics 1998 (35)
Logistics 2000 (28)
Miconic (150)
Miconic Full ADL (150)
Miconic Simple ADL (150)
Movie (30)
MPrime (35)
Mystery (30)
Openstacks (30)
Optical Telegraphs (48)
Pathways (30)
Philosophers (48)
Pipesworld Notank. (50)
Pipesworld Tank. (50)
PSR Small (50)
Rovers (40)
Satellite (36)
Schedule (150)
Storage (30)
TPP (30)
Trucks (30)
Zenotravel (20)
Total (1512)
PSR Large (50)
PSR Middle (50)

37
30
31
22
15
80
5
20
35
28
150
136
150
30
34
16
30
13
20
13
36
21
41
40
36
133
18
28
11
20
1279



40
30
35
19
20
79
5
20
35
28
150
139
150
30
35
19
30
5
29
48
43
38
50
39
35
150
18
30
15
20
1384
31
50

36
30
35
17
20
79
5
20
35
28
150
137
150
30
35
19
30
2
29
29
44
38
50
40
34
150
19
30
13
20
1354
29
50

34
30
35
16
20
78
5
20
35
28
150
138
150
30
35
16
30
2
28
34
43
40
50
40
31
144
20
30
16
20
1348
16
41

Table 10: Coverage (problems solved) for FF, Fast Downward and LAMA as well as the experimental F configuration of LAMA without landmarks.

172

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

We have conducted an extensive experimental study on the set of benchmark tasks from the
last international planning competition, in order to identify how much each of the features of our
planner contributes to its performance in the setting of planning with action costs. We discussed
overall results and provided plausible explanations for deviating behaviour in some special cases.
The most noticeable outcome of our experiments is that using cost-sensitive heuristics did not
produce the desired outcome. In particular, the cost-sensitive FF/add heuristic performs significantly
worse than the FF/add heuristic that ignores costs. This is due to the cost-sensitive heuristic solving
far fewer tasks while leading to little improvement in solution quality on the tasks that it does solve,
especially when using iterated search. When investigating the reasons for this effect, we found that
the cost-sensitive FF/add heuristic reacts strongly to bad relaxed plans, i. e., it is in particular in
those domains where the relaxed plans computed by the heuristic have low quality that the costsensitive heuristic is likely to perform worse than the cost-unaware heuristic. As we showed for the
Elevators domain, action costs may also introduce local minima into the search space where without
action costs the search space of the FF/add heuristic would have plateaus. Moreover, the increased
complexity of planning for a cheaper goal that is potentially further away from the initial state may
lead to worse performance.
Landmarks prove to be very helpful in this context, as they mitigate the problems of the costsensitive FF/add heuristic. Using landmarks, the coverage of cost-sensitive search is improved to
nearly the same level as that of cost-unaware search, while not deteriorating solution quality. Despite
the mitigating effect of landmarks, however, LAMA would still have achieved a slightly higher
score at IPC 2008 if it had simply ignored costs, rather than using cost-sensitive heuristics. For
cost-unaware search, we found landmarks to improve coverage and solution quality in the domains
from the IPCs 19982006. On the domains from IPC 2008, landmarks improved solution quality
for the cost-unaware search, but did not further increase the (already very high) coverage.
Iterative search improves results notably for all of our experimental configurations, raising the
score of LAMA by a quarter on the IPC 2008 domains. In the Openstacks domain, we could
furthermore observe a synergy effect between the iterative search and landmarks. While landmarks
usually improve quality, in this domain they lead to bad plans by not accounting for action costs.
However, they speed up planning so that the planner evaluates substantially fewer states. Iterative
search then effectively improves on the initial bad plans while benefiting from the speed-up provided
by the landmarks. In general, we can use landmarks as a means to quickly find good solutions, while
using iterative search as a way to improve plan quality over time. Overall, we found that the domains
used at IPC 2008 constitute a varied benchmark set that reveals various strengths and weaknesses
in our planning system.
Building on the results presented in this article, we identify several directions for future work.
Firstly, our results suggest that more research into cost-sensitive heuristics is needed. We would
like to conduct a more thorough analysis of the short-comings of the cost-sensitive FF/add heuristic, to answer the question whether and how they might be overcome. Keyder and Geffner (2009)
propose a method for extracting better relaxed plans from the best supports computed by the costsensitive FF/add heuristic, resulting in improved coverage. However, the large ledge of the costunaware heuristic in our experiments suggests that the cost-unaware FF/add heuristic is still better than the improved cost-sensitive heuristic by Keyder and Geffner. It would be interesting to
examine to what degree the problems we experienced with the FF/add heuristic extend to other
delete-relaxation heuristics, and whether heuristics not based on the delete relaxation could be more
effectively adapted to action costs. In addition, future work could explore the benefit of combin173

fiRichter & Westphal

ing traditional distance estimators and cost-sensitive heuristics in more sophisticated ways than the
mechanism currently used in LAMA (see the discussion in Section 3.3.2).
Secondly, we believe it to be useful for future research to improve the definition of reasonable
orderings, eliminating the problems of the definition by Hoffmann et al. mentioned in Section 4.1.
Thirdly, we would like to extend the use of landmarks in our system in several ways. For one, our
current approach does not take into account whether the same landmark must be achieved several
times. Supporting such multiple occurrences of landmarks would be beneficial in the Openstacks
domain, for example, as it could help to minimise the creation of stacks by accounting for their
costs. While methods exist for detecting the multiplicity of landmarks (Porteous & Cresswell, 2002;
Zhu & Givan, 2003), it will be crucial to develop techniques for deriving orderings between the
individual occurrences of such landmarks. Furthermore, we would like to extend LAMA to support
more complex landmarks like conjunctions or other simple formulas. In addition to representing
and using such landmarks in the landmark heuristic this involves the development of new methods
for detecting them along with their corresponding orderings.

Acknowledgments
The authors thank Malte Helmert, Charles Gretton, Sylvie Thiebaux and Patrik Haslum as well as
the anonymous reviewers for helpful feedback on earlier drafts of this paper.
The computing resources for our experiments were graciously provided by Pompeu Fabra University. We thank Hector Palacios for his support in conducting the experiments.
NICTA is funded by the Australian Government, as represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council, through the
ICT Centre of Excellence program.
This work was partially supported by Deutsche Forschungsgemeinschaft as part of the Transregional Collaborative Research Center SFB/TR 8 Spatial Cognition, project R4-[LogoSpace].

References
Aine, S., Chakrabarti, P. P., & Kumar, R. (2007). AWA*  A window constrained anytime heuristic search algorithm. In Veloso, M. M. (Ed.), Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI 2007), pp. 22502255.
Bacchus, F. (2001). The AIPS00 planning competition. AI Magazine, 22(3), 4756.
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational Intelligence, 11(4), 625655.
Boddy, M., Gohde, J., Haigh, T., & Harp, S. (2005). Course of action generation for cyber security
using classical planning. In Biundo, S., Myers, K., & Rajan, K. (Eds.), Proceedings of the
Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005),
pp. 1221. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(1), 533.
Bryce, D., & Kambhampati, S. (2007). A tutorial on planning graph based reachability heuristics.
AI Magazine, 28(1), 4783.
174

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Buffet, O., & Hoffmann, J. (2010). All that glitters is not gold: Using landmarks for reward shaping
in FPG. In Proceedings of the ICAPS 2010 Workshop on Planning and Scheduling Under
Uncertainty.
Chen, Y., Wah, B. W., & Hsu, C.-W. (2006). Temporal planning using subgoal partitioning and
resolution in SGPlan. Journal of Artificial Intelligence Research, 26, 323369.
Cheng, J., & Irani, K. B. (1989). Ordering problem subgoals. In Sridharan, N. S. (Ed.), Proceedings
of the 11th International Joint Conference on Artificial Intelligence (IJCAI 1989), pp. 931
936. Morgan Kaufmann.
Do, M. B., & Kambhampati, S. (2003). Sapa: A scalable multi-objective heuristic metric temporal
planner. Journal of Artificial Intelligence Research, 20, 155194.
Do, M. B., Ruml, W., & Zhou, R. (2008). On-line planning and scheduling: An application to controlling modular printers. In Proceedings of the Twenty-Third AAAI Conference on Artificial
Intelligence (AAAI 2008), pp. 15191523. AAAI Press.
Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: The language for the classical part of the 4th
International Planning Competition. Tech. rep. 195, Albert-Ludwigs-Universitat Freiburg,
Institut fur Informatik.
Fink, A., & Vo, S. (1999). Applications of modern heuristic search methods to pattern sequencing
problems. Computers and Operations Research, 26(1), 1734.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal planning
domains. Journal of Artificial Intelligence Research, 20, 61124.
Fuentetaja, R., Borrajo, D., & Linares Lopez, C. (2009). A unified view of cost-based heuristics. In
ICAPS 2009 Workshop on Heuristics for Domain-Independent Planning, pp. 7077.
Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2009). Deterministic planning
in the fifth international planning competition: PDDL3 and experimental evaluation of the
planners. Artificial Intelligence, 173(56), 619668.
Gerevini, A., & Serina, I. (2002). LPG: A planner based on local search for planning graphs with
action costs. In Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings of the Sixth
International Conference on Artificial Intelligence Planning and Scheduling (AIPS 2002), pp.
1322. AAAI Press.
Gregory, P., Cresswell, S., Long, D., & Porteous, J. (2004). On the extraction of disjunctive landmarks from planning problems via symmetry reduction. In Proceedings of the Fourth International Workshop on Symmetry and Constraint Satisfaction Problems, pp. 3441.
Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. Journal of Artificial Intelligence
Research, 28, 267297.
Hansen, E. A., Zilberstein, S., & Danilchenko, V. A. (1997). Anytime heuristic search: First results.
Technical report cmpsci 97-50, University of Massachusetts, Amherst.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence Research, 26, 191246.
Helmert, M. (2009). Concise finite-domain representations for PDDL planning tasks. Artificial
Intelligence, 173, 503535.
175

fiRichter & Westphal

Helmert, M., Do, M., & Refanidis, I. (2008). IPC 2008, deterministic part. Web site, http://ipc.
informatik.uni-freiburg.de.
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In Rintanen,
J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings of the Eighteenth International
Conference on Automated Planning and Scheduling (ICAPS 2008), pp. 140147. AAAI Press.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal of
Artificial Intelligence Research, 22, 215278.
Irani, K. B., & Cheng, J. (1987). Subgoal ordering and goal augmentation for heuristic problem
solving. In McDermott, J. P. (Ed.), Proceedings of the 10th International Joint Conference
on Artificial Intelligence (IJCAI 1987), pp. 10181024. Morgan Kaufmann.
Jonsson, P., & Backstrom, C. (1998). State-variable planning under structural restrictions: Algorithms and complexity. Artificial Intelligence, 100(12), 125176.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Proceedings of the
21st International Joint Conference on Artificial Intelligence (IJCAI 2009), pp. 17281733.
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In Proceedings
of the 18th European Conference on Artificial Intelligence (ECAI 2008), pp. 588592.
Keyder, E., & Geffner, H. (2009). Trees of shortest paths vs. Steiner trees: Understanding and improving delete relaxation heuristics. In Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI 2009), pp. 17341749.
Keyder, E., Richter, S., & Helmert, M. (2010). Sound and complete landmarks for and/or graphs.
In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th European Conference on Artificial Intelligence (ECAI 2010), pp. 335340.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use in an
agenda-driven planning algorithm. Journal of Artificial Intelligence Research, 12, 338386.
Likhachev, M., Ferguson, D., Gordon, G. J., Stentz, A., & Thrun, S. (2008). Anytime search in
dynamic graphs. Artificial Intelligence, 172(14), 16131643.
Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* with provable bounds
on sub-optimality. In Thrun, S., Saul, L. K., & Scholkopf, B. (Eds.), Advances in Neural
Information Processing Systems 16 (NIPS 2003).
Lipovetzky, N., & Geffner, H. (2009). Inference and decomposition in planning using causal consistent chains. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the
Nineteenth International Conference on Automated Planning and Scheduling (ICAPS 2009).
AAAI Press.
Liu, Y., Koenig, S., & Furcy, D. (2002). Speeding up the calculation of heuristics for heuristic
search-based planning. In Proceedings of the Eighteenth National Conference on Artificial
Intelligence (AAAI 2002), pp. 484491. AAAI Press.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1, 193
204.
176

fiThe LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks

Porteous, J., & Cresswell, S. (2002). Extending landmarks analysis to reason about resources and
repetition. In Proceedings of the 21st Workshop of the UK Planning and Scheduling Special
Interest Group (PLANSIG 02), pp. 4554.
Porteous, J., Sebastia, L., & Hoffmann, J. (2001). On the extraction, ordering, and usage of landmarks in planning. In Cesta, A., & Borrajo, D. (Eds.), Pre-proceedings of the Sixth European
Conference on Planning (ECP 2001), pp. 3748, Toledo, Spain.
Richter, S., & Helmert, M. (2009). Preferred operators and deferred evaluation in satisficing planning. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS 2009), pp.
273280. AAAI Press.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. In Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence (AAAI 2008), pp. 975982. AAAI
Press.
Richter, S., Thayer, J. T., & Ruml, W. (2010). The joy of forgetting: Faster anytime search via
restarting. In Brafman, R., Geffner, H., Hoffmann, J., & Kautz, H. (Eds.), Proceedings of the
Twentieth International Conference on Automated Planning and Scheduling (ICAPS 2010).
AAAI Press. To appear.
Roger, G., & Helmert, M. (2010). The more, the merrier: Combining heuristic estimators for satisficing planning. In Brafman, R., Geffner, H., Hoffmann, J., & Kautz, H. (Eds.), Proceedings
of the Twentieth International Conference on Automated Planning and Scheduling (ICAPS
2010), pp. 246249. AAAI Press.
Ruml, W., & Do, M. B. (2007). Best-first utility-guided search. In Veloso, M. M. (Ed.), Proceedings
of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007), pp. 2378
2384.
Sebastia, L., Onaindia, E., & Marzal, E. (2006). Decomposition of planning problems. AI Communications, 19(1), 4981.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Zilberstein, S., Koehler, J.,
& Koenig, S. (Eds.), Proceedings of the Fourteenth International Conference on Automated
Planning and Scheduling (ICAPS 2004), pp. 150159. AAAI Press.
Zhou, R., & Hansen, E. A. (2005). Beam-stack search: Integrating backtracking with beam search.
In Biundo, S., Myers, K., & Rajan, K. (Eds.), Proceedings of the Fifteenth International
Conference on Automated Planning and Scheduling (ICAPS 2005), pp. 9098. AAAI Press.
Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. In ICAPS 2003
Doctoral Consortium, pp. 156160.

177

fiJournal of Artificial Intelligence Research 39 (2010) 269 - 300

Submitted 04/10; published 09/10

Case-Based Subgoaling in Real-Time Heuristic Search
for Video Game Pathfinding
Vadim Bulitko

BULITKO @ UALBERTA . CA

Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Yngvi Bjornsson

YNGVI @ RU . IS

School of Computer Science, Reykjavik University
Menntavegi 1, IS-101 Reykjavik, ICELAND

Ramon Lawrence

RAMON . LAWRENCE @ UBC . CA

Computer Science, University of British Columbia Okanagan
3333 University Way, Kelowna, British Columbia, V1V 1V7, CANADA

Abstract
Real-time heuristic search algorithms satisfy a constant bound on the amount of planning per
action, independent of problem size. As a result, they scale up well as problems become larger.
This property would make them well suited for video games where Artificial Intelligence controlled agents must react quickly to user commands and to other agents actions. On the downside,
real-time search algorithms employ learning methods that frequently lead to poor solution quality
and cause the agent to appear irrational by re-visiting the same problem states repeatedly. The
situation changed recently with a new algorithm, D LRTA*, which attempted to eliminate learning by automatically selecting subgoals. D LRTA* is well poised for video games, except it has
a complex and memory-demanding pre-computation phase during which it builds a database of
subgoals. In this paper, we propose a simpler and more memory-efficient way of pre-computing
subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games. The new algorithm solves a number of randomly chosen problems off-line,
compresses the solutions into a series of subgoals and stores them in a database. When presented
with a novel problem on-line, it queries the database for the most similar previously solved case
and uses its subgoals to solve the problem. In the domain of pathfinding on four large video game
maps, the new algorithm delivers solutions eight times better while using 57 times less memory
and requiring 14% less pre-computation time.

1. Introduction
Heuristic search is a core area of Artificial Intelligence (AI) research and its algorithms have been
widely used in planning, game-playing and agent control. In this paper we are interested in realtime heuristic search algorithms that satisfy a constant upper bound on the amount of planning
per action, independent of problem size. This property is important in a number of applications
including autonomous robots and agents in video games. A common problem in video games is
searching for a path between two locations. In most games, agents are expected to act quickly
in response to players commands and other agents actions. As a result, many game companies
impose a constant time limit on the amount of path planning per move (e.g., one millisecond for all
simultaneously moving agents).
c
2010
AI Access Foundation. All rights reserved.

fiB ULITKO , B J ORNSSON , & L AWRENCE

While in practice this time limit can be satisfied by limiting problem size a priori, a scientifically
more interesting approach is to impose a time per-move limit regardless of the problem size. Doing
so severely limits the range of applicable heuristic search algorithms. For instance, static search
algorithms such as A* (Hart, Nilsson, & Raphael, 1968), IDA* (Korf, 1985) and PRA* (Sturtevant & Buro, 2005; Sturtevant, 2007), re-planning algorithms such as D* (Stenz, 1995), anytime
algorithms such as ARA* (Likhachev, Gordon, & Thrun, 2004) and anytime re-planning algorithms
such as AD* (Likhachev, Ferguson, Gordon, Stentz, & Thrun, 2005) cannot guarantee a constant
bound on planning time per action. This is because all of them produce a complete, possibly abstract, solution before the first action can be taken. As the problem increases in size, their planning
time will inevitably increase, exceeding any a priori finite upper bound.
Real-time search addresses the problem in a fundamentally different way. Instead of computing
a complete, possibly abstract, solution before the first action is taken, real-time search algorithms
compute (or plan) only a few first actions for the agent to take. This is usually done by conducting
a lookahead search of a fixed depth (also known as search horizon, search depth or lookahead
depth) around the agents current state and using a heuristic (i.e., an estimate of the remaining
travel cost) to select the next few actions. The actions are then taken and the planning-execution
cycle repeats (Korf, 1990). Since the goal state is not seen in most such local searches, the agent
runs the risks of selecting suboptimal actions. To address this problem, real-time heuristic search
algorithms update (or learn) their heuristic function over time.
The learning process has precluded real-time heuristic search agents from being widely deployed for pathfinding in video games. The problem is that such agents tend to scrub (i.e., repeatedly re-visit) the state space due to the need to fill in heuristic depressions (Ishida, 1992). As a result,
solution quality can be quite low and, visually, the scrubbing behavior is perceived as irrational.
Since the seminal work on LRTA* (Korf, 1990), researchers have attempted to speed up the
learning process. We briefly describe the efforts in the related work section. Here, we note that
while the various approaches all brought about improvements, a breakthrough performance was
achieved by virtually eliminating the learning process in D LRTA* (Bulitko, Lustrek, Schaeffer,
Bjornsson, & Sigmundarson, 2008). This was done by computing the heuristic with respect to a
near-by subgoal as a distant goal. Offline, D LRTA* constructs a high-level graph of regions using
state abstractions, calculates optimal paths between all region pairs, and then stores as subgoals
the states where the paths cross region boundaries. During the online search, D LRTA* consults
the database to find the next subgoal with respect to the current and goal regions. Since heuristic
functions usually relax the problem (e.g., the Euclidean distance heuristic ignores obstacles on a
map), they tend to be more accurate closer to a goal. As a result, a heuristic function with respect
to a near-by goal tends to be more accurate and, therefore, requires less adjustment (i.e., learning).
Consequently, the solution quality is improved and the scrubbing behavior is reduced.
In this paper, we adapt the idea of subgoaling and make the following four contributions. First,
we simplify the pre-processing step of D LRTA*. Instead of using state abstraction to select subgoals, we employ a nearest-neighbour algorithm over a database of solved cases. Second, we introduce the idea of compressing a solution path into a series of subgoals so that each can be easily
reached from the previous one. In doing so, we use hill-climbing as a proxy for the notion of easy
reachability by LRTA*. Third, we employ kd-trees in order to access the case base effectively.
Finally, we evaluate the new algorithm empirically in large-scale problem spaces.
The new algorithm is called k Nearest Neighbor LRTA* (or kNN LRTA*) and, for the rest of the
paper, we set k = 1. This paper extends our previous conference publication (Bulitko & Bjornsson,
270

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

2009) in the following ways. We store multiple goals per path to reduce the number of database accesses and we use kd-trees to speed up each database access. Additionally, we make optimizations
to the on-line component of kNN LRTA*: evaluating only a small number of most similar database
entries, interrupting LRTA* if it starts learning excessively and engaging start and end path optimizations. On the empirical evaluation side, we now use native multi-million state static video game
maps and compare our algorithm to a newly published state-of-the-art non-learning real-time search
algorithm (Bjornsson, Bulitko, & Sturtevant, 2009).
The rest of the paper is organized as follows. In Sections 2 and 3 we formulate the problem of
real-time heuristic search and show how the core LRTA* algorithm can be extended with subgoal
selection. Section 4 analyzes related research. Section 5 provides intuition for the new algorithm
following with details and pseudocode in Section 6. In Section 7 we give theoretical analysis and,
in Section 8, empirically evaluate the algorithm in the domain of pathfinding. Section 9 summarizes
the empirical results. We then conclude with a discussion of current shortcomings and future work.

2. Problem Formulation
We define a heuristic search problem as a directed graph containing a finite set of states (vertices)
and weighted edges, with a single state designated as the goal state. At every time step, a search
agent has a single current state, a vertex in the search graph, and takes an action (or makes a move)
by traversing an out-edge of the current state. By traversing an edge between states s1 and s2 the
agent changes its current state from s1 to s2 . We say that a state is visited by the agent if and
only if it was the agents current state at some point of time. As it is usual in the field of real-time
heuristic search, we assume that path planning happens between the moves (i.e., the agent does not
think while traversing an edge). The plan a move - travel an edge loop continues until the agent
arrives at its goal state, thereby solving the problem.
Each edge has a positive cost associated with it. The total cost of edges traversed by an agent
from its start state until it arrives at the goal state is called the solution cost. We require algorithms
to be complete (i.e., produce a path from start to goal in a finite amount of time if such a path exists).
In order to guarantee completeness for real-time heuristic search we make the assumption of safe
explorability of our search problems. Specifically, all costs are finite and for any states s1 , s2 , s3 ,
if there is a path between s1 and s2 and there is a path between s1 and s3 then there is also a path
between s2 and s3 .
Formally, all algorithms discussed in this paper are applicable to any such heuristic search problem. To keep the presentation focused and intuitive as well as to afford a large-scale empirical
evaluation, we will use a particular type of heuristic search problem, pathfinding in grid worlds,
for the rest of the paper. We discuss applicability of the new methods we suggest to other heuristic
search and general planning problems in Section 11.
In video-game map settings, states are vacant square grid cells. Each cell is connected to four
cardinally (i.e., west, north, east, south) and four diagonally neighboring cells. Outbound edges of
a vertex are moves available in the corresponding cell and in the rest of the paper we will use the
terms action and move interchangeably. The edge costs are defined as 1 for cardinal moves and 1.4
for diagonal moves.1
An agent plans its next action by considering states in a local search space surrounding its
current position. A heuristic function (or simply heuristic) estimates the (remaining) travel cost
1. We use 1.4 instead of the Euclidean


2 to avoid errors in floating point computations.

271

fiB ULITKO , B J ORNSSON , & L AWRENCE

between a state and the goal. It is used by the agent to rank available actions and select the most
promising one. In this paper we consider only admissible and consistent heuristic functions which
do not overestimate the actual remaining cost to the goal and whose difference in values for any two
states does not exceed the cost of an optimal path between these states. In this paper we use octile
distance  the minimum cumulative edge cost between two vertices ignoring map obstacles  as our
heuristic. This heuristic is admissible and consistent and uses 1 and 1.4 as the edge costs. An agent
can modify its heuristic function in any state to avoid getting stuck in local minima of the heuristic
function, as well as to improve its action selection with experience.
The defining property of real-time heuristic search is that the amount of planning the agent does
per action has an upper bound that does not depend on the total number of states in the problem
space. Fast planning is preferred as it guarantees the agents quick reaction to a new goal specification. We measure mean planning time per action in terms of CPU time. We do not use the number
of states expanded as a CPU-independent measure of time because the algorithms evaluated in this
paper frequently perform time-consuming operations other than expanding states. Also note that
while total planning time per problem is important for non-real-time search, it is irrelevant in video
game pathfinding as we do not compute an entire path outright.
The second performance measure of our study is sub-optimality defined as the ratio of the so
lution cost
 found by the agent (c) to the minimum solution cost (c ) minus one and times 100%:
c
c  1  100. To illustrate, suboptimality of 0% indicates an optimal path and suboptimality of
50% indicates a path 1.5 times as costly as the optimal path.

3. LRTA*: The Core Algorithm
The core of most real-time heuristic search algorithms is an algorithm called Learning Real-Time
A* (LRTA*) (Korf, 1990). It is shown in Figure 1 and operates as follows. As long as the goal state
sglobal goal is not reached, the algorithm interleaves planning and execution in lines 4 through 7. In
our generalized version we added a new step at line 3 for selecting goal sgoal (the original algorithm
uses sglobal goal at all times). We will describe the details of subgoal selection later in the paper. In
line 4, a cost-limited breadth-first search with duplicate detection is used to find frontier states with
cost up to gmax away from the current state s. For each frontier state s, its value is the sum of the
cost of a shortest path from s to s, denoted by g(s, s), and the estimated cost of a shortest path from
s to sgoal (i.e., the heuristic value h(s, sgoal )). The state that minimizes the sum is identified as s0
in line 5. Ties are broken in favour of higher g values. Remaining ties are broken in a fixed order.
The heuristic value of the current state s is updated in line 6 (we keep separate heuristic tables for
the different goals and we never decrease heuristics). Finally, we take one step towards the most
promising frontier state s0 in line 7.
LRTA* is a special case of value iteration or real-time dynamic programming (Barto, Bradtke,
& Singh, 1995) and has a problem that has prevented its use in video game pathfinding. Specifically, it updates a single heuristic value per move on the basis of heuristic values of near-by states.
This means that when the initial heuristic values are overly optimistic (i.e., too low), LRTA* will
frequently re-visit these states multiple times, each time making updates of a small magnitude. This
behavior is known as scrubbing2 and appears highly irrational to an observer.
2. The term was coined by Nathan Sturtevant.

272

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

LRTA*(sstart , sglobal goal , gmax )
1
2
3
4
5
6
7
8

s  sstart
while s 6= sglobal goal do
if no subgoal is selected or the current subgoal is reached then select a (new) subgoal sgoal
generate successor states of s up to gmax cost, generating a frontier
find a frontier state s0 with the lowest g(s, s0 ) + h(s0 , sgoal )
update h(s, sgoal ) to g(s, s0 ) + h(s0 , sgoal )
change s one step towards s0
end while
Figure 1: LRTA* algorithm with dynamic subgoal selection.

4. Related Research
Since the seminal work on LRTA* described in the previous section, researchers have attempted to
speed up the learning process. Most of the resulting algorithms can be described by the following
four attributes:
The local search space is the set of states whose heuristic values are accessed in the planning
stage. The two common choices are full-width limited-depth lookahead (Korf, 1990; Shimbo &
Ishida, 2003; Shue & Zamani, 1993; Shue, Li, & Zamani, 2001; Furcy & Koenig, 2000; Hernandez
& Meseguer, 2005a, 2005b; Sigmundarson & Bjornsson, 2006; Rayner, Davison, Bulitko, Anderson, & Lu, 2007) and A*-shaped lookahead (Koenig, 2004; Koenig & Likhachev, 2006). Additional choices are decision-theoretic based shaping (Russell & Wefald, 1991) and dynamic lookahead depth-selection (Bulitko, 2004; Lustrek & Bulitko, 2006). Finally, searching in a smaller,
abstracted state has been used as well (Bulitko, Sturtevant, Lu, & Yau, 2007).
The local learning space is the set of states whose heuristic values are updated. Common
choices are: the current state only (Korf, 1990; Shimbo & Ishida, 2003; Shue & Zamani, 1993; Shue
et al., 2001; Furcy & Koenig, 2000; Bulitko, 2004), all states within the local search space (Koenig,
2004; Koenig & Likhachev, 2006) and previously visited states and their neighbors (Hernandez &
Meseguer, 2005a, 2005b; Sigmundarson & Bjornsson, 2006; Rayner et al., 2007).
A learning rule is used to update the heuristic values of the states in the learning space.
The common choices are mini-min (Korf, 1990; Shue & Zamani, 1993; Shue et al., 2001;
Hernandez & Meseguer, 2005a, 2005b; Sigmundarson & Bjornsson, 2006; Rayner et al., 2007),
their weighted versions (Shimbo & Ishida, 2003), max of mins (Bulitko, 2004), modified Dijkstras
algorithm (Koenig, 2004), and updates with respect to the shortest path from the current state to the
best-looking state on the frontier of the local search space (Koenig & Likhachev, 2006). Additionally, several algorithms learn more than one heuristic function (Russell & Wefald, 1991; Furcy &
Koenig, 2000; Shimbo & Ishida, 2003).
The control strategy decides on the move following the planning and learning phases. Commonly used strategies include: the first move of an optimal path to the most promising frontier
state (Korf, 1990; Furcy & Koenig, 2000; Hernandez & Meseguer, 2005a, 2005b), the entire
path (Bulitko, 2004), and backtracking moves (Shue & Zamani, 1993; Shue et al., 2001; Bulitko,
2004; Sigmundarson & Bjornsson, 2006).
Given the multitude of proposed algorithms, unification efforts have been undertaken. In particular, Bulitko and Lee (2006) suggested a framework, called Learning Real Time Search (LRTS), to
273

fiB ULITKO , B J ORNSSON , & L AWRENCE

combine and extend LRTA* (Korf, 1990), weighted LRTA* (Shimbo & Ishida, 2003), SLA* (Shue
& Zamani, 1993), SLA*T (Shue et al., 2001), and to a large extent, -Trap (Bulitko, 2004). In the
dimensions described above, LRTS operates as follows. It uses a full-width fixed-depth local search
space with transposition tables to prune duplicate states. LRTS uses a max of mins learning rule to
update the heuristic value of the current state (its local learning space). The control strategy moves
the agent to the most promising frontier state if the cumulative volume of heuristic function updates
on a trial is under a user-specified quota or backtracks to its previous state otherwise.
While the approaches listed above all brought about various improvements, breakthrough performance came in the form of subgoaling. Since commonly used heuristics simplify the problem
at hand (e.g., the octile distance in grid-world pathfinding ignores obstacles), using LRTA* with
near-by subgoals effectively increases heuristic quality and thus reduces the amount of learning.
Although in general planning a goal is often represented as a conjunction of simple subgoals, to
the best of our knowledge, the only real-time heuristic search algorithm to implement subgoaling
is D LRTA* (Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007; Bulitko et al., 2008).
In its pre-processing phase, D LRTA* uses the clique abstraction of Sturtevant and Buro (2005) to
create a smaller search graph. The clique abstraction collapses a set of fully connected states into
a single abstract state and can be applied iteratively to compute progressively smaller graphs. For
example, a 2-level abstraction applies the clique abstraction to a graph that has already been abstracted once. Similarly, an a-level abstraction applies the clique abstraction a times. If we assume
that each abstraction reduces the graph by a constant factor , an a-level abstract graph would contain a times fewer states than the original graph. This abstraction technique in effect partitions the
map into a number of regions, with each region corresponding to a single abstract state. Then for
every pair of distinct abstract states, D LRTA* computes an optimal path between corresponding
representative states (e.g., centroids of the regions) in the original non-abstracted space. The path
is followed until it exits the region corresponding to the start abstract state. The entry state to the
next region is recorded as the subgoal for the pair of abstract states. Once this pre-processing step is
finished, D LRTA* runs LRTA* for a given problem but selects the subgoal recorded for the current
and the goal regions. The off-line and on-line steps are illustrated in Figure 2.
The underlying intuition is that reaching the entry-to-the-next-region state requires LRTA* to
navigate within a single region and is, therefore, easy with a default heuristic function. As a result,
D LRTA* would rarely need to adjust its heuristic thereby virtually eliminating the costly learning
process and the resulting scrubbing.
There are three key problems with D LRTA*. First, due to the fact that entry states (i.e., subgoals) have to be computed and stored for each pair of distinct regions, the number of regions has
to be kept relatively small. In D LRTA* this is accomplished by applying the clique abstraction
procedure multiple times so that the regions become progressively larger and fewer in number. A
side effect is that regions will no longer be cliques and may, in fact, be quite complex in themselves.
As a result, LRTA* may encounter heuristic depressions within a region (e.g., this would actually
happen if LRTA* tries to go from S to E in the right diagram of Figure 2). Second, each state in
the original space needs to be assigned to a region. Since the regions are irregular in shape, explicit
membership records must be maintained. This may require as much additional memory as storing
the original grid-based map. Third, clique abstraction is a non-trivial process and puts an extra
programming burden on practitioners (e.g., game developers).
Another recent high-performance real-time search algorithm is Time-Bounded A* (TBA*)
by Bjornsson et al. (2009), a time-bounded variant of the classic A*. It expands states in an A*
274

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

2

2

2

2

2

2

7

7

2

7
7

2

5

5

5

5

5

7

2

1

1

1

1

5

7

2

1

1

5

7

2

1

1

1

3

6

2

1

1

3

3

6

2
4

G

3
4

4

4

4

6

C2

C1

E

S

E

6
6

6

6

Figure 2: Example of D LRTA* operation. Left: off-line, the map is partitioned into seven regions
(or abstract states). Each vacant cell is labeled with its region number. Center: off-line,
an optimal path between centroids of two regions (C1 and C2 ) is computed and the entry
state to the next region (E) is recorded as a subgoal for this pair of regions. Right: online, the agent intends to travel from S to G, it determines the corresponding regions and
sets the pre-computed entry state E as its subgoal.

fashion using a closed list and an open list, away from the original start state, towards the goal until
the goal state is expanded. However, unlike A* that plans a complete path before committing to the
first action, TBA* time-slices the planning by interrupting its search periodically and acts. Initially,
before a complete path to the goal is known, the agent takes an action that moves it towards the most
promising state on the open list. If on a subsequent time slice an alternative most promising path
is formed and the agent is not on that path, it backtracks its steps as necessary. This interleaving
of planning, acting, and backtracking is done in such a way that both real-time behavior and completeness are ensured. The size of the time-slice is given as a parameter to the algorithm, using as
a metric the number of states allowed to expand before the planning must be interrupted. Within a
single time-slice, however, operations for both state expansions and backtracing the closed list (to
form the path to the most promising state on the open list) must be performed. The cost of the latter
type of operations is thus converted to state expansion equivalence (typically several backtracing
steps can be performed at the same computational cost as a single state expansion). A key aspect of
TBA* over LRTA*-based algorithms is that it retains closed and open lists over its planning steps.
Thus, on each planning step it does not start planning from scratch, but continues with its open and
closed lists from the previous planning step. Also, it does not need to update heuristics online to
ensure completeness, nor does it require a precomputation phase. While the lack of precomputation
is certainly its strong side, the negatives include high suboptimality if the amount of time per move
is low and high on-line space complexity due to storing closed and open lists.
This research is related to work from the realm of non-real-time heuristic search where pattern
databases are widely used to store pre-calculated distance information about abstractions of the
original (ground) search space (Culberson & Schaeffer, 1998). A recent approach for using precalculated state-space information is to calculate true distances between selected state pairs and
then use them whenever possible to make the distance estimates of the search guidance heuristic
h more informative. Two such enhanced heuristics are the differential heuristic (Cazenave, 2006;
Sturtevant, Felner, Barrer, Schaeffer, & Burch, 2009) and the canonical heuristic (Sturtevant et al.,
275

fiB ULITKO , B J ORNSSON , & L AWRENCE

2009). In the former case, a true distance d is pre-calculated from all states to a small subset of
states S, so-called canonical states. During the on-line search the heuristic distance between any
two arbitrary states a and b is calculated as the maximum of h(a, b) = |d(a, s)  d(b, s)| over
all canonical states s  S. In the latter case, for each state in the state space the true distance
d to the closest canonical state is pre-calculated and stored and so is the true distance between
all pairs of canonical states. During the search, the heuristic distance between any two states a
and b is calculated as h(a, b) = d(C(a), C(b))  d(a, C(a))  d(b, C(b)) where C(s) returns
the closest canonical state to s. These heuristics may return a lower distance estimate than an
unmodified heuristic, so in practice one chooses the maximum of the two. An idea similar to the
canonical heuristic was proposed earlier in a more specialized context, where the heuristic function
was improved by pre-calculating true distances between several strategically chosen passageways
in a game map (Bjornsson & Halldorsson, 2006). These heuristics are not used in real-time search.
There is a large volume of work on case-based planning (e.g., Nebel & Koehler, 1995). This
includes path planning, where case-based approaches have been used to augment heuristic search
for tasks such as route selection in road maps and mobile robot navigation. Such approaches typically pre-compute and store paths, as opposed to distances, between selected states, and then use
them as model solutions for related pathfinding tasks in a case-based reasoning (CBR) fashion. One
of the early works on combining search and case-based reasoning in pathfinding on road maps was
done within the planning and learning system P RODIGY (Carbonell, Knoblock, & Minton, 1990),
with the goal of generating near-optimal routes for an autonomous navigation vehicle trying to
achieve multiple goals while driving in a city (Haigh & Veloso, 1993). The authors acknowledge
the benefits of such an approach in the situation where it is necessary to interleave planning and execution. Subsequent work on case-based route selection has though mainly focused on augmenting
non-interleaving path-planning algorithms, such as A* or Dijkstra, with the focus of the work on
how best to build the case base, for example, how to identify, compute, and store paths to critical
junctions that many paths pass through (Anwar & Yoshida, 2001; Weng, Wei, Qu, & Cai, 2009). As
for mobile robot navigation, two heuristic search algorithms working in ground space and using a
CBR-based approach were introduced by Branting and Aha (1995). The simpler one, when looking
for a path between states a and b, searches the pre-calculated case base for a path that contains both
a and b. If a match is found the best path is returned, otherwise a regular A* search is invoked to
calculate the solution path. The second, and more elaborate, algorithm searches the case base for a
match in the same fashion as the first, but if none is found, it adapts an existing case to fit the new
task. This is done by using A* to join a and b to an existing path in the case base so that the new
overall distance is minimized. There is still ongoing research in this area, for example, work on
storing the case base as a graph structure called a case-graph that gradually builds a waypoint-like
navigation network (Hodal & Dvorak, 2008). Note that these and many other existing algorithms
are not real-time as they generate or modify complete plans.

5. Intuition for kNN LRTA*
In our design of kNN LRTA* we address the three shortcomings of D LRTA* listed earlier. In
doing so, we identify two key aspects of a subgoal-based real-time heuristic search. First, we need
to define a set of subgoals that would be efficient to compute and store off-line. Second, we need to
define a way for the agent to find a subgoal relevant to its current problem on-line.
276

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

Intuitively, if an LRTA*-controlled agent is in the state s going to the state sgoal then the best
subgoal is a state sideal subgoal that resides on an optimal path between s and sgoal and can be reached
by LRTA* along an optimal path with no state re-visitation. Given that there can be multiple optimal
paths between two states, it is unclear how to computationally efficiently detect the LRTA* agents
deviation from an optimal path immediately after it occurs.
On the positive side, detecting state re-visitation can be done computationally efficiently by running a simple greedy hill-climbing agent. This is based on the fact that if a hill-climbing agent can
reach a state b from a state a without encountering a local minimum or a plateau in the heuristic then
an LRTA* agent can travel from a to b without state re-visitation (Theorem 5). Thus, we propose
an efficiently computable approximation to sideal subgoal . Namely, we define the subgoal for a pair
of states s and sgoal as the state skNN LRTA* subgoal farthest along an optimal path between s and sgoal
that can be reached by a simple hill-climbing agent (defined rigorously in the following section). In
summary, we select subgoals to eliminate any scrubbing (Theorem 5) but do not guarantee that the
LRTA* agent keeps on an optimal path between the subgoals (Theorem 6). In practice, however,
only a tiny fraction of our subgoals are reached by the hill-climbing agent suboptimally and even
then the suboptimality is minor.
This approximation to the ideal subgoal allows us to effectively compute a series of subgoals
for a given pair of start and goal states. Intuitively, we compress an optimal path into a series
of key states such that each of them can be reached from its predecessor without scrubbing. The
compression allows us to save a large amount of memory without much impact on time-per-move.
Indeed, hill-climbing from one of the key states to the next requires inspecting only the immediate
neighbors of the current state and selecting one of them greedily. The re-visitation-free reachability
of one subgoal from another addresses the first key shortcoming of D LRTA* where the agent may
get trapped within a single complex region and thus be unable to reach its prescribed subgoal.
However, it is still infeasible to compute and then compress an optimal path between every
two distinct states in the original search space. We solve this problem by compressing only a
pre-determined fixed number of optimal paths between random states off-line. Then on-line kNN
LRTA*, tasked with going from s to sgoal , retrieves the most similar compressed path from its
database and uses the associated subgoals. We define (dis-)similarity of a database path to the
agents current situation as the maximum of the heuristic distances between s and the paths beginning and between sgoal and the paths end. We use maximum because we would like both ends of
the path to be heuristically close to the agents current state and the goal respectively. Indeed, the
heuristic distance ignores walls and thus a large heuristic distance to the paths either end tends to
make that end hill-climbing unreachable.
Note that high similarity (i.e., both distances being low) does not guarantee that the path will
be useful to the kNN LRTA* agent. For instance, the beginning of the path can be heuristically
very close to the agent but on the other side of a long wall, making it unreachable without a lot
of learning and the associated scrubbing. To address this problem we complement the fast-tocompute similarity metric with more computationally demanding move-limited reachability checks
as detailed below.
We illustrate this intuition with a simple example. Figure 3 shows kNN LRTA* operation offline. On this map, two random start and goal pairs are selected and optimal paths are computed
between them. Then each path is compressed into a series of subgoals such that each of the subgoals
can be reached from the previous one via hill-climbing. The path from S1 to G1 is compressed into
two subgoals and the other path is compressed into a single subgoal.
277

fiB ULITKO , B J ORNSSON , & L AWRENCE

G1

G2

G1

G2

G1

G2

1

S1

S1

S1
S2

S2

S2

2

1

Figure 3: Example of kNN LRTA* off-line operation. Left: two subgoals (start,goal) pairs are
chosen: (S1 , G1 ) and (S2 , G2 ). Center: optimal paths between then are computed by
running A*. Right: the two paths are compressed into a total of three subgoals.

Once this database of two records is built, kNN LRTA* can be tasked with solving a problem
on-line. In Figure 4 it is tasked with going from the state S to the state G. The database is scanned
and similarity between (S, G) and each of the two database records is determined. The records are
sorted by their similarity: (S1 , G1 ) followed by (S2 , G2 ). Then the agent runs reachability checks:
from S to Si and from Gi to G where i runs the database indices in the order of record similarity.
In this example, S1 is found unreachable by hill-climbing from S and thus the record (S1 , G1 ) is
discarded. The second record passes hill-climbing checks and the agent is tasked with going to its
first subgoal (shown as 1 in the figure).
G

G

G

G1

G2

S

G1

G2

S

S

S1

S1
S2

S2

1

Figure 4: Example of kNN LRTA* on-line operation. Left: the agent intends to travel from S to
G. Center: similarity of (S, G) to (S1 , G1 ) and (S2 , G2 ) is computed. Right: while
(S1 , G1 ) is more similar to (S, G) than (S2 , G2 ), its beginning S1 is not reachable from
S via hill-climbing and hence the record (S2 , G2 ) is selected and the agent is tasked with
going to subgoal 1.
The similarity plus hill-climbing check approach makes the state abstraction of D LRTA* unnecessary, thereby addressing its other two key shortcomings: high memory requirements and a
complex pre-computation phase.
278

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

6. kNN LRTA* in Detail
In this section we flesh out kNN LRTA* in enough detail for other researchers to implement it. We
start with a basic version and then describe several significant enhancements.
6.1 Basic kNN LRTA*
kNN LRTA* consists of two parts: database pre-computation (off-line) and LRTA* with dynamically selected subgoals (on-line). Pseudocode for the off-line part is presented in Figure 5. The
top-level function computeSubgoals takes a user-controlled parameter N and a search graph
(e.g., a grid-based map in pathfinding) and builds a subgoal database of N compressed paths.
Each path is generated in line 4 from start and goal states randomly chosen in line 3. If the
path does not exist or is too short (line 5), we discard it and re-generate the start and goal
states. The compression takes place inthe function compress, which returns a sequence of states
np
, sgoal where np  0 is the number of subgoals (line 6). The
p = sstart , s1subgoal , . . . , ssubgoal
sequence p is the compressed representation of path p and forms a single record in the subgoal
database (line 7).
subgoal database  computeSubgoals(N, G)
1 subgoal database  
2 for n = 1, . . . , N do
3
generate a random pair of states (sstart , sgoal )
4
compute an optimal path p from sstart to sgoal with A*
5
if p =   |p| < 3 then go to step 3 end if
6
p  compress(p)
7
add p to the subgoal database
8 end for
Figure 5: kNN LRTA* off-line: building a subgoal database.
Pseudocode for the function compress is found in Figure 6. It takes the path p = (sstart , . . . , sgoal ) =
(s1 , . . . , st ) as an argument and returns a subset  of it  the states reachable from each other via
hill-climbing (and thus without scrubbing). The code builds the sequence  of indices of states
which will then be put into  as subgoals. As long as the path is not exhausted (line 2), the next
candidate subgoal is defined by the index i in line 3. Note that the state with the index i = end()+1
is always hill-climbing reachable from the state with the index end() because these two states are
immediate neighbours. We then run a binary search defined by the scope of indices [l, r] in lines
4 and 5. The middle of the scope is calculated in line 7 and its hill-climbing reachability from the
latest computed subgoal send() is checked in line 8. If the middle is indeed hill-climbing reachable
then the scope is moved to the upper half (line 10) and the candidate subgoal is updated (line 9).
Otherwise, the scope of the binary search is moved to the lower half in line 12. Once the binary
search is completed, the candidate subgoal is added to  in line 15.3 We convert the sequence of
indices  into the sequence of states  in line 17.
The function reachable(sa , sb ) checks if a hill-climbing agent can reach the state sb from the
state sa . The pseudocode is found in Figure 7. We start climbing from the state sa (line 1). As long
3. We use parentheses with set operations to indicate that  is an ordered set.

279

fiB ULITKO , B J ORNSSON , & L AWRENCE

  compress((s1 , . . . , st ))
1   (1)
2 while t 6  do
3
i  end() + 1
4
l i+1
5
rt
6
while l  r do
7
m  b l+r
2 c
8
if reachable(send() , sm ) then
9
im
10
l m+1
11
else
12
r m1
13
end if
14
end while
15
    (i)
16 end while
17   s
Figure 6: kNN LRTA* off-line: compressing a path into a sequence of subgoals.
  reachable(sa , sb )
1 s  sa
2 while s 6= sb do
3
generate immediate successor states of s, generating a frontier
4
if h(s)  mins00  frontier (h(s00 )) then break
5
find the frontier state s0 with the lowest g(s, s0 ) + h(s0 , sb )
6
s  s0
7 end while
8   (s = sb )
Figure 7: Checking if one state is reachable from another. When this function is called on-line, a
fixed cap is put on the number of iterations in the while loop.
as the goal is not reached (line 2), we generate immediate successors of the current state (line 3) and
check if we are in a local heuristic minimum or a plateau (line 4). If so we terminate our climb and
declare that sb is not hill-climbing reachable from sa . Otherwise we climb towards a frontier state
with the lowest g + h value (lines 5 and 6). We use g + h instead of h to make the move selection
correspond to that of LRTA*. Additionally, the ties are broken in exactly the same way as they are
with the LRTA* algorithm in Figure 1. Note that whenever the function reachable is called in the
on-line phase, we impose a fixed cutoff on the number of steps hill-climbing is allowed to travel.
This is done to place an upper bound on the time complexity of the reachability check independent
of the number of states in the search graph, as required by real-time operation.
In the on-line phase of kNN LRTA*, we run LRTA* as per Figure 1. Dynamic subgoal selection
(line 3) is done as per pseudocode in Figure 8. Given a start and goal state, we scan our subgoal
280

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

database and, for each record, compute heuristic distance between our start state and the records
first state as well as heuristic distance between our goal state and the records last state. As we mentioned earlier, we define the (dis-)similarity between our problem and the record as the maximum
of the two heuristic distances. This is done so that similar records are such where the start and end
are both close to the agents current position and its goal in terms of the heuristic distance.
All database records are sorted by their similarity to the agents current and global goal states
(line 1) and, starting with the most similar record, we check if its start and end are hill-climbing
reachable from the agents current state and the agents global goal respectively (line 4). If either
reachability check fails, we go onto the next record. Otherwise, we stop the database search (line
6). If we exhaust the database and find no reachable record, we resort to the global goal (line 9).
Once a record is found, all its subgoals are fed one by one to LRTA* in line 3 of Figure 1.
The intuition is that our similarity metric uses heuristic distance and, therefore, ignores some
constraints of the problem (e.g., walls in grid-based pathfinding). Thus, a database record with a
high similarity value may not be relevant to the agents situation as its start and goal may be on
the other side of a wall which means that its subgoals will not be reachable by LRTA* without
scrubbing and therefore are useless to the agent.
r  selectSubgoals(s, sglobal goal )
1
2
3
4
5
6
7
8
9

(r1 , . . . , rN )  database records from most to least similar
for i = 1, . . . , N do
retrieve ri = (sstart , . . . , send )
if reachable(s, sstart ) and reachable(send , sglobal goal ) then
r  ri
return
end if
end for

r  s, sglobal goal
Figure 8: kNN LRTA* on-line: selecting subgoals.

6.2 Enhanced kNN LRTA*
We have presented the basic kNN LRTA* algorithm. In this section we introduce six enhancements.
First, before selecting a database record in the function selectSubgoals, we check if the global
goal is reachable from the agents current state. This is done by calling the function reachable. If
the global goal is indeed reachable via move-limited hill-climbing then we set it as the agents goal
and do not look for a subgoal. Otherwise, we turn to the database for subgoals.
Second, having selected a database record in the routine selectSubgoals, we run a reachability
check between the agents current state and the first subgoal in the record. If the first subgoal is
reachable then we set it as the goal for LRTA*. Otherwise, we set the LRTA* to go to the start state
of the record which is already checked to be reachable within the function selectSubgoals.
Third, when LRTA* reaches the last subgoal (i.e., the state in the record immediately prior to
the end of the record), it checks if the global goal is reachable from it. If so, the global goal is used
as the next subgoal. Otherwise, the agent heads for the end of the record from which it can reach
the global goal as guaranteed by the record selection criteria.
281

fiB ULITKO , B J ORNSSON , & L AWRENCE

Each of the first three enhancements addresses a trade-off between path optimality and planning
time per move. Specifically, calling the function reachable, while real-time, increases kNN LRTA*
planning time per move but, at the same time, leads to a potentially shorter solution due to better
subgoal selection. Recall that the function reachable satisfies the real-time operation constraint
because we place an a priori limit on the number of moves it can take.
Reachability checks constitute a substantial portion of kNN LRTA*s planning time per move.
The other substantial contributor is accessing the record database and computing record similarity.
The basic algorithm described above always computes the similarity for all database records and, in
the worst case, runs reachability checks for all records in the function selectSubgoals. While this
does not depend on the search graph size and is thus real-time, we can still speed it up as follows.
The fourth enhancement is to run reachability checks only for a fixed number of most similar
records. This can be done simply by substituting the total number of database records N with a
fixed constant M  N in line 2 of Figure 8. The intuition is that only fairly similar records are
worth checking for reachability.
When M  N this enhancement can substantially reduce the amount of planning time taken
up by reachability checks. However, the similarity is still computed for all records in the database
(line 1 in Figure 8). The fifth enhancement speeds this step up by employing kd-trees instead of a
linear database scan. A kd-tree (Moore, 1991) is a spatial tree index that can have a sublinear time
complexity for nearest-neighbor searches. Specifically, our kd-tree indexes start and end states of
the subgoal database records. Each tree node is thus a four-tuple (xstart , ystart , xend , yend ). The index
works by dividing the search space along a dimension at each level of the tree. The search space is
divided on xstart below the root node of the tree, ystart at the next level down, xend on the next level,
yend on the next, and then the cycle repeats. For example, if the root node is (4, 5, 8, 9), then the
start state has coordinates (4, 5) and the end state has coordinates (8, 9). Further, any nodes in the
left subtree have xstart  4, and nodes in the right subtree have xstart > 4.
To illustrate, consider the tree in Figure 9 and a subgoal record whose start state is (8, 4) and
whose goal state is (4, 9). This records will be represented by a kd-tree node (8, 4, 4, 9). It is in the
right subtree of the root as its xstart = 8 which is greater than the roots value of 4. It is in the left
subtree of the next node as its value of 4 for ystart is less than the nodes value of 5. At the third
level, it is in the left subtree as its value of 4 for xend is less than 6. Finally, it is the right subtree of
its parent at level four because its value of yend = 9 is greater than the 8 of its parent.
4,5,8,9
startX <= 4

startY <= 7

7,5,6,4
startY > 7

4,5,4,5
endX <= 4

3,2,2,1

startY <= 5

2,8,3,3
endX > 4

1,6,8,3

divide by start xcoordinate

startX > 4

3,7,6,6

endX <= 3

1,9,2,6

8,3,6,4
endX > 3

5,9,4,6

endX <= 6

3,8,5,4

endX > 6

6,2,5,8
endY <= 8

8,3,3,7

divide by start ycoordinate

startY > 5

9,1,9,3
endY > 8

endX <= 4

9,9,4,4

endX > 4

divide by end xcoordinate

9,9,5,5
divide by end ycoordinate

8,4,4,9

Figure 9: A kd-tree for database access.
This structure allows nearest-neighbors to be computed without searching all paths in the tree
index by eliminating some subtrees based on distance. For instance, if the search currently has the
best M records found so far, and it encounters a node in the tree where it can be guaranteed that all
282

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

nodes are farther away than those M records from the search target, then that subtree is not searched.
The nearest-neighbor search algorithm is explained by Moore (1991). Note that the kd-tree index
works for regular grid pathfinding problems but not necessarily all heuristic search problems. For
instance, high-cost edges connecting states with similar coordinates or low-cost edges connecting
states with distant coordinates would present a problem for the kd-tree index.
Given a subgoal database, we build a kd-tree to index it off-line and store it together with the
database. On-line, we use the kd-tree to identify the M records relevant to the agents current start
and goal states (line 1 in Figure 8). We then compute the similarity metric only for these M records.
The sixth enhancement deals with the case where kNN LRTA* is unable to find a subgoal and
resorts to its global goal. This happens in the function selectSubgoals (line 9 of Figure 8). A failure
to find a subgoal is caused by none of the M most similar records passing our reachability checks.
Having to resort to a global goal indicates an insufficient database coverage of the current area in
the space of start and goal state pairs. Given that records are compressed optimal paths between
randomly generated start and goal states, database coverage is likely to be uneven. Thus resorting
to a global goal should not be a permanent step as the agent traveling to a global goal is likely to
enter an area covered by the database sooner or later. At that point, the record selection process can
be repeated, hopefully resulting in a database hit. We implement this intuition in kNN LRTA* by
imposing a travel quota on LRTA* after the function selectSubgoals fails to find a reachable record.
The quota is computed as a heuristic distance between the agents current state and the global goal
multiplied by a fixed constant greater than 1. Once the agent exhausts its quota, selectSubgoals
is called again. If it fails to find a reachable subgoal for a second time in a row, the quota is
set to infinity leading to no further interruptions. This is necessary to guarantee completeness.
Additionally, interrupting LRTA* indefinitely many times increases average planning time per move
due to subgoal selection attempts.
We have also experimented with the idea that a database record of the form (s1 , . . . , sn ) does not
have to be used in its entirety. Indeed, any of its fragments (i.e., (si , . . . , sj ) where 1  i < j  n)
can be used within kNN LRTA* in the same fashion as the entire record. We implemented this
idea by running the kd-tree search over all fragments of database records in addition to the whole
records. The results were disappointing in several ways. First, the kd-tree algorithm becomes more
complex and the kd-tree query time increases. Second, record fragments crowd the M hits that
the kd-tree returns and for which the similarity metric is computed. In practice this means that the
kd-tree returns M similar but not hill-climbing unreachable records and, thus, causes kNN LRTA*
to resort to its global goal more often. This can be fixed by increasing M accordingly but then the
similarity computation and the hill-climbing checks become more costly.

7. Theoretical Analysis
In this section we prove completeness of the algorithm and analyze its complexity.
7.1 Off-line Complexity
Off-line kNN LRTA* generates N records in a space of S states. Let the diameter of the space (i.e.,
the number of states along the longest possible shortest path between two states) be S .
Theorem 1 Off-line worst-case space complexity of kNN LRTA* is O(N S + S).
283

fiB ULITKO , B J ORNSSON , & L AWRENCE

Proof. In the worst case each optimal path kNN LRTA* generates between randomly selected start
and goal is S long and is minimally compressible. Minimum compression means that every state
on the path is stored. If all N records have this property then the total amount of database storage is
O(N S ). Additionally, A* is run for each record and has the worst-case space complexity of S. 2

Theorem 2 Off-line worst-case time complexity of kNN LRTA* is O(N S log S + N log N ).
Proof. kNN LRTA* runs A* to compute an optimal path for N pairs of randomly generated start
and goal states. With a consistent heuristic and other constraints from our problem formulation,
A*s worst case time complexity is O(S log S). Since S  S, A*s complexity dominates the
worst-case time complexity of the function compress which is O(S log S ). Additionally, building
a kd-tree takes O(N log N ). 2
7.2 On-line Complexity
In this section we assume that LRTA* generates all immediate neighbors of its current state and only
them on each move. In our grid pathfinding this can be easily accomplished by setting gmax = 1.4.
More generally, this can be guaranteed by substituting line 4 in Figure 1 with generate immediate
successor states of s.
Theorem 3 kNN LRTA*s on-line worst-case space complexity is O(dmax + S) where dmax is the
maximum out-degree of any vertex and S is the total number of states.
Proof. The open list of kNN LRTA* is at most the maximum number of immediate neighbors of
any state (i.e., dmax ). As LRTA* learns, it has to store updated heuristic values, of which there are
no more than S. Hence the overall space complexity is O(dmax + S). Note that in grid pathfinding
dmax  S and dmax does not increase with map size, thereby reducing the upper bound to O(S). 2

Theorem 4 kNN LRTA*s per-move worst-case time complexity is O(dmax +N +M log M ) where
dmax is the maximum out-degree of any vertex, N is the total number of records in the database and
M is the number of candidate records selected by the kd-tree.
Proof. On each move kNN LRTA* invokes LRTA* which generates at most dmax states. On some
moves, kNN LRTA* additionally searches its database to find the appropriate record. The database
search starts with querying the kd-tree for M records (M  N ). While balanced kd-trees can have
time complexity sub-linear in N , the worst case time for this step is still O(N ). We then sort the
M records by their similarity in O(M log M ) time. Finally, move-limited hill-climbing checks are
run for the records, collectively taking no more than O(M ) time. Thus, the overall per-move time
complexity is O(dmax + N + M log M ) in the worst case. 2
Note that this bound does not depend on S and, therefore, makes kNN LRTA* real-time by our
definition. Also note that in grid pathfinding dmax  N and M  N which makes kNN LRTA*s
per move time complexity simply O(N ).
284

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

7.3 Completeness
Theorem 5 For any two states s1 and s2 , if s2 is hill-climbing reachable from s1 then an LRTA*
agent starting in s1 will reach s2 without state re-visitation (i.e., scrubbing).
Proof. First, we show that if the hill-climbing agent (as specified in the function reachable in
Figure 7) can reach s2 from s1 then it can never re-visit any states on its way. Suppose, that is
not the case. Then there exists a state s3 re-visited by the hill-climbing agent. Because our ties
are broken in a fixed order, once the hill-climbing agent arrives at s3 for the second time, it will
continue following the same path as it did after the first visit and will, therefore, arrive at s3 for the
third time and so on. In other words, it will be in an infinite loop re-visiting s3 repeatedly. This
contradicts the fact that it was able to reach s2 .
From here we conclude that the path between s1 and s2 followed by the hill-climbing agent is
free of repeated states. We now have to show that the LRTA* agent starting in s1 will follow exactly
the same path as the hill-climbing agent. Observe that the only difference between our hill-climbing
agent (Figure 7) and the LRTA* agent (Figure 1) is the heuristic update rule in line 6 of the latter
figure. The update rule can only increase heuristic values (i.e., make them less attractive to the
agent) and only in already visited states. Since the hill-climbing agent never re-visits its states while
traveling between s1 and s2 , any increase in the heuristic values caused by LRTA* does not affect
LRTA*s move choice (line 5 in Figure 1). As a result, LRTA* will follow precisely the same path
between s1 and s2 as the hill-climbing agent and thus will not re-visit any states. 2
Theorem 6 There exist two states s1 and s2 such that s2 is hill-climbing reachable from s1 but the
path that the hill-climbing agent follows is not optimal (i.e., shortest).
Proof. The proof is constructive and presented in Figure 10. The darkened cells are walls. The hillclimbing agent, starting in the state s1 will hug the wall on its way to the state s2 .The resulting
path has the cost of 16.4. The optimal path, however, takes advantage of diagonal moves by making
a non-greedy move and going around the wall above the agent. Its cost is 15. 2
Theorem 7 kNN LRTA* is complete for any size of its subgoal database if the underlying kNN
LRTA* generates at least all immediate neighbors of the current state.
Proof. To prove completeness we need to show that for any pair of states s1 and s2 , if there is a
path between s1 and s2 , kNN LRTA* will reach s2 from s1 .
Given a problem, the subgoal selection module of kNN LRTA* (Figure 8) will either return a
record of the form r = (sstart , . . . , send ) or instruct LRTA* to go to the global goal. In the latter
case, kNN LRTA* is complete because the underlying LRTA* is complete (Korf, 1990) as long as
it generates all immediate neighbors of its current state.
In the former case, LRTA* is guaranteed to reach either sstart or the first subgoal of r due
to the way r is selected. Once any of the states in r is reached, LRTA* is guaranteed to reach
the subsequent states due to the completeness of the basic LRTA* and the way the subgoals are
generated. Note that the interruptibility enhancement does not interfere with completeness because
we can interrupt going for a global goal at most once. 2

285

fiB ULITKO , B J ORNSSON , & L AWRENCE

s1

s2

Figure 10: Hill-climbing reachability does not guarantee optimality.

8. Empirical Evaluation
Pathfinding in video games is a challenging task, frequently requiring many units to plan their paths
simultaneously and to react promptly to user commands. The task is made even more challenging
by ever-growing map sizes and little computational resources allocated to in-game AI. Accordingly,
most recent work in the field of real-time heuristic search uses video game pathfinding as a testbed.
8.1 Test Problems
Maps modelled after game levels from Baldurs Gate (BioWare Corp., 1998) and WarCraft III:
Reign of Chaos (Blizzard Entertainment, 2002) have been a common choice (e.g., Sturtevant &
Buro, 2005; Bulitko et al., 2008). These maps, however, are small by todays standards and do not
represent the state of the industry. For this paper, we developed a new set of maps modelled after
game levels from Counter-Strike: Source (Valve Corporation, 2004), a popular on-line first-person
shooter. In this game the level geometry is specified in a vector format. We developed software to
convert it to a grid of an arbitrary resolution. While previous papers commonly used maps in the
range of 104 to 105 grid cells (e.g., between 150  141 and 512  512 cells in Sturtevant, 2007;
Bulitko & Bjornsson, 2009), our new maps have between nine and thirteen million vacant cells (i.e.,
states). This is a two to three orders of magnitude increase in size. As a point of reference, the
entire road network of Western Europe used for state-of-the-art route planning has approximately
eighteen million vertices (Geisberger, Sanders, Schultes, & Delling, 2008).
The experiments in this paper were run on a set of 1000 randomly generated problems across the
four maps shown in Figure 11. There were 250 problems on each map and they were constrained to
have solution cost of at least 1000. The grid dimensions varied between 4096  4604 and 7261 
4096 cells. For each problem we computed an optimal solution cost by running A*. The optimal
cost was in the range of [1003.8, 2999.8] with a mean of 1881.76, a median of 1855.2 and a standard
deviation of 549.74. We also measured the A* difficulty defined as the ratio of the number of states
expanded by A* to the number of edges in the resulting optimal path. For the 1000 problems, the
286

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

Figure 11: The maps used in our empirical evaluation.

A* difficulty was in the range of [1, 199.8] with a mean of 62.60, a median of 36.47 and a standard
deviation of 64.14.
All algorithms compared were implemented in Java using common data structures as much as
possible. We used Java version 6 under SUSE Enterprise Linux 10 on a 2.1 GHz AMD Opteron
processor with 32 Gbytes of RAM. All timings are reported for single-threaded computations.

8.2 Algorithms Evaluated
We evaluated kNN LRTA* with the following parameters. Database size values were in
{1000, 5000, 10000, 40000, 60000, 80000} records. On-line, we allowed our hill-climbing test to
climb for up to 250 steps before concluding that the destination state is not hill-climbing reachable.
This value was picked after some experimentation and had to be appropriate for the record density
on the map. Indeed, a larger database requires fewer hill-climbing steps to maintain the likelihood
of finding a hill-climbing reachable record for a given problem.
287

fiB ULITKO , B J ORNSSON , & L AWRENCE

We ran reachability checks on the 10 most similar records.4 Whenever selectSubgoals failed
to find a matching record, we allowed LRTA* to travel towards its global goal up to 3 times the
heuristic estimate of the remaining path. After that, LRTA* was interrupted and the second attempt
to find an appropriate subgoal was run. LRTA*s parameter gmax was set to the cost of the most
expensive edge (i.e., 1.4) so that LRTA* generated only all immediate neighbors of its current state.
We also ran two recent high-performance real-time search algorithms to compare kNN LRTA*
against: D LRTA* and TBA*. D LRTA* was run with the databases computed for abstraction levels
of {9, 10, 11, 12}. TBA* was run with the time slices of {5, 10, 20, 50, 100, 500, 1000, 2000, 5000}.
The cost ratio of expanding a state to backtracing was set to 10.
We chose the space of control parameters via trial and error, with three considerations in mind.
First, we had to cover enough of the space to clearly determine the relationship between control
parameters and algorithms performance. Second, we attempted to establish the pareto-optimal
frontier (i.e., determine which algorithms dominate others by simultaneously outperforming them
along two performance measures such as time per move and suboptimality). Third, parameter values
had to be such that we could run the algorithms in a practical amount of time (e.g., building a
database for D LRTA*(8) would have taken us over 800 hours which is not practical). We detail our
observations with respect to all three considerations below.
8.3 Solution Suboptimality and Per-Move Planning Time
We begin the comparisons by looking at average solution suboptimality versus average time per
move. The left plot in Figure 12 shows the overall picture by plotting all algorithms and parameters.
The right plot zooms in on a high-performance area. Table 1 shows the individual values. kNN
LRTA* produces the highest quality solutions, followed by TBA*.
D LRTA* with its mean suboptimality of 819.72% delivers paths which are about 9 times
costlier than optimal paths. Such suboptimality is impractical in pathfinding and we included D
LRTA*(9) in the right subplot of Figure 12 only to illustrate the substantial gap in solution quality
between D LRTA* and kNN LRTA*. Optimality of D LRTA* solutions can be improved by lowering the abstraction level but the database pre-computation increases rapidly as we discuss below.
TBA* produces solutions substantially less costly than D LRTA* but cannot reach kNN LRTA*
with the database size of 60 and 80 thousand records. Additionally, TBA* is noticeably slower
per move as it expands more than one state and allocates some time to backtracking as well. The
time per move can be decreased by lowering the value of cutoff but already with the cutoff of 10,
TBA* produces unacceptably suboptimal solutions (666.5% suboptimal). As a result, kNN LRTA*
dominates TBA* by outperforming it with respect to both measures. This is intuitive as TBA* does
not benefit from subgoal precomputation.
On the other hand, D LRTA* stands non-dominated due to its low time per move. This is
also intuitive as it does not have to scan the database for most similar records and then check hillclimbing reachability for them. The differences between D LRTA* and kNN LRTA* are, however,
below 4 microseconds per move.
For the sake of reference, we also included A* results in the table. A* is not a real-time algorithm and its average time per move tends to increase with the number of states in the map. Also,
4. We have also experimented with querying the kd-tree for 100 most similar records and found a very minor improvement of suboptimality together with a significant increase in the mean time per move. This is because frequently a
hill-climbing-reachable database record will be among the top 10 candidates and thus the extra time spent querying
the kd-tree for 90 more records and then sorting them is wasted.

288

fi0

0

D LRTA* (9)
kNN LRTA* (60000)
2
kNN LRTA* (80000)
200
C ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH TBA* (50)
0
TBA* (100)
50
100
150
50
100
150
00
Mean precomputation time (hours per map)
0 Mean online
50memory (Kbytes
100per problem) 150
Mean precomputation time (hours per map)

Mean suboptimality (%)

10

Meanmemory
suboptimality
(%)per problem)
Mean online
(Kbytes

4

x 10

D LRTA*
kNN LRTA*
TBA*

8
6
4
2
0

0

50
100
150
200
250
Mean time per move (seconds)

300

150
1000
D LRTA* (9)
kNN LRTA* (60000)
kNN LRTA* (80000)
TBA* (50)
TBA* (100)

800
100
600
400
50

D LRTA*
kNN LRTA*
TBA*

200

0
00
0

50
100
150
200
250
50
100
Mean
per move
move (seconds)
(seconds)
Mean time
time per

2010/1/8, 16:29:14 : Java results for mm_cs_4_1000 scenario

300
150

Mean time per move (microseconds)
7.56
6.88
6.40
6.55
3.73
3.93
4.26
3.94
14.31
26.34
83.31
117.52
208.03

Solution suboptimality (%)
6851.62
620.63
12.77
11.96
15999.23
8497.09
6831.74
819.72
1504.54
666.50
131.12
64.66
0

Table 1: Suboptimality versus time per move.
it spends most of it time during the first move when it computes the entire path. Subsequent moves
require a trivial computation. In the table, we define A*s mean time per move as the total planning
time for a problem divided by the number of moves in the path A* finds. We average this quantity
over all problems. kNN LRTA* is about 30 times faster than A* per move.
Note that kNN LRTA*s time per move decreases with larger databases. This is intuitive as with
more database records there is a higher probability that an earlier record on the short list of records
for whom the reachability checks are run will pass the checks (line 4 in Figure 8). Consequently,
no further time-consuming reachability checks will be administered in the function selectSubgoals,
saving time per move. These time savings, resulting from a larger database, outweigh the extra time
spent traversing a correspondingly larger kd-tree to form the short list of most similar records. This
fact indicates that the kd-tree approach scales well with the database size.
289

400

D
kN
kN
T
T

200
0

0

Mean on

150

100

50

0

0

Mea

2010/1/8, 16:39:16 : Java results for mm_cs_4_1000 sc

Figure 12: Suboptimality vs. time per move: all algorithms (left), high-performance region (right).
Algorithm
kNN LRTA*(10000)
kNN LRTA*(40000)
kNN LRTA*(60000)
kNN LRTA*(80000)
D LRTA*(12)
D LRTA*(11)
D LRTA*(10)
D LRTA*(9)
TBA*(5)
TBA*(10)
TBA*(50)
TBA*(100)
A*

Mean subo

2

4
400

Mean online memory (Kbytes per problem)

Mean subo
Mean s

Mean s

4

fi2

B ULITKO , B J ORNSSON , & L AWRENCE
0.5
1
Mean relative database size (per map)

0
00
0

1.5

4

10
1000

D LRTA*
kNN LRTA*

8

4

D LRTA* (9)
kNN LRTA* (40000)
kNN LRTA* (60000)
D LRTA*
kNN LRTA* (80000)
kNN LRTA*

2
200

50
100
Mean precomputation time (hours per map)

0

150

0

10
Mean d

Mean suboptimality (%)
Mean online memory (Kbytes per problem)

kNN LRTA*

800

kNN LRTA* (40000)
kNN LRTA* (60000)
Solution suboptimality
(%)
kNN LRTA* (80000)
D LRTA*
6851.62
kNN LRTA*

Algorithm
Pre-computation time per map (hours)
10
600
kNN
13.10
6 LRTA*(10000)
kNN LRTA*(40000)
51.89
620.63
400
kNN
LRTA*(60000)
77.30
12.77
4
5
103.09
11.96
kNN LRTA*(80000)
200
0.25
15999.23
2D LRTA*(12)
D LRTA*(11)
1.57
8497.09
0
0
2
4
11.95
6831.746
0D LRTA*(10)
0
0
2
4
6
8
10
0
2
4 move (seconds)
6
8
Mean
time per
D LRTA*(9)
89.88
819.72
Mean time per move
(seconds)
Mean time per move (seconds)

8

10

800
600
400

D
kN
kN
kN

200
0

505
100
15015
10
(hours
perproblem)
map)
Mean precomputation
online memorytime
(Kbytes
per

4
Figurex 10
13:
Suboptimality versus database pre-computation time per map. Left: all pre-computing
1000
10
15
algorithms. Right: a high-performance
subplot.
D LRTA* (9)
D LRTA*

Mean suboptimality (%)

0

1000

4
400

2

8

0

x 10

6
600

0

200

60
1.5

8
800

6

0

D
kN
kN
kN

4

x 10

Mean suboptimality (%)
Mean suboptimality (%)

Mean suboptimality (%)

10

10
20
30
40
50
0.5
1 per map)
Mean database
size (MBytes
Mean relative database size (per map)

400

Mean suboptimality (%)

0

D LRTA* (9)
kNN LRTA* (40000)
kNN LRTA* (60000)
kNN LRTA* (80000)

0

Mean on

Mean online memory (Kbytes per problem)

0

2
200

Mean sub

4
400

Mean sub
Mean s

Mean s

4

1.5

1

D
kN
kN
kN

0.5

0

0

2
Mea

2010/1/8, 17:34:36 : Java results for mm_cs_4_10

2010/1/8, 17:41:03 : Java results for mm_cs_4_1000 scenario

Table 2: Suboptimality versus database pre-computation time.
8.4 Database Pre-computation Time
Suboptimality versus database pre-computation time is shown in Figure 13. The left subplot demonstrates all parametrizations of D LRTA* and kNN LRTA* while the right plot focuses on better
performing configurations. Table 2 shows the individual values.
kNN LRTA* has three advantages over D LRTA*. First, kNN LRTA* with 40 and 60 thousand records easily dominates D LRTA*(9): it has better suboptimality while requiring less precomputation time. kNN LRTA*(80000) is an overkill for these maps as it does not improve suboptimality by much (11.96% versus 12.77% achievable with 60000 records) while having the longest
precomputation time.
Second, the database computation can be parallelized more easily in the case of kNN LRTA* as
the individual records are completely independent of each other. This is not the case with D LRTA*.
Additionally, D LRTA* requires building a map abstraction which is more complex to do in parallel.
Third, the number of records in the kNN LRTA* database can be controlled much more easily
than in that of D LRTA*. Specifically, in D LRTA* one controls the level of abstraction. The
number Sa of abstract states at abstraction level a is approximately Sa where S is the number of
290

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

original non-abstract states and  is a constant reduction factor (Bulitko et al., 2007). The number
Na of records in D LRTA* database is Sa (Sa  1). Thus, the ratio between Na and Na1 is:
Na1
Na

=

Sa1 (Sa1  1)
=
Sa (Sa  1)

S
a1
S
a

S
1
a1 
S
a  1


=

S  a1 2
 = (2 ).
S  a

Thus by decreasing the level of abstraction by one, D LRTA* database size grows at least
quadratically in . On our maps, clique abstraction has  of approximately 3 which means that
there is nearly an order of magnitude in database size (and pre-computation time) when we go down
by one level of abstraction. To illustrate, building a database for D LRTA*(8) is estimated to take
over 800 CPU-hours. On the other hand, the number of records in the kNN LRTA* database is a
user-specified parameter, affording a much greater control.
Of a particular interest is the pair kNN LRTA* with a database of 10000 and D LRTA* with
abstraction level 10 as they perform closely in both measures. We discuss differences in their
database sizes in the next section.
8.5 Database Size
Memory is at premium in video games, especially on consoles. TBA* space complexity comes
from its open and closed list which it builds on-line. kNN LRTA* and D LRTA* expand only a
single state (the agents current state) and thus have the closed list of one state and the open list of
at most eight states (as any grid cell in our maps has at most eight neighbors). However, these two
algorithms consume memory as they store updated heuristic values. Additionally, they store their
subgoal databases. In this section we focus on the database size. The next section will cover the
total memory consumed on-line: open and closed lists as well as the updated heuristic values.
Each D LRTA* database record stores exactly three states. kNN LRTA* records have two or
more states each and the number of records is fixed by the algorithm parameter. Additionally, kNN
LRTA* stores start and end states of each record in a kd-tree. We define relative database size as
the ratio of the total number of states stored in all records to the total number of map grid cells.
In addition to subgoal records, D LRTA* databases contain explicit region assignment for each
state. Consequently, D LRTA* databases have a relative size of at least 1. This extra storage is a
major weakness of D LRTA* in comparison to kNN LRTA*. To illustrate, as in our implementation
we use 32 bits to index states, storing region assignment for each grid cell translates to an average
of about 84 megabytes per map. Full results are found in Figure 14 and Table 3.
Algorithm
kNN LRTA*(10000)
kNN LRTA*(40000)
kNN LRTA*(60000)
kNN LRTA*(80000)
D LRTA*(12)
D LRTA*(11)
D LRTA*(10)
D LRTA*(9)

Pre-computation time
13.10
51.89
77.30
103.09
0.25
1.57
11.95
89.88

Records
10000
40000
60000
80000
251.5
1896.5
14872.0
116048.5

Relative size
0.00308
0.01234
0.01851
0.02468
1.00001
1.00009
1.00068
1.00532

Size (megabytes)
0.25
1.00
1.51
2.01
84.96
84.97
85.02
85.40

Table 3: Database statistics. All values are averages per map. Pre-computation time is in hours.
291

fiB ULITKO , B J ORNSSON , & L AWRENCE

10
1000

D LRTA*
kNN LRTA*

8
800

6

D LRTA*
kNN LRTA*

6
600

4

4
400

2
0

1000

Mean suboptimality (%)

8

x 10

Mean suboptimality (%)
Mean suboptimality (%)

Mean suboptimality (%)

4

x 10

2
200

0

0.5
1
Mean relative database size (per map)

00
0

1.5

D LRTA* (9)
kNN LRTA* (60000)
kNN LRTA* (80000)

20
60 1
80
0.5 40
Mean relative
database
size (MBytes
permap)
map)
Mean
database
size (per

1000
10

D LRTA* (9)
D LRTA*
by requiring much less memory and,kNN
at the
same
time,
LRTA*
(60000)
kNN
LRTA*
8
LRTA* (80000) 57
instance, kNN800
LRTA*(60000)
requires kNN
approximately

suboptimality
(%)
Mean onlineMean
memory
(Kbytes per
problem)

Mean suboptimality (%)

0

2

4

6

8

10

Mean time
per move (seconds)
8.6 On-line Space
Complexity

0

0

20
Mean d

0

2
Mean cum

800
600
400
200
0

3

5

x 10

4
3
2
1

D
kN
kN

0
0
2
Mean time per move (seconds)
Mea
2
4
6
8
10
Mean time per move (seconds)
2010/1/11, 17:09:07 : Java results for mm_cs_4_

We will first analyze specifically
the
amount
of results
memory
allocated byscenario
the algorithms on-line. When
2010/1/11,
17:11:03
: Java
for mm_cs_4_1000
an algorithm solves a particular problem, we record the maximum size of its open and closed lists
as well as the total number of states whose heuristic values were updated. We count each updated
heuristic value as one state in terms of storage required.5 Adding these three measures together, we
5. Multiple heuristic updates in the same state do not increase the amount of storage.

292

D
kN
kN

200

1000

Mean suboptimality (%)

Mean suboptimality (%)

Mean suboptimality (%)
Mean suboptimality (%)

Again, kNN LRTA* dominates
8
producing
solutions of better quality. For
times less database memory than D LRTA*(9) while simultaneously producing solutions that are
600
6
6
about
eight times better.
Let us now re-visit the interesting case of kNN LRTA*(10000)
and D LRTA*(10) which closely
400
4
4
match each other with respect to database pre-computation time and solution suboptimality. As Table 3 reveals, kNN LRTA*(10000) uses approximately 340
200 times less memory than D LRTA*(10):
2
2
about 256 kilobytes versus 85.02 megabytes.
0
Note
that D LRTA*(10) averages approximately 49%
more records per map than kNN
0
00
20
40
60
80
100
120
0
100
150 less pre-computation
0 Mean 2precomputation
4 This
6is because
10
LRTA*(10000)
but50requires approximately
9%
time.
time (hours
per 8
map)(i) D
Mean precomputation time (hours per map)
Mean cumulative online memory (Kbytes)x 104
LRTA* averages fewer subgoals per record than kNN LRTA* and (ii) computing D LRTA* subgoals does
not require reachability checks  a time-consuming process.
4
1000
x 10
Also note that despite having 49% more records, D 0.06
LRTA* affords only a 0.3%
improvement
in
10
D LRTA*
(9)
LRTA*generally, additional experiments have demonstrated
kNN LRTA* (60000)
solution quality over kNN LRTA*. DMore
that
800
kNN LRTA*
kNN LRTA* (80000)
kNN8 LRTA* tends to outperform D LRTA* in solution0.05
quality given the same number of records.
There are two factors in play here. First, kNN LRTA*0.04
records often contain
several subgoals that
600
D LRTA*
6
are guaranteed
to be reachable from each other without scrubbing. D LRTA*
records offers no such
kNN LRTA*
0.03
guarantees and their only subgoal may be difficult to reach
400 from the agents start state when abstract
4
regions
become large and complex. On the upside, D LRTA* spaces its records in a systematic
0.02
fashion  one record per each pair of regions  thereby200
providing a potentially better coverage than
2
can be afforded by randomly selected starts and ends of
kNN LRTA* database records. It appears
0.01
that the former factor overcomes the latter, leading to kNN0LRTA*s
better
per-record
suboptimality.
0
2
4
6
8
0

400

0

D LRTA*
D
LRTA*
kNN
LRTA*

0

600

100
1.5

4
4
Figurex 10
14:
Suboptimality vs. database size: all algorithms x(left),
high-performance region (right).
10

10

800

Mean online memory (Kbytes per problem)

4

10

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

Algorithm
kNN LRTA*(10000)
kNN LRTA*(40000)
kNN LRTA*(60000)
kNN LRTA*(80000)
D LRTA*(12)
D LRTA*(11)
D LRTA*(10)
D LRTA*(9)
TBA*(5)
TBA*(10)
TBA*(50)
TBA*(100)
A*

Strictly on-line memory (Kbytes)
8.62
5.04
4.23
4.22
18.76
11.09
8.24
3.04
1353.94
1353.94
1353.94
1353.94
1353.94

Solution suboptimality (%)
6851.62
620.63
12.77
11.96
15999.23
8497.09
6831.74
819.72
1504.54
666.50
83.31
64.66
0

Table 4: Strictly on-line memory versus solution suboptimality.

record the amount of strictly on-line memory per problem. Averaging the strictly on-line memory
over all problems, we list the results in Table 4.
kNN LRTA* dominates all D LRTA* points except D LRTA*(9) which has the lowest mean
strictly on-line memory of 3.04 Kbytes per problem. TBA*, being effectively a time-sliced A*,
does not update heuristic values at all. However, its open and closed lists contribute to the highest
memory consumption at 1353.94 Kbytes. This is intuitive as TBA* does not use subgoals and therefore must fill in potentially large heuristic depressions with its open and closed lists. Also, notice
that the total size of these lists does not change with the cutoff as state expansions are independent
of agents moves in TBA*. A* has identical memory consumption as it expands states in the same
way as TBA*. Again, kNN LRTA* dominates TBA* for all cutoff values, using less memory and
producing better solutions.
Strictly on-line memory gives an insight into the algorithms but does not present a complete picture. Specifically, D LRTA* and kNN LRTA* must load their databases into their on-line memory.
Thus we define the cumulative on-line memory as the strictly on-line memory plus the size of the
database loaded. The values are found in Figure 15 and Table 5.
Several observations are due. First, TBA* is no longer dominated due to its low memory consumption. Second, D LRTA* is in its own league due to explicitly labelling every state with a
corresponding region as well as computing subgoals for all pairs of regions. Third, D LRTA* has a
sweet spot in its memory consumption which corresponds to the abstraction level of 11. A higher
level of abstraction reduces the database size but not enough to compensate for more updated heuristic values. Lower abstraction levels reduce the amount of learning but not enough to compensate
for large number of subgoals in the database.
293

fi50
2

1.5

20
40
60
80
Mean
(MBytes
map)
0.005 database
0.01 size
0.015
0.02per 0.025
Mean relative database size (per map)

100
0.03

kNN LRTA* (60000)
kNN LRTA* (80000)
TBA* (50)
L AWRENCETBA* (100)

50

B ULITKO , B J ORNSSON , &

0
0
0
0

(per map)

Mean subo

Mean
Mean
subos

1

D LRTA*
kNN LRTA*
TBA*

4

0

0

0.5
1
1.5
2
2.5
Mean database size (MBytes per map)

3

4

Mean suboptimality (%)

100
6

150

D LRTA*
kNN LRTA*
kNN(60000)
LRTA*
kNN LRTA*
(80000)
TBA*
TBA* (50)
TBA* (100)

4
50

2

100

50

0

2
6 100
8
10
50 4
150
4
Mean
online
(Kbytes)
Mean cumulative
precomputation
timememory
(hours per
map)x 10

kNN LRTA* (60000)
kNN LRTA* (80000)
TBA* (50)
TBA* (100)
0

500
1000
1500
2000
2500
Mean cumulative online memory (Kbytes)

Figure
15: Suboptimality versus cumulative on-line memory. Left: all algorithms. Right: a high1.5
150
1.5
performance subplot.
kNN LRTA* (60000)
Mean online memory (Kbytes per problem)

D LRTA*
kNN LRTA*
TBA*

8

00
00

150

Mean online
memory
(Kbytes(%)
per problem)
Mean
suboptimality

00
urs per map)

Meansuboptimality
suboptimality
(%)
Mean
(%)

D LRTA*
kNN LRTA*
TBA*

x 10
10
150

kNN LRTA* (80000)
TBA* (50)
Cumulative
TBA* (100) on-line

Algorithm
memory (Kbytes) Solution suboptimality (%)
1
kNN LRTA*(10000)
265.65
6851.62
kNN LRTA*(40000)
1034.08
620.63
kNN LRTA*(60000)
1547.85
12.77
D LRTA*
50
0.5
0.5
kNN LRTA*
kNN LRTA* (60000)
kNN
LRTA*(80000)
2062.20
11.96
TBA*
kNN LRTA* (80000)
D LRTA*(12)
87019.74
15999.23
TBA* (50)
D LRTA*(11)
87018.50
TBA* (100) 8497.09
0
0
0D
87066.34
6831.74
0 LRTA*(10)
50
100
150
0
50
100
150
00
250
300
0
50
100
150
200
250
300
Mean
time
per
move
(seconds)
Mean
time
per
move
(seconds)
D LRTA*(9)
819.72
Mean time per move (seconds) 87456.35
econds)
TBA*(5)
1353.94
1504.54
2010/1/13, 21:37:18 : Java results for mm_cs_4_1000 scenario
3, 21:30:33 : Java results for mm_cs_4_1000
scenario
TBA*(10)
1353.94
666.50
TBA*(50)
1353.94
83.31
TBA*(100)
1353.94
64.66
A*
1353.94
0
100
1

Table 5: Solution suboptimality versus cumulative on-line memory.
8.7 Simultaneous Pathfinding by Multiple Agents
If only a single agent is pathfinding at a time, the analysis above holds and TBA* is the most
memory efficient choice. However, in most video games, anywhere from half a dozen to a thousand
agents (e.g., Gas Powered Games, 2007) can be pathfinding simultaneously on the same map.
Such a scenario favors kNN LRTA* and D LRTA* whose subgoal databases are map-specific
but independent of start and goal states. Consequently, multiple agents running D LRTA* and kNN
LRTA* can all share the same subgoal database.6 In contrast, all memory consumed by TBA* is
specific to a given agent and cannot be shared with other agents operating on the same map.
6. Note that such multiple agents cannot, generally speaking, share their heuristic h as it is computed and updated with
respect to different goals.

294

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

Hence the total amount of cumulative on-line memory for K agents operating simultaneously
equals the amount of the database memory plus K times the amount of strictly on-line memory. A
break-even point for an algorithm A with respect to an algorithm B is then defined as the minimal
number of agents using A that collectively consume less memory than the same number of agents
using B. Table 6 lists the break-even points for D LRTA* and kNN LRTA* with respect to TBA*.
Algorithm
kNN LRTA*(10000)
kNN LRTA*(40000)
kNN LRTA*(60000)
kNN LRTA*(80000)
D LRTA*(9)
D LRTA*(10)
D LRTA*(11)
D LRTA*(12)

Break-even point with respect to TBA* (number of agents)
1
1
2
2
65
65
66
66

Table 6: Break-even points for kNN LRTA* and D LRTA* with respect to TBA*.
kNN LRTA* with ten and forty thousand record databases requires less cumulative on-line
memory than TBA* and hence the break-even point is just one agent. For sixty and eighty thousand
records, two kNN LRTA* agents take less total cumulative on-line memory than two TBA* agents.
It takes 65 to 66 simultaneously pathfinding agents to amortize the large D LRTA* databases and
gain a memory advantage over TBA*.

9. Discussion
This is the first time the high-performance real-time search algorithms TBA*, D LRTA* and kNN
LRTA* were evaluated on contemporarily sized maps. The results, presented in detail in the previous section, are summarized for representative algorithms in Table 7.
Dimension
Suboptimality
Time per move
Cumulative memory
Break-even point
Pre-computation time

kNN LRTA* versus other algorithms
kNN LRTA* is 8.16 times better than D LRTA* and 1.46 times better than TBA*
kNN LRTA* is 18.36 times better than TBA* and 62% worse than D LRTA*
kNN LRTA* is 57 times better than D LRTA* and 13% times worse than TBA*
kNN LRTA* takes less memory than TBA* with two or more agents
kNN LRTA* is 14% better than D LRTA*

Table 7: Comparisons of kNN LRTA*(60000) to D LRTA*(9) and TBA*(100).
kNN LRTA* achieves the best suboptimality of the three algorithms. kNN LRTA* is substantially faster per move than TBA* and is on par with D LRTA*. In terms of cumulative on-line
memory, kNN LRTA* outperforms D LRTA* by two orders of magnitude and is only 13% worse
than TBA*. Furthermore, with two or more simultaneously planning agents, kNN LRTA* takes less
memory than TBA*. In contrast, it takes 65 or more D LRTA* agents to amortize its database and
gain a memory advantage over TBA*. Off-line, kNN LRTA* outperforms D LRTA* by achieving
295

fiB ULITKO , B J ORNSSON , & L AWRENCE

an order of magnitude better solutions with a database two orders of magnitude smaller in size and
slightly faster to compute.
While the results of comparisons to TBA* were expected as TBA* does not benefit from precomputation, the comparison between kNN LRTA* and D LRTA* unearthed unexpected results.
Specifically, subgoal databases of kNN LRTA* are a more effective use of pre-computation time
and memory than those of D LRTA*. The lower memory consumption with kNN LRTA* databases
is achieved by not having to store explicit region membership. Better pre-computation times come
from not having to compute shortest paths for all pairs of abstract times. Finally, kNN LRTA* is
better than D LRTA* on a per record basis. This is because compressing an entire optimal path
into a series of subgoals reachable from each other via hill-climbing guarantees that once a single
subgoal is reached, the underlying LRTA* agent will reach its global goal without scrubbing. In
contrast, D LRTA* subgoals can be difficult to reach from the agents current position. And even
when reached, the difficulties can recur with the subsequent subgoals.
In terms of applications, kNN LRTA* can be the algorithm of choice to use in video game
pathfinding. For instance, kNN LRTA*(60000) is over 30 times faster per average move than commonly used A* and produces solutions which are less than 15% suboptimal. This performance
comes at the cost of about 77 hours of pre-computation time per map which can be easily reduced
to under 10 hours on a modern eight-core workstation. This is negligible comparing to the amount
of time a game company spends hand-crafting a single map.

10. Current Shortcomings and Future Work
Despite outperforming existing state-of-the-art real-time search algorithms on our problems overall,
kNN LRTA* has several shortcomings. First, the database records are generated off randomly
selected start and end states. This means that the coverage of the space is not necessarily even:
some small, but difficult to reach, regions of the space may never get a suitable record while some
easy to reach regions may get multiple redundant records covering it.
Increasing database efficiency would allow a smaller database to afford an equal coverage and
hence an equal on-line performance. This will in turn reduce the pre-computation time of kNN
LRTA* database which can presently reach over 100 hours per map. While the computation can
be sped up at a nearly linear scale by using multi-core processors and the time is affordable on the
game company side, most players would want their home-made game maps to be processed in a
matter of seconds or minutes.
Making the subgoal coverage more uniform can be accomplished via forgoing random start and
end selection in favor of space partitioning. However, unlike D LRTA*s abstract regions built via
repeated applications of the clique abstraction, such partitions should have all their states reachable
from each other by LRTA* without scrubbing. Then start and end states for the database records can
be selected within these partitions. To reduce the amount of pre-computation one can then compute
subgoals by compressing optimal paths only between neighboring regions (as opposed to all distinct
abstract regions as D LRTA* does). Note that unlike D LRTA*, the partitioning is necessarily only
off-line and no explicit region assignment is stored for every state. As a result, the on-line memory
consumption will be comparable or better than the existing kNN LRTA*.
A philosophically oriented project would be to develop a self-aware agent. Specifically, such
an agent would analyze the performance of its core algorithm (e.g., LRTA*) and decide on the
296

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

appropriate partitioning scheme. A similar meta-level control has been previously attempted for
dynamic selection of lookahead depth in real-time search (e.g., Russell & Wefald, 1991).

11. Beyond Grid Pathfinding
We presented and evaluated kNN LRTA* for grid-based pathfinding. Formally, the algorithm, with
the exception of its kd-tree module, is applicable to arbitrary weighted graphs that satisfy the constraints at the beginning of Section 2. In principle, it should be applicable to general planning
using the ideas from search-based planners ASP (Bonet, Loerincs, & Geffner, 1997), the HSPfamily (Bonet & Geffner, 2001), FF (Hoffmann, 2000), SHERPA (Koenig, Furcy, & Bauer, 2002)
and LDFS (Bonet & Geffner, 2006).
As we described earlier in the paper, using the kd-tree index requires a certain correspondence
between coordinate similarity and heuristic distance. Extending kd-trees or developing appropriate
new index structures for an arbitrary graph is an open research question. An interim solution is to
apply kNN LRTA* to arbitrary search problems without its kd-tree module. Doing so will, however,
slow down the on-line part because similarity must be computed between agents current situation
and every single record in the database. On the positive side, not computing kd-trees will speed up
the off-line part of kNN LRTA*.
Finally, while kNN LRTA* is theoretically applicable to arbitrary search problems, it is not
clear how well it will perform there with respect to its competitors D LRTA* and TBA*. Such an
investigation is left for future work.

12. Conclusions
In this paper we considered the problem of real-time heuristic search whose planning time per move
does not depend on the number of states. We proposed a new mechanism for selecting subgoals
automatically. The resulting algorithm was shown to be theoretically complete and, on large video
game maps, substantially outperformed the previous state-of-the-art algorithms D LRTA* and TBA*
along several important performance measures.

Acknowledgments
This research was supported by grants from the National Science and Engineering Research Council
of Canada (NSERC); Icelandic Centre for Research (RANNIS); and by a Marie Curie Fellowship
of the European Community programme Structuring the ERA under contract number MIRG-CT2005-017284. We appreciate help of Josh Sterling, Stephen Hladky and Daniel Huntley.

References
Anwar, M. A., & Yoshida, T. (2001). Integrating OO road network database, cases and knowledge
for route finding. In ACM Symposium on Applied Computing (SAC), pp. 215219. ACM.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1), 81138.
BioWare Corp. (1998). Baldurs Gate., Published by Interplay, http://www.bioware.com/bgate/,
November 30, 1998.
297

fiB ULITKO , B J ORNSSON , & L AWRENCE

Bjornsson, Y., Bulitko, V., & Sturtevant, N. (2009). TBA*: Time-bounded A*. In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI), pp. 431  436, Pasadena,
California. AAAI Press.
Bjornsson, Y., & Halldorsson, K. (2006). Improved heuristics for optimal path-finding on game
maps. In Laird, J. E., & Schaeffer, J. (Eds.), Proceedings of the Second Artificial Intelligence
and Interactive Digital Entertainment Conference (AIIDE), June 20-23, 2006, Marina del
Rey, California, pp. 914. The AAAI Press.
Blizzard Entertainment (2002). Warcraft III: Reign of chaos., Published by Blizzard Entertainment,
http://www.blizzard.com/war3, July 3, 2002.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: A unified approach to heuristic search
in deterministic and non-deterministic settings, and its application to MDPs. In Proceedings
of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 142
151, Cumbria, UK.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A fast and robust action selection mechanism for
planning. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp.
714719, Providence, Rhode Island. AAAI Press / MIT Press.
Branting, K., & Aha, D. W. (1995). Stratified case-based reasoning: Reusing hierarchical problem
solving episodes. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 384390.
Bulitko, V. (2004).
Learning for adaptive real-time search.
Tech.
http://arxiv.org/abs/cs.AI/0407016, Computer Science Research Repository (CoRR).

rep.

Bulitko, V., & Bjornsson, Y. (2009). kNN LRTA*: Simple subgoaling for real-time search. In
Proceedings of Artificial Intelligence and Interactive Digital Entertainment (AIIDE), pp. 27,
Stanford, California. AAAI Press.
Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic Control in Path-Planning with Real-Time Heuristic Search. In Proceedings of the International
Conference on Automated Planning and Scheduling (ICAPS), pp. 4956, Providence, RI.
Bulitko, V., & Lee, G. (2006). Learning in real time search: A unifying framework. Journal of
Artificial Intelligence Research (JAIR), 25, 119157.
Bulitko, V., Lustrek, M., Schaeffer, J., Bjornsson, Y., & Sigmundarson, S. (2008). Dynamic control
in real-time heuristic search. Journal of Artificial Intelligence Research (JAIR), 32, 419  452.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph abstraction in real-time heuristic search.
Journal of Artificial Intelligence Research (JAIR), 30, 51100.
Carbonell, J. G., Knoblock, C., & Minton, S. (1990). Prodigy: An integrated architecture for planning and learning. In Lehn, K. V. (Ed.), Architectures for Intelligence. Lawrence Erlbaum
Associates.
Cazenave, T. (2006). Optimizations of data structures, heuristics and algorithms for path-finding
on maps. In Louis, S. J., & Kendall, G. (Eds.), Proceedings of the 2006 IEEE Symposium
298

fiC ASE -BASED S UBGOALING IN R EAL -T IME H EURISTIC S EARCH

on Computational Intelligence and Games (CIG06), University of Nevada, Reno, campus in
Reno/Lake Tahoe, 22-24 May, 2006, pp. 2733. IEEE.
Culberson, J., & Schaeffer, J. (1998). Pattern Databases. Computational Intelligence, 14(3), 318
334.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In Proceedings
of the National Conference on Artificial Intelligence (AAAI), pp. 891897.
Gas

Powered Games (2007).
Supreme Commander.,
http://www.supremecommander.com/, February 20, 2007.

Published

by

THQ,

Geisberger, R., Sanders, P., Schultes, D., & Delling, D. (2008). Contraction hierarchies: Faster and
simpler hierarchical routing in road networks. In McGeoch, C. C. (Ed.), WEA, Vol. 5038 of
Lecture Notes in Computer Science, pp. 319333. Springer.
Haigh, K., & Veloso, M. (1993). Combining search and analogical reasoning in path planning from
road maps. In Proceedings of the AAAI-93 Workshop on Case-Based Reasoning, pp. 7985,
Washington, DC. AAAI. AAAI Press technical report WS-93-01.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2), 100107.
Hernandez, C., & Meseguer, P. (2005a). Improving convergence of LRTA*(k). In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI), Workshop on Planning
and Learning in A Priori Unknown or Dynamic Domains, pp. 6975, Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2005b). LRTA*(k). In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), pp. 12381243, Edinburgh, UK.
Hodal, J., & Dvorak, J. (2008). Using case-based reasoning for mobile robot path planning. Engineering Mechanics, 15, 181191.
Hoffmann, J. (2000). A heuristic for domain independent planning and its use in an enforced hillclimbing algorithm. In Proceedings of the 12th International Symposium on Methodologies
for Intelligent Systems (ISMIS), pp. 216227.
Ishida, T. (1992). Moving target search with intelligence. In National Conference on Artificial
Intelligence (AAAI), pp. 525532.
Koenig, S. (2004). A comparison of fast search methods for real-time situated agents. In Proceedings of Int. Joint Conf. on Autonomous Agents and Multiagent Systems, pp. 864  871.
Koenig, S., Furcy, D., & Bauer, C. (2002). Heuristic search-based replanning. In Proceedings of the
Int. Conference on Artificial Intelligence Planning and Scheduling, pp. 294301.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. In Proceedings of the International
Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 281288.
Korf, R. (1985). Depth-first iterative deepening: An optimal admissible tree search. Artificial Intelligence, 27(3), 97109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(23), 189211.
Likhachev, M., Ferguson, D. I., Gordon, G. J., Stentz, A., & Thrun, S. (2005). Anytime dynamic
A*: An anytime, replanning algorithm. In ICAPS, pp. 262271.
299

fiB ULITKO , B J ORNSSON , & L AWRENCE

Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* with provable bounds on
sub-optimality. In Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology in real-time path-finding. In Proceedings of
the National Conference on Artificial Intelligence (AAAI), Workshop on Learning For Search,
pp. 108114, Boston, Massachusetts.
Moore, A. (1991). Efficient Memory-based Learning for Robot Control. Ph.D. thesis, University of
Cambridge.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: A theoretical and empirical
analysis. Artificial Intelligence, 76, 427454.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic search
with a priority queue. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI), pp. 23722377, Hyderabad, India.
Russell, S., & Wefald, E. (1991). Do the right thing: Studies in limited rationality. MIT Press.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic search.
Artificial Intelligence, 146(1), 141.
Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). An intelligent heuristic algorithm for project scheduling
problems. In Proceedings of the 32nd Annual Meeting of the Decision Sciences Institute, San
Francisco.
Shue, L.-Y., & Zamani, R. (1993). An admissible heuristic search algorithm. In Proceedings of the
7th International Symposium on Methodologies for Intelligent Systems (ISMIS-93), Vol. 689
of LNAI, pp. 6975.
Sigmundarson, S., & Bjornsson, Y. (2006). Value Back-Propagation vs. Backtracking in RealTime Search. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
Workshop on Learning For Search, pp. 136141, Boston, Massachusetts, USA.
Stenz, A. (1995). The focussed D* algorithm for real-time replanning. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI), pp. 16521659.
Sturtevant, N. (2007). Memory-efficient abstractions for pathfinding. In Proceedings of the third
conference on Artificial Intelligence and Interactive Digital Entertainment, pp. 3136, Stanford, California.
Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction and refinement. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 13921397,
Pittsburgh, Pennsylvania.
Sturtevant, N. R., Felner, A., Barrer, M., Schaeffer, J., & Burch, N. (2009). Memory-based heuristics
for explicit state spaces. In Boutilier, C. (Ed.), IJCAI 2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17,
2009, pp. 609614.
Valve Corporation (2004).
Counter-Strike: Source., Published by Valve Corporation,
http://store.steampowered.com/app/240/, October 7, 2004.
Weng, M., Wei, X., Qu, R., & Cai, Z. (2009). A path planning algorithm based on typical case
reasoning. Geo-spatial Information Science, 12, 6671.
300

fiJournal of Artificial Intelligence Research 39 (2010) 533-579

Submitted 12/09; published 10/10

Theta*: Any-Angle Path Planning on Grids
Kenny Daniel
Alex Nash
Sven Koenig

KFDANIEL @ USC . EDU
ANASH @ USC . EDU
SKOENIG @ USC . EDU

Computer Science Department
University of Southern California
Los Angeles, California 90089-0781, USA

Ariel Felner

FELNER @ BGU . AC . IL

Department of Information Systems Engineering
Ben-Gurion University of the Negev
Beer-Sheva, 85104, Israel

Abstract
Grids with blocked and unblocked cells are often used to represent terrain in robotics and video
games. However, paths formed by grid edges can be longer than true shortest paths in the terrain
since their headings are artificially constrained. We present two new correct and complete anyangle path-planning algorithms that avoid this shortcoming. Basic Theta* and Angle-Propagation
Theta* are both variants of A* that propagate information along grid edges without constraining
paths to grid edges. Basic Theta* is simple to understand and implement, fast and finds short paths.
However, it is not guaranteed to find true shortest paths. Angle-Propagation Theta* achieves a
better worst-case complexity per vertex expansion than Basic Theta* by propagating angle ranges
when it expands vertices, but is more complex, not as fast and finds slightly longer paths. We
refer to Basic Theta* and Angle-Propagation Theta* collectively as Theta*. Theta* has unique
properties, which we analyze in detail. We show experimentally that it finds shorter paths than
both A* with post-smoothed paths and Field D* (the only other version of A* we know of that
propagates information along grid edges without constraining paths to grid edges) with a runtime
comparable to that of A* on grids. Finally, we extend Theta* to grids that contain unblocked cells
with non-uniform traversal costs and introduce variants of Theta* which provide different tradeoffs
between path length and runtime.

1. Introduction
In this article, we study path planning for robotics and video games (Choset, Lynch, Hutchinson,
Kantor, Burgard, Kavraki, & Thrun, 2005; Deloura, 2000; Patel, 2000; Murphy, 2000; Rabin, 2002),
where a two-dimensional continuous terrain is discretized into a grid with blocked and unblocked
cells. Our objective is to find a short unblocked path from a given start vertex to a given goal vertex
(both at the corners of cells). A* finds grid paths (that is, paths constrained to grid edges) quickly, but
grid paths are often not true shortest paths (that is, shortest paths in the terrain) since their potential
headings are artificially constrained to multiples of 45 degrees, as shown in Figure 1(a) (Yap, 2002).
This shortcoming led to the introduction of what we call any-angle path planning (Nash, Daniel,
Koenig, & Felner, 2007; Ferguson & Stentz, 2006). Any-angle path-planning algorithms find paths
c
2010
AI Access Foundation. All rights reserved.

fiDANIEL , NASH , KOENIG , & F ELNER

A

1

2

3

4

5

A

s start

B

C

1

2

3

4

5

s start

B

C

s goal
(a) Grid path

s goal
(b) True shortest path

Figure 1: Grid path versus true shortest path
without constraining the headings of the paths, as shown in Figure 1(b). We present two new
correct and complete any-angle path-planning algorithms. Basic Theta* and Angle-Propagation
Theta* are both variants of A* that propagate information along grid edges (to achieve a short
runtime) without constraining paths to grid edges (to find any-angle paths). Unlike A* on visibility
graphs, they are not guaranteed to find true shortest paths. The asterisk in their names thus does not
denote their optimality but rather their similarity to A*. Basic Theta* is simple to understand and
implement, fast and finds short paths. Angle-Propagation Theta* achieves a worst-case complexity
per vertex expansion that is constant rather than linear in the number of cells (like that of Basic
Theta*) by propagating angle ranges when it expands vertices, but is more complex, is not as fast
and finds slightly longer paths. We refer to Basic Theta* and Angle-Propagation Theta* collectively
as Theta*. Theta* has unique properties, which we analyze in detail. We show experimentally that
it finds shorter paths than both A* with post-smoothed paths and Field D* (the only other version
of A* we know of that propagates information along grid edges without constraining paths to grid
edges) with a runtime comparable to that of A* on grids. Finally, we extend Theta* to grids that
contain unblocked cells with non-uniform traversal costs and introduce variants of Theta* which
provide different tradeoffs between path length and runtime.

2. Path-Planning Problem and Notation
In this section, we describe the path-planning problem that we study in this article, namely path
planning on eight-neighbor grids with blocked and unblocked cells of uniform size. Cells are labeled
as either blocked (grey) or unblocked (white). We use the corners of cells (rather than their centers)
as vertices. S is the set of all vertices. The path-planning problem is to find an unblocked path from
a given start vertex sstart to a given goal vertex sgoal .
A path is unblocked iff each vertex on the path has line-of-sight to its successor on the path. Vertex
s has line-of-sight to vertex s , written as LineOfSight(s, s ), iff the straight line from vertex s to
vertex s neither passes through the interior of blocked cells nor passes between blocked cells that
share an edge. Pseudocode for implementing the line-of-sight function is given in Appendix A. For
simplicity, we allow a straight line to pass between diagonally touching blocked cells.
c(s, s ) is the length of the straight line from vertex s to vertex s . nghbrsvis (s) is the set of visible
neighbors of vertex s in the eight compass directions, that is those neighbors of vertex s that have
534

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

line-of-sight to vertex s. Figure 1 shows an example where the visible neighbors of vertex B4 are
vertices A3, A4, A5, B3, B5, C3 and C4.

3. Existing Terrain Discretizations
Continuous terrain needs to be discretized for path planning. In this section, we compare grids to
other existing terrain discretizations. We use grids to discretize terrain since they are widely used in
robotics and video games (Deloura, 2000; Murphy, 2000; Rabin, 2004) and have several desirable
properties:
 Grids are simple data structures and allow for simple path-planning algorithms.
 Terrain can easily be discretized into a grid by laying the grid over the terrain and labeling all
cells that are partially or completely obstructed as blocked.
 Grids provide a comprehensive picture of all the traversable surfaces in the continuous terrain.
This is essential when the path planning algorithm is used in a dynamic environment and
must interact with a navigation planner. For example if a robot or video game character
encounters a temporary blockage to its path, it can easily determine whether it is best to
divert left (unblocked) or right (blocked) (Tozour, 2004).
 Cells can store information in addition to their traversability, such as the amount of gold
hidden in the region of the terrain that corresponds to the cell or a rendering of the region
when displaying the terrain.
 The information stored in cells can be accessed quickly since grids are random access data
structures.
 The precision of path and navigation planning can be improved by simply increasing the grid
resolution.
We now list some alternative terrain discretizations, assuming for simplicity that the obstacles in the
terrain are polygonal.
 Voronoi graphs (Aurenhammer, 1991) discretize the terrain by biasing paths away from
blocked polygons. The resulting paths can thus be much longer than true shortest paths.
 The discretization in the work of Mitchell and Papadimitriou (1991) partitions the terrain into
regions with linear and hyperbolic edges, which allows one to find true shortest paths with
time and space complexity O(m5/3 ), where m is the number of corners of blocked polygons.
Thus, the runtime of path planning can grow superlinearly in the number of corners of blocked
polygons.
 Framed Quadtrees (Yahja, Stentz, Singh, & Brumitt, 1998) recursively subdivide terrain into
four equally sized cells until all cells are completely obstructed, completely unobstructed or
of sufficiently small size. The resulting paths can have unnecessary heading changes (that is,
heading changes that occur in free space rather than the corners of blocked polygons).
535

fiDANIEL , NASH , KOENIG , & F ELNER

1 Main()
2
g(sstart ) := 0;
3
parent(sstart ) := sstart ;
4
open := ;
5
open.Insert(sstart , g(sstart ) + h(sstart ));
6
closed := ;
7
while open 6=  do
8
s := open.Pop();
if s = sgoal then
9
10
return path found;
11
12
13
14
15
16
17
18
19

closed := closed  {s};
/* The following line is executed only by AP Theta*.
[UpdateBounds(s)];
foreach s  nghbrsvis (s) do
if s 6 closed then
if s 6 open then
g(s ) := ;
parent(s ) := N U LL;

*/;

UpdateVertex(s, s );

20
return no path found;
21 end
22 UpdateVertex(s,s)
23
if g(s) + c(s, s ) < g(s ) then
24
g(s ) := g(s) + c(s, s );
25
parent(s ) := s;
26
if s  open then
27
open.Remove(s );
28

open.Insert(s , g(s ) + h(s ));

29 end

Algorithm 1: A*

 Probabilistic roadmaps (Kavraki, Svestka, Latombe, & Overmars, 1996) or rapidly-exploring
random trees (LaValle & Kuffner, 2001) place vertices randomly (in addition to the start and
goal vertex). Two vertices are connected via a straight line iff they have line-of-sight. The
random placement of vertices needs to be tuned carefully since it influences the runtime of
path planning, the likelihood of finding a path and the length of the path.
 Visibility graphs (Lee, 1978; Lozano-Perez & Wesley, 1979) use the corners of each blocked
polygon as vertices (in addition to the start and goal vertex). Two vertices are connected via
a straight line iff they have line-of-sight, which allows one to find true shortest paths. The
runtime of path planning can grow superlinearly in the number of vertices since the number
of edges can grow quadratically in the number of vertices.

4. Existing Path-Planning Algorithms
In this section, we describe some existing path-planning algorithms, all of which are variants of A*
(Hart, Nilsson, & Raphael, 1968). A* is a popular path-planning algorithm in robotics and video
games. Algorithm 1 shows the pseudocode of A*. Line 13 is to be ignored. A* maintains three
values for every vertex s:
536

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

 The g-value g(s) is the length of the shortest path from the start vertex to vertex s found so
far and thus is an estimate of the start distance of vertex s.
 The user-provided h-value h(s) is an estimate of the goal distance of vertex s. A* uses the
h-value to calculate an f-value to focus the A* search. The f-value f (s) = g(s) + h(s) is an
estimate of the length of a shortest path from the start vertex via vertex s to the goal vertex.
 The parent parent(s) is used to extract a path from the start vertex to the goal vertex after A*
terminates.
A* also maintains two global data structures:
 The open list is a priority queue that contains the vertices that A* considers for expansion.
In the pseudocode, open.Insert(s, x) inserts vertex s with key x into the priority queue open,
open.Remove(s) removes vertex s from the priority queue open, and open.Pop() removes a
vertex with the smallest key from the priority queue open and returns it.
 The closed list is a set that contains the vertices that A* has already expanded. It ensures that
A* expands every vertex at most once.
A* sets the g-value of every vertex to infinity and the parent of every vertex to NULL when it
encounters the vertex for the first time [Lines 17-18]. It sets the g-value of the start vertex to zero
and the parent of the start vertex to the start vertex itself [Lines 2-3]. It sets the open and closed
lists to the empty list and then inserts the start vertex into the open list with the f-value as its key
[4-6]. A* then repeatedly executes the following procedure: If the open list is empty, then it reports
that there is no path [Line 20]. Otherwise, it identifies a vertex s with the smallest f-value in the
open list [Line 8]. If this vertex is the goal vertex, then A* reports that it has found a path [Line 10].
Path extraction [not shown in the pseudocode] follows the parents from the goal vertex to the start
vertex to retrieve a path from the start vertex to the goal vertex in reverse. Otherwise, A* removes
the vertex from the open list [Line 8] and expands it by inserting the vertex into the closed list [Line
11] and then generating each of its unexpanded visible neighbors, as follows: A* checks whether
the g-value of vertex s plus the length of the straight line from vertex s to vertex s is smaller than
the g-value of vertex s [Line 23]. If so, then it sets the g-value of vertex s to the g-value of vertex
s plus the length of the straight line from vertex s to vertex s , sets the parent of vertex s to vertex
s and finally inserts vertex s into the open list with the f-value as its key or, if it was already in the
open list, sets its key to the f-value [Lines 24-28]. It then repeats this procedure.
To summarize, when A* updates the g-value and parent of an unexpanded visible neighbor s of
vertex s in procedure UpdateVertex, it considers the path from the start vertex to vertex s [= g(s)]
and from vertex s to vertex s in a straight line [= c(s, s )], resulting in a length of g(s) + c(s, s )
[Line 23]. A* updates the g-value and parent of vertex s if the considered path is shorter than the
shortest path from the start vertex to vertex s found so far [= g(s )].
We now describe several existing path-planning algorithms that are versions of A* and how they
trade off between two conflicting criteria, namely runtime and path length, as shown in Figure 2.
We introduce them in order of decreasing path lengths.
537

fiDANIEL , NASH , KOENIG , & F ELNER

10

1

Runtime

A*
A* PS
FD*

0.1

Visibility Graphs
Basic Theta*

0.01

0.001
1

1.01

1.02

1.03

1.04

1.05

1.06

Path Length / Length of True Shortest Path

Figure 2: Runtime versus path length (relative to the length of true shortest path) on random 100 
100 grids with 20 percent blocked cells

30 PostSmoothPath([s0 , . . . , sn ])
31
k := 0;
32
tk := s0 ;
33
foreach i := 1 . . . n  1 do
34
if NOT LineOfSight(tk , si+1 ) then
35
k := k + 1;
36
tk := si ;
37
k := k + 1;
38
tk := sn ;
39
return [t0 , . . . , tk ];
40 end

Algorithm 2: Post-smoothing

4.1 A* on Grids
One can run A* on grids, that is, on the graphs given by the grid vertices and edges. The resulting
paths are artificially constrained to be formed by the edges of the grid, which can be seen in Figure
1(a). As a result the paths found by A* on grids are not equivalent to the true shortest paths and
are unrealistic looking since they either deviate substantially from the true shortest paths or have
many more heading changes, which provides the motivation for smoothing them. We use the octile
distances, which can be computed using Algorithm 5, as h-values in the experiments.
538

fiT HETA *: A NY-A NGLE PATH P LANNING

1

2

3

4

5

A

ON

G RIDS

6
s goal
true shortest path

B
shortest grid path
A* PS path

C

s start

Figure 3: A* PS path versus true shortest path

4.2 A* with Post-Smoothed Paths (A* PS)
One can run A* with post-smoothed paths (A* PS) (Thorpe, 1984). A* PS runs A* on grids and
then smoothes the resulting path in a post-processing step, which often shortens it at an increase in
runtime. Algorithm 2 shows the pseudocode of the simple smoothing algorithm that A* PS uses
in our experiments (Botea, Muller, & Schaeffer, 2004), which provides a good tradeoff between
runtime and path length. Assume that A* on grids finds the path [s0 , s1 , . . . , sn ] with s0 = sstart
and sn = sgoal . A* PS uses the first vertex on the path as the current vertex. It then checks whether
the current vertex s0 has line-of-sight to the successor s2 of its successor on the path. If so, A*
PS removes the intermediate vertex s1 from the path, thus shortening it. A* PS then repeats this
procedure by checking again whether the current vertex s0 has line-of-sight to the successor s3 of
its successor on the path, and so on. As soon as the current vertex does not have line-of-sight to the
successor of its successor on the path, A* PS advances the current vertex and repeats this procedure
until it reaches the end of the path. We use the straight-line distances h(s) = c(s, sgoal ) as h-values
in the experiments.
A* PS typically finds shorter paths than A* on grids, but is not guaranteed to find true shortest paths.
Figure 3 shows an example. Assume that A* PS finds the dotted blue path, which is one of many
shortest grid paths. It then smoothes this path to the solid blue path, which is not a true shortest
path. The dashed red path, which moves above (rather than below) blocked cell B2-B3-C3-C2 is a
true shortest path. A* PS is not guaranteed to find true shortest paths because it only considers grid
paths during the A* search and thus cannot make informed decisions regarding other paths during
the A* search, which motivates interleaving searching and smoothing. In fact, Theta* is similar to
A* PS except that it interleaves searching and smoothing.
4.3 Field D* (FD*)
One can run Field D* (Ferguson & Stentz, 2006) (FD*). FD* propagates information along grid
edges without constraining the paths to grid edges. FD* was designed to use D* Lite (Koenig &
Likhachev, 2002) for fast replanning (by reusing information from the previous A* search to speed
up the next one) and searches from the goal vertex to the start vertex. Our version of FD* uses
A* and searches from the start vertex to the goal vertex, like all other path-planning algorithms in
this article, which allows us to compare them fairly, except for their replanning abilities. (Theta* is
currently in the process of being extended for fast replanning in Nash, Koenig, & Likhachev, 2009.)
539

fiDANIEL , NASH , KOENIG , & F ELNER

1

B

2

3

4

2.00

2.32

2.83

1.00

1.41

2.41

5

sgoal
0.45

A

C

D

0.55

X

sstart
0.00

1.00

2.00

3.00

1.00

1.41

2.32

3.27

Field D* path
Figure 4: FD* path

Figure 5: Screenshot of FD* path versus true shortest path
When FD* updates the g-value and parent of an unexpanded visible neighbor s of vertex s, it
considers all paths from the start vertex to any point X (not necessarily a vertex) on the perimeter of
vertex s [= g(X)] that has line-of-sight to vertex s , where the perimeter is formed by connecting
all the neighbors of vertex s , and from point X to vertex s in a straight line [= c(X, s )], resulting
in a length of g(X) + c(X, s ). FD* updates the g-value and parent of vertex s if the considered
path is shorter than the shortest path from the start vertex to vertex s found so far [= g(s )]. We use
the straight-line distances h(s) = c(s, sgoal ) as h-values in the experiments.
Figure 4 shows an example. The perimeter of vertex s = B4 is formed by connecting all of the
neighbors of vertex B4, as shown in bold. Consider point X on the perimeter. FD* does not know
the g-value of point X since it only stores g-values for vertices. It calculates the g-value using
linear interpolation between the g-values of the two vertices on the perimeter that are adjacent to
the point X. Thus, it linearly interpolates between g(B3) = 2.41 and g(C3) = 2.00, resulting in
g(X) = 0.55  2.41 + 0.45  2.00 = 2.23 since 0.55 and 0.45 are the distances from point X to
vertices B3 and C3, respectively. The calculated g-value of point X is different from its true start
distance [= 2.55] even though the g-values of vertices B3 and C3 are both equal to their true start
distances. The reason for this mistake is simple. There exist true shortest paths from the start vertex
through either vertex C3 or vertex B3 to the goal vertex. Thus, the linear interpolation assumption
predicts that there must also exist a short path from the start vertex through any point along the
edge that connects vertices B3 and C3 to the goal vertex. However, this is not the case since these
540

fiT HETA *: A NY-A NGLE PATH P LANNING

A

1

2

3

4

ON

G RIDS

5

s start

B

C

s goal
true shortest path
(a) Simple visibility graph

(b) Terrain resulting in a more complex visibility graph

Figure 6: Visibility graphs
paths need to circumnavigate blocked cell B2-B3-C3-C2, which makes them longer than expected.
As a result of miscalculating the g-value of point X, FD* sets the parent of vertex B4 to point X,
resulting in a path that has an unnecessary heading change at point X and is longer than even a
shortest grid path.
The authors of FD* recognize that the paths found by FD* frequently have unnecessary heading
changes and suggest to use a one-step look-ahead algorithm during path extraction (Ferguson &
Stentz, 2006), which FD* uses in our experiments. This one-step look-ahead algorithm allows FD*
to avoid some of the unnecessary heading changes, like the one in Figure 4, but does not eliminate
all of them. Figure 5 shows an example of an FD* path in red and the corresponding true shortest
path in blue. The FD* path still has many unnecessary heading changes.
4.4 A* on Visibility Graphs
One can run A* on visibility graphs. The visibility graph of a grid with blocked and unblocked
cells contains the start vertex, the goal vertex and the corners of all blocked cells (Lozano-Perez &
Wesley, 1979). We use the straight-line distances h(s) = c(s, sgoal ) as h-values in the experiments.
A* on visibility graphs finds true shortest paths, as shown in Figure 6(a). True shortest paths have
heading changes only at the corners of blocked cells, while the paths found by A* on grids, A* PS
and FD* can have unnecessary heading changes. On the other hand, A* on visibility graphs can be
slow. It propagates information along visibility graph edges, whose number can grow quadratically
in the number of cells, while A* on grids, A* PS and FD* propagate information along grid edges,
whose number grows only linearly in the number of cells. If one constructed the visibility graphs
before the A* search, one would need to perform a line-of-sight check for every pair of corners of
blocked cells to determine whether or not there should be a visibility graph edge between them,
which requires at least 2,556 line-of-sight checks for the room in Figure 6(b) (Tozour, 2004). The
number of line-of-sight checks performed by A* on visibility graphs can be reduced by constructing
541

fiDANIEL , NASH , KOENIG , & F ELNER

41 UpdateVertex(s,s)
42
if LineOfSight(parent(s), s ) then
43
/* Path 2 */
44
if g(parent(s)) + c(parent(s), s ) < g(s ) then
45
g(s ) := g(parent(s)) + c(parent(s), s );
46
parent(s ) := parent(s);
47
if s  open then
48
open.Remove(s );
open.Insert(s , g(s ) + h(s ));

49
50
51
52
53
54
55
56
57

else
/* Path 1 */
if g(s) + c(s, s ) < g(s ) then
g(s ) := g(s) + c(s, s );
parent(s ) := s;
if s  open then
open.Remove(s );
open.Insert(s , g(s ) + h(s ));

58 end

Algorithm 3: Basic Theta*

the visibility graphs during the A* search. When it expands a vertex, it performs line-of-sight checks
between the expanded vertex and the corners of all blocked cells (and the goal vertex). While
this can significantly reduce the number of line-of-sight checks performed in some environments,
such as simple outdoor terrain, it fails to do so in others, such as cluttered indoor terrain. More
complex optimizations, such as reduced visibility graphs can further reduce the number of line-ofsight checks, but do not sufficiently speed up A* on visibility graphs (Liu & Arimoto, 1992).

5. Basic Theta*
In this section, we introduce Theta* (Nash et al., 2007), our version of A* for any-angle path
planning that propagates information along grid edges without constraining the paths to grid edges.
It combines the ideas behind A* on visibility graphs (where heading changes occur only at the
corners of blocked cells) and A* on grids (where the number of edges grows only linearly in the
number of cells). Its paths are only slightly longer than true shortest paths (as found by A* on
visibility graphs), yet is only slightly slower than A* on grids, as shown in Figure 2. The key
difference between Theta* and A* on grids is that the parent of a vertex can be any vertex when
using Theta*, while the parent of a vertex has to be a neighbor of the vertex when using A*. We
first introduce Basic Theta*, a simple version of Theta*.
Algorithm 3 shows the pseudocode of Basic Theta*. Procedure Main is identical to that of A* in
Algorithm 1 and thus is not shown. Line 13 is to be ignored. We use the straight-line distances
h(s) = c(s, sgoal ) as h-values in the experiments.
5.1 Operation of Basic Theta*
Basic Theta* is simple. It is identical to A* except that, when it updates the g-value and parent of
an unexpanded visible neighbor s of vertex s in procedure UpdateVertex, it considers two paths
542

fiT HETA *: A NY-A NGLE PATH P LANNING

1

2

3

4

A

1

sgoal

s

B

s

C

Path 1

G RIDS

2

3

4

A

sstart

B

C

5

ON

Path 2

sstart

s

s

sgoal

Path 1

(a) Path 2 is unblocked

5

Path 2

(b) Path 2 is blocked

Figure 7: Paths 1 and 2 considered by Basic Theta*

instead of only the one path considered by A*. Figure 7(a) shows an example. Basic Theta* is
expanding vertex B3 with parent A4 and needs to update the g-value and parent of unexpanded
visible neighbor C3. Basic Theta* considers two paths:

 Path 1: Basic Theta* considers the path from the start vertex to vertex s [= g(s)] and from
vertex s to vertex s in a straight line [= c(s, s )], resulting in a length of g(s) + c(s, s ) [Line
52]. Path 1 is the path considered by A*. It corresponds to the dashed red path [A4, B3, C3]
in Figure 7(a)).
 Path 2: Basic Theta* also considers the path from the start vertex to the parent of vertex s [=
g(parent(s))] and from the parent of vertex s to vertex s in a straight line [= c(parent(s), s )],
resulting in a length of g(parent(s)) + c(parent(s), s ) [Line 44]. Path 2 is not considered
by A* and allows Basic Theta* to construct any-angle paths. It corresponds to the solid blue
path [A4, C3] in Figure 7(a).

Path 2 is no longer than Path 1 due to the triangle inequality. The triangle inequality states that
the length of any side of a triangle is no longer than the sum of the lengths of the other two sides.
It applies here since Path 1 consists of the path from the start vertex to the parent of vertex s, the
straight line from the parent of vertex s to vertex s (Line A) and the straight line from vertex s to
vertex s (Line B), Path 2 consists of the same path from the start vertex to the parent of vertex s
and the straight line from the parent of vertex s to vertex s (Line C) and Lines A, B and C form a
triangle. Path 1 is guaranteed to be unblocked but Path 2 is not. Thus, Basic Theta* chooses Path
2 over Path 1 if vertex s has line-of-sight to the parent of vertex s and Path 2 is thus unblocked.
Figure 7(a) shows an example. Otherwise, Basic Theta* chooses Path 1 over Path 2. Figure 7(b)
shows an example. Basic Theta* updates the g-value and parent of vertex s if the chosen path is
shorter than the shortest path from the start vertex to vertex s found so far [= g(s )]. We use the
straight-line distances h(s) = c(s, sgoal ) as h-values in the experiments.
543

fiDANIEL , NASH , KOENIG , & F ELNER

1

2

5

4

3

1

A

1.00

0.00

1.00

A4

sstart

A4

B

1.41

1.00

1.41

A4

A4

A4

C

sgoal

2

3

B

C

A4

sgoal

2

1.00

A4

sstart

A4

2.41

1.41

1.00

1.41

B3

A4

A4

A4

2.83

2.24

2.00

A4

A4

A4

(b)

3

4

3.82

3.41

1.00

0.00

1.00

B2

B2

A4

sstart

A4

B

3.41

2.41

1.41

1.00

1.41

B3

B3

A4

A4

A4

3.65

2.83

2.24

2.00

B3

A4

A4

A4

sgoal

1

5

A

C

5

0.00

(a)
1

4
1.00

A

2

3

4

5

A

3.82

3.41

1.00

0.00

1.00

B2

B2

A4

sstart

A4

B

3.41

2.41

1.41

1.00

1.41

B3

B3

A4

A4

A4

C

3.65

2.83

2.24

2.00

B3

A4

A4

A4

sgoal

(c)

(d)

Figure 8: Example trace of Basic Theta*

5.2 Example Trace of Basic Theta*
Figure 8 shows an example trace of Basic Theta*. The vertices are labeled with their g-values and
parents. The arrows point to their parents. Red circles indicate vertices that are being expanded, and
blue arrows indicate vertices that are generated during the current expansion. First, Basic Theta*
expands start vertex A4 with parent A4, as shown in Figure 8(a). It sets the parent of the unexpanded
visible neighbors of vertex A4 to vertex A4, just like A* would do. Second, Basic Theta* expands
vertex B3 with parent A4, as shown in Figure 8(b). Vertex B2 is an unexpanded visible neighbor of
vertex B3 that does not have line-of-sight to vertex A4. Basic Theta* thus updates it according to
Path 1 and sets its parent to vertex B3. On the other hand, vertices C2, C3 and C4 are unexpanded
visible neighbors of vertex B3 that have line-of-sight to vertex A4. Basic Theta* thus updates them
according to Path 2 and sets their parents to vertex A4. (The g-values and parents of the other
unexpanded visible neighbors of vertex B3 are not updated.) Third, Basic Theta* expands vertex
B2 with parent B3, as shown in Figure 8(c). Vertices A1 and A2 are unexpanded visible neighbors
of vertex B2 that do not have line-of-sight to vertex B3. Basic Theta* thus updates them according
to Path 1 and sets their parents to vertex B2. On the other hand, vertices B1 and C1 are unexpanded
visible neighbors of vertex B2 that do have line-of-sight to vertex B3. Basic Theta* thus updates
them according to Path 2 and sets their parents to vertex B3. Fourth, Basic Theta* expands goal
vertex C1 with parent B3 and terminates, as shown in Figure 8(d). Path extraction then follows the
parents from goal vertex C1 to start vertex A4 to retrieve the true shortest path [A4, B3, C1] from
the start vertex to the goal vertex in reverse.
544

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

5.3 Properties of Basic Theta*
We now discuss the properties of Basic Theta*.
5.3.1 C ORRECTNESS

AND

C OMPLETENESS

Basic Theta* is correct (that is, finds only unblocked paths from the start vertex to the goal vertex)
and complete (that is, finds a path from the start vertex to the goal vertex if one exists). We use the
following lemmata in the proof.
Lemma 1. If there exists an unblocked path between two vertices then there also exists an unblocked
grid path between the same two vertices.
Proof. An unblocked path between two vertices exists iff an unblocked any-angle path [s0 , . . . , sn ]
exists between the same two vertices. Consider any path segment sk sk+1 of this any-angle path. If
the path segment is horizontal or vertical, then consider the unblocked grid path from vertex sk to
vertex sk+1 that coincides with the path segment. Otherwise, consider the sequence (b0 , . . . , bm )
of unblocked cells whose interior the path segment passes through. Any two consecutive cells
bj and bj+1 share at least one vertex sj+1 since the cells either share an edge or are diagonally
touching. (If they share more than one vertex, pick one arbitrarily.) Consider the grid path [s0 =
sk , s1 , . . . , sm , sm+1 = sk+1 ]. This grid path from vertex sk to vertex sk+1 is unblocked since any
two consecutive vertices on it are corners of the same unblocked cell and are thus visible neighbors.
Repeat this procedure for every path segment of the any-angle path and concatenate the resulting
grid paths to an unblocked grid path from vertex s0 to vertex sn . (If several consecutive vertices on
the grid path are identical, then all of them but one can be removed.)
Lemma 2. At any point during the execution of Basic Theta*, following the parents from any vertex
in the open or closed lists to the start vertex retrieves an unblocked path from the start vertex to this
vertex in reverse.
Proof. We prove by induction that the lemma holds and that the parent of any vertex in the union
of the open or closed lists itself is in the union of the open or closed lists. This statement holds
initially because the start vertex is the only vertex in the union of the open or closed lists and it is
its own parent. We now show that the statement continues to hold whenever a vertex changes either
its parent or its membership in the union of the open or closed lists. Once a vertex is a member of
the union of the open or closed lists, it continues to be a member. A vertex can become a member
in the union of the open or closed lists only when Basic Theta* expands some vertex s and updates
the g-value and parent of an unexpanded visible neighbor s of vertex s in procedure UpdateVertex.
Vertex s is thus in the closed list, and its parent is in the union of the open or closed lists according
to the induction assumption. Thus, following the parents from vertex s (or its parent) to the start
vertex retrieves an unblocked path from the start vertex to vertex s (or its parent, respectively) in
reverse according to the induction assumption. If Basic Theta* updates vertex s according to Path
1, then the statement continues to hold since vertices s and s are visible neighbors and the path
segment from vertex s to vertex s is thus unblocked. If Basic Theta* updates vertex s according
to Path 2, then the statement continues to hold since Basic Theta* explicitly checks that the path
545

fiDANIEL , NASH , KOENIG , & F ELNER

segment from the parent of vertex s to vertex s is unblocked. There are no other ways in which the
parent of a vertex can change.
Theorem 1. Basic Theta* terminates and path extraction retrieves an unblocked path from the start
vertex to the goal vertex if such a path exists. Otherwise, Basic Theta* terminates and reports that
no unblocked path exists.
Proof. The following properties together prove the theorem. Their proofs utilize the fact that Basic
Theta* terminates iff the open is empty or it expands the goal vertex. The start vertex is initially in
the open list. Any other vertex is initially neither in the open nor closed lists. A vertex neither in the
open nor closed lists can be inserted into the open list. A vertex in the open list can be removed from
the open list and be inserted into the closed list. A vertex in the closed list remains in the closed list.
 Property 1: Basic Theta* terminates. It expands one vertex in the open list during each
iteration. In the process, it removes the vertex from the open list and can then never insert it
into the open list again. Since the number of vertices is finite, the open list eventually becomes
empty and Basic Theta* has to terminate if it has not terminated earlier already.
 Property 2: If Basic Theta* terminates because its open list is empty, then there does not
exist an unblocked path from the start vertex to the goal vertex. We prove the contrapositive.
Assume that there exists an unblocked path from the start vertex to the goal vertex. We prove
by contradiction that Basic Theta* then does not terminate because its open list is empty.
Thus, assume also that Basic Theta* terminates because its open list is empty. Then, there
exists an unblocked grid path [s0 = sstart , . . . , sn = sgoal ] from the start vertex to the goal
vertex according to Lemma 1. Choose vertex si to be the first vertex on the grid path that is
not in the closed list when Basic Theta* terminates. The goal vertex is not in the closed list
when Basic Theta* terminates since Basic Theta* would otherwise have terminated when it
expanded the goal vertex. Thus, vertex si exists. Vertex si is not the start vertex since the start
vertex would otherwise be in the open list and Basic Theta* could not have terminated because
its open list is empty. Thus, vertex si has a predecessor on the grid path. This predecessor is
in the closed list when Basic Theta* terminates since vertex si is the first vertex on the grid
path that is not in the closed list when Basic Theta* terminates. When Basic Theta* expanded
the predecessor, it added vertex si to the open list. Thus, vertex si is still in the open list when
Basic Theta* terminates. But then Basic Theta* could not have terminated because its open
list is empty, which is a contradiction.
 Property 3: If Basic Theta* terminates because it expands the goal vertex, then path extraction
retrieves an unblocked path from the start vertex to the goal vertex because following the
parents from the goal vertex to the start vertex retrieves an unblocked path from the start
vertex to the goal vertex in reverse according to Lemma 2.

546

fiT HETA *: A NY-A NGLE PATH P LANNING

A

1

2

3

4

5

ON

G RIDS

6

7

8

9

10

6

7

8

9

10

B

C

D

E

s start

(a)
A

1

2

3

4

5

B

C

D

E

s start

(b)

true shortest path

Basic Theta* path

Figure 9: Basic Theta* paths versus true shortest paths

5.3.2 O PTIMALITY
Basic Theta* is not optimal (that is, it is not guaranteed to find true shortest paths) because the
parent of a vertex has to be either a visible neighbor of the vertex or the parent of a visible neighbor,
which is not always the case for true shortest paths. Figure 9(a) shows an example where the dashed
red path [E1, B9] is a true shortest path from start vertex E1 to vertex B9 since vertex E1 has lineof-sight to vertex B9. However, vertex E1 is neither a visible neighbor nor the parent of a visible
neighbor of vertex B9 since vertex E1 does not have line-of-sight to these vertices (highlighted in
red). Thus, Basic Theta* cannot set the parent of vertex B9 to vertex E1 and does not find a true
shortest path from vertex E1 to vertex B9. Similarly, Figure 9(b) shows an example where the
dashed red path [E1, D8, C10] is a true shortest path from vertex E1 to vertex C10. However, vertex
D8 is neither a visible neighbor nor the parent of a visible neighbor of vertex C10 since start vertex
E1 either has line-of-sight to them or Basic Theta* found paths from vertex E1 to them that do not
547

fiDANIEL , NASH , KOENIG , & F ELNER

A

1 sstart

2

3

4

5

6

B

C

f=6.02

D
f=6.00

true shortest path

sgoal
f=6.00

Basic Theta* path

Figure 10: Heading changes of Basic Theta*

contain vertex D8. In fact, the truly shortest paths from vertex E1 to all visible neighbors of vertex
C10 that vertex E1 does not have line-of-sight to move above (rather than below) blocked cell C7C8-D8-D7. Thus, Basic Theta* cannot set the parent of vertex C10 to vertex D8 and thus does not
find a true shortest path from vertex E1 to vertex C10. The solid blue path from vertex E1 to vertex
B9 in Figure 9(a) and the solid blue path from vertex E1 to vertex C10 in Figure 9(b) are less than
a factor of 1.002 longer than the true shortest paths.
5.3.3 H EADING C HANGES
Basic Theta* takes advantage of the fact that true shortest paths have heading changes only at
the corners of blocked cells. However, the paths found by Basic Theta* can occasionally have
unnecessary heading changes. Figure 10 shows an example where Basic Theta* finds the solid blue
path [A1, D5, D6] from vertex A1 to vertex D6. The reason for this mistake is simple. Assume
that the open list contains both vertices C5 and D5. The f-value of vertex C5 is f (C5) = g(C5) +
h(C5) = 4.61 + 1.41 = 6.02 and its parent is vertex C4. The f-value of vertex D5 is f (D5) =
5.00 + 1.00 = 6.00 and its parent is vertex A1. Thus Basic Theta* expands vertex D5 before
vertex C5 (since its f-value is smaller). When Basic Theta* expands vertex D5 with parent A1,
it generates vertex D6. Vertex D6 is an unexpanded visible neighbor of vertex D5 that does not
have line-of-sight to vertex A1. Basic Theta* thus updates it according to Path 1, sets its f-value to
f (D6) = 6.00 + 0.00 = 6.00, sets its parent to vertex D5 and inserts it into the open list. Thus
Basic Theta* expands goal vertex D6 before vertex C5 (since its f-value is smaller) and terminates.
Path extraction then follows the parents from goal vertex D6 to start vertex A1 to retrieve the solid
blue path [A1, D5, D6]. Thus, Basic Theta* never expands vertex C5, which would have resulted in
it setting the parent of vertex D6 to vertex C4 according to Path 2 and path extraction retrieving the
dashed red path [A1, C4, D6] which is the true shortest path. The solid blue path from vertex A1 to
vertex D6 in Figure 10 is less than a factor of 1.027 longer than true shortest path.
548

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

59 UpdateVertex(s,s)
60
if s 6= sstart AND lb(s)  (s, parent(s), s )  ub(s) then
61
/* Path 2 */
62
if g(parent(s)) + c(parent(s), s ) < g(s ) then
63
g(s ) := g(parent(s)) + c(parent(s), s );
64
parent(s ) := parent(s);
65
if s  open then
66
open.Remove(s );
open.Insert(s , g(s ) + h(s ));

67
68
69
70
71
72
73
74
75

else
/* Path 1 */
if g(s) + c(s, s ) < g(s ) then
g(s ) := g(s) + c(s, s );
parent(s ) := s;
if s  open then
open.Remove(s );
open.Insert(s , g(s ) + h(s ));

76 end
77 UpdateBounds(s)
78
lb(s) := ; ub(s) := ;
79
if s 6= sstart then
80
foreach blocked cell b adjacent to s do
81
if s  corners(b) : parent(s) = s OR (s, parent(s), s ) < 0 OR
82
((s, parent(s), s ) = 0 AND c(parent(s), s )  c(parent(s), s)) then
83
lb(s) = 0;
84
85
86
87
88
89
90
91
92
93
94
95
96
97

if s  corners(b) : parent(s) = s OR (s, parent(s), s ) > 0 OR
((s, parent(s), s ) = 0 AND c(parent(s), s )  c(parent(s), s)) then
ub(s) = 0;
foreach s  nghbrsvis (s) do
if s  closed AND parent(s) = parent(s ) AND s 6= sstart then
if lb(s ) + (s, parent(s), s )  0 then
lb(s) := max(lb(s), lb(s ) + (s, parent(s), s ));
if ub(s ) + (s, parent(s), s )  0 then
ub(s) := min(ub(s), ub(s ) + (s, parent(s), s ));
if c(parent(s), s ) < c(parent(s), s) AND parent(s) 6= s AND (s 6 closed OR parent(s) 6= parent(s ))
then
if (s, parent(s), s ) < 0 then
lb(s) := max(lb(s), (s, parent(s), s ));
if (s, parent(s), s ) > 0 then
ub(s) := min(ub(s), (s, parent(s), s ));

98 end

Algorithm 4: AP Theta*

6. Angle-Propagation Theta* (AP Theta*)
The runtime of Basic Theta* per vertex expansion (that is, the runtime consumed during the generation of the unexpanded visible neighbors when expanding a vertex) can be linear in the number
of cells since the runtime of each line-of-sight check can be linear in the number of cells. In this
section, we introduce Angle-Propagation Theta* (AP Theta*), which reduces the runtime of Basic
549

fiDANIEL , NASH , KOENIG , & F ELNER

A

1

2

B

3

4

O

1

5

6

O

2

C

D

E

s

F

Figure 11: Region of points with line-of-sight to vertex s
Theta* per vertex expansion from linear to constant.1 The key difference between AP Theta* and
Basic Theta* is that AP Theta* propagates angle ranges and uses them to determine whether or not
two vertices have line-of-sight.
If there is a light source at a vertex and light cannot pass through blocked cells, then cells in the
shadows do not have line-of-sight to the vertex while all other cells have line-of-sight to the vertex.
Each contiguous region of points that have line-of-sight to the vertex can be characterized by two
rays emanating from the vertex and thus by an angle range defined by two angle bounds. Figure
11 shows an example where all points within the red angle range defined by the two angle bounds
1 and 2 have line-of-sight to vertex s. AP Theta* calculates the angle range of a vertex when
it expands the vertex and then propagates it along grid edges, resulting in a constant runtime per
vertex expansion since the angle ranges can be propagated in constant time and the line-of-sight
checks can be performed in constant time as well.
Algorithm 4 shows the pseudocode of AP Theta*. Procedure Main is identical to that of A* in
Algorithm 1 and thus is not shown. Line 13 is to be executed. We use the straight-line distances
h(s) = c(s, sgoal ) as h-values in the experiments.
6.1 Definition of Angle Ranges
We now discuss the key concept of an angle range. AP Theta* maintains two additional values for
every vertex s, namely a lower angle bound lb(s) of vertex s and an upper angle bound ub(s) of
vertex s, that together form the angle range [lb(s), ub(s)] of vertex s. The angle bounds correspond
to headings of rays (measured in degrees) that originate at the parent of vertex s. The heading of
the ray from the parent of vertex s to vertex s is zero degrees. A visible neighbor of vertex s is
guaranteed to have line-of-sight to the parent of vertex s if (but not necessarily only if) the heading
of the ray from the parent of vertex s to the visible neighbor of vertex s is contained in the angle
1. While AP Theta* provides a significant improvement in the worst case complexity over Basic Theta*, our experimental results in Section 7 show that it is slower and finds slightly longer paths than Basic Theta*.

550

fiT HETA *: A NY-A NGLE PATH P LANNING

1

2

3

4

A

ON

G RIDS

5

sstart
lb

B

18O
O

27
C

sgoal

s

ub

Figure 12: Angle range of AP Theta*
range of vertex s. Figure 12 shows an example where vertex C3 with parent A4 has angle range
[18, 27]. Thus, all visible neighbors of vertex C3 in the red region are guaranteed to have line-ofsight to the parent of vertex C3. For example, vertex C4 is guaranteed to have line-of-sight to the
parent of vertex C3 but vertex B2 is not. AP Theta* therefore assumes that vertex B2 does not have
line-of-sight to the parent of vertex C3.
We now define the concept of an angle range more formally. (s, p, s )  [90, 90], which gives
AP Theta* its name, is the angle (measured in degrees) between the ray from vertex p to vertex s and
the ray from vertex p to vertex s . It is positive if the ray from vertex p to vertex s is clockwise from
the ray from vertex p to vertex s , zero if the ray from vertex p to vertex s has the same heading as the
ray from vertex p to vertex s , and negative if the ray from vertex p to vertex s is counterclockwise
from the ray from vertex p to vertex s . Figure 12 shows an example where (C3, A4, C4) = 27
and (C3, A4, B3) = 18. A visible neighbor s of vertex s is guaranteed to have line-of-sight to
the parent of vertex s if (but not necessarily only if) lb(s)  (s, parent(s), s )  ub(s) (Visibility
Property).
6.2 Update of Angle Ranges
We now discuss how AP Theta* calculates the angle range of a vertex when it expands the vertex.
This calculation is complicated by the fact that AP Theta* is not guaranteed to have sufficient
information to determine the angle range exactly since the order of vertex expansions depends on a
variety of factors, such as the h-values. In this case, AP Theta* can constrain the angle range more
than necessary to guarantee that the Visibility Property holds and that it finds unblocked paths.
When AP Theta* expands vertex s, it sets the angle range of vertex s initially to [, ], meaning
that all visible neighbors of the vertex are guaranteed to have line-of-sight to the parent of the vertex.
It then constrains the angle range more and more if vertex s is not the start vertex.
AP Theta* constrains the angle range of vertex s based on each blocked cell b that is adjacent to
vertex s (that is, that vertex s is a corner of b, written as s  corners(b)) provided that at least one
of two conditions is satisfied:
 Case 1: If every corner s of blocked cell b satisfies at least one of the following conditions:
 parent(s) = s or
551

fiDANIEL , NASH , KOENIG , & F ELNER

 (s, parent(s), s ) < 0 or
 (s, parent(s), s ) = 0 and c(parent(s), s )  c(parent(s), s),
then AP Theta* assumes that a vertex s does not have line-of-sight to the parent of vertex s
if the ray from the parent of vertex s to vertex s is counterclockwise from the ray from the
parent of vertex s to vertex s , that is, if (s, parent(s), s ) < 0. AP Theta* therefore sets
the lower angle bound of vertex s to (s, parent(s), s) = 0 [Line 83].
 Case 2: If every corner s of blocked cell b satisfies at least one of the following conditions:
 parent(s) = s or
 (s, parent(s), s ) > 0 or
 (s, parent(s), s ) = 0 and c(parent(s), s )  c(parent(s), s),
then AP Theta* assumes that a vertex s does not have line-of-sight to the parent of vertex s
if the ray from the parent of vertex s to vertex s is clockwise from the ray from the parent of
vertex s to vertex s , that is, if (s, parent(s), s ) > 0. AP Theta* therefore sets the upper
angle bound of vertex s to (s, parent(s), s) = 0 [Line 86].
AP Theta* also constrains the angle range of vertex s based on each visible neighbor s of vertex s
provided that at least one of two conditions is satisfied:
 Case 3: If vertex s satisfies all of the following conditions:
 s  closed and

 parent(s) = parent(s ) and
 s 6= sstart ,
then AP Theta* constrains the angle range of vertex s by intersecting it with the angle range
of vertex s [Lines 90 and 92]. To do that, it first shifts the angle range of vertex s by
(s, parent(s), s ) degrees to take into account that the angle range of vertex s is calibrated
so that the heading of the ray from the joint parent of vertices s and s to vertex s is zero
degrees, while the angle range of vertex s is calibrated so that the heading of the ray from the
joint parent of vertices s and s to vertex s is zero degrees. Lines 89 and 91 ensure that the
lower angle bound always remains non-positive and the upper angle bound always remains
non-negative, respectively. The fact that lower angle bounds should be non-positive (and
upper angle bounds non-negative) is intuitive in that if a vertex s is assigned parent vertex p
then the angle of the ray from vertex p to vertex s should be included in the angle range of
vertex s.
 Case 4: If vertex s satisfies all of the following conditions:
 c(parent(s), s ) < c(parent(s), s) and
 parent(s) 6= s and
552

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

 s 6 closed or parent(s) 6= parent(s ),
then AP Theta* has insufficient information about vertex s . AP Theta* therefore cannot
determine the angle range of vertex s exactly and makes the conservative assumption that
vertex s barely has line-of-sight to the parent of vertex s [Lines 95 and 97].
The Visibility Property holds after AP Theta* has updated the angle range of vertex s in procedure
UpdateBounds. Thus, when AP Theta* checks whether or not a visible neighbor s of vertex s has
line-of-sight to the parent of vertex s, it now checks whether or not lb(s)  (s, parent(s), s ) 
ub(s) [Line 60] is true instead of whether or not LineOfSight(parent(s), s ) [Line 42] is true . These
are the only differences between AP Theta* and Basic Theta*.
Figure 13(a) shows an example where AP Theta* calculates the angle range of vertex A4. It sets
the angle range to [, ]. Figure 13(b) shows an example where AP Theta* calculates the angle
range of vertex B3. It sets the angle range initially to [, ]. It then sets the lower angle bound
to 0 degrees according to Case 1 based on the blocked cell A2-A3-B3-B2 [Line 83]. It sets the
upper angle bound to 45 degrees according to Case 4 based on vertex B4, which is unexpanded and
thus not in the closed list [Line 97]. Figure 13(c) shows an example where AP Theta* calculates
the angle range of vertex B2. It sets the angle range initially to [, ]. It then sets the lower
angle bound to 0 degrees according to Case 1 based on the blocked cell A2-A3-B3-B2 [Line 83].
Assume that vertex C1 is not the goal vertex. Figure 13(d) then shows an example where AP Theta*
calculates the angle range of vertex C1. It sets the angle range initially to [, ]. It then sets the
lower angle bound to -27 degrees according to Case 3 based on vertex B2 [Line 90] and the upper
angle bound to 18 degrees according to Case 4 based on vertex C2, which is unexpanded and thus
not in the closed list [Line 97].
6.3 Example Trace of AP Theta*
Figure 13 shows an example trace of AP Theta* using the path-planning problem from Figure 8.
The labels of the vertices now include the angle ranges.
6.4 Properties of AP Theta*
We now discuss the properties of AP Theta*. AP Theta* operates in the same way as Basic Theta*
and thus has similar properties as Basic Theta*. For example, AP Theta* is correct and complete. It
is not guaranteed to find true shortest paths, and its paths can occasionally have unnecessary heading
changes.
AP Theta* sometimes constrains the angle ranges more than necessary to guarantee that it finds
unblocked paths, which means that its line-of-sight checks sometimes fail incorrectly in which case
it has to update vertices according to Path 1 rather than Path 2. AP Theta* is still complete since
it finds an unblocked grid path if all line-of-sight checks fail, and there always exists an unblocked
grid path if there exists an unblocked any-angle path. However, the paths found by AP Theta* can be
longer than those found by Basic Theta*. Figure 14 shows an example. When AP Theta* expands
vertex C4 with parent B1 and calculates the angle range of vertex C4, vertex C3 is unexpanded and
thus not in the closed list. This means that AP Theta* has insufficient information about vertex
553

fiDANIEL , NASH , KOENIG , & F ELNER

2

3

4

1

5

1.00

0.00

1.00

A4

sstart

A4

3

4

1.00
A4

]

8
8

[ ,

2

A

B

1.41

1.00

1.41

A4

A4

A4

C

2.41

1.41

1.00

1.41

B3

A4
[0,45]

A4

A4

C

2.83

2.24

2.00

A4

A4

A4

2

(b)

3

3.82

3.41

B2

B2

4

1

5

0.00

1.00
A4

1.00
A4

sstart

2

3

3.82

3.41

B2

B2

4
1.00
A4

]

2.41

B3

B3
[0, ]

C

3.65

2.83

B3

A4

1.41
A4
[0,45]

1.00
A4

2.24
A4

2.00
A4

B

1.41
A4

3.41

2.41

B3

B3
[0, ]

3.65

2.83

B3
[27,18]

A4

8

3.41

8

B

5

0.00

sstart

1.00
A4

[ , ]

8
8

[ ,

A

8
8

1

1.00
A4

sstart
[ , ]

B

(a)
A

5

0.00
8
8

1
A

C

(c)

1.41
A4
[0,45]

1.00
A4

2.24
A4

2.00
A4

1.41
A4

(d)

Figure 13: Example trace of AP Theta*
1

2

3

4

5

6

A

s
B

C

D

start

1111
0000
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111

sgoal

Basic Theta* path

AP Theta* path

Figure 14: Basic Theta* path versus AP Theta* path
C3 because, for example, it does not know whether or not cell C2-C3-D3-D2 is unblocked. AP
Theta* therefore cannot determine the angle range of vertex C4 exactly and makes the conservative
assumption that vertex C3 barely has line-of-sight to vertex B1 and sets the lower angle bound
of vertex C4 according to Case 4 based on vertex C3. It then uses the resulting angle range to
determine that the unexpanded visible neighbor D4 of vertex C4 is not guaranteed to have line-ofsight to vertex B1. However, vertex D4 does have line-of-sight to vertex B1 if cell C2-C3-D3-D2
554

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

Figure 15: Map of Baldurs Gate II
is unblocked. AP Theta* eventually finds the solid blue path [B1, C3, D4] from start vertex B1 to
vertex D4, while Basic Theta* finds the dashed red path [B1, D4], which is the true shortest path.
The correctness and completeness proof of Basic Theta* needs to get changed slightly for AP Theta*
since AP Theta* performs its line-of-sight checks differently.
Theorem 2. AP Theta* terminates and path extraction retrieves an unblocked path from the start
vertex to the goal vertex if such a path exists. Otherwise, AP Theta* terminates and reports that no
unblocked path exists.
Proof. The proof is similar to the proof of Theorem 1 since AP Theta* uses the angle ranges only
to determine whether or not Path 2 is blocked but not to determine whether or not Path 1 is blocked.
The only property that needs to be proved differently is that two vertices indeed have line-of-sight
if (but not necessarily only if) the line-of-sight check of AP Theta* succeeds, see Appendix B.

7. Experimental Results
In this section, we compare Basic Theta* and AP Theta* to A* on grids, A* PS, FD* and A* on
visibility graphs with respect to their path length, number of vertex expansions, runtime (measured
in seconds) and number of heading changes.
We compare these path-planning algorithms on 100  100 and 500  500 grids with different percentages of randomly blocked cells (random grids) and scaled maps from the real-time strategy
game Baldurs Gate II (game maps). Figure 15 (Bulitko, Sturtevant, & Kazakevich, 2005) shows an
example of a game map. The start and goal vertices are the south-west corners of cells. For random
grids, the start vertex is in the south-west cell. The goal vertex is in a cell randomly chosen from
the column of cells furthest east. Cells are blocked randomly but a one-unit border of unblocked
cells guarantees that there is path from the start vertex to the goal vertex. For game maps, the start
and goal vertices are randomly chosen from the corners of unblocked cells. We average over 500
random 100  100 grids, 500 random 500  500 grids and 118 game maps.
555

fiDANIEL , NASH , KOENIG , & F ELNER

500500

100100

FD*
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%

40.04
114.49
114.15
114.74
115.20
115.45
223.64
576.19
568.63
576.23
580.19
581.73

Basic Theta* AP Theta* A* on Visibility Graphs A* on Grids
(true shortest path)
39.98
40.05
39.96
41.77
114.33
114.33
114.33
120.31
113.94
113.94
113.83
119.76
114.51
114.51
114.32
119.99
114.93
114.95
114.69
120.31
115.22
115.25
114.96
120.41
223.30
224.40
N/A
233.66
575.41
575.41
N/A
604.80
567.30
567.34
N/A
596.45
574.57
574.63
N/A
603.51
578.41
578.51
N/A
604.93
580.18
580.35
N/A
606.38

A* PS
40.02
114.33
114.71
115.46
116.16
116.69
223.70
575.41
573.46
581.03
585.62
588.98

Table 1: Path length

500500

100100

FD*
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%

0.0111
0.0229
0.0275
0.0305
0.0367
0.0429
0.1925
0.3628
0.4514
0.5608
0.6992
0.8562

Basic Theta* AP Theta* A* on Visibility Graphs A* on Grids
(true shortest path)
0.0060
0.0084
0.4792
0.0048
0.0073
0.0068
0.0061
0.0053
0.0090
0.0111
0.0766
0.0040
0.0111
0.0145
0.3427
0.0048
0.0150
0.0208
1.7136
0.0084
0.0183
0.0263
3.7622
0.0119
0.1166
0.1628
N/A
0.0767
0.1000
0.0234
N/A
0.0122
0.1680
0.1962
N/A
0.0176
0.2669
0.3334
N/A
0.0573
0.3724
0.5350
N/A
0.1543
0.5079
0.7291
N/A
0.3238

A* PS
0.0052
0.0208
0.0206
0.0204
0.0222
0.0240
0.1252
0.6270
0.6394
0.6717
0.6852
0.7355

Table 2: Runtime

500500

100100

FD*
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%

247.07
592.74
760.17
880.21
1175.42
1443.44
6846.62
11468.11
15804.81
19874.62
26640.83
34313.28

Basic Theta* AP Theta* A* on Visibility Graphs A* on Grids
(true shortest path)
228.45
226.42
68.23
197.19
240.42
139.53
1.00
99.00
430.06
361.17
35.35
111.96
591.31
520.91
106.23
169.98
851.79
813.14
357.33
386.41
1113.40
1089.96
659.36
620.18
6176.37
6220.58
N/A
5580.32
2603.40
663.34
N/A
499.00
7450.85
5917.25
N/A
755.66
11886.95
10405.34
N/A
2203.83
18621.61
17698.75
N/A
6777.15
25744.57
25224.92
N/A
14641.36

A* PS
315.08
1997.29
1974.27
1936.56
2040.10
2153.28
9673.88
49686.47
49355.41
50924.01
50358.66
53732.82

Table 3: Number of vertex expansions

All path-planning algorithms are implemented in C# and executed on a 3.7 GHz Core 2 Duo with 2
GByte of RAM. Our implementations are not optimized and can possibly be improved.
556

fiT HETA *: A NY-A NGLE PATH P LANNING

500500

100100

FD*
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%
Game Maps
Random Grids 0%
Random Grids 5%
Random Grids 10%
Random Grids 20%
Random Grids 30%

34.25
123.40
113.14
106.66
98.76
96.27
219.70
667.00
592.65
559.69
506.10
481.16

G RIDS

ON

Basic Theta* AP Theta* A* on Visibility Graphs A* on Grids
(true shortest paths)
3.08
3.64
2.92
5.21
0.00
0.00
0.00
0.99
5.14
6.03
5.06
6.00
8.96
9.87
8.84
10.85
15.21
15.96
14.74
19.42
19.96
20.62
19.44
26.06
4.18
7.58
N/A
10.19
0.00
0.00
N/A
1.00
21.91
27.99
N/A
24.68
41.60
47.40
N/A
49.73
72.49
76.79
N/A
91.40
97.21
100.31
N/A
123.81

A* PS
2.83
0.00
4.53
8.48
14.45
18.35
3.84
0.00
22.27
43.16
69.44
89.43

592

0.9

589

0.8

586

0.7

583

0.6

580

0.5

Runtime

Path Length

Table 4: Number of heading changes

577

0.4

574

0.3

571

0.2

568

0.1

565

0
0

5

10

20

30

0

5

10

% Blocked
FD*

Basic Theta*

20

30

% Blocked

AP Theta*

A* PS

FD*

Basic Theta*

(a) Path length

AP Theta*

A* PS

A*

(b) Runtime

60000

800

700

Number of Heading Changes

50000

Vertex Expansions

40000

30000

20000

600

500

400

300

200
10000
100

0

0
0

5

10

20

30

0

5

% Blocked
FD*

Basic Theta*

10

20

30

% Blocked

AP Theta*

A* PS

A*

FD*

(c) Number of vertex expansions

Basic Theta*

AP Theta*

A* PS

A*

(d) Number of heading changes

Figure 16: Random 500  500 grids

557

fiDANIEL , NASH , KOENIG , & F ELNER

99 h(s)
x := |s.x  (sgoal ).x|;
100
y := |s.y  (sgoal ).y|;
101

102
largest := max(x , y );
103
smallest := min(x , y );

104
return 2  smallest + (largest  smallest);
105 end

Algorithm 5: Calculation of octile distances

A* on grids, A* PS, FD* and A* on visibility graphs break ties among vertices with the same fvalue in the open list in favor of vertices with larger g-values (when they decide which vertex to
expand next) since this tie-breaking scheme typically results in fewer vertex expansions and thus
shorter runtimes for A*. Care must thus be taken when calculating the g-values, h-values and fvalues precisely. The numerical precision of these
 floating point numbers can be improved for A*
on grids by representing them in the form m + 2n for integers m and n. Basic Theta* and AP
Theta* break ties in favor of vertices with smaller g-values for the reasons explained in Section 9.
We use all path-planning algorithms with consistent h-values since consistent h-values result in
short paths for A*. Consistent h-values satisfy the triangle inequality, that is, the h-value of the
goal vertex is zero and the h-value of any potential non-goal parent of any vertex is no greater than
the distance from the potential non-goal parent of the vertex to the vertex plus the h-value of the
vertex (Hart et al., 1968; Pearl, 1984). Consistent h-values are lower bounds on the corresponding
goal distances of vertices. Increasing consistent h-values typically decreases the number of vertex
expansions for A* and thus also the runtime of A*. We thus use all path-planning algorithms with
the largest consistent h-values that are easy to calculate. For Basic Theta*, AP Theta*, FD* and
A* on visibility graphs, the goal distances of vertices can be equal to the true goal distances, that
is, the goal distances on grids if the paths are not constrained to grid edges. We therefore use
these path planning algorithms with the straight-line distances h(s) = c(s, sgoal ) as h-values in our
experiments. The straight-line distances are the goal distances on grids without blocked cells if the
paths are not constrained to grid edges. For A* on grids and A* PS, the goal distances of vertices
are equal to the goal distances on grids if the paths are constrained to grid edges. We could therefore
use them with the larger octile distances as h-values in our experiments. The octile distances are the
goal distances on grids without blocked cells if the paths are constrained to grid edges. Algorithm
5 shows how to calculate the octile distance of a given vertex s, where s.x and s.y are the x and y
coordinates of vertex s, respectively. We indeed use A* on grids with the octile distances but A*
PS with the straight-line distances since smoothing is then typically able to shorten the resulting
paths much more at an increase in the number of vertex expansions and thus runtime. Grids without
blocked cells provide an example. With the octile 
distances as h-values, A* on grids finds paths in
which all diagonal movements (whose lengths are 2) precede all horizontal or vertical movements
(whose lengths are 1) because the paths with the largest number of diagonal movements are the
longest ones among all paths with the same number of movements due to the tie-breaking scheme
used. On the other hand, with the straight-line distances as h-values, A* on grids finds paths that
interleave the diagonal movements with the horizontal and vertical movements (which means that
it is likely that there are lots of opportunities to smooth the paths even for grids with some blocked
cells) and that are closer to the straight line between the start and goal vertices (which means that
it is likely that the paths are closer to true shortest paths even for grids with some blocked cells),
558

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

Figure 17: True shortest paths found by FD* (left), A* PS (middle) and Basic Theta* (right)
because the h-values of vertices closer to the straight line are typically smaller than the h-values of
vertices farther away from the straight line.
Tables 1-4 report our experimental results. The runtime of A* on visibility graphs (which finds
true shortest paths) is too long on 500  500 grids and thus is omitted. Figure 16 visualizes the
experimental results on random 500  500 grids. The path length of A* on grids is much larger than
the path lengths of the other path-planning algorithms and thus is omitted.
We make the following observations about the path lengths:
 The path-planning algorithms in order of increasing path lengths tend to be: A* on visibility
graphs (which finds true shortest paths), Basic Theta*, AP Theta*, FD*, A* PS and A* on
grids. On random 500  500 grids with 20 percent blocked cells, Basic Theta* finds shorter
paths than AP Theta* 70 percent of the time, shorter paths than FD* 97 percent of the time,
shorter paths than A* PS 94 percent of the time and shorter paths than A* on grids 99 percent
of the time.
 The paths found by Basic Theta* and AP Theta* are almost as short as true shortest paths even
though AP Theta* sometimes constrains the angle ranges more than necessary. For example,
they are on average less than a factor of 1.003 longer than true shortest paths on 100  100
grids.
 Basic Theta* finds true shortest paths more often than FD* and A* PS. Figure 17 shows an
example where the light green vertex in the center is the start vertex and the red, green and
blue vertices represent goal vertices to which FD*, A* PS and Basic Theta* find true shortest
paths, respectively.
We make the following observations about the runtimes. The path-planning algorithms in order of
increasing runtimes tend to be: A* on grids, Basic Theta*, AP Theta*, A* PS, FD* and A* on
visibility graphs.
We make the following observations about the numbers of vertex expansions. The path-planning
algorithms in order of increasing numbers of vertex expansions tend to be: A* on visibility graphs,
A* on grids, AP Theta*, Basic Theta*, FD* and A* PS. (The number of vertex expansions of A*
on grids and A* PS are different because we use them with different h-values.)
559

fiDANIEL , NASH , KOENIG , & F ELNER

Runtime
Runtime per Vertex Expansion

FD*
5.21
0.000021

Basic Theta*
3.65
0.000015

AP Theta*
5.70
0.000023

A* PS
3.06
0.000012

Table 5: Path-planning algorithms without post-processing steps on random 500  500 grids with
20 percent blocked cells

Finally, we make the following observations about the number of heading changes. The pathplanning algorithms in order of increasing numbers of heading changes tend to be: A* PS, A* on
visibility graphs, Basic Theta*, AP Theta*, A* on grids and FD*.
There are some exceptions to the trends reported above. We therefore perform paired t-tests. They
show with confidence level  = 0.01 that Basic Theta* indeed finds shorter paths than AP Theta*,
A* PS and FD* and that Basic Theta* indeed has a shorter runtime than AP Theta*, A* PS and
FD*.
To summarize, A* on visibility graphs finds true shortest paths but is slow. On the other hand, A*
on grids finds long paths but is fast. Any-angle path planning lies between these two extremes.
Basic Theta* dominates AP Theta*, A* PS and FD* in terms of the tradeoff between runtime and
path length. It finds paths that are almost as short as true shortest paths and is almost as fast as
A* on grids. It is also simpler to implement than AP Theta*. Therefore, we build on Basic Theta*
for the remainder of this article, although we report some experimental results for AP Theta* as
well. However, AP Theta* reduces the runtime of Basic Theta* per vertex expansion from linear to
constant. It is currently unknown whether or not constant time line-of-sight checks can be devised
that make AP Theta* faster than Basic Theta*. This is an interesting area of future research since
AP Theta* is potentially a first step toward significantly reducing the runtime of any-angle path
planning via more sophisticated line-of-sight checks.

8. Extensions of Theta*
In this section, we extend Basic Theta* to find paths from a given start vertex to all other vertices
and to find paths on grids that contain unblocked cells with non-uniform traversal costs.
8.1 Single Source Paths
So far, Basic Theta* has found paths from a given start vertex to a given goal vertex. We now discuss
a version of Basic Theta* that finds single source paths (that is, paths from a given start vertex to all
other vertices) by terminating only when the open list is empty instead of when either the open list
is empty or it expands the goal vertex.
Finding single source paths requires all path-planning algorithms to expand the same number of
vertices, which minimizes the influence of the h-values on the runtime and thus results in a clean
comparison since the h-values sometimes are chosen to trade off between runtime and path length.
The runtimes of A* PS and FD* are effected more than those of Basic Theta* and AP Theta*
when finding single source paths since they require post-smoothing or path-extraction steps for each
560

fiT HETA *: A NY-A NGLE PATH P LANNING

A

1

2

3

4

5

ON

G RIDS

6
I6

I5
I4

B

I3
I2
I1

C

I0

Basic Theta* path with Non-Uniform Traversal Costs

Figure 18: Basic Theta* on grids that contain unblocked cells with non-uniform traversal costs

(a) Small contiguous regions of uniform traversal costs
Path Cost
Runtime

A* on Grids
4773.59
11.28

FD*
4719.26
14.98

Basic Theta*
4730.96
19.02

(b) Large contiguous regions of uniform traversal costs
Path Cost
Runtime

A* on Grids
1251.88
3.42

FD*
1208.89
5.31

Basic Theta*
1207.06
5.90

Table 6: Path-planning algorithms on random 1000  1000 grids with non-uniform traversal costs

path, and thus need to post-process many paths. Table 5 reports the runtimes of the path-planning
algorithms without these post-processing steps. The runtime of Basic Theta* per vertex expansion
is similar to that of A* PS and shorter than that of either AP Theta* and FD* because the later two
algorithms require more floating point operations.
8.2 Non-Uniform Traversal Costs
So far, Basic Theta* has found paths on grids that contain unblocked cells with uniform traversal
costs. In this case, true shortest paths have heading changes only at the corners of blocked cells and
the triangle inequality holds, which means that Path 2 is no longer than Path 1. We now discuss
a version of Basic Theta* that finds paths on grids that contain unblocked cells with non-uniform
traversal costs by computing and comparing path lengths (which are now path costs) appropriately.
In this case, true shortest paths can also have heading changes at the boundaries between unblocked
cells with different traversal costs and the triangle inequality is no longer guaranteed to hold, which
means that Path 2 can be more costly than Path 1. Thus, Basic Theta* no longer unconditionally
chooses Path 2 over Path 1 if Path 2 is unblocked [Line 42] but chooses the path with the smaller
cost. It uses the standard Cohen-Sutherland clipping algorithm from computer graphics (Foley, van
Dam, Feiner, & Hughes, 1992) to calculate the cost of Path 2 during the line-of-sight check. Figure
18 shows an example for the path segment C1A6 from vertex C1 to vertex A6. This straight line is
split into line segments at the points where it intersects with cell boundaries. The cost of the path
segment is the sum of the costs of its line segments Ii Ii+1 , and the cost of each line segment is the
product of its length and the traversal cost of the corresponding unblocked cell.
We found that changing the test on Line 52 in Algorithm 3 from strictly less than to less than or
equal to slightly reduces the runtime of Basic Theta*. This is a result of the fact that it is faster to
compute the cost of a path segment that corresponds to Path 1 than Path 2 since it tends to consist
of fewer line segments.
561

fiDANIEL , NASH , KOENIG , & F ELNER

A

2

1

3

4

5

A

s goal

1

2

3

4

5

s goal
h =2.24

h =3.16
B

B

g =1.41
g =2.24
C

C

s start

s start

(a)

(b)

Figure 19: Non-monotonicity of f-values of Basic Theta*

We compare Basic Theta* to A* on grids and FD* with respect to their path cost and runtime
(measured in seconds) since A* can easily be adapted to grids that contain unblocked cells with
non-uniform traversal costs and FD* was designed for this case. We compare these path-planning
algorithms on 1000  1000 grids, where each cell is assigned an integer traversal cost from 1 to 15
(corresponding to an unblocked cell) and infinity (corresponding to a blocked cell), similar to the
technique used in the work of Ferguson and Stentz (2006) . If a path lies on the boundary between
two cells with different traversal costs, then we use the smaller traversal cost of the two cells. The
start and goal vertices are the south-west corners of cells. The start vertex is in the south-west cell.
The goal vertex is in a cell randomly chosen from the column of cells furthest east. We average
over 100 random grids. Table 6 (a) reports our results if every traversal cost is chosen with uniform
probability, resulting in small contiguous regions of uniform traversal costs. The path cost and
runtime of FD* are both smaller than those of Basic Theta*. The path cost of A* on grids is only
about 1 percent larger than that of FD* although its runtime is much smaller than that of FD*. Thus,
any-angle planning does not have a large advantage over A* on grids. Table 6(b) reports our results
if traversal cost one is chosen with probability 50 percent and all other traversal costs are chosen
with uniform probability, resulting in large contiguous regions of uniform traversal costs. The path
cost of Basic Theta* is now smaller than that of FD* and its runtime is about the same as that of
FD*. The paths found by FD* tend to have many more unnecessary heading changes in regions
with the same traversal costs than those of Basic Theta*, which outweighs the paths found by Basic
Theta* not having necessary heading changes on the boundary between two cells with different
traversal costs. The path cost of A* on grids is more than 3 percent larger than that of Basic Theta*.
Thus, any-angle planning now has a larger advantage over A* on grids.

9. Trading Off Runtime and Path Length: Exploiting h-Values
There are strategies for trading off runtime and path length that A* on grids and Basic Theta* share.
However, their behavior can be very different even though the two algorithms have very similar
pseudocode. In this section, we develop versions of Basic Theta* that might be able to find shorter
paths at an increase in runtime, including versions that use weighted h-values with weights less than
one, that break ties among vertices with the same f-value in the open list in favor of vertices with
smaller g-values (when they decide which vertex to expand next) and that re-expand vertices whose
f-values have decreased.
562

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

We use all path-planning algorithms with consistent h-values. A* on grids then has the following
properties (Pearl, 1984): The f-value of any expanded vertex is no larger than the f-value of any of
its unexpanded visible neighbors after updating them according to Path 1, which implies that the
f-value of any vertex that is expanded before some other vertex is no larger than the f-value of this
other vertex. Consequently, at any point in time during a search once a vertex has been expanded,
following the parents from the expanded vertex to the start vertex retrieves a shortest path from
the start vertex to the expanded vertex in reverse, which implies that A* cannot find shorter paths
by expanding vertices more than once. Basic Theta* has different properties: The f-value of an
expanded vertex can be larger than the f-value of one or more of its unexpanded visible neighbors
after updating them according to Path 2, which implies that the f-value of a vertex that is expanded
before some other vertex can be larger than the f-value of this other vertex. Consequently, at any
point in time during a search once a vertex has been expanded, following the parents from the
expanded vertex to the start vertex is not guaranteed to retrieve a shortest path from the start vertex
to the vertex in reverse, which implies that Basic Theta* might find shorter paths by expanding
vertices more than once. Figure 19 shows an example. When Basic Theta* expands start vertex C1
with parent C1, it generates vertex B2. Vertex B2 is an unexpanded visible neighbor of vertex C1
that has line-of-sight to vertex C1. Basic Theta* thus updates it according to Path 2 (which is the
same as Path 1 in this case), sets its f-value to f (B2) = 1.41 + 3.16 = 4.57, sets its parent to vertex
C1 and inserts it into the open list (Figure 19(a)). When Basic Theta* later expands vertex B2 with
parent C1, it generates vertex B3. Vertex B3 is an unexpanded visible neighbor of vertex B2 that
has line-of-sight to vertex C1. Basic Theta* thus updates it according to Path 2, sets its f-value to
f (B3) = 2.24 + 2.24 = 4.48, sets its parent to vertex C1 and inserts it into the open list (Figure
19(b)). Thus, the f-value of expanded vertex B2 is indeed larger than the f-value of its unexpanded
visible neighbor B3 after updating it according to Path 2 because the increase in g-value from vertex
B2 to vertex B3 [= 0.83] is less than the decrease in h-value from vertex B2 to vertex B3 [= 0.92].
When Basic Theta* later expands vertex B3, the f-value of vertex B2 [= 4.57] that is expanded
before vertex B3 is indeed larger than the f-value of vertex B3 [= 4.48].
These properties suggest that Basic Theta* might be able to find shorter paths at an increase in
runtime by re-expanding vertices or expanding additional vertices (for example by using weighted
h-values with weights less than one) while A* cannot. At the same time, standard optimizations of
A* that decrease its runtime might also be able to decrease the runtime of Basic Theta* (such as
breaking ties among vertices with the same f-value in the open list in favor of vertices with larger
g-values). In this section we investigate these tradeoffs.
9.1 Weighted h-Values
So far, Basic Theta* has used consistent h-values h(s). A* with consistent h-values finds paths of the
same length no matter how small or large the h-values are. Decreasing consistent h-values typically
increases the number of vertex expansions for A*. We therefore now discuss a version of Basic
Theta* that might be able to find shorter paths at an increase in runtime by using weighted h-values
with weights less than one. This version of Basic Theta* uses the h-values h(s) = w  c(s, sgoal )
for a given weight 0  w < 1 and thus is similar to Weighted A* (Pohl, 1973), except that Weighted
A* typically uses weights greater than one. Figure 20(a) shows an example of the resulting effect
on the number of vertex expansions and path length. The green vertex in the north-east is the start

563

fiDANIEL , NASH , KOENIG , & F ELNER

(a) Expanded vertices by Basic Theta* with different weights
578.6

250000

578.4
200000
578.2

Path Length

150000

577.8

100000
577.6

Vertex Expansions

578

577.4
50000
577.2

577

0
0

0.25

0.5

0.75

0.8

0.85

0.9

0.95

1

w
Basic Theta* Path Length

AP Theta* Path Length

Basic Theta* Vertex Expansions

AP Theta* Vertex Expansions

(b) Random 500  500 grids with 20 percent blocked cells

Figure 20: Weighted h-values

vertex, and the red vertex in the south-west is the goal vertex. Basic Theta* with weight 1.00 (as
used so far) expands the orange vertices and finds the red path. Basic Theta* with weight 0.75
expands the blue vertices and finds the blue path. Thus, Basic Theta* expands more vertices with
564

fiT HETA *: A NY-A NGLE PATH P LANNING

Path Length
Number of Vertex Expansions
Runtime

Smaller g-Values
Basic Theta* AP Theta*
578.41
578.51
18621.61
17698.75
0.3724
0.5350

ON

G RIDS

Larger g-Values
Basic Theta* AP Theta*
578.44
578.55
18668.03
17744.94
0.3829
0.5389

Table 7: Random 500  500 grids with 20 percent blocked cells


weight 0.75 than with weight 1.00 and the resulting path is shorter since it passes through vertices
that are expanded with weight 0.75 but not with weight 1.00.
Figure 20(b) reports the effect of different weights on the path length and number of vertex expansions of Basic Theta* and AP Theta* on random 500  500 grids with 20 percent blocked cells.
(The graphs of the number of vertex expansions of Basic Theta* and AP Theta* nearly coincide.)
Decreasing the weight decreases the path length at an increase in the number of vertex expansions
and thus the runtime. The path length decreases more for AP Theta* than Basic Theta* since AP
Theta* can constrain the angle ranges more than necessary and thus benefits in two ways from expanding more vertices. However, neither Basic Theta* nor AP Theta* are guaranteed to find true
shortest paths even if their weights are zero.
9.2 Tie Breaking
So far, Basic Theta* has broken ties among vertices in the open list with the same f-value in favor
of vertices with larger g-values (when it decides which vertex to expand next). A* with consistent
h-values finds paths of the same length no matter which tie-breaking scheme it uses. Breaking ties
in favor of vertices with smaller g-values typically increases the number of vertex expansions and
thus the runtime. We therefore discuss a version of Basic Theta* that might be able to find shorter
paths at an increase in runtime by breaking ties in favor of vertices with smaller g-values. Figure 21
shows an example of the resulting effect on path length. Vertices C4 and B4 have the same f-value
but vertex B4 has a larger g-value since f (C4) = 3.83+1.41 = 5.24 and f (B4) = 4.24+1 = 5.24.
If Basic Theta* breaks ties in favor of vertices with larger g-values, then it expands vertex B4 with
parent E1 before vertex C4 with parent C3 and eventually expands the goal vertex with parent B4
and terminates. Path extraction then follows the parents from goal vertex B5 to start vertex E1 to
retrieve the dashed red path [E1, B4, B5]. However, if Basic Theta* breaks ties in favor of vertices
with smaller g-values, then it expands vertex C4 with parent C3 before vertex B4 with parent E1
and eventually expands the goal vertex with parent C3 and terminates. Path extraction then follows
the parents from goal vertex B5 to start vertex E1 to retrieve the shorter solid blue path [E1, C3,
B5].
Table 7 reports the effect of the tie-breaking scheme on the path length, number of vertex expansions
and runtime of Basic Theta* and AP Theta* on random 500  500 grids with 20 percent blocked
cells. Breaking ties in favor of vertices with smaller g-values neither changes the path length,
number of vertex expansions nor runtime significantly. The effect of the tie-breaking scheme is
small since fewer vertices have the same f-value for Basic Theta* and AP Theta* than for A* on
grids because the number of possible g-values and h-values is larger for any-angle path planning.
565

fiDANIEL , NASH , KOENIG , & F ELNER

A

1

2

3

B

4

5

s goal

C

D

E

s start

Basic Theta* path (Larger g-values)

Basic Theta* path (Smaller g-values)

Figure 21: Basic Theta* paths for different tie-breaking schemes

Path Length
Number of Vertex Expansions
Runtime

Basic Theta* without Vertex Re-Expansions
578.41
18621.61
0.3724

Basic Theta* with Vertex Re-Expansions
577.60
22836.37
0.5519

Table 8: Random 500  500 grids with 20 percent blocked cells

There is also a second method in which breaking ties can effect path length. So far, Basic Theta*
has chosen Path 2 over Path 1 if an unexpanded visible neighbor of a vertex has line-of-sight to
the parent of the vertex. However, it can choose Path 1 over Path 2 if both paths are equally long,
which increases the runtime due to the additional comparison. Figure 21 shows an example of the
resulting effect on path length. Assume that Basic Theta* expands vertex B4 before vertex C4. If
Basic Theta* chooses Path 2 over Path 1 then it expands vertex B4 with parent E1 and eventually
expands the goal vertex B5 with parent B4 and terminates. Path extraction then follows the parents
from goal vertex B5 to start vertex E1 to retrieve the dashed red path [E1, B4, B5]. However, if
Basic Theta* chooses Path 1 over Path 2 then it expands vertex B4 with parent C3 and eventually
expands goal vertex B5 with parent C3 and terminates. Path extraction then follows the parents
from goal vertex B5 to start vertex E1 to retrieve the shorter solid blue path [E1, C3, B5].
9.3 Re-Expanding Vertices
So far, Basic Theta* has used a closed list to ensure that it expands each vertex at most once. A*
with consistent h-values does not re-expand vertices whether or not it uses a closed list since it
cannot find a shorter path from the start vertex to a vertex after expanding that vertex. On the other
hand, Basic Theta* can re-expand vertices if it does not use a closed list since it can find a shorter
path from the start vertex to a vertex after expanding the vertex. It then re-inserts the vertex into
566

fiT HETA *: A NY-A NGLE PATH P LANNING

A

1

2

3

4

5

6

ON

7

G RIDS

8

B

9

s goal

C

D

E

s start

Basic Theta* path

Basic Theta* path with vertex re-expansions

Figure 22: Basic Theta* paths with and without vertex re-expansions
the open list and eventually re-expands it.2 Figure 22 shows an example of the effect of vertex
re-expansions on path length. Basic Theta* without vertex re-expansions eventually expands vertex
C8 with parent D4. Vertex C9 is an unexpanded visible neighbor of vertex C8 that has line-of-sight
to vertex D4. Basic Theta* without vertex re-expansions thus updates it according to Path 2 and
sets its parent to vertex D4. After termination, path extraction follows the parents from goal vertex
B9 to start vertex E1 to retrieve the dashed red path [E1, D4, C9, B9]. However, Basic Theta* with
vertex re-expansions eventually expands vertex C8 with parent D4 and later re-expands vertex C8
with parent E1. Vertex C9 is a visible neighbor of vertex C8 that has line-of-sight to vertex E1.
Basic Theta* with vertex re-expansions thus updates it according to Path 2 and sets its parent to
vertex E1. After termination, path extraction follows the parents from goal vertex B9 to start vertex
E1 to retrieve the shorter solid blue path [E1, C9, B9].
Theorem 3. Basic Theta* with vertex re-expansions terminates and path extraction returns an
unblocked path from the start vertex to the goal vertex if such a path exists. Otherwise, Basic
Theta* with vertex re-expansions terminates and reports that no unblocked path exists.

Proof. The proof is similar to the proof of Theorem 1. The only property that needs to be proved
differently is that Basic Theta* with vertex re-expansions terminates since it is no longer true that it
can never insert a vertex into the open list again once it has removed the vertex from the open list.
However, since the number of vertices is finite, there are only a finite number of acyclic paths from
the start vertex to each vertex. Therefore, the number of possible g-values is finite. Therefore, Basic
Theta* with vertex re-expansions can reduce the g-value of each vertex only a finite number of times
and thus inserts each vertex into the open list a finite number of times. Thus, the open list eventually
becomes empty and Basic Theta* has to terminate if it has not terminated earlier already.
2. Basic Theta* with vertex re-expansions could also delay the expansion of the goal vertex (for example, by increasing
its f-value artificially) so that it can re-expand more vertices before it terminates but our version of Basic Theta* with
vertex re-expansions does not do that.

567

fiDANIEL , NASH , KOENIG , & F ELNER

Table 8 reports the effect of vertex re-expansions on the path length, number of vertex expansions
and runtime of Basic Theta* on random 500  500 grids with 20 percent blocked cells. Vertex
re-expansions decrease the path length slightly at an increase in the number of vertex expansions
and thus the runtime.

10. Trading Off Runtime and Path Length: Other Approaches
There are additional strategies for trading off runtime and path length that are specific to Basic
Theta*. In this section, we develop versions of Basic Theta* that might be able to find shorter
paths at an increase in runtime by examining more paths, including versions that check for line-ofsight to the parent of a parent, that use key vertices to identify promising parents and that increase
the number of visible neighbors and thus the number of potential parents when updating vertices
according to Path 1.
10.1 Three Paths
So far, Basic Theta* has considered two paths (namely Paths 1 and 2) when it updates the gvalue and parent of an unexpanded visible neighbor s of vertex s. We now discuss a version
of Basic Theta* that considers a third path, namely the path from the start vertex to the parent
of the parent of vertex s [= g(parent(parent(s)))] and from it to vertex s in a straight line [=
c(parent(parent(s)), s )], resulting in a length of g(parent(parent(s))) + c(parent(parent(s)), s ).
This version of Basic Theta* might be able to find shorter paths at an increase in runtime since the
third path is no longer than Path 2 due to the triangle inequality. However, our experimental results
(not reported here) show that the third path does not decrease the path length significantly because
the original version of Basic Theta* already determines that the parent of the parent of vertex s does
not have line-of-sight to some vertex that shares its parent with vertex s. Thus, it is very unlikely
that the parent of the parent of vertex s has line-of-sight to vertex s and thus that the third path is
unblocked.
10.2 Key Vertices
So far, Basic Theta* has considered two paths (namely Paths 1 and 2) when it updates the g-value
and parent of an unexpanded visible neighbor s of vertex s. The parent of a vertex then is either
a visible neighbor of the vertex or the parent of a visible neighbor, which is not always the case
for true shortest paths. We now discuss a version of Basic Theta* that considers additional paths,
namely the paths from the start vertex to cached key vertices and from them to vertex s in a straight
line. This version of Basic Theta* might be able to find shorter paths at an increase in runtime
due to the fact that the parent of a vertex can now also be one of the key vertices. However, our
experimental results (not reported here) show that key vertices decrease the path length only slightly
at a larger increase in runtime due to the overhead of having to select key vertices, maintain them
and consider a larger number of paths.

568

fiT HETA *: A NY-A NGLE PATH P LANNING

(a) Branching factor 4

(b) Branching factor 8

ON

G RIDS

(c) Branching factor 16

Figure 23: Grids with different branching factors

0.6

581
580.5

0.5

580

0.4

579
0.3

578.5

Runtime

Path Length

579.5

578
0.2

577.5
577

0.1

576.5
576

0
4

Basic Theta*

16

Branching Factor
Path Length

Runtime

Figure 24: Basic Theta* on random 500  500 grids with 20 percent blocked cells

10.3 Larger Branching Factors
So far, Basic Theta* has operated on eight-neighbor grids. We now discuss a version of Basic Theta*
that operates on grids with different numbers of neighbors and thus different branching factors.
Figure 23 shows the neighbors of the center vertex for branching factors 4, 8 and 16 respectively.
This version of Basic Theta* might be able to find shorter paths at an increase in runtime since
larger branching factors increase the number of visible neighbors of vertices and thus their number
of potential parents when updating them according to Path 1. Figure 24 reports the effect of larger
branching factors on the path length and runtime of Basic Theta* on random 500  500 grids with
20 percent blocked cells. Larger branching factors indeed decrease the path length at an increase in
runtime.
569

fiDANIEL , NASH , KOENIG , & F ELNER

11. Conclusions
Any-angle path-planning algorithms find paths without artificially constraining the headings of the
paths. We presented two new correct and complete any-angle path-planning algorithms. Basic
Theta* and Angle-Propagation Theta* (AP Theta*) are both variants of A* that propagate information along grid edges (to achieve a short runtime) without constraining paths to grid edges (to find
any-angle paths). Basic Theta* is simple to understand and implement, fast and finds short paths.
However, it is not guaranteed to find true shortest paths. AP Theta* achieves a worst-case complexity per vertex expansion that is constant (like that of A* on grids) rather than linear in the number
of cells (like that of Basic Theta*) by propagating angle ranges when it expands vertices. However,
AP Theta* is more complex than Basic Theta*, is not as fast and finds slightly longer paths.
We proved the correctness and completeness of Basic Theta* and AP Theta* and then compared
them against three existing any-angle path-planning algorithms, namely A* with post-smoothed
paths (A* PS), A* on visibility graphs and Field D* (FD*), the only other version of A* we know
of that propagates information along grid edges without constraining the paths to grid edges. Basic
Theta* and AP Theta* (unlike A* PS) consider paths not constrained to grid edges during their
search and thus can make informed decisions regarding these paths during the search. Basic Theta*
and AP Theta* (unlike FD*) take advantage of the fact that true shortest paths have heading changes
only at the corners of blocked cells.
A* on visibility graphs finds true shortest paths but is slow. On the other hand, A* on grids finds
long paths but is fast. Any-angle path planning lies between these two extremes. Basic Theta*
dominates AP Theta*, A* PS and FD* in terms of their tradeoffs between runtime and path length.
It finds paths that are almost as short as true shortest paths and is almost as fast as A* on grids.
We extended Basic Theta* to find paths from a given start vertex to all other vertices and to find
paths on grids that contain cells with non-uniform traversal costs. The f-value of an expanded vertex
of Basic Theta* (unlike A* on grids) with consistent h-values can be larger than the f-value of one
or more of its unexpanded visible neighbors, which means that Basic Theta* might be able to find
shorter paths at an increase in runtime by re-expanding vertices or expanding additional vertices.
We thus developed versions of Basic Theta* that use weighted h-values with weights less than one,
that break ties among vertices with the same f-value in the open list in favor of vertices with smaller
g-values (when they decide which vertex to expand next), that re-expand vertices whose f-values
have decreased, that check for line-of-sight to the parent of a parent, that use key vertices to identify
promising parents and that increase the number of visible neighbors.
In the future, we intend to develop a worst-case bound on the path lengths of Basic Theta* and AP
Theta*, to better understand their properties and to investigate faster versions of AP Theta* that
perform line-of-sight checks in constant time.

Appendix A. Checking Line-of-Sight
In this appendix, we explain how to perform line-of-sight checks fast. For simplicity, we allow
straight lines to pass between diagonally touching blocked cells. Performing a line-of-sight check
is similar to determining which points to plot on a raster display when drawing a straight line between two points. The plotted points correspond to the cells that the straight line passes through.
570

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

106 LineOfSight(s, s)
107
x0 := s.x;
108
y0 := s.y;
109
x1 := s .x;
110
y1 := s .y;
111
dy := y1  y0 ;
112
dx := x1  x0 ;
113
f := 0;
114
if dy < 0 then
115
dy := dy ;
116
sy := 1;
117
118

else

119
120
121

if dx < 0 then
dx := dx ;
sx := 1;

122
123

else

124
125
126
127
128
129

if dx  dy then
while x0 6= x1 do
f := f + dy ;
if f  dx then
if grid(x0 + ((sx  1)/2), y0 + ((sy  1)/2)) then
return false;

sy := 1;

sx := 1;

y0 := y0 + sy ;
f := f  dx ;

130
131
132
133

if f 6= 0 AN D grid(x0 + ((sx  1)/2), y0 + ((sy  1)/2)) then
return false;

134
135

if dy = 0 AN D grid(x0 + ((sx  1)/2), y0 ) AN D grid(x0 + ((sx  1)/2), y0  1) then
return false;

136

x0 := x0 + sx ;

137
138
139
140
141
142

else
while y0 6= y1 do
f := f + dx ;
if f  dy then
if grid(x0 + ((sx  1)/2), y0 + ((sy  1)/2)) then
return false;
x0 := x0 + sx ;
f := f  dy ;

143
144
145
146

if f 6= 0 AN D grid(x0 + ((sx  1)/2), y0 + ((sy  1)/2)) then
return false;

147
148

if dx = 0 AN D grid(x0 , y0 + ((sy  1)/2)) AN D grid(x0  1, y0 + ((sy  1)/2)) then
return false;

149

y0 := y0 + sy ;

150
return true;
151 end

Algorithm 6: Line-of-sight algorithm

Thus, two vertices have line-of-sight iff none of the plotted points correspond to blocked cells. This
allows Basic Theta* to perform its line-of-sight checks with the standard Bresenham line-drawing
algorithm from computer graphics (Bresenham, 1965), that uses only fast logical and integer operations rather than floating-point operations. Algorithm 6 shows the resulting line-of-sight algorithm,
571

fiDANIEL , NASH , KOENIG , & F ELNER

II

yaxis

I

p
b0

xaxis

b1

b2
b3

b4

b5
b6

b7

b8
b9

s

III

b 10

Upper Boundary

IV
Lower Boundary

Figure 25: Parent, blocked cell and boundary vertices
where s.x and s.y are the x and y coordinates of vertex s, respectively, grid represents the grid and
grid(x, y) is true iff the corresponding cell is blocked.

Appendix B. AP Theta* Returns Unblocked Paths
In this appendix, we prove that AP Theta* never returns a blocked path.
Theorem 4. AP Theta* never returns a blocked path.
Proof. We define a path to be blocked iff at least one vertex on the path does not have line-of-sight
to its successor on the path. Thus, a path is blocked iff at least one of its path segments passes
through the interior of a blocked cell or passes between two blocked cells that share an edge.
We first prove that AP Theta* never returns a path with a path segment that passes through the
interior of a blocked cell. We prove by contradiction that AP Theta* cannot assign some parent p
to some vertex s such that the path segment from parent p to vertex s passes through the interior of
some blocked cell b. Assume otherwise. To simplify the proof, we translate and rotate the grid such
that blocked cell b is immediately south-west of the origin b0 of the grid and parent p is in quadrant
II, as shown in Figure 25. We define the quadrant of a vertex s as follows, where s.x and s.y are the
x and y coordinates of vertex s, respectively:
 Quadrant I is the north-east quadrant (excluding the x-axis) given by s.x  0 and s.y > 0.
 Quadrant II is the north-west quadrant (excluding the y-axis) given by s.x < 0 and s.y  0.
 Quadrant III is the south-west quadrant (excluding the x-axis) given by s.x  0 and s.y < 0.
572

fiT HETA *: A NY-A NGLE PATH P LANNING

northwest(s)
west(s)
southwest(s)

north(s)
s

south(s)

ON

G RIDS

northeast(s)
east(s)
southeast(s)

Figure 26: Neighbors of vertex s
 Quadrant IV is the south-east quadrant (excluding the y-axis but including the origin b0 ) given
by s.x > 0 and s.y  0 or s.x = 0 and s.y = 0.
We refer to the neighbors of vertex s as east(s), northeast(s), north(s), northwest(s), west(s),
southwest(s), south(s), southeast(s), as shown in Figure 26.
Assume that there is a light source at vertex p and that light cannot pass through blocked cell b,
which creates a shadow. A vertex s is in the shadow iff the straight line from parent p to vertex
s passes through the interior of blocked cell b. We distinguish two parts of the perimeter of this
shadow, namely the upper and lower boundary, as shown in Figure 25. We define a boundary vertex
to be any vertex not in the shadow that has at least one neighbor (although not necessarily a visible
neighbor) in the shadow. The origin b0 is not in the shadow but its neighbor south(b0 ) is in the
shadow. Thus, the origin b0 is a boundary vertex. We consider only the upper boundary without
loss of generality. Then, a boundary vertex (to be precise: an upper boundary vertex) is any vertex
s with (s, p, b0 )  0 (that is, on or above the upper boundary and thus outside of the shadow) that
has at least one neighbor s with (s , p, b0 ) > 0 (that is, below the upper boundary and thus inside
of the shadow). It is easy to see that all boundary vertices are in quadrant IV and form an infinite
boundary path [b0 , b1 , . . .] that starts at the origin b0 and repeatedly moves either south or east, that
is, bi+1 = south(bi ) or bi+1 = east(bi ).
We define a vertex s to be sufficiently constrained iff (s, p, b0 )  lb(s) for its parent p. Once
vertex s is sufficiently constrained, it remains sufficiently constrained since no operation of AP
Theta* can decrease its lower angle bound lb(s). We prove in the following that every boundary
vertex is sufficiently constrained at the time it is expanded if it is expanded with parent p. Consider
any vertex s below the upper boundary (that is, (s, p, b0 ) > 0 and thus (b0 , p, s) < 0) that is a
visible neighbor of some boundary vertex bi . Vertex s cannot have been updated according to Path 1
and been assigned parent p at the time its parent p was expanded since the straight line from parent p
to vertex s passes through the interior of a blocked cell and they are therefore not visible neighbors.
It cannot have been updated according to Path 2 and been assigned parent p at the time boundary
vertex bi was expanded with parent p because boundary vertex bi is sufficiently constrained at that
time and thus (bi , p, b0 )  lb(bi ), which implies that (bi , p, s) = (bi , p, b0 ) + (b0 , p, s) <
(bi , p, b0 )  lb(bi ) and the condition on Line 60 remains unsatisfied. Consequently, no vertex in
the shadow can have parent p.
We now prove by induction on the order of the vertex expansions that every boundary vertex is
sufficiently constrained at the time it is expanded if it is expanded with parent p. Assume that
boundary vertex b0 is expanded with parent p. Then, the condition on Line 81 is satisfied and
573

fiDANIEL , NASH , KOENIG , & F ELNER

Line 83 is executed for blocked cell b at the time boundary vertex b0 is expanded with parent p.
Boundary vertex b0 is sufficiently constrained afterwards since its lower angle bound is set to zero.
Now assume that boundary vertex bi with i > 0 is expanded with parent p. Then, boundary vertex bi
cannot be identical to parent p (since they are in different quadrants) nor to the start vertex (since the
start vertex does not have parent p). Boundary vertex bi cannot have been updated according to Path
1 and been assigned parent p at the time its parent p was expanded since p.x < 0 and (bi ).x > 0 and
they are thus not neighbors. Consequently, boundary vertex bi must have been updated according
to Path 2 and been assigned parent p at the time one of its visible neighbors x was expanded with
parent p. Vertex x must be on or above the upper boundary (that is, (x, p, b0 )  0) and cannot be
identical to parent p (since they are in different quadrants). We distinguish two cases:
 Assume that vertex x is a boundary vertex. It is sufficiently constrained at the time it is expanded with parent p according to the induction assumption (that is, (x, p, b0 )  lb(x))
since it is expanded before boundary vertex bi . Boundary vertex bi was updated according
to Path 2 at the time vertex x was expanded with parent p. Thus, the condition on Line
60 is satisfied at that time (that is, lb(x)  (x, p, bi )) and thus lb(x) + (bi , p, x) =
lb(x)  (x, p, bi )  0. Then, the conditions on Lines 88 and 89 are satisfied and
Line 90 is executed with s = x at the time boundary vertex bi is expanded with parent
p. Boundary vertex bi is sufficiently constrained afterwards since its lower angle bound
is set to max(lb(bi ), lb(x) + (bi , p, x)) and (bi , p, b0 ) = (bi , p, x) + (x, p, b0 ) 
lb(x) + (bi , p, x)  max(lb(bi ), lb(x) + (bi , p, x)).
 Assume that vertex x is not a boundary vertex.
Lemma 3. Assume that a vertex s and a boundary vertex bi are visible neighbors, c(p, bi ) <
c(p, s) and (s, p, bi ) < 0. Assume that boundary vertex bi is sufficiently constrained at the
time vertex s is expanded with parent p if boundary vertex bi has been expanded with parent
p at that time. Then, vertex s is sufficiently constrained at the time it is expanded if it is
expanded with parent p.
Proof. Assume that vertex s is expanded with parent p. Then, (s, p, b0 ) = (s, p, bi ) +
(bi , p, b0 ) < 0 since (s, p, bi ) < 0 and (bi , p, b0 )  0. We distinguish two cases:
 Assume that boundary vertex bi is not expanded before vertex s or is expanded with
a parent other than parent p. Then, the conditions on Lines 93 and 94 are satisfied
and Line 95 is executed with s = bi at the time vertex s is expanded with parent
p. Vertex s is sufficiently constrained afterwards since its lower angle bound is set to
max(lb(s), (s, p, bi )) and (s, p, b0 ) = (s, p, bi ) + (bi , p, b0 )  (s, p, bi ) 
max(lb(s), (s, p, bi )).
 Assume that boundary vertex bi is expanded with parent p before vertex s is expanded
with parent p. Boundary vertex bi is sufficiently constrained at the time vertex s is
expanded with parent p according to the premise (that is, (bi , p, b0 )  lb(bi )). Furthermore, lb(bi )  0 (since no operation of AP Theta* can make the lower angle bound
positive) and thus lb(bi ) + (s, p, bi )  0. Then, the conditions on Lines 88 and 89
are satisfied and Line 90 is executed with s = bi at the time vertex s is expanded with
parent p. Vertex s is sufficiently constrained afterwards since its lower angle bound is
574

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

set to max(lb(s), lb(bi ) + (s, p, bi )) and (s, p, b0 ) = (s, p, bi ) + (bi , p, b0 ) 
lb(bi ) + (s, p, bi )  max(lb(s), lb(bi ) + (s, p, bi )).

Boundary vertex bi is either immediately south or east of boundary vertex bi1 since the
boundary path moves only south or east. We distinguish three subcases:
 Assume that parent p is on the x-axis in quadrant II. Then, the boundary path is
along the x-axis. Vertices west(bi ) and east(bi ) are boundary vertices, and vertices
southwest(bi ), south(bi ), and southeast(bi ) are below the upper boundary. Thus,
vertex x is identical to one of vertices northwest(bi ), north(bi ) or northeast(bi ). In
all cases, there is a boundary vertex bj immediately south of vertex x. If vertices x
and bj were not visible neighbors, then there would be blocked cells immediately southwest and south-east of vertex x and vertices x and bi could thus not be visible neighbors.
Thus, vertices x and bj are visible neighbors. Furthermore, boundary vertex bj is immediately south of vertex x and thus c(p, bj ) < c(p, x) and (x, p, bj ) < 0. Finally,
boundary vertex bj is sufficiently constrained according to the induction assumption at
the time boundary vertex bi is expanded with parent p if boundary vertex bj has been
expanded with parent p at that time. Thus, vertex x is sufficiently constrained at the time
it is expanded with parent p according to Lemma 3 (that is, (x, p, b0 )  lb(x)). Consequently, the conditions on Lines 88 and 89 are satisfied (for the reason given before) and
Line 90 is executed with s = x at the time boundary vertex bi is expanded with parent
p. Boundary vertex bi is sufficiently constrained afterwards since its lower angle bound
is set to max(lb(bi ), lb(x) + (bi , p, x)) and (bi , p, b0 ) = (bi , p, x) + (x, p, b0 ) 
lb(x) + (bi , p, x)  max(lb(bi ), lb(x) + (bi , p, x)).
 Assume that parent p is not on the x-axis in quadrant II and that boundary vertex
bi is immediately east of boundary vertex bi1 and thus c(p, bi1 ) < c(p, bi ) and
(bi , p, bi1 ) < 0. Furthermore, boundary vertex bi1 is sufficiently constrained according to the induction assumption at the time boundary vertex bi is expanded with
parent p if boundary vertex bi1 has been expanded with parent p at that time. If boundary vertices bi1 and bi are visible neighbors, then boundary vertex bi is sufficiently
constrained at the time it is expanded with parent p according to Lemma 3. If boundary
vertices bi1 and bi are not visible neighbors, then there must be blocked cells immediately north-west and south-west of boundary vertex bi . Then, Line 81 is satisfied and
Line 83 is executed for the blocked cell immediately south-west of boundary vertex bi at
the time boundary vertex bi is expanded with parent p. Boundary vertex bi is sufficiently
constrained afterwards since its lower angle bound is set to zero.
 Assume that parent p is not on the x-axis in quadrant II and that boundary vertex bi is
immediately south of boundary vertex bi1 .
Lemma 4. Assume that a vertex s in quadrant IV is on or above the upper boundary.
Then, vertex s is a boundary vertex iff the vertex immediately south-west of vertex s is
below the upper boundary.
575

fiDANIEL , NASH , KOENIG , & F ELNER

Proof. If the vertex s immediately south-west of vertex s is below the upper boundary,
then vertex s is a boundary vertex by definition. On the other hand, if vertex s is on
or above the upper boundary (that is, (s , p, b0 )  0), then vertex s is not a boundary
vertex because every neighbor of it is on or above the upper boundary. The neighbors
of vertex s are
east(s), northeast(s), north(s), northwest(s),
west(s), southwest(s), south(s) and southeast(s).
or, equivalently,
east(east(north(s ))), east(east(north(north(s )))), east(north(north(s ))),
north(north(s )), north(s ), s , east(s ) and east(east(s )).
Thus, every neighbor s of vertex s can be reached from vertex s by repeatedly moving either north or east and thus (s , p, s )  0. Consequently, (s , p, b0 ) =
(s , p, s ) + (s , p, b0 )  0 and thus every neighbor s of vertex s is on or above the
upper boundary.
We distinguish two subcases:
 Assume that boundary vertex bi+1 is immediately east of boundary vertex
bi . Vertices north(bi ) and east(bi ) are boundary vertices. Vertices west(bi ),
southwest(bi ) and south(bi ) are south-west of boundary vertices bi1 , bi and bi+1 ,
respectively, and thus below the upper boundary according to Lemma 4. Vertices
northwest(bi ) and southeast(bi ) are either boundary vertices or south-west of
boundary vertices bi2 and bi+2 , respectively, and then below the upper boundary
according to Lemma 4. Thus, vertex x is identical to vertex northwest(bi ).
 Assume that boundary vertex bi+1 is immediately south of boundary vertex bi .
Vertices north(bi ) and south(bi ) are boundary vertices. Vertices west(bi ) and
southwest(bi ) are south-west of boundary vertices bi1 and bi , respectively, and
thus below the upper boundary according to Lemma 4. Vertex northwest(bi ) is
either a boundary vertex or south-west of boundary vertex bi2 and then below the
upper boundary according to Lemma 4. Thus, vertex x is identical to one of vertices
northeast(bi ), east(bi ) or southeast(bi ).
In all cases, vertex x is immediately east of some boundary vertex bj and thus c(p, bj ) <
c(p, x) and (x, p, bj ) < 0. If vertices x and bj were not visible neighbors, then there
would be blocked cells immediately north-west and south-west of vertex x and vertices
x and bi could not be visible neighbors. Thus, vertices x and bj are visible neighbors.
Furthermore, boundary vertex bj is sufficiently constrained according to the induction
assumption at the time boundary vertex bi is expanded with parent p if boundary vertex bj has been expanded with parent p at that time. Thus, vertex x is sufficiently
constrained at the time it is expanded with parent p according to Lemma 3 (that is,
(x, p, b0 )  lb(x)). Consequently, the conditions on Lines 88 and 89 are satisfied (for
the reason given before) and Line 90 is executed with s = x at the time boundary vertex
bi is expanded with parent p. Boundary vertex bi is sufficiently constrained afterwards
since its lower angle bound is set to max(lb(bi ), lb(x) + (bi , p, x)) and (bi , p, b0 ) =
(bi , p, x) + (x, p, b0 )  lb(x) + (bi , p, x)  max(lb(bi ), lb(x) + (bi , p, x)).
576

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

This concludes the proof that every boundary vertex is sufficiently constrained at the time it is
expanded if it is expanded with parent p and thus also the proof that AP Theta* never returns a path
with a path segment that passes through the interior of a blocked cell.
We now prove that AP Theta* never returns a path with a path segment that passes between two
blocked cells that share an edge. We prove by contradiction that AP Theta* cannot assign some
parent p to some vertex s such that the path segment from parent p to vertex s passes between two
blocked cells that share an edge. Assume otherwise and consider the first time AP Theta* assigns
some parent p to some vertex s such that the path segment from parent p to vertex s passes between
two blocked cells that share an edge. The path segment must be either horizontal or vertical. Vertex
s cannot have been updated according to Path 1 and been assigned parent p at the time its parent p
was expanded since then the straight line from parent p to vertex s passes through the interior of a
blocked cell and they are therefore not visible neighbors. It cannot have been updated according to
Path 2 and been assigned parent p at the time some visible neighbor s was expanded with parent p
since then either a) neighbor s would not be colinear with vertices p and s and the straight line from
parent p to vertex s would thus pass through the interior of a blocked cell or b) neighbor s would
be colinear with vertices p and s and the straight line from parent p to vertex s would pass between
two blocked cells that share an edge, which is a contradiction of the assumption. This concludes
the proof that AP Theta* never returns a path with a path segment that passes between two blocked
cells that share an edge.
Thus, AP Theta* never returns a blocked path.

Appendix C. Acknowledgments
This article is an extension of an earlier publication (Nash et al., 2007) and contains additional
expositions, examples and proofs. We thank Vadim Bulitko from the University of Alberta for
making maps from the real-time game Baldurs Gate II available to us. Our research was done while
Ariel Felner spent his sabbatical at the University of Southern California, visiting Sven Koenig.
This research has been partly supported by a U.S. Army Research Laboratory (ARL) and U.S.
Army Research Office (ARO) award to Sven Koenig under grant W911NF-08-1-0468, by a Office
of Naval Research (ONR) award to Sven Koenig under grant N00014-09-1-1031, by a National
Science Foundation (NSF) award to Sven Koenig under grant 0413196 and by an Israeli Science
Foundation (ISF) award to Ariel Felner under grants 728/06 and 305/09. Alex Nash was funded
by the Northrop Grumman Corporation. The views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the official policies, either
expressed or implied, of the sponsoring organizations, agencies, companies or the U.S. government.

References
Aurenhammer, F. (1991). Voronoi diagramsa survey of a fundamental geometric data structure.
ACM Computing Surveys, 23(3), 345405.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near optimal hierarchical path-finding. Journal of
Game Development, 1(1), 122.
577

fiDANIEL , NASH , KOENIG , & F ELNER

Bresenham, J. (1965). Algorithm for computer control of a digital plotter. IBM Systems Journal,
4(1), 2530.
Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding up learning in real-time search via
automatic state abstraction. In Proceedings of the AAAI Conference on Artificial Intelligence,
pp. 13491354.
Choset, H., Lynch, K., Hutchinson, S., Kantor, G., Burgard, W., Kavraki, L., & Thrun, S. (2005).
Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press.
Deloura, M. (2000). Game Programming Gems. Charles River Media.
Ferguson, D., & Stentz, A. (2006). Using interpolation to improve path planning: The Field D*
algorithm. Journal of Field Robotics, 23(2), 79101.
Foley, J., van Dam, A., Feiner, S., & Hughes, J. (1992). Computer Graphics: Principles and Practice. Addison-Wesley.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, SCC-4(2),
100107.
Kavraki, L., Svestka, P., Latombe, J., & Overmars, M. (1996). Probabilistic roadmaps for path
planning in high-dimensional configuration spaces. IEEE Transactions on Robotics and Automation, 12(4), 566580.
Koenig, S., & Likhachev, M. (2002). D* Lite. In Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 476483.
LaValle, S., & Kuffner, J. (2001). Rapidly-exploring random trees: Progress and prospects. In
Donald, B., Lynch, K., & Rus, D. (Eds.), Algorithmic and Computational Robotics: New
Directions, pp. 293308. A K Peters.
Lee, D.-T. (1978). Proximity and reachability in the plane. Ph.D. thesis, University of Illinois at
Urbana-Champaign.
Liu, Y.-H., & Arimoto, S. (1992). Path planning using a tangent graph for mobile robots among
polygonal and curved obstacles. International Journal Robotics Research, 11(4), 376382.
Lozano-Perez, T., & Wesley, M. (1979). An algorithm for planning collision-free paths among
polyhedral obstacles. Communication of the ACM, 22, 560570.
Mitchell, J., & Papadimitriou, C. (1991). The weighted region problem: Finding shortest paths
through a weighted planar subdivision. Journal of the ACM, 38(1), 1873.
Murphy, R. (2000). Introduction to AI Robotics. MIT Press.
Nash, A., Daniel, K., Koenig, S., & Felner, A. (2007). Theta*: Any-angle path planning on grids.
In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 11771183.
Nash, A., Koenig, S., & Likhachev, M. (2009). Incremental Phi*: Incremental any-angle path planning on grids. In Proceedings of the International Joint Conference on Aritificial Intelligence,
pp. 18241830.
Patel, A. (2000).
Amits Game Programming Information.
available online at
http://theory.stanford.edu/amitp/GameProgramming/MapRepresentations.html.
578

fiT HETA *: A NY-A NGLE PATH P LANNING

ON

G RIDS

Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley.
Pohl, I. (1973). The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic
weighting and computational issues in heuristic problem solving. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 1217.
Rabin, S. (2002). AI Game Programming Wisdom. Charles River Media.
Rabin, S. (2004). AI Game Programming Wisdom 2. Charles River Media.
Thorpe, C. (1984). Path relaxation: Path planning for a mobile robot. In Proceedings of the AAAI
Conference on Artificial Intelligence, pp. 318321.
Tozour, P. (2004). Search space representations. In Rabin, S. (Ed.), AI Game Programming Wisdom
2, pp. 85102. Charles River Media.
Yahja, A., Stentz, A., Singh, S., & Brumitt, B. (1998). Framed-quadtree path planning for mobile
robots operating in sparse environments. In Proceedings of the International Conference on
Robotics and Automation, pp. 650655.
Yap, P. (2002). Grid-based path-finding. In Proceedings of the Canadian Conference on Artificial
Intelligence, pp. 4455.

579

fiJournal of Artificial Intelligence Research 39 (2010) 335-371

Submitted 10/09; published 09/10

Active Tuples-based Scheme for Bounding Posterior Beliefs
Bozhena Bidyuk

bbidyuk@google.com

Google Inc.
19540 Jamboree Rd
Irvine, CA 92612

Rina Dechter

dechter@ics.uci.edu

Donald Bren School of Information and Computer Science
University Of California Irvine
Irvine, CA 92697-3425

Emma Rollon

erollon@lsi.upc.edu

Departament de Llenguatges i Sistemes Informatics
Universitat Politecnica de Catalunya
Barcelona, Spain

Abstract
The paper presents a scheme for computing lower and upper bounds on the posterior
marginals in Bayesian networks with discrete variables. Its power lies in its ability to use
any available scheme that bounds the probability of evidence or posterior marginals and
enhance its performance in an anytime manner. The scheme uses the cutset conditioning
principle to tighten existing bounding schemes and to facilitate anytime behavior, utilizing
a fixed number of cutset tuples. The accuracy of the bounds improves as the number of
used cutset tuples increases and so does the computation time. We demonstrate empirically
the value of our scheme for bounding posterior marginals and probability of evidence using
a variant of the bound propagation algorithm as a plug-in scheme.

1. Introduction
This paper addresses the problem of bounding the probability of evidence and posterior
marginals in Bayesian networks with discrete variables. Deriving bounds on posteriors with
a given accuracy is clearly an NP-hard problem (Abdelbar & Hedetniemi, 1998; Dagum
& Luby, 1993; Roth, 1996) and indeed, most available approximation algorithms provide
little or no guarantee on the quality of their approximations. Still, a few approaches were
presented in the past few years for bounding posterior marginals (Horvitz, Suermondt, &
Cooper, 1989; Poole, 1996, 1998; Mannino & Mookerjee, 2002; Mooij & Kappen, 2008) and
for bounding the probability of evidence (Dechter & Rish, 2003; Larkin, 2003; Leisink &
Kappen, 2003).
In this paper we develop a framework that can accept any bounding scheme and improve
its bounds in an anytime manner using the cutset-conditioning principle (Pearl, 1988). To
facilitate our scheme we develop an expression that converts bounds on the probability of
evidence into bounds on posterior marginals.
Given a Bayesian network defined over a set of variables X , a variable X  X , and a
domain value x  D(X), a posterior marginal P (x|e) (where e is a subset of assignments
to the variables, called evidence) can be computed directly from two joint probabilities,
c
2010
AI Access Foundation. All rights reserved.

fiBidyuk, Dechter & Rollon

P (x, e) and P (e):
P (x|e) =

P (x, e)
P (e)

(1)

Given a set C={C1 , ..., Cp }  X of cutset variables (e.g., a loop-cutset), we can compute
the
Q
probability of evidence by enumerating over all the cutset tuples ci  D(C) = pi=1 D(Ci )
using the formula:
M
X
P (ci , e)
(2)
P (e) =
i=1

where M = |D(C)|. We can also compute the posterior marginals using the expression:
P (x|e) =

M
X

P (x|ci , e)P (ci |e)

(3)

i=1

The computation of P (ci , e) for any assignment c = ci is linear in the network size if C
is a loop-cutset and it is exponential in w if C is a w-cutset (see definition in Section 2).
The limitation of the cutset-conditioning method, as defined in Eq. (2) and (3), is that the
number of cutset tuples M grows exponentially with the cutset size.
There are two basic approaches for handling the combinatorial explosion in the cutsetconditioning scheme. One is to sample over the cutset space and subsequently approximate
the distribution P (C|e) from the samples, as shown by Bidyuk and Dechter (2007). The
second approach, which we use here, is to enumerate h out of M tuples and bound the
rest. We shall refer to the selected tuples as active tuples. A lower bound on P (e) can
be obtained by computing exactly the quantities P (ci , e) for 1  i  h resulting in a partial
sum in Eq. (2). This approach is likely to perform well if the selected h tuples contain
most of the probability mass of P (e). However, this approach cannot be applied directly
to obtain the bounds on the posterior marginals in Eq. (3). Even a partial sum in Eq. (3)
requires computing P (ci |e) which in turn requires a normalization constant P (e). We can
obtain naive bounds on posterior marginals from Eq. (1) using P L (e) and P U (e) to denote
available lower and upper bounds over joint probabilities:
P L (x, e)
P U (x, e)

P
(x|e)

P U (e)
P L (e)
However, those bounds usually perform very poorly and often yield an upper bound > 1.
Horvitz et. al (1989) were the first to propose a scheme for bounding posterior marginals
based on a subset of cutset tuples. They proposed to select h highest probability tuples from
P (c) and derived lower and upper bounds on the sum in Eq. (3) from the joint probabilities
P (ci , e) and priors P (ci ) for 1  i  h. Their resulting bounded conditioning algorithm
was shown to compute good bounds on the posterior marginals of some variables in an
Alarm network (with M = 108). However, the intervals between lower and upper bound
values increase as the probability of evidence becomes smaller because the prior distribution
becomes a bad predictor of the high probability tuples in P (C|e) and P (c) becomes a bad
upper bound for P (c, e).
The expression we derive in this paper yields a significantly improved formulation which
results in our Active Tuples Bounds (AT B) framework. The generated bounds facilitate
336

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

anytime performance and are provably tighter than the bounds computed by bounded
conditioning. In addition, our expression accommodates the use of any off-the-shelf scheme
which bounds the probability of evidence. Namely, AT B accepts any algorithm for bounding
P (e) and generates an algorithm that bounds the posterior marginals. Moreover, it can also
tighten the input bounds on P (e).
The time complexity of AT B is linear in the number of active (explored) cutset tuples
h. If the complexity of bounding P (e) is O(T ), then bounding the probability mass of the
unexplored tuples is O(T  h  (d  1)  |C|) where |C| is the number of variables in the cutset
and d is the maximum domain size.
We evaluate our framework experimentally, using a variant of bound propagation (BdP )
(Leisink & Kappen, 2003) as the plug-in bounding scheme. BdP computes bounds by
iteratively solving a linear optimization problem for each variable where the minimum and
maximum of the objective function correspond to lower and upper bounds on the posterior
marginals. The performance of BdP was demonstrated on the Alarm network, the Ising
grid network, and on regular bipartite graphs. Since bound propagation is exponential
in the Markov boundary size, and since it requires solving linear programming problems
many times, its overhead as a plug-in scheme was too high and not cost-effective. We
therefore utilize a variant of bound propagation called ABdP +, introduced by Bidyuk and
Dechter (2006b), that trades accuracy for speed.
We use Gibbs cutset sampling (Bidyuk & Dechter, 2003a, 2003b) for finding highprobability cutset tuples. Other schemes, such as stochastic local search (Kask & Dechter,
1999) can also be used. The investigation into generating high-probability cutset tuples is
outside the primary scope of the paper.
We show empirically that AT B using bound propagation is often superior to bound
propagation alone when both are given comparable time resources. More importantly,
AT Bs accuracy improves with time. We also demonstrate the power of AT B for improving
the bounds on probability of evidence. While the latter is not the main focus of our paper,
lower and upper bounds on the probability of evidence are contained in the expression for
bounding posterior marginals.
The paper is organized as follows. Section 2 provides background on the previously proposed method of bounded conditioning. Section 3 presents and analyzes our AT B framework. Section 4 describes the implementation details of using bound propagation as an
AT B plug-in and presents our empirical evaluation. Section 5 discusses related work, and
Section 6 concludes.

2. Background
For background, we define key concepts and describe the bounded conditioning algorithm
which inspired our work.
2.1 Preliminaries
In this section, we define essential terminology and provide background information on
Bayesian networks.

337

fiBidyuk, Dechter & Rollon

Definition 2.1 (graph concepts) A directed graph is a pair G=< V, E >, where V =
{X1 , ..., Xn } is a set of nodes and E = {(Xi , Xj )|Xi , Xj  V} is the set of edges. Given
(Xi , Xj )  E, Xi is called a parent of Xj , and Xj is called a child of Xi . The set of
Xi s parents is denoted pa(Xi ), or pai , while the set of Xi s children is denoted ch(Xi ), or
chi . The family of Xi includes Xi and its parents. The moral graph of a directed graph
G is the undirected graph obtained by connecting the parents of each of the nodes in G and
removing the arrows. A cycle-cutset of an undirected graph is a subset of nodes that,
when removed, yields a graph without cycles. A loop in a directed graph G is a subgraph
of G whose underlying graph is a cycle (undirected). A directed graph is acyclic if it has
no directed loops. A directed graph is singly-connected (also called a poly-tree), if its
underlying undirected graph has no cycles. Otherwise, it is called multiply-connected.
Definition 2.2 (loop-cutset) A vertex v is a sink with respect to a loop L if the two
edges adjacent to v in L are directed into v. A vertex that is not a sink with respect to a
loop L is called an allowed vertex with respect to L. A loop-cutset of a directed graph G
is a set of vertices that contains at least one allowed vertex with respect to each loop in G.
Definition 2.3 (Bayesian network) Let X = {X1 , ..., Xn } be a set of random variables
over multi-valued domains D(X1 ), ..., D(Xn ). A Bayesian network B (Pearl, 1988) is
a pair <G, P> where G is a directed acyclic graph whose nodes are the variables X and
P = {P (Xi |pai ) | i = 1, ..., n} is the set of conditional probability tables (CPTs) associated
with each Xi . B represents a joint probability distribution having the product form:
P (x1 , ...., xn ) =

n
Y

P (xi |pa(Xi ))

i=1

An evidence e is an instantiated subset of variables E  X .
Definition 2.4 (Markov blanket and Markov boundary) A Markov blanket of Xi
is a subset of variables Y  X such that Xi is conditionally independent of all other variables
given Y . A Markov boundary of Xi is its minimal Markov blanket (Pearl, 1988).
In our following discussion we will identify Markov boundary Xi with the Markov blanket
consisting of Xi s parents, children, and parents of its children.
Definition 2.5 (Relevant Subnetwork) Given evidence e, relevant subnetwork of
Xi relativde to e is a subnetwork of B obtained by removing all descendants of Xi that are
not observed and do not have observed descendants.
If the observations change, the Markov boundary of Xi will stay the same while its
relevant subnetwork may change. As most inference tasks are defined relative to a specific
set of observations e, it is often convenient to restrict attention to the Markov boundary of
Xi in the relevant subnetwork of Xi .
The most common query over Bayesian networks is belief updating which is the task
of computing the posterior distribution P (Xi |e) given evidence e and a query variable
Xi  X . Another query is to compute probability of evidence P (e). Both tasks are NPhard (Cooper, 1990). Finding approximate posterior marginals with a fixed accuracy is also
338

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

NP-hard (Dagum & Luby, 1993; Abdelbar & Hedetniemi, 1998). When the network is a
poly-tree, belief updating and other inference tasks can be accomplished in time linear in
the size of the network. In general, exact inference is exponential in the induced width of
the networks moral graph.
Definition 2.6 (induced width) The width of a node in an ordered undirected graph is
the number of the nodes neighbors that precede it in the ordering. The width of an ordering
o, denoted w(o), is the width over all nodes. The induced width of an ordered graph, w (o),
is the width of the ordered graph obtained by processing the nodes from last to first.
Definition 2.7 (w-cutset) A w-cutset of a Bayesian network B is a subset of variables
C such that, when removed from the moral graph of the network, its induced width is  w.
Throughout the paper, we will consider a Bayesian network over a set of variables X ,
evidence variables E  X and evidence E = e, and a cutset C = {C1 , ..., Cp }  X \E.
Lower-caseQc = {c1 , ..., cp } will denote an arbitrary instantiation of cutset C, and M =
|D(C)| = Ci C |D(Ci )| will denote the number of different cutset tuples.
2.2 Bounded Conditioning

Bounded conditioning (BC) is an anytime scheme for computing posterior bounds in Bayesian
networks proposed by Horvitz et. al (1989). It is derived from the loop-cutset conditioning
method (see Eq. 3). Given a node X  X and a domain value x  D(X), they derive
bounds from the following formula:
P (x|e) =

M
X

i

i

P (x|c , e)P (c |e) =

h
X

i

i

P (x|c , e)P (c |e) +

i=1

i=1

M
X

P (x|ci , e)P (ci |e)

(4)

i=h+1

The hard-to-compute P (ci |e) is replaced for i  h with a normalization formula:
Ph

i
i
i=1 P (x|c , e)P (c , e)
P
M
i
i
i=1 P (c , e) +
i=h+1 P (c , e)

P (x|e) = Ph

+

M
X

P (x|ci , e)P (ci |e)

(5)

i=h+1

BC computes exactly P (ci , e) and P (x|ci , e) for the h cutsetPtuples and bounds the rest.
i
The lower bound is obtained from Eq. (5) by replacing M
i=h+1 P (c , e) in the denomiPM
i
nator with the sum of priors i=h+1 P (c ) and simply dropping the sum on the right:
L
PBC
(x|e)

Ph

i
i=1 P (x, c , e)
P
M
i
i
i=h+1 P (c )
i=1 P (c , e) +

, Ph

(6)

P
i
The upper bound is obtained from Eq. (5) by replacing M
i=h+1 P (c , e) in the denominator with a zero, and replacing P (x|ci , e) and P (ci |e) for i > h with the upper bounds of
1 and a derived upper bound (not provided here) respectively:
PM
Ph
i
i
i=h+1 P (c )
U
i=1 P (x, c , e)
PBC (x|e) , Ph
+ Ph
Ph
i
L i
U i
i=1 P (c , e)
i=1 P (c |e) + 1 
i=1 P (c |e)
339

fiBidyuk, Dechter & Rollon

Applying definitions for P L (ci |e) =

Ph

i=1

P (ci ,e)
PM

P (ci ,e)+

i=h+1

P (ci )

and P U (ci |e) =

P (ci ,e)
Ph
i
i=1 P (c ,e)

Horvitz et al. (1989), we get:
P
Ph
PM
Ph
i
i
i
i
( M
i=h+1 P (c ))( i=1 P (c , e) +
i=h+1 P (c ))
U
i=1 P (x, c , e)
+
PBC (x|e) , Ph
P
h
i
i
i=1 P (c , e)
i=1 P (c , e)

from

(7)

The bounds expressed in Eq. (6) and (7) converge to the exact posterior marginals as
h  M . However, we can show that,

Theorem 2.1 (bounded conditioning bounds interval) The interval between lower and
upper bounds computed by bounded conditioning is lower bounded by the probability mass of
prior distribution P (C) of the unexplored cutset tuples:
U
h, PBC
(x|e)



L
PBC
(x|e)



M
X

P (ci )

i=h+1

Proof. See Appendix A.



3. Architecture for Active Tuples Bounds
In this section, we describe our Active Tuples Bounds (AT B) framework. It builds on the
same principles as bounded conditioning. Namely, given a cutset C and some method for
generating h cutset tuples, the probabilities P (c, e) of the h tuples are evaluated exactly and
the rest are upper and lower bounded. The worst bounds on P (c, e) are the lower bound of
0 and the upper bound of P (c). AT B bounds can be improved by using a plug-in algorithm
that computes tighter bounds on the participating joint probabilities. It always computes
tighter bounds than bounded conditioning, even when using 0 and P (c) to bound P (c, e).
For the rest of the section, c1:q = {c1 , ..., cq } with q < |C| denotes a generic partial
instantiation of the first q variables in C, while ci1:q indicates a particular partial assignment.
Given h cutset tuples, 0  h  M , that we assume without loss of generality to be the
first h tuples according to some enumeration order, a variable X  X \E and x  D(X), we
can rewrite Eq. (3) as:
Ph
PM
PM
i
i
i
i=1 P (x, c , e) +
i=h+1 P (x, c , e)
i=1 P (x, c , e)
P (x|e) = PM
(8)
= Ph
P
M
i
i
i
i=1 P (c , e)
i=h+1 P (c , e)
i=1 P (c , e) +

The probabilities P (x, ci , e) and P (ci , e), 1  i  h, can be computed in polynomial time if
C is a loop-cutset and in timePand space exponentialPin w if C is a w-cutset. The question
M
i
i
is how to compute or bound M
i=h+1 P (c , e) in an efficient manner.
i=h+1 P (x, c , e) and
h+1
Our approach first replaces the sums over the tuples c ,...,cM with a sum over a
polynomial number (in h) of partially-instantiated tuples. From that, we develop new
expressions for lower and upper bounds on the posterior marginals as a function of the
lower and upper bounds on the joint probabilities P (x, c1:q , e) and P (c1:q , e). We assume in
our derivation that there is an algorithm A that can compute those bounds, and refer to
them as PAL (x, c1:q , e) (resp. PAL (c1:q , e)) and PAU (x, c1:q , e) (resp. PAU (c1:q , e)) respectively.
340

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

C1
0

1

C2
0

1

2

C3

C3

0

1

0

1

C4
0

C4
1

0

1

Figure 1: A search tree for cutset C = {C1 , ..., C4 }.
3.1 Bounding the Number of Processed Tuples
We will now formally define partially-insantiated tuples and replace the sum over the exponential number of uninstantiated tuples (h + 1 through M ) with a sum over polynomial
number of partially-instantiated tuples (h + 1 through M 0 ) in Eq. 8.
Consider a fully-expanded search tree of depth |C| over the cutset search space expanded
in the order C1 ,...,Cp . A path from the root to the leaf at depth |C| corresponds to a full
cutset tuple. We will call such a path an active path and the corresponding tuple an active
tuple. We can obtain the truncated search tree by trimming all branches that are not on
the active paths:
Definition 3.1 (truncated search tree) Given a search tree T covering the search space
H over variables Y = {Y1 , . . . , Ym }  X , a truncated search tree relative to a subset
S = {y 1 , ..., y t }  D(Y1 )  ...  D(Ym ) of full assignments, is obtained by marking the edges
on all the paths appearing in S and removing all unmarked edges and nodes except those
emanating from marked nodes.
Let S = {c1 , . . . , ch }. Clearly, the leaves at depth q < |C| in the truncated search
tree relative to S correspond to the partially instantiated cutset tuples c1:q which are not
extended to full cutset assignments.
Example 3.1 Consider a Bayesian network B with cutset variables C={C1 , ..., C4 }, domain values D(C1 )=D(C3 )=D(C4 )={0, 1}, D(C2 )={0, 1, 2}, and four fully-instantiated tuples {0, 1, 0, 0}, {0, 1, 0, 1}, {0, 2, 1, 0}, {0, 2, 1, 1}. Figure 1 shows its truncated search tree,
where the remaining partially instantiated tuples are {0, 0}, {0, 1, 1}, {0, 2, 0}, and {1}.
Proposition 3.1 Let C be a cutset, d be the maximum domain size, and h be the number
of generated cutset tuples. Then the number of partially-instantiated cutset tuples in the
truncated search tree is bounded by O(h  (d  1)  |C|).
341

fiBidyuk, Dechter & Rollon

Proof. Since every node in the path from the root C1 to a leaf Cp can not have more than
(d  1) emanating leaves, the theorem clearly holds. 
Let M 0 be the number of truncated tuples. We can enumerate the partially instantiated
tuples, denoting the j-th tuple as cj1:qj , 1  j  M 0 , where qj is the tuples length. Clearly,
the probability mass over the cutset tuples ch+1 , ..., cM can be captured by a sum over the
truncated tuples. Namely:
Proposition 3.2
M
X

0

i

P (c , e) =

P (cj1:qj , e)

(9)

j=1

i=h+1
M
X

M
X
0

i

P (x, c , e) =

M
X

P (x, cj1:qj , e)

(10)

j=1

i=h+1


Therefore, we can bound the sums over the tuples h + 1 through M in Eq. (8) by bounding
a polynomial (in h) number of partially-instantiated tuples as follows,
P (x|e) =

Ph

i
i=1 P (x, c , e)
Ph
i
i=1 P (c , e)

+
+

PM 0

j
j=1 P (x, c1:qj , e)
PM 0
j
j=1 P (c1:qj , e)

(11)

3.2 Bounding the Probability over the Truncated Tuples

In the following, we develop lower and upper bound expressions used by AT B.
3.2.1 Lower Bounds
First, we decompose P (cj1:qj , e), 0  j  M 0 , as follows. Given a variable X  X and a
distinguished value x  D(X):
P (cj1:qj , e) =

X

P (x0 , cj1:qj , e) = P (x, cj1:qj , e) +

X

P (x0 , cj1:qj , e)

(12)

x0 6=x

x0 D(X)

Replacing P (cj1:qj , e) in Eq. (11) with the right-hand side of Eq. (12), we get:
P (x|e) = Ph

i=1 P (c

PM 0
j
i
j=1 P (x, c1:qj , e)
i=1 P (x, c , e) +
PM 0 P
P 0
j
0 j
+ M
j=1
x0 6=x P (x , c1:qj , e)
j=1 P (x, c1:qj , e) +

Ph

i , e)

(13)

We will use the following two lemmas:

Lemma 3.1 Given positive numbers a > 0, b > 0,   0, if a < b, then:

342

a
b



a+
b+ .



fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Lemma 3.2 Given positive numbers a, b, ,  L ,  U , if a < b and  L     U , then:
a + L
a+
a + U


b + L
b+
b + U


The proof of both lemmas is straight forward.
Lemma 3.2 says that if the sums in the numerator and denominator have some component  in common, then replacing  with a larger value  U in both the numerator and the
denominator yields a larger fraction. Replacing  with a smaller value  L in both places
yields a smaller fraction.
Observe now that in Eq. (13) the sums in both the numerator and the denominator
contain P (x, cj1:qj , e). Hence, we can apply Lemma 3.2. We will obtain a lower bound by
replacing P (x, cj1:qj , e), 1  j  M 0 , in Eq. (13) with corresponding lower bounds in both
numerator and denominator, yielding:
h
X

0

i

P (x, c , e) +

i=1

P (x|e) 

h
X

P (ci , e) +

M
X

PAL (x, cj1:qj , e)

j=1

M0
X

PAL (x, cj1:qj , e) +

P (x0 , cj1:qj , e)

j=1 x0 6=x

j=1

i=1

(14)

0

M X
X

P
Subsequently, grouping PAL (x, cj1:qj , e) and x0 6=x P (x0 , cj1:qj , e) under one sum and replacing
P
PAL (x, cj1:qj , e) + x0 6=x P (x0 , cj1:qj , e) with its corresponding upper bound (increasing denominator), we obtain:

P (x|e) 

h
X

0

P (x, c , e) +

i=1

h
X

i

P (c , e) +

i

M0
X

M
X

PAL (x, cj1:qj , e)

j=1

U B[PAL (x, cj1:qj , e)

j=1

i=1

, PAL (x|e)
+

X

P (x

0

(15)

, cj1:qj , e)]

x0 6=x

where upper bound UB can be obtained as follows:
(
P
X
PAL (x, cj1:qj , e) + x0 6=x PAU (x0 , cj1:qj , e)
j
0 j
L
P (x , c1:qj , e)] , min
U B[PA (x, c1:qj , e) +
PAU (cj1:qj , e)
0
x 6=x

(16)
P
0 , cj , e). The
U (x0 , cj , e) is, obviously, an upper bound of
P
(x
P
1:qj
1:qj
x0 6=x
x0 6=x A
P
j
j
j
U
L
0
value PA (c1:qj , e) is also an upper bound since PA (x, c1:qj , e)+ x0 6=x P (x , c1:qj , e)  P (cj1:qj , e)

The value

P

 PAU (cj1:qj , e). Neither bound expression in Eq. (16) dominates the other. Thus, we compute the minimum of the two values.
343

fiBidyuk, Dechter & Rollon

Please note that the numerator in Eq. (15) above also provides an anytime lower bound
on the joint probability P (x, e) and can be used to compute a lower bound on the probability
of evidence. In general, a lower bound denoted PAL (e) is obtained by:
P (e) 

h
X

0

i

P (c , e) +

M
X

PAL (cj1:qj , e) , PAL (e)

(17)

j=1

i=1

3.2.2 Upper Bound
The upper bound expression can be obtained in a similar manner. Since both numerator
and denominator in Eq. (13) contain addends P (x, cj1:qj , e), using Lemma 3.2 we replace
each P (x, cj1:qj , e) with an upper bound PAU (x, cj1:qj , e) yielding:

P (x|e) 

h
X

0

i

P (x, c , e) +

i

P (c , e) +

PAU (x, cj1:qj , e)

j=1

i=1

h
X

M
X

M0
X

PAU (x, cj1:qj , e)

+

P (x

0

, cj1:qj , e)

j=1 x0 6=x

j=1

i=1

(18)

0

M X
X

Subsequently, replacing each P (x0 , cj1:qj , e), x0 6= x, with a lower bound (reducing denominator), we obtain a new upper bound expression on P (x|e):

P (x|e) 

h
X

0

i

P (x, c , e) +

i=1

P (ci , e) +

PAU (x, cj1:qj , e)

j=1

i=1

h
X

M
X

M0
X

PAU (x, cj1:qj , e) +

M0
X

, PAU (x|e)
X

(19)

PAL (x0 , cj1:qj , e)

j=1 x0 6=x

j=1

Similar to the lower bound, the numerator in the upper bound expression PAU (x|e) provides an anytime upper bound on the joint probability P (x, ci , e) which can be generalized
to upper bound the probability of evidence:
P (e) 

h
X
i=1

0

i

P (c , e) +

M
X

PAU (cj1:qj , e) , PAU (e)

(20)

j=1

The derived bounds PAL (x|e) and PAU (x|e) are never worse than those obtained by bounded
conditioning, as we will show in Section 3.4.
3.3 Algorithmic Description
Figure 2 summarizes the active tuples-based bounding scheme AT B. In steps 1 and 2, we
generate h fully-instantiated cutset tuples and compute exactly the probabilities P (ci , e) and
P (X, ci , e) for i  h, X  X \(C  E), using, for example, the bucket-elimination algorithm
(Dechter, 1999). In step 3, we compute bounds on the partially instantiated tuples using
algorithm A. In step 4, we compute the lower and upper bounds on the posterior marginals
using expressions (15) and (19), respectively.
344

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Active Tuples-based Bounds Architecture
Input: A Bayesian network (B), variables X , evidence E  X , cutset C  X \E, constant
h, algorithm A for computing lower and upper bounds on joint probabilities.
Output: lower bounds P L , upper bounds P U .
1. Generate h cutset tuples.
2. Compute:
Ph
S  i=1 P (ci , e)
Ph
Sx  i=1 P (x, ci , e), x  D(X), X  X \(C  E)
3. Traverse partially-instantiated tuples:
0
3.1 Generate the truncated tree associated with the h tuples and let c11:q1 , ..., cM
1:qM 0
be the M 0 partial assignments.
3.2 x  D(X), X  X \(C  E), compute:
PM 0 L
(x, cj1:qj , e)
LBA (x)  j=1 PA
U BA (x) 

0
U BA
(x) 

4. Compute bounds:

PM 0

j=1

PM 0

j=1

U
PA
(x, cj1:qj , e)

U B[P (x, cj1:qj , e) +

L
PA
(x|e)
U
PA
(x|e)

=
=

P

x0 6=x

PA (x0 , cj1:qj , e)]

Sx +LBA (x)
0 (x)
S+U BA

Sx +U BA (x)
S+U BA (x)+LBA (x)

L
u
5. Output {PA
(x|e)} and {PA
(x|e)}.

Figure 2: Active Tuples Bounds Architecture

Example 3.2 Consider again the Bayesian network B described in Example 3.1. Recall
that B has a cutset C = {C1 , ..., C4 } with domains D(C1 ) = D(C3 ) = D(C4 ) = {0, 1} and
D(C2 ) = {0, 1, 2}. The total number of cutset tuples is M = 24. Let X 6 C be a variable in
B with domain D(X) = {x, x0 }. We will compute bounds on P (x|e). Assume we generated
the same four cutset tuples (h = 4) as before:

c1 = {C1 = 0, C2 = 1, C3 = 0, C4 = 0} = {0, 1, 0, 0}
c2 = {C1 = 0, C2 = 1, C3 = 0, C4 = 1} = {0, 1, 0, 1}
c3 = {C1 = 0, C2 = 2, C3 = 1, C4 = 0} = {0, 2, 1, 0}
c4 = {C1 = 0, C2 = 2, C3 = 1, C4 = 1} = {0, 2, 1, 1}

The corresponding truncated search tree is shown in Figure 1. For the tuple {0, 1, 0, 0},
we compute exactly the probabilities P (x, C1 =0, C2 =1, C3 =0, C4 =0, e) and P (C1 =0, C2 =1,
C3 = 0, C4 = 0). Similarly, we obtain exact probabilities P (x, C1 = 0, C2 = 1, C3 = 0,
C4 = 1) and P (C1 = 0, C2 = 1, C3 = 0, C4 = 1) for the second cutset instance {0, 1, 0, 1}.
345

fiBidyuk, Dechter & Rollon

Since h = 4,

Ph

i=1 P (x

4
X

0 , ci , e)

and

Ph

i=1 P (c

i , e)

are:

P (x, ci , e) = P (x, c1 , e) + P (x, c2 , e) + P (x, c3 , e) + P (x, c4 , e)

i=1

4
X

P (ci , e) = P (c1 , e) + P (c2 , e) + P (c3 , e) + P (c4 , e)

i=1

The remaining partial tuples are: c11:2 = {0, 0}, c21:3 = {0, 1, 1}, c31:3 = {0, 2, 0}, and c41:1 =
{1}. Since these 4 tuples are not full cutsets, we compute bounds on their joint probabilities.
Using the same notation as in Figure 2, the sums over the partially instantiated tuples will
have the form:
U BA (x) , PAU (x, c11:2 , e) + PAU (x, c21:3 , e) + PAU (x, c31:3 , e) + PAU (x, c41:1 , e)
LBA (x) , PAL (x, c11:2 , e) + PAL (x, c21:3 , e) + PAL (x, c31:3 , e) + PAL (x, c41:1 , e)
From Eq. (19) we get:
PAU (x|e)

P4

i
i=1 P (x, c , e) + U BA (x)
i
0
i=1 P (c , e) + U BA (x) + LBA (x )

= P4

From Eq. (15) and (16) we get:
PAL (x|e)

P4

i
i=1 P (x, c , e) + LBA (x)
i
0
i=1 P (c , e) + LBA (x) + U BA (x )

= P4

The total number of tuples processed is M 0 = 4 + 4 = 8 < 24.
3.4 AT B Properties
In this section we analyze the time complexity of the AT B framework, evaluate its worstcase lower and upper bounds, and analyze the monotonicity properties of its bounds interval
(as a function of h).
Theorem 3.1 (complexity) Given an algorithm A that computes lower and upper bounds
on joint probabilities P (c1:qi , e) and P (x, c1:qi , e) in time O(T ), and a loop-cutset C, PAL (x|e)
and PAU (x|e) are computed in time O(h  N + T  h  (d  1)  |C|) where d is the maximum
domain size and N is the problem input size.
Proof. Since C is a loop-cutset, the exact probabilities P (ci , e) and P (x, ci , e) can be
computed in time O(N ). From Proposition 3.1, there are O(h  (d  1)  |C|) partiallyinstantiated tuples. Since algorithm A computes upper and lower bounds on P (cj1:qj , e) and
P (x, cj1:qj , e) in time O(T ), the bounds on partially-instantiated tuples can be computed in
time O(T  h  (d  1)  |C|)). 

346

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Let the plug-in algorithm A be a brute-force algorithm, denoted BF , that trivially
L (x, cj , e) = 0, P U (x, cj , e) = P (cj ), and U B[P (cj , e)] = P (cj ).
instantiates PBF
1:qj
1:qj
1:qj
1:qj
1:qj
BF
Then, from Eq. (15):
L
PBF
(x|e)

,

Ph

i
i=1 P (x, c , e)
Ph
P
j
M0
i
i=1 P (c , e) +
j=1 P (c1:qj )

while from Eq. (19):

U
PBF
(x|e)

,

PM 0
j
i
j=1 P (c1:qj )
i=1 P (x, c , e) +
Ph
PM 0
j
i
i=1 P (c , e) +
j=1 P (c1:qj )

Ph

Ph

i
i=1 P (x, c , e)
P
M
j
i
j=h+1 P (c )
i=1 P (c , e) +

= Ph

=

Ph

PM
i
j
i=1 P (x, c , e) +
j=h+1 P (c )
Ph
PM
i
j
i=1 P (c , e) +
j=h+1 P (c )

(21)

(22)

Assuming that algorithm A computes bounds at least as good as those computed by
L (x|e) and P U (x|e) are the worst-case bounds computed by AT B.
BF , PBF
BF
Now, we are ready to compute an upper bound on the AT B bounds interval:
Theorem 3.2 (AT B bounds interval upper bound) AT B length of the interval between its lower and upper bounds is upper bounded by a monotonic non-increasing function
of h:
PM
j
j=h+1 P (c )
U
L
PA (x|e)  PA (x|e)  Ph
, Ih
PM
i , e) +
j)
P
(c
P
(c
i=1
j=h+1

Proof. See Appendix C.



Next we show that AT B lower and upper bounds are as good or better than the bounds
computed by BC.
L (x|e).
Theorem 3.3 (tighter lower bound) PAL (x|e)  PBC
L (x|e) is the worst-case lower bound computed by AT B. Since P L (x|e) =
Proof. PBF
BF
L
L (x|e), then P L (x|e)  P L (x|e).
PBC (x|e), and PAL (x|e)  PBF

A
BC

U (x|e).
Theorem 3.4 (tighter upper bound) PAU (x|e)  PBC
U (x|e) is the worst-case upper bound computed by AT B. Since P U (x|e) 
Proof. PBF
BF
U (x|e) due to lemma 3.1, it follows that P U (x|e)  P U (x|e).
PBC

A
BC

4. Experimental Evaluation
The purpose of the experiments is to evaluate the performance of our AT B framework on
the two probabilistic tasks of single-variable posterior marginals and probability of evidence.
The experiments on the first task were conducted on 1.8Ghz CPU with 512 MB RAM, while
the experiments on the second task were conducted on 2.66GHz CPU with 2GB RAM.
347

fiBidyuk, Dechter & Rollon

Recall that AT B has a control parameter h that fixes the number of cutset tuples for
which the algorithm computes its exact joint probability. Given a fixed h, the quality of
the bounds will presumably depend on the ability to select h high probability cutset tuples.
In our implementation, we use an optimized version of Gibbs sampling, that during the
sampling process maintains a list of the h tuples having the highest joint probability. As
noted, other schemes should be considered for this subtask as part of the future work. We
obtain the loop-cutset using mga algorithm (Becker & Geiger, 1996).
Before we report the results, we describe bound propagation and its variants, which we
use as a plug-in algorithm A and also as a stand-alone bounding scheme.
4.1 Bound Propagation
Bound propagation (BdP ) (Leisink & Kappen, 2003) is an iterative algorithm that bounds
the posterior marginals of a variable. The bounds are initialized to 0 and 1 and are iteratively
improved by solving a linear optimization problem for each variable X  X such that the
minimum and maximum of the objective function correspond to the lower and upper bound
on the posterior marginal P (x|e), x  D(X).
We cannot directly plug BdP into AT B to bound P (c1:q , e) because it only bounds
conditional probabilities. Thus, we factorize P (c1:q , e) as follows:
P (c1:q , e) =

Y

P (ej |e1 , . . . , ej1 , c1:q )P (c1:q )

ej E

Each factor P (ej |e1 , . . . , ej1 , c1:q ) can be bounded by BdP , while P (c1:q ) can be computed
L
exactly since the relevant subnetwork over c1:q (see Def. 2.5) is singly connected. Let PBdP
U
and PBdP denote the lower and upper bounds computed by BdP on some marginal. The
bounds BdP computes on the joint probability are:
Y

L
(ej |e1 , . . . , ej1 , c1:q )P (c1:q )  P (c1:q , e) 
PBdP

Y

U
(ej |e1 , . . . , ej1 , c1:q )P (c1:q )
PBdP

ej E

ej E

Note that BdP has to bound a large number of tuples when plugged into AT B, and therefore, solve a large number of linear optimization problems. The number of variables in each
problem is exponential in the size of the Markov blanket of X.
As a baseline for comparison with AT B, we use in our experiments a variant of bound
propagation called BdP + (Bidyuk & Dechter, 2006b) that exploits the structure of the
network to restrict the computation of P (x|e) to the relevant subnetwork of X (see Def. 2.5).
The Markov boundary of X (see Def. 2.4) within relevant subnetwork of X does not include
the children of X that are not observed and have no observed descedants; therefore, it is
a subnetwork of the Markov boundary in the original network. Sometimes, the Markov
boundary of X is still too big to compute under limited memory resouces. BdP + uses
a parameter k to specify the maximum size of the Markov boundary domain space. The
algorithm skips the variables whose Markov boundary domain size exceeds k, and so their
lower and upper bound values remain 0 and 1, respectively. When some variables are
skipped, the bounds computed by BdP + for the remaining variables may be less accurate.
348

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

network

N

w

|LC|

|D(LC)|

|E|

Time(BE)

Time(LC)

Alarm
Barley
cpcs54
cpcs179
cpcs360b
cpcs422b
Munin3
Munin4

37
48
54
179
360
422
1044
1041

4
7
15
8
21
22
7
8

5
12
15
8
26
47
30
49

108
> 227
32768
49152
26
27
> 230
> 249

1-4
4-8
2-8
12-24
11-23
4-10
257
235

0.01 sec
50 sec
1 sec
2 sec
20 min
50 min
8 sec
70 sec

0.05 sec
>22 hrs1
22 sec
37 sec
> 8 hrs1
> 2  109 hrs1
> 1700 hrs1
> 1  108 hrs1

Table 1: Complexity characteristics of the benchmarks from the UAI repository: N -number
of nodes, w -induced width, |LC|-number of nodes in a loop-cutset, |D(LC)|loop-cutset state space size, Time(BE) is the exact computation time via bucket
elimination, Time(LC) is the exact computation time via loop-cutset conditioning.
The results are averaged over a set of network instances with different evidence.
Evidence nodes and their values are selected at random.

Our preliminary tests showed that plugging BdP + into AT B is timewise infeasible
(even for small k). Instead, we developed and used a different version of bound propagation
called ABdP + (Bidyuk & Dechter, 2006b) as a plug-in algorithm A, which was more costeffective in terms of accuracy and time overhead. ABdP + includes the same enhancements
as BdP +, but solves the linear optimization problem for each variable using an approximation algorithm. This implies that we obtain bounds faster but they are not as accurate.
Roughly, the relaxed linear optimization problem can be described as a fractional packing and covering with multiple knapsacks and solved by a fast greedy algorithm (Bidyuk
& Dechter, 2006b). ABdP + is also parameterized by k to control the maximum size of
the linear optimization problem. Thus, AT B using ABdP + as a plug-in has two control
parameters: h and k.
4.2 Bounding Single-Variable Marginals
We compare the performance of the following three algorithms: AT B (with ABdP + as
a plug-in), BdP +, as described in the previous section, and BBdP + (Bidyuk & Dechter,
2006a). The latter is a combination of AT B and BdP +. First, we run algorithm AT B
with ABdP + plug-in. Then, we use the bounds computed by AT B to initialize bounds
in BdP + (instead of 0 and 1) and run BdP +. Note that, given fixed values of h and k,
BBdP + will always compute tighter bounds than either AT B and BdP +. Our goal is
to analyze its trade-off between the increase of the bounds accuracy and the computation
time overhead. We also compare with approximate decomposition (AD) (Larkin, 2003)
whenever it is feasible and relevant. We did not include the results for the stand-alone
ABdP + since our objective was to compare AT B bounds with the most accurate bounds
obtained by bound propagation. Bidyuk (2006) provides additional comparison with various
refinements of BdP (Bidyuk & Dechter, 2006b) mentioned earlier.

1. Times are extrapolated.

349

fiBidyuk, Dechter & Rollon

4.2.1 Benchmarks
We tested our framework on four different benchmarks: Alarm, Barley, CPCS, and Munin.
Alarm network is a model for monitoring patients undergoing surgery in an operating room
(Beinlich, Suermondt, Chavez, & Cooper, 1989). Barley network is a part of the decisionsupport system for growing malting barley (Kristensen & Rasmussen, 2002). CPCS networks are derived from the Computer-Based Patient Care Simulation system and based
on INTERNIST-1 and Quick Medical Reference Expert systems (Pradhan, Provan, Middleton, & Henrion, 1994). We experiment with cpcs54, cpcs179, cpcs360b, and cpcs422b
networks. Munin networks are a part of the expert system for computer-aided electromyography (Andreassen, Jensen, Andersen, Falck, Kjaerulff, Woldbye, Srensen, Rosenfalck, &
Jensen, 1990). We experiment with Munin3 and Munin4 networks. For each network, we
generated 20 different sets of evidence variables picked at random. For Barley network, we
select evidence variables as defined by Kristensen and Rasmussen (2002).
Table 1 summarizes the characteristic of each network. For each one, the table specifies
the number of variables N , the induced width w , the size of loop cutset |LC|, the number of
loop-cutset tuples |D(LC)|, and the time needed to compute the exact posterior marginals
by bucket-tree elimination (exponential in the induced width w ) and by cutset conditioning
(exponential in the size of loop-cutset).
Computing the posterior marginals exactly is easy in Alarm network, cpcs54, and
cpcs179 using either bucket elimination or cutset conditioning since they have small induced width and a small loop-cutset. We include those benchmarks as a proof of concept
only. Several other networks, Barley, Munin3, and Munin4, also have small induced width
and, hence, their exact posterior marginals can be obtained by bucket elimination. However, since AT B is linear in space, it should be compared against linear-space schemes such
as cutset-conditioning. From this perspective, Barley, Munin3, and Munin4 are hard. For
example, Barley network has only 48 variables, its induced width is w = 7, and exact inference by bucket elimination takes only 30 seconds. Its loop-cutset contains only 12 variables,
but the number of loop-cutset tuples exceeds 2 million because some variables have large
domain sizes (up to 67 values). Enumerating and computing all cutset tuples, at a rate of
about 1000 tuples per second, would take over 22 hours. Similar considerations apply in
case of Munin3 and Munin4 networks.
4.2.2 Measures of Performance
We measure the quality of the bounds via the average length of the interval between lower
and upper bound:
P
P
U
L
XX
xD(X) (P (x|e)  P (x|e))
P
I=
XX |D(X)|

We approximate posterior marginal as the midpoint between lower and upper bound in
order to show whether the bounds are well-centered around the posterior marginal P (x|e).
Namely:
P (x|e) =

PAU (x|e) + PAL (x|e)
2
350

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

and then measure the average absolute error  with respect to that approximation:
P
P
XX
xD(X) |P (x|e)  P (x|e)|
P
=
XX |D(X)|
Ph

P (x,ci ,e)

 100% that was covered by the explored cutset
Finally, we report %P (e) = i=1P (e)
tuples. Notably, in some benchmarks, a few thousand cutset tuples is enough to cover
> 90% of P (e).
4.2.3 Results
We summarize the results for each benchmark in a tabular format and charts. We highlight
in bold face the first AT B data point where the average bounds interval is as good or better
than BdP +. The charts show the convergence of the bounds interval length as a function
of h and time.
For AT B and BBdP + the maximum Markov boundary domain size was fixed at k = 210 .
For BdP +, we vary parameter k from 214 to 219 . Note that BdP + only depends on k, not
on h. In the tables, we report the best result obtained by BdP + and its computation time
so that it appears as constant with respect to h. However, when we plot accuracy against
time, we include BdP + bounds obtained using smaller values of parameter k. In the case
of Alarm network, varying k did not make any difference since the full Markov boundary
domain size equals 210 < 214 . The computation time of BBdP + includes the AT B plus
the BdP + time.
Alarm network. Figure 3 reports the results. Since the maximum Markov boundary in
Alarm network is small, BdP + runs without limitations and computes an average bounds
interval of 0.61 in 4.3 seconds. Note that the enumeration of less than the 25% of the total
number of cutset tuples covers 99% of the P (e). This fact suggests that schemes based on
cutset conditioning should be very suitable for this benchmark. Indeed, AT B outperforms
BdP +, computing more accurate bounds starting with the first data point of h = 25 where
the mean interval I AT B = 0.41 while the computation time is 0.038 seconds, an order of
magnitude less than BdP +. The extreme efficiency of AT B in terms of time is clearly seen
in the right chart. The x-axis scale is logarithmic to fit all the results. As expected, the
average bounds interval generated by AT B and BBdP + decrease as the number of cutset
tuples h increases, demonstrating the anytime property of AT B with respect to h. Given a
fixed h, BBdP + has a very significant overhead in time with respect to AT B (two orders
of magnitude for values of h smaller than 54) and only a minor improvement in accuracy.
Barley network. Figure 4 reports the results. AT B and BBdP + improve as h increases.
However, the improvement is quite moderate while very time consuming due to more uniform shape of the distribution P (C|e) as reflected by the very small % of P (e) covered by
explored tuples (only 1% for 562 tuples and only 52% for 12478 tuples). For example, the
average AT B (resp. BBdP +) bounds interval decreases from 0.279 (resp. 0.167), obtained
in 9 (resp. 10) seconds, to 0.219 (resp. 0.142) obtained in 139 (resp. 141) seconds. Given a
fixed h, BBdP + substantially improves AT B bounds with little time overhead (2 seconds
in general). Namely, in this benchmark, BBdP + computation time is dominated by AT B
351

fiBidyuk, Dechter & Rollon

h
25
34
40
48
50
54

%P(e)
86
93
96
97
98
99

I
0.61
0.61
0.61
0.61
0.61
0.61

Alarm, N=37, w =5, |LC|=8, |DLC |=108, |E|=1-4
BdP +
AT B

time(sec)
I

time(sec)
I
0.21
4.3
0.41
0.12
0.038
0.35
0.21
4.3
0.31
0.09
0.039
0.27
0.21
4.3
0.25
0.07
0.044
0.22
0.21
4.3
0.24
0.05
0.051
0.15
0.21
4.3
0.16
0.04
0.052
0.12
0.21
4.3
0.13
0.03
0.059
0.09
ATB

Alarm, N=37, w*=5, |LC|=8, |E|=1-4

0.6
0.5
0.4
0.3
0.2

ATB
BdP+

BBdP+

0.7

Avg Bounds Interval

Avg Bounds Interval

Alarm, N=37, w*=5, |LC|=8, |E|=1-4

BdP+

0.7

BBdP +

time(sec)
0.10
3.4
0.08
2.3
0.06
2.1
0.04
1.5
0.03
1.2
0.02
0.86

BBdP+

0.6
0.5
0.4
0.3
0.2
0.1

0.1

0

0
0

10

20

30

40

50

0.01

60

0.1

1

10

time (sec)

h

Figure 3: Results for Alarm network. The table reports the average bounds interval I,
average error , computation time (in seconds), and percent of probability of
evidence P (e) covered by the fully-instantiated cutset tuples as a function of h.
We highlight in bold face the first AT B data point where the average bounds
interval is as good or better than BdP +. The charts show the convergence of the
bounds interval length as a function of h and time. BdP + uses full size Markov
boundary since its domain size is small (< 214 ), resulting in only one data point
on the chart on the right.

computation time. Note that the computation time of the stand-alone BdP + algorithm
is less than 2 seconds. Within that time, BdP + yields an average interval length of 0.23,
while AT B and BBdP + spend 86 and 10 seconds, respectively, to obtain the same quality
bounds. However, the anytime behavior of the latter algorithms allows them to improve
with time, a very desirable characteristic when computing bounds. Moreover, note that its
overhead in time with respect to AT B is completely negligible.
CPCS networks. Figures 5 to 8 show the results for cpcs54, cpcs179, cpcs360b and
cpcs422b, respectively. The behavior of the algorithms in all networks is very similar. As in
the previous benchmarks, AT B and BBdP + bounds interval decreases as h increases. Given
a fixed h, BBdP + computes slightly better bounds intervals than AT B in all networks but
cpcs179. For all networks, BBdP + has overhead in time with respect to AT B. This
overhead is constant for all values of h and for all networks except for cpcs54, for which the
overhead decreases as h increases. AT B and BBdP + outperform BdP +. Both algorithms
compute the same bound interval length as BdP +, improving the computation time in one
order of magnitude. Consider for example cpcs422b, a challenging instance for any inference
scheme as it has relatively large induced width and loop-cutset size. AT B outperforms
BdP + after 50 seconds starting with h = 1181, and BBdP + outperforms BdP + in 37
352

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

h
562
1394
2722
4429
6016
7950
9297
12478

Barley, N =48, w =7, |LC|=12, |DLC | > 2  106 , |E|=4-8
BdP +
AT B
I

time(sec)
I

time(sec)
I
0.23
0.07
1.7
0.279
0.097
9
0.167
0.23
0.07
1.7
0.263
0.090
23
0.162
0.23
0.07
1.7
0.247
0.084
43
0.154
0.23
0.07
1.7
0.235
0.079
65
0.147
0.23
0.07
1.7
0.230
0.078
86
0.145
0.23
0.07
1.7
0.228
0.077
99
0.145
0.23
0.07
1.7
0.224
0.075
111
0.143
0.23
0.07
1.7
0.219
0.073
139
0.142

%P(e)
1
3
6
14
22
33
40
52

Barley, N=48, w*=7, |LC|=12, |E|=4-8

BBdP+

time(sec)
0.047
10
0.045
25
0.042
45
0.040
67
0.040
88
0.040
101
0.039
113
0.038
141
ATB

Barley, N=48, w*=7, |LC|=12, |E|=4-8

ATB
BdP+

0.25

0.2

0.15

BBdP+

0.3

Avg Bounds Interval

Avg Bounds Interval

BdP+

BBdP+

0.3

0.25

0.2

0.15

0.1

0.1
0

2000

4000

6000

8000

10000

12000

14000

0

20

40

60

80

100

120

time (sec)

h

Figure 4: Results for Barley network. The table reports the average bounds interval I,
average error , computation time (in seconds), and percent of probability of
evidence P (e) covered by the fully-instantiated cutset tuples as a function of h.
We highlight in bold face the first AT B data point where the average bounds
interval is as good or better than BdP +. The charts show the convergence of the
bounds interval length as a function of h and time.

seconds starting with h = 253 (BdP + convergence is shown in the plot, but only the best
result is reported in the table).
Larkin (2003) reported bounds on cpcs360b and cpcs422b using AD algorithm. For the
first network, AD achieved bounds interval length of 0.03 in 10 seconds. Within the same
time, AT B computes an average bounds interval of  0.005. For cpcs422b, AD achieved
bounds interval of 0.15, obtained in 30 seconds. Within the same time, AT B and BBdP +
obtain comparable results computing average bounds interval of 0.24 and 0.15, respectively.
It is important to note that the comparison is not on the same instances since the evidence
nodes are not the same. Larkins code was not available for further experiments.
Munin networks. Figure 9 reports the results for both Munin networks. Let us first
consider Munin3 network. Given a fixed h, AT B and BBdP + compute almost identical
bound intervals with BBdP + having a noticeable time overhead. Note that the two curves
in the chart showing convergence as a function of h are very close and hard to distinguish,
while the points of BBdP + in the chart showing convergence as a function of time are
shifted to the right with respect to the ones of AT B. AT B is clearly superior to BdP +
both in accuracy and time. BdP + computes bounds interval of 0.24 within 12 seconds,
while AT B computes bounds interval of 0.050 in 8 seconds. In Munin4, given a fixed
353

fiBidyuk, Dechter & Rollon

h
513
1114
1581
1933
2290
2609
3219
3926
6199
7274

%P(e)
10
19
29
34
40
46
53
59
63
68

cpcs54, N =54, |LC|=15, w =15, |DLC |=32678, |E|=2-8
BdP +
AT B
I

time(sec)
I

time(sec)
I
0.35
0.02
24
0.51
0.027
0.9
0.34
0.35
0.02
24
0.45
0.023
1.5
0.32
0.35
0.02
24
0.42
0.021
1.9
0.31
0.35
0.02
24
0.40
0.020
2.2
0.30
0.35
0.02
24
0.38
0.019
2.4
0.30
0.35
0.02
24
0.37
0.018
2.7
0.29
0.35
0.02
24
0.34
0.016
3.2
0.27
0.35
0.02
24
0.31
0.014
3.8
0.25
0.35
0.02
24
0.23
0.010
5.9
0.20
0.35
0.02
24
0.20
0.008
6.9
0.17
ATB

cpcs54, N=54, |LC|=15, w*=15, |E|=2-8

BBdP+

time(sec)
0.011
3.1
0.010
3.1
0.009
3.4
0.009
3.6
0.008
3.9
0.007
4.0
0.007
4.5
0.006
5.2
0.006
6.6
0.006
7.3

cpcs54, N=54, |LC|=15, w*=15, |E|=2-8

ATB

BdP+

0.6

Avg Bounds Interval

Avg Bounds Interval

BdP+

BBdP+

0.6
0.5
0.4
0.3
0.2
0.1
0

BBdP+

0.5
0.4
0.3
0.2
0.1
0

0

1000

2000

3000

4000

5000

6000

7000

8000

h

0

2

4

6

8

10

time (sec)

Figure 5: Results for cpcs54 network. The table reports the average bounds interval I,
average error , computation time (in seconds), and percent of probability of
evidence P (e) covered by the fully-instantiated cutset tuples as a function of h.
We highlight in bold face the first AT B data point where the average bounds
interval is as good or better than BdP +. The charts show the convergence of the
bounds interval length as a function of h and time.

h, BBdP + computes tighter bounds than AT B with some time overhead. However, the
improvement decreases as h increases as shown by the convergence of both curves either
as a function of h and time. Since the loop-cutset size is large, the convergence of AT B
is relatively slow. BdP + computes bounds interval of 0.23 within 15 seconds, while AT B
and BBdP + compute bounds of the same quality within 54 and 21 seconds, respectively.
4.3 Bounding the Probability of Evidence
We compare the performance of the following three algorithms: AT B, mini-bucket elimination (M BE) (Dechter & Rish, 2003), and variable elimination and conditioning (V EC).
For AT B, we test different configurations of the control parameters (h, k). Note that when
h = 0, AT B is equivalent to its plug-in algorithm A, which in our case is ABdP +.
4.3.1 Algorithms and Benchmarks
M BE is a general bounding algorithm for graphical model problems. In particular, given a
Bayesian network, M BE computes lower and upper bound on the probability of evidence.
354

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

h
242
334
406
574
801
996
1285
1669

%P(e)
70
75
78
82
85
87
88
90

cpcs179, N =179, w =8, |LC|=8, |DLC |=49152, |E|=12-24
BdP +
AT B
I

time(sec)
I

time(sec)
I
0.15
0.05
20
0.22
0.067
4
0.092
0.15
0.05
20
0.12
0.033
6
0.054
0.15
0.05
20
0.09
0.024
7
0.037
0.15
0.05
20
0.07
0.018
9
0.029
0.15
0.05
20
0.05
0.014
10
0.022
0.15
0.05
20
0.04
0.010
12
0.017
0.15
0.05
20
0.03
0.006
13
0.012
0.15
0.05
20
0.02
0.003
16
0.007
ATB

cpcs179, N=179, |LC|=8, w*=8, |E|=12-24

BBdP+

time(sec)
0.029
11
0.016
13
0.010
13
0.008
15
0.006
17
0.005
18
0.003
20
0.002
22
ATB

cpcs179, N=179, |LC|=8, w*=8, |E|=12-24

BdP+

BdP+
BBdP+

1.E-01

1.E-02

1.E-03

BBdP+

1.E+00

Avg Bounds Interval

Avg Bounds Interval

1.E+00

1.E-01

1.E-02

1.E-03

0

500

1000

1500

2000

2500

3000

h

0

5

10

15

20

25

time (sec)

Figure 6: Results for cpcs179 network. The table reports the average bounds interval I,
average error , computation time (in seconds), and percent of probability of
evidence P (e) covered by the fully-instantiated cutset tuples as a function of h.
We highlight in bold face the first AT B data point where the average bounds
interval is as good or better than BdP +. The charts show the convergence of the
bounds interval length as a function of h and time.

M BE has a control parameter z, that allows trading time and space for accuracy. As the
value of the control parameter z increases, the algorithm computes tighter bounds using
more time and space, which is exponential in z.
V EC is an algorithm that combines conditioning and variable elimination. It is based
on the w-cutset conditioning scheme. Namely, the algorithm conditions or instantiates
enough variables so that the remaining problem conditioned on the instantiated variables
can be solved exactly using bucket elimination (Dechter, 1999). The exact probability of evidence can be computed by summing over the exact solution output by bucket elimination
for all possible instantiations of the w-cutset. When V EC is terminated before completion, it outputs a partial sum yielding a lower bound on the probability of evidence. The
implementation of V EC is publicly available1 .
We tested AT B for bounding P (e) on three different benchmarks: Two-layer Noisy-Or,
grids and coding networks. All instances are included in the UAI08 evaluation2 .
In two-layer noisy-or networks, variables are organized in two layers where the ones in
the second layer have 10 parents. Each probability table represents a noisy OR-function.
1. http://graphmod.ics.uci.edu/group/Software
2. http://graphmod.ics.uci.edu/uai08/Evaluation/Report

355

fiBidyuk, Dechter & Rollon

h
121
282
501
722
938
1168
1388
1582
1757

%P(e)
83
92
96
97
98
98
99
99
99

cpcs360b, N=360, w = 21, |LC| = 26, |DLC |=226 , |E|=11-23
BdP +
AT B
I
 time(sec)
I
 time(sec)
I
0.027 0.009
55
0.0486 1.6E-2
5
0.0274
0.027 0.009
55
0.0046 9.0E-4
10
0.0032
0.027 0.009
55
0.0020 3.6E-4
15
0.0014
0.027 0.009
55
0.0012 2.4E-4
19
0.0009
0.027 0.009
55
0.0006 8.4E-5
25
0.0004
0.027 0.009
55
0.0005 7.5E-5
29
0.0004
0.027 0.009
55
0.0004 5.9E-5
35
0.0003
0.027 0.009
55
0.0003 5.3E-5
39
0.0002
0.027 0.009
55
0.0003 4.7E-5
43
0.0002

ATB

cpcs360b, N=360, |LC|=26, w*=21, |E|=11-23

cpcs360b, N=360, |LC|=26, w*=21, |E|=11-23
ATB

1.E+00

BBdP+
 time(sec)
1.0E-2
7
8.5E-4
12
3.5E-4
17
2.3E-4
21
7.8E-5
27
6.9E-5
31
5.4E-5
37
4.8E-5
41
4.4E-5
46

BdP+

1.E+00

BdP+

BBdP+

Avg Bounds Interval

Avg Bounds Interval

BBdP+

1.E-01

1.E-02

1.E-03

1.E-01

1.E-02

1.E-03

1.E-04
1.E-04

1.E-05
0

200

400

600

800

1000

1200

h

0

5

10

15

20

25

30

Time (sec)

Figure 7: Results for cpcs360b. The table reports the average bounds interval I, average
error , computation time (in seconds), and percent of probability of evidence
P (e) covered by the fully-instantiated cutset tuples as a function of h. We highlight in bold face the first AT B data point where the average bounds interval is
as good or better than BdP +. The charts show the convergence of the bounds
interval length as a function of h and time.

Each parent variable yj has a value Pj  [0..Pnoise ]. QThe CPT for each variable in the
second layer is then defined as P (x = 0|y1 , . . . , yP ) = yj =1 Pj and P (x = 1|y1 , . . . , yP ) =
1  P (x = 0|y1 , . . . , yP ). We experiment with a class of problems called bn2o instances in
the UAI08.
In grid networks, variables are organized as an M  M grid. We experiment with
grids2 instances, as they were called in UAI08, which are characterized by two parameters
(M, D), where D is the percentage of determinism (i.e., the percentage of values in all CPTs
assigned to either 0 or 1). For each parameter configuration, 10 samples were generated by
randomly assigning value 1 to one leaf node. In UAI08 competition, these instances were
named D-M -I, where I is the instance number.
Coding networks can be represented as a four layer Bayesian network having M nodes in
each layer. The second and third layer correspond to input information bits and parity check
bits respectively. Each parity check bit represents an XOR function of input bits. Input
and parity check nodes are binary while the output nodes are real-valued. We consider the
BN 126 to BN 134 instances in the UAI08 evaluation. Each one has M = 128, 4 parents
356

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

h
64
256
379
561
861
1181
1501
2427
3062
4598

%P(e)
1.7
2.0
2.6
2.9
3.4
4.5
5.4
8.0
9.5
12.2

cpcs422b, N=422, w = 22, |LC| = 47, |DLC |=247 , |E|=4-10
BdP +
ATB
I

time(sec)
I

time(sec)
I
0.19
0.06
120
0.28
0.100
21
0.19
0.19
0.06
120
0.24
0.090
26
0.15
0.19
0.06
120
0.22
0.078
32
0.14
0.19
0.06
120
0.20
0.073
36
0.13
0.19
0.06
120
0.19
0.068
43
0.12
0.19
0.06
120
0.18
0.064
50
0.12
0.19
0.06
120
0.17
0.062
56
0.12
0.19
0.06
120
0.16
0.058
73
0.12
0.19
0.06
120
0.16
0.057
83
0.12
0.19
0.06
120
0.16
0.053
110
0.11
ATB

cpcs422b, N=422, |LC|=47, w*=22, |E|=4-10
0.3

BBdP+

time(sec)
0.056
23
0.050
35
0.049
41
0.046
46
0.044
54
0.041
60
0.041
65
0.039
82
0.038
92
0.036
120

cpcs422b, N=422, |LC|=47, w*=22, |E|=4-10

ATB

0.3

BdP+

BdP+

0.25

Avg Bounds Interval

Avg Bounds Interval

BBdP+

0.2
0.15
0.1
0.05
0

BBdP+

0.25
0.2
0.15
0.1
0.05
0

0

100

200

300

400

500

600

0

20

40

60

80

100

120

140

time (sec)

h

Figure 8: Results for cpcs422b. The table reports the average bounds interval I, average
error , computation time (in seconds), and percent of probability of evidence
P (e) covered by the fully-instantiated cutset tuples as a function of h. We highlight in bold face the first AT B data point where the average bounds interval is
as good or better than BdP +. The charts show the convergence of the bounds
interval length as a function of h and time.

for each node and channel noise variance ( = 0.40). These networks are very hard and
exact results are not available.
Table 2 summarizes the characteristics of each network. For each one, the table specifies
the number of variables N , the induced width w , the size of loop cutset |LC|, the number of
loop-cutset tuples |D(LC)|, and the time needed to compute the exact posterior marginals
by bucket-tree elimination (exponential in the induced width w ) and by cutset conditioning
(exponential in the size of loop-cutset). An out indicates that bucket-tree elimination is
unfeasible in terms of memory demands. Note that the characteristics of grid networks only
depend on their sizes but not on the percentage of determinism; the characteristics of all
coding networks are the same.
For our purposes, we consider V EC as another exact algorithm to compute the exact
P (e) in the first and second benchmarks and as a lower bounding technique for the third
benchmark. We fix the control parameter z of M BE and the w-cutset of V EC so that the
algorithms require less than 1.5GB of space.
357

fiBidyuk, Dechter & Rollon

MUNIN3

h
196
441
882
1813
2695
2891
3185
3577
4312

%P(e)
64
72
78
79
80
81
82
82
83

I
0.24
0.24
0.24
0.24
0.24
0.24
0.24
0.24
0.24

Munin3, N=1044, w =7, |LC|=30, |E|=257
BdP+
ATB

time(sec)
I

time(sec)
0.1
12
0.050
0.020
8
0.1
12
0.030
0.011
12
0.1
12
0.025
0.009
18
0.1
12
0.020
0.007
32
0.1
12
0.018
0.006
46
0.1
12
0.017
0.006
49
0.1
12
0.014
0.005
54
0.1
12
0.013
0.004
68
0.1
12
0.011
0.004
80

Munin3, N=1044, |LC|=30, w*=7, |E|=257

0.10

0.01
0

500

1000

1500

BBdP+

time(sec)
0.020
16
0.012
20
0.009
26
0.007
40
0.007
54
0.006
57
0.005
62
0.004
76
0.004
88

Munin3, N=1044, |LC|=30, w*=7, |E|=257

ATB
BdP+
BBdP+

ATB
BdP+

1.00

Avg Bounds Interval

Avg Bounds Interval

1.00

I
0.048
0.029
0.025
0.019
0.017
0.016
0.014
0.012
0.010

BBdP+

0.10

0.01

2000

0

20

40

h

60

80

100

time (sec)

MUNIN4

h
245
441
1029
2058
3087
5194

%P(e)
1
7
11
17
20
24

I
0.23
0.23
0.23
0.23
0.23
0.23

Munin4, N=1041, w =8, |LC|=49, |E|=235
BdP+
ATB

time(sec)
I

time
I
0.1
15
0.39
0.16
14
0.24
0.1
15
0.32
0.13
17
0.22
0.1
15
0.28
0.12
34
0.21
0.1
15
0.25
0.11
54
0.19
0.1
15
0.22
0.11
83
0.18
0.1
15
0.21
0.09
134
0.17

Munin4, N=1041, |LC|=49, w*=8, |E|=235

BBdP+

time(sec)
0.102
21
0.095
24
0.089
44
0.082
65
0.077
91
0.072
145

Munin4, N=1041, |LC|=49, w*=8, |E|=235

ATB

ATB

BBdP+

Avg Bounds Interval

0.4
0.3
0.2
0.1
0.0
0

5000

10000

15000

20000

Avg Bounds Interval

BdP+

BdP+

0.4

BBdP+
0.3
0.2
0.1
0.0
0

50

100

150

200

250

time (sec)

h

Figure 9: Results for munin3 and munin4. The tables report the average bounds interval
I, average error , computation time (in seconds), and percent of probability of
evidence P (e) covered by the fully-instantiated cutset tuples as a function of h.
We highlight in bold face the first AT B data point where the average bounds
interval is as good or better than BdP +. The charts show the convergence of the
bounds interval length as a function of h and time.
358

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

network
bn2o-15-30-15
bn2o-15-30-20
bn2o-15-30-25
Grids
Grids
Grids
Grids
coding

16
20
26
42

N

w

|LC|

|D(LC)|

|E|

Time(BE)

Time(LC)

45
50
55
 16
 20
 26
 42
512

22
25
24
22
29
40
70
54-61

24
26
25
116
185
325
863
59-64

224
226
225
2116
2185
2325
2863
259 -264

15
20
25
1
1
1
1
256

14.51
174.28
66.23
27.59
out
out
out
out

17.4 hrs
93.2 hrs
75.76 hrs
> 293 hrs1
> 266 hrs1
> 2306 hrs1
> 2844 hrs1
> 242 hrs1

Table 2: Complexity characteristics of the benchmarks from the UAI repository: N -number
of nodes, w -induced width, |LC|-number of nodes in a loop-cutset, |D(LC)|loop-cutset state space size, Time(BE) is the exact computation time via bucket
elimination, Time(LC) is the exact computation time via loop-cutset conditioning.
The results are averaged over the set of network instances of each benchmark.

4.3.2 Results
We summarize the results for each benchmark in a tabular format. The tables report the
bounds and computation time (in seconds) for each compared algorithm. For AT B, we
report results by varying the values of the control parameters (h, k). In particular, we consider values of h in the range 4 to 200, and values of k in the set {210 , 212 , 214 }. By doing
so, we analyze the impact of each control parameter on the performance of the algorithm.
Grey areas in the tables correspond to (h, k) configurations that cannot be compared due
to computation time.
Two-layer noisy-or networks. Table 3 shows the results. As expected, the quality
of the bounds produced by AT B improves when the values of the control parameters (h, k)
increase. We observe that the best bounds are obtained when fixing h to the highest value
(i.e., 200) and k to the smallest value (i.e., 210 ). However, the increase in the value of
h leads to higher computation times than when increasing the value of k. When taking
time into account, comparing configurations with similar time (see (h = 50, k = 210 ) and
(h = 4, k = 214 ), and (h = 200, k = 210 ) and (h = 50, k = 212 ), respectively), we observe
that the configuration with the highest value of h and the smallest value of k outperforms
the other ones.
When compared with M BE, there is no clear superior approach. The accuracy of the
algorithms depends on whether we look at upper or lower bounds. When considering upper bounds, AT B outperforms M BE for all instances 1b, 2b and 3b. Note that for those
instances, M BE computes worse upper bounds than the trivial one (i.e., greater than 1).
However, for instances 1a, 2a and 3a, M BE computes tighter upper bounds than AT B.
For lower bounds, in general AT B outperforms MBE for instances with 20 and 25 evidence
variables, while M BE is more accurate for instances having 15 evidence variables. Regarding computation time, AT B is definitely slower than M BE.

1. Times are extrapolated.

359

fiBidyuk, Dechter & Rollon

Inst.

P(e)

h %P(e)

ATB(h, k = 210 )
LB/UB
Time

ATB(h, k = 212 )
LB/UB
Time

ATB(h, k = 214 )
LB/UB
Time

MBE(z=18)
LB/UB
Time

bn2o-30-15-150, |E| = 15
1a 5.9E-05

1b 0.56565

2a 4.0E-07

2b 0.54111

3a 1.2E-04

3b 0.18869

4
50
200
4
50
200
4
50
200
4
50
200
4
50
200
4
50
200

0.0004
0.100
0.250
0.007
0.120
0.460
0.003
0.020
0.320
0.008
0.210
1.110
0.216
1.040
3.580
0.076
0.470
1.440

5.7E-10/5.3E-01
1.2E-07/1.1E-01
3.5E-07/5.7E-02
3.1E-04/9.3E-01
4.1E-03/8.6E-01
1.5E-02/8.5E-01
2.0E-11/1.3E-01
1.5E-10/1.0E-02
1.7E-09/4.0E-03
6.9E-03/7.9E-01
5.5E-02/7.8E-01
1.1E-01/7.5E-01
2.9E-07/1.7E-01
1.7E-06/4.6E-02
5.3E-06/2.7E-02
1.1E-03/7.7E-01
6.8E-03/6.3E-01
2.1E-02/5.4E-01

4
50
200
4
50
200
4
50
200
4
50
200
4
50
200
4
50
200

0.004
0.050
1.880
0.012
0.140
0.430
0.013
0.410
1.410
0.020
0.430
1.620
0.002
0.060
0.090
0.0002
0.110
0.660

5.4E-12/1.6E-02
9.1E-11/1.8E-03
2.8E-09/5.7E-04
1.0E-04/7.3E-01
3.3E-03/6.7E-01
1.1E-02/5.9E-01
3.8E-11/1.6E-02
1.4E-09/3.3E-03
4.5E-09/2.4E-03
6.4E-03/8.3E-01
3.0E-02/7.7E-01
5.9E-02/6.9E-01
8.3E-14/1.8E-03
2.2E-12/1.1E-04
3.6E-12/3.3E-05
4.5E-05/9.7E-01
5.4E-02/9.3E-01
1.1E-01/8.8E-01

4
50
200
4
50
200
4
50
200
4
50
200
4
50
200
4
50
200

0.0004
0.01
0.06
0.016
0.22
1.07
0.0004
0.0012
0.07
0.018
0.19
0.65
0.0001
0.01
0.20
0.0065
0.45
1.52

1.3E-14/6.6E-02
3.7E-13/3.3E-03
2.0E-12/1.1E-03
4.3E-04/8.1E-01
4.6E-03/7.2E-01
1.3E-02/6.5E-01
1.8E-12/1.9E-01
5.7E-12/4.5E-02
1.8E-10/2.2E-02
5.3E-04/7.6E-01
5.4E-03/7.4E-01
1.4E-02/7.1E-01
1.7E-16/1.1E-01
4.3E-14/1.9E-02
5.5E-13/7.1E-03
1.0E-03/7.9E-01
4.2E-02/7.7E-01
8.5E-02/7.5E-01

2
32
103
2
31
102
2
29
89
2
29
90
2
26
74
2
25
69

5.9E-10/4.8E-01
1.2E-07/9.4E-02

8 6.0E-10/4.4E-01
129

38

4.3E-04/9.3E-01
4.8E-03/8.6E-01

8 5.0E-04/9.3E-01
124

38

2.0E-11/1.1E-01
1.5E-10/8.9E-03

8 2.1E-11/8.5E-02
115

38

8.6E-03/8.0E-01
6.1E-02/7.7E-01

8 9.2E-03/8.0E-01
115

38

2.9E-07/1.5E-01
1.7E-06/4.2E-02

8 2.9E-07/1.4E-01
103

38

1.1E-03/7.7E-01
7.1E-03/6.3E-01

8 1.2E-03/7.7E-01
95

38

9.1E-06/4.8E-04

2

0.17277/1.42

2

8.4E-10/2.1E-05

2

0.02647/1.8

2

4.4E-07/1.5E-03

2

0.03089/0.81

2

2.4E-15/3.3E-04

3

9.8E-04/1.9

3

4.4E-15/8.0E-05

3

2.3E-05/2.9

3

5.2E-13/1.7E-06

3

5.3E-03/1.9

3

1.7E-16/3.1E-06

4

1.4E-03/1.4

4

1.8E-12/1.2E-05

4

7.2E-03/1.7

4

1.3E-15/4.9E-07

4

3.5E-03/2.7

4

bn2o-30-20-200, |E| = 20
1a 1.4E-07

1b 0.15654

2a 2.2E-07

2b 0.27695

3a 2.4E-09

3b 0.48039

3
62
195
3
64
218
3
52
169
3
51
145
3
58
198
3
64
194

5.4E-12/1.5E-02
9.1E-11/1.6E-03

16 5.4E-12/1.4E-02
264

67

1.1E-04/7.3E-01
3.6E-03/6.7E-01

16 1.1E-04/7.3E-01
279

68

3.8E-11/1.6E-02
1.3E-09/3.1E-03

16 3.8E-11/1.5E-02
211

70

7.3E-03/8.3E-01
3.3E-02/7.7E-01

16 8.0E-03/8.3E-01
197

68

8.3E-14/1.8E-03
2.2E-12/1.1E-04

16 8.3E-14/1.8E-03
236

68

5.1E-05/9.7E-01
5.9E-02/9.3E-01

16 6.2E-05/9.7E-01
277

68

bn2o-30-25-250, |E| = 25
1a 2.9E-09

1b 0.15183

2a 2.4E-07

2b 0.30895

3a 2.7E-10

3b 0.46801

6
119
396
6
120
381
6
112
402
6
107
367
6
119
409
6
106
337

1.3E-14/6.5E-02
3.7E-13/2.8E-03

22 1.3E-14/4.8E-02
429

99

5.7E-04/8.1E-01
6.7E-03/7.2E-01

22 6.2E-04/8.1E-01
437

99

1.8E-12/1.9E-01
5.7E-12/3.9E-02

22 1.8E-12/1.7E-01
398

99

5.9E-04/7.6E-01
6.1E-03/7.4E-01

22 6.3E-04/7.6E-01
374

99

1.7E-16/1.1E-01
4.3E-14/1.6E-02

22 1.7E-16/8.1E-02
427

99

1.2E-03/7.9E-01
4.8E-02/7.7E-01

22 1.3E-03/7.9E-01
352

98

Table 3: Results for bn2o networks. The table shows the LB and UB computed by AT B
varying the number of cutset tuples h and the maximum domain k of the Markov
boundary.
360

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

(M, D)

P(e)

h

4
(16, 50) 0.6172 100
200
4
(20, 50) 0.4441 100
200
4
(20, 75) 0.4843 100
200
4
(26, 75) 0.6579 100
200
4
(26, 90) 0.8206 100
200
4
(42, 90) 0.4933 100
200

%P(e)
1.57E-14
3.50E-11
4.22E-11
1.07E-24
1.57E-21
1.13E-20
1.25E-09
2.40E-09
2.89E-09
3.88E-19
7.32E-19
1.55E-18
3.47E-08
3.41E-06
8.38E-06
8.65E-29
2.32E-25
3.48E-25

Grids2, |E| = 1
ATB(k = 210 ,h)
ATB(k = 212 ,h)
LB/UB
Time
LB/UB
Time
0.3127/0.8286
1 0.3127/0.8286
1
0.3127/0.8286
57 0.3127/0.8286
57
0.3127/0.8286
111
0.1765/0.4939
5 0.1765/0.4939
5
0.1765/0.4939
208 0.1765/0.4939
203
0.1765/0.4939
412
0.2106/0.7454
3 0.2106/0.7454
3
0.2106/0.7454
81 0.2106/0.7454
80
0.2106/0.7454
156
0.0506/0.9338
6 0.0506/0.9338
6
0.0506/0.9338
268 0.0506/0.9338
270
0.0506/0.9338
534
0.1858/0.8943
2 0.1858/0.8943
2
0.1858/0.8943
85 0.1858/0.8943
84
0.1858/0.8943
164
0.0048/0.9175
10 0.0048/0.9175
10
0.0048/0.9175
436 0.0048/0.9175
439
0.0048/0.9175
866

ATB(k = 214 ,h)
MBE
LB/UB
Time LB / UB Time
0.3127/0.8286
1
0/5.13
16
0.1765/0.4939

0.2106/0.7454

0.0506/0.9338

0.1858/0.8943

0.0048/0.9175

5
0/12411

39

0/1E+05

39

0/1E+10

84

0/1E+10

87

0/1E+10

110

3

6

2

10

Table 4: Results on grid networks. The table shows the LB and UB computed by AT B
varying the number of cutset tuples h and the maximum length k of the conditional
probability tables over the Markov boundary.

Grid networks. Table 4 reports the results. The first thing to observe is that M BE
computes completely uninformative bounds. In this case, the anytime behavior of AT B is
not effective either. The increase of the value of its control parameters (h, k) does not affect
its accuracy. Since the Markov boundary in grid networks is relatively small, the smallest
tested value of k is higher than its Markov boundary size which explains the independence
on k. Another reason for its ineffectiveness may be the high percentage of determinism
in these networks. It is known that sampling methods are inefficient in the presence of
determinism. As a consequence, the percentage of probability mass accumulated in the
h sampled tuples is not significant, which cancels the benefits of computing exact probability of evidence for that subset of tuples. Therefore, in such cases a more sophisticated
sampling scheme should be used, for example (Gogate & Dechter, 2007). Consequently, for
these deterministic grids, AT Bs performance is controlled totally by its bound propagation
plugged-in algorithm.
Coding networks. Table 5 shows the results. We do not report the percentage of P (e)
covered by the fully-instantiated cutset tuples because the exact P (e) is not available. We
set the time limit of V EC to 1900 seconds (i.e., the maximum computation time required
by running AT B in these instances). We only report the results for k = 210 and k = 214
because the increase in the value of k was not effective and did not result in increased
accuracy. In this case, the accuracy of AT B increases as the value of h increases. In comparing AT B with the other algorithms we have to distinguish between lower and upper
bounds. Regarding lower bounds, AT B clearly outperforms M BE and V EC in all instances. Indeed, the lower bound computed by M BE and V EC is very loose. Regarding
361

fiBidyuk, Dechter & Rollon

Inst.

BN 126

BN 127

BN 128

BN 129

BN 130

BN 131

BN 132

BN 133

BN 134

h
4
50
150
4
50
150
4
50
150
4
50
150
4
50
150
4
50
150
4
50
150
4
50
150
4
50
150

coding, |E| = 256
ATB(k = 210 ,h)
ATB(k = 214 ,h)
LB/UB
Time
LB/UB
Time
1.9E-76/1.5E-41
50 1.9E-76/1.52E-41 3494
1.9E-69/2.5E-42
632
1.9E-58/1.3E-42 1442
5.3E-60/2.3E-43
55 5.3E-60/2.3E-43
399
1.3E-58/2.3E-44
426
1.6E-58/1.9E-44
946
7.2E-54/1.6E-42
85 7.2E-54/1.6E-42
582
4.9E-48/7.2E-43
637
4.9E-48/1.4E-43 1225
1.4E-72/8.2E-45
50 1.5E-72/8.2E-45
362
1.5E-64/2.1E-45
585
8.5E-64/5.4E-46 1400
4.7E-65/2.9E-44
47 4.7E-65/2.9E-44
324
6.3E-63/2.9E-45
619
3.7E-58/2.3E-45 1299
1.9E-60/1.3E-44
52 1.9E-60/1.3E-44
367
2.3E-54/3.7E-45
484
2.3E-54/1.1E-45 1276
2.3E-79/6.3E-44
50 2.3E-79/6.3E-44
363
3.6E-67/1.0E-44
689
1.5E-66/8.1E-45 1627
1.6E-56/2.7E-42
53 1.6E-56/2.7E-42
398
1.1E-54/2.4E-43
671
2.3E-54/9.5E-44 1846
8.9E-63/1.8E-43
47 8.9E-63/1.8E-43
355
1.2E-62/8.6E-45
606
6.1E-57/4.8E-45 1412

MBE(z=22)
LB/UB
Time

VEC
LB
Time

1.4E-139/1.5E-044

143 9.2E-102

1900

1.6E-134/1.0E-045

164 5.3E-115

1900

1.2E-144/5.1E-043

124 1.9E-112

1900

2.8E-139/4.8E-043

144 1.5E-115

1900

1.1E-132/1.9E-045

112

1.3E-96

1900

2.3E-141/3.2E-045

119 3.2E-102

1900

2.8E-134/2.3E-048

109 8.9E-111

1900

1.8E-136/4.1E-045

147 1.9E-109

1900

1.9E-148/3.9E-045

163 4.2E-111

1900

Table 5: Results on coding networks. The table shows the LB and UB computed by AT B
varying the number of cutset tuples h and the maximum length k of the conditional
probability tables over the Markov boundary.

upper bounds, AT B(h = 150, k = 210 ) outperforms M BE in three instances (i.e., BN 128,
BN 129 and BN 131). When taking time into account AT B only outperforms M BE in
instance BN 129.
Summary of empirical evaluation. We demonstrated that AT Bs bounds converge as
h, the number of cutset tuples computed exactly, increases. The speed of convergence varied
among benchmarks. The convergence was faster when the active cutset tuples accounted
for a large percentage of the probability mass of P (C|e), as shown for the case of cpcs54,
cpcs179, and cpcs360 networks. Comparing with a variant of bound propagation called
BdP +, AT B was more accurate if given sufficient time and even when given the same time
bound, it computed more accurate bounds on many benchmarks.
We showed that AT Bs bounds on the posterior marginals can be further improved
when used as initial bounds in BdP +. We call this hybrid of AT B followed by BdP +
the BBdP + algorithm. Our experiments demonstrated the added power of BBdP + in
exploiting the time-accuracy trade-off.
We also compared the power of AT B to bound the probability of evidence against the
mini-bucket elimination (M BE). We showed that neither algorithm was dominating on all
benchmarks. Given the same amount of time, AT B computed more accurate bounds than
362

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

M BE on some instances of bn2o and coding networks. AT B outperformed M BE on all
instances of the grid networks on which M BE only computed bounds of 0 and 1. On this
benchmark, however, AT B converged very slowly. We believe in part this is due to the
grids large loop-cutset sizes.
We compared AT Bs ability to compute the lower bound on P (e) to V EC on coding
networks. V EC obtains the bound by computing a partial sum in the cutset-conditioning
formula (see Eq. 2). By comparing the lower bounds generated by AT B and V EC, we can
gain insight into the trade-off between enumerating more cutset tuples and bounding the
uninstantiated tuples. Since AT Bs lower bound was consistently tighter, we conclude that
bounding the uninstantiated tuples is cost-effective.

5. Related Work
There are three early approaches which use the same principle as AT B: Pooles algorithm
(1996), bounded conditioning (BC) (Horvitz et al., 1989) which we have already described,
and bounded recursive decomposition (Monti & Cooper, 1996). In all these cases the computation of the bounds is composed of an exact inference over a subset of the tuples and a
bounding scheme over the total probabilities over the rest of the tuples.
Similar to AT B, Pooles scheme is based on a partial exploration of a search tree.
However, his search tree corresponds to the state space over all the variables of the whole
network and hence, it is exponential in the total number of variables. In contrast, the tree
structure used by our approach corresponds to the state space of the loop-cutset variables;
therefore, it is exponential in the loop-cutset size only. In addition, Poole updates the
bounding function when a tuple with probability 0 (i.e., a conflict) is discovered.
As discussed in Section 2.2, BC is also based on the cutset conditioning principle, but
there are two main differences relative to AT B: (i) the probability mass of the missing
tuples is bounded via prior probabilities, and consequently (ii) as we proved, the upper
bound expression is looser.
Bounded recursive decomposition uses Stochastic simulation (Pearl, 1988) to generate
highly probable instantiations of the variables, which is similar to AT B, and bounds the
missing elements with 0 and prior values. Therefore this approach resembles Pooles algorithm and bounded conditioning. Unlike AT B, bounded recursive decomposition requires
instantiation of all the variables in the network and relies on priors to guide the simulation. In contrast, our algorithm uses Gibbs sampling on a cutset only which is likely to
be more accurate at selecting high probability tuples in presence of evidence. AT B subsumes all three algorithms offering a unifying approach to bounding posteriors with anytime
properties, able to improve its bounds by investing more time and exploring more cutset
tuples.
There are a number of alternative approaches for computing bounds on the marginals.
Poole (1998) proposed context-specific bounds obtained from simplifying the conditional
probability tables. The method performs a variant of bucket elimination where intermediate tables are collapsed by grouping some probability values together. However, since the
method was validated only on a small car diagnosis network with 10 variables, it is hard
to draw any conclusions. Larkin (2003) also obtains bounds by simplifying intermediate
probability tables in the variable elimination order. He solves an optimization problem to
363

fiBidyuk, Dechter & Rollon

find a table decomposition that minimizes the error. Kearns and Saul (1999, 1998) proposed a specialized large deviation bounds approach for layered networks, while Mannino
and Mookerjee (2002) suggested an elaborate bounding scheme with nonlinear objective
functions. Jaakkola and Jordan (1999) proposed a variational method for computing lower
and upper bounds on posterior marginals in Noisy-Or networks and evaluated its performance in the case of diagnostic QMR-DT network. More recent approaches (Tatikonda,
2003; Taga & Mase, 2006; Ihler, 2007; Mooij & Kappen, 2008) aim to bound the error of
belief propagation marginals. The first two approaches are exponential in the size of the
Markov boundary. The third approach is linear in the size of the network, but is formulated
for pairwise interactions only. Finally, the fourth algorithm is exponential in the number of
domain values. Recently, Mooij and Kappen (2008) proposed the box propagation algorithm
that propagates local bounds (convex sets of probability distributions) over a subtree of the
factor graph representing the problem, rooted in the variable of interest.
It is important to note that our approach offers an anytime framework for computing
bounds where any of the above bounding algorithms can be used as a subroutine to bound
joint probabilities for partially-instantiated tuples within AT B and therefore may improve
the performance of any bounding scheme.
Regarding algorithms that bound the probability of evidence, we already mentioned the
mini-bucket schemes and compared against it in Section 4.3. Another recent approach is
the tree-reweighted belief propagation (T RW -BP ) (Wainwright, Jaakkola, & Willsky, 2005).
T RW -BP is a class of message-passing algorithms that compute an upper bound of P (e)
as a convex combination of tree-structured distributions. In a recent paper, Rollon and
Dechter (2010) compare T RW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), M BE and AT B-ABdP +. Their empirical evaluation
shows the relative strength of each scheme on the different benchmarks (Rollon & Dechter,
2010). In another recent work Wexler and Meek (2008) have proposed MAS, a bounding
algorithm for computing the probability of evidence. Shekhar (2009) describes the adjustments required to produce bounds using MAS for Bayesian networks, where the potentials
are less than 1. In a forthcoming paper, Wexler and Meek (2010) improve their MAS scheme
to obtain tighter bounds and describe how to obtain bounds for Bayesian networks for P (e)
as well as for other inferential problems such as the maximal a posteriori and most probable
explanation problems. The comparison with this approach is left as future work.

6. Summary and Conclusions
The paper explores a general theme of approximation and bounding algorithms for likelihood computation, a task that is known to be hard. While a few methods based on one
or two principles emerge, it is clear that pooling together a variety of ideas into a single
framework can yield a significant improvement. The current paper provides such a framework. It utilizes the principle of cutset conditioning harnessing the varied strengths of
different methods. The framework is inherently anytime, an important characteristic for
approximation schemes.
Cutset conditioning is a universal principle. It allows decomposing a problem into a
collection of more tractable ones. Some of these subproblems can be solved exactly while
others can be approximated. The scheme can be controlled by several parameters. In w364

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

cutset we condition on a subset of variables until their treewidth is bounded by w. Each
subproblem can then be solved exactly in time and space exponential in w. If the number
of subproblems is too large, we can use another parameter, h, to control the number of
subproblems solved exactly. The rest of the subproblems are solved using an off-the-shelf
bounding scheme.
We developed an expression that incorporates all these aspects using the parameters:
w - the induced-width of the cutset, h - the number of cutset conditioning subproblems to
be solved exactly (e.g., by bucket elimination), and A - the approximation algorithm that
bounds each of the bounded subproblems. We showed that the number of subproblems that
are approximated is polynomial in h.
In our empirical evaluation of the general framework, called AT B, we used the loopcutset scheme (w = 1) and chose as a bounding algorithm a variant of bound propagation
(Leisink & Kappen, 2003), yielding the integrated scheme which we call ABdP +. We experimented with several benchmarks for the computing posterior marginals and the probability
of evidence, and compared against relevant state of the art algorithms.
Our results demonstrate the value of our AT B framework across all the benchmarks
we have tried. As expected, its anytime aspect is visible showing improved accuracy as
a function of time. More significantly, even when provided with equal time and space
resources, AT B showed remarkable superiority when compared with our variant of bound
propagation and with the mini-bucket elimination algorithm (M BE) (Dechter & Rish,
2003). The latter was recently investigated further by Rollon and Dechter (2010).
Overall, we can conclude that AT B is a competitive algorithm for both bounding posterior marginals and probability of evidence. Generally, we can expect AT B to perform well
in networks whose cutset C is small relative to the total number of variables and whose
distribution P (C|e) has a small number of high probability tuples.
The possibilities for future work are many. We can explore additional trade offs such as
increasing w and therefore decreasing h and improving the selection of the h tuples. We have
looked at only one possible instantiation of the plug-in algorithm A. Other approximation
algorithms can be tried which may offer different time/accuracy trade-offs. In particular,
we plan to investigate the effectiveness of AT B using M BE as plug-in algorithm.

Acknowledgments
This work was supported in part by the NSF under award numbers IIS-0331707, IIS-0412854
and IIS-0713118 and by the NIH grant R01-HG004175-02.
Emma Rollons work was done while a postdoctoral student at the Bren School of
Information and Computer Sciences, University of California, Irvine.
The work here was presented in part in (Bidyuk & Dechter, 2006a, 2006b).

Appendix A. Analysis of Bounded Conditioning
Theorem 2.1 The interval between lower and upper bounds computed by bounded
conditioning is lower bounded by the probability mass
of prior distribution P (C) of the
PM
U (x|e)  P L (x|e) 
i
unexplored cutset tuples: h, PBC
i=h+1 P (c ).
BC
365

fiBidyuk, Dechter & Rollon

Proof.
U
PBC
(x|e)



L
PBC
(x|e)

=
+

=



PM

PM
i
i
i=1 P (c , e) +
i=h+1 P (c ))
Ph
i
i=1 P (c , e)
Ph
Ph
i
i
i=1 P (x, c , e)
i=1 P (x, c , e)
 Ph
Ph
P
M
i
i
i
i=1 P (c , e)
i=1 P (c , e) +
i=h+1 P (c )
P
P
PM
M
h
i
i
i
i=h+1 P (c ))
i=h+1 P (c )( i=1 P (c , e) +
Ph
i
i=1 P (c , e)
P
M
M
i 2
X
X
( M
i=h+1 P (c ))
i
P (c ) + Ph
P (ci )

i , e)
P
(c
i=1
i=h+1
i=h+1
i=h+1 P (c

i )(

Ph

Appendix B. Bounding Posteriors of Cutset Nodes
So far, we only considered computation of posterior marginals for variable X  X \(C 
E). Now we focus on computing bounds for a cutset node Ck  C. Let c0k  D(C) be some
value in domain of Ck . Then, we can compute exact posterior marginal P (ck |e) using Bayes
formula:
PM
P (c0k , e)
(c0 , ci )P (ci , e)
0
P (ck |e) =
(23)
= i=1
PM k
i , e)
P (e)
P
(c
i=1

where (c0k , ci ) is a Dirac delta-function so that (c0k , ci ) = 1 iff cik = c0k and (c0k , ci ) = 0
otherwise. To simplify notation, let Z = C\Z. Let Mk denote the number of tuples in
state-space of Z. Then we can re-write the numerator as:
M
X

(c0k , ci )P (ci , e)

=

i=1

Mk
X

P (c0k , z i , e)

i=1

and the denominator can be decomposed as:
M
X

P (ci , e) =

i=1

X

Mk
X

P (c0k , z i , e)

ck D(Ck ) i=1

Then, we can re-write the expression for P (c0k |e) as follows:
PM k
0
i
0
i=1 P (ck , z , e)
P (ck |e) = P
PM k
i
ck D(Ck )
i=1 P (ck , z , e)

(24)

Let hck be the number of full cutset tuples where cik = ck . Then, we can decompose the
numerator in Eq. (24) as follows:
Mk
X
i=1

hc0

P (c0k , z i , e)

=

k
X

P (c0k , z i , e)

+

Mk
X

i=hc0 +1

i=1

k

366

P (c0k , z i , e)

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Similarly, we can decompose the sums in the denominator:
X

Mk
X

P (ck , z , e) =

ck D(Ck ) i=1

hck
X

X

i

P (ck , z i , e) +

X

Mk
X

P (ck , z i , e)

ck D(Ck ) i=hck +1

ck D(Ck ) i=1

After decomposition, the Eq. (24) takes on the form:
P (c0k |e)

Phc0k

i
0
i=1 P (ck , z , e)

=P

ck D(Ck )

Phck

i=1 P (ck , z

i , e)

+
+

P Mk

0
i
i=hc0 +1 P (ck , z , e)

P

k

ck D(Ck )

P Mk

i=hck +1 P (ck , z

i , e)

(25)

Now, for conciseness, we can group together all fully instantiated tuples in the denominator:
hck
X

X

P (ck , z i , e) =

h
X

P (ci , e)

i=1

ck D(Ck ) i=1

Then, Eq. (25) transforms into:
P (c0k |e)

Phc0k

0
i
i=1 P (ck , z , e)

= Ph

i=1

Now, we can replace each sum

P (ci , e)

PM k

+

i=hc0 +1

+

PM k

P Mk

i=hck +1

0
i
i=hc0 +1 P (ck , z , e)

P

k

i
ck D(Ck ) P (ck , z , e)

(26)

over unexplored cutset tuples with a sum over

k

the partially-instantiated cutset tuples. Denoting as Mc0k = Mk  hck + 1 the number of
partially instantiated cutset tuples for Ck = ck , we obtain:
0

PMc0k
j
0 , z i , e) +
0
P
(c
i=1
j=1 P (ck , z1:qj , e)
k
P (c0k |e) = P
PMc0k P
j
h
i
ck D(Ck ) P (ck , z1:qj , e)
i=1 P (c , e) +
j=1
Phc0k

(27)

In order to obtain lower and upper bounds formulation, we will separate the sum of joint
j
, e) where Ck = c0k from the rest:
probabilities P (c0k , z1:q
j
P (c0k |e) =
Ph

i=1 P (c

i , e)

0
PMc0k
j
0
i
0
i=1 P (ck , z , e) +
j=1 P (ck , z1:qj , e)
0
PMc0
PMc0 P
j
j
, e)
+ j=1k P (c0k , z1:q
, e) + j=1k ck 6=c0 P (ck , z1:q
j
j
k

Phc0k

(28)

In the expression above, probabilities P (ck , z i , e) and P (ci , e) are computed exactly since
i , e), however, will
they correspond to full cutset instantiations. Probabilities P (ck , z1:q
i
be bounded since only partial cutset is observed. Observing that both the numerator
i , e) and replacing it with an upper bound
and denominator have component P (c0k , z1:q
i
i , e) in both the numerator and denominator, we will obtain an upper bound
P U (c0k , z1:q
i
on P (c0k |e) due to Lemma 3.2:
P (c0k |e) 
Ph

i=1

P (ci , e)

0
PMc0k U 0 j
0
i
i=1 P (ck , z , e) +
j=1 PA (ck , z1:qj , e)
0
M
P c0
PMc0 P
j
j
, e)
+ j=1k PAU (c0k , z1:q
, e) + j=1k ck 6=c0 P (ck , z1:q
j
j
k

Phc0k

367

(29)

fiBidyuk, Dechter & Rollon

j
Finally, replacing P (ck , z1:q
, e), ck 6= c0k , with a lower bound (also increasing fraction value),
j
we obtain:

Phc0k

PMc0k
0
i
i=1 P (ck , z , e) +
j=1
PMc0k U 0 j
j=1 PA (ck , z1:qj , e) +

j
, e)
PAU (c0k , z1:q
j
= PcU
P
0 P
P
M
ck
j
h
L
i
ck 6=c0k PA (ck , z1:qj , e)
i=1 P (c , e) +
j=1
(30)
i , e)
The lower bound derivation is similar. We start with Eq. (28) and replace P (c0k , z1:q
i
in the numerator and denominator with a lower bound. Lemma 3.2 guarantees that the
resulting fraction will be a lower bound on P (c0k |e):

P (c0k |e)

P (c0k |e) 
Ph

i=1

P (ci , e)

0
PMc0k L 0 j
0
i
i=1 P (ck , z , e) +
j=1 PA (ck , z1:qj , e)
0
M
P c0
PMc0k P
j
j
+ j=1k PAL (c0k , z1:q
,
e)
+
ck 6=c0k P (ck , z1:qj , e)
j=1
j

Phc0k

(31)

P
j
j
, e) and
Finally, grouping PAL (c0k , z1:q
ck 6=c0k P (ck , z1:qj , e) under one sum and replacing
j
P
j
j
PAL (c0k , z1:q
, e) with an upper bound, we obtain the lower bound PcL :
, e) + ck 6=c0 P (ck , z1:q
j
j
k

P (c0k |e)

0

0
i
i=1 P (ck , z , e) +


Ph

i=1

where

Phc0k

j
, e)
U B[PAL (c0k , z1:q
j

P (ci , e)

+

X

+

0

PMc0k

PMc0k
j=1

j
, e)
PAL (c0k , z1:q
j

j
L 0
j=1 U B[PA (ck , z1:qj , e)

j
, e)]
P (ck , z1:q
j

= min

ck 6=c0k

(

+

P

= PcL

(32)

j
ck 6=c0k P (ck , z1:qj , e)]

j
, e) +
PAL (c0k , z1:q
j
j
, e)
PAU (z1:q
j

P

ck 6=c0k

j
, e)
PAU (ck , z1:q
j

The lower bound PcL is a cutset equivalent of the lower bound P L obtained in Eq. (15).
With respect to computing bounds on P (c0k , z1:q , e) in Eq. (30) and (32) in practice, we
distinguish two cases. We demonstrate them on the example of upper bound.
In the first case, each partially instantiated
tuple c1:q that includes node Ck , namely
S
k  q, can be decomposed as c1:q = z1:q c0k so that:
P U (c0k , z1:q , e) = P U (c1:q , e)

The second case concerns the partially instantiated tuples c1:q that do not include node
Ck , namely k > q. In that case, we compute upper bound by decomposing:
P U (c0k , z1:q , e) = P U (ck |c1:q )P U (c1:q , e)

Appendix C. ATB Properties
Theorem 3.2 AT B bounds interval length is upper bounded by a monotonic nonincreasing function of h:
PM
j
j=h+1 P (c )
U
L
PA (x|e)  PA (x|e)  Ph
, Ih
P
M
i
j
i=1 P (c , e) +
j=h+1 P (c )
368

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Proof. The upper bound on the bounds interval follows from the fact that, PAU (x|e) 
U (x|e)  P L (x|e) and from the definitions of brute force lower and up PBF
BF
per bounds given by Eq. (21) and (22). We only need to prove that the upper bound is
monotonously non-increasing as a function of h.
P
PM
j
j
P (ch ) + M
j=h+1 P (c )
j=h P (c )
=
Ih1 = Ph1
P
P
P
M
M
h1
j
j
i
i
h
j=h P (c )
j=h+1 P (c )
i=1 P (c , e) +
i=1 P (c , e) + P (c ) +

PAL (x|e)

Since P (ch )  P (ch , e), then replacing P (ch ) with P (ch , e) and applying Lemma 3.1, yields:
P
P
j
j
P (ch , e) + M
P (ch , e) + M
j=h+1 P (c )
j=h+1 P (c )
Ih1  Ph1
= Ph
PM
PM
i
h
j
i
j
i=1 P (c , e) + P (c , e) +
j=h+1 P (c )
i=1 P (c , e) +
j=h+1 P (c )
PM
j
j=h+1 P (c )
 Ph
= Ih
P
M
i
j
i=1 P (c , e) +
j=h+1 P (c )

Thus, Ih1  Ih .



References
Abdelbar, A. M., & Hedetniemi, S. M. (1998). Approximating MAPs for belief networks is
NP-hard and other theorems. Artificial Intelligence, 102, 2138.
Andreassen, S., Jensen, F., Andersen, S., Falck, B., Kjaerulff, U., Woldbye, M., Srensen, A.,
Rosenfalck, A., & Jensen, F. (1990). Munin - an expert EMG assistant. In Desmedt,
J. E. (Ed.), Computer-Aided Electromyography and Expert Systems, chap. 21. Elsevier
Science Publishers, Amsterdam.
Becker, A., & Geiger, D. (1996). A sufficiently fast algorithm for finding close to optimal
junction trees. In Proceedings of the 12th Conference on Uncertainty in Artificial
Intelligence (UAI-96), pp. 8189, Portland, Oregon, USA. Morgan Kaufmann.
Beinlich, I., Suermondt, G., Chavez, R., & Cooper, G. (1989). The ALARM monitoring
system: A case study with two probabilistic inference techniques for belief networks. In
Proceedings of the Second European Conference on AI and Medicine. SpringerVerlag.
Bidyuk, B. (2006). Exploiting Graph Cutsets for Sampling-Based Approximations in
Bayesian Networks. Ph.D. Thesis. Ph.D. thesis, University of California, Irvine.
Bidyuk, B., & Dechter, R. (2003a). Cycle-cutset sampling for Bayesian networks. In Proceedings of the 16th Canadian Conference on Artificial Intelligence (AI2006), pp.
297312, Halifax, Canada.
Bidyuk, B., & Dechter, R. (2003b). Empirical study of w-cutset sampling for Bayesian networks. In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence
(UAI-2003), pp. 3746, Acapulco, Mexico. Morgan Kaufmann.
Bidyuk, B., & Dechter, R. (2006a). An anytime scheme for bounding posterior beliefs. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI2006),
pp. 10951100, Boston, MA, USA.
369

fiBidyuk, Dechter & Rollon

Bidyuk, B., & Dechter, R. (2006b). Improving bound propagation. In Proceedings of the
17th European Conference on AI (ECAI2006), pp. 342346, Riva Del Garda, Italy.
Bidyuk, B., & Dechter, R. (2007). Cutset sampling for bayesian networks. Journal of
Artificial Intelligence Research, 28, 148.
Cooper, G. (1990). The computational complexity of probabilistic inferences. Artificial
Intelligence, 42, 393405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60 (1), 141153.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113, 4185.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for bounded inference.
Journal of the ACM, 50, 107153.
Gogate, V., & Dechter, R. (2007). Samplesearch: A scheme that searches for consistent
samples. In Proceedings of 11th International Conference on Artificial Intelligence
and Statistics (AISTATS2007), pp. 198203.
Horvitz, E., Suermondt, H., & Cooper, G. (1989). Bounded conditioning: Flexible inference for decisions under scarce resources. In Workshop on Uncertainty in Artificial
Intelligence, pp. 181193, Windsor, ON.
Ihler, A. (2007). Accuracy bounds for belief propagation. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI-2007), pp. 183190, Corvallis,
Oregon. AUAI Press.
Jaakkola, T. S., & Jordan, M. I. (1999). Variational probabilistic inference and the qmr-dt
network. Journal of Artificial Intelligence Research, 10, 291322.
Kask, K., & Dechter, R. (1999). Stochastic local search for Bayesian networks. In Heckerman, D., & Whittaker, J. (Eds.), Workshop on AI and Statistics, pp. 113122. Morgan
Kaufmann.
Kearns, M., & Saul, L. (1998). Large deviation methods for approximate probabilistic inference, with rates of convergence. In Proceedings of the 14th Conference on Uncertainty
in Artificial Intelligence (UAI), pp. 311319. Morgan Kaufmann.
Kearns, M., & Saul, L. (1999). Inference in multilayer networks via large deviation bounds.
Advances in Neural Information Processing Systems, 11, 260266.
Kristensen, K., & Rasmussen, I. (2002). The use of a Bayesian network in the design
of a decision support system for growing malting Barley without use of pesticides.
Computers and Electronics in Agriculture, 33, 197217.
Larkin, D. (2003). Approximate decomposition: A method for bounding and estimating
probabilistic and deterministic queries. In Proceedings of the 19th Conference on
Uncertainty in Artificial Intelligence (UAI-2003), pp. 346353, Acapulco, Mexico.
Leisink, M. A. R., & Kappen, H. J. (2003). Bound propagation. Journal of Artificial
Intelligence Research, 19, 139154.
370

fiActive Tuples-based Scheme for Bounding Posterior Beliefs

Mannino, M. V., & Mookerjee, V. S. (2002). Probability bounds for goal directed queries in
Bayesian networks. IEEE Transactions on Knowledge and Data Engineering, 14 (5),
11961200.
Monti, S., & Cooper, G. (1996). Bounded recursive decomposition: a search-based method
for belief network inference under limited resources. International Journal of Approximate Reasoning, 15, 4975.
Mooij, J. M., & Kappen, H. J. (2008). Bounds on marginal probability distributions. In
Advances in Neural Information Processing Systems 22 (NIPS2008), pp. 11051112,
Vancouver, British Columbia, Canada, December 8-11.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Poole, D. (1996). Probabilistic conflicts in a search algorithm for estimating posterior
probabilities in Bayesian networks. Artificial Intelligence, 88 (12), 69100.
Poole, D. (1998). Context-specific approximation in probabilistic inference. In Proceedings
of 14th Uncertainty in Artificial Intelligence (UAI-98), pp. 447454.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering for
large belief networks. In Proceedings of 10th Conference on Uncertainty in Artificial
Intelligence, Seattle, WA, pp. 484490.
Rollon, E., & Dechter, R. (2010). New mini-bucket partitioning heuristics for bounding the
probability of evidence. In Proceedings of the 24th National Conference on Artificial
Intelligence (AAAI2010), pp. 11991204, Atlanta, GA.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (1-2),
273302.
Shekhar, S. (2009). Fixing and extending the multiplicative approximation scheme. Masters
thesis, School of Information and Computer Science, University of California, Irvine.
Taga, N., & Mase, S. (2006). Error bounds between marginal probabilities and beliefs of
loopy belief propagation algorithm. In Advances in Artificial Intelligence, Proceedings
of 5th Mexican International Conference on Artificial Intelligence (MICAI2006), pp.
186196, Apizaco, Mexico, November 13-17.
Tatikonda, S. C. (2003). Convergence of the sum-product algorithm. In Proceedings of
IEEE Information Theory Workshop, pp. 222225.
Wainwright, M. J., Jaakkola, T., & Willsky, A. S. (2005). A new class of upper bounds on
the log partition function. IEEE Trans. on Information Theory, 51 (7), 23132335.
Wexler, Y., & Meek, C. (2008). MAS: a multiplicative approximation scheme for probabilistic inference. In Koller, D., Schuurmans, D., Bengio, Y., & Bottou, L. (Eds.),
Advances in Neural Information Processing Systems 22 (NIPS2008), pp. 17611768.
MIT Press.
Wexler, Y., & Meek, C. (2010). Approximating max-sum-product problems using multiplicative error bounds. In Bayesian Statistics 9, p. to appear. Oxford University
Press.

371

fiJournal of Artificial Intelligence Research 39 (2010) 179- 216

Submitted 04/10; published 09/10

Cooperative Games with Overlapping Coalitions
Georgios Chalkiadakis

GC 2@ ECS . SOTON . AC . UK

School of Electronics and Computer Science,
University of Southampton, SO17 1BJ, UK

Edith Elkind

EELKIND @ NTU . EDU . SG

School of Physical and Mathematical Sciences,
Nanyang Technological University, 637371, Singapore

Evangelos Markakis

MARKAKIS @ GMAIL . COM

Department of Infomatics,
Athens University of Economics and Business, GR10434, Greece

Maria Polukarov
Nicholas R. Jennings

MP 3@ ECS . SOTON . AC . UK
NRJ @ ECS . SOTON . AC . UK

School of Electronics and Computer Science,
University of Southampton, SO17 1BJ, UK

Abstract
In the usual models of cooperative game theory, the outcome of a coalition formation process is
either the grand coalition or a coalition structure that consists of disjoint coalitions. However, in
many domains where coalitions are associated with tasks, an agent may be involved in executing
more than one task, and thus may distribute his resources among several coalitions. To tackle such
scenarios, we introduce a model for cooperative games with overlapping coalitionsor overlapping coalition formation (OCF) games. We then explore the issue of stability in this setting. In
particular, we introduce a notion of the core, which generalizes the corresponding notion in the
traditional (non-overlapping) scenario. Then, under some quite general conditions, we characterize
the elements of the core, and show that any element of the core maximizes the social welfare. We
also introduce a concept of balancedness for overlapping coalitional games, and use it to characterize coalition structures that can be extended to elements of the core. Finally, we generalize the
notion of convexity to our setting, and show that under some natural assumptions convex games
have a non-empty core. Moreover, we introduce two alternative notions of stability in OCF that
allow a wider range of deviations, and explore the relationships among the corresponding definitions of the core, as well as the classic (non-overlapping) core and the Aubin core. We illustrate the
general properties of the three cores, and also study them from a computational perspective, thus
obtaining additional insights into their fundamental structure.

1. Introduction
Coalition formation, widely studied in game theory and economics (Myerson, 1991), has attracted
much attention in AI as means of forming teams of autonomous selfish agents that need to cooperate
to perform certain tasks (Sandholm & Lesser, 1997; Shehory & Kraus, 1998; Sandholm, Larson,
Andersson, Shehory, & Tohme, 1999; Manisterski, Sarne, & Kraus, 2008; Rahwan, Ramchurn,
Jennings, & Giovannucci, 2009). Traditionally, in the game theory literature it is assumed that the
outcome of the coalition formation process is either the grand coalition (i.e., the set of all agents), or
a coalition structure that consists of disjoint coalitions (i.e., a partition of the set of agents). While
natural for some settings, in many scenarios of interest this assumption is not applicable.
c
2010
AI Access Foundation. All rights reserved.

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

Specifically, it is often natural to associate coalitions with tasks to be performed by the agents. In
such situations, some agents may be involved in several tasks, and therefore may need to distribute
their resources among the coalitions in which they participate. Indeed, such overlaps may be
necessary to obtain a good outcome, and are natural in a plethora of interesting applications. As
a simple e-commerce example, consider online trading agents representing individuals or virtual
enterprises, and facing the challenge of allocating their owners capital to a variety of projects
(i.e., coalitions) simultaneously. There are many other examples of settings in which an agent (be
it a software entity or a human) splits his resources (such as processing power, time or money)
among several tasks. These tasks, in turn, may require the participation of more than one agent: a
computation may run on several servers, a software project usually involves more than one engineer,
and a start-up may rely on several investors. Thus, each task corresponds to a coalition of agents,
but agents contributions to those coalitions may be fractional, and, moreover, agents can participate
in several tasks at once, resulting in coalition structures with overlapping coalitions. The formation
of overlapping coalitions is particularly prevalent in systems demanding multiagent or multirobot
coordination, computational grid networks, and sensor networkssee, e.g., the work of Patel et
al. (2005), and Dang, Dash, Rogers, & Jennings (2006). To date, however, there has been essentially
no theoretical treatment of the topic, with just a few exceptions (which we discuss in Section 3).
Against this background, the goal of this paper is to introduce and study a model that explicitly
takes overlapping coalition formation (OCF) into account. Our model is applicable in situations
where agents need to allocate different parts of their resources to simultaneously serve different
tasks as members of different coalitions. Besides allowing for overlapping coalitions, it departs
from the conventional coalition formation framework in two important aspects. First, there is no
inherent superadditivity assumption in our work, and hence the grand coalition does not always
emerge. Thus, our subsequent definition of the core incorporates coalition structures. Second, exactly because we are interested in outcomes other than the grand coalition formation, we do not
use the standard transferable utility (TU) framework, where agents can make arbitrary payments to
each other. Instead, following the seminal paper by Aumann and Dreze (1974), we allow arbitrary
monetary transfers within coalitions, but not cross-coalitional transfers. That is, an agent not contributing to a coalition should not expect to receive payoff from it. Indeed, as argued by Aumann
and Dreze, the inability of some of the agents to work together and share payoffs may be one of the
primary reasons why the grand coalition does not form, and a particular coalition structure arises.
Finally, our model can take task (coalitional action) execution explicitly into account; this facilitates
possible extensions to tackle coalition formation under uncertainty.1
Apart from defining a model for overlapping coalition formation, the main contribution of this
work is exploring the stability concept of the core in the OCF setting. We suggest three different
notions of the core, depending on the nature of deviations allowed, since, as we shall see, the
range of permissible deviations in an overlapping setting can be much richer than in the traditional
non-overlapping one. More specifically, the definition of stability depends on whether a deviator
who reduced his contribution to somebut not allcoalitions, expects to get any payoff from the
coalitions that he did not abandon completely.
To provide more intuition, consider the example of two construction companies, 1 and 2, who
are currently partners (not necessarily the only partners) working on construction projects A (building a university campus) and B (building a hospital). Assume that partner 1 has more stakes in
1. To simplify notation, we only show how to incorporate coalitional actions in the model in Section 10.

180

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

project B, expecting to extract from it a great value, and has contributed to it 75% of its available resources, contributing the remaining 25% to A; while partner 2 contributes most of its resources (say
67%) to project A and the remaining fraction (say 33%) to B. Thus, they currently participate in two
overlapping coalitions, each one performing a different task. Now, if partner 2 feels unhappy about
the current payoff division arrangement, it might consider abandoning project A (by cancelling the
project if it is the project leader, or by taking advantage of some contractual exit clause) in order to
commit its resources to a more profitable to 2 project (say C). However, by doing so, it might hurt
project As chances of completion. Does this mean that 2s actions will trigger the spite of company
1, which might use available means to kick 2 out of project B? And what if company 2 lowered
its degree of participation in A instead of withdrawing completely? How much of the profits from
completing A would 2 then be entitled to? The different answers one can provide to these questions correspond to different notions of profitable deviations, and, therefore, to different notions of
core-stability. In particular, we demonstrate that the core notions we put forward in this paper are
substantially different from each other with respect to the sets of outcomes they characterize.
Our main technical results involve the c-core, the first core concept that we suggest. Among the
three concepts of the core introduced in this paper, the c-core is the closest to the standard definition
of the core in general non-transferable utility (NTU) games. In particular, we provide conditions
for the existence of the c-core as follows. Under quite general assumptions, we first provide a
characterization for outcomes, i.e., pairs of the form (overlapping coalition structure, imputation),
to be in the c-core. Our proof is based on a graph-theoretic argument, which may be of independent
interest. As a corollary of this result, we show that any outcome in the c-core maximizes the social
welfare. Second, we characterize coalition structures that admit payoff allocations such that the
resulting outcome is in the c-core. This is done by generalizing the Bondareva-Shapley theorem
to our setting (note that this theorem does not hold for arbitrary non-transferable utility games).
Furthermore, we extend the notion of convexity in coalitional games to overlapping coalitions, and
show that under mild assumptions any convex OCF game has a non-empty c-core.
We then discuss the properties of all three versions of the OCF-core we suggest, and relate
them to each other and to the classic core. We also demonstrate how our model and core concepts differ from fuzzy coalitional games (Aubin, 1981); though relevant to that model, our work
is fundamentally different. In addition, we initiate the study of computational aspects of stability
in the overlapping setting. Note that the computational analysis of coalitional games, even in nonoverlapping scenarios, is hindered by the fact that, in general, coalitional games do not possess a
compact representation, as one may have to list the value of every possible coalition. Thus, the existing work on algorithmic aspects of coalitional games focused on game representations that are either incompletesuch as, e.g., weighted voting games (Elkind, Goldberg, Goldberg, & Wooldridge,
2009), induced subgraph games (Deng & Papadimitriou, 1994), or network flow games (Bachrach
& Rosenschein, 2007)or are only guaranteed to be succinct for specific subclasses of games, such
as MC-nets (Ieong & Shoham, 2005) or coalitional skill games (Bachrach & Rosenschein, 2008);
another approach is to show complexity bounds for all games representable by polynomial-sized
circuits (Greco, Malizia, Palopoli, & Scarcello, 2009). This issue is even more severe in the OCF
setting, as now we have to specify the value of every partial coalition. Therefore, in this paper,
we follow the first of these approaches, and introduce a formalism of threshold task games that is
capable of describing a large family of overlapping coalition formation settings in a succinct manner. Within this formalism, we obtain both negative and positive results regarding the complexity of
181

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

deciding the questions of membership and non-emptiness for our OCF-core concepts. We conclude
by describing some natural extensions of our model and suggesting directions for future work.2

2. Preliminaries
In this section, we provide a brief overview of the basic concepts in cooperative game theory regarding non-overlapping coalition structures. To begin, let N = {1, . . . , n} be a set of players (or
agents). A subset S  N is called a coalition. A coalition structure (CS ) in non-overlapping
environments is a partition of the set of agents.
Under the assumption of transferable utility, coalition formation can be abstracted into a fairly
simple model. This assumption postulates the existence of a (divisible) commodity (e.g., money)
that can be freely transferred among players. The role of the characteristic function of a coalitional
game with transferable utility (TU-game) is to specify a single number denoting the worth of a
coalition. Formally, a characteristic function v : 2N 7 R defines the value v(S) of each coalition
S (von Neumann & Morgenstern, 1944). A transferable utility game G is completely specified by
the set of players N and the characteristic function v; we can therefore write G = (N, v).
While the characteristic function describes the payoffs available to coalitions, it does not prescribe a way of distributing these payoffs. This is captured by the notion of an imputation, defined as
follows. We say that an allocation is a vector of payoffs x = (x1 , . . . , xn ) assigning
Psome payoff to
each j  N . An allocation x is efficient with respect to a coalition structure CS if jS xj = v(S)
for all S  CS ; and it is called an imputation if it is efficient and satisfies individual rationality, i.e.,
xj  v({j}) for j = 1, . . . , n. The set of all imputations of CS is denoted by I(CS ).
Now, when rational agents seek to maximize their individual payoffs, the stability of the underlying coalition structure becomes critical, as agents might be tempted to abandon agreements in
pursuit of further gains for themselves. A structure is stable only if the outcomes attained by the
coalitions and the payoff combinations agreed to by the agents satisfy both individual and group
rationality. Given this requirement, research in coalition formation has developed several notions of
stability, among the strongest and the most well-studied ones being the core (Gillies, 1953). Taking
coalition structures into account, the core of a TU game is a set of outcomes (CS , x), x  I(CS ),
such that no subgroup of agents is motivated to depart from their coalitions in CS .
Definition 1. Let CS be a coalition structure, and let x  Rn be an allocation of payoffs to the
agents. The core of a TUP
game (N, v) is the set of all pairs (CS , x) such that x  I(CS ) and for
any S  N it holds that jS xj  v(S).
Hence, no coalition would ever block the proposal for a core allocation. It is well-known that
the core is a strong notion, and there exist many games where it is empty (Myerson, 1991).
The core definition above is essentially the definition provided by Sandholm and Lesser (1997)
(and is also very similar to the one given by Dieckmann & Schwalbe, 1998). If we assume superadditivity of the characteristic function (i.e., v(U  T )  v(U ) + v(T ) for any disjoint coalitions U
and T ) then in the definition above we may only consider outcomes where CS is simply the grand
2. Parts of this work, namely the model and the statement of some of our results, have appeared in a preliminary
conference paper (Chalkiadakis, Elkind, Markakis, & Jennings, 2008). However, (a) the introduction of alternative
notions of the core and all related results presented here are entirely novel; (b) similarly, our complexity-related
results are entirely novel; and (c) the discussion on the properties of the cores and the in-depth comparison with
fuzzy coalitional games appear here for the first time as well.

182

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

P
coalition and jN xj = v(N ). The core definition then becomes the traditional definition that has
been used in the vast majority of the economics literature (Osborne & Rubinstein, 1994).
The environments of interest in our work however are mainly non-superadditive and we will
not make any such assumption on the characteristic function. Indeed, there is a plethora of realistic
application scenarios where the emergence of the grand coalition is either not guaranteed, might be
perceivably harmful, or is plainly impossible (Sandholm & Lesser, 1997; Sandholm et al., 1999).
In addition to such motivations, Aumann and Dreze (1974) also provide a thorough and insightful
discussion on why coalition structures arise: they put forward a series of arguments on how this
might happen, and explain that coalition structures may emerge naturally even in superadditive
environments for a variety of reasons. Briefly, their arguments describe how a subset of agents
might find it more worthwhile to bargain within the framework of a specific structure, than within
the framework of the grand coalition; or how the emergence of a coalition structure may reflect
considerations that are by necessity excluded from the formal description of the game because they
are impossible to measure or communicate. Exogenous arguments for the emergence of coalition
structures naturally include the impossibility of communication among all negotiators, or the by law
prohibition of the grand coalition (Aumann & Dreze, 1974).

3. Related Work
The work that is most relevant to ours is the research on fuzzy coalitional games, introduced by
Aubin (1981). Branzei, Dimitrov, & Tijs (2005) also provide a detailed exposition of such games.
A player in a fuzzy game can participate in a coalition at various levels, and the value of a coalition
S depends on the participation levels of the agents in S. Given this model, Aubin then defines the
core for fuzzy games (also referred to as the Aubin core). Though our model also allows for partial
participation in a coalition, there are several crucial differences between fuzzy games and OCF
games, and the corresponding notions of stability. We postpone listing these until after presenting
our model and results, but will do so in Section 8.2. For now, let us just point out that, in distinction
to our work, the formation of coalition structures (overlapping or not) is not addressed in the fuzzy
games literature.
Apart from fuzzy games, very little work exists on overlapping coalition formation settings.
Here we discuss some notable exceptions, as well as some related work on the core in the context
of non-overlapping coalition structures.
To begin, Shehory and Kraus (1996) present a setting for overlapping coalition formation. In
their model, the agents have goals and capabilitiesi.e., abilities to execute certain actions. To
serve their goals, the agents have to participate in coalitions, to each of which they contribute some
of their capabilities, which can thus be thought of as resources. The authors then propose heuristic
algorithms that lead to the creation of overlapping coalition structures. However, the authors stop
short of addressing the question of the stability of overlapping coalitions. Dang et al. (2006) also
examine heuristic algorithms for overlapping coalition formation to be used in surveillance multisensor networks. However, their work does not deal with payoff allocation issues, and does not
view the overlapping coalition formation problem from a game-theoretic perspective.
Conconi and Perroni (2001) present a model of international multidimensional policy coordination in a non-cooperative setting: agreement structures between countries can be overlapping,
namely a country may participate in multiple agreements, by contributing any number of proposed
elementary strategies (which can be regarded as being chosen from discrete sets of resources).
183

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

They then introduce an equilibrium concept to describe stability in this setting. However, in contrast to our work, the setting in the work of Conconi and Perroni is non-cooperative, and does not
apply to agents with continuous resources.
More recently, Albizuri, Aurrecoechea, & Zarzuelo (2006) presented an extension of Owens
value (1977)which, in turn, can be thought of as a generalization of the Shapley value (1953)to
an overlapping coalition formation setting. Specifically, they present an axiomatic characterization
of their configuration value. However, in the work of Albizuri et al. there exists no notion of
resources that an agent needs to distribute across coalitions.
With regard to non-overlapping coalition structures as presented in Section 2, Sandholm and
Lesser (1997) examine the problem of allocating computational resources to coalitions. They do not
restrict themselves to superadditive settings, but discuss the stability of coalition structures instead.
In particular, they introduce a notion of bounded rational core that explicitly takes into account coalition structures. Apt & Radzik (2006) and Apt & Witzel (2009) also do not restrain themselves to
coalition formation problems where the outcome is the grand coalition only. Instead, they introduce
various stability notions for abstract games whose outcomes can be coalition structures, and discuss
simple transformations (e.g., split and merge rules) by which stable partitions of the set of players
may emerge. However, none of these papers considers any extensions to overlapping coalitions.

4. Our Model
In this section we extend the traditional model of Section 2 to cooperative games with overlapping
coalitions. In most scenarios of interest, even if overlapping coalitions are allowed, an agent would
not be able to participate in all possible coalitions due to lack of time, cash flow, or energy. To model
this, we assume that each agent possesses a certain amount of resources which he can distribute
among the coalitions he joins. Without loss of generality, we can make a normalization and assume
that each agent has one unit of resource: an agents contribution to a coalition is thus given by the
fraction of his resources that he allocates to it. We can also think of this as the agents participation
level, or the fraction of time he devotes to a coalition. Of course, an agent may own several types
of resources (e.g., time and money), and his contribution to a coalition would then be described
by a vector rather than a scalar. Our model, and all of our results, extend to this more general
setting in a straightforward manner. Nevertheless, for conciseness, we restrict our presentation to
the single-resource setting.
As discussed above, in the non-overlapping model a coalition is a subset of agents, and a game
is defined by its characteristic function v : 2N 7 R, representing the maximum total payoff that
a coalition can get. In our setting, a partial coalition is given by a vector r = (r1 , . . . , rn ), where
rj is the fraction of agent js resources contributed to this coalition (rj = 0 means that j is not a
member of the coalition). The support of a partial coalition r is denoted by supp(r) and is defined
as supp(r) = {j  N | rj 6= 0}. We can now define the cooperative games with overlapping
coalitions, or overlapping coalition formation games (OCF-games for short), which we will be
considering in the rest of this work.
Definition 2. An OCF-game G with player set N = {1, . . . , n} is given by a function v : [0, 1]n 
R, where v(0n ) = 0.
Function v maps each partial coalition r to the corresponding payoff. We denote this game by
G = (N, v), or, if N is clear from the context, simply by v. Clearly, a classic coalition S  N
184

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

can now be represented as the vector eS , where (eS )j = 1 for j  S and 0 otherwise. In the
economics literature, these are sometimes called crisp coalitions, whereas coalitions of the form
(r1 , . . . , rn ) with at least one rj in (0, 1) are referred to as fuzzy coalitions (Branzei et al., 2005).
We will avoid the latter term in our work so as not to cause confusion with fuzzy games, and refer
instead to coalitions of this kind as partial coalitions, or simply coalitions.
In most scenarios of interest, v is monotone, i.e., satisfies v(r)  v(r  ) for any r, r  such that
rj  rj for all j = 1, . . . , n. Note that if v is monotone, we have v(r)  0 for any r  [0, 1]n , since
we set v(0, . . . , 0) = 0. In our discussion of stability of overlapping coalitions, we will assume that
v is monotone.
We now need to specify the possible outcomes of an OCF-game. In the non-overlapping setting,
an outcome is a pair (CS , x), where CS is a partition on N and x is an imputation for CS . To
extend this definition to our scenario, we start by introducing the notion of a coalition structure
with overlapping coalitions. While we will be mostly interested in coalition structures over N , the
definition below is given for coalition structures over an arbitrary subset T  N , as this will be
useful for defining the maximum profit a subset of agents can achieve (see the definition of the
function v  below).
Definition 3. For a set of agents T  N , a coalition structure on T is a finite list of vectors
(partial coalitions) CS TP= (r 1 , . . . , r k ) that satisfies (i) r i  [0, 1]n ; (ii) supp(r i )  T for all
i = 1, . . . , k; and (iii) ki=1 rji  1 for all j  T . We will refer to k as the size of the coalition
structure CS T and write |CS T | = k. Also, CS T denotes the set of all coalition structures on T .
In the definition above, each r i = (r1i , r2i , . . . , rni ) corresponds to some partial coalition (rji
being the fraction of the resources that agent j contributes to r i ). The constraints state that every
agent from T distributes at most one unit of his resources among the various coalitions he participates in (those may include the singleton coalition). This allows coalitions to be overlapping. Note
that the coalition structure is a list rather than a set, i.e., it can contain two or more identical partial
coalitions. Observe
also that an agent is not required to allocate all of his resources, i.e., it can be
P
the case that ki=1 rji < 1. However, under monotonicity, we can assume that for each agent j we
P
have ki=1 rji = 1 (i.e., a coalition structure is a fractional partition of the agents).
We would like to remark that one could conceive of other models that also allow agents to form
overlapping coalitions. As an example, instead of requiring agents to distribute at most one unit of
resources among partial coalitions, we could have constraints on the number of (crisp) coalitions
an agent could take part in. While we believe that our model is flexible enough to represent a wide
range of realisitc scenarios, and we focus on it throughout our work, in Section 10, we discuss
several extensions of our model.
The introduction of overlapping coalition structures imposes some new technical challenges.
For instance, while in the non-overlapping setting the number of different coalition structures is
finite, in our setting there can be infinitely many different partial coalitions, and hence infinitely
many coalition structures. This implies that it is impossible to find the social welfare-maximizing
coalition structure by enumerating all candidate solutionsin fact, the maximum may not even be
attained. In contrast, in a non-OCF setting this approach is possiblethough, in general,
P infeasible.
We now extend the definition of v to coalition structures by setting v(CS ) =
r CS v(r).
Furthermore, for any S  N we define v  (S) = supCS CS S v(CS ). Intuitively, v  (S) is the
least upper bound on the value that the members of S can achieve by forming a coalition structure;
for the interested reader, we note that it corresponds to the characteristic function of the games
185

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

superadditive cover (Aumann & Dreze, 1974). Clearly, v  (S) may exceed the value of coalition
S itself, i.e., v(eS ), since it may be profitable for the players in S to form several overlapping
coalitions over S. We say that v is bounded if v  (N ) < ; for most games of interest, v is likely
to be bounded.
As in our setting the agents will not necessarily form the grand coalition, we will be interested
in reasoning about coalition structures from CS N . The coalition structure will impose restrictions
on admissible ways of distributing the gains; a payoff vector corresponds to an imputation if and
only if it is obtained by distributing the value of each coalition:
Definition 4. Given a coalition structure CS  CS N , |CS | = k, an imputation for CS is a k-tuple
x = (x1 , . . . , xk ), where xi  Rn for i = 1, . . . , k, such that
P
 (Payoff Distribution) for every partial coalition r i  CS we have nj=1 xij = v(r i ) and rji =
0 implies xij = 0;
 (Individual Rationality)
the total payoff of agent j is at least as large as what he can achieve
Pk
i
on his own: i=1 xj  v  ({j}).

The set of all imputations for CS is denoted by I(CS ). Notice that in Definition 4, the profit
from a task assigned to a partial coalition is only distributed among agents involved in executing it.
Thus, no transfers of that payoff are allowed to outsiders. Note also that the individual rationality
constraint is defined in terms of v  rather than v, as even for a single agent it may be profitable to
split into several partial coalitions (e.g., if there are many tasks, each of which only requires a small
fraction of his resources).
Now, the set of outcomes that is of interest to us is the set of feasible agreements:

Definition 5. A feasible agreement (or an outcome) for a set of agents J  N is a tuple (CS , x)
where CS  CS J , |CS | = k for some k  N, and x = (x1 , . . . , xk )  I(CS ). We denote the set
of all feasible agreements for J by F(J).
P
The payoff pj of an agent j under a feasible agreement (CS , x) is pj (CS , x) = ki=1 xij . We
write p(CS , x) to denote the vector (p1 (CS , x), . . . , pn (CS , x)). Finally, note that it is straightforward to extend the definitions above to games on subsets of the agents. In particular, we require
that an imputation x  I(CS J ) satisfies xij = 0 for j 6 J.
Given this model, we are now ready to define the concept of the core for cooperative games with
overlapping coalitions.

5. The Core with Overlapping Coalitions
In this section, we investigate several approaches to defining stability in OCF-games. Specifically,
here we propose and analyze three alternative definitions of the core.
Before presenting the core definitions, we define a new class of games, which we will be using
as our running example, namely the class of threshold task games (TTGs). TTGs form a simple,
but expressive class of coalitional games, and can be used to model collaboration in multi-agent
systems. In TTGs agents pool resources in order to accomplish tasks, so the idea of agents contributing resources to more than one task and thus participating in several coalitions simultaneously
is extremely natural in this context. Thus, and due to their simplicity, TTGs provide a convenient
vehicle for the study of core-stability in the overlapping setting, and we will be using them for this
purpose throughout the rest of the paper (though our work is not limited to this class of games).
186

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

5.1 Threshold Task Games
Threshold task games are defined as follows.
Definition 6. A threshold task game G = (N ; w; t) is given by:
 a set of agents N = {1, . . . , n};
 a vector w = (w1 , . . . , wn )  R+ of the agents weights;
 a list t = (t1 , . . . , tm ) of task types, where each task type tj is described by a threshold
T j  0 and a utility uj  0; we write tj = (T j , uj ).
Intuitively, such games describe scenarios where agents can split into teams to work on tasks.
There is one type of resource (e.g., time or money) that is needed for all tasks, and each agent has
a certain amount of this resource which corresponds to his weight wi (we chose the term weight
to avoid confusion with the use of the term resource in the context of OCF-games). There are m
types of tasks, each of which is described by a resource requirement T j and a utility uj . If the team
of agents that works on tj has total weight at least T j , this means that it has sufficient resources to
complete the task, so it obtains the full value of this task uj . Otherwise, its payoff from this task is
0. We assume that there are infinitely many tasks of each type, so that if one team of agents chooses
to work on tj , this does not prevent another team from choosing tj as well. In what follows, we
assume that the list t is monotone, i.e., it satisfies T 1 < . . . < T m and u1 < . . . < um . Indeed, if
there are two task types ti , tj such that T i  T j , but ui  uj , we can safely assume that no team
of agents will choose to work on tj , and hence tj can be deleted from t. Hence, our monotonicity
assumption can be made without loss of generality.
The description above suggests that we can interpret a TTG G = (N, w, t) as a (non-overlapping)
coalitional game G = (N, v), where for S  N we set
v(S) = max{0, max{uj | w(S)  T j }}
(note that we use the standard convention max  = ). Such games provide a direct generalization of weighted voting games (WVGs) with coalition structures introduced by Elkind, Chalkiadakis, & Jennings (2008). Indeed, WVGs with coalition structures can be seen as TTGs in which
there is only one task type t = t1 with utility 1.
At the same time, one can also interpret TTGs as games with overlapping coalitions by allowing
each agent to spread his weight across several tasks. The corresponding OCF-game G = (N, v) is
given by
n
X
v(r1 , . . . , rn ) = max{0, max{uj |
ri wi  T j }}.
i=1

That is, a partial coalition can successfully complete a task of type tj and earn its value uj if the
total weight contributed by all agents to this partial coalition is at least T j .
Example 1. Consider a TTG G = (N ; w; t), where N = {1, 2, 3}, w = (2, 2, 2) and t =
t1 = (3, 1). For the corresponding non-overlapping game G we have v({1}) = 0, v({1, 2}) =
v({1, 2, 3}) = 1. Note that when overlapping coalitions are not allowed, the maximum social
welfare achievable by any coalition structure over N is 1, as agents cannot split into two disjoint
groups each of which having weight at least 3.
187

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

In contrast, for the corresponding OCF-game G = (N, v) we have v(1, 0, 0) = 0, v(1, 1, 0) =
v(1, 1, 1) = 1, and, moreover, v(1, .5, 0) = 1 and v(0, .5, 1) = 1. Hence the maximum social
welfare is 2 in the overlapping setting since the second agent can split his weight between two
coalitions so that each of them has enough resources to complete the task.
From Example 1, it should be clear that for any TTG G, the maximum social welfare achievable
in its overlapping version G is at least as large as the maximum social welfare in its non-overlapping
version Gi.e., allowing agents to split their weights between the tasks can only increase efficiency.
Moreover, this increase can be arbitrarily large even for a single agent. Indeed, consider one agent
of weight w and one task type t with T = 1, u = 1. If overlapping coalitions are not allowed,
the agents total utility is 1, while in the overlapping scenario he can obtain w. For the interested
reader, Appendix A discusses algorithmic aspects of social welfare maximization in TTGs, both in
the overlapping and in the non-overlapping scenario.
5.2 Three Definitions of the Core
As explained in Section 2 above, core-stability implies that no group of agents should be able to
profitably deviate from a configuration in the core. Hence, any definition of the core has to depend
on the notion of permissible deviations used. Now, in the non-overlapping setting a deviator abandons the coalition he originally participated in, and joins a new coalition. Thus, there is no reason
why he should obtain any payoff from the coalition that he left. In the overlapping setting, the situation is less clear-cut. Indeed, when deviating, an agent may abandon some coalitions completely,
withdraw somebut not allof his contribution to other coalitions, and keep his contribution to
the remaining coalitions unchanged. The question then is whether this agent should expect to obtain
any payoff from the partial coalitions with non-deviators that he is still contributing to.
Our first notion of the core assumes that the answer to this question is no. Thus, once an
agent is identified as a deviatori.e., he alters his contribution to any given coalitionhe no longer
expects to benefit from his cooperation with non-deviators. By monotonicity, this means that the
deviators have nothing to gain from contributing resources to coalitions with non-deviators. Therefore, under the first definition of the core which we present here, we assume that the deviators only
form coalitions among themselves, or, in other words, each deviation can be seen as an overlapping
coalition structure over the set of deviators. We remark that this definition can be seen as the most
straightforward generalization of the standard notion of the core: indeed, just as in the standard
setting, each deviator completely withdraws from coalitions with non-deviators, and only benefits
from coalitions with other deviators. We formalize this approach as follows.
Definition 7. Given an OCF-game G = (N, v) and a set of agents J  N , let (CS , x) and
(CS  , y) be two outcomes of G such that for any partial coalition s  CS  either supp(s )  J
or supp(s )  N \ J. Then we say that (CS  , y) is a profitable deviation of J from (CS , x) if for
all j  J we have pj (CS  , y) > pj (CS , x). We say that an outcome (CS , x) is in the core of G if
no subset of agents J has a profitable deviation from it. That is, for any set of agents J  N , any
coalition structure CS J on J, and any imputation y  I(CS J ), we have pj (CS J , y)  pj (CS , x)
for some agent j  J.
In this definition, the deviation CS  is restricted to be a coalition structure in which there are
no partial coalitions involving both the deviators and the non-deviatorsi.e., each partial coalition
contains either deviators only (supp(s )  J) or non-deviators only (supp(s )  N \ J). Thus,
188

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

any payoff that the players in J can receive under CS  would have to come from partial coalitions
over J only.
Example 2. Consider the OCF-game G that corresponds to a threshold task game G = (N ; w; t),
where N = {1, 2}, w = (4, 6), and t = (t1 , t2 ) with t1 = (5, 15), t2 = (4, 10) (one can think of
the players as the two companies A and B discussed in Section 1; the tasks then correspond to the
two construction projects). Suppose that the players form two partial coalitions r 1 and r 2 of total
weight 5 each so that player 1 contributes a unit of weight to r 1 and 3 units of weight to r 2 , while
player 2 contributes 4 units of weight to r 1 , and 2 units of weight to r 2 , that is, CS = (r 1 , r 2 ),
where r 1 = ( 14 , 23 ), r 2 = ( 43 , 13 ). Both of these partial coalitions have weight 5, so each of them can
successfully complete t1 , resulting in a payoff of 15 for each of them. Now, suppose that the players
divide the gains using an imputation x = ((7, 8), (9, 6)). Then, the total payoff obtained by player
2 is 14, so he can successfully deviate by withdrawing from both of these coalitions, and forming a
single partial coalition of weight 5. This coalition can complete t1 and receive a payoff of 15 > 14.
On the other hand, suppose that the players keep the same coalition structure, but distribute the
gains as y = ((7, 8), (8, 7)). Then player 2 can no longer gain by withdrawing from both of these
coalitions. He is tempted to withdraw his resources from r 1 , as he can use these 4 units of weight
to complete t2 and earn u2 = 10 > 8. However, if he does that, he can no longer get his share of
payoffs from r 2 . Hence, in case of this deviation his total payoff will be 10 < 15. Also, it is easy
to see that player 2 cannot gain by deviating from r 2 only, and player 1 is better off in CS than he
would be on his own. Hence, (CS , y) is in the OCF-core of G.
In some sense, Definition 7 takes a rather pessimistic, or conservative, view on what the members of the deviating group can expect to get from the non-deviators: indeed, in Example 2 as soon
as player 2 withdraws from the partial coalition r 1  CS he expects to be thrown out of r 2 , even
though r 2 is not affected by this deviation. Therefore, in what follows, we will refer to the notion of
profitable deviation introduced in Definition 7 as a c-profitable deviation, and to the corresponding
notion of the core as the conservative core, or the c-core.
This definition is applicable when a deviation by an agent is interpreted by other agents as an
indicator that this agent is not trustworthy, and therefore one should immediately stop all collaboration with him. While this kind of reaction is not unusual, there may be coalitions that are not
affected by the deviation and may not want to punish the deviators. In this case, the deviators need
to decide which of the existing coalitions to abandon and for which existing coalitions to keep their
contribution intact. The members of these partial coalitions will react accordingly, sharing the payoff as before if they have not been affected by the deviation and punishing the deviators otherwise.
Therefore, we refer to the corresponding notion of the core as refined. Before giving the formal
definition, we first introduce a notion of agreement between two coalition structures.
Definition 8. Given a set of agents J  N , we say that two coalition structures CS and CS  over
N agree outside of J with respect to a function f if f is a a bijection between the lists of partial
coalitions {r i  CS | supp(r i ) * J} and {s  CS  | supp(s ) * J} such that f (r i ) = s
implies rji = sj for all j 
/ J. Further, we say that CS and CS  agree outside of J if they agree
outside of J with respect to some function f .
Intuitively, this definition says that if two coalition structures agree outside of J, then the contributions of all players not in J to all partial coalitions must be the same under both outcomes.
If J is the set of deviators, this condition captures the fact that the deviation by the players in J
189

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

does not change the behavior of the non-deviators; the function f is used to establish a correspondence between the partial coalitions involving the non-deviators before and after the deviation. For
illustration, consider the following example.
Example 3. Consider a game with three players N = {1, 2, 3} and a coalition structure CS =
(q 1 , q 2 ), where q 1 = (1, 12 , 21 ), q 2 = (0, 21 , 21 ). Let CS  = (s1 , s2 , s3 ), where s1 = (0, 0, 12 ),
s2 = (0, 21 , 12 ), s3 = (1, 12 , 0). Intuitively, CS  can be obtained from CS when players 1 and 2
deviate by abandoning their joint project with player 3 and forming a coalition of their own. Set
J = {1, 2}. It is not hard to see that CS and CS  agree outside of J with respect to the function
f given by f (q 1 ) = s1 , f (q 2 ) = s2 . On the other hand, CS and CS  also agree outside of J
with respect to the function f  given by f  (q 1 ) = s2 , f  (q 2 ) = s1 ; this function assumes that when
players 1 and 2 decided to deviate, player 1 withdrew his contribution to q 1 and player 2 withdrew
his contribution to q 2 .
Definition 9. Given an OCF-game G = (N, v) and a set of agents J  N , let (CS , x) and (CS  , y)
be two outcomes such that CS and CS  agree outside of J with respect to a function f . Suppose
that for any partial coalition s  CS  with supp(s ) * J and for all j  J we have yj = xij
if r i = f 1 (s ) and yj = 0 otherwise. Then we say that (CS  , y) is an r-profitable deviation of
J from (CS , x) w.r.t. f if for all j  J we have pj (CS  , y) > pj (CS , x). Further, we say that
(CS  , y) is an r-profitable deviation of J from (CS , x) if there exists a function f such that CS and
CS  agree outside of J with respect to f and (CS  , y) is an r-profitable deviation of J from (CS , x)
w.r.t. f . We say that an outcome (CS , x) is in the refined core, or the r-core, of G if no subset of
agents J posesses an r-profitable deviation from it.
In Definition 9, the bijection f matches the partial coalitions in CS and CS  that involve nondeviators; the number of such coalitions is the same in both coalition structures. Moreover, the
contribution of the non-deviators to the partial coalitions matched by f is the same in CS and CS  .
Now, if also the deviators do not change their contribution to some partial coalition r, they can
claim their share of its payoff, as determined by x. On the other hand, if the deviators change their
contribution to r, they are not entitled to any of its payoff. Observe that we allow the deviators
to pick the most favourable bijection f between CS and CS  : for instance, in the context of
Example 3 we would pick f rather than f  , thereby allowing the deviators to claim their payoff from
the coalition (0, 21 , 12 ). In other words, we assume that the deviators will withdraw their contributions
to disturb the non-deviators as little as possible.
Example 4. Consider the game G and the outcome (CS , y) as described in Example 2. While it has
been argued that player 2 cannot c-profitably deviate from (CS , y), he can r-profitably deviate from
it by withdrawing his weight from r 1 and dedicating it to t2 . As he does not change his contribution
to r 2 , he can still claim the payoff he gets from r 2 , so his total payoff is 10 + 7 = 17 > 15.
On the other hand, suppose that players 1 and 2 both split their weights equally between two
partial coalitions, forming the structure CS  = (q 1 , q 2 ), where q 1 = q 2 = ( 21 , 12 ). Clearly, both
q 1 and q 2 have weight 5, so each of them can earn 15 by completing t1 . Now, suppose that the
players distribute the gains using an imputation x = ((3, 12), (12, 3)). Now, both players earn
15, so none of them can benefit from withdrawing from both partial coalitions at the same time,
and therefore the outcome (CS  , x ) is in the c-core. Moreover, if any of the players deviates from
one coalition only, he does not have enough weight to complete any of the tasks, and therefore the
outcome (CS  , x ) is also in the r-core.
190

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

We now provide another example, which suggests that the set of profitable deviations allowed
by Definition 9 may still be too small.
Example 5. Consider again the game G and a coalition structure CS  = (s1 , s2 ), where player 1
contributes all of his weight to s1 , while player 2 contributes 3 units of weight to s1 and 3 units of
weight to s2 , i.e., s1 = (1, 12 ), s2 = (0, 21 ). Observe that we have v(s2 ) = 0, as the total weight
of s2 is 3 only. Now, consider an imputation z = ((3, 12), (0, 0)). Note that player 2 could reduce
his contribution to s1 by 2 units of weight without affecting the value of this coalition, and use this
weight to boost the value of s2 . However, this is not allowed by our definition of an r-profitable
deviation, since as soon as player 2 alters his contribution to s1 , he loses the payoff of 12 that he
gets from s1 . This does not mean, however, that the outcome (CS  , z) is in the r-core of G: players
1 and 2 can collectively deviate to ((1, 61 ), (0, 65 )). If they share the payoff as ((4, 11), (0, 15)), this
will constitute an r-profitable deviation for both of them.
Example 5 demonstrates that Definition 9, while being considerably more lax with respect to
the deviators than Definition 7, can still be too strict: the deviators are punished as soon as they
reduce their contribution to a coalition, irrespective of whether it affects the value of this coalition.
In fact, according to Definition 9, the deviators would still be punished even if they increase their
contribution to a partial coalition with non-deviators (though this type of deviation is, of course,
unlikely). One way to fix this is to allow the deviators to claim their share of payoffs from a
coalition s = f (r i ) as long as v(s ) = v(r i ). However, the non-deviators can be even more
generous to deviators. Indeed, it can be the case that after the deviators reduce their contribution
to a particular partial coalition, this coalition is still able to perform some task, albeit of a smaller
value. If the value of this task is still larger than the total amount of payoff originally received by
the non-deviators from this partial coalition, the deviators could be allowed to claim the leftover
payoff. In other words, this notion of deviation assumes that the non-deviators have no objection to
switching tasks, and only care about the payoff they receive. While this may well be the case, it is
quite optimistic of the deviators to expect this kind of reaction when they contemplate whether to
deviate. Therefore, we refer to this notion of deviation as o-profitable, and call the corresponding
solution concept the optimistic core, or the o-core.
Definition 10. Given an OCF-game G = (N, v) and a set of agents J  N , let (CS , x) and
(CS  , y) be two outcomes such that CS and CS  agree outside of J with respect to aP
function f .
Suppose also that for any partial coalition s  CS  with supp(s ) * J we have jJ yj =
P
max{v(s )  kN \J xik , 0}, where r i = f 1 (s ). We say that (CS  , y) is an o-profitable deviation of J from (CS , x) w.r.t. f if for all j  J we have pj (CS  , y) > pj (CS, x). Further, we say
that (CS  , y) is an o-profitable deviation of J from (CS , x) if there exists a function f such that CS
and CS  agree outside of J with respect to f and (CS  , y) is an o-profitable deviation of J from
(CS , x) w.r.t. f . We say that an outcome (CS , x) is in the optimistic core, or the o-core, of G if no
subset of agents J has an o-profitable deviation from it.
Example 6. Consider again the game G discussed in Examples 2, 4, and 5, and the outcome
(CS  , x ), where CS  = (q 1 , q 2 ), q 1 = q 2 = ( 12 , 12 ), x = ((3, 12), (12, 3)), which was described
in Example 4. Note that if player 2 reduces his contribution to q 1 to 2, this coalition would still
be able to earn 10 by focusing on task t2 . As player 1 only gets 3 units of payoff from q 1 anyway,
under our definition of an o-profitable deviation, player 2 is entitled to the remaining payoff from
this modified partial coalition, i.e., 10  3 = 7. He can then combine the unit of weight saved in
191

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

this manner with the weight he contributes to q 2 , and embark on t2 making a profit of 10. Thus,
by abandoning q 2 altogether and reducing his contribution to q 1 , player 2 can earn 7 + 10 > 15.
Thus, the outcome (CS  , x ) is not in the o-core of G.
In contrast, consider an outcome that combines CS with a more symmetric payoff division
scheme, such as, e.g., y = ((7, 8), (8, 7)). Now, if player 2 reduces his contribution to q 1 by 1, the
resulting partial coalition can earn 10 by focusing on t2 . Of those payoffs, player 1 must receive 7,
leaving 3 for player 2. While player 2 can still use his remaining weight to complete t2 , this will
only give him a total profit of 10 + 3 = 13 < 15, i.e., this deviation is not o-profitable. Similarly, we
can show that withdrawing some of the resources from q 2 and abandoning q 1 is even less profitable
for player 2. Finally, it is easy to see that player 1 does not have an o-profitable deviation either.
Hence, the outcome (CS  , y) is in the o-core of (G).

6. Core Characterization
In the previous section, we introduced three definitions of the core for overlapping coalition formation games. Among the three definitions of the core, the c-core, though in some sense conservative,
is the closest to the traditional definition of the core in general NTU games (Osborne & Rubinstein,
1994). Indeed, unlike the other two definitions, it does not assume any interaction between the deviators and the non-deviators. This motivates us to study this overlapping core variant in more detail,
which we proceed to do in this section and the next. To promote readability, in those two sections
we will be referring to the c-core simply as the core.
We start by providing a characterization of the set of outcomes in the core: essentially, an
outcome is in the core if and only if under this outcome the total payments to each subset of agents
match or exceed the maximum value that can be achieved by this subset. Our proof relies on
some technical restrictions on the function v that defines the game. In particular, we require v to be
continuous, monotone and bounded (observe that if a game is monotone and bounded, then v  (S) <
 for any S  N ), as well as to satisfy another natural restriction defined later. These assumptions
allow us to avoid some pathological situations that may arise in our model at its generality, such as
the supremum v  (N ) being unachievable (e.g., if v is strictly concave in one of its arguments, it can
be the case that no finite coalition structure can achieve v  (N )).
Specifically, we say that a game (N, v) is U -finite if for any (CS , x) such that |CS | > U and
x  I(CS ), there exists a (CS  , y) such that |CS  |  U , y  I(CS  ), and pj (CS , x)  pj (CS  , y)
for all j = 1, . . . , n (i.e., for any outcome (CS , x) with more than U coalitions there exists another
outcome (CS  , y) with at most U coalitions that is weakly prefered to (CS , x) by all agents).
When this condition holds, we can assume that all coalition structures that arise in a game consist
of at most U partial coalitions. This is a natural restriction in many practical scenarios, as it might
be difficult for agents to maintain a very complicated collaboration pattern. It holds when, for
example, there is a bound on the number of partial coalitions each agent can be involved in. In
general U -finiteness imposes some upper bound on the total number of partial coalitions with the
same support that can occur. A natural example is provided by a class of games where for any two
partial coalitions r, r  such that supp(r) = supp(r  ) and rj + rj  1 for any j = 1, . . . , n, we
have v(r + r  )  v(r) + v(r  ). Note that in such games we can assume that no coalition structure
contains two partial coalitions with the same support S, as it is at least as profitable for the players
in S to merge these partial coalitions. (However, notice that this does not imply superadditivity,
192

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

nor does it mean that the grand coalition necessarily emerges, as the criterion above refers only to
coalitions with identical support.) Hence, any such game is 2n -finite.
Remark 1. Note that in all of our results U can be a function of n (as long as U (n) < ).
Alternatively, instead of imposing the condition of U -finiteness on v(), we could restrict the set of
allowed outcomes (or potential deviations) to coalition structures with at most U partial coalitions.
All of our results hold under this model as well.
We now state and prove the first of our main results.
Theorem 1. Given a game (N, v), where v is monotone, continuous, bounded, and U -finite for
some U  N, an outcome (CS , x) is in the c-core of (N, v) if and only if for all S  N
X
pj (CS , x)  v  (S).
(1)
jS

P
Proof. For the if direction, suppose that (CS , x) satisfies jS pj (CS , x)  v  (S) for all S 
N . Assume for the sake of contradiction that (CS , x) is not in the core, i.e., there exists a set S, a
coalition structure CS S  CS S and an imputation
that pj (CS S , y) > pj (CS , x)
P y  I(CS S ) suchP

for all j  S. Then we have v(CS S ) =
jS pj (CS , x)  v (S), a
jS pj (CS S , y) >

contradiction with the way v (S) was defined.
For the only if direction, consider an outcome (CS , x) that does not
P satisfy (1); we will show
that (CS , x) is not in the core. To begin, set p = p(CS , x), and assume jS pj < v  (S) for some
S  N . To show that (CS , x) is not in the core, we will construct a set S  , a coalition structure
CS S   CS S  and P
an imputation y  I(CS S  ) such that pj (CS S  , y)P
> pj for all j  S  . Fix a

set S that satisfies jS pj < v (S). Choose  small enough so that jS pj < v  (S)  , and
let CS S = {CS S  CS S | v(CS S )  v  (S)  }. By definition of v  (S), there is an infinite
sequence of coalition structures CS (t) that satisfies limt v(CS (t) ) = v  (S), so the set CS S is
non-empty. Given a coalition structure CS S  CS S , an imputation y  I(CS SP
) and a respective
payoff vector q = p(CS S , y), define the total loss TL(CS S , q) of (CS S , q) as j:pj >qj (pj  qj ).
Set TLmin = inf{TL(CS S , q) | CS S  CS S , y  I(CS S ), q = p(CS S , y)}. First, we prove that
there exists a coalition structure CS  CS S and an imputation y  I(CS S ) that achieve the total
loss of TLmin .
Lemma 1. Under the theorems conditions, there exists a CS S  CS S , an imputation y  I(CS S )
and a payoff vector q = p(CS S , y) s.t. TL(CS S , q) = TLmin .
(t)

Proof. By definition of TLmin , there exists an infinite sequence of coalition structures CS S , t =
1, . . . , , and respective imputations y (t) , t = 1, . . . , , such that
lim TL(CS (t) , p(CS (t) , y (t) )) = TLmin

t
(t)

and CS S  CS S for all t = 1, . . . , . As the game is U -finite, a coalition structure can be
seen as a list of at most U vectors in [0, 1]n . By adding all-zero partial coalitions if necessary, we
can assume that each coalition structure is a list of exactly U vectors in [0, 1]n , which are ordered
lexicographically. As v is monotone and bounded, there exists a B > 0 such that the value of each
(t)
partial coalition in any of the CS S is between 0 and B. Consequently, each y (t) corresponds to a
193

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

(t)

vector in [0, B]nU . Hence, the sequence (CS S , y (t) ), t = 1, . . . ,  can be viewed as a subset of
[0, B]K (for sufficiently large but finite value of K) and hence has a limit point, which we denote by
(CS  , y  ). It is easy to see that the limit of a sequence of coalition structures is a coalition
P structure,
i
i.e., for any r i  CS  we have r  [0, 1]n , and for any j = 1, . . . , n it holds that U
i=1 rj  1.
Moreover, by continuity of v, the value of each partial coalition in CS  is the limit of the values
(t)
of the respective partial coalitions in CS S , t = 1, . . . , . From this, it is easy to see that y  is in
(t)
I(CS  ). Also, as all CS S are in CS S , so is CS  . Finally, as p(, ) and TL(, ) are continuous
functions of their arguments, we conclude that TL(CS  , p(CS  , y  )) = TLmin .
Continuing with the proof of our Theorem, let (CS S , y) be an outcome that satisfies v(CS S ) 
 , TL(CS S , p(CS S , y)) = TLmin , whose existence is guaranteed by Lemma 1. Set
q = p(CS S , y). Let us now construct a directed graph  whose vertices are the agents and there
is an edge from j to i if there exists a coalition in CS S containing both j and i such that under y,
agent j gets a non-zero payoff from that coalition, i.e., for some r k  CS S we have rjk , rik > 0
and yjk > 0. Observe that if there is an edge (j, i) in , we can change y k by increasing the
payoff to i by a small enough  and decreasing the payoff to j by the same value of  without
violating the constraints, i.e., we have z = (z 1 , . . . , z t )  I(CS S ), where z l = y l for l 6= k and
z k = (y1k , . . . , yjk  , . . . , yik + , . . . , ynk ). Now, color all vertices of  as follows: a vertex j is red
if the agent j is underpaid under P
y, i.e., qj < pj , white if j isPindifferent, i.e., qj = pj , and green if
he is overpaid, i.e., qj > pj . As jS pj < v  (S)   and jS qj = v(CS S )  v  (S)  , the
graph contains at least one green vertex. As argued above, if there is a path from a green vertex j
to a red vertex i, we can transfer a small amount of payoff from j to i and hence decrease the total
loss, which is a contradiction with our choice of (CS S , y). Hence, given an arbitrary green vertex
j, the set of all vertices reachable from j in the graph, which we denote by R(j), can only contain
green or white vertices.
We would now like to argue that the agents in R(j) can successfully deviate from (CS , x).
Indeed, let CS  be the coalition structure that consists of the coalitions that the agents in R(j) form
among themselves in CS S . Clearly, the value of CS  is equal to the total value of the coalitions
formed by these agents in CS S . Note also that under (CS S , y), the agents in R(j) do not get any
payoffs from coalitions that involve agents not in R(j). Indeed, suppose that an i  R(j) gets a
non-zero payoff from a coalition that involves an agent k 6 R(j). Then in  there is an edge from
i to k, a contradiction with how R(j) was constructed. In other words, in CS S , the payoffs that the
agents in R(j) get come only from the coalitions that they form among themselves, and yet these
agents are all green or white, i.e., each of them is doing no worse than what he was doing under CS ,
and some of them (in particular, agent j) are doing strictly better. To finish the proof, let the agents
in R(j) distribute the payoffs in the same way as in (CS S , y), except that player j transfers a small
fraction of his payoffs to each of the white players in R(j) (this is possible by construction). The
last step ensures that each agent in R(j) is strictly better off than in (CS , x). This demonstrates that
(CS , x) is not in the core, as required.
v  (S)

Remark 2. Note that we did not have to make use of the additional restrictions we imposed on v
to prove the if direction of the theorem (these are used in the proof of Lemma 1). Hence, this
implication holds for an arbitrary G.
It is easily verifiable that Theorem 1 holds in the non-overlapping case with coalition structures
as well. The result is trivial to prove in that setting, as each agents payoffs come from just one
194

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

coalition; in contrast, we had to use more involved combinatorial arguments for transferring payoffs
among agents. We also get the following interesting result as a corollary:
Corollary 1. By setting S = N in the statement of Theorem 1, we conclude that any outcome in the
c-core maximizes the social welfare.
We now turn our attention to characterizing the set of coalition structures CS that admit payoff
allocations x such that the corresponding tuple (CS , x) belongs to the core. That is, while in
Theorem 1 we saw a necessary and sufficient condition for a tuple (CS , x) to belong to the core,
suppose that we are now only given a structure CS = (r 1 , . . . , r k ) and we want to check whether
there exists some payoff allocation x such that (CS , x) belongs to the core. Our characterization
can be seen as a generalization of the notion of balancedness in the context of overlapping coalition
formation. In the classic setting, the analogous question is when does the grand coalition admit
a payoff allocation in the core, answered by Bondareva (1963) and Shapley (1967). Before we
proceed to our result, we define balancedness with respect to a coalition structure.
Definition 11. Fix a coalition structure CS = (r 1 , . . . , r k ), k  N, and let K = {1, ..., k}. A
collection of numbers {S }SN , {P
i }iK is called balanced w.r.t. the given coalition structure CS
if and only if S  0 for all S, and S:jS S + i = 1 for all i  K, j  supp(r i ).
Definition 12. A game is called balanced w.r.t. a coalition structure CS = (r 1 , ...,P
r k ) if and only
if for every collection {S }SN , {i }iK that is balanced w.r.t. CS it holds that S S v  (S) +
Pk
i

i=1 i v(r )  v (N ).
The proof of the following theorem is based on LP-duality, and relies on the characterization
result of Theorem 1; furthermore, the proof illustrates that the condition of balancedness introduced
above arises rather naturally.
Theorem 2. Let (N, v) be an OCF-game, where v is monotone, continuous, bounded, and U -finite
for some U  N and consider a coalition structure CS = (r 1 , ..., r k ), for some k  N. There exists
an imputation x s.t. (CS , x) belongs to the c-core if and only if the game is balanced w.r.t. CS .
Proof. Suppose there exists a payoff allocation x such that (CS , x) belongs to the core, and let
K = {1, . . . , k}. Then the following linear program (denoted as LP) has an optimal solution:
P
x
min
PiK,jN
P ij

S  N
s.t.
r i ) xij  v (S)
PjS i:jsupp(
i
x
=
v(r
)
i  K
j ij
The first constraint expresses the condition of Theorem 1, and the second the fact that the payoff
of each partial coalition needs to be distributed exactly. Note that we have no variables xij if
j 6 supp(r i )recall Definition 4. These are precisely the conditions that need to be satisfied for
(CS , x) to be in the core and clearly the optimal value of the LP is v  (N ) (using the first constraint
and Corollary 1). By the LP-duality theorem, this means that the dual program also has an optimal
solution of value v  (N ). The dual is given by:
P
P
max PS S v  (S) + ki=1 i v(r i )
i  K, j  supp(r i )
s.t.
S:jS S + i = 1
S  0
S  N
195

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

Hence for any feasible solution of the dual, the value of the objective function is P
at most v  (N ),
which implies that for any balanced collection {S }SN , {i }iK , it holds that S S v  (S) +
Pk
i

i=1 i v(r )  v (N ).
For the other direction, suppose that for any balanced collection, the above holds. This means
that for any feasible solution, the value of the dual is at most v  (N ). Therefore the dual is both
bounded and feasible (setting i = 1 and the rest to 0 is feasible), which implies that it has an
optimal solution. But then the primal program also has an optimal solution x and this means by
Theorem 1 that (CS , x) belongs to the core.
Remark 3. In the traditional superadditive setting, the condition of balancedness is somewhat
simpler and more intuitive. In our setting, the characterization leads to a slightly more complicated
expression, essentially due to the fact that the linear program that describes core allocations for
each coalition structure requires a larger set of constraints.

7. Convex OCF-Games Have a Non-Empty Core
In this section, we first generalize the notion of convexity to OCF-games and then proceed to show
that it provides a sufficient condition for non-emptiness of the c-core.
Recall that for classical TU-games convexity means that for R  N and S  T  N \ R it
holds that v(S  R)  v(S)  v(T  R)  v(T ). Thus, convexity in the classic TU-games setting
means that it is more useful for a coalition R to join a larger coalition than a smaller one. We now
apply this intuition to our setting (recall that F(S) denotes the set of all feasible agreements for S):
Definition 13. An OCF-game G = (N, v) is convex if for each R  N and S  T  N \ R
the following condition holds: for any (CS S , xS )  F(S), any (CS T , xT )  F(T ), and any
(CS SR , xSR )  F(S  R) that satisfies pj (CS SR , xSR )  pj (CS S , xS ) j  S, there exists
an outcome (CS T R , xT R )  F(T  R) s.t.
pj (CS T R , xT R )  pj (CS T , xT )
pj (CS

T R

,x

T R

)  pj (CS

SR

j  T , and
SR

,x

) j  R.

This definition is similar in flavour to that provided by Suijs and Borm (1999), where a generalization of convexity is defined in the context of stochastic cooperative games. The intuition behind
this definition is as follows: Consider two fixed agreements, one on S and one on T respectively.
Any time that there is a feasible agreement on S  R that the members of S do not object to compared to their own agreement (i.e., all members of S are weakly better off than in their previous
agreement), then there is a feasible agreement on T  R such that (i) the members of T do not object to this agreement, compared to the previous agreement on T and (ii) the members of R weakly
prefer this agreement to the agreement on S  R.
We note that a different notion of convexity has been defined for fuzzy games by Branzei,
Dimitrov, & Tijs (2003). That definition deals with the marginal contribution of a partial coalition
when joining another existing partial coalition, where the result of the join is a new partial coalition.
We, on the other hand, quantify the marginal contribution of adding a set of players R, to a set of
players T , w.r.t. the best overlapping coalition structure that the set R  T can form. Secondly, the
definition of Branzei et al., as well as the classic definition of convexity, simply enforce a property
on the function v(), concerning the marginal contribution v(R  T )  v(T ). In our case, our games
196

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

are not fully transferable and hence we cannot simply talk about the difference in values. Instead,
our definition has to enforce the existence of a coalition structure on R  T such that individually
every player is at least as well-off as in the coalition structure over R  S, where S  T .
We now show that convexity is a sufficient condition for the non-emptiness of the core, in
analogy to the classic result on convex TU-games (Shapley, 1971).
Theorem 3. If an OCF-game G = (N, v) is convex and v is continuous, bounded, monotone and
U -finite for some U  N, then the c-core of this game is not empty.
Proof. Let G = (N, v) be a convex OCF-game. For any S  N , let GS be the restriction of
G on S. To prove the theorem, we explicitly construct an outcome (CS , x), x  I(CS ), and
show that it belongs to the core of G: Fix an arbitrary ordering of the players 1, 2, . . . , n  1, n.
The construction takes place in rounds. First, let p1 = v  ({1}), p2 = v  ({2}); by assumptions
of the theorem and using arguments similar to those in the proof of Lemma 1, there exist coalition
structures in CS {1} , CS {2} that achieve these payoffs. Let CS 1 be the structure that achieves this for
player 1 in G{1} , and let x1 be the corresponding imputation. We know that there exists at least one
coalition structure CS 2  CS {1,2} and a corresponding imputation x2 such that p1 (CS 2 , x2 )  p1 ,
p2 (CS 2 , x2 )  p2 (e.g., take the union of payoff-maximizing structures in G{1} and G{2} , and
combine the corresponding imputations). If there exist more than one such feasible agreement, we
pick the one most preferred by player 2. More formally, we choose a feasible agreement (CS 2 , x2 )
that maximizes the payoff p2 (CS 2 , x2 ) (which will be at least p2 ) over all feasible agreements on
{1, 2} subject to p1 (CS 2 , x2 )  p1 (CS 1 , x1 ) (by our assumptions on v(), this maximum exists).
Now, let p3 be the maximum payoff that agent 3 can get in G{3} . Again, there exists at least one
coalition structure CS 3 in CS {1,2,3} and a corresponding imputation x3 such that agents 1, 2 are
(weakly) better off than in (CS 2 , x2 ), and 3 is also weakly better off than being on its own. If there
exist more than one such feasible agreement, we pick one that maximizes 3s payoff, i.e., we pick
an agreement (CS 3 , x3 ) so that p3 (CS 3 , x3 ) is maximized over all agreements on {1, 2, 3} subject
to the constraints p1 (CS 3 , x3 )  p1 (CS 2 , x2 ), p2 (CS 3 , x3 )  p2 (CS 2 , x2 ).
Continuing in the same manner, at every round k we pick an outcome (CS k , xk ) that maximizes
pk (CS k , xk ) subject to constraints pi (CS k , xk )  pi (CS k1 , xk1 ) for i  {1, ..., k  1}; the
assumptions on v() ensure that all these maxima exist. In the end, we obtain a feasible agreement
(CS n , xn ) on N in which all the agents are weakly better off than on their own, as well as weakly
better off compared to the agreements of the previous rounds.
We now show that (CS n , xn ) belongs to the core of G. For this it suffices to prove the following
stronger claim.
Claim 1. For k = 1, . . . , n, the feasible agreement (CS k , xk ) belongs to the core of the game
G{1,...,k} .
Proof. We prove this by induction. For k = 1, it is obvious that (CS 1 , x1 ) belongs to the core of
G{1} .
Now, suppose that for some m, 2  m  n, we have (CS k , xk )  core(G{1,...,k} ) for all
k < m. We will prove that (CS m , xm ) is in the core of G{1,...,m} .
Suppose, for the sake of contradiction, that this is not the case. Then there is a subset S 
{1, ..., m} and (CS  , x )  F(S) such that
pi (CS  , x ) > pi (CS m , xm ) i  S.
197

(2)

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

We consider three different cases for the members of S:
Case 1: m 6 S. In this case we know by construction that for all i  {1, . . . , m  1} we have
pi (CS m , xm )  pi (CS m1 , xm1 ), which implies that pi (CS  , x ) > pi (CS m1 , xm1 ) for all
i  S. Hence, the tuple (CS  , x ) is a deviation that makes the members of S strictly better off than
in the agreement (CS m1 , xm1 ). But this is a contradiction since by induction (CS m1 , xm1 ) 
core(G{1,...,m1} ).
Case 2: S = {1, . . . , m}. Now we will get a contradiction with how we constructed (CS m , xm ).
Indeed, we chose (CS m , xm ) to maximize pm (CS m , xm ) subject to the constraints pi (CS m , xm )
 pi (CS m1 , xm1 ) for all i = 1, . . . , m  1. However, by (2), the outcome (CS  , x ) also
satisfies these constraints and provides a higher payoff to m than (CS m , xm ) does, a contradiction.
Case 3: S = S   {m}, where S  is a strict subset of {1, . . . , m  1}. In this case we will utilize
convexity. Let CS  be the coalition structure that consists of the singleton coalitions for all agents of
S  , and let x be the corresponding imputation. By construction, (CS  , x ) is a feasible agreement
on S   {m} such that pi (CS  , x )  pi (CS  , x ) for all i  S  . Let T = {1, . . . , m  1}.
Since (CS m1 , xm1 )  F(T ), by applying Def. 13 for S   T and with R = {m}, we get
that there exists a feasible agreement (CS , x) on T  {m} = {1, . . . , m} such that pi (CS , x) 
pi (CS m1 , xm1 ) for i = 1, . . . , m  1, and pm (CS , x)  pm (CS  , x ). But then by (2) above
we get that pm (CS , x) > pm (CS m , xm ), a contradiction with how we chose (CS m , xm ).
Applying Claim 1 with k = n, we get that the core of G is non-empty.
In the traditional setting, if a game is represented using oracle access for v(S), there is a trivial
algorithm for computing an element of the core in convex games. Indeed, one can set the payoff
vector to be the vector of the marginal contributions of the agents for an arbitrary permutation of
the set of agents. In our setting, our proof does yield a procedure for constructing an element of the
core, though not a polynomial-time one. Our procedure requires solving a series of optimization
questions, which for arbitrary convex games are NP-hard. In the future, we would like to find
classes of convex games where our proof yields a polynomial-time algorithm. In particular, looking
at our proof, this would be true for games in which we can solve in polynomial time the following
problem: Given a set of agents S  N , a feasible agreement on S, an outcome (CS , x), and an
agent k 6 S, find a feasible agreement (CS  , y) on S  {k} that maximizes pk (CS  , y) subject to
the constraints pj (CS  , y)  pj (CS , x).

8. Properties of the Three Cores
Following the detailed study of the c-core stability concept in the previous two sections, in this
section we further explore the properties of our three notions of the OCF-core. In particular, we
investigate the relationships among these notions, and study the effects of allowing overlapping
coalition formation on the stability of the underlying game. We also compare our OCF model and
notions of the core to the fuzzy games setting and the notion of the fuzzy core (Aubin, 1981).
We start by exploring the connection between stability and social welfare maximization in
TTGs. As demonstrated earlier in the paper, in OCF-games these two properties are closely related. Indeed, Theorem 1 and Corollary 1 show that any outcome in the c-core of an OCF-game
maximizes the social welfare as long as the characteristic function of the game satisfies a number of
technical conditions; by Theorem 5 below the same holds for the r-core and the o-core. However,
as one of these conditions is continuity, this result does not directly apply to TTGs. While the proof
198

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

of Theorem 1 can be adapted to work for the TTG setting, there also exists a direct proof for the
following theorem.
Theorem 4. For any TTG G = (N ; w; t) and any outcome (CS , x)  c-core(G), we have
v(CS )  v(CS  ) for any coalition structure CS   CS N .
Proof. Fix an outcome (CS , x)  c-core(G), and let p be the payoff vector that corresponds to
(CS , x). Suppose that there exists a coalition structure CS   CS N such that v(CS  ) > v(CS ).
Let CS  = (r 1 , . . . , r k ). For j = 1, . . . , k, let z j be the total weight of the partial coalition r j , i.e.,
set z j = r1j w1 +    + rnj wn .
Now, consider a coalition structure CS  = (q 1 , . . . , q k ) given by qij = z j /w(N ) for all i  N ,
P
all j = 1, . . . , k; note that we have kj=1 qij  1. The total weight of a partial coalition q j can be
P
computed as iN qij wi = z j . Therefore, q j  CS  can accomplish the same task as r j  CS  ,
and hence v(CS  ) = v(CS  ) > v(CS ). Now, observe that since in CS  all players contribute to
all partial coalitions, there are no restrictions on how the value of CS  can be distributed among

)
the players. In particular, we can set  = v(CS )v(CS
, and construct an imputation y  I(CS  )
n
Pk
P
v(r j )
j
j
j
by setting yij = v(CS
 ) (pi + ). Indeed, we have
iN yi = v(r ),
j=1 yi = pi + . Now,
it is clear that the entire set of agents N can deviate from (CS , x) to (CS  , y); as they all deviate
simultaneously, this is a c-profitable deviation, a contradiction with (CS , x) being in the c-core of
G.
The discussion in Section 5.2 suggests a natural relationship between the three notions of a
successful deviation, and, consequently, between the three cores. (In what follows, we refer to the
outcomes in the c-core, r-core and o-core as c-stable, r-stable and o-stable, respectively.)
Theorem 5. For any OCF-game G, we have o-core(G)  r-core(G)  c-core(G). Moreover, these
containments can be strict, i.e., there exists an OCF-game G such that o-core(G)  r-core(G) 
c-core(G).
Proof. Observe that any c-profitable deviation can be viewed as an r-profitable deviation in which all
players abandon all coalitions they contributed to. Similarly, any r-profitable deviation corresponds
to an o-profitable deviation where whenever a deviator changes his contribution to coalition, he
withdraws all of his resources from it; note that, as illustrated by Example 5, the deviators payoff
in this o-profitable deviation can be strictly higher than in the original r-profitable deviation. It
follows that any outcome that is r-stable is also c-stable, and any outcome that is o-stable is also
r-stable, thus proving the first part of the theorem.
To prove the second part of the theorem, consider the game G described in Examples 2, 4, 5
and 6. We have demonstrated that the outcome (CS , x) is in c-core(G) \ r-core(G) and that the
outcome (CS  , x ) is in r-core(G) \ o-core(G).
Theorem 5 shows that our three notions of stability can be substantially different with respect to
individual outcomes. However, it does not exclude the possibility that they are equivalent when seen
as notions of stability of the entire game, i.e., that for any OCF-game G we have c-core(G) 6=  iff
r-core(G) 6=  iff o-core(G) 6= . We will now show that this is not the case. The games used in
the proofs of the following two propositions are not threshold task games. However, they, too, can
be described in terms of agents weights and tasks.
199

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

Proposition 1. There exists an OCF-game G such that c-core(G) 6=  while r-core(G) = .
Proof. Consider an OCF-game G = (N, v) with seven agents N = {1, . . . , 7} whose weights are
given by w = (1, 1, 1, 1, 3, 3, 3), and two task types t1 and t2 with values 100 and 2, respectively.
The first task can be completed in any of the following four ways:
 1 unit of player 1s weight and 2 units of player 5s weight;
 1 unit of player 2s weight and 2 units of player 6s weight;
 1 unit of player 3s weight and 2 units of player 7s weight;
 1 unit of player 4s weight and 2 units of weight from either of the players 5, 6, or 7.
That is, v(r) = 100 if wi ri  1 and wj rj  2, where
(i, j)  {(1, 5), (2, 6), (3, 7), (4, 5), (4, 6), (4, 7)}.
The second task t2 requires 2 units of weight in total from players 5, 6 and 7.
Consider a coalition structure CS = (r 1 , r 2 , r 3 , r 4 ), given by
2
r 1 = (1, 0, 0, 0, , 0, 0),
3
2
3
r = (0, 0, 1, 0, 0, 0, ),
3

2
r 2 = (0, 1, 0, 0, 0, , 0),
3
1 1
4
r = (0, 0, 0, 0, , , 0).
3 3

That is, partial coalitions r 1 , r 2 and r 3 successfully complete t1 , while r 4 successfully completes t2 . Consider also an imputation x  I(CS ) given by
x1 = (0, 0, 0, 0, 100, 0, 0),

x2 = (0, 0, 0, 0, 0, 100, 0),

x3 = (0, 0, 0, 0, 0, 0, 100),

x4 = (0, 0, 0, 0, 1, 1, 0).

Let p be the payoff vector that corresponds to x: we have p1 = p2 = p3 = p4 = 0, p5 =
p6 = 101, p7 = 100. It is not hard to see that (CS , x)  c-core(G). Indeed, suppose for the sake
of contradiction that there is a set of players J that can c-profitably deviate from (CS , x). Since
(CS , x) maximizes the social welfare, the deviation cannot be simultaneously profitable for all
players in N , so |J| < 7. Moreover, J cannot contain 2 or more players from the set S = {5, 6, 7}:
indeed, if one of these players deviates, he loses 100 units of payoff, which can only be replaced
if he forms a coalition with 4. However, since 4 cannot form two distinct coalitions of value 100
each, this is not possible. Therefore, J cannot contain any of the players in the set S: each of these
players already gets the maximum payoff from t1 , and, since the other two players from S are not
in J, the set of deviators does not have enough resources for t2 . Finally, there is no c-profitable
deviation for players in N \ S, as no task can be completed by agents in N \ S only.
We will now show that the r-core of G is empty. Suppose otherwise, and let (CS  , y) be an
outcome in the r-core of G. Let p be the payoff vector that corresponds to y. It is not hard to
show that any outcome in the r-core of G maximizes the social welfare; the proof is similar to that
of Theorem 4. Hence, we can assume without loss of generality that CS = (q 1 , q 2 , q 3 , q 4 ) with
v(q 1 ) = v(q 2 ) = v(q 3 ) = 100 and v(q 4 ) = 2, and, moreover, q51  23 , q62  23 , q73  32 . It follows
200

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

that either (a) q11 = q22 = q33 = 1 or (b) q4j = 1 for some j  {1, 2, 3} and qii = 1 for i  {1, 2, 3},
i 6= j. We say that a player i is useful for a coalition r if v(r  ) < v(r), where r  is given by ri = 0,
rj = rj for all j 6= i. Observe that in an r-stable outcome no player can get any payoff from a
partial coalition for which he is not useful: otherwise the other members of that coalition, who can
complete the corresponding task on their own, can r-profitably deviate. We will now show that we
have p1 = . . . = p4 = 0 both in case (a) and in case (b). Observe that by the argument above player
1 can get payoff from q 1 only, player 2 can get payoff from q 2 only, player 3 can get payoff from
q 3 only, and player 4 can get payoff from exactly one of the coalitions q 1 , q 2 , and q 3 .
In case (a), we clearly have p4 = 0, as player 4 is not useful for any coalition in CS  . Now, if,
e.g., y11 > 0, then y51 < 100, and players 4 and 5 can r-profitably deviate by forming a coalition that
performs t1 . Hence y11 = y22 = y33 = 0, and therefore p1 = p2 = p3 = 0. In case (b), assume
without loss of generality that q41 = 1. Then p1 = 0, as player 1 is not useful for any coalition
in CS  , so y41 = 0, since otherwise players 1 and 5 can r-profitably deviate, and, consequently,
p4 = 0. This implies that also y22 = y33 = 0: if, e.g., y22 > 0, then y62 < 100, and players 4 and
6 can r-profitably deviate by forming a coalition that performs t1 . Hence, in both cases we have
p1 =    = p4 = 0.
Now, as v(q 4 ) = 2, we have y54 + y64 + y74 = 2, so at least one of the payoffs y54 , y64 and y74 is
strictly positive. Assume without loss of generality that y54 =  > 0. Then players 6, 7 and their
partners in q 2 and q 3 (i.e., players i , i such that qi2 = 1, qi3 = 1) can r-profitably deviate from
(CS  , y) by forming a coalition structure CS  = (s1 , s2 , s3 ), where s1 is given by
s1i = 1,

2
s16 = ,
3

s1 = 0 for  6= i , 6,

s2i = 1,

2
s27 = ,
3

s2 = 0 for  6= i , 7,

s2 is given by

and s3 = (0, 0, 0, 0, 0, 13 , 13 ). We will now construct an imputation z for CS  by setting zi1 =
zi2 = 4 , z61 = z72 = 100  4 , z63 = y64 + 2 , z73 = y74 + 2 , and zij = 0 for all (i, j) 6=
(i , 1), (6, 1), (i , 2), (7, 2), (6, 3), (7, 3). It is not hard to see that z  I(CS  ), and, moreover,
the deviation (CS  , z) is r-profitable for 6, 7, i and i . Hence, (CS  , y) is not in the r-core of
G.
Proposition 2. There exists an OCF-game G such that r-core(G) 6=  while o-core(G) = .
Proof. Consider an OCF-game G = (N, v) with 3 agents N = {1, 2, 3} whose weights are given
by w = (8, 8, 8), and 2 task types t1 and t2 . The first task needs 6 units of weight from each player,
and has value 300, i.e. v(r1 , r2 , r3 ) = 300 if wi ri  6 for i = 1, 2, 3. The second task needs 4 units
of weight in total from any of the players
 7 7and6 has2value
 12. 1 2 
1
2
1
Let CS = (r , r ), where r = 8 , 8 , 8 , r = 8 , 8 , 8 . Clearly, v(r 1 ) = 300, v(r 2 ) = 2.
Consider also an imputation x  I(CS ) given by x1 = (100, 100, 100), x2 = (0.5, 0.5, 1). It is
not hard to see that (CS , x)  r-core(G). Indeed, as CS maximizes the social welfare, there is no
deviation that will be simultaneously profitable for all agents. Furthermore, if any agent withdraws
his contribution from r 1 , he will lose the associated payoff of 100 and no deviation can compensate
for this loss. Moreover, it is clear that withdrawing contribution from r 2 cannot be profitable either,
as there is no way to earn more than 2 = v(r 2 ) with this amount of weight.
201

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

We will now show that G has an empty o-core. Suppose for the sake of contradiction that there
exists an outcome (CS  , y)  o-core(G). It is not hard to show that any outcome in the o-core of
G maximizes the social welfare; the proof is similar to that of Theorem 4. Hence, we can assume
that CS  = (q 1 , q 2 ), where v(q 1 ) = 300, v(q 2 ) = 2, and, moreover, qi1  68 for i = 1, 2, 3. We
have y12 + y22 + y32 = 2, so we can assume without loss of generality that y12 =  > 0. This means
that players 2 and 3 can o-profitably deviate from (CS  , y) as follows: players 2 and 3 withdraw
q21 w2  6 and q31 w3  6 units of weight from q 1 , respectively (as argued above, we have q21 w2  6,
q31 w3  6), as well as their entire contribution to q 2 , and use these resources to complete t2 . If they
divide the resulting payoff by allocating y22 + 2 to player 2 and y32 + 2 to player 3, this constitutes
an o-profitable deviation for them. Thus, (CS  , y) is not in the o-core of G.
Thus, so far in this section we investigated the relationships among our notions of the overlapping core; it is also insightful to compare them to the non-overlapping and the fuzzy one. We now
proceed to do so.
8.1 Comparison with the Non-Overlapping Core
Given an OCF-game G = (N, v), we can define a non-overlapping game Gno = (N, v no ) by
setting v no (C) = v(r C ), where the partial coalition r C is given by riC = 1 if i  C and riC = 0
otherwise for all C  N . Observe that for a threshold task game G applying this transformation to
its overlapping version G gives us exactly its non-overlapping version G. We can now compare the
core of the game Gno and the overlapping cores of the original game G. In particular, it is natural
to ask whether the core of Gno can be empty when the o-core of G (and hence by Theorem 5 also
the r-core and the c-core of G) is not, and vice versa, i.e., whether the c-core (the largest of the
overlapping cores) of G can be empty when the core of Gno is not. Interestingly, it turns out that the
answer to both of these questions is positive. We demonstrate this via examples based on threshold
task games; as argued above, for any such game G we have Gno = G.
Proposition 3. There exists a TTG G with core(G) = , but o-core(G) 6= .
Proof. Consider a threshold task game G = (N ; w; t), where N = {1, 2, 3}, w = (2, 2, 2),
t = t1 = (3, 1). In G, any coalition structure CS contains at most one coalition C with v(C) = 1.
Let p = (p1 , p2 , p3 ) be an imputation for CS . As v(CS ) = 1, there exists some i  N with pi > 0.
Then the coalition C  = N \ {i} can successfully deviate from (CS , p), as we have w(C  ) = 4,
p(C  ) = 1  pi < 1. Hence, any outcome of G is not stable.
In G, the players can form two successful partial coalitions. Now, consider an outcome (CS , x),
where CS = (r 1 , r 2 ) with r 1 = (1, 12 , 0), r 2 = (0, 21 , 1), and x1 = ( 23 , 31 , 0), x2 = (0, 31 , 23 ). We
claim that (CS , x) is in the o-core of G. Indeed, suppose for the sake of contradiction that there is
a group of players J that has an o-profitable deviation from (CS , x). We have |J|  {1, 2, 3}. It is
easy to see that |J| =
6 1: no player has enough weight to complete t1 on his own. Also, |J| =
6 2: any
4
4
pair of players earns 3 in (CS , x), and on their own they can make at most 1 < 3 . Finally, |J| =
6 3,
as (CS , x) maximizes the social welfare. The contradiction completes the proof.
Intuitively, Proposition 3 holds because G has more feasible outcomes than G, and some of
these additional outcomes turn out to be stable. On the flip side, G allows for a wider range of
deviations, so an outcome that is stable with respect to G may be unstable with respect to G. Our
next proposition illustrates this.
202

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Proposition 4. There exists a TTG G with c-core(G) = , but core(G) 6= .
Proof. Consider a threshold task game G = (N ; w; t), where N = {1, 2, 3}, w = (9, 1, 1),
t = (t1 , t2 ) with t1 = (8, 100), t2 = (2, 1).
In G, player 1 can work on task t1 , while players 2 and 3 can cooperate on task t2 , sharing the
profits equally. Clearly, the resulting outcome is stable.
On the other hand, G has no c-stable outcomes. Indeed, suppose that there is an outcome
(CS , x) in the c-core of G, and let p be the corresponding payoff vector. By Theorem 4, CS
consists of two partial coalitions: r 1 , which completes t1 , and r 2 , which completes t2 . Hence,
v(CS ) = 101. If p1 > 100, then p2 + p3 < 1, and hence players 2 and 3 can deviate by forming a
coalition r = (0, 1, 1) that can complete t2 and has value 1. If p1 < 100, player 1 can deviate by
forming a coalition r = (1, 0, 0) that can complete t1 and has value 100. Hence, we have p1 = 100,
p2 + p3 = 1, and therefore we can assume without loss of generality that p2  21 . Now, players
1 and 2 can deviate by forming a coalition structure CS  = ( 98 , 0, 0), ( 19 , 1, 0) and distributing the
payoffs as ((100, 0, 0), ( 13 , 23 , 0)). We conclude that (CS , x) is not c-stable, a contradiction.
8.2 Comparison with Fuzzy Games
As mentioned earlier in this paper, Aubin (1981) introduces the notion of a fuzzy game, in which a
player can participate in a coalition at various levels, and the value of a coalition S depends on the
participation levels of its members. Thus, at a first glance, the definition of a fuzzy game is identical
to the definition of an OCF-game, as both are given by characteristic functions defined on [0, 1]n .
However, there are several crucial differences between fuzzy and OCF-games.
First, fuzzy games and OCF-games differ in their definition of an outcome. Indeed, while in
OCF-games an outcome is an (overlapping) coalition structure together with a list of payoff vectors,
in fuzzy games the only allowable outcome is the formation of the grand coalition. Furthermore, an
outcome of an OCF-core needs to be stable against any deviation of a set S to a (possibly overlapping) coalition structure. In the Aubin core, outcomes need only be stable against a deviation to a
partial (fuzzy) coalition, but not necessarily against deviations to a coalition structure. Indeed, the
formation of coalition structures (overlapping or not) is not addressed in the fuzzy games literature.
One could try to represent games with overlapping coalition structures using the fuzzy games
formalism. Indeed, given an OCF-game, we can construct a fuzzy game whose characteristic
function simulates the behaviour of the characteristic function of the original OCF-game on coalition structures. Specifically, given any OCF-game G = (N, v), we define a related fuzzy game
G = (N, v  ) as follows. For any r  [0, 1]n , we define
1

k

CS r = {(q , . . . , q ) | k 

1, qij

 0 for i = 1, . . . , n, j = 1, . . . , k,

k
X

qij = ri },

j=1

and set v  (r) = supCS CS r v(CS ). That is, for each partial coalition r, v  identifies the best
coalition structure CS that can be obtained by splitting r into subcoalitions, and returns its value
v(CS ). The resulting fuzzy game G is very similar to the original OCF-game G. For example, for
TTGs, this transformation would enable the members of the grand coalition to work on several tasks
simultaneously. More generally, given a TTG G, any outcome of (G) (i.e., a payoff vector for the
grand coalition) corresponds to a social-welfare maximizing outcome (CS , x) of G and vice versa.
203

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

In fact, this relationship holds between any OCF-game G and the corresponding fuzzy game G as
long as the set {v(CS ) | CS  CS (1,...,1) } is compact (and thus contains its least upper bound).
However, this approach fails to capture several delicate aspects of overlapping coalition formation. The main reason for this is that in the fuzzy game formulation, the actual set of tasks executed
by a partial coalition is implicit in the definition of the characteristic function. Indeed, an outcome
of the fuzzy game is simply a payoff vector, and while we are guaranteed that there is a set of tasks
that provides the corresponding total payoff, this set of tasks cannot be read off the description of
the outcome. This leads to a number of difficulties.
First, the fuzzy games formalism would not allow us to reason about partial coalition structures
with suboptimal social welfare. While by Theorem 4 such coalition structures are unlikely to be the
final outcomes of a game, a dynamic coalition formation protocol may produce such partial coalition
structures as intermediate steps. Thus, using the language of fuzzy coalitions impairs our ability to
study the processes that lead to the formation of partial coalition structures. As such processes are
of great interest from the practical perspective, this is an important disadvantage of the fuzzy model.
Further, under the OCF representation, there is a one-to-one correspondence between partial
coalitions and tasks. This makes the OCF approach intuitively appealing, and suggests that it provides the right level of granularity for reasoning about partial coalition formation. Indeed, consider
our problem from a computational perspective in the context of TTGs. While under the OCF representation finding a socially optimal coalition structure can be difficult (see Appendix A), computing
the value of a given partial coalition r is straightforward: we simply pick the most valuable task
that can be completed using the resources posessed by r. In contrast, in the fuzzy game framework,
the two issues are intertwined, so even computing a partial coalitions worth is a hard problem.
Even more importantly, the definition of the fuzzy core given by Aubin (1981) is not appropriate
for many natural scenarios, and, in particular, TTGs. Specifically, the fuzzy core of a fuzzy game
G = (N, v) is defined as the setP
of all outcomes (N, p) such that p(N ) = v(1, . . . , 1) and for any
partial coalition r it holds that ni=1 pi ri  v(r). Essentially, this means that when a group of
players deviates from the grand coalition via a partial coalition r, each deviating player i receives
both her payoff from r, and her original payoff from the grand coalition, scaled down by a factor of
(1  ri ). Thus, the fuzzy core is even more optimistic from the deviators perspective than the ocore. Indeed, the deviators do not worry what the grand coalition will be able to do once they leave.
They simply assume that if they withdraw, say, 40% of their resources, they will get 60% of what
they used to get. However, in many gamesand, in particular, TTGsif some players abandon the
grand coalition, the latter may not have sufficient resources to complete any task. Clearly, in this
case the deviators could not possibly get any payoff from what remains of the grand coalition. Thus,
the fuzzy core may be empty, even if in practice the game is stable. The example in the proof of
Proposition 5 illustrates this.

Proposition 5. There exists a TTG G such that o-core(G) 6= , but the fuzzy core of the corresponding fuzzy game (G) is empty.

Proof. Consider a TTG G given by N = {1, 2}, w = (10, 10), and t = ((20, 20), (7, 9)), and the
induced OCF-game G. The corresponding fuzzy game (G) = (N, v  ) is given by
204

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS



20



18
v  (r) =

9



0

if r1 + r2 = 2
if 1.4  r1 + r2 < 2
if 0.7  r1 + r2 < 1.4
if r1 + r2 < 0.7

It is not hard to see that the outcome (CS , x) of G, where CS = r = (1, 1) and x = (10, 10) is
o-stable. Moreover, intuitively, it is clear that no rational agent or a coalition of agents would want
to deviate from this outcome. On the other hand, under the definition of the fuzzy core the outcome
(10, 10) of (G) is not stable: indeed, for q = (.7, .7) we have p1 q1 + p2 q2 = 14 < 18 = v  (q).
We will now prove that no outcome of (G) is in the fuzzy core. Observe that since v  (1, 1) =
20, any outcome of (G) is of the form (z1 , z2 ), where z1 + z2 = 20. Clearly, any outcome with
z1 < 9 or z2 < 9 is unstable, as the partial coalition (1, 0) (respectively, (0, 1)) can profitably
deviate from it. Thus we can assume that z1  9, z2  9, or, equivalently, z2  11, z1  11. Thus,
for the partial coalition q considered above, we have z1 q1 + z2 q2  11  1.4 = 15.4 < 18 = v(q),
which means that (z1 , z2 ) is not in the fuzzy core.
Remark 4. To remedy some of the difficulties illustrated above, we can devise a notion of stability
that is defined within the framework of fuzzy games, yet is essentially equivalent to the
P c-core. Let

n

us say that an outcome p of G is f-stable if for any r  [0, 1] we have v (r)  isupp(r ) pi ,
and define the f-core of G to be the set of all f-stable outcomes of G . Note that this definition is
different from the standard definition of the fuzzy core. For TTGs, one can show that an outcome
p of G is in the f-core of G if and only if the corresponding outcome (CS , x) of G is in the ccore of G. The proof makes use of the fact that in TTGs one can distribute the profit v  (r) of a
deviating partial coalition r among the members of supp(r) arbitrarily. (In more detail, one can
construct a partial coalition structure CS involving agents in supp(r) that performs tasks of total
value v  (r) so that each agent in supp(r) participates in each partial coalition in CS .) Moreover,
this equivalence is true for general OCF games whose characteristic functions satisfy some natural
regularity conditions; the proof is similar to the proof of Theorem 1. Unfortunately, while the f-core
provides an analogue of the c-core in the fuzzy game setup, it is not clear how to devise an analogue
of the r-core or the o-core for this setting. Indeed, to define these concepts, we would have to reason
about partial coalitions that are hurt by a deviation. However, the description of an outcome of
a fuzzy game does not indicate which partial coalitions a given player belongs to, so we cannot
determine which tasks will be affected by a deviation.
We conclude that there are natural settings where OCF-games provide a more realistic and
nuanced model than fuzzy games; threshold task games appear to be one such example.

9. Computational Aspects of Stability in Threshold Task Games
In this section, we investigate the computational complexity of core-related questions in TTGs. Our
goal here is twofold. First, TTGs provide a natural model of agent collaboration, and therefore it
is important to understand how to allocate resources in such games in a stable manner. Second,
our analysis highlights important differences between the three definitions of the core for games
with overlapping coalitions. In particular, the results presented in this section provide a complexitytheoretic separation between the c-core, on one hand, and the r-core and the o-core, on the other
205

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

hand. We believe that results of this type are useful for building a better understanding of stability
in the context of general OCF games.
Unless explicitly stated otherwise, we make the usual assumption that all parameters of the
gamei.e., all weights, thresholds and task utilitiesare integers given in binary. This assumption
can be made without loss of generality, and is necessary for a formal complexity-theoretic analysis.
9.1 Games with Non-Overlapping Coalitions
We start by analyzing the complexity of TTGs in the non-overlapping setting. As mentioned in
Section 5.1, such games can be seen as a generalization of weighted voting games with coalition
structures. Elkind, Chalkiadakis & Jennings (2008) show that several stability-related questions in
such games are computationally hard when weights are integers given in binary. Hence, we can
formulate the following proposition, whose proof follows immediately from those results.
Proposition 6. Given a TTG G = (N ; w; t), it is coNP-hard to decide whether the corresponding
game G has an empty core. Also, given an outcome (CS , p) of G, it is coNP-complete to decide
whether (CS , p) is in the core of G. These results hold even if there is only one task type, and the
utility of this task is 1.
On the other hand, Elkind et al. (2008) provide a polynomial-time algorithm for checking if an
outcome of a weighted voting game is in the core if weights are given in unary. That algorithm is
based on dynamic programming: given a weighted voting game G described by a set of players N ,
a list of weights w and a threshold T , for each weight 1, . . . , w(N ) it identifies the minimum payoff
Pw to a coalition that has weight w, and then checks if Pw < 1 for some w  T .
It is not hard to see that a similar approach works for threshold task games as well. The only
complication is that for each weight w, in addition to computing the minimum payoff to a coalition
of this weight under the given imputation, we have to compute the maximum utility available to a
coalition of this weight, i.e., max{uj | w  T j }, and compare the two quantities. However, these
additional steps are very easy (in particular, they can be performed efficiently even if task utilities
are large). This gives us the following result.
Proposition 7. There exists an algorithm that, given a TTG G = (N ; w; t) and an outcome (CS , p)
of G, checks whether (CS , p) is in the core of G and runs in time poly(w(N ), |p|), where |p| is the
number of bits in the binary representation of p.
For weighted voting games with unary weights, Elkind et al. (2008) also show that, by constructing a linear program that uses the algorithm of Proposition 7 as an oracle, we can check in
polynomial time whether a given coalition structure CS can be stabilized, i.e., whether there exists
a payoff vector p  I(CS ) such that (CS , p) is in the core. This algorithm can be easily adapted to
work for TTGs with unary weights. Hence, the question of whether a given coalition structure can
be stabilized is poly-time solvable for these games, too.
9.2 Games with Overlapping Coalitions
We will now show that, similarly to the non-overlapping case, if all weights, thresholds and utilities
in a TTG are integers given in binary, then it is computationally hard to check if a given outcome
of the corresponding OCF game is stable. Moreover, this hardness result holds for all three definitions of stabilty, i.e., the c-core, the r-core, and the o-core. While these results are perhaps not
206

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

surprising given the similar result for the non-overlapping setting (i.e., Proposition 6 above), the
reason behind the computational hardness is quite different. Indeed, the reduction used in the proof
of Proposition 6 is based on PARTITION, a classic NP-hard problem which asks whether, given a
set of weights, we can split it into two sets of the same weight. Essentially, the proof proceeds by
constructing an outcome that is stable if and only if a certain subset of agents cannot be split into
two groups that have the same weight. This proof technique is unlikely to work in the overlapping
scenario, as one can always form two partial coalitions of the same weight by allowing all agents to
split their weight equally between two coalitions. Hence, the proof of the following theorem uses a
somewhat different approach.
Theorem 6. Given a TTG G = (N ; w; t) and an outcome (CS , x) of the corresponding OCF game
G, it is coNP-complete to decide whether (CS , x) is in the c-core of G.
Proof. Our reduction is based on U NBOUNDED K NAPSACK, a well-known NP-hard problem. An
instance of U NBOUNDED K NAPSACK (Martello & Toth, 1990) is given by a set of  items, where
each item i has a size si and a value zi , the knapsack size B and the target value Z. It is a yesinstance if we can fill the knapsack using an unlimited number of copies of each item so that the
total size of the resulting set of items is at most B, while their
PZ, i.e., if there
P total value is at least
is a vector of non-negative integers (1 , . . . ,  ) such that i=1 i si  B and i=1 i zi  Z.
Otherwise, it is a no-instance.
Consider an instance I = ((s1 , . . . , s ); (z1 , . . . , z ); B; Z) of U NBOUNDED K NAPSACK. We
can assume without loss of generality that sj < B, zj < Z for all j = 1, . . . , . Moreover, we can
assume that I is monotone, i.e., si  sj implies zi  zj . Indeed, if we have a pair of items such that
si  sj , but zi > zj , we can simply delete the jth item, as it is not used by any optimal solution.
We will now construct an instance of our problem as follows. Set N = {1} and let w1 = B.
Set t = (t1 , t2 , . . . , t+1 ), where T j = sj , uj = zj for j = 1, . . . ,  and T +1 = B, u+1 = Z  1.
Due to our restrictions on I, the game G = (N ; w; t) is a threshold task game.
Consider an outcome (CS , p) where CS consists of a single partial coalition r with r1 = 1
and p  I(CS ). As B > sj for all j = 1, . . . , , this coalition executes the task t+1 and receives
utility of Z  1. Hence, player 1 can c-profitably deviate from (CS , p) if and only if he can find a
collection of tasks whose total resource requirement is at most his weight B and whose total utility
is at least Z, i.e., if and only if we started with a yes-instance of U NBOUNDED K NAPSACK.
In the proof of Theorem 6 the outcome (CS , x) consists of a single partial coalition. Thus, any
r-profitable deviation from (CS , x) is c-profitable. This implies the following corollary.
Corollary 2. Given a TTG G and an outcome (CS , x) of the corresponding OCF game G, it is
coNP-complete to decide if (CS , x) is in the r-core of G.
For the o-core, the situation is somewhat more complicated. However, a more careful examination
of the proof of Theorem 6 allows us to obtain the following corollary.
Corollary 3. Given a TTG G = (N ; w; t) and an outcome (CS , x) of the corresponding OCF
game G, it is coNP-complete to decide if (CS , x) is in the o-core of G.
Proof. In the proof of Theorem 6, we construct an OCF game with 1 player and an outcome (r, x).
Consider any o-profitable deviation (CS, y) from (r, x). This deviation itself is not necessarily a
c-profitable deviation from (r, x): under (CS , y), agent 1 may withdraw some, but not all of his
207

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

resources from (r, x) and therefore continue to derive some benefit from it. However, for a single
agent, allocating some of the resources to the original partial coalition r is equivalent to forming
a new partial coalition using that amount of resources, i.e., given (CS, y), one can construct a
deviation from (r, x) that will be c-profitable for agent 1. On the other hand, any c-profitable
deviation from (r, x) is also o-profitable. Hence, (r, x) is o-stable if and only if it is c-stable, i.e.,
if and only if we started with a no-instance of U NBOUNDED K NAPSACK.
In the rest of the section, we will focus on the case where all parameters of the game (i.e.,
all players weights, all thresholds and all task utilities) are integers that are given in unary, or,
equivalently, are at most polynomial in the number of
Given a game G = (N ; w; t), where
Pplayers.
j + uj ).
tj = (T j , uj ) for j = 1, . . . , m, let |G| = w(N ) + m
(T
j=1
It turns out that in this setting checking whether an outcome is in the c-core becomes easy.
Intuitively, the reason for this is that once a group of players decides to deviate, the agents in this
group can easily decide how to proceed: they need to pool their weights and find the most profitable
set of tasks that can be completed using this amount of resources.
Theorem 7. There exists an algorithm that, given a TTG G = (N ; w; t) and an outcome (CS , x)
of the corresponding OCF game G, checks whether (CS , x) is in the c-core of G and runs in time
poly(|G|, |x|), where |x| is the size of the binary representation of the imputation x.
Proof. Our algorithm is based on dynamic programming. First, for any w = 1, . . . , w(N ), let Uw
be the maximum profit that a coalition of weight w can make, i.e.,


m
m
X

X
Uw = max
 j uj |
j T j  w, (1 , . . . , m )  Nm .


j=1

j=1

For each w = 1, . . . , w(N ), the quantity Uw can be computed using the dynamic programming
algorithm for U NBOUNDED K NAPSACK. The running time of this procedure is polynomial in |G|.
Now, let p be the payoff vector that corresponds to the imputation x. For all i = 1, . . . , n and
all w = 1, . . . , w(N ), set Pi,w = min{p(S) | S  {1, . . . , i}, w(S) = w}. The quantities Pi,w
can be easily computed using dynamic programming. Indeed, we have P1,w = p1 if w = w1 and
P1,w = + otherwise (we use the convention that min  = +). Furthermore, we can compute
Pi+1,w given the values (Pi,w )w =1,...,w by setting Pi+1,w = min{Pi,w , pi + Pi,wwi }. The running
time of this procedure is poly(|G|, |p|).
Suppose that we have computed Pn,w for w = 1, . . . , w(N ). Observe that the value Pn,w is the
least amount received by a coalition of weight w under p. Now, for each w = 1, . . . , w(N ), we can
compare the quantities Pn,w and Uw . If there is a value of w for which the latter exceeds the former,
there is a coalition in N that could increase its collective earnings by deviating from (CS , x), i.e.,
(CS , x) is not in the c-core of G. It is not hard to see that the converse is also true: if Pn,w  Uw
for all w = 1, . . . , w(N ), then no coalition has a c-profitable deviation from (CS , x), and hence
(CS , x) is in the c-core of G.
Clearly, this algorithm runs in time poly(|G|, |x|).
In contrast, the corresponding problems for the r-core and the o-core are computationally hard.
Intuitively, the reason for this is that the decisions the players make are no longer binary: instead
of simply deciding whether or not to deviate, they have to decide which of their coalitions with
208

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

non-deviators to abandon. In the case of the o-core, there is also the possibility of reducing ones
contribution to a partial coalition rather than abandoning it altogether.
Theorem 8. Given a TTG G = (N ; w; t) and an outcome (CS , x) of the corresponding OCF game
G, it is strongly coNP-complete to decide whether (CS , x) is in the r-core of G.
Proof. It is not hard to see that this problem is in coNP: to show that an outcome (CS , x) is not in
the r-core of G, we can guess a set of deviators J and a deviation (CS  , y), and check that (CS  , y)
is r-profitable for J by computing the payoffs of all players in J under x and y.
To show coNP-hardness, we reduce from M AXIMUM E DGE B ICLIQUE (Peeters, 2003). An
instance on M AXIMUM E DGE B ICLIQUE is given by a bipartite graph B = (L, R, E) with a set
of vertices L  R and a set of edges E  L  R, and a parameter K. It is a yes-instance if B
contains a biclique of size at least K, i.e., if there are sets L  L, R  R such that |L |  |R |  K,
and for all   L and all   R we have (, )  E. Otherwise, it is a no-instance.
Suppose that we are given an instance (B, K) of M AXIMUM E DGE B ICLIQUE with B =
(L, R, E), L = {1 , . . . , |L| }, R = {1 , . . . , |R| }. Then we create an instance of our problem as
follows. Assume without loss of generality that |L|  |R|, We set n = |R|+1, k = |L|, M = k 2 n2 ,
V = k 2 nM , and create n players with weights w1 =    = wn1 = k, wn = k(kn  n + 1) and
2 task types t1 = (kn; V ) and t2 = (K; (n  1)k + 1). Also, we create a coalition structure
CS = (r 1 , . . . , r k ) given by rij = 1/k for all i = 1, . . . , n and all j = 1, . . . , k. Observe that the
total weight of each rj  CS is kn, so each such partial coalition performs t1 . Finally, to construct
the imputation x = (x1 , . . . , xk ), for all j = 1, . . . , k and all i = 1, . . . , n  1, we set xji = 1 if
Pn1 j
xi for all j = 1, . . . , k.
(i, j)  E and xji = M otherwise. Also, we set xjn = V  i=1
Suppose we started with a yes-instance of M AXIMUM E DGE B ICLIQUE, and let (L , R ) be
the corresponding subgraph of B. Then the subset of players J = {i | i  R } can r-profitably
deviate from (CS , x) by abandoning the partial coalitions in the set S = {r j | j  L }, and using
the freed-up resources to embark on t2 . Indeed, under x the players in J collectively earn at most
(n  1)k from partial coalitions in S, and devote at least K units of weight to these coalitions.
Conversely, consider any r-profitable deviation (CS  , y), and let J be the corresponding set of
deviators. Suppose that k1 coalitions in CS  work on t1 , and k2 coalitions work on t2 . First, suppose
n  J. Observe that (CS  , y) is profitable for player n if and only if k1 = k, k2 = 0: indeed, under
(CS , x) player n earns at least k(V  (n  1)M ), whereas under any outcome that completes less
2
that k copies of t1 he earns at most (k  1)V + kKn ((n  1)k + 1) < k(V  (n  1)M ). However,
any deviation that results in executing k copies of t1 must involve all resources of all players, i.e.,
J = {1, . . . , n}, and any such deviation cannot be simultaneously profitable for all members of the
deviating set. Hence, we have n 6 J, and therefore w(J)  k(n  1). Consequently, k1 = 0

and the deviators total profit is at most w(J)
K ((n  1)k + 1) < M . This means that (CS , y) is an
r-profitable deviation only if no player i  J abandons a coalition r j  CS such that xji = M .
On the other hand, to successfully execute even one copy of t2 , the members of J must collectively
withdraw at least K units of weight. Let R = {i | i  J}, and let L correspond to the set of
partial coalitions in CS affected by the deviation; then (L , R ) is a biclique of size at least K.
It is not hard to check that in the proof of Theorem 8 no player can withdraw part of his resources from a partial coalition in CS and still claim any profit from that coalition. This implies
that checking whether a given outcome is in the o-core is computationally hard, too.
209

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

Corollary 4. Given a TTG G and an outcome (CS , x) of the corresponding OCF game G, it is
strongly coNP-complete to decide whether (CS , x) is in the o-core of G.
On the other hand, combining the techniques of Theorem 7 and Theorem 4 leads to a pseudopolynomial algorithm for checking whether the c-core of a TTG is non-empty.
Theorem 9. Given a TTG G = (N ; w; t), one can check in time poly(|G|) whether the corresponding OCF game G has a non-empty c-core.
Proof. We will show that if the c-core of a game G is non-empty, then for any social welfaremaximizing set of tasks we can construct a coalition structure CS that executes this set of tasks and
an imputation x  I(CS ) such that (CS , x) is in the c-core of G; moreover, in CS each agent
contributes to each coalition. Hence, our algorithm first selects a social welfare-maximizing set of
tasks, then constructs a coalition structure that can perform this set of tasks, and finally solves a
linear program to check if this coalition structure can be stabilized. The details follow.
Assume for simplicity that t contains a task type t with T = 1; if this is not the case we
can add a task type t0 = (1, 0) to t. This allows us to assume that in any coalition structure all
agents resources are committed to some tasks. Fix a social welfare-maximizing multi-set of tasks
{1 t1 , . . . , m tm }. Suppose
let (CS  , y) be an outcome in the c-core of G.
Pmc-core(j G) 6= , and

By Theorem 4, we have j=1 j u = v(CS ). Consider a coalition structure CS that contains
1 +    + m coalitions: the first 1 coalitions have weight T 1 each, the next 2 coalitions have
weight T 2 each, etc., and each agent i distributes his resources evenly between all coalitions, i.e.,
T1
he contributes wi w(N
) units of weight to each of the first 1 coalitions, etc. As in CS all agents
contribute to all partial coalitions, and v(CS ) = v(CS  ), we have y  I(CS ). Moreover, it is
clear that the outcome (CS , y) is in c-core(G): any c-profitable deviation from (CS , y) is also a
c-profitable deviation from (CS  , y).
By Proposition 9 when all weights are given in unary, we can find a social welfare-maximizing
coalition structure CS = (r 1 , . . . , r k ) in polynomial time. Consider the following linear program:
pi  0 for i = 1, . . . , n
X

pi = v(CS )

X

pi  Uw(J) for all J  N,

iN

iJ

where Uw is defined as in the proof of Theorem 7. While this linear program has exponentially
many constraints, it can be solved in linear time by the ellipsoid method (Schrijver, 1986), since it
has a polynomial-time separation oracle. Indeed, we can decide whether a given candidate solution
is feasible using the algorithm described in the proof of Theorem 7.
Clearly, if this linear program has a feasible solution p, then the imputation x given by xji =
j
p v(r ) for all i  N and all j = 1, . . . , |CS | satisfies x  I(CS ), and, moreover, (CS , x) 
i v(CS )

c-core(G). Conversely, if it does not have a feasible solution, then CS cannot be stabilized, and
hence by the argument above the c-core of G is empty.
210

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

10. Conclusions, Extensions, and Future Work
In this paper we introduced a model of cooperative games that allows for overlapping coalitions and
takes into account the need for resource allocation. In doing so, we generalized the usual models
where either the grand coalition is the only desirable outcome or the outcomes are required to be
partitions of the set of agents. Given our model, we defined and studied in depth a notion of the
core (the c-core) which is a generalization of the core in the traditional models of cooperative game
theory. Under some quite general conditions, we provided a characterization for an outcomethat
is, a (coalition structure, imputation) pairto belong to the core. We also showed that any outcome
in the core maximizes the social welfare. Further, we introduced a notion of balancedness for OCFgames, and showed that a coalition structure CS admits an imputation x so that (CS , x) is in the
core if and only if the game is balanced. Moreover, we extended the notion of convexity to our
setting and showed that convex games have a non-empty core.
In addition, we considered two other notions of core-stability in OCF-games, which differ from
each other (as well as from the first one) in what the deviators expect to obtain from their collaboration with non-deviators. Together, our three notions of the core span a wide range of beliefs that the
deviators may hold regarding payoffs from coalitions with non-deviators, and can be substantially
different from each other with respect to the sets of outcomes that they characterize, and with respect
to their computational complexity. We also compared the OCF-games with their non-overlapping
analogues, and showed that from the social welfare maximization perspective, OCF-games may
provide higher total utility, and are easier to work with than their classic counterparts. We have
also argued that OCF-games provide a more appropriate modelling framework than fuzzy games
for many scenarios; in particular, this is certainly the case for threshold task games. To summarize,
our paper is one of the very first attempts to provide a theoretical treatment of overlapping coalition
formation, and to study stability in this setting in a thorough manner.
10.1 Extensions
In many environments, when a coalition is formed, it may have a choice of actions to execute.
While in a deterministic setting such as the one considered in this paper, the coalition will simply
choose the action that results in the highest possible payoff, in a probabilistic environment this
choice is more difficult: a coalition may want to strike a balance between the expected payoff and
the variance. To address this issue, we can incorporate coalitional actions in our model as follows.
A coalition is allowed to select an action from a (usually finite) action space A. Without loss of
generality, we assume that each coalition can undertake any action in A.3 The value of a coalition is
then determined by the resource contribution levels of its members and the action selected. Therefore, the characteristic function in our setting is then defined on (r, a) pairs, where r = (r1 , . . . , rn )
is a vector of resources, and a  A is an action. All of our definitions and results generalize readily
to the situation where each coalition has a choice of actions (simply put, our presentation so far
corresponds to a situation where each coalition had exactly one action available to it).
Another extension we have examined has to do with modelling the available resources. For
ease of presentation it was assumed throughout the paper that there exists only one type of (continuous) resource. Nevertheless, all of our results still hold if we assume multiple types of resources
(e.g., agents have to distribute both time and money among their coalitions). Moreover, we have
3. The situation where this is not the case can be modeled by setting the value of the respective (coalition, action) pair
to 0.

211

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

also studied a discrete OCF setting, with agent contribution levels taking values in a finite set
(i.e., an agent may be able to contribute 20%, but not 21% of his resources to a given coalition).
Such a setting is obviously of interest in many applications involving countable resources (as the
discretization of effectively any kind of resources is common in practice). With discrete resources,
the number of possible coalition structures is now finite (as a coalition in our setting is a collection
of resourcessee Section 4). All of our definitions and theorems carry through in this setting with
minor differences in the arguments used in the proofs.
10.2 Future Work
There exist many exciting open questions for future work. First of all, an important research direction is to develop a better understanding of scenarios where overlapping coalitions can naturally arise, and to identify the appropriate stability concepts for these scenarios. We believe that
techniques developed in this paper will prove useful for this purpose. Moreover, one of our first
priorities is to investigate further the alternative notions of stability (i.e., the o-core and the r-core)
proposed above, and obtain relevant characterization results, as we did with the c-core. Extending
other solution concepts for coalitional gamessuch as, e.g., the Shapley valueto OCF settings is
an important research direction as well.
We also plan to study further the computational complexity of core-related questions in this
setting. First, while we have initiated the study of complexity-theoretic aspects of stability in OCF
games, in this paper we have focused on the complexity of checking whether a given outcome is
stable. Another natural problem in this domain is studying the complexity of checking whether a
game has a stable solutioni.e., whether its c-core (r-core, o-core) is non-empty. Theorem 9 makes
the first steps in this direction, suggesting that this problem may be easier in the overlapping setting
than in the classic setting: indeed, Elkind et al. (2008) conjecture that for WVGs with coalition
structures checking the non-emptiness of the core is hard for unary weights.
Now, the hardness results for computing an allocation in the core or checking if the core is nonempty in the traditional settingas those in the work of Chvatal (1978), Tamir (1991), Deng and
Papadimitriou (1994), Sandholm et al. (1999), Conitzer and Sandholm (2006)and our hardness
results in this paper suggest that one can only hope to identify special classes of games where we can
have efficient algorithms for computing core allocations. As noted earlier, an element of the core in
convex games can be computed in the traditional setting simply by taking the vector of the marginal
contributions of the agents for an arbitrary permutation of the set of agents. In our setting, even
though our proof yields a procedure for constructing an element of the c-core, it requires solving a
series of optimization questions, which for arbitrary convex games are NP-hard. Naturally, it would
be desirable to find classes of convex games where our proof yields a polynomial time algorithm.
We are also interested in finding processes that lead to the core in not necessarily convex games;
though randomized algorithms such as the ones of Dieckmann and Schwalbe (1998) and Chalkiadakis and Boutilier (2004) trivially extend to the overlapping setting, they would be of little practical value here due to the huge space of potential overlapping configurations. Therefore, we are
interested in finding ways to exploit known game structure to prune the search space for potential
stable configurations. Another subject of future research is extending our model to allow for infinite
coalition structures. Furthermore, it would be interesting to establish links between outcomes in the
core and outcomes of bargaining equilibria in overlapping coalitional bargaining games.
212

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Finally, the incorporation of actions in our model allows for the investigation of action stochasticity and, more generally, uncertainty in an OCF setting. For instance, a coalitional action can
be associated with a distribution over possible payoff outcomes resulting from its execution. This
poses challenges to study such models from both a theoretical and a practical standpoint, since the
introduction of uncertainty leads to several intricacies not readily resolved by the use of deterministic concepts and models, as the work of Suijs and Borm (1999), Suijs, Borm, Wagenaere, and Tijs
(1999), Blankenburg, Klusch, and Shehory (2003), Chalkiadakis and Boutilier (2004) and Chalkiadakis, Markakis, and Boutilier (2007) demonstrates. On a related note, enriching our model description so as to capture type uncertainty (Chalkiadakis & Boutilier, 2004; Chalkiadakis et al.,
2007) would allow for the ready translation of uncertainty regarding the types (capabilities) of players to coalitional value uncertainty, while still capturing the potential stochasticity of coalitional
action outcomes at the same time.

11. Acknowledgments
We would like to thank the anonymous reviewers for their constructive comments. This research was
supported by the ALADDIN (Autonomous Learning Agents for Decentralised Data and Information
Networks) project, which is jointly funded by a BAE Systems and EPSRC strategic partnership
(EP/C548051/1); as well as by EPSRC (GR/T10664/01), ESRC (ES/F035845/1), and Singapore
NRF Research Fellowship 2009-08.

Appendix A. Algorithmic Aspects of Social Welfare Maximization in TTGs
In this appendix, we study the complexity of finding a social welfare-maximizing outcome in TTGs,
both in the overlapping and in the non-overlapping scenario. Unless explicitly mentioned otherwise,
we make the standard assumption that all parameters in the description of a TTG (i.e., all agents
weights, all thresholds and all task utilities), are integers given in binary.
It is not hard to see that finding a non-overlapping coalition structure that maximizes the social
welfare is an NP-hard problem.
Proposition 8. Given a TTG G = (N ; w; t) and a parameter K, it is NP-complete to decide if G
has an outcome (CS , p) with v(CS )  K. This holds even if there is just one task type, i.e., t = t1 ,
and all weights, thresholds and utilities are given in unary.
Proof. It is easy to see that the problem is in NP. To show NP-hardness, we give a reduction from 3PARTITION (Garey & Johnson, 1990) to our problem. An instance of 3-PARTITION is given
P3 by a list
of non-negative integers A = (a1 , . . . , a3 ) and an integer parameter B that satisfies i=1 = B
and B/4 < ai < B/2 for all i = 1, . . . , 3. It is a yes-instance if the elements of A can
be partitioned into  sets S1 , . . . , S such that a(S1 ) =    = a(S ) = B and a no-instance
otherwise.
Given an instance of 3-PARTITION, consider a TTG G with N = {1, . . . , 3}, wi = ai for
i = 1, . . . , 3 and a single task type t = (T, u) with T = B and u = 1. Clearly, deciding whether
the maximum social welfare achievable in G is at least  is equivalent to checking whether the given
instance of 3-PARTITION is a yes-instance. Moreover, since 3-PARTITION is known to remain
NP-hard when the input is given in unary, the same is true for our problem.
213

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

In contrast, finding a social welfare-maximizing coalition structure in the OCF game that corresponds to a TTG is a somewhat easier problem. Indeed, we can simply add together all agents
weights, and then find an optimal set of tasks to execute given this amount of resource. The latter
problem is equivalent to U NBOUNDED K NAPSACK, which is known to be NP-hard when the inputs
are given in binary, but is polynomial-time solvable if all elements of the input are given in unary or
if there are at most 2 items; for details, see (Martello & Toth, 1990), Section 3.6. Consequently, a
similar conclusion holds for our problem.
Proposition 9. Given a TTG G = (N ; w; t) and a parameter K, it is NP-complete to decide if
G has an outcome (CS , x) with v(CS)  K. However, this problem becomes polynomial-time
solvable if all weights, thresholds and utilities are given in unary or if there are at most 2 task types.

References
Albizuri, M., Aurrecoechea, J., & Zarzuelo, J. (2006). Configuration values: Extensions of the
coalitional Owen value. Games and Economic Behavior, 57, 117.
Apt, K., & Radzik, T. (2006). Stable partitions in coalitional games.. Working Paper, available at
http://arxiv.org/abs/cs.GT/0605132.
Apt, K., & Witzel, A. (2009). A generic approach to coalition formation. International Game
Theory Review (IGTR), 11, 347367.
Aubin, J.-P. (1981). Cooperative fuzzy games. Mathematics of Operations Research, 6(1), 113.
Aumann, R., & Dreze, J. (1974). Cooperative games with coalition structures. International Journal
of Game Theory, 3(4), 217237.
Bachrach, Y., & Rosenschein, J. (2007). Computing the Banzhaf power index in network flow
games. In Proc. of the 6th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS-07), pp. 335341.
Bachrach, Y., & Rosenschein, J. (2008). Coalitional skill games. In Proc. of the 7th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS-08), pp. 10231030.
Blankenburg, B., Klusch, M., & Shehory, O. (2003). Fuzzy kernel-stable coalitions between rational
agents. In Proc. of the 2nd International Conference on Autonomous Agents and Multiagent
Systems (AAMAS-03), pp. 916.
Bondareva, O. N. (1963). Some applications of linear programming methods to the theory of cooperative games (in russian). Problemy Kibernetiki, 10, 119139.
Branzei, R., Dimitrov, D., & Tijs, S. (2005). Models in cooperative game theory. Springer.
Branzei, R., Dimitrov, D., & Tijs, S. (2003). Convex fuzzy games and participation monotonic
allocation schemes. Fuzzy Sets and Systems, 139(2), 267281.
Chalkiadakis, G., & Boutilier, C. (2004). Bayesian reinforcement learning for coalition formation
under uncertainty. In Proc. of the 3rd International Conference on Autonomous Agents and
Multiagent Systems (AAMAS-04), pp. 10901097.
Chalkiadakis, G., Elkind, E., Markakis, E., & Jennings, N. R. (2008). Overlapping coalition formation. In Proc. of the 4th International Workshop on Internet and Network Economics
(WINE-08), pp. 307  321.
214

fiC OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Chalkiadakis, G., Markakis, E., & Boutilier, C. (2007). Coalition formation under uncertainty:
Bargaining equilibria and the Bayesian core stability concept. In Proc. of the 6th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS-07), pp. 400407.
Chvatal, V. (1978). Rational behavior and computational complexity.. Technical Report SOCS-78.9,
School of Computer Science, McGill University, Montreal.
Conconi, P., & Perroni, C. (2001). Issue linkage and issue tie-in in multilateral negotiations.. CESifo
Working Paper No. 601.
Conitzer, V., & Sandholm, T. (2006). Complexity of constructing solutions in the core based on
synergies among coalitions. Artificial Intelligence, 170(6-7), 607619.
Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalition formation
for efficient data fusion in multi-sensor networks. In Proc. of the 21st National Conference
on AI (AAAI-06), pp. 635640.
Deng, X., & Papadimitriou, C. (1994). On the complexity of cooperative solution concepts. Mathematics of Operation Research, 19, 257266.
Dieckmann, T., & Schwalbe, U. (1998). Dynamic coalition formation and the core.. Economics
Department Working Paper Series, Department of Economics, National University of Ireland
- Maynooth.
Elkind, E., Chalkiadakis, G., & Jennings, N. R. (2008). Coalition structures in weighted voting
games. In Proc. of the 18th European Conference on Artificial Intelligence (ECAI-2008), pp.
393  397.
Elkind, E., Goldberg, L. A., Goldberg, P. W., & Wooldridge, M. (2009). Computational complexity
of weighted voting games. Annals of Mathematics and Artificial Intelligence, 2(56), 109131.
Garey, M. R., & Johnson, D. S. (1990). Computers and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co.
Gillies, D. (1953). Some Theorems on n-Person Games. Ph.D. thesis, Department of Mathematics,
Princeton University, Princeton.
Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009). On the complexity of compact coalitional games. In Proc. of the 21st International Joint Conference on Artificial Intelligence
(IJCAI-09), pp. 147152.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: A compact representation scheme for
coalitional games. In Proc. of the Sixth ACM Conference on Electronic Commerce (ACM
EC05), pp. 193202.
Manisterski, E., Sarne, D., & Kraus, S. (2008). Cooperative search with concurrent interactions.
Journal of Artificial Intelligence Research (JAIR), 32(136).
Martello, S., & Toth, P. (1990). Knapsack Problems: Algorithms and Computer Implementations.
John Wiley & Sons.
Myerson, R. (1991). Game Theory: Analysis of Conflict. Harvard University Press.
Osborne, M., & Rubinstein, A. (1994). A course in game theory. MIT Press.
Owen, G. (1977). Values of games with a priori unions. In Henn, R., & Moeschlin, O. (Eds.),
Mathematical Economics and Game Theory, pp. 7687. Springer-Verlag.
215

fiC HALKIADAKIS , E LKIND , M ARKAKIS , P OLUKAROV & J ENNINGS

Patel, J., Teacy, W., Jennings, N. R., Luck, M., Chalmers, S., Oren, N., Norman, T., Preece, A., Gray,
P., Shercliff, G., Stockreisser, P., Shao, J., Gray, W., Fiddian, N., & Thompson, S. (2005).
Agent-based virtual organisations for the Grid. Multiagent and Grid Systems, 1(4), 237249.
Peeters, R. (2003). The maximum edge biclique problem is NP-complete. Discrete Applied Mathematics, 131(3), 651654.
Rahwan, T., Ramchurn, S., Jennings, N. R., & Giovannucci, A. (2009). An anytime algorithm
for optimal coalition structure generation. Journal of Artificial Intelligence Research (JAIR),
34(521567).
Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition structure
generation with worst case guarantees. Artificial Intelligence, 111(12), 209238.
Sandholm, T., & Lesser, V. (1997). Coalitions among computationally bounded agents. Artificial
Intelligence, 94(1), 99137.
Schrijver, A. (1986). Theory of Linear and Integer Programming. John Wiley & Sons.
Shapley, L. S. (1967). On balanced sets and cores. Naval Research Logistics Quarterly, 14, 453
460.
Shapley, L. (1953). A value for n-person games. In Kuhn, H., & Tucker, A. (Eds.), Contributions
to the Theory of Games II, pp. 307317. Princeton University Press, Princeton.
Shapley, L. (1971). Cores of convex games. International Journal of Game Theory, 1, 1126.
Shehory, O., & Kraus, S. (1996). Formation of overlapping coalitions for precedence-ordered taskexecution among autonomous agents. In Proc. of the 2nd International Conference on MultiAgent Systems (ICMAS-96), pp. 330337.
Shehory, O., & Kraus, S. (1998). Methods for task allocation via agent coalition formation. Artificial
Intelligence, 101(12), 165200.
Suijs, J., & Borm, P. (1999). Stochastic cooperative games: superadditivity, convexity and certainty
equivalents. Journal of Games and Economic Behavior, 27, 331345.
Suijs, J., Borm, P., Wagenaere, A. D., & Tijs, S. (1999). Cooperative games with stochastic payoffs.
European Journal of Operational Research, 113, 193205.
Tamir, A. (1991). On the core of network synthesis games. Mathematical Programming, 50, 123
135.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior. Princeton
University Press, Princeton.

216

fiJournal of Artificial Intelligence Research 39 (2010) 51-126

Submitted 4/10; published 9/10

Implicit Abstraction Heuristics
Michael Katz
Carmel Domshlak

dugi@tx.technion.ac.il
dcarmel@ie.technion.ac.il

Faculty of Industrial Engineering & Management,
Technion, Israel

Abstract
State-space search with explicit abstraction heuristics is at the state of the art of costoptimal planning. These heuristics are inherently limited, nonetheless, because the size of
the abstract space must be bounded by some, even if a very large, constant. Targeting
this shortcoming, we introduce the notion of (additive) implicit abstractions, in which
the planning task is abstracted by instances of tractable fragments of optimal planning.
We then introduce a concrete setting of this framework, called fork-decomposition, that is
based on two novel fragments of tractable cost-optimal planning. The induced admissible
heuristics are then studied formally and empirically. This study testifies for the accuracy
of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff
between their accuracy and the runtime complexity of computing them. Indeed, some
of the power of the explicit abstraction heuristics comes from precomputing the heuristic
function offline and then determining h(s) for each evaluated state s by a very fast lookup
in a database. By contrast, while fork-decomposition heuristics can be calculated in
polynomial time, computing them is far from being fast. To address this problem, we
show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can
be successfully overcome. We demonstrate that an equivalent of the explicit abstraction
notion of a database exists for the fork-decomposition abstractions as well, despite their
exponential-size abstract spaces. We then verify empirically that heuristic search with the
databased fork-decomposition heuristics favorably competes with the state of the art of
cost-optimal planning.

1. Introduction
Heuristic search, either through progression in the space of world states or through regression in the space of subgoals, is a common and successful approach to classical planning.
It is probably the most popular approach to cost-optimal planning, that is, finding a plan
with a minimal total cost of its actions. The difference between various heuristic-search
algorithms for optimal planning is mainly in the admissible heuristic functions they employ.
In state-space search, such a heuristic estimates the cost of achieving the goal from a given
state and guarantees not to overestimate that cost.
A useful heuristic function must be accurate as well as efficiently computable. Improving
the accuracy of a heuristic function without substantially worsening the time complexity
of computing it usually translates into faster search for optimal solutions. During the last
decade, numerous computational ideas evolved into new admissible heuristics for classical
planning; these include the delete-relaxing max heuristic hmax (Bonet & Geffner, 2001), critical path heuristics hm (Haslum & Geffner, 2000), landmark heuristics hL , hLA (Karpas &
Domshlak, 2009) and hLM-cut (Helmert & Domshlak, 2009), and abstraction heuristics such
c
2010
AI Access Foundation. All rights reserved.

fiKatz & Domshlak

as pattern database heuristics (Edelkamp, 2001) and merge-and-shrink heuristics (Helmert,
Haslum, & Hoffmann, 2007). Our focus in this work is on the abstraction heuristics.
Generally speaking, an abstraction of a planning task is given by a mapping  : S  S 
from the states of the planning tasks transition system to the states of some abstract
transition system such that, for all states s, s0  S, the cost from (s) to (s0 ) is upperbounded by the cost from s to s0 . The abstraction heuristic value h (s) is then the cost from
(s) to the closest goal state of the abstract transition system. Perhaps the most well-known
abstraction heuristics are pattern database (PDB) heuristics, which are based on projecting
the planning task onto a subset of its state variables and then explicitly searching for optimal
plans in the abstract space. Over the years, PDB heuristics have been shown to be very
effective in several hard search problems, including cost-optimal planning (Culberson &
Schaeffer, 1998; Edelkamp, 2001; Felner, Korf, & Hanan, 2004; Haslum, Botea, Helmert,
Bonet, & Koenig, 2007). The conceptual limitation of these heuristics, however, is that the
size of the abstract space and its dimensionality must be fixed.1 The more recent merge-andshrink abstractions generalize PDB heuristics to overcome the latter limitation (Helmert
et al., 2007). Instead of perfectly reflecting just a few state variables, merge-and-shrink
abstractions allow for imperfectly reflecting all variables. As demonstrated by the formal
and empirical analysis of Helmert et al., this flexibility often makes the merge-and-shrink
abstractions much more effective than PDBs. However, the merge-and-shrink abstract
spaces are still searched explicitly, and thus they still have to be of fixed size. While quality
heuristics estimates can still be obtained for many problems, this limitation is a critical
obstacle for many others.
Our goal in this paper is to push the envelope of abstraction heuristics beyond explicit
abstractions. We introduce a principled way to obtain abstraction heuristics that limit neither the dimensionality nor the size of the abstract spaces. The basic idea behind what we
call implicit abstractions is simple and intuitive: instead of relying on abstract problems
that are easy to solve because they are small, we can rely on abstract problems belonging to
provably tractable fragments of optimal planning. The key point is that, at least theoretically, moving to implicit abstractions removes the requirement on the abstractions size to
be small. Our contribution, however, is in showing that implicit abstractions are far from
being of theoretical interest only. Specifically,
1. We specify acyclic causal-graph decompositions, a general framework for additive implicit abstractions that is based on decomposing the problem at hand along its causal
graph. We then introduce a concrete family of such abstractions, called fork decompositions, that are based on two novel fragments of tractable cost-optimal planning.
Following the type of analysis suggested by Helmert and Mattmuller (2008), we formally analyze the asymptotic performance ratio of the fork-decomposition heuristics
and prove that their worst-case accuracy on selected domains is comparable with that
of (even parametric) state-of-the-art admissible heuristics. We then empirically evaluate the accuracy of the fork-decomposition heuristics on a large set of domains from
recent planning competitions and show that their accuracy is competitive with the
state of the art.
1. This does not necessarily apply to symbolic PDBs which, on some tasks, may exponentially reduce the
PDBs representation (Edelkamp, 2002).

52

fiImplicit Abstraction Heuristics

2. The key attraction of explicit abstractions is that state-to-goal costs in the abstract
space can be precomputed and stored in memory in a preprocessing phase so that
heuristic evaluation during search can be done by a simple lookup. A necessary condition for this would seem to be the small size of the abstract space. However, we
show that an equivalent of the PDB and merge-and-shrinks notion of database
exists for the fork-decomposition abstractions as well, despite the exponential-size abstract spaces of the latter. These databased implicit abstractions are based on a proper
partitioning of the heuristic computation into parts that can be shared between search
states and parts that must be computed online per state. Our empirical evaluation
shows that A equipped with the databased fork-decomposition heuristics favorably
competes with the state of the art of cost-optimal planning.
This work is a revision and extension of the formulation and results presented by Katz
and Domshlak (2008, 2009), which in turn is based on ideas first sketched also by Katz and
Domshlak (2007a).

2. Preliminaries
We consider classical planning tasks corresponding to state models with a single initial state
and only deterministic actions. Specifically, we consider state models captured by the sas+
formalism (Backstrom & Nebel, 1995) with nonnegative action costs. Such a planning task
is given by a quintuple  = hV, A, I, G, costi, where:
 V is a set of state variables, with each v  V being associated with a finite domain
D(v). For a subset of variables V 0  V , we denote the set of assignments to V 0 by
D(V 0 ) = vV 0 D(v). Each complete assignment to V is called a state, and S = D(V )
is the state space of . I is an initial state. The goal G is a partial assignment to V ;
a state s is a goal state iff G  s.
 A is a finite set of actions. Each action a is a pair hpre(a), eff(a)i of partial assignments
to V called preconditions and effects, respectively. By Av  A we denote the actions
affecting the value of v. cost : A  R0+ is a real-valued, nonnegative action cost
function.
For a variable v and a value   D(v), instantiation of v by  is denoted by v : . For a
partial assignment p, V(p)  V denotes the subset of state variables instantiated by p. In
turn, for any V 0  V(p), by p[V 0 ] we denote the value of V 0 in p; if V 0 = {v} is a singleton,
we use p[v] for p[V 0 ]. For any sequence of actions  and variable v  V , by v we denote the
restriction of  to actions changing the value of v; that is, v is the maximal subsequence
of  consisting only of actions in Av .
An action a is applicable in a state s iff s[v] = pre(a)[v] for all v  V(pre(a)). Applying
a changes the value of v  V(eff(a)) to eff(a)[v]. The resulting state is denoted by sJaK; by
sJha1 , . . . , ak iK we denote the state obtained from sequential application of the (respectively
applicable) actions a1 , . . . , ak starting at state s. Such an action sequence is an s-plan if
G  sJha1 , . . . , ak iK, and it is a cost-optimal (or, in what follows, optimal) s-plan if the
sum of its action costs is minimal among all s-plans. The purpose of (optimal) planning is
finding an (optimal) I-plan. For a pair of states s1 , s2  S, by cost(s1 , s2 ) we refer to the
53

fiKatz & Domshlak

p2

B

c2

F
c

A
c1

t

D

c

c

t

E
p

c3

C

p

G
p1

(a)

(b)
in t

in c

B
A

D

F
D

E

C

E

at A

at B

at C

at D

at E

at F

at G

G
in c

(c)

in c

(d)

Figure 1: Logistics-style example adapted from Helmert (2006) and illustrated in (a). The
goal is to deliver p1 from C to G and p2 from F to E using the cars c1 , c2 , c3 and
truck t, making sure that c3 ends up at F . The cars may only use city roads (thin
edges); the truck may only use the highway (thick edge). Figures (b), (c), and
(d) depict, respectively, the causal graph of the problem, the domain transition
graphs (labels omitted) of c1 and c2 (left), t (center), and c3 (right), and the
identical domain transition graphs of of p1 and p2 .

cost of a cost-optimal plan from s1 to s2 ; h (s) = mins0 G cost(s, s0 ) is the custom notation
for the cost of the optimal s-plan in . Finally, important roles in what follows are played
by a pair of standard graphical structures induced by planning tasks.
 The causal graph CG() of  is a digraph over nodes V . An arc (v, v 0 ) is in CG()
iff v 6= v 0 and there exists an action a  A such that (v, v 0 )  V(eff(a))  V(pre(a)) 
V(eff(a)). In this case, we say that (v, v 0 ) is induced by a. By succ(v) and pred(v) we
respectively denote the sets of immediate successors and predecessors of v in CG().
 The domain transition graph DTG(v, ) of a variable v  V is an arc-labeled digraph
over the nodes D(v) such that an arc (, 0 ) labeled with pre(a)[V \ {v}] and cost(a)
exists in the graph iff both eff(a)[v] = 0 , and either pre(a)[v] =  or v 6 V(pre(a)).
To illustrate various constructs, we use a slight variation of a Logistics-style example
from Helmert (2006). This example is depicted in Figure 1a, and in sas+ it has
54

fiImplicit Abstraction Heuristics

V

= {p1 , p2 , c1 , c2 , c3 , t}

D(p1 ) = D(p2 ) = {A, B, C, D, E, F, G, c1 , c2 , c3 , t}
D(c1 ) = D(c2 ) = {A, B, C, D}

D(c3 ) = {E, F, G}
D(t) = {D, E}

I = {p1 : C, p2 : F, t : E, c1 : A, c2 : B, c3 : G}

G = {p1 : G, p2 : E, c3 : F },

and actions corresponding to all possible loads and unloads, as well as single-segment movements of the vehicles. For instance, if action a captures loading p1 into c1 at C, then
pre(a) = {p1 : C, c1 : C}, and eff(a) = {p1 : c1 }. All actions in the example have unit cost.
The causal graph of this example, as well as the domain transition graphs of the state
variables, are depicted in Figures 1b-1d.
Heuristic functions are used by informed-search procedures to estimate the cost (of the
cheapest path) from a search node to the nearest goal node. Our focus here is on statedependent, admissible abstraction heuristics. A heuristic function h is state-dependent if its
estimate for a search node depends only on the problem state associated with that node,
that is, h : S  R0+  {}. Most heuristics in use these days are state-dependent (though
see, e.g., Richter, Helmert, & Westphal, 2008 and Karpas & Domshlak, 2009 for a different
case). A heuristic h is admissible if h(s)  h (s) for all states s. If h1 and h2 are two
admissible heuristics, and h2 (s)  h1 (s) for all states s, we say that h1 dominates h2 .
For any set of admissible heuristics h1 , . . . , hm , their pointwise maximum is always an
admissible heuristic, dominating each individual heuristic in the set. For some sets of admissible heuristics, their pointwise sum is also admissible and dominates their pointwise
maximum. Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, &
Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp,
2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert &
Domshlak, 2009). In particular, Katz and Domshlak (2007a, 2008) and Yang et al. (2007,
2008) independently introduced a general criterion for admissible additive ensembles of
heuristics, called in the former work action cost partitioning. This criterion can be formalized as follows. Let  = hV, A, I, G,
be a planning task and {costi : A  R0+ }m
i=1 a
Pcosti
m
family of cost functions such that i=1 costi (a)  cost(a) for all actions a  A. If {hi }m
i=1
is a set
of
arbitrary
admissible
heuristic
functions
for

=
hV,
A,
I,
G,
cost
i,
respectively,
i
i
P
m
then m
i=1 hi is also an admissible heuristic for . The set of cost functions {costi }i=1 can
be seen as a partition of the action costs cost.

3. Abstractions and Abstraction Heuristics
The semantics of any planning task  is given by its induced state-transition model, often
called the transition graph of .

55

fiKatz & Domshlak

Definition 1 A transition graph is a tuple T = (S, L, Tr, s0 , S ? , $) where S is a finite
set of states, L is a finite set of transition labels, Tr  S  L  S is a set of (labeled)
transitions, s0  S is an initial state, S ?  S is a set of goal states, and $ : L  R0+ is a
transition cost function.
 For a state s  S and a subset of states S 0  S in T, cost(s, S 0 ) is the cost (of a
cheapest with respect to $ path) from s to a state in S 0 along the transitions of T; if
no state in S 0 is reachable from s, then we have cost(s, S 0 ) = .
 Any path from s0 to S ? is a plan for T, and cheapest such plans are called optimal.
The states of the transition graph T() induced by a planning task  = hV, A, I, G, costi
are the states of . The transition labels of T() are the actions A; there is a transition
(s, a, sJaK)  Tr iff a is applicable in s; the initial state s0 = I; the set of goal states
S ? = {s  S | s  G}; and the transition cost function $ = cost.
We now proceed
with formally specifying the notion of abstraction. Our definition of abstraction resembles
that of Prieditis (1993), and right from the beginning we specify a more general notion of
additive abstraction. Informally, by additive abstraction we refer to a set of abstractions
interconstrained by a requirement to jointly not overestimate the transition-path costs in
the abstracted transition graph.
Definition 2 An additive abstraction of a transition graph T = (S, L, Tr, s0 , S ? , $) is
a set of pairs {hTi , i i}m
i=1 where, for 1  i  m,
 Ti = (Si , Li , Tri , s0i , Si? , $i ) is a transition graph,
 i : S  Si is a function, called abstraction mapping, such that
 i (s0 ) = s0i , i (s)  Si? for all s  S ? , and,
 for all pairs of states s, s0  S holds
m
X
i=1

cost(i (s), i (s0 ))  cost(s, s0 ).

(1)

A few words on why we use this particular notion of abstraction. The term abstraction
is usually associated with simplifying the original system, reducing and factoring out details
less crucial in the given context. Which details can be reduced and which should better
be preserved depends, of course, on the context. For instance, in the context of formal
verification, the abstract transition graphs are required not to decrease the reachability
between the states; that is, if there is a path from s to s0 in the original transition graph,
then there should be a path from (s) to (s0 ) in the abstract transition graph (Clarke,
Grumberg, & Peled, 1999). In addition, the reachability should also be increased as little as
possible. Beyond that, the precise relationship between the path costs in the original and
abstract transition graphs is only of secondary importance. In contrast, when abstractions
are designed to induce admissible heuristic functions for heuristic search, the relationship
between the path costs as captured by Eq. 1 is what must be obeyed. However, requirements
above and beyond the general requirement of Eq. 1 not to overestimate the distances between
56

fiImplicit Abstraction Heuristics

the states are unnecessary. Hence, in particular, Definition 2 generalizes the notion of
abstraction by Helmert et al. (2007) by replacing the condition of preserving individual
transitions and their labels, that is, ((s), l, (s0 )) if (s, l, s0 ), with a weaker condition stated
in Eq. 1. The reader, of course, may well ask whether the generality of the condition in
Eq. 1 beyond the condition of Helmert et al. (2007) really delivers any practical gain, and
later we show that the answer to this question is affirmative. For now, we proceed with
adding further requirements essential to making abstraction usable as a basis for heuristic
functions.
Definition 3 Let  be a planning task over states S, and let {hTi , i i}m
i=1 be an additive
abstraction of the transition graph T(). If m = O(poly(||||)) and, for all states s  S
and all 1P
 i  m, the cost cost(i (s), Si? ) in Ti is computable in time O(poly(||||)), then
?
hA (s) = m
i=1 cost(i (s), Si ) is an abstraction heuristic function for .
Note that admissibility of hA is implied by the cost conservation condition of Eq. 1. To further illustrate the connection between abstractions and admissible heuristics, consider three
well-known mechanisms for devising admissible planning heuristics: delete relaxation (Bonet
& Geffner, 2001), critical-path relaxation (Haslum & Geffner, 2000),2 and pattern database
heuristics (Edelkamp, 2001).
First, while typically not considered this way, the delete relaxation of a planning task
? , $ ),  i
 = hV, A, I, G, costi does correspond to an abstraction hT+ = (S+ , L+ , Tr+ , s0+ , S+
+
+
of the transition
graph
T().
Assuming
unique
naming
of
the
variable
values
in

and
deS
noting D+ = vV D(v), we have the abstract states S+ being the power-set of D+ , and the
labels L+ = {a, a+ | a  A}. The transitions come from two sources: for each abstract state
s+  S+ and each original action a  A applicable in s+ , we have both (s+ , a, s+ JaK)  Tr+
and (s+ , a+ , s+  eff(a))  Tr+ . With a minor abuse of notation, the initial state and the
? = {s  S | s  G}, and the abstraction
goal states of the abstraction are s0+ = I and S+
+
+
+
mapping + is simply the identity function. It is easy to show that, for any state s of our
? ) = h+ (s), where h+ (s) is the delete-relaxation
planning task , we have cost(+ (s), S+
estimate of the cost from s to the goal. As an aside, we note that this delete-relaxation
abstraction hT+ , + i in particular exemplifies that nothing in Definition 2 requires the
size of the abstract state space to be limited by the size of the original state space. In any
event, however, the abstraction hT+ , + i does not induce a heuristic in terms of Definition 3
because computing h+ (s) is known to be NP-hard (Bylander, 1994).
The situation for critical-path relaxation is exactly the opposite. While computing
the corresponding family of admissible estimates hm is polynomial-time for any fixed m,
this computation is not based on computing the shortest paths in an abstraction of the
planning task. The state graph over which hm is computed is an AND/OR-graph (and not
an OR-graph such as transition graphs), and the actual computation of hm corresponds
to computing a critical tree (and not a shortest path) to the goal. To the best of our
knowledge, the precise relation between critical path and abstraction heuristics is currently
an open question (Helmert & Domshlak, 2009).
Overall, the only abstraction heuristics in the toolbox of planning these days appear to
be the explicit homomorphism abstractions, whose best-known representative is probably
2. We assume the reader is familiar with these two relaxations. If not, their discussion here can be safely
skipped.

57

fiKatz & Domshlak

the pattern database (PDB) heuristics. Given a planning task  over state variables V ,
a PDB heuristic is based on projecting  onto a subset of its variables V   V . Such a
homomorphism abstraction  maps two states s1 , s2  S into the same abstract state iff
s1 [V  ] = s2 [V  ]. Inspired by the (similarly named) domain-specific heuristics for search
problems such as (n2  1)-puzzles or Rubiks Cube (Culberson & Schaeffer, 1998; Hernadvolgyi & Holte, 1999; Felner et al., 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al.,
2007). The key decision in constructing PDBs is what sets of variables the problem is
projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to
automatically select good projections, the two limitations of PDB heuristics are the size of
the abstract space S  and its dimensionality. First, the number of abstract states should
be small enough to allow reachability analysis in S  by exhaustive search. Moreover, an
O(1) bound on |S  | is typically set explicitly to fit the time and memory limitations of
the system. Second, since PDB abstractions are projections, the explicit constraint on |S  |
implies a fixed-dimensionality constraint |V  | = O(1). In planning tasks with, informally,
many alternative resources, this limitation is a pitfall. For instance, suppose {i }
i=1 is a
sequence of Logistics problems of growing size with |Vi | = i. If each package in i can be
transported by some (i) vehicles, then starting from some i, h will not account at all for
movements of vehicles essential for solving i (Helmert & Mattmuller, 2008).
Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology
of Drager, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink
(MS) abstractions for planning. MS abstractions are homomorphisms that generalize PDB
abstractions by allowing for more flexibility in selection of pairs of states to be contracted.
The problems state space is viewed as the synchronized product of its projections onto the
single state variables. Starting with all such atomic abstractions, this product can be
computed by iteratively composing two abstract spaces, replacing them with their product.
While in a PDB the size of the abstract space S  is controlled by limiting the number of
product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites. Helmert et al. (2007) have
proposed a concrete strategy for this interleaved abstraction/refinement scheme and empirically demonstrated the power of the merge-and-shrink abstraction heuristics. Like PDBs,
however, MS abstractions are explicit abstractions, and thus computing their heuristic values is also based on explicitly searching for optimal plans in the abstract spaces. Hence,
while merge-and-shrink abstractions escape the fixed-dimensionality constraint of PDBs,
the constraint on the abstract space to be of a fixed size still holds.

4. Implicit Abstractions
Focusing on the O(1) bound posted by explicit abstractions on the size of the abstract
space, our first observation is that explicit abstractions are not necessarily the only way to
proceed with abstraction heuristics. Given a planning task  over states S, suppose we can
transform it into a different planning task  such that
1. the transformation induces an abstraction mapping  : S  S  where S  is the state
space of  , and
58

fiImplicit Abstraction Heuristics

2. both the transformation of  to  , as well as computing  for any state in S, can
be done in time polynomial in ||||.
Having such planning-task-to-planning-task transformations in mind, we define what we
call (additive) implicit abstractions.
Definition 4 An additive implicit abstraction of a planning task  is a set of pairs
m
m
A = {hi , i i}m
i=1 such that {i }i=1 are some planning tasks and {hT(i ), i i}i=1 is an
additive abstraction of T().
Let us now examine the notion of implicit abstractions more closely. First, implicit
abstractions allow for a natural additive combination of admissible heuristics for the abstract
tasks. This composition is formulated below by Theorem 1, extending the original criterion
for admissibility of additive heuristics described in Section 2. Second, as formulated by
Theorem 2, implicit abstractions can be composed via the functional composition of their
abstraction mappings. These two easy-to-prove properties of implicit abstractions allow us
then to take the desired step from implicit abstractions to implicit abstraction heuristics.
Theorem 1 (Admissibility) Let  be a planning task and A = {hi , i i}m
i=1 be an additive implicit abstraction of
.
If,
for
each
1

i

m,
h
is
an
admissible
heuristic
for i ,
i
Pm
then the function h(s) = i=1 hi (i (s)) is an admissible heuristic for .
Proof: The proof is straightforward. Let T = (S, L, Tr, s0 , S ? , $) be the transition graph
of , and let s be some state in S. For each 1  i  m, let Ti = (Si , Li , Tri , s0i , Si? , $i ) be
the transition graph of i .
First, if hi is an admissible heuristic for i , then for all si  Si? ,
hi (i (s))  cost(i (s), si ).
Now, for each state s0  S ? , from Definition 2 we have i (s0 )  Si? , and from Eq. 1 we have
m
X
i=1

and thus
h(s) =

m
X
i=1

cost(i (s), i (s0 ))  cost(s, s0 ),

hi (i (s)) 

giving us an admissible estimate for

m
X

i=1

h (s).

cost(i (s), i (s0 ))  cost(s, s0 ),


Theorem 2 (Composition) Let  be a planning task and A = {hi , i i}m
i=1 be an addimi
tive implicit abstraction of . If, for each
i,j , i,j i}j=1 is an additive
S 1  i  m, Ai = m{h
i
implicit abstraction of i , then A0 = m
{h
,



i}
is
an additive implicit abi,j
i,j
i j=1
i=1
straction of .
Proof: Let T = (S, L, Tr, s0 , S ? , $) be the transition graph of . For each 1  i  m,
let Ti = (Si , Li , Tri , s0i , Si? , $i ) be the transition graph of i , and for each 1  j  mi , let
? , $ ) be the transition graph of  . We need to show that
Ti,j = (Si,j , Li,j , Tri,j , s0i,j , Si,j
i,j
i,j
i,j  i is an abstraction mapping as in Definition 2. From i and i,j being abstraction
mappings, we have
59

fiKatz & Domshlak

 s0i,j = i,j (s0i ) = i,j (i (s0 )) = i,j  i (s0 ),
? , and
 for all s  S ? we have i (s)  Si? and thus i,j (i (s)) = i,j  i (s)  Si,j
P i
0
0
 for all si , s0i  Si , cost(si , s0i )  m
j=1 cost(i,j (si ), i,j (si )), and thus for all s, s  S,

cost(s, s0 ) 

m
X
i=1

cost(i (s), i (s0 )) 
=

mi
m X
X
i=1 j=1
mi
m X
X
i=1 j=1

cost(i,j (i (s)), i,j (i (s0 )))
cost(i,j  i (s), i,j  i (s0 )).


Together, Theorems 1 and 2 suggest the following scheme for deriving abstraction heuristics. Given an additive implicit abstraction A = {hi , i i}m
i=1 , if all its individual abstract
tasks belong to some tractable fragments of optimal planning, then we can use in practice
the (sum of the) true costs in all i as the admissible estimates for the costs in . Otherwise, if optimal planning for some abstract tasks i in A cannot be proven polynomial-time
solvable, then we can further abstract these tasks, obtaining admissible estimates for the
true costs in i .
Definition 5 Let  be a planning task over states S, and let A = {hi , i i}m
i=1 be an
additive implicit abstraction of . If m = O(poly(||||)), and, for allP
states s  S and all

1  i  m, h (i (s)) is polynomial-time computable, then hA (s) = m
i=1 h (i (s)) is an
implicit abstraction heuristic function for .
Compared to explicit abstraction heuristics such as PDB heuristics and merge-andshrink heuristics, the direction of implicit abstraction heuristics is, at least in principle,
appealing because neither the dimensionality nor even the size of the state spaces induced
by implicit abstractions are required to be bounded by something restrictive, if at all. The
pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of
optimal planning, and the palette of such known fragments is extremely limited (Backstrom
& Nebel, 1995; Bylander, 1994; Jonsson & Backstrom, 1998; Jonsson, 2007; Katz & Domshlak, 2007b). In fact, none so far has appeared to us very convenient for automatically devising useful problem transformations as above. Fortunately, we show next that the boundaries
of tractability can be expanded in the right way, allowing us to successfully materialize the
idea of implicit abstraction heuristics.
In the following, a key role is played by the causal graphs induced by the planning
tasks. Informally, the basic idea behind what we call causal-graph decompositions is to
abstract the given planning task  along a subgraph of s causal graph, with the goal of
obtaining abstract problems of specific structure. Naturally, there are numerous possibilities
for obtaining such structure-oriented abstractions. We now present one such decomposition
that is tailored to abstractions around acyclic subgraphs. Informally, this decomposition
can be seen as a sequential application of two kinds of task transformations: dropping
preconditions (Pearl, 1984) and (certain form of) breaking actions with conjunctive effects
into single-effect actions.
60

fiImplicit Abstraction Heuristics

Definition 6 Let  = hV, A, I, G, costi be a planning task, and let G = (VG , EG ) be an
acyclic subgraph of the causal graph CG(). A planning task G = hVG , AG , IG , GG , costG i
is an acyclic causal-graph decomposition of  with respect to G if
1. IG = I[VG ], GG = G[VG ],
S
2. AG = aA AG (a) where each AG (a) = {a1 , . . . , al(a) } is a set of actions over VG
such that, for a topological with respect to G ordering of the variables {v1 , . . . , vl(a) } =
V(eff(a))  VG , and 1  i  l(a),
(
eff(a)[v],
v = vi
i
eff(a )[v] =
unspecified, otherwise

(2)

(v, vi )  EG  v 6 V(eff(a)) or v = vi
pre(a)[v],
pre(ai )[v] = eff(a)[v],
(v, vi )  EG  v  V(eff(a))


unspecified, otherwise
3. For each action a  A,

X
a0 AG (a)

costG (a0 )  cost(a).

(3)

It is not hard to verify from Definition 6 that for any planning task  and any acyclic
causal-graph decomposition G of , the causal graph CG(G ) is exactly the subgraph G underlying the decomposition. To illustrate the notion of acyclic causal-graph decomposition,
we consider a planning task  = hV, A, I, G, costi over five state variables V = {u, v, x, y, z},
two unit-cost actions A = {a1 , a2 } as in Figure 2a, initial state I = {u : 0, v : 0, x : 0, y : 0, z : 0},
and goal G = {u : 1, v : 1, x : 0, y : 1, z : 1}. The causal graph CG() is depicted in Figure 2a.
Figures 2b-c show two subgraphs G1 and G2 of CG(), respectively, as well as the action sets AG1 (a1 ) = {a11 , a21 , a31 } and AG1 (a2 ) = {a12 , a22 , a32 } in Figure 2(b), and the action
sets AG2 (a1 ) = {a11 , a21 , a31 } and AG2 (a2 ) = {a12 , a22 , a32 } in Figure 2(c). For i  {1, 2}, let
i = hV, Ai , I, G, costi i be the planning task with Ai = AGi (a1 )AGi (a2 ) and costi (a) = 1/3
for all a  Ai . These two planning tasks i (individually) satisfy the conditions of Definition 6 with respect to  and Gi , and thus they are acyclic causal-graph decompositions of
 with respect to Gi .
We now proceed with specifying implicit abstractions defined via acyclic causal-graph
decompositions.
Definition 7 Let  = hV, A, I, G, costi be a planning task over states S, and let G = {Gi =
m
(VGi , EGi )}m
i=1 be a set of acyclic subgraphs of the causal graph CG(). A = {hGi , i i}i=1
is an acyclic causal-graph abstraction of  over G if, for some set of cost functions
{costi : A  R0+ }m
i=1 satisfying
a  A :

m
X
i=1

costi (a)  cost(a),

we have, for 1  i  m,
61

(4)

fiKatz & Domshlak

a1 = h{x : 0, y : 0, z : 0}, {x : 1, y : 1, z : 1}i

a11 = h{x : 0}, {x : 1}i
a21 = h{x : 1, y : 0}, {y : 1}i
a31 = h{x : 1, z : 0}, {z : 1}i

a11 = h{y : 0}, {y : 1}i
a21 = h{z : 0}, {z : 1}i
a31 = h{y : 1, z : 1, x : 0}, {x : 1}i

a2 = h{u : 0, v : 0, x : 1}, {u : 1, v : 1, x : 0}i

a12 = h{x : 1}, {x : 0}i
a22 = h{x : 0, u : 0}, {u : 1}i
a32 = h{x : 0, v : 0}, {v : 1}i

a12 = h{u : 0}, {u : 1}i
a22 = h{v : 0}, {v : 1}i
a32 = h{u : 1, v : 1, x : 1}, {x : 0}i

u

a1

a2

x

a2
a2

y

x
a1

a22

a1

v

z

u

a31

a32 a21

y

v

(a)

u

(b)

a32

z

y

v
a32

a31

z
a31

x
(c)

Figure 2: (a) The actions and causal graph CG() of the planning graph in the example
illustrating Definition 2. (b) Subgraph G1 of CG() and the induced action sets
AG1 (a1 ) and AG1 (a2 ). (c) Subgraph G2 of CG() and the induced action sets
AG2 (a1 ) and AG2 (a2 ). The arcs of both CG() and its subgraphs G1 and G2 are
labeled with the actions inducing the arcs.

 Gi = hVGi , AGi , IGi , GGi , costGi i is an acyclic causal-graph decomposition of i =
hV, A, I, G, costi i with respect to Gi , and
 the abstraction mapping i : S  Si is the projection mapping i (s) = s[VGi ].
Theorem 3 Acyclic causal-graph abstractions of the planning tasks are additive implicit
abstractions of these tasks.
Proof: Let  = hV, A, I, G, costi be a planning task, and let A = {hGi , i i}m
i=1 be an
acyclic causal-graph abstraction of  over a set of subgraphs G = {Gi = (VGi , EGi )}m
i=1 .
Let T = (S, L, Tr, s0 , S ? , $) be the transition graph of , and, for 1  i  m, Ti =
(Si , Li , Tri , s0i , Si? , $i ) be the transition graph of Gi . We need to show that i is an abstraction mapping as in Definition 2.
First, from Definitions 6 and 7, we have
 s0i = IGi = I[VGi ] = s0 [VGi ] = i (s0 ), and
 for all s  S ? we have s  G and thus i (s) = s[VGi ]  G[VGi ] = GGi , providing us
with i (s)  Si? .
Now, if s is a state of  and a  A is an action with pre(a)  s, then i (s) is a state of Gi
and pre(a)[VGi ]  i (s). Let the action sequence  = ha1 , a2 , . . . , al(a) i be constructed from
a as in Eq. 2. We inductively prove that  is applicable in i (s). First, for each v  VGi ,
either pre(a1 )[v] = pre(a)[v], or pre(a1 )[v] is unspecified, and thus 1 = ha1 i is applicable in
i (s). The inductive hypothesis is now that j = ha1 , a2 , . . . , aj i is applicable in i (s), and
0
let s0 = i (s)Jj K. From Eq. 2, for each 1  j 0  j, aj changes the value of vj 0 to eff(a)[vj 0 ],
62

fiImplicit Abstraction Heuristics

and that is the only change of vj 0 along j . Likewise, since all the actions constructed as in
Eq. 2 are unary-effect, {v1 , . . . , vj } are the only variables in VGi affected along j . Hence,
for all v  VGi , if v = vj 0 , 1  j 0  j, then s0 [v] = eff(a)[v] = pre(aj+1 )[v], and otherwise,
s0 [v] = i (s)[v], and if pre(aj+1 )[v] is specified, then pre(aj+1 )[v] = pre(a)[v] = i (s)[v]. This
implies that aj+1 is applicable in s0 and, as a result, j+1 = ha1 , a2 , . . . , aj+1 i is applicable in
i (s), finalizing the inductive proof. Likewise, exactly the same arguments on the affect of
l(a)
{aj }j=1 on i (s) immediately imply that, if  = ha1 , a2 , . . . , al(a) i, then i (sJaK) = i (s)JK.
Next, for each a  A, from Eqs. 3 and 4 we have
m
X

X

i=1 a0 AGi (a)

costGi (a0 ) 

m
X
i=1

costi (a)  cost(a).

(5)

Now, let s, s0  S be a pair of original states such that cost(s, s0 ) < , and let % =
0
ha1 , . . . , ak i be the sequence
Pk of labels along a cheapest path from s to s in T. From that,
0
cost(s, s ) = cost(%) = j=1 cost(aj ). The decomposition of such a path to the sequences
of actions as in Eq. 2 is aP(not P
neccesarily cheapest) path from i (s) to i (s0 ) in Ti , and
k
0
thus cost(i (s), i (s ))  j=1 a0 AG (aj ) costGi (a0 ), providing us with
i

m
X
i=1

0

cost(i (s), i (s )) 
(5)



m X
k
X

X

0

costGi (a ) =

i=1 j=1 a0 AGi (aj )
k
X

k X
m
X

X

costGi (a0 )

j=1 i=1 a0 AGi (aj )

cost(aj ) = cost(s, s0 ).

j=1


Thus, if we can decompose the given task  into a set of tractable acyclic causalgraph decompositions  = {G1 , . . . , Gm }, then we can solve all these tasks in polynomial
time, and derive an additive admissible heuristic for . Before we proceed with considering
concrete acyclic causal-graph decomposition, note that Definition 2 leaves the decision about
the actual partition of the action costs rather open. In what follows we adopt the most
straightforward, uniform action cost partition in which theScost of each action a is equally
split among all the non-redundant representatives of a in m
i=1 AGi (a). However, a better
choice of action cost partition can sometimes be made. In fact, sometimes it can even be
optimized (Katz & Domshlak, 2010)

5. Fork Decompositions
We now proceed with introducing two concrete acyclic causal-graph decompositions that,
when combined with certain variable domain abstractions, provide us with implicit abstraction heuristics. These so called fork-decomposition heuristics are based on two novel
fragments of tractable cost-optimal planning for tasks with fork and inverted-fork structured
causal graphs.
Definition 8 For a planning task  over variables V , and a variable v  V ,
63

fiKatz & Domshlak

(1) v-fork of  is the subgraph Gvf of CG() over nodes VGvf = {v}  succ(v) and edges
EGvf = {(v, u) | u  succ(v)}, and
(2) v-ifork (short for inverted fork) of  is a subgraph Gvi of CG() over nodes VGvi =
{v}  pred(v) and edges EGvi = {(u, v) | u  pred(v)}.
The sets of all v-forks and all v-iforks of  are denoted by GF = {Gvf }vV and GI =
{Gvi }vV , respectively.
For any planning task and each of its state variables v, both v-fork and v-ifork are
acyclic digraphs, allowing us to define our three implicit abstractions as follows.
Definition 9 For any planning task  = hV, A, I, G, costi,

(1) any acyclic causal-graph abstraction AF = {hfv , vf i}vV of  over GF is called
F-abstraction, and the set of abstract planning tasks F = {fv }vV is called
F-decomposition of ;
(2) any acyclic causal-graph abstraction AI = {hiv , vi i}vV of  over GI is called
I-abstraction, and the set of abstract planning tasks I = {iv }vV is called
I-decomposition of ;
(3) any acyclic causal-graph abstraction AFI = {hfv , vf i, hiv , vi i}vV of  over
GFI = GF  GI is called FI-abstraction, and the set of abstract planning tasks
FI = {fv , iv }vV is called FI-decomposition of .

Definition 9 can be better understood by considering the FI-abstraction of the problem
 from our Logistics example; Figure 3 schematically illustrates the process. To simplify
the example, here we as if eliminate from GFI all the single-node subgraphs, obtaining
AFI = {hfc1 , cf 1 i, {hfc2 , cf 2 i, {hfc3 , cf 3 i, {hft , tf i, {hip1 , pi 1 i, {hip2 , pi 2 i}.
Considering the action sets of the problems in FI = {fc1 , fc2 , fc3 , ft , ip1 , ip2 }, we see
that each original driving action has one nonredundant (that is, changing some variable)
representative in three of the abstract planning tasks, while each load/unload action has
one nonredundant representative in five of these tasks. For instance, the action drive-c1 from-A-to-D has one nonredundant representative in each of the tasks {fc1 , ip1 , ip2 }, and
the action load-p1 -into-c1 -at-A has one nonredundant representative in each of the tasks
{fc1 , fc2 , fc3 , ft , ip1 }. Since we assume a uniform partition of the action costs, the cost
of each driving and load/unload action in each relevant abstract planning task is thus set
to 1/3 and 1/5, respectively. From Theorem 3 we have AFI being an additive implicit
abstraction of , and from Theorem 1 we then have

X
hFI =
hf + hi ,
(6)
v

v

vV

being an admissible estimate of h in . The question now is how good this estimate is.
The optimal cost of solving our running example is 19. Taking as a reference the well-known
admissible heuristics hmax (Bonet & Geffner, 2001) and h2 (Haslum & Geffner, 2000), we
have hmax (I) = 8 and h2 (I) = 13. Considering our FI-abstraction, the optimal plans for
the tasks in FI are as follows.
64

fiImplicit Abstraction Heuristics

fc1 : load-p1 -into-c2 -at-C, unload-p1 -from-c2 -at-D, load-p1 -into-t-at-D,
unload-p1 -from-t-at-E, load-p1 -into-c3 -at-E, unload-p1 -from-c3 -at-G,
load-p2 -into-c3 -at-F, unload-p2 -from-c3 -at-E.
fc2 : load-p1 -into-c1 -at-C, unload-p1 -from-c1 -at-D, load-p1 -into-t-at-D,
unload-p1 -from-t-at-E, load-p1 -into-c3 -at-E, unload-p1 -from-c3 -at-G,
load-p2 -into-c3 -at-F, unload-p2 -from-c3 -at-E.
fc3 : load-p1 -into-c1 -at-C, unload-p1 -from-c1 -at-D, load-p1 -into-t-at-D,
unload-p1 -from-t-at-E, drive-c3 -from-G-to-E, load-p1 -into-c3 -at-E,
drive-c3 -from-E-to-G, unload-p1 -from-c3 -at-G, drive-c3 -from-G-to-E,
drive-c3 -from-E-to-F, load-p2 -into-c3 -at-F, drive-c3 -from-F-to-E,
unload-p2 -from-c3 -at-E, drive-c3 -from-E-to-F.
ft : load-p1 -into-c1 -at-C, unload-p1 -from-c1 -at-D, drive-t-from-E-to-D,
load-p1 -into-t-at-D, drive-t-from-D-to-E, unload-p1 -from-t-at-E,
load-p1 -into-c3 -at-E, unload-p1 -from-c3 -at-G, load-p2 -into-c3 -at-F,
unload-p2 -from-c3 -at-E.
ip1 : drive-c1 -from-A-to-D, drive-c1 -from-D-to-C, load-p1 -into-c1 -at-C,
drive-c1 -from-C-to-D, unload-p1 -from-c1 -at-D, drive-t-from-E-to-D,
load-p1 -into-t-at-D, drive-t-from-D-to-E, unload-p1 -from-t-at-E,
drive-c3 -from-G-to-E, load-p1 -into-c3 -at-E, drive-c3 -from-E-to-G,
unload-p1 -from-c3 -at-G, drive-c3 -from-G-to-E, drive-c3 -from-E-to-F.
ip2 : drive-c3 -from-G-to-E, drive-c3 -from-E-to-F, load-p2 -into-c3 -at-F,
drive-c3 -from-F-to-E, unload-p2 -from-c3 -at-E, drive-c3 -from-E-to-F.
Hence, we have
hFI = hf

c1

=

8
5

+ hf

+

8
5

+

c2

+

hf
8
5

c3

+

6
3

hf

+
+

8
5

t

+

2
3

+

hi

+

6
5

p1

+

9
3

+

hf

+

2
5

p2

+

4
3

= 15,

(7)

and so hFI appears at least promising.
Unfortunately, despite the seeming simplicity of the planning tasks in FI , it turns out
that implicit fork-decomposition abstractions as in Definitions 9 do not fit the requirements
of implicit abstraction heuristics as in Definition 5. The causal graphs of the planning
tasks in F and I form directed forks and directed inverted forks, respectively, and, in
general, the number of variables in each such planning task can be as large as (|V |).
The problem is that even satisficing planning for sas+ fragments with fork and inverted
fork causal graphs is NP-complete (Domshlak & Dinitz, 2001). In fact, recent results by
Chen and Gimenez (2008) show that planning for any sas+ fragment characterized by any
nontrivial form of causal graph is NP-hard. Moreover, even if the domain transition graphs
of all the state variables are strongly connected (as in our example), optimal planning for
fork and inverted fork structured problems remain NP-hard (see Helmert 2003, and 2004
for the respective results). Next, however, we show that this is not the end of the story for
fork decompositions.
65

fiKatz & Domshlak



B
A
c1

p2

c2

CG()

F
c!

t

D

p!

G

p1

c#

t

E
c3

C

c"

p"

{fv , iv }vV
fc1

c!

p!

c!

p"

c"

c#

t

ip1

p!
if
CG(
CG(p1ip1))

CG(fcfc11))
CG(

Figure 3: Schematic illustration of FI-decomposition for our running Logistics example
While the hardness of optimal planning for problems with fork and inverted fork causal
graphs casts a shadow on the relevance of fork decompositions, a closer look at the proofs of
the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003, 2004)
reveals that they in particular rely on root variables having large domains. Exploiting this
observation, we now show that this reliance is not incidental and characterize two substantial
islands of tractability within the structural fragments of sas+ .
Theorem 4 (Tractable Forks) Given a planning task  = hV, A, I, G, costi with a fork
causal graph rooted at r  V , if |D(r)| = 2, the time complexity of the cost-optimal planning
for  is polynomial in ||||.
Proof: Observe that, for any planning task  as in the theorem, the fork structure of the
causal graph CG() implies that all the actions in  are unary-effect, and each leaf variable
v  succ(r) preconditions only the actions affecting v itself. The algorithm below is based
on the following three properties satisfied by the optimal plans  for .
(i) For any leaf variable v  succ(r), the path v from I[v] to G[v] induced by  in
DTG(v, ) is either cycle-free or contains only zero-cost cycles. This is the case because
otherwise all the nonzero-cost cycles can be eliminated from v while preserving its
validity, violating the assumed optimality of . Without loss of generality, in what
follows we assume that this path v in DTG(v, ) is cycle-free; in the case of fork
causal graphs, we can always select an optimal  that satisfies this requirement for all
v  succ(r). Thus, we have |v |  |D(v)|  1.
(ii) Having fixed a sequence of value changes of r, the forks leaves become mutually
independent; that is, our ability to change the value of one of them does not affect
our ability to change the value of all the others.
66

fiImplicit Abstraction Heuristics

(iii) Because r is binary-valued, if v  V \ {r} is the most demanding leaf variable in
terms of the number of value changes required from r by the action preconditions
along v , then these are the only value changes of r along , except for, possibly, a
final value change to G[r]. Thus, in particular, we have |r |  maxvsucc(r) |D(v)|.
We begin with introducing some auxiliary notations. With |D(r)| = 2, let D(r) = {0, 1}
with I[r] = 0. Let (r) be an alternating 0/1 sequence starting with 0, and having 0 in
all odd and 1 in all even positions. This sequence (r) is such that |(r)| = 1 if no action
in A can change rs value to 1, |(r)| = 2 if some action can change rs value to 1 but no
action can then restore it to value 0, and otherwise, |(r)| = 1 + maxvsucc(r) |D(v)|. Let
[(r)] be the set of all nonempty prefixes of (r) if G[r] is unspecified; otherwise, let it
be the set of all nonempty prefixes of (r) ending with G[r]. Note that, if [(r)] = ,
then the problem is trivially unsolvable; in what follows we assume this is not the case. For
each v  succ(r), let DT G0v and DT G1v be the subgraphs of the domain transition graphs
DTG(v, ), obtained by removing from DTG(v, ) all the arcs labeled with r : 1 and r : 0,
respectively.
The algorithm below incrementally constructs a set R of valid plans for , starting with
R = .
(1) For each v  succ(r), and each pair of vs values x, y  D(v), compute the cheapest
(that is, cost-minimal) paths v0 (x, y) and v1 (x, y) from x to y in DT G0v and DT G1v ,
respectively. For some pairs of values x, y, one or even both these paths may, of course,
not exist.
(2) For each sequence   [(r)], and each v  succ(r), construct a layered digraph Lv ()
with || + 1 node layers L0 , . . . , L|| , where L0 consists of only I[v], and for 1  i  ||,
[i]

Li consists of all nodes y  D(v) for which a path v (x, y) from some node x  Li1
has been constructed in step (1). For each x  Li1 , y  Li , Lv () contains an arc
[i]
(x, y) weighted with cost(v (x, y)).

(3) For each   [(r)], let k = ||. A candidate plan  for  is constructed as follows.

(a) For each v  succ(r), find a cost-minimal path from I[v] to G[v] in Lv (). If no such
path exists, then proceed with the next prefix in [(r)]. Otherwise, note that the
i-th edge on this path (taking us from some x  Li1 to some y  Li ) corresponds
[i]
to the cost-minimal path v (x, y) from x to y. Let us denote this path from x to
y by Svi .

(b) Set R = R{ }, where  = S 1 a[2] S 2 . . .a[k] S k , each sequence S i is obtained
by an arbitrary merge of the sequences {Svi }vsucc(r) , and a is the cheapest action
changing the value of r to value .
(4) If R = , then fail, otherwise return  = argmin R cost( ).
It is straightforward to verify that the complexity of the above procedure is polynomial
in the description size of . To prove correctness, we show that the procedure returns a
plan for any solvable task , and that the returned plan 0 satisfies cost(0 )  cost() for
any optimal plan  for .
67

fiKatz & Domshlak

Given a solvable task , let  be an optimal plan for  with all v for the leaf variables
v being cycle-free. Let r = ha2 . . . , ak i; the numbering of actions along r starts with
a2 to simplify indexing later on. For each v  succ(r), the actions of r divide v into
subsequences of v-changing actions v = 1v  . . .  kv , separated by the value changes
required from r. That is, for each 1  i  k, all actions in iv are preconditioned by the
same value of r, if any, and if two actions a  iv and a0  i+1
are preconditioned by r, then
v
pre(a)[r] 6= pre(a0 )[r]. Let   [(r)] be a value sequence such that || = k = |r | + 1. For
each v  succ(r), v is a path from I[v] to G[v] in Lv (), and therefore some  is added
into R by the algorithm, meaning that the algorithm finds a solution. Now, if   R,
then, for each v  succ(r), let Sv1  Sv2  . . .  Svk be a cost-minimal path from I[v] to G[v] in
Lv () such that Svi is the sequence of actions changing the value of v and preconditioned
either by r : 0 or nothing for odd i, and by r : 1 or nothing for even i. Thus,
cost(Sv1  Sv2  . . .  Svk ) =

k
X
i=1

cost(Svi )  cost(v ).

Because sequence S i is obtained by an arbitrary merge of the sequences {Svi }vsucc(r) , and
a is the cheapest action changing the value of r to , then  = S 1  a[2]  S 2  . . .  a[k]  S k
is an applicable sequence of actions that achieves the goal values for each v  succ(r) as
well as for r, and
cost( ) = cost(S 1  a[2]  S 2  . . .  a[k]  S k ) =

k
X

cost(a[i] ) +

i=2

cost(r ) +

k
X
i=1

X

cost(S i ) 

cost(v ) = cost().

vsucc(r)

Hence, if  is solvable, then the algorithm returns a plan for , and this plan must be
optimal. Finally, if  is not solvable, then R necessarily remains empty, and thus the
algorithm fails.

While Theorem 4 concerns the tractability tasks with fork-structured causal graphs and
roots with binary domains, in our earlier work we also reported an additional tractability
result for fork-structured causal graphs with the domains of all variables being of a fixed
size, though not necessarily binary-valued (Katz & Domshlak, 2008). We do not discuss
this result here in detail because, at least so far, we have not found it very helpful in the
context of devising effective abstraction heuristics.
Theorem 5 (Tractable Inverted Forks) Given a planning task  = hV, A, I, G, costi
with an inverted fork causal graph with sink r  V , if |D(r)| = O(1), the time complexity of
the cost-optimal planning for  is polynomial in ||||.
Proof: Let |D(r)| = d. Observe that the inverted-fork structure of the causal graph CG()
implies all the actions in  are unary-effect, and that the sink r preconditions only the
actions affecting r itself. Hence, in what follows we assume that G[r] is specified; otherwise
68

fiImplicit Abstraction Heuristics

Given a path ha1 , . . . , am i from I[r] to G[r] in DTG(r, ):
 := hi
am+1 := hG[pred(r)], i
foreach v  pred(r) do xv := I[v]
for i := 1 to m + 1 do
foreach v  pred(r) do
if pre(ai )[v] is specified and pre(ai )[v] 6= xv then
if pre(ai )[v] is not reachable from xv in DTG(v, ) then fail
append to  the actions induced by some cost-minimal path
from pre(ai )[v] to xv in DTG(v, )
xv := pre(ai )[v]
if i < m + 1 then append to  the action ai
return 
Figure 4: Detailed outline of step (3) of the planning algorithm for inverted-fork structured
task.

 breaks down to a set of trivial planning problems over a single variable each. Likewise,
from the above properties of  it follows that, if  is an optimal plan for , then the path
r from I[r] to G[r] induced by  in DTG(r, ) is either cycle-free or contains only zerocost cycles. The latter can be safely eliminated from , and thus we can assume that r
is cycle-free. Given that, a simple algorithm that finds a cost-optimal plan for  in time
(||||d + ||||3 ) is as follows.
(1) Create all (|Ar |d1 ) cycle-free paths from I[r] to G[r] in DTG(r, ).
(2) For each variable v  pred(r), and each pair of vs values x, y  D(v), compute the
cost-minimal path from x to y in DTG(v, ). The whole set of such cost-minimal paths
can be computed using (d|V |) applications of the Floyd-Warshall algorithm on the
domain transition graphs of the sinks parents pred(r).
(3) For each I[r]-to-G[r] path in DTG(r, ) generated in step (1), construct a plan for
 based on that path for r, and the cheapest paths computed in (2). This simple
construction, depicted in Figure 4, is possible because the values of each parent variable
can be changed independently of the values of all other variables in the inverted fork.
(4) Take the cheapest plan among those constructed in (3). If no plan was constructed in
step (3), then  is unsolvable.
We have already observed that, for each cost-optimal plan , r is one of the I[r]-to-G[r]
paths generated in step (1). For each v  pred(r), let Sv denote the sequence of values from
D(v) that is required by the preconditions of the actions along r . For each v  pred(r), we
have v corresponding to a (possibly cyclic) path from I[v] to G[v] in DTG(v, ), traversing
the values (= nodes) from Sv in the order required by Sv . In turn, the plan for  generated
in (3) consists of cost-minimal such paths for all v  pred(r). Therefore, at least one of the
69

fiKatz & Domshlak

plans generated in (3) must be cost-optimal for , and the minimization step (4) will select
one of them.

Theorems 4 and 5 clarify the gap between fork decompositions and implicit abstraction
heuristics, and now we can bridge this gap by further abstracting each task in the given fork
decomposition of . We do that by abstracting domains of the fork roots and inverted-fork
sinks to meet the requirements of the tractable fragments. We note that, in itself, the idea
of domain decomposition is not very new in general (Hernadvolgyi & Holte, 1999) and in
domain-independent planning in particular (Domshlak, Hoffmann, & Sabharwal, 2009). In
fact, the shrinking step of the algorithm for building the merge-and-shrink abstractions
is precisely a variable domain abstraction for meta-variables constructed in the merging
steps (Helmert et al., 2007).
Definition 10 Let  = hV, A, I, G, costi be a planning task over states S, v  V be a state
variable, and  = {1 , . . . , m } be a set of mappings from D(v) to some sets 1 , . . . , m ,
respectively. A = {hi , i i}m
i=1 is a domain abstraction of  over  if, for some set of
cost functions {costi : A  R0+ }m
i=1 satisfying
a  A :

m
X
i=1

costi (a)  cost(a),

(8)

we have, for 1  i  m,
 the abstraction mapping i of states S is
u  V :

(
i (s[u]),
i (s)[u] =
s[u],

u=v
,
u 6= v

and, extending i to partial assignments on V 0  V as i (s[V 0 ]) = i (s)[V 0 ],
 i = hV, Ai , Ii , Gi , costi i is a planning task with
1. Ii = i (I), Gi = i (G),
2. Ai = {ai = hi (pre(a)), i (eff(a))i | a  A}, and

3. for each action a  A,

costi (ai ) = costi (a).

(9)

We say that i is a domain decomposition of i = hV, A, I, G, costi i with respect to i .
Theorem 6 Domain abstractions of the planning tasks are additive implicit abstractions
of these tasks.
Proof: Let  = hV, A, I, G, costi be a planning task and A = {hi , i i}m
i=1 be a domain
abstraction of  over  = {1 , . . . , m }. Let T = (S, L, Tr, s0 , S ? , $) be the transition
graph of . For each 1  i  m, let Ti = (Si , Li , Tri , s0i , Si? , $i ) be the transition graph of
i . We need to show that i is an abstraction mapping as in Definition 2.
First, from Definition 10 we have
70

fiImplicit Abstraction Heuristics

 s0i = Ii = i (I) = i (s0 ), and
 for all s  S ? we have s  G and thus i (s)  i (G) = Gi , providing us with
i (s)  Si? .
Now, if s is a state of  and a  A is an action with pre(a)  s, then i (s) is a state of i
and pre(ai ) = i (pre(a))  i (s). Thus, ai is applicable in i (s), and now we show that
applying ai in i (s) results in i (s)Jai K = i (sJaK).
1. For the effect variables v  V(eff(a)) = V(eff(ai )), we have eff(ai )  i (s)Jai K and
eff(ai ) = i (eff(a))  i (sJaK).
2. For all other variables v 6 V(eff(a)), we have sJaK[v] = s[v] and i (s)Jai K[v] =
i (s)[v], and thus
i (s)Jai K[v] = i (s)[v] = i (s[v]) = i (sJaK[v]) = i (sJaK)[v].
Next, for each a  A, from Eqs. 8 and 9 we have
m
X

costi (ai ) =

i=1

m
X
i=1

costi (a)  cost(a).

(10)

Now, let s, s0  S be a pair of states such that cost(s, s0 )  , and let % = ha1 , . . . , al i be the
sequence
of labels along a cheapest path from s to s0 in T. From that, cost(s, s0 ) = cost(%) =
Pl
j
j=1 cost(a ). The decomposition of such a path to the actions as in Definition 10 is a
(not
cheapest) path from i (s) to i (s0 ) in Ti , and thus cost(i (s), i (s0 )) 
Pl neccesarily
j
j=1 costi (a ), providing us with
m
X
i=1

cost(i (s), i (s0 )) 

m X
l
X
i=1 j=1

costi (aji ) =

l X
m
X
j=1 i=1

(10)

costi (aji ) 

l
X

cost(aj ) = cost(s, s0 ).

j=1


Having put the notion of domain abstraction in the framework of implicit abstractions,
we are now ready to connect fork decompositions and implicit abstraction heuristics. Given
a FI-abstraction AFI = {hfv , vf i, hiv , vi i}vV of a planning task  = hV, A, I, G, costi,
 for each fv  FI , we associate the root v of CG(fv ) with mappings fv = {fv,1 , . . . , fv,kv }
such that kv = O(poly(||||)) and all fv,i : D(v)  {0, 1}, and then abstract fv with
f i}kv , and
Afv = {hfv,i , v,i
i=1
 for each iv  FI , we associate the sink v of CG(iv ) with mappings iv = {iv,1 , . . . , iv,kv0 }
such that kv0 = O(poly(||||)) and all iv,i : D(v)  {0, 1, . . . , bv,i }, bv,i = O(1), and
k0

i i} v .
then abstract iv with Aiv = {hiv,i , v,i
i=1

71

fiKatz & Domshlak

From Theorem 3, Theorem 6, and the composition Theorem 2, we then immediately have


kv0
kv
[ [
[
f
i
 {hfv,i , v,i
AFI =
 vf i}  {hiv,i , v,i
 vi i}
(11)
vV

i=1

i=1

being an additive implicit abstraction of . Hence, from Theorem 1,


kv0
kv
X X
X

hFI =
hi 
hf +
vV

v,i

i=1

i=1

v,i

(12)

is an admissible estimate of h for , and, from Theorems 4 and 5, hFI is also computable
in time O(poly(||||)).
This finalizes our construction of a concrete family of implicit abstraction heuristics. To
illustrate the mixture of acyclic causal-graph and domain abstractions as above, we again
use our running Logistics example. One bothersome question is to what extent further
abstracting fork decompositions using domain abstractions affects the informativeness of
the heuristic estimate. Though generally a degradation here is unavoidable, below we show
that the answer to this question can sometimes be somewhat surprising.
To begin with an extreme setting, let all the domain abstractions for roots of forks and
sinks of inverted forks be to binary-valued domains. Among multiple options for choosing the mapping sets {fv } and {iv }, here we use a simple choice of distinguishing between different values of each variable v on the basis of their cost from I[v] in DTG(v, ).
Specifically, for each v  V , we set fv = iv , and, for each value   D(v) and each
1  i  max0 D(v) d(I[v], 0 ),
(
0, d(I[v], ) < i
fv,i () = iv,i () =
(13)
1, otherwise
For example, the problem fc1 is decomposed (see the domain transition graph of c1
on the left in Figure 1c) into two problems, fc1 ,1 and fc1 ,2 , with the binary abstract
domains of c1 corresponding to the partitions {{A}, {B, C, D}} and {{A, D}, {B, C}} of
D(c1 ), respectively. As yet another example, the problem ip1 is decomposed (see the
domain transition graph of p1 in Figure 1d) into six problems ip1 ,1 , . . . , ip1 ,6 along the
abstractions of D(p1 ) depicted in Figure 5a. Now, given the FI-decomposition of  and
mappings {fv , iv }vV as above, consider the problem ip1 ,1 , obtained from abstracting 
along the inverted fork of p1 and then abstracting D(p1 ) using
(
0,   {C}
ip1 ,1 () =
.
1,   {A, B, D, E, F, G, c1 , c2 , c3 , t}
It is not hard to verify that, from the original actions affecting p1 , we are left in ip1 ,1 with
only actions conditioned by c1 and c2 . If so, then no information is lost3 if we remove
from ip1 ,1 both variables c3 and t, as well as the actions changing (only) these variables,
3. No information is lost here because we still keep either fork or inverted fork for each variable of .

72

fiImplicit Abstraction Heuristics

in t

in c

at A

at B

at C

at D

at E

in c

at F

at G

in c

(a)
in t

in c

at A

at B

at C

at D

in t

in c

at E

in c

at F

at G

at A

in c

at B

at C

at D

at E

in c

D(p1 ) in fp1 ,1

in t

in c

at F

at A

at G

at B

at C

at D

at E

in c

in c

D(p1 ) in fp1 ,2

at F

at G

in c

D(p1 ) in fp1 ,3

(b)
Figure 5: Domain abstractions for D(p1 ). (a) Binary-valued domain abstractions: the values inside and outside each dashed contour are mapped to 0 and 1, respectively.
(b) Ternary-valued domain abstractions: values that are mapped to the same
abstract value are shown as nodes with the same color and borderline.

and redistribute the cost of the removed actions between all other representatives of their
originals in . The latter revision of the action cost partition can be obtained directly by
replacing the cost-partitioning steps corresponding to Eqs. 3-4 and 8-9 by a single, joint
action cost partitioning applied over the final additive implicit abstraction AFI as in Eq. 11
and satisfying

kv
X X
cost(a) 

vV



0

X

costfv,i (fv,i (a0 )) +

i=1 a0 A f (a)
G

kv
X

X


costiv,i (iv,i (a0 )) .

(14)

i=1 a0 A i (a)
G
v

v

In what follows, by uniform action cost partition we refer to a partition in which the cost of
each action is equally split among all its nonredundant representatives in the final additive
implicit abstraction.
Overall, computing hFI as in Eq. 12 under our all binary-valued domain abstractions
7
and such a uniform action cost partition provides us with hFI (I) = 12 15
, and knowing that
FI
the original costs are all integers we can safely adjust it to h (I) = 13. Hence, even under
the most severe domain abstractions as above, the estimate of hFI in our example task is
not lower than that of h2 .
Let us now slightly refine our domain abstractions for the sinks of the inverted forks to
be to a ternary range {0, 1, 2}. While mappings {fv } remain unchanged, {iv } are set to
73

fiKatz & Domshlak



0, d(I[v], ) < 2i  1
i
  D(v) : v,i () = 1, d(I[v], ) = 2i  1


2, d(I[v], ) > 2i  1

.

(15)

For example, the problem ip1 is now decomposed into ip1 ,1 , . . . , ip1 ,3 along the abstractions
of D(p1 ) depicted in Figure 5b. Applying now the same computation of hFI as in Eq. 12
over the new set of domain abstractions gives hFI (I) = 15 12 , which, again, can be safely
adjusted to hFI (I) = 16. Note that this value is higher than hFI = 15 obtained using the
(generally intractable) pure fork-decomposition abstractions as in Eq. 6. At first view,
this outcome may seem counterintuitive as the domain abstractions are applied over the fork
decomposition, and one would expect a coarser abstraction to provide less precise estimates.
This, however, is not necessarily the case when the employed action cost partition is ad hoc.
For instance, domain abstraction for the sink of an inverted fork may create independence
between the sink and its parent variables, and exploiting such domain-abstraction specific
independence relations leads to more targeted action cost partition via Eq. 14.
To see why this surprising estimate improvement has been obtained, note that before
the domain abstraction in Eq. 15 is applied on our example, the truck-moving actions
drive-t-from-D-to-E and drive-t-from-E-to-D appear in three abstractions ft , ip1 and ip2 ,
while after domain abstraction they appear in five abstractions ft,1 , ip1 ,1 , ip1 ,2 , ip1 ,3 and
ip2 ,1 . However, a closer look at the action sets of these five abstractions reveals that the
dependencies of p1 in CG(ip1 ,1 ) and CG(ip1 ,3 ), and of p2 in CG(ip2 ,1 ) on t are redundant,
and thus keeping representatives of move-D-E and move-E-D in the corresponding abstract
tasks is entirely unnecessary. Hence, after all, the two truck-moving actions appear only in
two post-domain-abstraction tasks. Moreover, in both these abstractions the truck-moving
actions are fully counted, in contrast to the predomain-abstraction tasks where the portion
of the cost of these actions allocated to ip2 simply gets lost.

6. Experimental Evaluation: Take I
To evaluate the practical attractiveness of the fork-decomposition heuristics, we have conducted an empirical study on a wide sample of planning domains from the International
Planning Competitions (IPC) 1998-2006, plus a non-IPC Schedule-STRIPS domain.4
The domains were selected to allow a comparative evaluation with other, both baseline and
state-of-the-art, approaches/planners, not all of which supported all the PDDL features at
the time of our evaluation.
Later we formally prove that, under ad hoc action cost partitions such as our uniform
partition, none of the three fork decompositions as in Definition 9 is dominated by the
other two. Hence, we have implemented three additive fork-decomposition heuristics, hF ,
hI , and hFI , within the standard heuristic forward search framework of the Fast Downward
planner (Helmert, 2006) using the A algorithm with full duplicate elimination. The hF
heuristic corresponds to the ensemble of all (not clearly redundant) fork subgraphs of the
4. Schedule-STRIPS appears in the domains distribution of IPC-2000. Later we became aware of the
fact that this domain was excluded from the competition because its encoding generated problems for
various planners.

74

fiImplicit Abstraction Heuristics

domain

S

airport-ipc4
blocks-ipc2
depots-ipc3
driverlog-ipc3
freecell-ipc3
grid-ipc1
gripper-ipc1
logistics-ipc1
logistics-ipc2
miconic-strips-ipc2
mprime-ipc1
mystery-ipc1
openstacks-ipc5
pathways-ipc5
pipes-notank-ipc4
pipes-tank-ipc4
psr-small-ipc4
rovers-ipc5
satellite-ipc4
schedule-strips
tpp-ipc5
trucks-ipc5
zenotravel-ipc3

21
30
7
12
5
2
20
6
22
85
24
21
7
4
21
14
50
7
6
43
6
9
11

total

433

hF
s
11
17
2
9
3
1
5
3
21
45
17
16
7
4
9
6
47
5
6
42
5
5
8

%S
52
57
29
75
60
50
25
50
95
53
71
76
100
100
43
43
94
71
100
98
83
56
73
294

hI
s
14
15
2
10
2
1
5
2
15
42
17
15
7
4
11
6
48
6
6
35
5
5
9

%S
67
50
29
83
40
50
25
33
68
49
71
71
100
100
52
43
96
86
100
81
83
56
82
282

hFI
s
11
15
2
9
2
1
5
2
14
40
17
16
7
4
8
6
47
6
5
39
5
5
8

%S
52
50
29
75
40
50
25
33
64
47
71
76
100
100
38
43
94
86
83
91
83
56
73

MS -104

MS -105

s
19
18
7
12
5
2
7
4
16
54
21
17
7
3
20
13
50
6
6
22
6
6
11

s
17
20
4
12
1
2
7
5
21
55
12
13
7
4
12
7
50
7
6
1
6
5
11

274

%S
90
60
100
100
100
100
35
67
73
64
88
81
100
75
95
93
100
86
100
51
100
67
100
332

%S
81
67
57
100
20
100
35
83
95
65
50
62
100
100
57
50
100
100
100
2
100
56
100
285

HSPF

s
15
30
4
9
5
0
6
3
16
45
8
11
7
4
13
7
50
6
5
11
5
9
8

%S
71
100
57
75
100
0
30
50
73
53
33
52
100
100
62
50
100
86
83
26
83
100
73

277

Gamer
s
11
30
4
11
2
2
20
6
20
85
9
8
7
4
11
6
47
5
6
3
5
3
10

%S
52
100
57
92
40
100
100
100
91
100
38
38
100
100
52
43
94
71
100
7
83
33
91
315

blind
s
18
18
4
7
4
1
7
2
10
50
19
18
7
4
14
10
48
5
4
29
5
5
7

%S
86
60
57
58
80
50
35
33
45
59
79
86
100
100
67
71
96
71
67
67
83
56
64
296

hmax
s
20
18
4
8
5
2
7
2
10
50
24
18
7
4
17
10
49
6
5
31
6
7
8

%S
95
60
57
67
100
100
35
33
45
59
100
86
100
100
81
71
98
86
83
72
100
78
73
318

Table 1: A summary of the experimental results. Per domain, S denotes the number of
tasks solved by any planner. Per planner/domain, the number of tasks solved by
that planner is given both by the absolute number (s) and by the percentage from
solved by some planners (%S). The last row summarize the number of solved
instances.

causal graph, with the domains of the roots being abstracted using the leave-one-value-out
binary-valued domain decompositions as follows:
(
0,  = i
i  D(v) : fv,i () =
.
(16)
1, otherwise
The hI heuristic is the same but for the inverted fork subgraphs, with the domains of the
sinks being abstracted using the distance-to-goal-value ternary-valued domain decompositions5 as in Eq. 17.


0, d(, G[v]) < 2i  1
i
  D(v) : v,i () = 1, d(, G[v]) = 2i  1 .
(17)


2, d(, G[v]) > 2i  1
The ensemble of the hFI heuristic is the union of these for hF and hI . The action cost
partition in all three heuristics was what we call uniform.
We make a comparison with two baseline approaches, namely blind A  with heuristic
value 0 for goal states and 1 otherwise, and A with the hmax heuristic (Bonet & Geffner,
2001), as well as with state-of-the-art abstraction heuristics, represented by the mergeand-shrink abstractions of Helmert et al. (2007). The latter were constructed under the
5. While distance-from-initial-value is reasonable for the evaluation of just the initial state, leave-onevalue-out for fork roots and distance-to-goal-value for inverted-fork sinks should typically be much
more attractive for the evaluation of all the states examined by A .

75

fiKatz & Domshlak

linear, f -preserving abstraction strategy proposed by these authors, and this under two
fixed bounds on the size of the abstract state spaces, notably |S  | < 104 and |S  | < 105 .
These four (baseline and merge-and-shrink) heuristics were implemented by Helmert et al.
(2007) within the same planning system as our fork-decomposition heuristics, allowing for
a fairly unbiased comparison. We also compare to the Gamer (Edelkamp & Kissmann,
2009) and HSPF (Haslum, 2008) planners, the winner and the runner-up at the sequential
optimization track of IPC-2008. On the algorithmic side, Gamer is based on a bidirectional
blind search using sophisticated symbolic-search techniques, and HSPF uses A with an
additive critical-path heuristic. The experiments were conducted on a 3GHz Intel E8400
CPU with 2 GB memory, using 1.5 GB memory limit and 30 minute timeout. The only
exception was Gamer, for which we used similar machines but with 4 GB memory and 2
GB memory limit; this was done to provide Gamer with the environment for which it was
configured.
Table 1 summarizes our experimental results in terms of the number of tasks solved by
each planner. Our impression of fork-decomposition heuristics from Table 1 is somewhat
mixed. On the one hand, the performance of all three fork-decomposition based planners
was comparable to one of the settings of the merge-and-shrink heuristic, and this clearly
testifies for that the framework of implicit abstractions is not of theoretical interest only.
On the other hand, all the planners, except for A with the merge-and-shrink heuristic with
|S  | < 104 , failed to outperform A with the baseline hmax heuristic. More important for
us is that, unfortunately, all three fork-decomposition based planners failed to outperform
even the basic blind search.
This, however, is not the end of the story for the fork-decomposition heuristics. Some
hope can be found in the detailed results in Tables 9-14 in the appendix. As it appears from
Table 10, on, e.g., the Logistics-ipc2 domain, hF almost consistently leads to expanding
fewer search nodes than the (better between the two merge-and-shrink heuristics on this
domain) MS -105 , with the difference hitting four orders of magnitude. However, the time
complexity of hF per search node is substantially higher than that of MS -105 , with the
two expanding at a rate of approximately 40 and 100000 nodes per second, respectively.
The outcome is simple: while with no time limits (and only memory limit of 1.5 GB) hF
solves more tasks in Logistics-ipc2 than MS -105 (task 12-1 is solved with hF in 2519.01
seconds), this is not so with a standard time limit of half an hour used for Table 10. In what
follows we examine the possibility of exploiting the informativeness of fork-decomposition
heuristics while not falling into the trap of costly per-node heuristic evaluation.

7. Back to Theory: h-Partitions and Databased Implicit Abstraction
Accuracy and low time complexity are both desired yet competing properties of heuristic
functions. For many powerful heuristics, and abstraction heuristics in particular, computing
h(s) for each state s in isolation is impractical: while computing h(s) is polynomial in the
description size of , it is often not efficient enough to be performed at each search node.
However, for some costly heuristics this obstacle can be largely overcome by sharing most
of the computation between the evaluations of h on different states. If that is possible,
the shared parts of computing h for all problem states are precomputed and memorized
before the search, and then reused during the search by the evaluations of h on different
76

fiImplicit Abstraction Heuristics

states. Such a mixed offline/online heuristic computation is henceforth called h-partition,
and we define the time complexity of an h-partition as the complexity of computing h
for a set of states. Given a subset of k problem states S 0  S, the h-partitions time
complexity of computing {h(s) | s  S 0 } is expressed as O(X + kY ), where O(X) and O(Y )
are, respectively, the complexity of the (offline) pre-search and (online) per-node parts of
computing h(s).
These days h-partitions are being adopted by various optimal planners using criticalpath heuristics hm for m > 1 (Haslum et al., 2005), landmark heuristics hL and hLA (Karpas
& Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp,
2001; Helmert et al., 2007). Without effective h-partitions, optimal search with these
heuristics would not scale up well, while with such h-partitions it constitutes the state of the
art of cost-optimal planning. For instance, a very attractive property of PDB abstractions
is the complexity of their natural h -partition. Instead of computing h (s) = h ((s)) from
scratch for each evaluated state s (impractical for all but tiny projections), the practice is
to precompute and store h (s0 ) for all abstract states s0  S , after which the per-node
computation of h (s) boils down to a hash-table lookup for h ((s)) with a perfect hash
function. In our terms, the time and space complexity of that PDB h -partition for a set
of k states is O(|S  |(log(|S  |) + |A|) + k) and O(|S  |), respectively. This is precisely what
makes PDB heuristics so attractive in practice. In that respect, the picture with mergeand-shrink abstractions is very much similar. While the order in which composites are
formed and the choice of abstract states to contract are crucial to the complexity of their
natural h -partitions, the time and space complexity for the concrete linear abstraction
strategy of Helmert et al. are respectively O(|V ||S  |(log(|S  |) + |A|) + k  |V |) and O(|S  |).
Similarly to PDB abstractions, the per-node computation of h (s) with a merge-and-shrink
abstraction  is just a lookup in a data structure storing h ((s)) for all abstract states
(s)  S . Hence, while the pre-search computation with MS abstractions can be more
costly than with PDBs, the online part of computing heuristic values is still extremely
efficient. This per-node efficiency provides the merge-and-shrink heuristics with impressive
practical effectiveness on numerous IPC domains (Helmert et al., 2007).
To sum up, we can say that the fixed size of abstract spaces induced by explicit abstractions such as PDBs and merge-and-shrink is not only a limitation but also a key to obtaining
effective h-partitions. In contrast, escaping that limitation with implicit abstractions might
trap us into having to pay a high price for each search-node evaluation. We now show, however, that the time-per-node complexity bottleneck of fork-decomposition heuristics can
be successfully overcome. Specifically, we show that an equivalent of PDBs and mergeand-shrink notion of database exists for fork-decomposition abstractions as well, despite
their exponential-size abstract spaces. Of course, unlike with PDB and merge-and-shrink
abstractions, the databased fork-decomposition heuristics do not (and cannot) provide us
with a purely lookup online computation of h (s). The online part of the h -partition has
to be nontrivial in the sense that its complexity cannot be O(1). In what comes next we
prove the existence of such effective h-partitions for fork and inverted fork abstractions.
In Section 8 we then empirically show that these h-partitions lead to fast pre-search and
per-node computations, allowing the informativeness of the fork-decomposition heuristics
to be successfully exploited in practice.

77

fiKatz & Domshlak

Theorem 7 Let  = hV, A, I, G, costi be a planning task with a fork causal graph rooted at a
binary-valued variable r. There exists an h -partition for  such that, for any set of k states,
the time and space complexity of that h -partition is, respectively, O(d3 |V | + |Ar | + kd|V |)
and O(d2 |V |), where d = maxv D(v).
Proof: The proof is by a modification of the polynomial-time algorithm for computing
h (s) for a state s of such a task  used in the proof of Theorem 4 (Tractable Forks). Given
a state s, let D(r) = {0, 1}, where s[r] = 0. In what follows, for each of the two roots
values   D(r),  denotes the opposite value 1  ; (r), [(r)], DTG 0v and DTG 1v are
defined exactly as in the proof of Theorem 4.
(1) For each of the two values r  D(r) of the root variable, each leaf variable v  V \ {r},
and each pair of values , 0  D(v), let p,0 ;r be the cost of the cheapest sequence of
actions changing v from  to 0 provided r : r . The whole set {p,0 ;r } for all the leaves
v  V \{r} can be computed by a straightforward variant of the all-pairs-shortest-paths,
Floyd-Warshall algorithm on DTG v r in time O(d3 |V |).
(2) For each leaf variable v  V \ {r}, 1  i  d + 1, and   D(v), let g;i be the cost of
the cheapest sequence of actions changing s[v] to  provided a sequence   [(r)],
|| = i, of value changes of r. Having the values {p,0 ;r } from step (1), the set {g;i }
is given by the solution of the recursive equation


ps[v],;s[r] ,
i=1






min g0 ;i1 + p0 ,;s[r] ,
1 < i   , i is odd
0 

g;i =
,

0 ;i1 + p0 ,;s[r] ,
min
g
1
<
i


,
i
is
even



 0


g
 < i  d + 1
;i1 ,
where  = |D(v)| + 1. Given that, we have

h (s) =

min

 cost() +

[(r)]


X

gG[v];||  ,

vV \{r}

P||
with cost() = i=2 cost(a[i] ), where a[i]  A is the cheapest action changing the
value of r from [i  1] to [i].
Note that step (1) is already state-independent, but the heavy step (2) is not. However,
the state dependence of step (2) can mostly be overcome as follows. For each v  V \ {r},
  D(v), 1  i  d + 1, and r  D(r), let g;i (r ) be the cost of the cheapest sequence of
actions changing  to G[v] provided the value changes of r induce a 0/1 sequence of length
i starting with r . The set {g;i (r )} is given by the solution of the recursive equation


,
i=1

p,G[v];
 r

g0 ;i1 (r ) + p,0 ;r , 1 < i  
,
g;i (r ) = min
0


g
 < i  d + 1
;i1 (r ),
78

(18)

fiImplicit Abstraction Heuristics

24

0

1

r ||

24

0
1

1
1

1

0
100

1
0

1 1
1
50

1

2

3
0
100

3
0

0

1

0
100

0

0

2

1

1 1
1
50

1

5
0

1

1

4

1
2
3
4
5
6
7
1
2
3
4
5
6
7

cost()

v :0

v :1 v :2 v :3

u:0

u:1 u:2 u:3 u:4 u:5

0
24
48
72
96
120
144
0
24
48
72
96
120
144

100
100
100
100
3
3
3

100
3
3
3
3
3


2
2
2
2
2
2


2
2
2
2
2

201
201
53
53
5
5
5

101
101
53
53
5
5

200 101 100
200 101 100
102 3
2
102 3
2
4
3
2
4
3
2
4
3
2
  
52 51
2
52 51
2
4
3
2
4
3
2
4
3
2
4
3
2


1
1
1
1
1
1
1
1
1
1
1
1
1

0
0
0
0
0
0
0
0
0
0
0
0
0
0

(a)

1
1
1
1
1
1
1

1
1
1
1
1
1

0
0
0
0
0
0
0
0
0
0
0
0
0
0

(b)

Figure 6: The database for a fork-structured problem with a binary-valued root variable r
and two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5. (a) depicts the
domain transition graphs of r (top), v (middle), and u (bottom); the numbers
above and below each edge are the precondition on r and the cost of the respective
action. (b) depicts the database created by the algorithm. For instance, the entry
in row r : 0  || = 5 and column v : 0 captures the value of gv:0;5 (r : 0) computed as
in Eq. 18. The shaded entries are those examined during the online computation
of h (r : 0, v : 0, u : 0).
which can be solved in time O(d3 |V |). Note that this equation is now independent of the
evaluated state s, and yet {g;i (r )} allow for computing h (s) for a given state s via

h (s) =

min



 cost() +

[(r|s[r])]

X

gs[v];|| (s[r]) 

(19)

vV \{r}

where (r|r ) is defined similarly to (r) but with respect to the initial value r of r.
With the new formulation, the only computation that has to be performed online, per
search node, is the final minimization over [(r|s[r])] in Eq. 19, and this is the lightest
part of the whole algorithm anyway. The major computations, notably those of {p,0 ;r }
and {g;i (r )}, can now be performed offline and shared between the evaluated states. The
space required to store this information is O(d2 |V |) as it contains only a fixed amount of
information per pair of values of each variable. The time complexity of the offline computation is O(d3 |V | + |Ar |); the |Ar | component stems from precomputing the costs cost().
The time complexity of the online computation per state is O(d|V |); |V | comes from the
internal summation and d comes from the size of [(r|s[r])].

Figure 6b shows the database created for a fork-structured problem with a binary-valued
root r, two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5; the domain transition
79

fiKatz & Domshlak

graphs of v and u are depicted in Figure 6(a). Online computation of h (s) as in Eq. 19
for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each of the four rows having such
entries, and minimizes over the resulting four sums, with the minimum being obtained in
the row r : 0  || = 5.
Theorem 8 Let  = hV, A, I, G, costi be a planning task with an inverted fork causal graph
with sink r and |D(r)| = b = O(1). There exists an h -partition for  such that, for any set
of k states, the time and space complexity of that h -partition is O(b|V ||Ar |b1 + d3 |V | +
k|V ||Ar |b1 ) and O(|V ||Ar |b1 + d2 |V |), respectively, where d = maxv D(v).
Proof: Like the proof of Theorem 7, the proof of Theorem 8 is based on a modification
of the polynomial-time algorithm for computing h (s) used for the proof of Theorem 5
(Tractable Inverted Forks).
(1) For each parent variable v  V \ {r}, and each pair of its values , 0  D(v), let p,0
be the cost of the cheapest sequence of actions changing  to 0 . The whole set {p,0 }
can be computed using the Floyd-Warshall algorithm on the domain transition graph
of v in time O(d3 |V |).
(2) Given a state s, for each cycle-free path  = ha1 , . . . , am i from s[r] to G[r] in DTG(v, ),
let g be the cost of the cheapest plan from s in  based on , and the cheapest paths
{p,0 } computed in step (1). Each g can be computed as
g =

m
X
i=1

cost(ai ) +

m
X
X

pprei [v],prei+1 [v] ,

i=0 vV \{r}

where pre0 , . . . , prem+1 are the values required from the parents of r along the path .
That is, for each v  V \ {r}, and 0  i  m + 1,


s[v],
i=0



G[v],
i = m + 1, and G[v] is specified
prei [v] =
.

pre(a
)[v],
1

i

m,
and
pre(a
)[v]
is
specified
i
i



pre [v]
otherwise
i1
From that, we have h (s) = min g .
Note that step (1) is state-independent, but step (2) is not. However, the dependence
of step (2) on the evaluated state can be substantially relaxed. As there are only O(1)
different values of r, it is possible to consider cycle-free paths to G[r] from all values of r.
For each such path , and each parent variable v  V \ {r}, we know what the first value of
v required by  would be. Given that, we can precompute the cost-optimal plans induced
by each  assuming the parents start at their first required values. The remainder of the
computation of h (s) is delegated to online, and the modified step (2) is as follows.
For each r  D(r) and each cycle-free path  = ha1 , . . . , am i from r to G[r] in
DTG(r, ), let a proxy state s be


v=r
r ,
s [v] = G[v],
1  i  m : pre(ai )[v] is unspecified ,


pre(ai )[v], i = argminj {pre(aj )[v] is specified}
80

fiImplicit Abstraction Heuristics

that is, the nontrivial part of s captures the first values of V \ {r} required along .6 Given
that, let g be the cost of the cheapest plan from s in  based on , and the cheapest
paths {p,0 } computed in (1). Each g can be computed as
g =

m
X





cost(ai ) +

i=1

X

pprei [v],prei+1 [v]  ,

vV \{r}

where, for each v  V \ {r}, and 1  i  m + 1,


s [v],
i=1



G[v],
i = m + 1, and G[v] is specified
prei [v] =
.

pre(ai )[v], 2  i  m, and pre(ai )[v] is specified



pre [v], otherwise
i1
Storing the pairs (g , s ) accomplishes the offline part of the computation. Now, given a
search state s, we can compute

h (s) =

min

 s.t.
s [r]=s[r]



g +

X

ps[v],s [v] .

(20)

vV \{r}

The number of cycle-free paths to G[r] in DTG(r, ) is (|Ar |b1 ), and g for each
such path  can be computed in time O(b|V |). Hence, the overall offline time complexity is
O(b|V ||Ar |b1 + d3 |V |), and the space complexity (including the storage of the proxy states
s ) is O(|V ||Ar |b1 + d2 |V |). The time complexity of the online computation per state via
Eq. 20 is O(|V ||Ar |b1 ); |V | comes from the internal summation and |Ar |b1 from the upper
bound on the number of cycle-free paths from s[r] to G[r].

Figure 7(b) shows the database created for an inverted fork structured problem with a
ternary-valued sink variable r, two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2.
The domain transition graphs of u and v are depicted at the top of Figure 7(a); the actual
identities of actions affecting these two parents are not important here. The actions affecting
the sink r are
a1 = h{u : 1, r : 0}, {r : 1}i
a2 = h{v : 1, r : 0}, {r : 1}i

a3 = h{u : 2, r : 1}, {r : 2}i

a4 = h{v : 1, r : 1}, {r : 2}i.
The domain transition graph of r is depicted at the bottom of Figure 7(a). Online computation of h (s) as in Eq. 20 for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each
of the four rows having such entries, and minimizes over the resulting four sums, with the
minimum being obtained in the lowest such row.
81

fiKatz & Domshlak

0

2

50

0

50

50

2

1
1

1

100

1

u:2

u:1

1

1

0

1

2

v:1

v:1

50

100

r



ha1 , a3 i
0 ha1 , a4 i
ha2 , a3 i
ha2  a4 i
ha3 i
1
ha4 i

(a)

s

g

u:0

u:1 u:2

v :0

u : 1, v : 2
u : 1, v : 1
u : 2, v : 1
u : 0, v : 1
u : 2, v : 2
u : 0, v : 1

202
153
153
152
101
102

100
100
50
0
50
0

0
50
0
50
100 0
50 100
100 0
50 100

1
101
101
101
1
101

v :1 v :2
2
0
0
0
2
0

0
100
100
100
0
100

(b)

Figure 7: The database for an inverted fork-structured problem with a O(1) bounded sink
variable r and two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2.
(a) depicts the domain transition graphs of u (top left), v (top right), and r
(bottom); the numbers above and below each edge are the preconditions and the
cost of the respective action, respectively. (b) depicts the database created by the
algorithm. The shaded entries are those examined during the online computation
of h (r : 0, u : 0, v : 0).

8. Experimental Evaluation: Take II
To evaluate the practical attractiveness of the databased fork-decomposition heuristics, we
have repeated our empirical evaluation as in Section 6, but now for the databased versions
of the heuristics. The detailed results of this evaluation are relegated to Tables 15-20 in
the appendix, but they are summarized here in Table 2. For each domain, the S column
captures the number of tasks in that domain that were solved by at least one planner
in the suite. Per planner/domain, the number of tasks solved by that planner is given
both by the absolute number (s) and by the percentage from solved by some planners
(%S). Boldfaced results indicate the best performance within the corresponding domain.
The last three rows summarize the performance of the planners via three measures. The
first is the number of tasks solved in all the 23 domains; this is basically the performance
evaluation measure used in the optimization track at IPC-2008. As domains are not equally
challenging and do not equally discriminate between the planners performance, the second
is a domain-normalized performance measure
s(p) =

X
domain D

#tasks in D solved by planner p
.
#tasks in D solved by some planners

Finally, the third measure corresponds to the number of domains w in which the planner
in question solved at least as many tasks as any other planner.
Overall, Table 2 clearly suggests that heuristic search with databased fork-decomposition
heuristics favorably competes with the state of the art of optimal planning. In particular,
6. For ease of presentation, we omit here the case where v is required neither along , nor by the goal; such
variables should be simply ignored when accounting for the cost of .

82

fiImplicit Abstraction Heuristics

domain

S

airport-ipc4
blocks-ipc2
depots-ipc3
driverlog-ipc3
freecell-ipc3
grid-ipc1
gripper-ipc1
logistics-ipc2
logistics-ipc1
miconic-strips-ipc2
mprime-ipc1
mystery-ipc1
openstacks-ipc5
pathways-ipc5
pipes-notank-ipc4
pipes-tank-ipc4
psr-small-ipc4
rovers-ipc5
satellite-ipc4
schedule-strips
tpp-ipc5
trucks-ipc5
zenotravel-ipc3

22
30
7
12
5
2
20
22
7
85
24
21
7
4
21
14
50
7
6
46
6
9
11

total
s
w

438

hF
s
22
21
7
12
5
2
7
22
6
51
23
21
7
4
17
11
49
6
6
46
6
6
11

%S
100
70
100
100
100
100
35
100
86
60
96
100
100
100
81
79
98
86
100
100
100
67
100

368
20.56
14

hI
s
20
18
4
12
4
1
7
16
4
50
22
18
7
4
15
9
49
7
6
40
6
7
11

%S
91
60
57
100
80
50
35
73
57
59
92
86
100
100
71
64
98
100
100
87
100
78
100

337
18.38
7

hFI
s
21
18
7
12
4
1
7
16
5
50
21
21
7
4
16
9
49
6
6
46
6
7
11

%S
95
60
100
100
80
50
35
73
71
59
88
100
100
100
76
64
98
86
100
100
100
78
100

350
19.13
9

MS -104

MS -105

s
19
18
7
12
5
2
7
16
4
54
21
17
7
3
20
13
50
6
6
22
6
6
11

s
17
20
4
12
1
2
7
21
5
55
12
13
7
4
12
7
50
7
6
1
6
5
11

%S
86
60
100
100
100
100
35
73
57
64
88
81
100
75
95
93
100
86
100
48
100
67
100

332
19.07
11

%S
77
67
57
100
20
100
35
95
71
65
50
62
100
100
57
50
100
100
100
2
100
56
100

285
16.64
9

HSPF

s
15
30
4
9
5
0
6
16
3
45
8
11
7
4
13
7
50
6
5
11
5
9
8

%S
68
100
57
75
100
0
30
73
43
53
33
52
100
100
62
50
100
86
83
24
83
100
73

277
15.45
6

Gamer
s
11
30
4
11
2
2
20
20
6
85
9
8
7
4
11
6
47
5
6
3
5
3
10

%S
50
100
57
92
40
100
100
91
86
100
38
38
100
100
52
43
94
71
100
7
83
33
91

315
16.66
8

blind
s
18
18
4
7
4
1
7
10
2
50
19
18
7
4
14
10
48
5
4
29
5
5
7

%S
82
60
57
58
80
50
35
45
29
59
79
86
100
100
67
71
96
71
67
63
83
56
64

296
15.58
2

hmax
s
20
18
4
8
5
2
7
10
2
50
24
18
7
4
17
10
49
6
5
31
6
7
8

%S
91
60
57
67
100
100
35
45
29
59
100
86
100
100
81
71
98
86
83
67
100
78
73

318
17.66
6

Table 2: A summary of the experimental results with databased versions of the forkdecomposition heuristics. Per domain, S denotes the number of tasks solved by
any planner. Per planner/domain, the number of tasks solved by that planner
is given both by the absolute number (s) and by the percentage from solved by
some planners (%S). Boldfaced results indicate the best performance within the
corresponding domain. The last three rows summarize the number of solved instances, the domain-normalized measure of solved instances (s), and the number
of domains in which the planners achieved superior performance (w).

A with the only forks heuristic hF exhibited the best overall performance according to
all three measures. In terms of the absolute number of solved instances, A with all three
fork-decomposition heuristics outperformed all other planners in the suite. The contribution
of databasing to the success of the fork-decomposition heuristics was dramatic. Looking
back at the results with fully online heuristic computation depicted in Table 1, note that
the total number of solved instances for the fork-decomposition heuristics hF , hI , and hFI
increased by 74, 55, and 76, respectively, and this made the whole difference.
We have also performed a comparative evaluation on the planning domains from the
recent IPC-2008. The IPC-2008 domains differ from the previous domains in that actions
had various costs, and, more importantly, many actions had zero cost. The latter is an
issue for heuristic-search planners because heuristic functions cannot differentiate between
subplans that have the same cost of zero, but differ in length. In any case, the comparative
side of our evaluation on the IPC-2008 domains differ on several points from the previous
one. First, neither for merge-and-shrink nor for hmax heuristics, we had implementation
supporting arbitrary action costs. Hence, our comparison here is only with Gamer, HSPF ,
and blind search. Second, to ensure admissibility of the blind search, the latter has been
modified to return on non-goal states the cost of the cheapest applicable action. Finally, all
the planners were run on a 3GHz Intel E8400 CPU with 4 GB memory, using 2 GB memory
83

fiKatz & Domshlak

domain

S

elevators-strips-ipc6
openstacks-strips-ipc6
parcprinter-strips-ipc6
pegsol-strips-ipc6
scanalyzer-strips-ipc6
sokoban-strips-ipc6
transport-strips-ipc6
woodworking-strips-ipc6

22
21
16
27
12
28
11
14

total
s
w

152

hF
s
18
19
14
27
12
25
11
8

%S
82
90
88
100
100
89
100
57

134
7.06
3

hI
s
14
19
13
27
6
26
11
8

%S
64
90
81
100
50
93
100
57

124
6.35
2

hFI
s
15
19
13
27
6
27
11
8

%S
68
90
81
100
50
96
100
57

126
6.43
3

HSPF

s
7
21
16
27
6
13
9
9

%S
32
100
100
100
50
46
82
64

108
5.74
3

Gamer
s
22
19
9
24
11
20
11
14

%S
100
90
56
89
92
71
100
100

130
6.99
3

blind
s
11
19
10
27
12
20
11
7

%S
50
90
63
100
100
71
100
50

117
6.24
3

Table 3: A summary of the experimental results. Per domain, S denotes the number of
tasks solved by any planner. Per planner/domain, the number of tasks solved by
that planner is given both by the absolute number (s) and by the percentage from
solved by some planners (%S). Boldfaced results indicate the best performance
within the corresponding domain. The last three rows summarize the number of
solved instances, the domain-normalized measure of solved instances (s), and the
number of domains in which the planners achieved superior performance (w).

limit and 30 minute timeout. The results of this evaluation are summarized in Table 3; for
the detailed results we refer the reader to Tables 21-22 in the appendix. Overall, these
results show that A with the fork-decomposition heuristics are very much competitive on
the IPC-2008 domains as well.

9. Formal Analysis: Asymptotic Performance Ratios
Empirical evaluation on a concrete set of benchmark tasks is a standard and important
methodology for assessing the effectiveness of heuristic estimates: it allows us to study the
tradeoff between the accuracy of the heuristics and the complexity of computing them.
However, as rightfully noted by Helmert and Mattmuller (2008), such evaluations almost
never lead to absolute statements of the type Heuristic h is well-suited for solving problems from benchmark suite X, but only to relative statements of the type Heuristic h
expands fewer nodes than heuristic h0 on benchmark suite X. Moreover, one would probably like to obtain formal evidence of the effectiveness of a heuristic before proceeding with
its implementation, especially for very complicated heuristic procedures such as those underlying the proofs of Theorems 7 and 8. Our formal analysis of the effectiveness of the
fork-decomposition heuristics using the methodology suggested and exploited by Helmert
and Mattmuller was motivated primarily by this desire for formal evidence.
Given a planning domain D and heuristic h, Helmert and Mattmuller (2008) consider
the asymptotic performance ratio of h in D. The goal is to find a value (h, D)  [0, 1] such
that
(1) for all states s in all problems   D, h(s)  (h, D)  h (s) + o(h (s)), and
(2) there is a family of problems {n }nN  D and solvable, non-goal states {sn }nN such
that sn  n , limn h (sn ) = , and h(sn )  (h, D)  h (sn ) + o(h (sn )).
84

fiImplicit Abstraction Heuristics

Domain

h+

hk

hPDB

hPDB
add

hF

hI

hFI

Gripper
Logistics
Blocksworld
Miconic-Strips
Satellite

2/3
3/4
1/4
6/7
1/2

0
0
0
0
0

0
0
0
0
0

2/3
1/2
0
1/2
1/6

2/3
1/2
0
5/6
1/6

0
1/2
0
1/2
1/6

4/9
1/2
0
1/2
1/6

Table 4: Performance ratios of multiple heuristics in selected planning domains; ratios for
h+ , hk , hPDB , hPDB
add are by Helmert and Mattmuller (2008).

In other words, h is never worse than (, domain, )h (plus a sublinear term), and it can
become as bad as (h, D)  h (plus a sublinear term) for arbitrarily large inputs; note that
both the existence and uniqueness of (h, D) are guaranteed for any h and D.
Helmert and Mattmuller (2008) study the asymptotic performance ratio of some standard admissible heuristics on a set of well-known benchmark domains from the first four
IPCs. Their results for Gripper, Logistics, Blocksworld, Miconic, and Satellite
are shown in the first four columns of Table 4.
 The h+ estimate corresponds to the optimal cost of solving the well-known delete
relaxation of the original planning task, which is generally NP-hard to compute (Bylander, 1994).
 The hk , k  N+ , family of heuristics is based on a relaxation where the cost of
achieving a partial assignment is approximated by the highest cost of achieving its
sub-assignment of size k (Haslum & Geffner, 2000); computing hk is exponential only
in k.
 The hPDB and hPDB
add heuristics are regular (maximized over) and additive pattern
database heuristics where the size of each pattern is assumed to be O(log(n)) where
n = |V |, and, importantly, the choice of the patterns is assumed to be optimal.
These results provide us with a baseline for evaluating our fork-decomposition heuristics
hI , and hFI . First, however, Theorem 9 shows that these three heuristics are worth
analyzing because each alone can be strictly more informative than the other two, depending
on the planning task and/or the state being evaluated.7
hF ,

Theorem 9 (Undominance) Under uniform action cost partition, none of the heuristic
functions hF , hI , and hFI dominates another.
Proof: The proof is by example of two tasks, 1 and 2 , which illustrate the following
two cases: hF (I) > hFI (I) > hI (I) and hF (I) < hFI (I) < hI (I). These two tasks
are defined over the same set of binary-valued variables V = {v1 , v2 , v3 , u1 , u2 , u3 }, have
the same initial state I = {v1 : 0, v2 : 0, v3 : 0, u1 : 0, u2 : 0, u3 : 0}, and have the same goal
7. Theorem 9 is formulated and proven under the uniform action cost partition that we use throughout the
paper, including the experiments. For per-step optimal action cost partitions (Katz & Domshlak, 2010),
it is trivial to show that hFI dominates both hF and hI for all planning tasks.

85

fiKatz & Domshlak

A1

u1

u2

u3

v1

v2

v3

a1
a2
a3
a4
a5
a6
a7
a8
a9

h{v1 : 0, u1 : 0, u2 : 0, u3 : 0}, {v1 : 1}i
h{v2 : 0, u1 : 1, u2 : 0, u3 : 1}, {v2 : 1}i
h{v3 : 0, u1 : 1, u2 : 1, u3 : 0}, {v3 1}i
h{u1 : 0}, {u1 : 1}i
h{u1 : 1}, {u1 : 0}i
h{u2 : 0}, {u2 : 1}i
h{u2 : 1}, {u2 : 0}i
h{u3 : 0}, {u3 : 1}i
h{u3 : 1}, {u3 : 0}i

(a)

1
F

1
I

1
FI

1/3
1/3
1/3
1
1
1
1
1
1

1
1
1
1/3
1/3
1/3
1/3
1/3
1/3

1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4

(c)

u1

u2

u3

v1 v2 v3

v1 v2 v3

v1 v2 v3

Guf 1

Guf 2

Guf 3

u1 u2 u3

u1 u2 u3

u1 u2 u3

v1

v2

v3

Gvi 1

Gvi 2

Gvi 3

A2
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
a11
a12

(b)

h{v1 : 0, u1 : 1}, {v1 : 1}i
h{v1 : 0, u2 : 1}, {v1 : 1}i
h{v1 : 0, u3 : 1}, {v1 : 1}i
h{v2 : 0, u1 : 1}, {v2 : 1}i
h{v2 : 0, u2 : 1}, {v2 : 1}i
h{v2 : 0, u3 : 1}, {v2 : 1}i
h{v3 : 0, u1 : 1}, {v3 : 1}i
h{v3 : 0, u2 : 1}, {v3 : 1}i
h{v3 : 0, u3 : 1}, {v3 : 1}i
h{u1 : 0}, {u1 : 1}i
h{u2 : 0}, {u2 : 1}i
h{u3 : 0}, {u3 : 1}i

2
F

2
I

2
FI

1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1
1
1

1
1
1
1
1
1
1
1
1
1/3
1/3
1/3

1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4
1/4

(d)

Figure 8: Illustrations for the proof of Theorem 9: (a) causal graphs of 1 and 2 , (b) fork
and inverted fork subgraphs of the (same) causal graph of 1 and 2 , and the
action sets of (c) 1 and (d) 2 , as well as the costs of the action representatives
in each abstract problem along these subgraphs. Considering for example the
first row of table (c), the action a1 in 1 has a single representative in each of the
three fork abstractions, as well as a representative in the inverted-fork abstraction
1G i . Hence, the cost of each of its representatives in F-decomposition is 1/3,
v1

while the cost of its sole representative in I-decomposition is 1.

G = {v1 : 1, v2 : 1, v3 : 1}. The difference between 1 and 2 is in the action sets, listed in
Figure 8c-d, with all the actions being unit-cost actions. The two tasks induce identical
causal graphs, depicted in Figure 8a. Hence, the collections of v-forks and v-iforks of both
tasks are also identical; these are depicted in Figure 8b. The fractional costs of the tasks
action representatives in the corresponding abstract problems are given in Figure 8c-d.
Figure 9 shows the optimal plans for all the abstract problems in F-decompositions 1F =
{1G f , 1G f , 1G f } and 2F = {2G f , 2G f , 2G f }, I-decompositions 1I = {1G i , 1G i , 1G i }
u1

u2

u3

u1

u2

u3

v1

v2

v3

and 2I = {2G i , 2G i , 2G i }, and FI-decompositions 1FI = 1F  1I and 2FI = 2F  2I .
v1

v2

v3

The last column in both tables captures the estimates of the three heuristics for the initial
states of 1 and 2 , respectively. Together, these two cases show that none of the forkdecomposition heuristic functions hF , hI , and hFI dominates any other, and, since all the
86

fiImplicit Abstraction Heuristics

h
hF

hI

h

FI

task
1G f
u1
1G f
u2
1G f
u3
1G i
v1
1G i
v2
1G i
v3
1G f
u1
1G f
u2
1G f
u3
1G i
v1
1G i
v2
1G i
v

optimal plan

cost

ha1 , a4 , a2 , a3 i

2

ha1 , a2 , a6 , a3 i

2

ha1 , a3 , a8 , a2 i

2

ha1 i

1

ha4 , a8 , a2 i

5/3

ha4 , a6 , a3 i

5/3

ha1 , a4 , a2 , a3 i

1

ha1 , a2 , a6 , a3 i

1

ha1 , a3 , a8 , a2 i

1

ha1 i

1/4

ha4 , a8 , a2 i

3/4

ha4 , a6 , a3 i

3/4

h(I)

h
hF

6

hI

4 31

4 43

h

3

FI

task

optimal plan

2G f
u1
2G f
u2
2G f
u3
2G i
v1
2G i
v2
2G i
v3
2G f
u1
2G f
u2
2G f
u3
2G i
v1
2G i
v2
2G i
v

ha2 , a5 , a8 i

1

ha1 , a4 , a7 i

1

ha1 , a4 , a7 i

cost

h(I)
3

1

ha10 , a1 i

4/3

ha10 , a4 i

4/3

ha10 , a7 i

4/3

ha2 , a5 , a8 i

3/4

ha1 , a4 , a7 i

3/4

ha1 , a4 , a7 i

3/4

ha10 , a1 i

1/2

ha10 , a4 i

1/2

ha10 , a7 i

1/2

4

15/4

3

(a)

(b)

Figure 9: Illustrations for the proof of Theorem 9: Optimal plans for all the abstract problems of (a) 1 , where we have hF (I) > hFI (I) > hI (I), and (b) 2 , where we have
hF (I) < hFI (I) < hI (I).

variables above are binary-valued, the claim holds in conjunction with arbitrary variable
domain abstractions.

One conclusion from Theorem 9 is that it is worth studying the asymptotic performance
ratios for all three heuristics. The last three columns of Table 4 present our results for
hF , hI , and hFI for the Gripper, Logistics, Blocksworld, Miconic, and Satellite
domains. We also studied the performance ratios of max{hF , hI , hFI }, and in these five
domains they appear to be identical to those of hF . (Note that ratio of max should not
necessarily be identical to max of ratios, and thus this analysis is worthwhile.) Taking
a conservative position, the performance ratios for the fork-decomposition heuristics in
Table 4 are worst-case in the sense that
(i) here we neither optimize the action cost partition (setting it to uniform as in the rest
of the paper) nor eliminate clearly redundant abstractions, and
(ii) we use domain abstractions to (up to) ternary-valued abstract domains only.
The domains of the fork roots are all abstracted using the leave-one-out binary-valued
domain decompositions as in Eq. 16 while the domains of the inverted-fork sinks are all
abstracted using the distance-from-initial-value ternary-valued domain decompositions
as in Eq. 15.
Overall, the results for fork-decomposition heuristics in Table 4 are gratifying. First,
note that the performance ratios for hk and hPDB are all 0. This is because every subgoal
set of size k (for hk ) and size log(n) (for hPDB ) can be reached in the number of steps that
only depends on k (respectively, log(n)), and not n, while h (sn ) grows linearly in n in
all the five domains. This leaves us with hPDB
add being the only state-of-the-art (tractable
87

fiKatz & Domshlak

and) admissible heuristic to compare with. Table 4 shows that the asymptotic performance
F
ratio of hF heuristic is at least as good as that of hPDB
add in all five domains, while h is
+
PDB
superior to hPDB
add in Miconic, getting here quite close to h . When comparing hadd and
fork-decomposition heuristics, it is crucial to recall that the ratios devised by Helmert and
Mattmuller for hPDB
add are with respect to optimal, manually-selected set of patterns. By
contrast, the selection of variable subsets for fork-decomposition heuristics is completely
nonparametric, and thus requires no tuning of the abstraction-selection process.
In the rest of the section we prove these asymptotic performance ratios of hF , hI , and
FI
h in Table 4 for the five domains. We begin with a very brief outline of how the results are
obtained. Some familiarity with the domains is assumed. Next, each domain is addressed
in detail: we provide an informal domain description as well as its sas+ representation, and
then prove lower and upper bounds on the ratios for all three heuristics.
Gripper Assuming n > 0 balls should be moved from one room to another, all three
heuristics hF , hI , hFI account for all the required pickup and drop actions, and only for
O(1)-portion of move actions. However, the former actions are responsible for 2/3 of
the optimal-plan length (= cost). Now, with the basic uniform action-cost partition,
hF , hI , and hFI account for the whole, O(1/n), and 2/3 of the total pickup/drop
actions cost, respectively, providing the ratios in Table 4.8
Logistics An optimal plan contains at least as many load/unload actions as move actions,
and all three heuristics hF , hI , hFI fully account for the former, providing a lower bound
of 1/2. An instance on which all three heuristics achieve exactly 1/2 consists of two
trucks t1 , t2 , no airplanes, one city, and n packages such that the initial and goal
locations of all the packages and trucks are all pair-wise different.
Blocksworld Arguments similar to those of Helmert and Mattmuller (2008) for hPDB
add .
Miconic All three heuristics fully account for all the loads/unload actions. In addition, hF
accounts for the full cost of all the move actions to the passengers initial locations,
and for half of the cost of all the other move actions. This provides us with lower
bounds of 1/2 and 5/6, respectively. Tightness of 1/2 for hI and hFI is shown on a
task consisting of n passengers, 2n + 1 floors, and all the initial and goal locations
being pair-wise different. Tightness of 5/6 for hF is shown on a task consisting of n
passengers, n + 1 floors, the elevator and all the passengers are initially at floor n + 1,
and each passenger i wishes to get to floor i.
Satellite The length of an optimal plan for a problem with n images to be taken and k
satellites to be moved to some end-positions is  6n + k. All three heuristics fully
account for all the image-taking actions and one satellite-moving action per satellite
as above, providing a lower bound of 61 . Tightness of 1/6 for all three heuristics
is shown on a task as follows: Two satellites with instruments {i}li=1 and {i}2l
i=l+1 ,

respectively, where l = n  n. Each pair of instruments {i, l + i} can take images
in modes {m0 , mi }. There is a set of directions {dj }nj=0 and a set of image objectives
8. We note that a very slight modification of the uniform action-cost partition results in a ratio of 2/3 for
all three heuristics. Such optimizations, however, are outside of our scope here.

88

fiImplicit Abstraction Heuristics

right

lef t

robot

right
b1

b1



bn

...
f
Gright

lef t
bn

...

b1

right

robot
bn

b1

f
Glef
t

(a)

...

robot

bn

f
Grobot

lef t

b
Gbi , b  Balls

(b)

Figure 10: Grippers (a) causal graph and (b) the corresponding collection of v-forks and
v-iforks

{oi }ni=1 such that, for 1  i  l, oi = (d0 , mi ) and, for l < i  n, oi = (di , m0 ).
Finally, the calibration direction for each pair of instruments {i, l + i} is di .
9.1 Gripper
The domain consists of one robot robot with two arms Arms = {right, lef t}, two rooms
Rooms = {r1, r2}, and a set Balls of n balls. The robot can pick up a ball with an arm
arm  Arms if arm is empty, release a ball b  Balls from the arm arm if arm currently
holds b, and move from one room to another. All balls and the robot are initially in room
r1, both arms are empty, and the goal is to move all the balls to room r2. A natural
description of this planning task in sas+ is as follows.
S
S
 Variables V = {robot} Arms Balls with domains
D(robot) = Rooms

D(lef t) = D(right) = Balls  {empty}

b  Balls : D(b) = Rooms  {robot}.

 Initial state I = {b : r1 | b  Balls}  {robot : r1, right : empty, lef t : empty}.
 Goal G = {b : r2 | b  Balls}.
 Actions
A ={M ove(r, r0 ) | {r, r0 }  Rooms}

[

{P ickup(b, arm, r), Drop(b, arm, r) | b  Balls, arm  Arms, r  Rooms},
where
 move robot: M ove(r, r0 ) = h{robot : r}, {robot : r0 }i,

 pickup ball:
P ickup(b, arm, r) = h{b : r, arm : empty, robot : r}, {b : robot, arm : b}i, and

 drop ball: Drop(b, arm, r) = h{b : robot, arm : b, robot : r}, {b : r, arm : empty}i.

The (parametric in n) causal graph of this task is depicted in Figure 10a.
89

fiKatz & Domshlak

frobot

Action
0

M ove(r, r )
P ickup(b, arm, r)
Drop(b, arm, r)

farm,empty

1
1
1

0
2
2

farm,b

farm,b0

0
2
2

0
1
1

farm0 ,
0
1
1

ib
1
2
2

ib0
1
1
1

F

I

FI

1

1
n
1
n+1
1
n+1

1
n+1
1
3n+6
1
3n+6

1
2n+5
1
2n+5

Table 5: Number of representatives for each original Gripper action in each abstract task,
as well as the partition of the action costs between these representatives
frobot

fright,empty
fright,b
fright,b0
flef t,
ib
ib0

P ickup(b, right, r1) = h{robot : r1, b : r1}, {b : robot}i
P ickup(b, right, r1)1 = h{right : empty}, {right : b}i,
P ickup(b, right, r1)2 = h{right : b, b : r1}, {b : robot}i
P ickup(b, right, r1)1 = h{right : empty}, {right : b}i,
P ickup(b, right, r1)2 = h{right : b, b : r1}, {b : robot}i
P ickup(b, right, r1) = h{right : b, b : r1}, {b : robot}i
P ickup(b, right, r1) = h{right : b, b : r1}, {b : robot}i
P ickup(b, right, r1)1 = h{right : empty}, {right : b}i,
P ickup(b, right, r1)2 = h{right : b, robot : r1, b : r1}, {b : robot}i
P ickup(b, right, r1) = h{right : empty}, {right : b}i

Table 6: The sets of representatives of the original action P ickup(b, right, r1) in the abstract
tasks

9.1.1 Fork Decomposition
Since the variables robot, right, and lef t have no goal value, the collection of v-forks and
v-iforks is as in Figure 10b. The domains of inverted fork sinks are ternary valued. The
domains of fork roots are abstracted as in Eq. 16 (leave one out), and thus
F = {frobot }  {fright, , flef t, |   {empty}  Balls},
I = {ib | b  Balls},

FI = {frobot }  {fright, , flef t, |   {empty}  Balls}  {ib | b  Balls}.
For each original action, the number of its representatives in each abstract task, as well as
the cost assigned to each such representative, are listed in Table 5. Table 6 illustrates derivation of these numbers via decomposition of an example action P ickup(b, right, r1) in each
of the fork decomposition abstractions. That action has one nonredundant representative
in frobot , two such representatives in each of fright,empty and fright,b , one representative in
each fright,b0 for b0  Balls \ {b}, one representative in each flef t, for   Balls  {empty},
two representatives in ib , and one representative in each ib0 for b0  Balls \ {b}. This
1
1
results in cost 2n+5
for each representative in F , n+1
for each representative in I , and
1
3n+6 for each representative in FI .
Given that, the optimal plans for the abstract tasks are as follows.
90

fiImplicit Abstraction Heuristics

h

task
frobot

hF

fright,
flef t,

hI

ib
frobot

hFI

fright,
flef t,
ib

optimal plan
hP ickup(b1 , right, r1), . . . , P ickup(bn , right, r1),
, M ove(r1, r2), Drop(b1 , right, r2), . . . , Drop(bn , right, r2)i
hP ickup(b1 , lef t, r1), . . . , P ickup(bn , lef t, r1),
, Drop(b1 , lef t, r2), . . . , Drop(bn , lef t, r2)i
hP ickup(b1 , right, r1), . . . , P ickup(bn , right, r1),
, Drop(b1 , right, r2), . . . , Drop(bn , right, r2)i
hP ickup(b, right, r1)1 , P ickup(b, right, r1)2 , M ove(r1, r2), Drop(b, lef t, r2)2 i
hP ickup(b1 , right, r1), . . . , P ickup(bn , right, r1), M ove(r1, r2),
, Drop(b1 , right, r2), . . . , Drop(bn , right, r2)i
hP ickup(b1 , lef t, r1), . . . , P ickup(bn , lef t, r1),
, Drop(b1 , lef t, r2), . . . , Drop(bn , lef t, r2)i
hP ickup(b1 , right, r1), . . . , P ickup(bn , right, r1),
, Drop(b1 , right, r2), . . . , Drop(bn , right, r2)i
hP ickup(b, right, r1)1 , P ickup(b, right, r1)2 , M ove(r1, r2), Drop(b, lef t, r2)2 i

cost

#

4n+5
2n+5

1

2n
2n+5

n+1

2n
2n+5

n+1

3
1
+ n
n+1
1
2n +
3n+6
n+1

n

2n
3n+6

n+1

2n
3n+6

n+1

3
3n+6

1
+ n+1

h(I)

2n  2n5
2n+5

4n+1
n+1

1
4n
3

+ 4n+6
3n+6

n

Assuming n > 0 balls should be moved from one room to another, the cost of the optimal
plan for the original task is 3n  1 when n is even, and 3n when n is odd. Therefore, the
asymptotic performance ratios for the heuristics hF , hI , hFI on Gripper are 2/3, 0, and 4/9,
respectively.
9.2 Logistics
Each Logistics task consists of some k cities, x airplanes, y trucks and n packages. Each
city i is associated with a set Li = {li1 . . . , lii } of locations within that city; the union of
S
the locations of all the cities is denoted by L = ki=1 Li . In addition, precisely one location
in each city is an airport, and the set of airports is LA = {l11 . . . , lk1 }  L. Each truck can
move only within the city in which it is located, and airplanes can fly between airports.
The airplanes are denoted by U = {u1 , . . . , ux }, the trucks by T = {t1 , . . . , ty }, and the
packages by P = {p1 , . . . , pn }. Let Ti = {t  T | I[t]  Li } denote the trucks of city i, and
P = P1  P2  P3  P4  P5 denote a partition of the packages as follows:
 each package in P1 = {p  P | I[p], G[p]  LA } is both initially at an airport and
needs to be moved to another airport,
 each package in P2 = {p  P | I[p]  LA  Li , G[p]  Lj \ LA , i 6= j} is initially at an
airport and needs to be moved to a non-airport location in another city,
 each package in P3 = {p  P | I[p]  Li , G[p]  Li } needs to be moved within one
city,
 each package in P4 = {p  P | I[p]  Li \ LA , G[p]  LA \ Li } needs to be moved from
a non-airport location in one city to the airport of some other city, and
 each package in P5 = {p  P | I[p]  Li \ LA , G[p]  Lj \ LA , i 6= j} needs to be moved
from a non-airport location in one city to a non-airport location in another city.
A natural Logistics task description in sas+ is as follows.
 Variables V = U  T  P with domains
u  U : D(u) = LA ,

1  i  k, t  Ti : D(t) = Li ,

p  P : D(p) = L  U  T.
91

fiKatz & Domshlak

u1    ux

t1   

ty

u
p1 . . .

p1   

pi    pn
(a)

u1 . . . ux

t
pn

Guf , u  U

p1 . . .

pn

Gtf , t  T
(b)

t1 . . . ty
p

Gpi , p  P

Figure 11: Logisticss (a) causal graph and (b) the corresponding collection of v-forks and
v-iforks

 Initial state I  (LA )x  L1      Lk  (L)n .
 Goal G = {p1 : l1 , . . . , pn : ln }  (L)n .
 Actions
A=

k [ [
[

i=1 lLi tTi



[ [
lLA uU


{Lt(p, t, l), U t(p, t, l) | p  P }  {M t(t, l, l0 ) |, l0  Li \ {l}}


{La(p, u, l), U a(p, u, l) | p  P }  {M a(u, l, l0 ) | l0  LA \ {l}} ,

where
 load package p onto truck t in location l: Lt(p, t, l) = h{p : l, t : l}, {p : t}i,
 unload package p from truck t in location l: U t(p, t, l) = h{p : t, t : l}, {p : l}i,
 move truck t from location l to location l0 : M t(t, l, l0 ) = h{t : l}, {t : l0 }i,
 load package p onto airplane u in l: La(p, u, l) = h{p : l, u : l}, {p : u}i,
 unload package p from airplane u into l: U a(p, u, l) = h{p : u, u : l}, {p : l}i, and
 move airplane u from location l to l0 : M a(u, l, l0 ) = h{u : l}, {u : l0 }i.
The (parametrized in n, x, and y) causal graph of Logistics tasks is depicted in Figure 11a.
9.2.1 Fork Decomposition
Since the variables u  U and t  T have no goal value, the collection of v-forks and viforks is as in Figure 11b. The domains of the inverted-fork sinks are all abstracted as in
Eq. 15 (distance-from-initial-value), while the domains of the fork roots are abstracted
92

fiImplicit Abstraction Heuristics

fu,l fu,l0 fu,l00 fu0 ,l ft,l ft,l0 ft,l00 ft0 ,l ip,m F I FI

Action
0

M t(t, l, l )
M a(u, l, l0 )

0
1

0
1

0
0

0
0

1
0

1
0

0
0

0
0

1
2
1
2

1
1

1
1
ni 2+ni
1
1
ni 2+ni

(a)
I[p]  LA  Li
I[p]  Li \ LA
p  P1 p  P2 p  P3 p  P3 p  P4
p  P5
fu,l ft,l ip0 ,m ip,1 ip,1 ip,2 ip,1 ip,1 ip,1 ip,2 ip,1 ip,2 ip,3 F I FI

Action
l  Li
l  Lj
La(p, u, l), U a(p, u, l)

Lt(p, t, l), U t(p, t, l)

1
1
1

1
1
1

0
0
0

1
0
1

1
0
1

0
1
0

1
0
1

1
0
0

1
0
0

0
0
1

1
0
0

0
0
1

0
1
0

1
nf
1
nf
1
nf

1
1
1

1
nf +1
1
nf +1
1
nf +1

(b)
Figure 12: Number of representatives of each original Logistics action in each abstract
task, as well as the partition of the action costs between these representatives;
tables (a) and (b) capture the move and load/unload actions, respectively

as in Eq. 16 (leave-one-out). Thus, we have
F =

[ [
uU lLA

I =

FI =

{fu,l } 

[

[

k [ [
[
i=1 tTi lLi

{ip,1 } 
{ip,2 }
pP
pP2 P4 P5
[ [
uU lLA

{fu,l } 



{ft,l },

[
pP5

k [ [
[
i=1 tTi lLi

{ip,3 },

{ft,l } 

[
pP

{ip,1 } 

[

{ip,2 } 

pP2 P4 P5

[
pP5

{ip,3 }.

P
The total number of forks is nf = |F | = |U |  |LA | + ki=1 |Ti |  |Li |, and the total number
of inverted forks is ni = |I | = |P1 | + 2  |P2 | + |P3 | + 2  |P4 | + 3  |P5 |. For each action
a  A, the number of its representatives in each abstract task, as well as the cost assigned
to each such representative, are given in Figure 12. Each row in the tables of Figure 12
corresponds to a certain Logistics action, each column (except for the last three) represents
an abstract task, and each entry captures the number of representatives an action has in
the corresponding task. The last three columns show the portion of the total cost that is
given to an action representative in each task, in each of the three heuristics in question.
9.2.2 Lower Bound
Note that any optimal plan for a Logistics task contains at least as many load/unload
actions as move actions. Thus, the following lemma provides us with the lower bound of
1/2 for all three heuristics in question.

93

fiKatz & Domshlak

Lemma 1 For any Logistics task, hF , hI , and hFI account for the full cost of the load/unload
actions required by any optimal plan for that task.
Proof: For any Logistics task, all the optimal plans for that task contain the same amount
of load/unload actions for each package p  P as follows.

p  P1 :

2 actions  one load onto an airplane, and one unload from that airplane,

p  P2 : 4 actions  one load onto an airplane, one unload from that airplane, one load
onto a truck, and one unload from that truck,
p  P3 :

2 actions  one load onto a truck, and one unload from that truck,

p  P4 : 4 actions  one load onto a truck, one unload from that truck, one load onto an
airplane, and one unload from that airplane, and
p  P5 : 6 actions  two loads onto some trucks, two unloads from these trucks, one load
onto an airplane, and one unload from that airplane.
Consider the fork-decomposition F . Any optimal plan for each of the abstract tasks
will contain the number of load/unload actions exactly as above (the effects of these actions
remain unchanged in these tasks). The cost of each representative of each load/unload
action is n1f , and there are nf abstract tasks. Therefore, the heuristic hF fully accounts for
the cost of the required load/unload actions.
Now consider the fork-decomposition I . With m being the domain-decomposition
index of the abstraction, any optimal plan for the abstract task ip,m will include one load
and one unload actions as follows.
p  P1 :

one load onto an airplane and one unload from that airplane,

p  P2 , m = 1:

one load onto an airplane and one unload from that airplane,

p  P2 , m = 2:

one load onto a truck and one unload from that truck,

p  P3 :

one load onto a truck and one unload from that truck,

p  P4 , m = 1:

one load onto a truck and one unload from that truck,

p  P4 , m = 2:

one load onto an airplane, and one unload from that airplane,

p  P5 , m = 1:

one load onto a truck and one unload from that truck,

p  P5 , m = 2:

one load onto an airplane and one unload from that airplane, and

p  P5 , m = 3:

one load onto a truck and one unload from that truck.

The cost of each representative of load/unload actions is 1, and thus the heuristic hI fully
accounts for the cost of the required load/unload actions.
Finally, consider the fork-decomposition FI . Any optimal plan for each of the forkstructured abstract tasks will contain the same number of load/unload actions as for F .
The cost of each representative of load/unload actions is nf1+1 and there are nf such abstract
tasks. In addition, each of these load/unload actions will also appear in exactly one inverted
fork-structured abstract task. Therefore the heuristic hFI also fully accounts for the cost of
the required load/unload actions.

94

fiImplicit Abstraction Heuristics

t1
p1

t2

...

pn

p1

Gtf1

t1

...

t2

pn

p
Gpi , p  P

Gtf2

Figure 13: Collection of v-forks and v-iforks for the Logistics task used for the proof of
the upper bound of 1/2

9.2.3 Upper Bound
An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t1 , t2 , no
airplanes, one city, and n packages such that the initial and goal locations of all the packages
are all pairwise different, and both trucks are initially located at yet another location. More
+
formally, if L = {li }2n
i=0 , and T = {t1 , t2 }, then the sas encoding for this Logistics task
is as follows.
 Variables V = {t1 , t2 , p1 , . . . , pn } with domains
t  T : D(t) = L,

p  P : D(p) = L  T.
 Initial state I = {t1 : l0 , t2 : l0 , p1 : l1 , . . . , pn : ln }.
 Goal G = {p1 : ln+1 , . . . , pn : l2n }.
 Actions A = {Lt(p, t, l), U t(p, t, l) | l  L, t  T, p  P }  {M t(t, l, l0 ) | t  T, {l, l0 } 
L}.
The collection of v-forks and v-iforks for this task is depicted in Figure 13. The domains of
the inverted-fork sinks are all abstracted as in Eq. 15 (distance-from-initial-value), while
the domains of the fork roots are abstracted as in Eq. 16 (leave-one-out), and therefore
we have
F = {ft1 ,l ft2 ,l | l  L},
I = {ip,1 | p  P },

FI = {ft1 ,l ft2 ,l | l  L}  {ip,1 | p  P }.
The total number of forks is thus nf = 4n + 2 and the total number of inverted forks is
ni = n. The partition of the action costs for Logistics tasks is described in Figure 12.
Here we have P = P3 and thus the action cost partition is as follows.
ft,l

Action
0

M t(t, l, l )
Lt(p, t, l)
U t(p, t, l)

1
1
1

ft,l0
1
1
1

ft,l00
0
1
1

ft0 ,l
0
1
1

95

ip,1
1
1
1

ip0 ,1

F

I

FI

0
0
0

1
2
1
4n+2
1
4n+2

1
n

1
n+2
1
4n+3
1
4n+3

1
1

fiKatz & Domshlak

Given that, the optimal plans for the abstract task are
h

task

hF

ft1 ,l
ft2 ,l
ipi ,1
ft1 ,l
ft2 ,l
ipi ,1

hI
hFI

optimal plan
hLt(p1 , t2 , l1 ), . . . , Lt(pn , t2 , ln ), U t(p1 , t2 , ln+1 ), . . . , U t(pn , t2 , l2n )i
hLt(p1 , t1 , l1 ), . . . , Lt(pn , t1 , ln ), U t(p1 , t1 , ln+1 ), . . . , U t(pn , t1 , l2n )i
hM t(t1 , l0 , li ), Lt(pi , t1 , li ), M t(t1 , li , ln+i ), U t(pi , t1 , ln+i )i
hLt(p1 , t2 , l1 ), . . . , Lt(pn , t2 , ln ), U t(p1 , t2 , ln+1 ), . . . , U t(pn , t2 , l2n )i
hLt(p1 , t1 , l1 ), . . . , Lt(pn , t1 , ln ), U t(p1 , t1 , ln+1 ), . . . , U t(pn , t1 , l2n )i
hM t(t1 , l0 , li ), Lt(pi , t1 , li ), M t(t1 , li , ln+i ), U t(pi , t1 , ln+i )i

cost

#

2n
4n+2
2n
4n+2
2
+2
n
2n
4n+3
2n
4n+3

2n + 1
2n + 1
n
2n + 1
2n + 1
n

2
n+2

+

2
4n+3

h(I)
2n
2n + 2
2n +

2n
n+2

while an optimal plan for the original task, e.g., hM t(t1 , l0 , l1 ), Lt(p1 , t1 , l1 ), M t(t1 , l1 , l2 ), Lt(p2 , t1 , l2 ),
M t(t1 , l2 , l3 ), . . . , Lt(pn , t1 , ln ), M t(t1 , ln , ln+1 ), U t(p1 , t1 , ln+1 ), M t(t1 , ln+1 , ln+2 ), U t(p2 , t1 , ln+2 ),
M t(t1 , ln+2 , ln+3 ), . . . , U t(pn , t1 , l2n )i,

has the cost of 4n, providing us with the upper bound of
1/2 for all three heuristics. Putting our lower and upper bounds together, the asymptotic
ratio of all three heuristics in question is 1/2.
9.3 Blocksworld
Each Blocksworld task consists of a table table, a crane c, and n + 1 blocks B =
{b1 , . . . , bn+1 }. Each block can be either on the table, or on top of some other block,
or held by the crane. The crane can pick up a block if it currently holds nothing, and that
block has no other block on top of it. The crane can drop the held block on the table or on
top of some other block.
Consider now a Blocksworld task as follows. The blocks initially form a tower
b1 , . . . , bn , bn+1 with bn+1 being on the table, and the goal is to move them to form a
tower b1 , . . . , bn1 , bn+1 , bn with bn being on the table. That is, the goal is to swap the
lowest two blocks of the tower. A natural description of this task in sas+ is as follows.
 Variables V = {b, clearb | b  B}  {c} with domains
D(c) = {empty}  B,

b  B : D(b) = {table, c}  B \ {b},
D(clearb ) = {yes, no}.

 Initial state
I = {c : empty, bn+1 : table, clearb1 : yes}
[
{bi : bi+1 | 1  i  n}

[

{clearb : no | b  B \ {b1 }} .
 Goal G = {bn : table, bn+1 : bn , bn1 : bn+1 }  {bi : bi+1 | 1  i  n  2}.
 Actions A = {PT (b), DT (b) | b  B}  {P (b, b0 ), D(b, b0 ) | {b, b0 }  B} where
 pick block b from the table: PT (b) = h{c : empty, b : table, clearb : yes}, {cb, b : c}i,
 pick block b from block b0 :
P (b, b0 ) = h{c : empty, b : b0 , clearb : yes, clearb0 : no}, {c : b, b : c, clearb0 : yes}i,
96

fiImplicit Abstraction Heuristics

c

clearb1 . . . clearbn+1

c
b
Gbi , b  {bn1 , bn , bn+1 }

clearb0

clearb

c
b0

b

bn1

bn

clearb
bn+1

bn1

Gcf
(a)

bn

bn+1

f
Gclear
,b  B
b

(b)

Figure 14: (a) Causal graph and (b) the corresponding collection of v-forks and v-iforks for
the Blocksworld task used in the proof

 drop block b on the table: DT (b) = h{c : b, b : c}, {c : empty, b : table}i, and
 drop block b on block b0 :
D(b, b0 ) = h{c : b, b : c, clearb0 : yes}, {c : empty, b : b0 , clearb0 : no}i.

A schematic version of the causal graph of this task is depicted in Figure 14a. Since only
the variables bn1 , bn , bn+1 have goal values that are different from their values in the initial
state, the collection of v-forks and v-iforks is as in Figure 14b. After the (leave-one-out,
Eq. 16) domain abstraction of the variable c, c-fork Gcf breaks down into n + 2 abstract
tasks. The sinks of v-iforks Gbi n1 , Gbi n , and Gbi n+1 also go through the process of domain
decomposition (distance-from-initial-value, Eq. 15). However, due to the structure of the
domain transition graphs of the block variables, domain decomposition here results in only
a single abstract task for each of the v-iforks. Thus we have
F ={fc,empty }  {fc,b | b  B}  {fclearb | b  B},
I ={ibn1 ,1 , ibn ,1 , ibn+1 ,1 },

FI ={fc,empty }  {fc,b | b  B}  {fclearb | b  B}  ibn1 ,1 , ibn ,1 , ibn+1 ,1 }.
It is technically straightforward to verify that, for each abstract task in F , I , and FI ,
there exists a plan that (i) involves only the representatives of the actions
{P (bn1 , bn ), DT (bn1 ), P (bn , bn+1 ), DT (bn ), PT (bn+1 ), D(bn+1 , bn ), PT (bn1 ), D(bn1 , bn+1 )} ,
(21)
and (ii) involves each representative of each original action at most once. Even if together
these plans account for the total cost of all eight actions in Eq. 21, the total cost of all these
plans (and thus the estimates of all the three heuristics) is upper-bounded by 8, while an
optimal plan for the original task, e.g., hP (b1 , b2 ), DT (b1 ), P (b2 , b3 ), DT (b2 ), . . . , P (bn , bn+1 ), DT (bn ),
PT (bn+1 ), D(bn+1 , bn ), PT (bn1 ), D(bn1 , bn+1 ), PT (bn2 ), D(bn2 , bn1 ), . . . , PT (b1 ), D(b1 , b2 )i, has a cost
97

fiKatz & Domshlak

e

p1



e
p1   

pn

Gef

(a)

e
pn

(b)

p
Gpi , p  P

Figure 15: Miconics (a) causal graph and (b) the corresponding collection of v-forks and
v-iforks

of 4n. Hence, the asymptotic performance ratio of all three heuristics on the Blocksworld
domain is 0.
9.4 Miconic
Each Miconic task consists of one elevator e, a set of floors F , and the passengers P . The
elevator can move between |F | floors and on each floor it can load and/or unload passengers.
A natural sas+ description of a Miconic task is as follows.
 Variables V = {e}  P with domains
D(e) = F,

p  P : D(p) = F  {e}.
 Initial state I = {e : fe }  {p : fp | p  P }  (F )|P |+1 .
 Goal G = {p : fp0 | p  P }  (F )|P | .
 Actions A = {In(p, f ), Out(p, f ) | f  F, p  P }  {M ove(f, f 0 ) | {f, f 0 }  F }, where
 load passenger p into e on floor f : In(p, f ) = h{e : f, p : f }, {p : e}i,
 unload passenger p from e to floor f : Out(p, f ) = h{e : f, p : e}, {p : f }i, and
 move elevator from floor f to floor f 0 : M ove(f, f 0 ) = h{e : f }, {e : f 0 }i.
The (parametrized in n) causal graph of Miconic tasks is depicted in Figure 15a, and
Figure 15b depicts the corresponding collection of v-forks and v-iforks. The domains of the
inverted-fork sinks are all abstracted as in Eq. 15 (distance-from-initial-value), and the
domains of the fork roots are abstracted as in Eq. 16 (leave-one-out). Thus, we have
F = {fe,f | f  F },
I = {ip,1 | p  P },

FI = {fe,f | f  F }  {ip,1 | p  P }.
The total number of the fork-structured abstract tasks is thus nf = |F | = |F | and the
total number of the inverted fork structured abstract tasks is ni = |I | = |P |. For each
action a  A, the number of its representatives in each abstract task, as well as the cost
assigned to each such representative, are given in Table 7.
98

fiImplicit Abstraction Heuristics

Action
M ove(f, f 0 )
In(p, f )
In(p0 , f )
Out(p, f )
Out(p0 , f )

fe,f fe,f 0 fe,f 00 ip,1 ip0 ,1 F I FI
1
1
1
1
1

1
1
1
1
1

0
1
1
1
1

1
1
0
1
0

1
0
1
0
1

1
2
1
nf
1
nf
1
nf
1
nf

1
1
ni 2+ni
1 nf1+1
1 nf1+1
1 nf1+1
1 nf1+1

Table 7: Number of representatives for each original Miconic action in each abstract task,
as well as the partition of the action costs among these representatives

9.4.1 Lower Bounds
First, as Miconic is a special case of the Logistics domain, Lemma 1 applies here analogously, with each package in P3 corresponding to a passenger. Thus, for each p  P , all
three heuristics account for the full cost of the load/unload actions required by any optimal
plan for that task.
Let us now focus on the abstract tasks F = {fe,f | f  F }. Recall that the task fe,f
is induced by an e-fork and, in terms of domain decomposition, distinguishes between being
at floor f and being somewhere else. Without loss of generality, the set of floors F can be
restricted to the initial and the goal values of the variables, and this because no optimal
plan will move the elevator to or from a floor f that is neither an initial nor a goal location
of a passenger or the elevator. Let FI = {I[p] | p  P } and FG = {G[p] | p  P }. The costs
of the optimal plans for each abstract task fe,f are as follows.
f  FI  FG : Let p, p0  P be a pair of passengers with initial and goal locations in f ,
respectively; that is, I[p] = G[p0 ] = f . If f = I[e], then any plan for fe,f has to move
the elevator from f in order to load passenger p0 , and then move the elevator back
to f in order to unload passenger p0 . Therefore the cost of any plan for fe,f is at
|
least 2|P
|F | + 1, where (see the last three columns of Table 7) the first component of the
summation comes from summing the costs of the representatives of the load/unload
actions for all the passengers, and the second component is the sum of the costs of
representatives of the two respective move actions. Similarly, if f 6= I[e], then any
plan for fe,f has to move the elevator to f in order to load passenger p, and then
move the elevator from f in order to unload p. Therefore, here as well, the cost of
|
any plan for fe,f is at least 2|P
|F | + 1.

f  FI \ FG : Let p  P be a passenger initially at f , that is, I[p] = f . If f = I[e], then
any plan for fe,f has to move the elevator from f in order to unload p, and thus the
cost of any plan for fe,f is at least

2|P |
|F |

+ 12 . Otherwise, if f 6= I[e], then any plan

for fe,f has to move the elevator to f in order to load p, and then move the elevator
from f in order to unload p. Hence, in this case, the cost of any plan for fe,f is at
least

2|P |
|F |

+ 1.
99

fiKatz & Domshlak

f  FG \ FI : Let p  P be a passenger who must arrive at floor f , that is, G[p] = f . If
f = I[e], then any plan for fe,f has to move the elevator from f in order to load p,
and then move the elevator back to f in order to unload p. Hence, here as well, the
|
cost of any plan for fe,f is at least 2|P
|F | + 1. Otherwise, if f 6= I[e], then any plan for

fe,f has to move the elevator to f in order to unload p, and thus the cost of any plan

for fe,f is at least

2|P |
|F |

+ 12 .

f 6 FG  FI : If f = I[e], then any plan for fe,f has to include a move from f in order to

|
1
load/unload the passengers, and thus the cost of any plan for fe,f is at least 2|P
|F | + 2 .
Otherwise, if f 6= I[e], the elevator is initially in the set of all other locations, and
|
thus the cost of any plan for fe,f is at least 2|P
|F | .

Putting this case-by-case analysis together, we have

|FG \FI |

,
I[e]  FI  FG
2|P | + |FI  FG | + |FI \ FG | +
2


2|P | + |F  F | + |F \ F |  1 + 1 + |FG \FI | , I[e]  F \ F
I
G
I
G
I
G
2
2
.
hF (I) 
|FG \FI |1

2|P
|
+
|F

F
|
+
|F
\
F
|
+
1
+
,
I[e]

F
\
F
I
G
I
G
G
I

2


2|P | + |F  F | + |F \ F | + |FG \FI |1 + 1 ,
I[e] 6 FG  FI
I
G
I
G
2
2

Note that the value in the second case is the lowest. This gives us a lower bound on the hF
estimate as in Eq. 22.
|FG \ FI |
1
+ |FI  FG |  .
(22)
2
2
Now, let us provide an upper bound on the length (= cost) of the optimal plan for a
Miconic task. First, let P 0  P denote the set of passengers with both initial and goal
locations in FI  FG . Let m(P 0 , FI  FG ) denote the length of the optimal traversal of the
floors FI  FG such that, for each passenger p  P 0 , a visit of I[p] comes before some visit of
G[p]. Given that, on a case-by-case basis, a (not necessarily optimal) plan for the Miconic
task at hand is as follows.
hF (I)  2|P | + |FI \ FG | +

I[e]  FI  FG : Collect all the passengers at I[e] if any, then traverse all the floors in
FI \ FG and collect passengers from these floors, then move the elevator to the first
floor f on the optimal path  traversing the floors FI  FG , drop off the passengers
whose destination is f , collect the new passengers if any, keep moving along  while
collecting and dropping off passengers at their initial and target floors, and then
traverse FG \ FI , dropping off the remaining passengers at their destinations. The
cost of such a plan (and thus of the optimal plan) is upper-bounded as in Eq. 23
below.
h (I)  2|P | + |FI \ FG | + m(P 0 , FI  FG ) + |FG \ FI |.
(23)
I[e]  FI \ FG : Collect all the passengers at I[e] if any, then traverse all the floors in
FI \ FG and collect passengers from these floors while making sure that this traversal
ends up at the first floor f of the optimal path  traversing the floors FI  FG , then
follow  while collecting and dropping passengers off at their initial and target floors,
and then traverse FG \ FI , dropping the remaining passengers off at their destinations.
As in the first case, the cost of such a plan is upper-bounded as in Eq. 23.
100

fiImplicit Abstraction Heuristics

I[e] 6 FI : Traverse the floors FI \ FG and collect all the passengers from these floors, then
move along the optimal path  traversing the floors FI  FG while collecting and
dropping off passengers at their initial and target floors, and then traverse the floors
FG \ FI , dropping the remaining passengers off at their destinations. Here as well, the
cost of such a plan is upper-bounded by the expression in Eq. 23.
Lemma 2 For any Miconic task with passengers P , we have

hF (I)
h (I)



5|P |1
6|P | .

Proof: Recall that P 0  P is the set of all passengers with both initial and goal locations
in FI  FG . First we give two upper bounds on the length of the optimal traversal of the
floors FI  FG such that, for each passenger p  P 0 , a visit of I[p] comes before some visit
of G[p]. From Theorem 5.3.3 of Helmert (2008) we have
m(P 0 , FI  FG ) = |FI  FG | + m (G 0 ),

(24)

where m (G 0 ) is the size of the minimum feedback vertex set of the directed graph G 0 =
(V 0 , E 0 ), with V 0 = FI  FG and E 0 containing an arc from f to f 0 if and only if a passenger
p  P 0 is initially at floor f and should arrive at floor f 0 .
Note that m (G 0 ) is trivially bounded by the number of graph nodes V 0 . In addition,
observe that, for any order of the nodes V 0 , the arcs E 0 can be partitioned into forward and
0
backward arcs, and one of these subsets must contain no more than |E2 | arcs. Removing
from G 0 all the nodes that are origins of the arcs in that smaller subset of E 0 results in a
directed acyclic graph. Hence, the set of removed nodes is a (not necessarily minimum)
0
feedback vertex set of G 0 , and the size of this set is no larger than |E2 | . Putting these two
bounds on m (G 0 ) together with Eq. 24 we obtain


|P 0 |
0
m(P , FI  FG )  min 2|FI  FG |, |FI  FG | +
.
(25)
2
From the disjointness of FG \ FI and FI  FG , and the fact that the goal of all the
passengers in P 0 is in FI , we have |FG \ FI |  |P |  |P 0 |. From Eqs. 22 and 23 we have
2|P | + |FI \ FG | + |FG2\FI | + |FI  FG |  12
hF

.
h
2|P | + |FI \ FG | + |FG \ FI | + m(P 0 , FI  FG )

(26)

F

As we are interested in a lower bound on the ratio hh , the right-hand side of the
inequality should be minimized, and thus we can safely set |FI \ FG | = 0 and |FG \ FI | =
|P |  |P 0 |, obtaining
0

|
2|P | + |P ||P
+ |FI  FG |  12
hF
5|P |  |P 0 | + 2|FI  FG |  1
2

=
.
h
2|P | + |P |  |P 0 | + m(P 0 , FI  FG )
6|P |  2|P 0 | + 2m(P 0 , FI  FG )

(27)

Let us examine the right-most expression in Eq. 27 with respect to the two upper bounds
on m(P 0 , FI  FG ) as in Eq. 25.

 If the minimum is obtained on 2|FI  FG |, then m(P 0 , FI  FG )  2|FI  FG | 
0
|FI  FG | + |P2 | , where the last inequality can be reformulated as
2|FI  FG |  |P 0 |  0.
101

fiKatz & Domshlak

This allows us to provide a lower bound on the right-most expression in Eq. 27, and
F
thus on hh as
hF
5|P |  |P 0 | + 2|FI  FG |  1
5|P | + (2|FI  FG |  |P 0 |)  1
5|P |  1



.
h
6|P |  2|P 0 | + 2m(P 0 , FI  FG )
6|P | + 2(2|FI  FG |  |P 0 |)
6|P |
(28)
0

0

 If the minimum is obtained on |FI FG |+ |P2 | , then m(P 0 , FI FG )  |FI FG |+ |P2 | <
2|FI  FG |, where the last inequality can be reformulated as
2|FI  FG |  |P 0 | > 0.
This again allows us to provide a lower bound on

hF
h

via Eq. 27 as

5|P |  |P 0 | + 2|FI  FG |  1
5|P | + (2|FI  FG |  |P 0 |)  1
5|P |  1
hF



.

0
0
0
h
6|P |  2|P | + 2m(P , FI  FG )
6|P | + (2|FI  FG |  |P |)
6|P |
(29)
Note that both lower bounds on
the lemma.

hF
h

in Eq. 28 and Eq. 29 are as required by the claim of


9.4.2 Upper Bounds
A Miconic task on which the heuristic hF achieves the performance ratio of exactly 5/6
consists of an elevator e, floors F = {fi }ni=0 , passengers P = {pi }ni=1 , all the passengers and
the elevator being initially at f0 , and the target floors of the passengers all being pairwise
disjoint. The sas+ encoding for the Miconic task is as follows.
 Variables V = {e}  P with the domains D(e) = F and p  P : D(p) = F  {e}.
 Initial state I = {e : f0 , p1 : f0 , . . . , pn : f0 }.
 Goal G = {p1 : f1 , . . . , pn : fn }.
 Actions A = {In(p, f ), Out(p, f ) | f  F, p  P }  {M ove(f, f 0 ) | {f, f 0 }  F }.
The causal graph of this task and the corresponding collection of v-forks (consisting of
only one e-fork) are depicted in Figure 15. The domain of e is abstracted as in Eq. 16
(leave-one-out), providing us with
F = {fe,f0 , fe,f1 , . . . , fe,fn }.
The costs of the action representatives in these abstract tasks are given in Table 7 with
nf = n + 1. The optimal plans for the abstract tasks in F are
task optimal plan
fe,f0
fe,f1
fe,fn

cost

hIn(p1 , f0 ), . . . , In(pn , f0 ), M ove(f0 , f1 ), Out(p1 , f1 ), . . . , Out(pn , fn )i
hIn(p1 , f0 ), . . . , In(pn , f0 ), Out(p2 , f2 ), . . . , Out(pn , fn ), M ove(f0 , f1 ), Out(p1 , f1 )i
hIn(p1 , f0 ), . . . , In(pn , f0 ), Out(p1 , f1 ), . . . , Out(pn1 , fn1 ), M ove(f0 , fn ), Out(pn , fn )i

102

1
2
1
2
1
2

+
+
+

2n
n+1
2n
n+1
2n
n+1

#

hF (I)

n+1

5n+1
2

fiImplicit Abstraction Heuristics

while an optimal plan for the original task, hIn(p1 , f0 ), . . . , In(pn , f0 ), M ove(f0 , f1 ), Out(p1 , f1 ),
has a cost of 3n, providing us with the
F
upper bound of 5/6 for the h heuristic in Miconic. Putting this upper bound together with
the previously obtained lower bound of 5/6, we conclude that the asymptotic performance
ratio of hF in Miconic is 5/6.
A Miconic task on which the heuristics hI and hFI achieve exactly 1/2 consists of an
n
elevator e, floors F = {fi }2n
i=0 , passengers P = {pi }i=1 , and the initial and target floors for
all the passengers and the elevator being pairwise disjoint. The task description in sas+ is
as follows.
M ove(f1 , f2 ), Out(p2 , f2 ), M ove(f2 , f3 ), . . . , Out(pn , fn )i,

 Variables V = {e}  P with the domains D(e) = F and p  P : D(p) = F  {e}.
 Initial state I = {e : f0 , p1 : f1 , . . . , pn : fn }.
 Goal G = {p1 : fn+1 , . . . , pn : f2n }.
 Actions A = {In(p, f ), Out(p, f ) | f  F, p  P }  {M ove(f, f 0 ) | {f, f 0 }  F }.
The causal graph of this task and the corresponding collection of v-forks and v-iforks are
depicted in Figure 15. The domains of the inverted-fork sinks are all abstracted as in Eq. 15
(distance-from-initial-value), and the domains of the fork roots are all abstracted as in
Eq. 16 (leave-one-out). This provides us with
I = {ip1 ,1 , . . . , ipn ,1 },

FI = {fe,f0 , fe,f1 , . . . , fe,fn , fe,fn+1 , . . . , fe,f2n , ip1 ,1 , . . . , ipn ,1 }.
The costs of the action representatives in these abstract tasks are given in Table 7 with
nf = 2n + 1 and ni = n. The optimal plans for the abstract tasks in I and FI are
h

task

optimal plan

hI

ipi ,1
fe,f0

hM ove(f0 , fi ), In(pi , fi ), M ove(fi , fn+i ), Out(pi , fn+i )i
hM ove(f0 , f1 ), In(p1 , f1 ), . . . , In(pn , fn ),
Out(p1 , fn+1 ), . . . , Out(pn , f2n )i
hM ove(f0 , f1 ), In(p1 , f1 ), M ove(f1 , f2 ), In(p2 , f2 ), . . . , In(pn , fn ),
Out(p1 , fn+1 ), . . . , Out(pn , f2n )i
hM ove(f0 , fn ), In(pn , fn ), M ove(fn , f1 ),
In(p1 , f1 ), . . . , In(pn1 , fn1 ), Out(p1 , fn+1 ), . . . , Out(pn , f2n )i
hIn(p1 , f1 ), . . . , In(pn , fn ), Out(p2 , fn+2 ), . . . , Out(pn , f2n ),
M ove(f0 , fn+1 ), Out(p1 , fn+1 )i
hIn(p1 , f1 ), . . . , In(pn , fn ), Out(p1 , fn+1 ), . . . , Out(pn1 , f2n1 ),
M ove(f0 , f2n ), Out(pn , f2n )i
hM ove(f0 , fi ), In(pi , fi ), M ove(fi , fn+i ), Out(pi , fn+i )i

hFI

fe,f1
fe,fn
fe,f

n+1

fe,f2n
ipi ,1

cost

#

h(I)

n
1

2n + 2

1
n+2

+2
2n
+ 2n+2

2
n+2

+

2n
2n+2

n

2
n+2

+

2n
2n+2

1
n+2

+

2n
2n+2

1
n+2

+

2n
2n+2

2
n+2

+

2
2n+2

2
n

2n +

5n+1
n+2

n

n

while an optimal plan for the original task, hM ove(f0 , f1 ), In(p1 , f1 ), M ove(f1 , f2 ), In(p2 , f2 ),
M ove(f2 , f3 ), . . . , In(pn , fn ), M ove(fn , fn+1 ), Out(p1 , fn+1 ), M ove(fn+1 , fn+2 ), Out(p2 , fn+2 ),
M ove(fn+2 , fn+3 ), . . . , Out(pn , f2n )i, has the cost of 4n, providing us with the upper bound of
1/2 for the hI and hFI heuristics in Miconic. Putting this upper bound together with the
previously obtained lower bound of 1/2, we conclude that the asymptotic performance ratio
of hI and hFI in Miconic is 1/2.

103

fiKatz & Domshlak

9.5 Satellite
The Satellite domain is quite complex. A Satellite tasks
S consists of some satellites S,
each s  S with a finite set of instruments Is onboard, I = sS Is . There is a set of image
modes M, and for each mode m  M, there is a set Im  I of instruments supporting
mode m. Likewise, there is a set of directions L, image objectives O  LM, and functions
cal : I 7 L, p0 : S 7 L, and p : S0 7 L with S0  S, where cal is the calibration target
direction function, p0 is the initial direction function, and p is the goal pointing direction
function.
Let us denote by Oi = {o = (d,Sm)  O | i  Im } the subset of all images that can
be taken by instrument i, by Os = iIs Oi the subset of all images that can be taken by
instruments on satellite s, and by Sm = {s | Is  Im 6= } the subset of all satellites that
can take images in mode m. The problem description in sas+ is as follows.
 Variables V = S  {Oni , Ci | i  I}  O with domains
s  S : D(s) = L,

i  I : D(Oni ) = D(Ci ) = {0, 1},
o  O : D(o) = {0, 1}.

 Initial state I = {s : p0 (s) | s  S}  {Oni : 0, Ci : 0 | i  I}  {o : 0 | o  O}.
 Goal G = {s : p (s) | s  S0 }  {o : 1 | o  O}.
 Actions
[

A=
{T urn(s, d, d0 ) | {d, d0 }  L}  {SwOn(i, s), Cal(i, s), SwOf f (i) | i  Is } 
sS

{T akeIm(o, d, s, i) | o = (d, m)  O, s  Sm , i  Im  Is },
where
 turn satellite: T urn(s, d, d0 ) = h{s : d}, {s : d0 }i,

 power on instrument: SwOn(i, s) = h{Oni0 : 0 | i0  Is }, {Oni : 1}i,
 power off instrument: SwOf f (i) = h{Oni : 1}, {Oni : 0, Ci : 0}i,

 calibrate instrument: Cal(i, s) = h{Ci : 0, Oni : 1, s : cal(i)}, {Ci : 1}i, and
 take an image: T akeIm(o, d, s, i) = h{o : 0, Ci : 1, s : d}, {o : 1}i.

9.5.1 Fork Decomposition
The causal graph of an example Satellite task and a representative subset of the collection
of v-forks and v-iforks are depicted in Figure 16. Since the variables {Oni , Ci | i  I}S \S0
have no goal value, the collection of v-forks and v-iforks will be as follows in the general
case.
 For each satellite s  S, an s-fork with the leaves Os .
104

fiImplicit Abstraction Heuristics

o1

o2

o3

o4

s1

C3

s2

s2

C1

C2

C4

On1

On2

On3

On4

C5

C7

o1

C6

o3

C5
o4

Gsf 2
s1

o3

C6
o1

f
GC
5
s2

C2

C7
o3

f
GC
6
C4

o4
f
GC
7

C7

On7
o4
On5

On6

(a)

Goi 4
(b)

Figure 16: Satellite example task (a) causal graph and (b) a representative subset of the
collection of v-forks and v-iforks

 For each instrument i  I, a Ci -fork with the leaves Oi .
 For each image objective o = (d, m)  O, a o-ifork with the parents {Ci | i  Im }Sm .
The root domains of all forks rooted at instruments i  I and of all the inverted-fork sinks
are binary in the first place, and the root domains of the forks rooted at satellites s  S are
abstracted as in Eq. 16 (leave-one-out). This provides us with
F = {fs,d | s  S, d  L}  {fCi | i  I},
I = {io | o  O},

FI = {fs,d | s  S, d  L}  {fCi | i  I}  {io | o  O}.
The total number of forks is thus nf = |S|  |L| + |I| and the total number of inverted
forks is ni = |O|. For each action a  A, the number of its representatives in each abstract
task, as well as the cost assigned to each such representative, are given in Figure 17.
9.5.2 Lower Bounds
First, note that any optimal plan for a Satellite task contains at most 6 actions per image
objective o  O and one action per satellite s  S0 such that I[s] 6= G[s]. Now we show
that each of the three heuristics fully account for the cost of at least one action per image
objective o  O and one action per such a satellite. This will provide us with the lower
bound of 1/6 on the asymptotic performance ratios of our three heuristics.
Lemma 3 For any Satellite task, hF , hI , and hFI fully account for the cost of at least
one Take Image action T akeIm(o, d, s, i) for each image objective o  O.
Proof: For an image objective o = (d, m)  O, some actions T akeIm(o, d, s, i) = h{o :
0, Ci : 1, s : d}, {o : 1}i will appear in optimal plans for |Sm |  |L| fork abstract tasks rooted
105

fiKatz & Domshlak

fs,d

Action
0

T urn(s, d, d )
SwOn(i, s)
Cal(i, s)
SwOf f (i)

1
0
0
0

fs,d0
1
0
0
0

fs,d00

fs0 ,d

0
0
0
0

0
0
0
0

fCi
0
0
1
1

fCi0
0
0
0
0

o  Oi o  Os \ Oi o 6 Os
io
io
io
F I
1
0
1
1

1
0
0
0

1
2

0
0
0
0

0
1
1

1
|O s |

FI
1
|O s |+2

0

0

1
|Oi |
1
|Oi |

1
|Oi |+1
1
|Oi |+1

(a)

Action
T akeIm(o, d, s, i),
o = (d, m)

s0  Sm s0 6 Sm i0  Im i0 6 Im
fs0 ,d0
fs0 ,d0
fCi0
fCi0
io io0
1

0

1

0

1

0

F

I

FI

1
|Sm ||L|+|Im |

1

1
|Sm ||L|+|Im |+1

(b)
Figure 17: Number of representatives for each original Satellite action in each abstract
task, as well as the partition of the action costs between these representatives;
table (a) shows Turn, Switch On, Switch Off, and Calibrate actions, and
table (b) shows Take Image actions

in satellites, |Im | fork abstract tasks rooted in instrument calibration status variables Ci ,
and one inverted-fork abstract task with sink o. Together with the costs of the action
representatives in the abstract problems (see Figure 17), we have
hF : cost of each representative is
tasks,

1
|Sm ||L|+|Im |

and there are |Sm |  |L| + |Im | fork abstract

hI : cost of each representative is 1 and there is one inverted fork abstract task, and
hFI : cost of each representative is
tasks.

1
|Sm ||L|+|Im |+1

and there are |Sm |  |L| + |Im | + 1 abstract

Therefore, for each o  O, the cost of one T akeIm(o, d, s, i) action will be fully accounted
for by each of the three heuristics.

Lemma 4 For any Satellite task, hF , hI , and hFI fully account for the cost of at least
one Turn action T urn(s, d, d0 ) for each s  S0 such that I[s] 6= G[s].
Proof: If s  S0 is a satellite with I[s] 6= G[s], then an action T urn(s, I[s], d0 ) will appear
in any optimal plan for fs,I[s] , an action T urn(s, d, G[s]) will appear in any optimal plan
for fs,G[s] , and for each o  Os , an action T urn(s, d, G[s]) will appear in any optimal plan
for io . Together with the costs of the action representatives in the abstract problems (see
Figure 17) we have
hF : cost of each representative is

1
2

and there are 2 fork abstract tasks,
106

fiImplicit Abstraction Heuristics

hI : cost of each representative is
hFI : cost of each representative is

1
|Os |

and there are |Os | inverted fork abstract tasks, and

1
|Os |+2

and there are |Os | + 2 abstract tasks.

Therefore, for each s  S0 such that I[s] 6= G[s], the cost of one T urn(s, d, d0 ) action will
be fully accounted for by each of the three heuristics.

h
h

Together, Lemmas 3 and 4 imply that, for h  {hF , hI , hFI }, on Satellite we have
 1/6.

9.5.3 Upper Bound
A Satellite task on which all three heuristics achieve the ratio of exactly 1/6 consists of
two identical satellites S = {s, s0 } with l instruments each, I = Is  Is0 = {1, . . . , l}  {l +
1, . . . , 2l}, such that instruments {i, l+i} have two modes each: m0 and mi . There is a set of
n + 1 directions L = {dI , d1 , . . . , dn } and a set of n image objectives O = {o1 , . . . , on }, oi =
(dI , mi ) for 1  i  l and oi = (di , m0 ) for l < i  n. The calibration direction of
instruments {i, l + i} is di . The sas+ encoding for this planning task is as follows.
 Variables V = S  O  {Oni , Ci | i  I}.
 Initial state I = {s : dI | s  S}  {Oni : 0, Ci : 0 | i  I}  {o : 0 | o  O}.
 Goal G = {o : 1 | o  O}.
 Actions
[

A=
{T urn(s, d, d0 ) | {d, d0 }  L}  {SwOn(i, s), Cal(i, s), SwOf f (i) | i  Is } 
sS


[
sS

{T akeIm((dI , mi ), dI , s, i) | i  Is } 

n
[


{T akeIm((dj , m0 ), dj , s, i) | i  Is } .

j=l+1

The causal graph of this task is depicted in Figure 18a. The state variables {Oni , Ci |
i  I}  S have no goal value, and thus the collection of v-forks and v-iforks for this task
is as in Figure 18b. The domains of the inverted-fork sinks are binary, and the domains of
the fork roots are abstracted as in Eq. 16 (leave-one-out). This provides us with
F = {fs,d , fs0 ,d | d  L}  {fCi | i  I},
I = {io | o  O},

FI = {fs,d , fs0 ,d | d  L}  {fCi | i  I}  {io | o  O}.
The total number of forks in this task is nf = 2n + 2l + 2 and the total number of inverted
forks is ni = n. The costs of the action representatives in each abstract task are given in
0
Figure 17, where |Os | = |Os | = |O| = n, |Oi | = n  l + 1, |Sm | = 2, |Im0 | = 2l, |Imi | = 2,
and |L| = n + 1.
The optimal plans per abstract task are depicted in Table 8, while an optimal plan for
the original problem, hSwOn(1, s), T urn(s, dI , d1 ), Cal(1, s), T urn(s, d1 , dI ), T akeIm(o1 , dI , s, 1),
107

fiKatz & Domshlak

s0

s
o1

C1

ok

oi

Cl+1

Cl

Ci

ol+1

s

s0

o1 . . . on

o1 . . . on

Gsf

Gsf 0

Cl+i

C2l
s

Oni

On1

Ci

on

s0

Ci

oi

ol+1 . . . on
f
GC
,i  I
i

s

Cl+i

s0

C1 . . . C2l

Onl+i

Onl

Onl+1

On2l

oi

oi

Goi i , 1  i  l

Goi i , l < i  n

(a)

(b)

Figure 18: (a) Causal graph and (b) the corresponding collection of v-forks and v-iforks for
the Satellite task used in the proof of the upper bound of 1/6

h

hF

hI

hFI

task

optimal plan

cost

hT akeIm(o1 , dI , s0 , l+1), . . . , T akeIm(ol , dI , s0 , 2l),
fs,d
T akeIm(ol+1 , dl+1 , s0 , 2l), . . . , T akeIm(on , dn , s0 , 2l)i
hT akeIm(o1 , dI , s, 1), . . . , T akeIm(ol , dI , s, l),
f
s0 ,d
T akeIm(ol+1 , dl+1 , s, l), . . . , T akeIm(on , dn , s, l)i
hT akeIm(oi , dI , s0 , l + i),
f
 Ci , i  I s
T akeIm(ol+1 , dl+1 , s0 , 2l), . . . , T akeIm(on , dn , s0 , 2l)i
hT akeIm(oi , dI , s, i),
fCi , i  Is0
T akeIm(ol+1 , dl+1 , s, l), . . . , T akeIm(on , dn , s, l)i
ioj , 1  j  l hT urn(s, dI , dj ), Cal(j, s), T urn(s, dj , dI ), T akeIm(oj , dI , s, j)i
ioj , l < j  n hT urn(s, dI , d1 ), Cal(1, s), T urn(s, d1 , dj ), T akeIm(oj , dI , s, 1)i
hT akeIm(o1 , dI , s0 , l + 1), . . . , T akeIm(ol , dI , s0 , 2l),
fs,d
T akeIm(ol+1 , dl+1 , s0 , 2l), . . . , T akeIm(on , dn , s0 , 2l)i
hT akeIm(o1 , dI , s, 1), . . . , T akeIm(ol , dI , s, l),
f
s0 ,d
T akeIm(ol+1 , dl+1 , s, l), . . . , T akeIm(on , dn , s, l)i
hT akeIm(oi , dI , s0 , l + i),
f
 Ci , i  I s
T akeIm(ol+1 , dl+1 , s0 , 2l), . . . , T akeIm(on , dn , s0 , 2l)i
hT akeIm(oi , dI , s, i),
fCi , i  Is0
T akeIm(ol+1 , dl+1 , s, l), . . . , T akeIm(on , dn , s, l)i
ioj , 1  j  l hT urn(s, dI , dj ), Cal(j, s), T urn(s, dj , dI ), T akeIm(oj , dI , s, j)i
ioj , l < j  n hT urn(s, dI , d1 ), Cal(1, s), T urn(s, d1 , dj ), T akeIm(oj , dI , s, 1)i

#

l
2n+4

+

nl
2n+2l+2

n+1

l
2n+4

+

nl
2n+2l+2

n+1

1
2n+4

+

nl
2n+2l+2

l

nl
1
+ 2n+2l+2
2n+4
2
1
+ nl+1
+1
n
2
1
+
+1
n
nl+1

l
l
nl

h(I)

n

l
2n+5

+

nl
2n+2l+3

n+1

l
2n+5

+

nl
2n+2l+3

n+1

1
2n+5

+

nl
2n+2l+3

l

nl
1
+ 2n+2l+3
2n+5
2
1
+ nl+2 +
n+2
1
2n+5
2
1
+ nl+2
+
n+2
1
2n+2l+3

l

n + 2+
n
nl+1

n+

l

2n
n+2

+

n
nl+2

nl

Table 8: Optimal plans for the abstract tasks and the overall heuristic estimates for the
Satellite task used in the proof of the upper bound of 1/6

SwOf f (1), . . . SwOn(l  1, s), T urn(s, dI , dl1 ), Cal(l  1, s), T urn(s, dl1 , dI ), T akeIm(ol1 , dI , s, l  1),
SwOf f (l  1), SwOn(l, s), T urn(s, dI , dl ), Cal(l, s), T urn(s, dl , dI ), T akeIm(ol , dI , s, l), T urn(s, dI , dl+1 ),
T akeIm(ol+1 , dl+1 , s, l), . . . , T urn(s, dn1 , dn ), T akeIm(on , dn , s, l)i,

108

has the cost of 4l + 2n  1. For

fiImplicit Abstraction Heuristics


l = n  n, this provides us with the asymptotic performance ratio of 1/6 for all three
heuristics.

10. Summary
We considered heuristic search for cost-optimal planning and introduced a domain-independent
framework for devising admissible heuristics using additive implicit abstractions. Each such
implicit abstraction corresponds to abstracting the planning task at hand by an instance of a
tractable fragment of optimal planning. The key motivation for our investigation was to escape the restriction of explicit abstractions, such as pattern-database and merge-and-shrink
abstractions, to abstract spaces of a fixed size. We presented a concrete scheme for additive
implicit abstractions by decomposing the planning task along its causal graph and suggested
a concrete realization of this idea, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. We then studied the induced admissible heuristics
both formally and empirically, and showed that they favorably compete in informativeness
with the state-of-the-art admissible heuristics both in theory and in practice. Our empirical
evaluation stressed the tradeoff between the accuracy of the heuristics and runtime complexity of computing them. To alleviate the problem of expensive per-search-node runtime
complexity of fork-decomposition heuristics, we showed that an equivalent of the explicit
abstractions notion of database exists also for the fork-decomposition abstractions, and
this despite their exponential-size abstract spaces. Our subsequent empirical evaluation of
heuristic search with such databases for the fork-decomposition heuristics showed that it
favorably competes with the state of the art of cost-optimal planning.
The basic principles of the implicit abstraction framework motivate further research
in numerous directions, most importantly in (i) discovering new islands of tractability of
optimal planning, and (ii) abstracting the general planning tasks into such islands. Likewise, there is promise in combining implicit abstractions with other techniques for deriving admissible heuristic estimates. A first step towards combining implicit abstractions
with polynomial-time discoverable landmarks of the planning tasks has recently been taken
by Domshlak, Katz, and Lefler (2010). We believe that various combinations of such techniques might well improve the informativeness of the heuristics, and this without substantially increasing their runtime complexity.

Acknowledgments
The work of both authors was partly supported by Israel Science Foundation grants 670/07
and 1101/07.

109

fiKatz & Domshlak

Appendix A. Detailed Results of Empirical Evaluation
hF
task

hI

h nodes time nodes

hFI
time nodes

time

MS-104

MS-105

nodes

nodes

time

HSPF

time

nodes

9
0.00
10
0.00
18
0.03
21
0.01
22
0.01
42
0.17
42
0.17
96231549.13
89525 466.14
19
0.01
22
0.01
40
0.21
38
0.21
8968 238.16
8931 267.81
3053401077.90

9
10
29
21
22
42
42
203
12956
19
22
40
38
62
59

time

blind
nodes time

hmax
nodes time

airport-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
19
21
22
36

8
9
17
20
21
41
41
62
71
18
21
39
37
60
58
79
88
90
101
148
109

10 0.01
12 0.03
86 0.25
22 0.02
23 1.29
51336.72
51437.00

9 0.00
15 0.01
133 0.07
21 0.02
30 0.06
639 1.54
632 1.53
21544166.51

9 0.00
15 0.03
93 0.31
21 0.02
27 1.43
567 45.25
550 44.15

19 0.02
23 1.90
47554.18
43447.48

19 0.02
30 0.08
728 2.76
663 2.60
25110334.72
23317307.60

19 0.03
27 2.13
568 71.23
479 59.82

9 0.00
10 0.00
18 0.04
21 0.02
22 0.01
42 0.16
42 0.17
24372 25.42
152408 64.92
19 0.02
22 0.02
40 0.21
38 0.20
30637 51.23
28798 46.20
1031524200.95

7326372.92
1119943762.02
34365853.70

102

10.28

0.72
11 0.00
9 0.00
1.23
13 0.00
10 0.00
5.10
164 0.00
57 0.00
1.32
23 0.00
21 0.00
46.54
27 0.00
22 0.00
123.13
738 0.01
418 0.02
117.56
742 0.01
405 0.02
602.09 27032 0.28
9687 0.90
993.07 175717 2.47 56484 7.62
2.45
21 0.00
19 0.00
65.36
27 0.00
22 0.01
169.02
873 0.01
392 0.03
134.87
822 0.01
342 0.03
714.76 35384 0.39
9196 1.11
647.05 33798 0.38
8200 1.01
124746719.72 221993 49.03
1043661310.89
831632253.21
18809 0.42
3184 1.12
159967105.29
63061 1.44

blocks-ipc2
04-0
04-1
04-2
05-0
05-1
05-2
06-0
06-1
06-2
07-0
07-1
07-2
08-0
08-1
08-2
09-0
09-1
09-2
10-0
10-1
10-2
11-0
11-1
11-2
12-0
12-1
13-0
13-1
14-0
14-1

6
15 0.01
46 0.01
17 0.01
7 0.03
7
0.03
7
0.36
93 0.00
25 0.00
10
14 0.01
31 0.00
15 0.00
11 0.04
11
0.03
11
0.39
66 0.00
23 0.00
6
7 0.01
26 0.00
10 0.00
7 0.04
7
0.03
7
0.38
63 0.00
18 0.00
12
32 0.03
302 0.06
113 0.08
13 0.30
13
0.96
13
1.32
467 0.00
145 0.00
10
37 0.03
280 0.06
98 0.07
11 0.29
11
0.96
11
1.36
567 0.00
135 0.00
16
152 0.09
596 0.10
348 0.18
17 0.29
17
0.95
17
1.49
792 0.00
297 0.00
12
33 0.04
766 0.27
207 0.25
13 0.95
13
8.56
13
4.10
1826 0.00
276 0.00
10
41 0.07 2395 0.74
578 0.78
11 0.90
11
8.34
11
4.17
4887 0.01
755 0.01
20
855 0.80 5444 1.23 3352 2.88
733 0.87
85
8.84
31
4.29
6385 0.02
2556 0.03
20
278 0.56 20183 8.26 4022 8.18
577 1.93
144 23.32
22 11.47 37157 0.14
5943 0.11
22 691011.22 59207 17.37 38539 49.71
10071 1.70
1835 21.05
174 11.25 63376 0.21 33194 0.46
20 1458 2.85 46009 15.05 18854 29.61
1855 1.59
782 20.37
90 10.99 55218 0.19 18293 0.29
18 1533 4.79344157179.42 69830208.07
5557 3.67
678 36.80
25 26.00 519107 2.28 94671 2.07
20 1004027.97517514236.64191352475.33
45711 3.88
11827 33.49
151 26.57 636498 2.60 199901 3.85
16
479 1.79237140136.18 32567110.76
277 3.63
54 32.53
17 25.85 433144 1.93 52717 1.30
30
1233374 16.00 971409 77.74
464 56.76798464936.763840589 85.00
28 343518.17
95068 7.35
58873 63.15
82 56.98591457229.731200345 32.06
26 637935.22
161719 13.54
20050 82.45
81 57.02596316030.021211463 32.15
34
1800 114.26
32
12063665 228.76
1835 115.19
34
3685 116.75
32
7046739 141.44
2678 213.32
30
1510 203.79
34
3984 213.97
34
1184 370.06
34
614 382.34
42
83996 860.45
44
1634381104.27
38
27791063.02
36
71541087.40

depots-ipc3
01
02
03
04
07
10
13

10
15
27
30
21
24
25

114 0.24
113410.82

279 0.11
9344 12.40

161 0.32
2638 22.68

11 0.00
11
0.00
45
0.77
329 0.00
136 0.00
738 3.24
16
1.14
898 11.56 15404 0.11
3771 0.17
348288 20.69 239313 222.35 103089 247.13293039827.201204646 97.62
1284048 52.05 1273762 529.34
211820 37.54
41328 324.19650110071.581331701166.76
3241083157.52
1427824116.06

grid-ipc1
01
02

14
26

57160.28

1117

9.49

472 55.87

660 8.63
467 121.10
3392724 50.35 3244132 241.94

6446 0.08

190 0.10
664016231.26

Table 9: Runtimes of cost-optimal heuristic-search planners on the Airport,
Blocksworld, Depots, and Grid domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are computed
fully online. Column task denotes problem instance, column h denotes optimal
solution length. Other columns capture the run time and number of expanded
nodes.

110

fiImplicit Abstraction Heuristics

hF
task

h

nodes

hI

hFI

time

nodes

time nodes

time

0.05
18.27
0.25
19.15
45.02
5.21
9.56

37
18452
190
10778
11400
795
1730

0.01
37
10.29 15794
0.13
163
17.14 7665
18.91 10984
3.60
492
7.71 1006

0.04
23.80
0.31
29.88
46.16
6.05
13.80

HSPF

MS-104

MS-105

nodes

time

nodes

time

nodes

8 0.04
20 0.13
13 0.16
17 0.49
2614 0.60
291 1.35
14 1.42
287823 7.34
15504 1.70
18 1.64
34137 1.99
1298884 19.52

8
20
13
17
19
12
14
2952
23
18
10790
870875

0.03
0.26
0.25
2.41
4.58
9.72
15.35
20.31
10.43
18.54
17.01
35.33

44
15998
863
22933
24877
3804
25801

blind
time

nodes

time

hmax
nodes
time

driverlog-ipc3
01
02
03
04
05
06
07
08
09
10
11
13

7
19
12
16
18
11
13
22
22
17
19
26

49
15713
164
6161
13640
608
864

4304 199.81
433951421.90

198651 849.04
16099 85.74 4037 200.52
41445 186.53 390691395.51

0.47
182 0.00
20
4.55
68927 0.36
54283
1.25
16031 0.09
2498
12.20 999991 8.12 393673
18.77 6290803 61.57 1724611
10.08 681757 7.64
54451
41.34 6349767 81.53 493480

18234 68.22
5596231193.00

0.00
0.52
0.03
6.56
34.73
1.71
17.31

6141130 330.22

freecell-ipc3
01
02
03
04
05

8
234
1.54
14 30960 107.07
18 197647 877.16
26
30

974
4.88
274
3.25
75150 230.54 37131 224.62

87 3.12
31487 40.40
95805140.96
943074 86.78
5950977243.74

9 38.74

9 13.01
3437 0.03
1043
0.15
466 70.29 130883 1.46
41864 10.77
1589 169.39 944843 11.45 210503 75.62
15848 341.02 3021326 38.80 600525 247.70
40642 916.44
14080351062.25

gripper-ipc1
01
02
03
04
05
06
07

11
214
0.04
240
0.02
214
0.05
12 0.00
12 0.00
33
0.11
236 0.00
208
0.00
17
1768
0.54
1832
0.36 1803
0.75
18 0.11
18 0.08
680
0.37
1826 0.01
1760
0.01
23 11626
5.38 11736
4.05 11689
8.11
11514 0.47
2094 1.75
7370
1.52
11736 0.04
11616
0.08
29 68380 43.58 68558 35.24 68479 70.72
68380 1.24
68190 8.05
55568 10.29
68558 0.27
68368
0.56
35 376510 328.10 376784 296.59376653 560.93
376510 3.52 376510 19.46 344386 79.96 376772 1.59 376496
3.51
41
1982032 13.42 1982032 42.16 1911592 577.49 1982394 9.59 1982016 21.57
47
10091986 61.6610091986106.84
10092464 51.1010091968 119.64

logistics-ipc1
01
05
31
32
33

26
22
13
20
27

3293 945.35
436
9.67
392
2.57

1981
2704

2.53
2.24

1284
962

21.84
5.53

21
0.02
193
0.06
65
20
0.03
570
0.13
293
16
0.02
117
0.03
79
28
0.05
2550
0.98 1171
18
0.03
675
0.19
427
9
0.02
24
0.01
13
26
0.06
4249
1.85 2461
15
0.03
181
0.09
99
26
0.05
2752
1.22 1394
25
0.04
2395
0.94 1428
37
0.42 251287 203.64 98053
1689 10.08
32
0.42 82476 78.73 35805
45
0.6611836081306.92
37
0.54 351538 407.06167038
31
0.50 59336 80.88 25359
46
2.26
43
2.10
697 26.78
21959 696.23
43
2.78

0.06
0.16
0.05
1.09
0.31
0.02
2.54
0.13
1.51
1.34
386.80

1918881 41.03
768161 18.69
494 0.42
21 0.16

949586 34.82
609393 35.27
14 2.11
21 0.72
529338 32.55

2119551700.26
6.58
7.08

155645
245325

1.66
2.07

32282
81156

0.57
1.00

21 0.05
20 0.04
16 0.05
28 0.38
18 0.38
9 0.38
26 1.23
15 1.26
26 1.26
25 1.22
37 4.87
49 4.94
32 6.90
45 7.21
37 9.46
31 9.43
668834 29.73
1457130 43.00
701106 37.42

21
0.34
20
0.37
16
0.36
28
0.58
18
0.72
9
0.78
26
1.03
15
1.16
26
1.03
25
1.02
24317 35.46
362179 453.06
14890 33.50
114155 198.84
32017 83.16
6720 26.48

11246
9249
4955
109525
22307
1031
490207
24881
476661
422557

0.05
0.04
0.02
0.64
0.13
0.00
3.40
0.16
3.32
2.95

4884
4185
1205
74694
6199
280
202229
3604
200012
133521

0.03
0.03
0.01
0.59
0.05
0.00
1.92
0.03
1.98
1.29

3636

0.07

481
9598

logistics-ipc2
04-0
04-1
04-2
05-0
05-1
05-2
06-0
06-1
06-2
06-9
07-0
07-1
08-0
08-1
09-0
09-1
10-0
10-1
11-0
11-1
12-0
12-1

20
19
15
27
17
8
25
14
25
24
36
44
31
44
36
30
45
42
48
60
42
68

161.33
883.68
168.73

21
20
16
28
18
9
26
15
26
25
525
666324
1042
16708
20950
31

0.03
0.03
0.04
0.10
0.10
0.09
0.18
0.18
0.19
0.18
0.65
8.83
0.96
1.15
1.56
1.27

775996 43.56
2222340 87.47

mprime-ipc1
01
02
03
04
05
07
08
09
11
12
15
16
17
19
21
25
26
27
28
29
31
32
34
35

5
7
4
8
11
5
6
8
7
6
6
6
4
6
6
4
6
5
7
4
4
7
4
5

196
0.19
11604 422.83
427 35.09
3836
6.62
3314

14.91

10
0.03
440451620.68
7
0.50
1775
1.17
47

0.15

19838 454.91 1001881798.69
9
0.16
219
0.54
16320 192.10
8118 46.69
252 171.97

24
0.07
2565 242.83
11
3.15
1093
3.44
346

3.07

5227 284.13
8
0.16
5243 95.01
448 447.49

6 2.00
3317 88.58
36 33.64
9 6.09
1705009127.53
1667 46.72
1469752403.45
21993 36.25
8 4.69
34763 11.45

6 20.45
5463.85
9 82.71

108

49.59

19076 781.74

9868 0.67
599590 23.58
18744

8 62.68
42055143.27

22 394.26
25665 724.12

473 81.42

0.56

2197646 71.69
73260 2.21
108652 3.50
425144 32.17
172736 42.48

453 671.03
123039313.25
75

0.10

30

54
2.28
8
0.03
182
4.53
248 52.86
31759 133.33
234 11.65
392
3.09

1772
403
56
46
12436
46
290

0.04

29

0.08

33.82
9
0.23
37
1.11
32
7.83
19
34.94 11839
2.13
23
2.54
84

1.31
0.08
1.79
11.79
95.52
3.08
1.89

5 0.48
172432 46.33
6 11.59
8 1.88
5 14.92
419 99.87
19429 21.61
450151.69
359 3.63

5 2.75
189154454.69
6154.43
8 22.55
5201.40
7269292.37
6 43.43

1503293103.23
383 0.00
819590 61.01
84079 3.50
128 146.80
17333 0.25
3187 0.17
3584 0.19
110731701.00 115479 2.75
3618 0.19
706 96.55
2476 0.05
85

8.71

68
0.04
12606 36.65
5
0.07
200
0.24
14881571638.78
11
0.04
7650 84.33
19023 30.26
915
0.54
1520
1.78
1039 178.55
7962 35.65
5
1.06
36013 533.75
15250 101.75
6
0.00
440
2.69
831
2.08
211
0.06
7
0.10
11
0.17
3096
1.74
11
0.18
44
0.03

Table 10: Similar to Table 9 for the Driverlog, Freecell, Gripper, Logistics-ipc1,
Logistics-ipc2, and Mprime domains.
111

fiKatz & Domshlak

hF
task

h

nodes

hI
time

nodes

hFI
time nodes

time

MS-104

MS-105

nodes

nodes

time

time

HSPF

nodes

blind
time

nodes

time

hmax
nodes time

miconic-strips-ipc2
01-0
01-1
01-2
01-3
01-4
02-0
02-1
02-2
02-3
02-4
03-0
03-1
03-2
03-3
03-4
04-0
04-1
04-2
04-3
04-4
05-0
05-1
05-2
05-3
05-4
06-0
06-1
06-2
06-3
06-4
07-0
07-1
07-2
07-3
07-4
08-0
08-1
08-2
08-3
08-4
09-0
09-1
09-2
09-3
09-4
10-0
10-1
10-2
10-3
10-4
11-0
11-1
11-2
11-3
11-4

4
3
4
4
4
7
7
7
7
7
10
11
10
10
10
14
13
15
15
15
17
17
15
17
18
19
19
20
20
21
23
24
22
22
25
27
27
26
28
27
31
30
30
32
28
33
32
32
34
33
37
34
38
38
35

5 0.00
5
0.00
5 0.00
5 0.00
5
0.00
5 0.00
5 0.00
5
0.00
5 0.00
5 0.00
5
0.00
5 0.00
5 0.00
5
0.00
5 0.00
19 0.00
22
0.00
19 0.00
21 0.00
23
0.00
21 0.00
21 0.00
23
0.00
21 0.00
24 0.01
24
0.00
24 0.00
19 0.00
22
0.00
19 0.00
86 0.01
129
0.01
98 0.01
120 0.01
168
0.01
147 0.01
137 0.01
143
0.01
137 0.01
96 0.01
153
0.01
117 0.01
103 0.01
149
0.01
115 0.01
524 0.06
843
0.08
686 0.12
505 0.06
817
0.08
663 0.12
685 0.08
942
0.09
802 0.13
681 0.07
942
0.09
798 0.13
685 0.07
942
0.09
802 0.13
2468 0.37
4009
0.66 3307 0.93
2807 0.42
4345
0.71 3677 1.01
1596 0.29
2981
0.55 2275 0.73
2256 0.36
3799
0.62 3104 0.87
3210 0.46
4732
0.78 4267 1.11
9379 1.98 17665
4.74 13531 5.90
9106 1.93 18134
4.75 14052 5.94
10900 2.19 19084
4.90 15111 6.28
12127 2.43 21708
5.69 17807 7.19
13784 2.62 23255
5.93 19536 7.66
53662 13.29 96092 37.56 79449 46.76
56328 13.86 99109 38.56 83677 47.49
48141 12.52 96139 38.02 78471 46.17
46867 12.11 93117 36.63 75424 44.43
84250 18.24 126595 46.11111984 61.34
272580 81.51 485051 267.27408114317.78
284415 86.93 527216 288.07446837347.43
207931 66.37 414294 235.89330993271.03
369479104.29 598031 320.33527216392.87
297516 87.65 507910 278.64431432333.91
1461729497.72
1207894438.6923351661787.13
1294691460.1123404111791.16
1840936589.09
1252484467.94

5 0.00
5 0.00
4 0.00
4 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
8 0.00
11 0.00
11 0.00
12 0.00
12 0.00
11 0.00
11 0.00
11 0.00
11 0.00
11 0.00
11 0.00
15 0.01
15 0.01
14 0.01
14 0.01
16 0.01
16 0.01
16 0.01
16 0.01
16 0.01
16 0.01
18 0.06
18 0.05
18 0.06
18 0.05
16 0.06
16 0.05
18 0.06
18 0.05
19 0.06
19 0.05
20 0.18
20 0.32
20 0.18
20 0.32
21 0.18
21 0.32
21 0.17
21 0.32
22 0.17
22 0.32
24 0.32
24 1.75
7001 0.38
25 1.75
1646 0.33
23 1.71
1861 0.33
23 1.74
23159 0.52
26 1.71
41629 0.91
28 4.18
42679 0.90
28 4.25
37744 0.86
27 4.25
140453 1.94
29 4.21
62933 1.16
28 4.12
684737 9.07 126918 8.89
406041 5.61 100937 8.73
442547 6.06 82946 8.63
765455 10.00 277302 11.14
317692 4.65
29 7.03
2436164 35.24 863244 23.76
2340169 34.09 335745 15.68
1735477 25.29 486286 17.72
3952148 55.86 940556 24.24
2715866 39.44 625559 19.91
11473359183.604724980 93.56
7535468124.801934943 47.91
14645785233.686330198120.71
5809711110.10
5853546 95.561082086 32.22

5
0.01
4
0.00
5
0.01
5
0.00
5
0.01
26
0.01
26
0.01
27
0.00
20
0.01
23
0.01
100
0.03
140
0.02
122
0.02
131
0.02
114
0.02
669
0.10
634
0.11
822
0.12
820
0.12
821
0.12
2829
0.44
3260
0.49
1594
0.32
2568
0.42
3953
0.55
9312
1.76
10252
1.96
11247
2.11
14216
2.56
16880
3.04
56686 14.31
63035 16.33
55751 13.98
53121 13.27
96327 24.76
290649 104.18
339177 123.10
204614 73.39
435617 160.49
315339 111.84
1555286 794.93
1344815 683.05
1357681 692.11
20831681051.95
1231554 605.01

5 0.00
5 0.00
5 0.00
4 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
30 0.00
20 0.00
30 0.00
22 0.00
30 0.00
22 0.00
26 0.00
17 0.00
31 0.00
20 0.00
193 0.00
105 0.00
218 0.00
150 0.00
164 0.00
92 0.00
197 0.00
130 0.00
190 0.00
114 0.00
1182 0.00
866 0.00
1176 0.00
860 0.00
1277 0.00
969 0.00
1319 0.00
970 0.00
1334 0.00
969 0.00
6350 0.03
4387 0.03
6602 0.03
4664 0.03
5565 0.03
3524 0.03
5944 0.03
4140 0.03
6949 0.04
5268 0.04
30786 0.20
21194 0.20
30093 0.20
21255 0.20
32390 0.21
21694 0.21
32574 0.21
24552 0.23
33793 0.22
26167 0.24
155466 1.22 116685 1.32
164470 1.29 118494 1.33
161342 1.27 119688 1.36
155176 1.23 114649 1.30
168219 1.33 140128 1.58
755255 7.16 594032 7.95
794365 7.56 636587 8.66
731622 6.92 534711 7.37
833421 7.97 690267 9.29
771608 7.33 613253 8.43
3685552 41.04 3006991 49.12
3649801 40.32 2893803 47.54
3576134 39.61 2895182 47.26
3796035 42.13 3304570 53.29
3589382 39.29 2956995 48.84
15804498200.9013267920250.58
16472633208.3913720664256.89
15867374201.0112497087236.89
16309701208.4213801989262.53
16472551209.1313925654262.57

mystery-ipc1
01
02
03
04
07
09
11
12
15
16
17
18
19
20
24
25
26
27
28
29
30

5
7
4


8
7

6

4

6
7

4
6
5
7
4
9

7 0.01
2404 64.94
73 1.92
0 0.01
0 0.00
3049 47.68
9 0.02

0 0.14
354200.98
0 0.00

0 0.13
9 0.02
1807 50.40
14 0.27
8 0.01
31 0.26

6
0.00
8012 234.10
7
0.12
0
0.00
10764 137.61
33
0.03
2093419 938.05

85 26.31
0
0.00
4968 183.24

10
1835
159
47
14

0.01
25.34
1.61
0.02
0.10

6 0.01
722 47.50
11 0.59
0 0.00
0 0.00
1215 40.75
8 0.02

0 0.19
83 90.17
0 0.00

0 0.30
9 0.02
1344 60.20
6 0.22
15 0.02
10 0.17

6 0.20
1672 82.70
5 16.46

6

1.79

10

5.38

5193.75

65 811.87

0 0.00
0 0.00
3165 29.34
8 1.51
8 16.59
2102777 14.612102729 27.84

0
0.00
3868 670.08
34 41.20

198445.85
0 0.00
0 0.00
12478 96.38
285069 59.22 547246578.39
5 0.10
2526 5.94
6 4.80
8 0.63
5 8.94
42112 28.07

5 0.10
346 70.78
6 80.48
8 6.77
5107.10
44893357.07

0

0.00

14
1.22
3107 291.36
7 243.78
31 16.67
27 536.30

30 0.00
770852 21.85
507 0.02

0.00
4.47
0.03

0 0.00
0 0.00
138289 2.18
1458 1.44
426 0.00
19 0.00
2102777 15.09 1177842 21.87
279973 13.21
135 2.62
5400 0.41
0 0.00
133871 3.65
686125 23.28
31 0.00
8455 0.10
2174 0.03
843 0.00
153 0.01
1977063 38.26

Table 11: Similar to Table 9 for the Miconic and Mystery domains.
112

8
2368
5

5
0
1516
718

0.35
0.00
5.44
3.76

6 0.00
37 0.05
73 0.04
32 0.00
7 0.02
26686 28.27

fiImplicit Abstraction Heuristics

hF
task

h

hI

hFI

MS-104

MS-105

time

nodes

nodes

2264
0.49
3895
1.19
3070
1.36
2617
0.56
4485
1.32
3561
1.57
2264
0.49
3895
1.15
3070
1.36
2264
0.49
3895
1.15
3070
1.36
2264
0.48
3895
1.15
3070
1.35
366768 255.00 7797101599.86 5874821498.20
410728 277.99 7606681546.44 6067821515.46

24
24
24
24
24
621008
594758

nodes

time

nodes

time

nodes

time

time

HSPF

nodes

blind

hmax
nodes
time

time

nodes

time

0.06
2000
1.02
0.06
2378
1.07
0.06
2000
1.02
0.06
2000
1.02
0.05
2000
1.02
7.86 379735 217.37
7.34 405564 226.32

4822
5501
4822
4822
4822
882874
836647

0.01
0.02
0.01
0.02
0.01
4.91
4.62

4016
4594
4016
4016
4016
822514
787163

0.03
0.04
0.03
0.03
0.03
18.71
17.81

1624
2984
87189
456143

0.00
0.02
1.06
8.22

36
348
4346
104068

0.00
0.01
0.16
2.61

openstacks-ipc5
01
02
03
04
05
06
07

23
23
23
23
23
45
46

0.05
24
0.06
24
0.06
24
0.06
24
0.06
24
4.85 279614
4.69 264535

pathways-ipc5
01
02
03
04

6
12
18
17

1624
2755
44928
126950

0.03
0.08
2.59
11.45

1299
2307
20416
33788

0.02
0.06
1.06
2.97

1299
2437
29106
58738

0.03
0.09
2.14
7.07

7
1946
21671

1.14
2.56
6.43

7 0.79
13 42.11
14901129.23
98484288.39

1405
990
14772
34206

0.28
0.29
6.99
27.00

pipesworld-notankage-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
17
19
21
23
24
41

5
12
8
11
8
10
8
10
13
18
20
24
16
30
26
22
24
14
18
24
12

121
0.15
1413
2.05
1742
5.26
7007 24.71
4093 27.45
12401 105.37
4370 71.75
18851 406.67

109
0.05
1542
0.86
3001
3.31
8911 12.43
6805 19.74
27377 103.75
9168 68.10
56189 483.28

121
0.18
1413
2.42
1742
6.43
7007 30.79
4093 35.40
12401 140.53
4370 105.53
20584 600.94

4729501577.22
117475 899.72

238331663.46

49035 495.53

6 0.04
6 0.04
6
2.79
121 0.00
13
0.00
169 0.30
13 0.17
435
3.07
1808 0.01
792
0.02
9 1.15
9 0.69
128
3.84
3293 0.02
262
0.02
651 1.95
12 7.05
812
8.84
16088 0.11
2925
0.13
77 5.63
9 21.15
155 16.53
11128 0.12
1121
0.15
1299 5.26
61 39.31
1151 23.41
49905 0.48
7102
0.72
233 19.78
9 59.70
185 29.88
46502 0.57
2631
0.48
561 12.42
497 94.69
1673 48.84 273585 3.39
22874
3.58
104875 25.48
10478 74.26 5513309 80.62 321861 68.99
2982520 66.89
6898321439.64
111212451579.77
90598 9.20 52159 43.24 108503 625.52 710123 3.86 107061 14.51
594661 12.41 416184109.43 4332961117.57 2467804 13.83 464982 56.82
12835 34.28
242241019.65 481045 3.14
33417
6.38
13255718119.54
648132 65.43
4921698 34.90 555619 105.49
3200672 90.07
8767431150.88
3992 18.13
948159.63
157782 1.31
8966
2.42
296506 49.11 104750256.13
481859 229.00
7315150142.82
114257 250.18

pipesworld-tankage-ipc4
01
02
03
04
05
06
07
08
11
13
15
17
21
31

5
12
8
11
8
10
8
11
22
16
30
44
14
39

77
0.13
126
0.07
105
0.20
960
1.20
1005
0.60
960
1.55
20803 155.53 52139 158.91 20803 207.57
1102841004.10 157722 668.67 1102841408.50
6531 73.63 13148 79.04
6531 112.61
20171 329.40 43583 310.24 20171 460.45

6 3.54
110 3.04
244 22.64
3892 16.68
376 15.46
1794328.18

6 0.13
13 0.20
9 36.89
12155.03
9120.06
11201.44

6
3.88
128 0.00
179
6.04
1012 0.01
818 24.47
52983 0.77
8116 64.68 221429 3.06
313 59.99
12764 0.21
3102 97.31
58487 0.87
2695 339.76 5404036198.08

13
0.01
659
0.02
1802
1.33
41540 14.49
2834
1.61
15746
6.61
104531 420.47

4116344 30.67

752867 334.42

4423951 65.44
1726598 13.56

126845 222.23
919764 381.66

96043191.77
660104 28.60 660102162.93
188517122.11
2546587141.12
12850247352.46
13241 69.80
1357801124.64

tpp-ipc5
01
02
03
04
05
06

5
8
11
14
19
25

6
9
12
15
623

0.00
0.00
0.00
0.01
0.52

6
11
27
78
5110

0.00
0.00
0.00
0.01
1.36

6
9
16
47
1455

0.00
0.00
0.00
0.01
1.21

6 0.00
9 0.00
12 0.00
15 0.01
20 0.36
947059 14.22

6 0.00
9 0.00
12 0.00
15 0.00
20 0.77
74798 23.97

6
9
12
15
624

0.01
0.01
0.03
0.07
0.48

7
26
116
494
24698

0.00
0.00
0.00
0.00
0.12

6
0.00
16
0.00
83
0.00
430
0.00
17398
0.15
9267024 216.69

trucks-ipc5
01
02
03
04
05
06
07
08
09

13
1691
0.41
1027
0.22
1039
0.40
14 0.03
14 0.02
285
0.56
5774 0.02
402
0.01
17
9624
2.68
2898
0.57
2957
1.35
4192 0.22
18 0.17
1413
1.04
28348 0.14
939
0.03
20
80693 71.37 20752 19.93 22236 31.25
199405 2.89 173790 6.88
4049
4.43 379582 2.97
9465
0.40
23 17538661237.601205793 850.3413156721394.88 2591561 29.172568634 56.96
8817
7.75 2990366 26.65 209140
9.43
25
23444940392.99
14744 23.12
1248571 90.78
30
308920 343.47
23 21347281313.60 719751 408.75 755608 820.55 7575415 88.918080496117.13 43270 27.6212410588117.92 223011 19.34
25
49663 47.61
3106944 403.36
28
233577 248.21

Table 12: Similar to Table 9 for the Openstacks, Pathways, Pipesworld-NoTankage,
Pipesworld-Tankage, TPP, and Trucks domains.

113

fiKatz & Domshlak

hF
task

h

nodes

hI
time

nodes

hFI
time

nodes

time

MS-104

MS-105

nodes time

nodes

time

HSPF

nodes

blind
time

hmax
nodes time

nodes

time

0.00
9
0.01
11
0.00
20
0.08
71
0.00
20
0.04
33
0.00
12
0.34
332
0.00
23
0.11
154
0.00
9
0.01
11
0.00
26
0.09
122
0.00
9
0.12
128
0.00
9
0.06
49
0.04
18
1.04
1358
0.00
96
0.19
153
0.00
40
0.17
153
0.00
59
0.16
95
0.00
13
0.06
27
2.58
356 18.99
3562
0.12
2287
1.34
2742
0.00
13
0.03
16
0.00
29
0.21
158
0.77
6338
4.46
9009
0.00
52
0.18
84
0.00
21
0.12
42
0.87
22315
8.16 189516
0.01
30
0.43
200
0.00
21
0.12
42
37.93
28 780.38
8913
0.00
52
0.28
182
0.01
179
0.85
773
0.00
49
0.29
95
1.43
3337
7.12 244499
0.02
393
1.35
2295
6.55
7530 32.97 53911
0.00
352
0.74
435
0.63
947
2.29
2291
0.00
158
0.50
227
6.36
7448
8.27 165170
14.07 188564 111.991669788
0.01
277
2.10
1532
0.01
33
0.74
562
0.07
146
1.78
4103
12.86
23371 87.911036992
0.00
21
0.16
54
0.18
1773
1.29
1908
0.00
256
0.50
333
0.05
407
2.18
4142
0.00
121
0.74
434
4.05
19865
6.91 80785
0.04
515
2.32
5075
11.08 200559 101.21
23.32 27728751408.64
0.02
390
1.40
690

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.02
0.01
0.00
0.00
0.04
0.00
0.00
0.67
0.00
0.00
0.12
0.00
0.00
0.00
1.27
0.01
0.25
0.00
0.01
0.00
0.63
9.44
0.00
0.00
0.01
6.74
0.00
0.01
0.00
0.01
0.00
0.25
0.01

9 0.00
47 0.00
28 0.00
102 0.00
69 0.00
9 0.00
62 0.00
52 0.00
20 0.00
376 0.01
142 0.00
113 0.00
86 0.00
18 0.00
324 0.02
1876 0.01
14 0.00
91 0.00
6925 0.08
75 0.00
31 0.00
177138 1.43
116 0.00
31 0.00
854 0.18
142 0.00
616 0.00
79 0.00
192459 2.32
1834 0.01
16766 0.36
424 0.00
1073 0.01
216 0.00
61548 1.06
717884 18.27
1342 0.01
357 0.00
2597 0.02
229210 9.51
35 0.00
1636 0.01
315 0.00
3235 0.02
358 0.00
65984 0.63
4406 0.02
19020089286.02

0.00

642

psr-small-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50

8
11
11
10
11
8
11
8
8
7
19
16
15
9
10
25
9
12
25
17
10
33
12
10
9
17
21
14
21
22
19
24
21
21
22
22
23
13
23
20
10
30
20
19
20
34
27
37
47
23

10
52
31
66
75
10
61
24
18
131
149
120
90
19
1200
2328
15
85
8025
80
28
163299
77
28
485
144
616
79
142772
1791
11278
431
1480
223
65965
571766
1307
301
2486
31
1855
328
2990
347
60888
4104

637

0.00
10
0.00
10
0.00
0.01
55
0.00
52
0.01
0.01
31
0.00
31
0.00
0.04
91
0.03
73
0.06
0.01
79
0.01
75
0.02
0.00
10
0.00
10
0.00
0.01
61
0.00
61
0.01
0.01
29
0.00
25
0.01
0.01
19
0.00
18
0.00
0.20
183
0.18
155
0.32
0.03
149
0.02
149
0.04
0.03
123
0.02
120
0.04
0.02
90
0.01
90
0.02
0.00
19
0.00
19
0.00
6.55
708
6.25
769
9.91
0.65
2158
0.34
2176
0.85
0.00
15
0.00
15
0.00
0.03
90
0.01
85
0.03
4.31
7856
2.19
7876
5.80
0.02
80
0.01
80
0.02
0.01
28
0.00
28
0.01
405.65 176058 245.42 168685 617.45
0.04
93
0.03
77
0.06
0.01
28
0.00
28
0.01
84.24
463 145.38
482 213.42
0.05
150
0.03
146
0.06
0.33
675
0.21
650
0.49
0.02
79
0.01
79
0.02
436.34 187319 307.77 159325 709.89
1.25
1982
0.80
1883
1.90
25.93
6810 38.66
8297 53.43
0.17
431
0.10
431
0.25
0.84
1436
0.30
1391
1.00
0.07
223
0.04
223
0.09
160.36 63186 39.55 68281 199.30
392.49 371834 786.06 4584021094.61
1.29
1417
0.95
1363
2.10
0.20
372
0.15
326
0.32
2.49
2942
1.64
2682
3.91
1826081384.90
0.01
34
0.00
31
0.01
0.50
1747
0.17
1739
0.59
0.09
328
0.05
328
0.12
3.25
3430
2.30
3121
5.24
0.16
376
0.11
359
0.25
51.77 61842 21.14 61563 68.33
5.27
4522
3.93
4284
8.70

0.39

659

0.26

645

9 0.00
9
12 0.00
12
12 0.00
12
11 0.00
11
12 0.00
12
9 0.00
9
12 0.00
12
9 0.00
9
9 0.00
9
8 0.04
8
20 0.00
20
17 0.00
17
16 0.00
16
10 0.00
10
11 0.46
11
975 0.11
26
10 0.00
10
13 0.00
13
2910 0.27
26
18 0.00
18
11 0.00
11
34 0.28
34
13 0.00
13
11 0.00
11
10 5.42
10
18 0.00
18
22 0.01
22
15 0.00
15
22 0.39
22
23 0.01
23
2647 0.89
723
25 0.00
25
446 0.26
22
22 0.00
22
24021 0.83 11113
48350 2.98
2783
24 0.02
24
14 0.01
14
24 0.08
24
38837 1.88
7767
11 0.00
11
1117 0.18
31
21 0.00
21
20 0.05
20
21 0.01
21
36941 0.67 32582
28 0.04
28
129627 2.37
2500
204836815.84 594399
0.60
24 0.02
24

0.00

rovers-ipc5
01
02
03
04
05
07
12

10
8
11
8
22
18
19

147
0.01
147
0.01
147
0.02
11 0.03
11 0.03
48
0.07
1104 0.00
283 0.00
44
0.01
44
0.01
44
0.01
9 0.00
9 0.00
16
0.03
254 0.00
129 0.00
672
0.11
419
0.05
448
0.10
12 0.11
12 0.12
804
0.16
3543 0.02
757 0.00
47
0.02
20
0.00
24
0.01
9 0.04
9 0.04
58
0.08
897 0.00
223 0.00
808084 237.13 410712 123.64 522937 231.28 61726711.48 375808 18.46 298400 101.658559690126.19 4318309 81.53
741649 517.1816822451780.27 328088451.022212903 59.20 1459792 866.93
9618062199.91
5187273166.77

satellite-ipc4
01
02
03
04
05
06

9
24
0.00
32
0.00
13
86
0.02
337
0.10
11
2249
1.24
656
0.53
17
9817 10.65 14860 24.90
15 2795691251.83 46453 515.80
20 1496577 968.2415723271721.87

29
0.00
241
0.13
728
0.82
11250 26.18
61692 877.26

10 0.00
10 0.00
14 0.01
14 0.01
12 0.56
12 0.64
4152 0.99
18 4.43
81972 7.26 148667 69.28
276922974.73 307962 32.52

46
0.06
89
646
0.21
1728
1945
0.93 15185
15890
9.50 345663
267513 565.18

2 0.00
2 0.00
7 0.00
7 0.00
7 0.21
7 0.90
9 0.20
9 0.89
12 0.25
12 1.90
12 0.38
12 3.54
16 0.38
16 3.48
14354 2.00
12 14.48
251703551.18 611457 30.47
132287134.84 137872 25.44
31003011.28 110726 26.65

2
0.45
2
9
0.46
58
40
3.42
5160
215
3.44
5256
422
7.70 82289
1957 11.81 596531
34890 30.36 405626
83533 292.05

0.00
0.01
0.17
4.70

59
940
6822
180815

0.00
0.00
0.11
3.37

10751017371.43

zenotravel-ipc3
01
02
03
04
05
06
07
08
09
10
11

1
6
6
8
11
11
15
11
21
22
14

2
17
28
99
177
2287
5088
3268

0.01
0.02
0.08
0.15
0.32
5.51
9.63
43.96

2
18
18
88
220
1144
4234
1026

0.00
0.02
0.12
0.26
0.22
2.00
5.56
8.92

769041090.67

2
17
12
81
136
504
4199
1655

0.01
0.02
0.11
0.30
0.36
2.40
10.58
30.06

0.00
0.00
0.04
0.03
0.63
5.90
3.56

2 0.00
22 0.00
492 0.02
665 0.01
12466 0.33
85931 2.47
115348 2.60
687846 50.76

Table 13: Similar to Table 9 for the PSR, Rovers, Satellite, and Zenotravel domains.

114

fiImplicit Abstraction Heuristics

hF
task

h nodes

hI
timenodes

hFI
timenodes

MS-104
time nodes

MS-105

timenodes

HSPF

time nodes

blind
hmax
time nodes time nodes
time

schedule-strips
02-0
02-1
02-2
02-3
02-4
02-5
02-6
02-7
02-8
02-9
03-0
03-1
03-2
03-3
03-4
03-5
03-6
03-7
03-8
03-9
04-0
04-1
04-2
04-3
04-4
04-5
04-6
04-7
04-8
04-9
05-0
05-1
05-3
05-4
05-5
05-6
05-7
05-8
05-9
06-2
06-4
07-0
07-9

3
5
0.15
5
0.14
5
0.22
2
3
0.16
4
0.11
3
0.18
2
3
0.32
3
0.17
3
0.40
3
26
0.50
37
0.76
26
0.61
3
68
1.34 188
2.24 220
7.20
2
3
0.33
3
0.14
3
0.38
2
3
0.14
5
0.12
3
0.17
2
3
0.30
3
0.13
3
0.34
2
3
0.32
3
0.14
3
0.38
3
5
0.15
5
0.14
5
0.22
4
40
2.72 407 12.16 140 14.55
2
3
0.51
3
0.35
3
0.72
4
27
1.16
50
1.83
33
2.33
4
15
0.79
91
2.39
15
0.96
3
4
1.11
16
2.08
4
1.52
4
73
6.13 471 16.71
74
8.32
4
72
1.27
75
1.80
69
1.33
4
28
1.05
50
1.83
28
1.43
4
273 11.53 266 11.46 273 17.48
4
8
0.96
31
1.77
14
2.13
5
373 13.91 1498 74.46 167 24.60
6 175591373.8010707 626.54
5
209
9.88 406 20.85
66
5.30
5
142 10.47 674 33.29 251 29.28
5
921 64.48 450 46.95 574 116.65
6
483 47.25 4544 268.77 850 187.46
6
779 27.0911610 361.74 1834 102.68
5
99 18.48 424 38.04 163 40.04
5
102 16.01 573 31.87 111 23.35
4 1043 80.06 996 76.64 1050 143.48
5
163 41.61 483 63.23 167 62.53
6 2701 213.92
1257 286.28
7
136221693.68
6
989 100.02 3433 229.05 582 100.05
6
198 21.67 9550 767.94 347 68.64
7 6033 743.61
103251508.56
6
944 131.19175621446.20 2107 379.70
7 1190 172.59
2709 730.54
6 1537 140.49158291248.19 2717 547.56
6
888 243.14
1709 730.36
8 115351776.87
7 2489 786.76
8 68291559.86

4 511.10
3 104.98
3 231.99
4 56.51
3
3
3
3
4

363.11
121.84
323.77
316.53
251.46

5 191.03
5 259.13
5 682.30
5 121.58
5 195.72
5 235.48
71115.76
6 267.29
7
7
6
6
5

837.68
459.19
936.68
711.65
316.22

41743.32

5
3
3
4
4
3
3
3
3
5

577.39
754.26
495.56
658.90
484.62
667.32
697.42
604.06
668.79
577.16

76 0.02
5
0.09
6 0.02
3
0.07
5 0.02
3
0.07
529 0.03
95
0.45
543 0.03
108
0.44
3 0.03
3
0.07
6 0.02
3
0.06
13 0.02
3
0.07
8 0.02
3
0.07
76 0.03
5
0.09
11915 0.60 1127
8.98
31 0.04
25
0.37
3617 0.23 1228
9.56
3379 0.23
170
1.85
41223.90
301 0.06
22
0.27
12217 0.64 1175 12.43
2663 0.19 1542 11.73
12859 0.68 1323 13.47
12616 0.65 1590 11.13
4339 0.27
913
7.69
31219326.88 22993 273.38
55206949.79
47696 4.97 9703 131.69
89272 8.74 12941 163.84
62013 6.03 13614 168.07
1079781399.99
1071151001.40
61327 5.97 8683 103.50
34046729.56 15122 181.98
41673 4.27 5480 83.69
14335022.71 43336 751.35

120602 989.42

Table 14: Similar to Table 9 for the (non-IPC) Schedule-STRIPS domain.

115

fiKatz & Domshlak
hF
task

h

nodes

hI
time

nodes

hFI
time

nodes

time

MS-104

MS-105

nodes

nodes

time

HSPF

time

nodes

9
0.00
10
0.00
18
0.03
21
0.01
22
0.01
42
0.17
42
0.17
96231549.13
89525 466.14
19
0.01
22
0.01
40
0.21
38
0.21
8968 238.16
8931 267.81
3053401077.90

9
10
29
21
22
42
42
203
12956
19
22
40
38
62
59

time

blind
nodes time

hmax
nodes time

airport-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
19
21
22
36
37

8
10
0.01
9 0.00
9
0.00
9 0.00
9
12
0.01
15 0.00
15
0.01
10 0.00
17
86
0.02
133 0.01
93
0.02
18 0.04
20
22
0.01
21 0.00
21
0.01
21 0.02
21
23
0.08
30 0.02
27
0.09
22 0.01
41
513
0.16
639 0.06
567
0.19
42 0.16
41
514
0.15
632 0.05
550
0.19
42 0.17
62
12733
1.89 21544 1.36 14398
4.02
24372 25.42
71
88670 16.58 136717 9.60 90412 38.78 152408 64.92
18
19
0.01
19 0.01
19
0.01
19 0.02
21
23
0.10
30 0.03
27
0.12
22 0.02
39
475
0.20
728 0.07
568
0.25
40 0.21
37
434
0.20
663 0.07
479
0.24
38 0.20
60
12040
2.90 25110 1.86 15948
4.64
30637 51.23
58
11477
2.74 23317 1.71 14557
4.25
28798 46.20
79 267277 77.39 824491 97.12 353592 114.58 1031524200.95
88 2460667 708.82
26786891235.79
90 1354353 592.533400142492.061462739 660.17
101
5156 48.29 11259 3.72
4773 51.13
7326372.92
148 6066481110.091063668318.90 4778361082.91 1119943762.02
109
9504 129.73 34986 14.41
9436 140.75
34365853.70
142
37873 820.33

102

10.28

0.72
11 0.00
9 0.00
1.23
13 0.00
10 0.00
5.10
164 0.00
57 0.00
1.32
23 0.00
21 0.00
46.54
27 0.00
22 0.00
123.13
738 0.01
418 0.02
117.56
742 0.01
405 0.02
602.09 27032 0.28
9687 0.90
993.07 175717 2.47 56484 7.62
2.45
21 0.00
19 0.00
65.36
27 0.00
22 0.01
169.02
873 0.01
392 0.03
134.87
822 0.01
342 0.03
714.76 35384 0.39
9196 1.11
647.05 33798 0.38
8200 1.01
124746719.72 221993 49.03
1043661310.89
831632253.21
18809 0.42
3184 1.12
159967105.29
63061 1.44

blocks-ipc2
04-0
04-1
04-2
05-0
05-1
05-2
06-0
06-1
06-2
07-0
07-1
07-2
08-0
08-1
08-2
09-0
09-1
09-2
10-0
10-1
10-2
11-0
11-1
11-2
12-0
12-1
13-0
13-1
14-0
14-1

6
15
10
14
6
7
12
32
10
37
16
152
12
33
10
41
20
855
20
278
22
6910
20
1458
18
1533
20
10040
16
479
30 134185
28
3435
26
6379
34 1524599
32 610206
34 1516087
32
30
34
34
34
42
44
38
36

0.00
46 0.00
17
0.00
7 0.03
7
0.03
0.00
31 0.00
15
0.00
11 0.04
11
0.03
0.00
26 0.00
10
0.00
7 0.04
7
0.03
0.00
302 0.01
113
0.00
13 0.30
13
0.96
0.00
280 0.00
98
0.00
11 0.29
11
0.96
0.00
596 0.00
348
0.01
17 0.29
17
0.95
0.00
766 0.01
207
0.01
13 0.95
13
8.56
0.00
2395 0.03
578
0.02
11 0.90
11
8.34
0.01
5444 0.05
3352
0.06
733 0.87
85
8.84
0.01 20183 0.28
4022
0.12
577 1.93
144 23.32
0.10 59207 0.60 38539
0.67
10071 1.70
1835 21.05
0.02 46009 0.52 18854
0.39
1855 1.59
782 20.37
0.03 344157 5.46 69830
2.09
5557 3.67
678 36.80
0.17 517514 7.22 191352
4.91
45711 3.88
11827 33.49
0.02 237140 4.08 32567
1.09
277 3.63
54 32.53
3.107405904117.144346535 118.23 1233374 16.00 971409 77.74
0.094145371 77.54 917197 33.32
95068 7.35
58873 63.15
0.174145278 78.21 923365 33.79 161719 13.54
20050 82.45
36.52
15.79
12063665 228.76
37.71
7046739 141.44

7
0.36
93 0.00
25 0.00
11
0.39
66 0.00
23 0.00
7
0.38
63 0.00
18 0.00
13
1.32
467 0.00
145 0.00
11
1.36
567 0.00
135 0.00
17
1.49
792 0.00
297 0.00
13
4.10
1826 0.00
276 0.00
11
4.17
4887 0.01
755 0.01
31
4.29
6385 0.02
2556 0.03
22 11.47 37157 0.14
5943 0.11
174 11.25 63376 0.21 33194 0.46
90 10.99 55218 0.19 18293 0.29
25 26.00 519107 2.28 94671 2.07
151 26.57 636498 2.60 199901 3.85
17 25.85 433144 1.93 52717 1.30
464 56.76798464936.763840589 85.00
82 56.98591457229.731200345 32.06
81 57.02596316030.021211463 32.15
1800 114.26
1835 115.19
3685 116.75
2678 213.32
1510 203.79
3984 213.97
1184 370.06
614 382.34
83996 860.45
1634381104.27
27791063.02
71541087.40

depots-ipc3
01
02
03
04
07
10
13

10
114
0.01
279 0.01
161
0.02
11 0.00
11
0.00
45
0.77
329 0.00
136 0.00
15
1134
0.08
9344 0.31
2638
0.22
738 3.24
16
1.14
898 11.56 15404 0.11
3771 0.17
27 134428
8.592520703159.84 581726 66.43 348288 20.69 239313 222.35 103089 247.13293039827.201204646 97.62
30 1254545 101.18
5835295 923.87 1284048 52.05 1273762 529.34
21 109765
9.174271196336.59 487961 76.02 211820 37.54
41328 324.19650110071.581331701166.76
24 2964635 283.55
60814781187.66 3241083157.52
25 1003709 152.30
81618721559.21 1427824116.06

driverlog-ipc3
01
02
03
04
05
06
07
08
09
10
11
13

7
49
0.00
37 0.00
37
0.00
8 0.04
19
15713
0.42 18452 0.27 15794
0.55
20 0.13
12
164
0.00
190 0.00
163
0.01
13 0.16
16
6161
0.42 10778 0.30
7665
0.62
17 0.49
18
13640
1.01 11400 0.36 10984
1.07
2614 0.60
11
608
0.09
795 0.06
492
0.11
291 1.35
13
864
0.14
1730 0.11
1006
0.21
14 1.42
22 669994 75.741181268 61.32 694996 104.59 287823 7.34
22 150255 14.72 198651 11.44 164109 23.06
15504 1.70
17
4304
0.44 16099 1.21
4037
0.69
18 1.64
19
43395
4.99 41445 2.22 39069
5.90
34137 1.99
26 1303099 325.711014865144.641098694 422.20 1298884 19.52

8
20
13
17
19
12
14
2952
23
18
10790
870875

0.03
44
0.47
182 0.00
20 0.00
0.26 15998
4.55 68927 0.36 54283 0.52
0.25
863
1.25 16031 0.09
2498 0.03
2.41 22933 12.20 999991 8.12 393673 6.56
4.58 24877 18.77629080361.571724611 34.73
9.72
3804 10.08 681757 7.64 54451 1.71
15.35 25801 41.34634976781.53 493480 17.31
20.31
10.43
18.54 18234 68.22
17.01 5596231193.00
6141130330.22
35.33

Table 15: Runtimes of cost-optimal heuristic-search planners on the Airport,
Blocksworld, Depots, and Driverlog domains.
The description of
the planners is given in Section 6; here the fork-decomposition heuristics are via
structural-pattern databases. Column task denotes problem instance, column
h denotes optimal solution length. Other columns capture the run time and
number of expanded nodes.
116

fiImplicit Abstraction Heuristics

hF
taskh

nodes

hI
time

nodes

hFI
time

nodes

time

MS-104

MS-105

nodes

nodes

time

time

HSPF

nodes

blind
time

nodes

time

hmax
nodes
time

freecell-ipc3
01
02
03
04
05

8
14
18
26
30

234
0.10
974
0.15
274 0.17
30960
1.95
75150
5.53
37131 4.79
197647 14.41 533995 78.27 240161 51.24
997836 60.67 1921470 232.95 1218329213.02
6510089 448.22

87 3.12
31487 40.40
95805140.96
943074 86.78
5950977243.74

9 38.74

9 13.01
3437 0.03
1043
0.15
466 70.29 130883 1.46
41864 10.77
1589 169.39 944843 11.45 210503 75.62
15848 341.02 3021326 38.80 600525 247.70
40642 916.44
14080351062.25

grid-ipc1
01
02

14
26

571
0.60
33302741078.55

1117

0.34

472

0.78

660 8.63
467121.10
3392724 50.35 3244132241.94

6446

0.08

190
0.10
664016 231.26

gripper-ipc1
01
02
03
04
05
06
07

11
214
0.00
240
0.00
214 0.00
12 0.00
12 0.00
33
0.11
236 0.00
208
0.00
17
1768
0.02
1832
0.01
1803 0.03
18 0.11
18 0.08
680
0.37
1826 0.01
1760
0.01
23
11626
0.19
11736
0.08
11689 0.22
11514 0.47
2094 1.75
7370
1.52
11736 0.04
11616
0.08
29
68380
1.46
68558
0.51
68479 1.63
68380 1.24
68190 8.05
55568 10.29
68558 0.27
68368
0.56
35
376510 10.07 376784
3.20 376653 11.11
376510 3.52 376510 19.46 344386 79.96 376772 1.59 376496
3.51
41 1982032 70.91 1982408 19.08 1982227 77.81 1982032 13.42 1982032 42.16 1911592 577.49 1982394 9.59 1982016 21.57
47 10091986 438.4110092464 105.6710092241478.67 10091986 61.6610091986106.84
10092464 51.1010091968 119.64

logistics-ipc1
01
05
31
32
33
35

26
22
13
20
27
30

77763
7.14 1469610
3293
0.46 850312
436
0.03
1981
392
0.01
2704
312180 27.19
477883 183.08

95.49
42.43
0.07
0.07

830292 98.59
173477 18.19
1284 0.09
962 0.05
3617185427.52

1918881 41.03
768161 18.69
494 0.42
21 0.16

949586 34.82
609393 35.27
14 2.11
21 0.72
529338 32.55

2119551700.26
6.58
7.08

155645
245325

1.66
2.07

32282
81156

0.57
1.00

21 0.05
20 0.04
16 0.05
28 0.38
18 0.38
9 0.38
26 1.23
15 1.26
26 1.26
25 1.22
37 4.87
49 4.94
32 6.90
45 7.21
37 9.46
31 9.43
668834 29.73
1457130 43.00
701106 37.42

21
0.34
20
0.37
16
0.36
28
0.58
18
0.72
9
0.78
26
1.03
15
1.16
26
1.03
25
1.02
24317 35.46
362179 453.06
14890 33.50
114155 198.84
32017 83.16
6720 26.48

11246
9249
4955
109525
22307
1031
490207
24881
476661
422557

0.05
0.04
0.02
0.64
0.13
0.00
3.40
0.16
3.32
2.95

4884
4185
1205
74694
6199
280
202229
3604
200012
133521

0.03
0.03
0.01
0.59
0.05
0.00
1.92
0.03
1.98
1.29

3636

0.07

481
9598

logistics-ipc2
04-0 20
04-1 19
04-2 15
05-0 27
05-1 17
05-2 8
06-0 25
06-1 14
06-2 25
06-9 24
07-0 36
07-1 44
08-0 31
08-1 44
09-0 36
09-1 30
10-0 45
10-1 42
11-0 48
11-1 60
12-0 42
12-1 68

21
20
16
28
18
9
26
15
26
25
37
1689
32
45
37
31
46
43
697
21959
43
106534

0.00
193
0.00
570
0.00
117
0.00
2550
0.00
675
0.00
24
0.00
4249
0.00
181
0.00
2752
0.00
2395
0.00 251287
0.07 3532213
0.00
82476
0.01 1183608
0.00 351538
0.00
59336
0.01
0.01
0.09
2.22
0.02
11.64

0.00
65 0.00
0.01
293 0.00
0.00
79 0.00
0.05
1171 0.03
0.01
427 0.01
0.00
13 0.00
0.09
2461 0.07
0.00
99 0.00
0.06
1394 0.04
0.04
1428 0.04
7.52
98053 4.59
99.33 1705009 72.35
2.69
35805 1.78
45.72 462244 25.36
13.75 167038 9.76
2.48
25359 1.73

21
20
16
28
18
9
26
15
26
25
525
666324
1042
16708
20950
31

0.03
0.03
0.04
0.10
0.10
0.09
0.18
0.18
0.19
0.18
0.65
8.83
0.96
1.15
1.56
1.27

775996 43.56
2222340 87.47

mprime-ipc1
01
02
03
04
05
07
08
09
11
12
15
16
17
19
21
25
26
27
28
29
31
32
34
35

5
7
4
8
11
5
6
8
7
6
6
6
4
6
6
4
6
5
7
4
4
7
4
5

196
0.02
10
0.01
11604
2.72
44045 80.68
427
0.27
7
0.08
3836
0.22
1775
0.10
1745027 195.08
3314
0.25
47
0.03
485381 491.53 13767801426.21
19838
2.92 100188 74.85
9
0.02
219
0.03
16320
1.89
8118
0.73
252
0.76
2746 10.47
727401 521.78
174221 55.09
75
0.01
77622 24.69
54
0.16
8
0.01
182
0.12
248
0.51
31759
1.73
234
0.26
392
0.07

51590 135.00
453 18.78
95361 485.79
34022 47.43
30
0.01
147854 48.25
1772
1.50
403
0.02
56
0.08
46
0.68
12436
1.46
46
0.16
290
0.06

24 0.01
2565 4.20
11 0.16
1093 0.09
604756592.60
346 0.08
5227
8
5243

6.31
0.03
1.13

448 2.76
451 21.40

6 2.00
3317 88.58
36 33.64
9 6.09
1705009127.53
1667 46.72
1469752403.45
21993 36.25
8 4.69
34763 11.45

6 20.45
5463.85
9 82.71

108

49.59

19076 781.74

9868 0.67
599590 23.58
18744

8 62.68
42055143.27

0.56

2197646 71.69
22 394.26
73260 2.21
25665 724.12 108652 3.50

473 81.42

425144 32.17
172736 42.48

123039313.25
169400392.30
29 0.01
68239106.35
9 0.18
37 0.02
32 0.11
19 1.00
11839 1.93
23 0.28
84 0.08

5 0.48
172432 46.33
6 11.59
8 1.88
5 14.92
419 99.87
19429 21.61
450151.69
359 3.63

5 2.75
189154454.69
6154.43
8 22.55
5201.40
7269292.37
6 43.43

1503293103.23
383 0.00
819590 61.01
84079 3.50
128 146.80
17333 0.25
3187 0.17
3584 0.19
110731701.00 115479 2.75
3618 0.19
706 96.55
2476 0.05
85

8.71

68
0.04
12606 36.65
5
0.07
200
0.24
14881571638.78
11
0.04
7650 84.33
19023 30.26
915
0.54
1520
1.78
1039 178.55
7962 35.65
5
1.06
36013 533.75
15250 101.75
6
0.00
440
2.69
831
2.08
211
0.06
7
0.10
11
0.17
3096
1.74
11
0.18
44
0.03

Table 16: Similar to Table 15 for the Freecell, Grid, Gripper, Logistics-ipc1,
Logistics-ipc2, and Mprime domains.

117

fiKatz & Domshlak

hF
task

h

nodes

hI
time

nodes

hFI
time

nodes

time

MS-104

MS-105

nodes

nodes

time

time

HSPF

nodes

blind
time

nodes

time

hmax
nodes time

miconic-strips-ipc2
01-0
01-1
01-2
01-3
01-4
02-0
02-1
02-2
02-3
02-4
03-0
03-1
03-2
03-3
03-4
04-0
04-1
04-2
04-3
04-4
05-0
05-1
05-2
05-3
05-4
06-0
06-1
06-2
06-3
06-4
07-0
07-1
07-2
07-3
07-4
08-0
08-1
08-2
08-3
08-4
09-0
09-1
09-2
09-3
09-4
10-0
10-1
10-2
10-3
10-4
11-0
11-1
11-2
11-3
11-4

4
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
3
5 0.00
5 0.00
5 0.00
4 0.00
4 0.00
4
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
4
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
4
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
7
19 0.00
22 0.00
19 0.00
8 0.00
8 0.00
7
21 0.00
23 0.00
21 0.00
8 0.00
8 0.00
7
21 0.00
23 0.00
21 0.00
8 0.00
8 0.00
7
24 0.00
24 0.00
24 0.00
8 0.00
8 0.00
7
19 0.00
22 0.00
19 0.00
8 0.00
8 0.00
10
86 0.00
129 0.00
98 0.00
11 0.00
11 0.00
11
120 0.00
168 0.00
147 0.00
12 0.00
12 0.00
10
137 0.00
143 0.00
137 0.00
11 0.00
11 0.00
10
96 0.00
153 0.00
117 0.00
11 0.00
11 0.00
10
103 0.00
149 0.00
115 0.00
11 0.00
11 0.00
14
524 0.00
843 0.00
686 0.01
15 0.01
15 0.01
13
505 0.00
817 0.00
663 0.01
14 0.01
14 0.01
15
685 0.00
942 0.00
802 0.01
16 0.01
16 0.01
15
681 0.00
942 0.00
798 0.01
16 0.01
16 0.01
15
685 0.00
942 0.00
802 0.01
16 0.01
16 0.01
17
2468 0.03
4009 0.03
3307 0.05
18 0.06
18 0.05
17
2807 0.04
4345 0.03
3677 0.06
18 0.06
18 0.05
15
1596 0.02
2981 0.02
2275 0.04
16 0.06
16 0.05
17
2256 0.03
3799 0.03
3104 0.05
18 0.06
18 0.05
18
3210 0.04
4732 0.03
4267 0.06
19 0.06
19 0.05
19
9379 0.18
17665 0.15 13531 0.26
20 0.18
20 0.32
19
9106 0.17
18134 0.15 14052 0.27
20 0.18
20 0.32
20
10900 0.20
19084 0.16 15111 0.28
21 0.18
21 0.32
20
12127 0.23
21708 0.18 17807 0.33
21 0.17
21 0.32
21
13784 0.24
23255 0.19 19536 0.35
22 0.17
22 0.32
23
53662 1.19
96092 0.97 79449 1.76
24 0.32
24 1.75
24
56328 1.24
99109 0.96 83677 1.83
7001 0.38
25 1.75
22
48141 1.10
96139 0.94 78471 1.77
1646 0.33
23 1.71
22
46867 1.08
93117 0.92 75424 1.69
1861 0.33
23 1.74
25
84250 1.70 126595 1.22 111984 2.36
23159 0.52
26 1.71
27
272580 7.05 485051 5.51 408114 10.53
41629 0.91
28 4.18
27
284415 7.56 527216 6.01 446837 11.58
42679 0.90
28 4.25
26
207931 5.60 414294 4.79 330993 8.90
37744 0.86
27 4.25
28
369479 9.25 598031 6.74 527216 13.30
140453 1.94
29 4.21
27
297516 7.74 507910 5.79 431432 11.04
62933 1.16
28 4.12
31 1461729 43.82 2491975 32.672138656 63.58
684737 9.07 126918 8.89
30 1207894 37.47 2335166 30.761952916 59.39
406041 5.61 100937 8.73
30 1294691 40.03 2340411 30.971972234 59.25
442547 6.06 82946 8.63
32 1840936 52.68 2889342 38.122571844 74.47
765455 10.00 277302 11.14
28 1252484 40.34 2352633 31.351944297 59.37
317692 4.65
29 7.03
33 5716041202.3710316603153.808774563300.08 2436164 35.24 863244 23.76
32 5601282201.4310789013162.699144153315.23 2340169 34.09 335745 15.68
32 4153191155.86 9148616138.697466572265.86 1735477 25.29 486286 17.72
34 6108094214.6810960203167.109400386320.13 3952148 55.86 940556 24.24
33 5920127211.4011075136170.829448049322.74 2715866 39.44 625559 19.91
37
11473359183.604724980 93.56
34 15349953668.77
7535468124.801934943 47.91
38
14645785233.686330198120.71
38
5809711110.10
35
5853546 95.561082086 32.22

5
0.01
4
0.00
5
0.01
5
0.00
5
0.01
26
0.01
26
0.01
27
0.00
20
0.01
23
0.01
100
0.03
140
0.02
122
0.02
131
0.02
114
0.02
669
0.10
634
0.11
822
0.12
820
0.12
821
0.12
2829
0.44
3260
0.49
1594
0.32
2568
0.42
3953
0.55
9312
1.76
10252
1.96
11247
2.11
14216
2.56
16880
3.04
56686 14.31
63035 16.33
55751 13.98
53121 13.27
96327 24.76
290649 104.18
339177 123.10
204614 73.39
435617 160.49
315339 111.84
1555286 794.93
1344815 683.05
1357681 692.11
20831681051.95
1231554 605.01

5 0.00
5 0.00
5 0.00
4 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
5 0.00
30 0.00
20 0.00
30 0.00
22 0.00
30 0.00
22 0.00
26 0.00
17 0.00
31 0.00
20 0.00
193 0.00
105 0.00
218 0.00
150 0.00
164 0.00
92 0.00
197 0.00
130 0.00
190 0.00
114 0.00
1182 0.00
866 0.00
1176 0.00
860 0.00
1277 0.00
969 0.00
1319 0.00
970 0.00
1334 0.00
969 0.00
6350 0.03
4387 0.03
6602 0.03
4664 0.03
5565 0.03
3524 0.03
5944 0.03
4140 0.03
6949 0.04
5268 0.04
30786 0.20
21194 0.20
30093 0.20
21255 0.20
32390 0.21
21694 0.21
32574 0.21
24552 0.23
33793 0.22
26167 0.24
155466 1.22 116685 1.32
164470 1.29 118494 1.33
161342 1.27 119688 1.36
155176 1.23 114649 1.30
168219 1.33 140128 1.58
755255 7.16 594032 7.95
794365 7.56 636587 8.66
731622 6.92 534711 7.37
833421 7.97 690267 9.29
771608 7.33 613253 8.43
3685552 41.04 3006991 49.12
3649801 40.32 2893803 47.54
3576134 39.61 2895182 47.26
3796035 42.13 3304570 53.29
3589382 39.29 2956995 48.84
15804498200.9013267920250.58
16472633208.3913720664256.89
15867374201.0112497087236.89
16309701208.4213801989262.53
16472551209.1313925654262.57

mystery-ipc1
01
02
03
04
07
09
11
12
15
16
17
18
19
20
24
25
26
27
28
29
30

5
7
4


8
7

6

4

6
7

4
6
5
7
4
9

7 0.00
6 0.00
6 0.00
2404 0.50
8012 11.19
722 1.01
73 0.08
7 0.04
11 0.10
0 0.00
0 0.00
0 0.00
0 0.00
0 0.00
3049 0.37
10764 5.66
1215 1.01
9 0.01
33 0.01
8 0.01
2102777 33.84 2093419 55.582093419 76.80
28271 20.21
21572 41.22
5079 44.42
0 0.15
0 0.27
354 1.32
85 2.74
83 3.59
0 0.00
0 0.00
0 0.00
21717 4.87
4968 5.26 16276 29.28
89887 46.32
84572153.53 53114173.34
0 0.13
0 0.30
9 0.00
10 0.00
9 0.01
1807 0.27
1835 0.30
1344 0.69
14 0.05
159 0.09
6 0.07
8 0.00
47 0.00
15 0.00
31 0.04
14 0.03
10 0.06
23175 5.16
76480169.86
7232 13.30

6 0.20
1672 82.70
5 16.46

6

1.79

10

5.38

5193.75

65 811.87

0 0.00
0 0.00
3165 29.34
8 1.51
8 16.59
2102777 14.612102729 27.84

0
0.00
3868 670.08
34 41.20

198445.85
0 0.00
0 0.00
12478 96.38
285069 59.22 547246578.39
5 0.10
2526 5.94
6 4.80
8 0.63
5 8.94
42112 28.07

5 0.10
346 70.78
6 80.48
8 6.77
5107.10
44893357.07

0

0.00

14
1.22
3107 291.36
7 243.78
31 16.67
27 536.30

30 0.00
770852 21.85
507 0.02

0.00
4.47
0.03

0 0.00
0 0.00
138289 2.18
1458 1.44
426 0.00
19 0.00
2102777 15.09 1177842 21.87
279973 13.21
135 2.62
5400 0.41
0 0.00
133871 3.65
686125 23.28
31 0.00
8455 0.10
2174 0.03
843 0.00
153 0.01
1977063 38.26

Table 17: Similar to Table 15 for the Miconic and Mystery domains.
118

8
2368
5

5
0
1516
718

0.35
0.00
5.44
3.76

6 0.00
37 0.05
73 0.04
32 0.00
7 0.02
26686 28.27

fiImplicit Abstraction Heuristics

hF
task h

nodes

hI
time

nodes

hFI

MS-104

MS-105

nodes

time

nodes

nodes

time

0.03
3070
0.04
3561
0.03
3070
0.03
3070
0.03
3070
18.93 587482
18.33 606782

0.05
0.05
0.05
0.05
0.05
22.20
22.53

24
24
24
24
24
621008
594758

0.05
24
0.06
24
0.06
24
0.06
24
0.06
24
4.85 279614
4.69 264535

0.06
0.06
0.06
0.06
0.05
7.86
7.34

0.00
0.02
0.43
1.31

7
1946
21671

time

time

HSPF

nodes

blind
time

hmax
nodes
time

nodes

time

2000
1.02
4822
2378
1.07
5501
2000
1.02
4822
2000
1.02
4822
2000
1.02
4822
379735 217.37 882874
405564 226.32 836647

0.01
0.02
0.01
0.02
0.01
4.91
4.62

4016
4594
4016
4016
4016
822514
787163

0.03
0.04
0.03
0.03
0.03
18.71
17.81

0.00
0.02
1.06
8.22

36
348
4346
104068

0.00
0.01
0.16
2.61

openstacks-ipc5
01
02
03
04
05
06
07

23
23
23
23
23
45
46

2264
2617
2264
2264
2264
366768
410728

0.02
3895
0.03
4485
0.02
3895
0.02
3895
0.02
3895
7.52 779710
8.23 760668

pathways-ipc5
01
02
03
04

6
12
18
17

1624
2755
44928
126950

0.00
0.02
0.62
2.66

1299
2307
20416
33788

0.00
0.01
0.25
0.59

1299
2437
29106
58738

1.14
2.56
6.43

7 0.79
13 42.11
14901129.23
98484288.39

1405
990
14772
34206

0.28
1624
0.29
2984
6.99 87189
27.00 456143

pipesworld-notankage-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
17
19
21
23
24
41

5
12
8
11
8
10
8
10
13
18
20
24
16
30
26
22
24
14
18
24
12

121
0.02
109
0.01
121
0.02
1413
0.06
1542
0.02
1413
0.08
1742
0.14
3001
0.07
1742
0.18
7007
0.45
8911
0.22
7007
0.59
4093
0.49
6805
0.26
4093
0.65
12401
1.44 27377
1.34 12401
2.03
4370
0.97
9168
0.77
4370
1.34
18851
3.84 56189
6.21 20584
6.42
1092472 160.712419903 151.991092472 219.75
313952
684234
39998

27.68 472950 29.55 313952 43.90
75.721319980 133.58 686186 145.41
6.02 117475 18.08 40226 12.69

1594863 254.432588849 192.901594863 353.40
54373931588.68
23833
4.02 49035
7.76 23833
7.87
2285790 568.937047138 871.032282678 843.28
502308 370.68

6 0.04
6 0.04
169 0.30
13 0.17
9 1.15
9 0.69
651 1.95
12 7.05
77 5.63
9 21.15
1299 5.26
61 39.31
233 19.78
9 59.70
561 12.42
497 94.69
104875 25.48
2982520 66.89
90598 9.20 52159 43.24
594661 12.41 416184109.43
12835 34.28
13255718119.54
648132 65.43
3200672 90.07
8767431150.88
3992 18.13
948159.63
296506 49.11 104750256.13
7315150142.82

6
2.79
121 0.00
13
0.00
435
3.07
1808 0.01
792
0.02
128
3.84
3293 0.02
262
0.02
812
8.84 16088 0.11
2925
0.13
155 16.53 11128 0.12
1121
0.15
1151 23.41 49905 0.48
7102
0.72
185 29.88 46502 0.57
2631
0.48
1673 48.84 273585 3.39
22874
3.58
10478 74.265513309 80.62 321861 68.99
6898321439.64
111212451579.77
108503 625.52 710123 3.86 107061 14.51
4332961117.572467804 13.83 464982 56.82
242241019.65 481045 3.14
33417
6.38
4921698 34.90

157782

1.31

5023081092.50

555619 105.49

8966
2.42
481859 229.00
114257 250.18

pipesworld-tankage-ipc4
01
02
03
04
05
06
07
08
11
13
15
17
21
31

5
77
0.02
126
0.01
105
0.02
6 3.54
6 0.13
12
960
0.05
1005
0.02
960
0.06
110 3.04
13 0.20
8
20803
1.89 52139
2.46 20803
2.82
244 22.64
9 36.89
11 110284
8.06 157722
9.60 110284 14.05
3892 16.68
12155.03
8
6531
0.86 13148
1.03
6531
1.32
376 15.46
9120.06
10
20171
2.41 43583
4.32 20171
4.41
1794328.18
11201.44
8 202706 73.8326437521379.11 202706 208.81
11
96043191.77
22 2345399 296.872629204 662.942365735 838.85
660104 28.60 660102162.93
16
188517122.11
30 96520911721.67
2546587141.12
44
12850247352.46
14 839847 250.39
13241 69.80
39 1501847 240.381568963 661.881504072 850.16 1357801124.64

6
3.88
128 0.00
179
6.04
1012 0.01
818 24.47 52983 0.77
8116 64.68 221429 3.06
313 59.99 12764 0.21
3102 97.31 58487 0.87
2695 339.765404036198.08

13
0.01
659
0.02
1802
1.33
41540 14.49
2834
1.61
15746
6.61
104531 420.47

4116344 30.67

752867 334.42

4423951 65.44
1726598 13.56

126845 222.23
919764 381.66

rovers-ipc5
01
02
03
04
05
07
12

10
147
0.00
147
8
44
0.00
44
11
672
0.01
419
8
47
0.00
20
22 808084 22.61 410712
18 4546797 191.34 741649
19
1529551

0.00
147
0.00
0.00
44
0.00
0.00
448
0.01
0.00
24
0.00
9.23 522937 18.29
21.011682245 102.77
76.46

11 0.03
11 0.03
48
0.07
1104 0.00
283
0.00
9 0.00
9 0.00
16
0.03
254 0.00
129
0.00
12 0.11
12 0.12
804
0.16
3543 0.02
757
0.00
9 0.04
9 0.04
58
0.08
897 0.00
223
0.00
617267 11.48 375808 18.46 298400 101.658559690126.19 4318309 81.53
3280884 51.022212903 59.20 1459792 866.93
9618062 199.91
5187273166.77

0.00
29
0.00
0.00
241
0.01
0.01
728
0.04
0.38 11250
0.76
4.92 61692 18.85
51.681518261 105.65

10 0.00
10 0.00
14 0.01
14 0.01
12 0.56
12 0.64
4152 0.99
18 4.43
81972 7.26 148667 69.28
2769229 74.73 307962 32.52

satellite-ipc4
01
02
03
04
05
06

9
24
13
86
11
2249
17
9817
15 279569
20 1496577

0.00
32
0.00
337
0.08
656
0.57 14860
49.47 46453
92.221572327

46
0.06
89
646
0.21
1728
1945
0.93 15185
15890
9.50 345663
267513 565.18

0.00
0.01
0.17
4.70

59
940
6822
180815

0.00
0.00
0.11
3.37

10751017 371.43

Table 18: Similar to Table 15 for the Openstacks, Pathways, PipesworldNoTankage, Pipesworld-Tankage, Rovers, and Satellite domains.

119

fiKatz & Domshlak

hF
task h

nodes

hI
time

nodes

hFI
time

nodes

time

MS-104

MS-105

nodes

nodes

time

time

HSPF

nodes

blind
time

hmax
nodes time

nodes

time

0.00
9
0.01
11
0.00
20
0.08
71
0.00
20
0.04
33
0.00
12
0.34
332
0.00
23
0.11
154
0.00
9
0.01
11
0.00
26
0.09
122
0.00
9
0.12
128
0.00
9
0.06
49
0.04
18
1.04
1358
0.00
96
0.19
153
0.00
40
0.17
153
0.00
59
0.16
95
0.00
13
0.06
27
2.58
356 18.99
3562
0.12
2287
1.34
2742
0.00
13
0.03
16
0.00
29
0.21
158
0.77
6338
4.46
9009
0.00
52
0.18
84
0.00
21
0.12
42
0.87
22315
8.16 189516
0.01
30
0.43
200
0.00
21
0.12
42
37.93
28 780.38
8913
0.00
52
0.28
182
0.01
179
0.85
773
0.00
49
0.29
95
1.43
3337
7.12 244499
0.02
393
1.35
2295
6.55
7530 32.97
53911
0.00
352
0.74
435
0.63
947
2.29
2291
0.00
158
0.50
227
6.36
7448
8.27 165170
14.07 188564 111.99 1669788
0.01
277
2.10
1532
0.01
33
0.74
562
0.07
146
1.78
4103
12.86
23371 87.91 1036992
0.00
21
0.16
54
0.18
1773
1.29
1908
0.00
256
0.50
333
0.05
407
2.18
4142
0.00
121
0.74
434
4.05
19865
6.91
80785
0.04
515
2.32
5075
11.08 200559 101.21
23.32 27728751408.64
0.02
390
1.40
690

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.02
0.01
0.00
0.00
0.04
0.00
0.00
0.67
0.00
0.00
0.12
0.00
0.00
0.00
1.27
0.01
0.25
0.00
0.01
0.00
0.63
9.44
0.00
0.00
0.01
6.74
0.00
0.01
0.00
0.01
0.00
0.25
0.01

9 0.00
47 0.00
28 0.00
102 0.00
69 0.00
9 0.00
62 0.00
52 0.00
20 0.00
376 0.01
142 0.00
113 0.00
86 0.00
18 0.00
324 0.02
1876 0.01
14 0.00
91 0.00
6925 0.08
75 0.00
31 0.00
177138 1.43
116 0.00
31 0.00
854 0.18
142 0.00
616 0.00
79 0.00
192459 2.32
1834 0.01
16766 0.36
424 0.00
1073 0.01
216 0.00
61548 1.06
717884 18.27
1342 0.01
357 0.00
2597 0.02
229210 9.51
35 0.00
1636 0.01
315 0.00
3235 0.02
358 0.00
65984 0.63
4406 0.02
19020089286.02

0.00

642

psr-small-ipc4
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50

8
10 0.00
10 0.00
10 0.00
11
52 0.00
55 0.00
52 0.00
11
31 0.00
31 0.00
31 0.00
10
66 0.00
91 0.00
73 0.00
11
75 0.00
79 0.00
75 0.00
8
10 0.00
10 0.00
10 0.00
11
61 0.00
61 0.00
61 0.00
8
24 0.00
29 0.00
25 0.00
8
18 0.00
19 0.00
18 0.00
7
131 0.01
183 0.00
155 0.01
19
149 0.00
149 0.00
149 0.00
16
120 0.00
123 0.00
120 0.00
15
90 0.00
90 0.00
90 0.00
9
19 0.00
19 0.00
19 0.00
10
1200 0.08
708 0.03
769 0.09
25
2328 0.02
2158 0.01
2176 0.03
9
15 0.00
15 0.00
15 0.00
12
85 0.00
90 0.00
85 0.00
25
8025 0.11
7856 0.05
7876 0.12
17
80 0.00
80 0.00
80 0.00
10
28 0.00
28 0.00
28 0.00
33
163299 4.17 176058 1.56 168685 5.01
12
77 0.00
93 0.00
77 0.00
10
28 0.00
28 0.00
28 0.00
9
485 3.06
463 0.58
482 3.28
17
144 0.00
150 0.00
146 0.00
21
616 0.01
675 0.00
650 0.01
14
79 0.00
79 0.00
79 0.00
21
142772 4.55 187319 2.12 159325 5.80
22
1791 0.03
1982 0.01
1883 0.04
19
11278 0.25
6810 0.08
8297 0.24
24
431 0.01
431 0.00
431 0.01
21
1480 0.02
1436 0.01
1391 0.03
21
223 0.00
223 0.00
223 0.00
22
65965 1.43
63186 0.46
68281 1.70
22
571766 12.62 371834 3.41 458402 11.77
23
1307 0.03
1417 0.01
1363 0.03
13
301 0.01
372 0.00
326 0.01
23
2486 0.05
2942 0.02
2682 0.07
20
259683 8.59 182608 2.70 270195 11.73
10
31 0.00
34 0.00
31 0.00
30
1855 0.02
1747 0.01
1739 0.02
20
328 0.00
328 0.00
328 0.00
19
2990 0.07
3430 0.03
3121 0.08
20
347 0.00
376 0.00
359 0.01
34
60888 0.86
61842 0.31
61563 0.99
27
4104 0.09
4522 0.03
4284 0.11
37 12080249604.4317435137247.2013514084784.80
47
23
637 0.01
659 0.01
645 0.02

9 0.00
9
12 0.00
12
12 0.00
12
11 0.00
11
12 0.00
12
9 0.00
9
12 0.00
12
9 0.00
9
9 0.00
9
8 0.04
8
20 0.00
20
17 0.00
17
16 0.00
16
10 0.00
10
11 0.46
11
975 0.11
26
10 0.00
10
13 0.00
13
2910 0.27
26
18 0.00
18
11 0.00
11
34 0.28
34
13 0.00
13
11 0.00
11
10 5.42
10
18 0.00
18
22 0.01
22
15 0.00
15
22 0.39
22
23 0.01
23
2647 0.89
723
25 0.00
25
446 0.26
22
22 0.00
22
24021 0.83 11113
48350 2.98
2783
24 0.02
24
14 0.01
14
24 0.08
24
38837 1.88
7767
11 0.00
11
1117 0.18
31
21 0.00
21
20 0.05
20
21 0.01
21
36941 0.67 32582
28 0.04
28
129627 2.37
2500
2048368 15.84 594399
24 0.02
24

0.00

tpp-ipc5
01
02
03
04
05
06

5
8
11
14
19
25

6 0.00
6 0.00
6 0.00
9 0.00
11 0.00
9 0.00
12 0.00
27 0.00
16 0.00
15 0.00
78 0.00
47 0.00
623 0.02
5110 0.08
1455 0.05
5843306179.03 6916518 95.86 6153923222.35

6 0.00
9 0.00
12 0.00
15 0.01
20 0.36
947059 14.22

6 0.00
9 0.00
12 0.00
15 0.00
20 0.77
74798 23.97

6
9
12
15
624

0.01
0.01
0.03
0.07
0.48

7
26
116
494
24698

0.00
0.00
0.00
0.00
0.12

6 0.00
16 0.00
83 0.00
430 0.00
17398 0.15
9267024216.69

trucks-ipc5
01
02
03
04
05
06
07
08
09

13
1691 0.03
1027 0.01
1039 0.03
14 0.03
14 0.02
17
9624 0.23
2898 0.04
2957 0.11
4192 0.22
18 0.17
20
80693 2.99
20752 0.44
22236 1.14
199405 2.89 173790 6.88
23 1753866 48.55 1205793 23.48 1315672 50.35 2591561 29.172568634 56.96
25 12472562515.50 8007189242.98 9483222512.55 23444940392.99
30
23 2134728 96.15 719751 16.91 755608 50.72 7575415 88.918080496117.13
25
5199440221.76 6630689687.95
28

285
0.56
5774 0.02
402 0.01
1413
1.04
28348 0.14
939 0.03
4049
4.43 379582 2.97
9465 0.40
8817
7.75 2990366 26.65 209140 9.43
14744 23.12
1248571 90.78
308920 343.47
43270 27.6212410588117.92 223011 19.34
49663 47.61
3106944403.36
233577 248.21

zenotravel-ipc3
01
02
03
04
05
06
07
08
09
10
11

1
6
6
8
11
11
15
11
21
22
14

2 0.00
2 0.00
2 0.00
17 0.00
18 0.00
17 0.00
28 0.01
18 0.01
12 0.01
99 0.01
88 0.01
81 0.01
177 0.01
220 0.01
136 0.02
2287 0.10
1144 0.05
504 0.05
5088 0.16
4234 0.09
4199 0.19
3268 0.35
1026 0.12
1655 0.32
2844771177.70 2842546176.05 2433822262.84
2283679295.65 1921903196.38 1832871383.99
139687 18.63
76904 8.20
93782 19.51

2 0.00
2 0.00
7 0.00
7 0.00
7 0.21
7 0.90
9 0.20
9 0.89
12 0.25
12 1.90
12 0.38
12 3.54
16 0.38
16 3.48
14354 2.00
12 14.48
2517035 51.18 611457 30.47
1322871 34.84 137872 25.44
310030 11.28 110726 26.65

2
0.45
9
0.46
40
3.42
215
3.44
422
7.70
1957 11.81
34890 30.36
83533 292.05

2
58
5160
5256
82289
596531
405626

0.00
0.00
0.04
0.03
0.63
5.90
3.56

2 0.00
22 0.00
492 0.02
665 0.01
12466 0.33
85931 2.47
115348 2.60
687846 50.76

Table 19: Similar to Table 15 for the PSR, TPP, Trucks, and Zenotravel domains.
120

fiImplicit Abstraction Heuristics

hF
task

h

hI

hFI

nodes timenodes time nodes

MS-104
time nodes

MS-105

timenodes

HSPF

time nodes

blind
hmax
time nodes time nodes
time

schedule-strips
02-0
02-1
02-2
02-3
02-4
02-5
02-6
02-7
02-8
02-9
03-0
03-1
03-2
03-3
03-4
03-5
03-6
03-7
03-8
03-9
04-0
04-1
04-2
04-3
04-4
04-5
04-6
04-7
04-8
04-9
05-0
05-1
05-2
05-3
05-4
05-5
05-6
05-7
05-8
05-9
06-2
06-4
06-6
07-0
07-7
07-9

3
5 0.07
5 0.04
5 0.08
2
3 0.08
4 0.05
3 0.10
2
3 0.17
3 0.06
3 0.19
3
26 0.17
37 0.06
26 0.18
3
68 0.17 188 0.07
220 0.26
2
3 0.17
3 0.05
3 0.19
2
3 0.07
5 0.04
3 0.09
2
3 0.15
3 0.05
3 0.17
2
3 0.17
3 0.05
3 0.19
3
5 0.07
5 0.04
5 0.08
4
40 0.31 407 0.16
140 0.45
2
3 0.22
3 0.08
3 0.25
4
27 0.21
50 0.09
33 0.25
4
15 0.13
91 0.09
15 0.15
3
4 0.39
16 0.10
4 0.44
4
73 0.38 471 0.14
74 0.43
4
72 0.12
75 0.08
69 0.13
4
28 0.23
50 0.09
28 0.25
4
273 0.43 266 0.14
273 0.48
4
8 0.23
31 0.09
14 0.27
5
373 0.45 1498 0.50
167 0.54
6 1755915.4510707 3.48 17686 17.58
5
209 0.40 406 0.19
66 0.34
5
142 0.40 674 0.25
251 0.58
5
921 1.14 450 0.31
574 1.39
6
483 0.95 4544 1.11
850 2.11
6
779 0.5611610 2.44 1834 1.43
5
99 0.58 424 0.31
163 0.78
5
102 0.52 573 0.24
111 0.60
4
1043 1.27 996 0.67 1050 1.66
5
163 0.86 483 0.51
167 1.05
6
2701 2.951887811.36 1257 3.10
7 11885586.65
158640178.66
7 2715924.884144713.08 13622 16.72
6
989 1.63 3433 1.29
582 1.36
6
198 0.61 9550 4.61
347 1.05
7
603311.164987316.17 10325 16.63
6
944 1.9217562 9.03 2107 4.10
7
1190 2.436153920.22 2709 7.24
6
1537 2.2415829 6.85 2717 5.45
6
888 3.292698622.47 1709 6.91
8 1153520.81
56273131.69
8 1558946.68
41764133.76
7
2489 9.10
6995 25.49
8 1072641.01
38251154.49
8
682919.20
30148109.49

4 511.10
3 104.98
3 231.99
4 56.51
3
3
3
3
4

363.11
121.84
323.77
316.53
251.46

5 191.03
5 259.13
5 682.30
5 121.58
5 195.72
5 235.48
71115.76
6 267.29
7
7
6
6
5

837.68
459.19
936.68
711.65
316.22

41743.32

5
3
3
4
4
3
3
3
3
5

577.39
754.26
495.56
658.90
484.62
667.32
697.42
604.06
668.79
577.16

76 0.02
5
0.09
6 0.02
3
0.07
5 0.02
3
0.07
529 0.03
95
0.45
543 0.03
108
0.44
3 0.03
3
0.07
6 0.02
3
0.06
13 0.02
3
0.07
8 0.02
3
0.07
76 0.03
5
0.09
11915 0.60 1127
8.98
31 0.04
25
0.37
3617 0.23 1228
9.56
3379 0.23
170
1.85
41223.90
301 0.06
22
0.27
12217 0.64 1175 12.43
2663 0.19 1542 11.73
12859 0.68 1323 13.47
12616 0.65 1590 11.13
4339 0.27
913
7.69
31219326.88 22993 273.38
55206949.79
47696 4.97 9703 131.69
89272 8.74 12941 163.84
62013 6.03 13614 168.07
1079781399.99
1071151001.40
61327 5.97 8683 103.50
34046729.56 15122 181.98
41673 4.27 5480 83.69
14335022.71 43336 751.35

120602 989.42

Table 20: Similar to Table 15 for the (non-IPC) Schedule-STRIPS domain.

121

fiKatz & Domshlak

hF
task

h

nodes

hI

hFI

HSPF

blind

time

nodes

time

nodes

time

nodes

time

nodes

time

0.39
0.45
4.00
10.59
68.19
125.83
1.34
3.39
18.43
43.80
31.37
327.82
5.99
112.11
335.39
291.88
203.57
249.28

10507
5184
219439
294029
3269854
3869775
50734
78362
432280
1325517
2823019

0.84
1.12
13.43
74.29
290.07
1167.78
4.64
23.84
66.00
337.57
570.43

8333
4044
139760
146396
2113017
1965371
31545
46386
297147
687420
1255479

1.03
1.46
15.62
62.26
317.53
965.39
5.00
21.36
68.80
290.86
425.27

12935
4810
276441
278087

30.55
42.63
469.96
885.94

26670
16162
650316
1025329
9567169

0.40
0.49
11.32
29.51
174.38

72109
74663

190.25
325.43

145170
152021
1426461
6238743

2.53
4.47
32.06
199.63

79574
859710
10935187

9.93
349.90
1208.05

66582
757718
7542146

13.62
395.63
1319.93

123510

443.99

194669
1633295

4.28
57.19

4430537

1578.04

49
144
317
2208
4220
998
61253
70808
4920
5261
98783
10580
398023
157304
711526
411732
421646
34754
812451
473553
173929

0.37
0.73
1.32
2.63
4.87
5.66
40.74
57.12
18.26
23.40
105.44
43.05
443.57
222.14
1034.92
671.53
745.34
186.40
1731.49
1018.62
651.93

193
769
1665
8113
17151
3288
201137
234328
114281
72673
563261
341169
2547985
1233115
11926297
4928793
8065113
953049

0.00
0.00
0.01
0.06
0.16
0.02
2.31
2.92
1.32
0.76
7.17
4.11
35.07
17.19
184.57
75.73
128.80
14.32

1536764

27.62

12
19
334
993
6922
19613
10
153
8348
422571
9
22
260
2281
68293
121897

0.20
1.44
0.72
11.21
35.00
115.36
0.55
3.39
18.14
792.78
0.09
0.51
2.03
6.38
145.47
404.98

20
1375
4903
12302518

0.00
0.01
0.03
126.46

23
5138
1130810

0.00
0.05
12.72

16
2485
285823

0.00
0.02
3.32

44047
45529
45882
10175657
10310817
10321465
54
54
54
10170980
10254740
10294023

0.68
0.54
0.49
314.87
242.27
222.66
0.01
0.01
0.01
113.29
95.91
88.03

elevators-strips-ipc6
01
02
03
04
05
06
11
12
13
14
15
18
21
22
23
24
25
26

42
26
55
40
55
53
56
54
59
63
66
61
48
54
69
56
63
48

7483
2898
61649
60039
909822
716238
18313
21812
186526
248709
201777
1057327
71003
890048
4089071
1430559
1384406
699757

openstacks-strips-ipc6
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
21
22

2
2
2
3
4
2
5
5
3
3
4
3
4
4
4
4
4
3
4
3
4

209
769
1729
8209
16705
3658
195109
228847
116425
77681
575677
354913
2596593
1260363
11995225
5064737
8193065
1020905

0.00
0.02
0.04
0.17
0.41
0.11
5.85
7.77
5.03
3.57
28.75
19.85
150.86
81.43
867.27
379.45
673.91
88.15

209
769
1729
8209
16705
3658
195109
228847
116425
77681
575677
354913
2596593
1260363
11995225
5064737
8193065
1020905

0.00
0.00
0.01
0.07
0.18
0.04
2.45
3.23
1.61
1.10
9.11
5.63
46.30
23.36
245.32
104.37
179.00
22.24

209
769
1729
8209
16705
3658
195109
228847
116425
77681
575677
354913
2596593
1260363
11995225
5064737
8193065
1020905

0.00
0.02
0.04
0.20
0.48
0.13
6.85
9.06
5.77
4.14
32.97
22.85
172.13
93.01
987.24
432.44
765.15
99.67

1805050

204.83

1805050

48.73

1805050

233.98

0.01
0.02
0.04
13.85
219.49
613.02
0.01
0.07
5.83
463.93
0.00
0.01
0.28
8.49

15
183
821
77520
892002
3529327
24
1243
144084

0.00
0.01
0.01
2.28
31.78
148.95
0.00
0.04
4.27

15
179
668
68116
822442
3443221
24
1135
97683

0.00
0.02
0.03
7.56
115.96
557.75
0.01
0.08
9.25

13
303
15825
694503

0.00
0.01
0.47
24.62

13
282
8778
316839

0.00
0.02
0.63
31.37

1.68
1.88
1.90
687.38
870.50
869.52
0.14
0.15
0.14
834.36
720.23
643.41

22012
37569
43298

5.20
4.36
4.02

19809
37524
43298

6.39
6.07
5.71

21259
29253
37754

13.69
13.92
14.05

51
51
51

0.08
0.08
0.08

46
46
46

0.20
0.19
0.19

6
6
6

0.05
0.05
0.05

parcprinter-strips-ipc6
01
02
03
04
05
06
11
12
13
14
21
22
23
24
25
26

169009
438047
807114
876094
1145132
1514200
182808
510256
693064
1020512
143411
375821
519232
751642
1215840
1216460

19
240
880
142314
1780073
4113487
25
1183
74201
4491265
13
225
4376
96748

scanalyzer-strips-ipc6
01
02
03
04
05
06
22
23
24
25
26
27

18
22
26
24
30
36
13
13
13
26
30
34

19788
37182
43115
3947796
9193480
10140909
46
46
46
8974317
9936832
10202674

Table 21: Runtimes of cost-optimal heuristic-search planners on the Elevators,
Openstacks-strips-08, Parcprinter, and Scanalyzer domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance,
column h denotes optimal solution length. Other columns capture the run time
and number of expanded nodes.

122

fiImplicit Abstraction Heuristics

hF
task

h

hI

nodes

hFI

HSPF

blind

time

nodes

time

nodes

time

nodes

time

nodes

time

0.02
0.07
0.07
0.07
0.03
0.16
0.08
2.56
0.36
2.45
1.08
2.98
2.51
4.82
5.84
0.83
22.38
4.93
20.71
27.36
41.78
5.66
97.46
106.00
68.95
553.11
523.12

10
83
209
181
251
901
110
25253
3951
28241
12881
37358
33374
55127
73733
10598
300972
50222
257988
293860
494477
48190
954593
1219589
899323
6943124
2121936

0.02
0.12
0.12
0.12
0.02
0.14
0.12
0.71
0.20
0.77
0.38
0.86
0.76
1.29
1.67
0.33
6.38
1.37
5.62
8.56
11.49
1.43
25.16
31.83
25.33
177.06
82.61

10
83
209
181
251
901
110
25253
3951
28241
12881
37358
33374
55127
73733
10598
300972
50222
257988
293860
494477
48190
954593
1219589
899323
6943124
2121936

0.03
0.18
0.19
0.19
0.04
0.27
0.18
2.89
0.53
3.21
1.36
3.63
3.09
5.63
7.09
1.10
27.00
5.55
24.49
36.43
50.51
5.95
108.53
136.27
107.61
719.81
339.83

6
20
50
15
43
247
26
7898
757
7522
5979
21133
25897
17144
37810
7939
282810
10358
90950
83693
141906
13123
181830
271157
201932
2031156
132701

0.16
5.17
6.91
1.82
5.62
25.68
11.67
28.50
23.02
28.25
20.60
32.73
33.29
32.20
38.72
27.70
124.39
29.81
61.77
63.99
87.89
30.94
114.89
157.50
122.27
1024.04
118.20

11
66
174
192
242
1265
215
30776
3538
29658
13430
38561
32370
62047
76150
10090
294396
62726
275969
328583
545896
69465
1258767
1324907
830182
7178802
6091864

0.00
0.01
0.00
0.01
0.01
0.01
0.01
0.15
0.03
0.14
0.06
0.18
0.15
0.29
0.35
0.05
1.44
0.29
1.29
1.63
2.64
0.33
6.17
6.69
4.33
37.78
34.53

372
551
394
130524
50
526
47522
2114443
23083
69797
271598
155166
169436
20737
7943562
335238
80459
2109516
5238957
648
337852
5866700
3565151
14504610

0.03
0.02
0.01
5.57
0.32
0.04
2.81
135.12
1.47
3.17
15.63
10.98
8.93
1.05
602.99
20.20
4.17
156.88
354.84
0.14
74.21
473.77
222.48
1151.55

287
497
177
45048
203202
534
42195
1204212
26189
21291
282061
60655
294710
6984
7742698
242778
40425
119938
3558809
648
450027
4053413
3613835
2244156
23044275
12138101

0.01
0.01
0.00
0.28
7.28
0.01
0.38
11.27
0.26
0.18
2.09
0.70
3.63
0.07
84.75
1.66
0.29
0.97
33.60
0.69
14.64
45.18
50.31
30.10
275.91
152.95

269
509
173
44198
3073
526
28163
1080337
16013
20741
271598
46865
169436
6952
7456505
240912
36889
119784
3459314
648
76647
3868663
2563159
1759660
17832156
10473204
8738457

0.02
0.02
0.01
2.07
0.96
0.04
1.84
74.55
1.12
1.07
16.87
3.69
10.26
0.41
622.89
15.14
2.05
9.18
251.38
0.79
16.85
335.31
181.66
154.33
1612.04
996.25
1131.26

1079
700
621
282895

6.17
4.57
2.63
177.07

6815
75669

6.83
174.92

459188
620685
440869

400.45
315.43
586.91

1631677
178574

994.61
121.96

852948
239522

859.44
220.86

1762
1348
1165
320446
9607487
10526
315405
13329538
818693
852150
531305
4705742
2363177
255203
21598353
935561
317984
7219504
23255133
649

0.01
0.00
0.00
1.43
81.84
0.04
1.49
77.70
4.09
4.07
2.71
25.21
12.60
1.17
120.25
4.74
1.43
39.20
130.46
0.01

2074534

679.61
12
874
225310
1462063
111
9976
224986
67
4455
56897
292004

0.00
0.10
48.69
714.49
0.01
1.41
74.09
0.00
0.37
13.82
120.98

16
998
257608
1660874
103
11130
246069
62
5408
70579
382588

0.01
0.15
59.28
856.87
0.02
1.70
89.04
0.01
0.54
19.32
196.82

60
1567
380982

0.48
6.36
274.86

135
14874
373133
62
7544
100347

0.94
19.85
454.55
0.50
7.71
92.93

64
2093
408643
4204372
164
14796
408449
112
7610
106548
1663856

0.00
0.01
3.69
50.69
0.00
0.12
4.36
0.00
0.06
1.07
19.29

119
409
80794
50
11665
113386
16
1931
4673

0.85
3.03
136.95
0.93
18.49
273.76
0.91
5.96
9.05

9086
21076

0.09
0.30

3487
1862476

0.05
37.91

227
177942
962698

0.00
3.97
23.76

pegsol-strips-ipc6
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

2
5
4
4
4
4
3
6
5
6
7
8
9
7
8
8
10
7
8
7
8
6
8
8
8
9
7

12
84
208
193
266
1343
217
31681
3743
29756
13832
39340
33379
63096
77932
10491
299676
63247
279822
329570
548254
69922
1262645
1326517
830637
7196836
6092258

sokoban-strips-ipc6
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
30

11
9
10
29
8
9
15
31
19
30
35
32
20
29
76
50
37
49
47
2
10
44
31
50
39
33
23
14

transport-strips-ipc6
01
02
03
04
11
12
13
21
22
23
24

54
131
250
318
456
594
550
478
632
630
614

60
1558
380375
3526204
135
14873
372845
62
7544
100269
1587821

0.01
0.06
10.47
164.35
0.02
0.37
15.07
0.01
0.18
3.65
77.96

woodworking-strips-ipc6
01
02
03
11
12
13
21
22
23

170
185
275
130
225
215
95
185
195

4313
5550

0.23
0.34

3716
5054

0.10
0.14

4157
5408

0.28
0.41

860
328229
4413726
54
31189
44641

0.10
41.44
954.34
0.02
4.66
8.39

987
328728
4125788
54
67528
155426

0.05
16.57
455.35
0.02
3.26
9.71

897
328930
4404104
53
38912
64840

0.13
52.03
1297.06
0.03
6.83
14.42

Table 22: Similar to Table 21 for the Pegsol, Sokoban, Transport, and Woodworking domains.
123

fiKatz & Domshlak

References
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (1-2), 165204.
Chen, H., & Gimenez, O. (2008). Causal graphs and structurally restricted planning. In Proceedings of the 18th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 3643, Sydney, Australia.
Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). Additive-disjunctive heuristics for
optimal planning. In Proceedings of the 18th International Conference on Automated
Planning and Scheduling (ICAPS), pp. 4451.
Culberson, J., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14 (4),
318334.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent off-line coordination: Structure and complexity. In Proceedings of Sixth European Conference on Planning (ECP), pp. 277288.
Domshlak, C., Hoffmann, J., & Sabharwal, A. (2009). Friends or foes? On planning as
satisfiability and abstract CNF encodings. Journal of Artificial Intelligence Research,
36, 415469.
Domshlak, C., Katz, M., & Lefler, S. (2010). When abstractions met landmarks. In Proceedings of the 20th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 5056, Toronto, Canada.
Drager, K., Finkbeiner, B., & Podelski, A. (2006). Directed model checking with distancepreserving abstractions. In Valmari, A. (Ed.), Proceedings of the 13th International
SPIN Workshop on Model Checking Software, Vol. 3925 of Lecture Notes in Computer
Science, pp. 1936, Berlin Heidelberg. Springer-Verlag.
Edelkamp, S. (2001). Planning with pattern databases. In Proceedings of the European
Conference on Planning (ECP), pp. 1334.
Edelkamp, S. (2002). Symbolic pattern databases in heuristic search planning. In Proceedings of the International Conference on AI Planning and Scheduling (AIPS), pp.
274293.
Edelkamp, S. (2006). Automated creation of pattern database search heuristics. In Proceedings of the 4th Workshop on Model Checking and Artificial Intelligence (MoChArt).
Edelkamp, S., & Kissmann, P. (2009). Optimal symbolic planning with action costs and
preferences. In Proceedings of the 21st International Joint Conference on Artificial
Intelligence (IJCAI), pp. 16901695, Pasadena, CA, US.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
of Artificial Intelligence Research, 22, 279318.
124

fiImplicit Abstraction Heuristics

Haslum, P. (2008). Additive and reversed relaxed reachability heuristics revisited. In Proceedings of the 6th International Planning Competition.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics for domainindependent planning. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI), pp. 11631168.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction of pattern database heuristics for cost-optimal planning. In Proceedings
of the 19th National Conference on Artificial Intelligence (AAAI), pp. 10071012.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings of the Fifth International Conference on Artificial Intelligence Planning Systems
(ICAPS), pp. 140149.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 146 (2), 219262.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Proceedings of
the 14th International Conference on Automated Planning and Scheduling (ICAPS),
pp. 161170, Whistler, Canada.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats
the difference anyway?. In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS), pp. 162169, Thessaloniki, Greece.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal
sequential planning. In Proceedings of the 17th International Conference on Automated
Planning and Scheduling (ICAPS), pp. 176183, Providence, RI, USA.
Helmert, M., & Mattmuller, R. (2008). Accuracy of admissible heuristic functions in selected planning domains. In Proceedings of the 23rd AAAI Conference on Artificial
Intelligence, pp. 938943, Chicago, USA.
Helmert, M. (2008). Understanding Planning Tasks: Domain Complexity and Heuristic
Decomposition, Vol. 4929 of Lecture Notes in Computer Science. Springer.
Hernadvolgyi, I., & Holte, R. (1999). PSVN: A vector representation for production systems.
Tech. rep. 1999-07, University of Ottawa.
Jonsson, A. (2007). The role of macros in tractable planning over causal graphs. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-07),
pp. 19361941.
Jonsson, P., & Backstrom, C. (1998). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (12), 125176.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Proceedings
of the International Joint Conference on Artificial Intelligence (IJCAI-09), pp. 1728
1733, Pasadena, CA, USA.
125

fiKatz & Domshlak

Katz, M., & Domshlak, C. (2007a). Structural patterns heuristics. In ICAPS-07 Workshop on Heuristics for Domain-independent Planning: Progress, Ideas, Limitations,
Challenges, Providence, RI, USA.
Katz, M., & Domshlak, C. (2007b). Structural patterns of tractable sequentially-optimal
planning. In Proceedings of the 17th International Conference on Automated Planning
and Scheduling (ICAPS), pp. 200207, Providence, RI, USA.
Katz, M., & Domshlak, C. (2008). Structural patterns heuristics via fork decomposition. In
Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS), pp. 182189, Sydney, Australia.
Katz, M., & Domshlak, C. (2009). Structural-pattern databases. In Proceedings of the
19th International Conference on Automated Planning and Scheduling (ICAPS), pp.
186193, Thessaloniki, Greece.
Katz, M., & Domshlak, C. (2010). Optimal admissible composition of abstraction heuristics.
Artificial Intelligence, 174, 767798.
Pearl, J. (1984). Heuristics - Intelligent Search Strategies for Computer Problem Solving.
Addison-Wesley.
Prieditis, A. (1993). Machine discovery of effective admissible heuristics. Machine Learning,
12, 117141.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. In Proceedings of
the Twenty-Third National Conference on Artificial Intelligence (AAAI), pp. 975982,
Chicago, IL, USA.
Yang, F., Culberson, J., & Holte, R. (2007). A general additive search abstraction. Tech.
rep. TR07-06, University of Alberta.
Yang, F., Culberson, J., Holte, R., Zahavi, U., & Felner, A. (2008). A general theory
of additive state space abstractions. Journal of Artificial Intelligence Research, 32,
631662.

126

fi